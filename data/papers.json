{
  "last_updated": "2025-10-30T00:52:49.237104",
  "papers": [
    {
      "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?",
      "authors": [
        "Yihao Li",
        "Saeed Salehi",
        "Lyle Ungar",
        "Konrad P. Kording"
      ],
      "abstract": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.",
      "pdf_url": "http://arxiv.org/pdf/2510.24709v1",
      "published": "2025-10-28T17:57:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24709v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ]
    },
    {
      "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?",
      "authors": [
        "Shuqing Li",
        "Jiayi Yan",
        "Chenyu Niu",
        "Jen-tse Huang",
        "Yun Peng",
        "Wenxuan Wang",
        "Yepang Liu",
        "Michael R. Lyu"
      ],
      "abstract": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.",
      "pdf_url": "http://arxiv.org/pdf/2510.24706v1",
      "published": "2025-10-28T17:55:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24706v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.SE"
      ]
    },
    {
      "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents",
      "authors": [
        "Yueqi Song",
        "Ketan Ramaneti",
        "Zaid Sheikh",
        "Ziru Chen",
        "Boyu Gou",
        "Tianbao Xie",
        "Yiheng Xu",
        "Danyang Zhang",
        "Apurva Gandhi",
        "Fan Yang",
        "Joseph Liu",
        "Tianyue Ou",
        "Zhihao Yuan",
        "Frank Xu",
        "Shuyan Zhou",
        "Xingyao Wang",
        "Xiang Yue",
        "Tao Yu",
        "Huan Sun",
        "Yu Su",
        "Graham Neubig"
      ],
      "abstract": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.",
      "pdf_url": "http://arxiv.org/pdf/2510.24702v1",
      "published": "2025-10-28T17:53:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24702v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Tongyi DeepResearch Technical Report",
      "authors": [
        "Tongyi DeepResearch Team",
        "Baixuan Li",
        "Bo Zhang",
        "Dingchu Zhang",
        "Fei Huang",
        "Guangyu Li",
        "Guoxin Chen",
        "Huifeng Yin",
        "Jialong Wu",
        "Jingren Zhou",
        "Kuan Li",
        "Liangcai Su",
        "Litu Ou",
        "Liwen Zhang",
        "Pengjun Xie",
        "Rui Ye",
        "Wenbiao Yin",
        "Xinmiao Yu",
        "Xinyu Wang",
        "Xixi Wu",
        "Xuanzhong Chen",
        "Yida Zhao",
        "Zhen Zhang",
        "Zhengwei Tao",
        "Zhongwang Zhang",
        "Zile Qiao",
        "Chenxi Wang",
        "Donglei Yu",
        "Gang Fu",
        "Haiyang Shen",
        "Jiayin Yang",
        "Jun Lin",
        "Junkai Zhang",
        "Kui Zeng",
        "Li Yang",
        "Hailong Yin",
        "Maojia Song",
        "Ming Yan",
        "Peng Xia",
        "Qian Xiao",
        "Rui Min",
        "Ruixue Ding",
        "Runnan Fang",
        "Shaowei Chen",
        "Shen Huang",
        "Shihang Wang",
        "Shihao Cai",
        "Weizhou Shen",
        "Xiaobin Wang",
        "Xin Guan",
        "Xinyu Geng",
        "Yingcheng Shi",
        "Yuning Wu",
        "Zhuo Chen",
        "Zijian Li",
        "Yong Jiang"
      ],
      "abstract": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
      "pdf_url": "http://arxiv.org/pdf/2510.24701v1",
      "published": "2025-10-28T17:53:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24701v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Greedy Sampling Is Provably Efficient for RLHF",
      "authors": [
        "Di Wu",
        "Chengshuai Shi",
        "Jing Yang",
        "Cong Shen"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a key\ntechnique for post-training large language models. Despite its empirical\nsuccess, the theoretical understanding of RLHF is still limited, as learning\nthe KL-regularized target with only preference feedback poses additional\nchallenges compared with canonical RL. Existing works mostly study the\nreward-based Bradley-Terry (BT) preference model, and extend classical designs\nutilizing optimism or pessimism. This work, instead, considers the general\npreference model (whose practical relevance has been observed recently) and\nobtains performance guarantees with major, order-wise improvements over\nexisting ones. Surprisingly, these results are derived from algorithms that\ndirectly use the empirical estimates (i.e., greedy sampling), as opposed to\nconstructing optimistic or pessimistic estimates in previous works. This\ninsight has a deep root in the unique structural property of the optimal policy\nclass under the KL-regularized target, and we further specialize it to the BT\nmodel, highlighting the surprising sufficiency of greedy sampling in RLHF.",
      "pdf_url": "http://arxiv.org/pdf/2510.24700v1",
      "published": "2025-10-28T17:52:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24700v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT",
        "stat.ML"
      ]
    },
    {
      "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
      "authors": [
        "Baixuan Li",
        "Dingchu Zhang",
        "Jialong Wu",
        "Wenbiao Yin",
        "Zhengwei Tao",
        "Yida Zhao",
        "Liwen Zhang",
        "Haiyang Shen",
        "Runnan Fang",
        "Pengjun Xie",
        "Jingren Zhou",
        "Yong Jiang"
      ],
      "abstract": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
      "pdf_url": "http://arxiv.org/pdf/2510.24698v1",
      "published": "2025-10-28T17:51:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24698v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
      "authors": [
        "Rui Ye",
        "Zhongwang Zhang",
        "Kuan Li",
        "Huifeng Yin",
        "Zhengwei Tao",
        "Yida Zhao",
        "Liangcai Su",
        "Liwen Zhang",
        "Zile Qiao",
        "Xinyu Wang",
        "Pengjun Xie",
        "Fei Huang",
        "Siheng Chen",
        "Jingren Zhou",
        "Yong Jiang"
      ],
      "abstract": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
      "pdf_url": "http://arxiv.org/pdf/2510.24699v1",
      "published": "2025-10-28T17:51:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24699v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
      "authors": [
        "Yida Zhao",
        "Kuan Li",
        "Xixi Wu",
        "Liwen Zhang",
        "Dingchu Zhang",
        "Baixuan Li",
        "Maojia Song",
        "Zhuo Chen",
        "Chenxi Wang",
        "Xinyu Wang",
        "Kewei Tu",
        "Pengjun Xie",
        "Jingren Zhou",
        "Yong Jiang"
      ],
      "abstract": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
      "pdf_url": "http://arxiv.org/pdf/2510.24694v1",
      "published": "2025-10-28T17:50:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24694v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning",
      "authors": [
        "Shengjie Liu",
        "Li Dong",
        "Zhenyu Zhang"
      ],
      "abstract": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.",
      "pdf_url": "http://arxiv.org/pdf/2510.24690v1",
      "published": "2025-10-28T17:50:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24690v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry",
      "authors": [
        "Andreas Hauptmann",
        "Leonid Kunyansky",
        "Jenni Poimala"
      ],
      "abstract": "The inverse source problem arising in photoacoustic tomography and in several\nother coupled-physics modalities is frequently solved by iterative algorithms.\nSuch algorithms are based on the minimization of a certain cost functional. In\naddition, novel deep learning techniques are currently being investigated to\nfurther improve such optimization approaches. All such methods require multiple\napplications of the operator defining the forward problem, and of its adjoint.\nIn this paper, we present new asymptotically fast algorithms for numerical\nevaluation of the forward and adjoint operators, applicable in the circular\nacquisition geometry. For an $(n \\times n)$ image, our algorithms compute these\noperators in $\\mathcal{O}(n^2 \\log n)$ floating point operations. We\ndemonstrate the performance of our algorithms in numerical simulations, where\nthey are used as an integral part of several iterative image reconstruction\ntechniques: classic variational methods, such as non-negative least squares and\ntotal variation regularized least squares, as well as deep learning methods,\nsuch as learned primal dual. A Python implementation of our algorithms and\ncomputational examples is available to the general public.",
      "pdf_url": "http://arxiv.org/pdf/2510.24687v1",
      "published": "2025-10-28T17:49:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24687v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.NA",
        "math.AP",
        "math.NA",
        "math.OC"
      ]
    },
    {
      "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation",
      "authors": [
        "Xun Liang",
        "Huayi Lai",
        "Hanyu Wang",
        "Wentao Zhang",
        "Linfeng Zhang",
        "Yanfang Chen",
        "Feiyu Xiong",
        "Zhiyu Li"
      ],
      "abstract": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor",
      "pdf_url": "http://arxiv.org/pdf/2510.24677v1",
      "published": "2025-10-28T17:40:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24677v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Learning to Drive Safely with Hybrid Options",
      "authors": [
        "Bram De Cooman",
        "Johan Suykens"
      ],
      "abstract": "Out of the many deep reinforcement learning approaches for autonomous\ndriving, only few make use of the options (or skills) framework. That is\nsurprising, as this framework is naturally suited for hierarchical control\napplications in general, and autonomous driving tasks in specific. Therefore,\nin this work the options framework is applied and tailored to autonomous\ndriving tasks on highways. More specifically, we define dedicated options for\nlongitudinal and lateral manoeuvres with embedded safety and comfort\nconstraints. This way, prior domain knowledge can be incorporated into the\nlearning process and the learned driving behaviour can be constrained more\neasily. We propose several setups for hierarchical control with options and\nderive practical algorithms following state-of-the-art reinforcement learning\ntechniques. By separately selecting actions for longitudinal and lateral\ncontrol, the introduced policies over combined and hybrid options obtain the\nsame expressiveness and flexibility that human drivers have, while being easier\nto interpret than classical policies over continuous actions. Of all the\ninvestigated approaches, these flexible policies over hybrid options perform\nthe best under varying traffic conditions, outperforming the baseline policies\nover actions.",
      "pdf_url": "http://arxiv.org/pdf/2510.24674v1",
      "published": "2025-10-28T17:40:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24674v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder",
      "authors": [
        "Li Li",
        "Tobias Brinkmann",
        "Till Temmen",
        "Markus Eisenbarth",
        "Jakob Andert"
      ],
      "abstract": "With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement.",
      "pdf_url": "http://arxiv.org/pdf/2510.24671v1",
      "published": "2025-10-28T17:36:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24671v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries",
      "authors": [
        "Mingyi Deng",
        "Lijun Huang",
        "Yani Fan",
        "Jiayi Zhang",
        "Fashen Ren",
        "Jinyi Bai",
        "Fuzhen Yang",
        "Dayi Miao",
        "Zhaoyang Yu",
        "Yifan Wu",
        "Yanfei Zhang",
        "Fengwei Teng",
        "Yingjia Wan",
        "Song Hu",
        "Yude Li",
        "Xin Jin",
        "Conghao Hu",
        "Haoyu Li",
        "Qirui Fu",
        "Tai Zhong",
        "Xinyu Wang",
        "Xiangru Tang",
        "Nan Tang",
        "Chenglin Wu",
        "Yuyu Luo"
      ],
      "abstract": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.",
      "pdf_url": "http://arxiv.org/pdf/2510.24668v1",
      "published": "2025-10-28T17:35:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24668v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs",
      "authors": [
        "Yifu Lu",
        "Shengjie Liu",
        "Li Dong"
      ],
      "abstract": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.",
      "pdf_url": "http://arxiv.org/pdf/2510.24663v1",
      "published": "2025-10-28T17:28:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24663v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning",
      "authors": [
        "Nitin Rai",
        "Daeun",
        "Choi",
        "Nathan S. Boyd",
        "Arnold W. Schumann"
      ],
      "abstract": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.",
      "pdf_url": "http://arxiv.org/pdf/2510.24650v1",
      "published": "2025-10-28T17:16:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24650v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling",
      "authors": [
        "Zengzhuang Xu",
        "Bingguang Hao",
        "Zechuan Wang",
        "Yuntao Wen",
        "Maolin Wang",
        "Yang Liu",
        "Long Chen",
        "Dong Wang",
        "Yicheng Chen",
        "Cunyin Peng",
        "Chenyi Zhuang",
        "Jinjie Gu",
        "Leilei Gan",
        "Xiangyu Zhao",
        "Shi Gu"
      ],
      "abstract": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.",
      "pdf_url": "http://arxiv.org/pdf/2510.24645v1",
      "published": "2025-10-28T17:15:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24645v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets",
      "authors": [
        "Yujun Kim",
        "Chaewon Moon",
        "Chulhee Yun"
      ],
      "abstract": "We study the parameter complexity of robust memorization for $\\mathrm{ReLU}$\nnetworks: the number of parameters required to interpolate any given dataset\nwith $\\epsilon$-separation between differently labeled points, while ensuring\npredictions remain consistent within a $\\mu$-ball around each training sample.\nWe establish upper and lower bounds on the parameter count as a function of the\nrobustness ratio $\\rho = \\mu / \\epsilon$. Unlike prior work, we provide a\nfine-grained analysis across the entire range $\\rho \\in (0,1)$ and obtain\ntighter upper and lower bounds that improve upon existing results. Our findings\nreveal that the parameter complexity of robust memorization matches that of\nnon-robust memorization when $\\rho$ is small, but grows with increasing $\\rho$.",
      "pdf_url": "http://arxiv.org/pdf/2510.24643v1",
      "published": "2025-10-28T17:09:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24643v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Causal Ordering for Structure Learning From Time Series",
      "authors": [
        "Pedro P. Sanchez",
        "Damian Machlanski",
        "Steven McDonagh",
        "Sotirios A. Tsaftaris"
      ],
      "abstract": "Predicting causal structure from time series data is crucial for\nunderstanding complex phenomena in physiology, brain connectivity, climate\ndynamics, and socio-economic behaviour. Causal discovery in time series is\nhindered by the combinatorial complexity of identifying true causal\nrelationships, especially as the number of variables and time points grow. A\ncommon approach to simplify the task is the so-called ordering-based methods.\nTraditional ordering methods inherently limit the representational capacity of\nthe resulting model. In this work, we fix this issue by leveraging multiple\nvalid causal orderings, instead of a single one as standard practice. We\npropose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based\ncausal discovery for temporal data. By integrating multiple orderings, DOTS\neffectively recovers the transitive closure of the underlying directed acyclic\ngraph, mitigating spurious artifacts inherent in single-ordering approaches. We\nformalise the problem under standard assumptions such as stationarity and the\nadditive noise model, and leverage score matching with diffusion processes to\nenable efficient Hessian estimation. Extensive experiments validate the\napproach. Empirical evaluations on synthetic and real-world datasets\ndemonstrate that DOTS outperforms state-of-the-art baselines, offering a\nscalable and robust approach to temporal causal discovery. On synthetic\nbenchmarks ($d{=}\\!3-\\!6$ variables, $T{=}200\\!-\\!5{,}000$ samples), DOTS\nimproves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the\nCausalTime real-world benchmark ($d{=}20\\!-\\!36$), while baselines remain the\nbest on individual datasets, DOTS attains the highest average summary-graph\n$F1$ while halving runtime relative to graph-optimisation methods. These\nresults establish DOTS as a scalable and accurate solution for temporal causal\ndiscovery.",
      "pdf_url": "http://arxiv.org/pdf/2510.24639v1",
      "published": "2025-10-28T17:06:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24639v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level Spiking Neural Networks",
      "authors": [
        "Andrea Castagnetti",
        "Alain Pegatoquet",
        "Benoît Miramond"
      ],
      "abstract": "Spiking Neural Networks (SNNs) are one of the most promising bio-inspired\nneural networks models and have drawn increasing attention in recent years. The\nevent-driven communication mechanism of SNNs allows for sparse and\ntheoretically low-power operations on dedicated neuromorphic hardware. However,\nthe binary nature of instantaneous spikes also leads to considerable\ninformation loss in SNNs, resulting in accuracy degradation. To address this\nissue, we propose a multi-level spiking neuron model able to provide both\nlow-quantization error and minimal inference latency while approaching the\nperformance of full precision Artificial Neural Networks (ANNs). Experimental\nresults with popular network architectures and datasets, show that multi-level\nspiking neurons provide better information compression, allowing therefore a\nreduction in latency without performance loss. When compared to binary SNNs on\nimage classification scenarios, multi-level SNNs indeed allow reducing by 2 to\n3 times the energy consumption depending on the number of quantization\nintervals. On neuromorphic data, our approach allows us to drastically reduce\nthe inference latency to 1 timestep, which corresponds to a compression factor\nof 10 compared to previously published results. At the architectural level, we\npropose a new residual architecture that we call Sparse-ResNet. Through a\ncareful analysis of the spikes propagation in residual connections we highlight\na spike avalanche effect, that affects most spiking residual architectures.\nUsing our Sparse-ResNet architecture, we can provide state-of-the-art accuracy\nresults in image classification while reducing by more than 20% the network\nactivity compared to the previous spiking ResNets.",
      "pdf_url": "http://arxiv.org/pdf/2510.24637v1",
      "published": "2025-10-28T17:03:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24637v1",
      "categories": [
        "cs.NE",
        "cs.AI"
      ]
    },
    {
      "title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation",
      "authors": [
        "Snegha A",
        "Sayambhu Sen",
        "Piyush Singh Pasi",
        "Abhishek Singhania",
        "Preethi Jyothi"
      ],
      "abstract": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.",
      "pdf_url": "http://arxiv.org/pdf/2510.24619v1",
      "published": "2025-10-28T16:48:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24619v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ]
    },
    {
      "title": "DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment",
      "authors": [
        "Hao Wang",
        "Licheng Pan",
        "Yuan Lu",
        "Zhixuan Chu",
        "Xiaoxi Li",
        "Shuting He",
        "Zhichao Chen",
        "Haoxuan Li",
        "Qingsong Wen",
        "Zhouchen Lin"
      ],
      "abstract": "Training time-series forecast models requires aligning the conditional\ndistribution of model forecasts with that of the label sequence. The standard\ndirect forecast (DF) approach resorts to minimize the conditional negative\nlog-likelihood of the label sequence, typically estimated using the mean\nsquared error. However, this estimation proves to be biased in the presence of\nlabel autocorrelation. In this paper, we propose DistDF, which achieves\nalignment by alternatively minimizing a discrepancy between the conditional\nforecast and label distributions. Because conditional discrepancies are\ndifficult to estimate from finite time-series observations, we introduce a\nnewly proposed joint-distribution Wasserstein discrepancy for time-series\nforecasting, which provably upper bounds the conditional discrepancy of\ninterest. This discrepancy admits tractable, differentiable estimation from\nempirical samples and integrates seamlessly with gradient-based training.\nExtensive experiments show that DistDF improves the performance diverse\nforecast models and achieves the state-of-the-art forecasting performance. Code\nis available at https://anonymous.4open.science/r/DistDF-F66B.",
      "pdf_url": "http://arxiv.org/pdf/2510.24574v1",
      "published": "2025-10-28T16:09:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24574v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis",
      "authors": [
        "Qingyue Zhang",
        "Chang Chu",
        "Tianren Peng",
        "Qi Li",
        "Xiangyang Luo",
        "Zhihao Jiang",
        "Shao-Lun Huang"
      ],
      "abstract": "With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication.",
      "pdf_url": "http://arxiv.org/pdf/2510.24561v1",
      "published": "2025-10-28T15:55:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24561v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives",
      "authors": [
        "Gang Chen",
        "Changshuo Liu",
        "Gene Anne Ooi",
        "Marcus Tan",
        "Zhongle Xie",
        "Jianwei Yin",
        "James Wei Luen Yip",
        "Wenqiao Zhang",
        "Jiaqi Zhu",
        "Beng Chin Ooi"
      ],
      "abstract": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.",
      "pdf_url": "http://arxiv.org/pdf/2510.24551v1",
      "published": "2025-10-28T15:47:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24551v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Quantum-Resistant Networks Using Post-Quantum Cryptography",
      "authors": [
        "Xin Jin",
        "Nitish Kumar Chandra",
        "Mohadeseh Azari",
        "Kaushik P. Seshadreesan",
        "Junyu Liu"
      ],
      "abstract": "Quantum networks rely on both quantum and classical channels for coordinated\noperation. Current architectures employ entanglement distribution and key\nexchange over quantum channels but often assume that classical communication is\nsufficiently secure. In practice, classical channels protected by traditional\ncryptography remain vulnerable to quantum adversaries, since large-scale\nquantum computers could break widely used public-key schemes and reduce the\neffective security of symmetric cryptography. This perspective presents a\nquantum-resistant network architecture that secures classical communication\nwith post-quantum cryptographic techniques while supporting entanglement-based\ncommunication over quantum channels. Beyond cryptographic protection, the\nframework incorporates continuous monitoring of both quantum and classical\nlayers, together with orchestration across heterogeneous infrastructures, to\nensure end-to-end security. Collectively, these mechanisms provide a pathway\ntoward scalable, robust, and secure quantum networks that remain dependable\nagainst both classical and quantum-era threats.",
      "pdf_url": "http://arxiv.org/pdf/2510.24534v1",
      "published": "2025-10-28T15:39:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24534v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning",
      "authors": [
        "Zihan Chen",
        "Song Wang",
        "Xingbo Fu",
        "Chengshuai Shi",
        "Zhenyu Lei",
        "Cong Shen",
        "Jundong Li"
      ],
      "abstract": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.",
      "pdf_url": "http://arxiv.org/pdf/2510.24528v1",
      "published": "2025-10-28T15:37:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24528v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Audio Signal Processing Using Time Domain Mel-Frequency Wavelet Coefficient",
      "authors": [
        "Rinku Sebastian",
        "Simon O'Keefe",
        "Martin Trefzer"
      ],
      "abstract": "Extracting features from the speech is the most critical process in speech\nsignal processing. Mel Frequency Cepstral Coefficients (MFCC) are the most\nwidely used features in the majority of the speaker and speech recognition\napplications, as the filtering in this feature is similar to the filtering\ntaking place in the human ear. But the main drawback of this feature is that it\nprovides only the frequency information of the signal but does not provide the\ninformation about at what time which frequency is present. The wavelet\ntransform, with its flexible time-frequency window, provides time and frequency\ninformation of the signal and is an appropriate tool for the analysis of\nnon-stationary signals like speech. On the other hand, because of its uniform\nfrequency scaling, a typical wavelet transform may be less effective in\nanalysing speech signals, have poorer frequency resolution in low frequencies,\nand be less in line with human auditory perception. Hence, it is necessary to\ndevelop a feature that incorporates the merits of both MFCC and wavelet\ntransform. A great deal of studies are trying to combine both these features.\nThe present Wavelet Transform based Mel-scaled feature extraction methods\nrequire more computation when a wavelet transform is applied on top of\nMel-scale filtering, since it adds extra processing steps. Here we are\nproposing a method to extract Mel scale features in time domain combining the\nconcept of wavelet transform, thus reducing the computational burden of\ntime-frequency conversion and the complexity of wavelet extraction. Combining\nour proposed Time domain Mel frequency Wavelet Coefficient(TMFWC) technique\nwith the reservoir computing methodology has significantly improved the\nefficiency of audio signal processing.",
      "pdf_url": "http://arxiv.org/pdf/2510.24519v1",
      "published": "2025-10-28T15:31:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24519v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments",
      "authors": [
        "Mortesa Hussaini",
        "Jan Theiß",
        "Anthony Stein"
      ],
      "abstract": "In the context of Federated Learning with heterogeneous data environments,\nlocal models tend to converge to their own local model optima during local\ntraining steps, deviating from the overall data distributions. Aggregation of\nthese local updates, e.g., with FedAvg, often does not align with the global\nmodel optimum (client drift), resulting in an update that is suboptimal for\nmost clients. Personalized Federated Learning approaches address this challenge\nby exclusively focusing on the average local performances of clients' models on\ntheir own data distribution. Generalization to out-of-distribution samples,\nwhich is a substantial benefit of FedAvg and represents a significant component\nof robustness, appears to be inadequately incorporated into the assessment and\nevaluation processes. This study involves a thorough evaluation of Federated\nLearning approaches, encompassing both their local performance and their\ngeneralization capabilities. Therefore, we examine different stages within a\nsingle communication round to enable a more nuanced understanding of the\nconsidered metrics. Furthermore, we propose and incorporate a modified approach\nof FedAvg, designated as Federated Learning with Individualized Updates (FLIU),\nextending the algorithm by a straightforward individualization step with an\nadaptive personalization factor. We evaluate and compare the approaches\nempirically using MNIST and CIFAR-10 under various distributional conditions,\nincluding benchmark IID and pathological non-IID, as well as additional novel\ntest environments with Dirichlet distribution specifically developed to stress\nthe algorithms on complex data heterogeneity.",
      "pdf_url": "http://arxiv.org/pdf/2510.24503v1",
      "published": "2025-10-28T15:15:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24503v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC",
        "cs.MA"
      ]
    },
    {
      "title": "Design and Optimization of Cloud Native Homomorphic Encryption Workflows for Privacy-Preserving ML Inference",
      "authors": [
        "Tejaswini Bollikonda"
      ],
      "abstract": "As machine learning (ML) models become increasingly deployed through cloud\ninfrastructures, the confidentiality of user data during inference poses a\nsignificant security challenge. Homomorphic Encryption (HE) has emerged as a\ncompelling cryptographic technique that enables computation on encrypted data,\nallowing predictions to be generated without decrypting sensitive inputs.\nHowever, the integration of HE within large scale cloud native pipelines\nremains constrained by high computational overhead, orchestration complexity,\nand model compatibility issues.\n  This paper presents a systematic framework for the design and optimization of\ncloud native homomorphic encryption workflows that support privacy-preserving\nML inference. The proposed architecture integrates containerized HE modules\nwith Kubernetes-based orchestration, enabling elastic scaling and parallel\nencrypted computation across distributed environments. Furthermore,\noptimization strategies including ciphertext packing, polynomial modulus\nadjustment, and operator fusion are employed to minimize latency and resource\nconsumption while preserving cryptographic integrity. Experimental results\ndemonstrate that the proposed system achieves up to 3.2times inference\nacceleration and 40% reduction in memory utilization compared to conventional\nHE pipelines. These findings illustrate a practical pathway for deploying\nsecure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality\nunder zero-trust cloud conditions.",
      "pdf_url": "http://arxiv.org/pdf/2510.24498v1",
      "published": "2025-10-28T15:13:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24498v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Online neural fusion of distortionless differential beamformers for robust speech enhancement",
      "authors": [
        "Yuanhang Qian",
        "Kunlong Zhao",
        "Jilu Jin",
        "Xueqin Luo",
        "Gongping Huang",
        "Jingdong Chen",
        "Jacob Benesty"
      ],
      "abstract": "Fixed beamforming is widely used in practice since it does not depend on the\nestimation of noise statistics and provides relatively stable performance.\nHowever, a single beamformer cannot adapt to varying acoustic conditions, which\nlimits its interference suppression capability. To address this, adaptive\nconvex combination (ACC) algorithms have been introduced, where the outputs of\nmultiple fixed beamformers are linearly combined to improve robustness.\nNevertheless, ACC often fails in highly non-stationary scenarios, such as\nrapidly moving interference, since its adaptive updates cannot reliably track\nrapid changes. To overcome this limitation, we propose a frame-online neural\nfusion framework for multiple distortionless differential beamformers, which\nestimates the combination weights through a neural network. Compared with\nconventional ACC, the proposed method adapts more effectively to dynamic\nacoustic environments, achieving stronger interference suppression while\nmaintaining the distortionless constraint.",
      "pdf_url": "http://arxiv.org/pdf/2510.24497v1",
      "published": "2025-10-28T15:12:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24497v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Diffusion Models for Wireless Transceivers: From Pilot-Efficient Channel Estimation to AI-Native 6G Receivers",
      "authors": [
        "Yuzhi Yang",
        "Sen Yan",
        "Weijie Zhou",
        "Brahim Mefgouda",
        "Ridong Li",
        "Zhaoyang Zhang",
        "Mérouane Debbah"
      ],
      "abstract": "With the development of artificial intelligence (AI) techniques, implementing\nAI-based techniques to improve wireless transceivers becomes an emerging\nresearch topic. Within this context, AI-based channel characterization and\nestimation become the focus since these methods have not been solved by\ntraditional methods very well and have become the bottleneck of transceiver\nefficiency in large-scale orthogonal frequency division multiplexing (OFDM)\nsystems. Specifically, by formulating channel estimation as a generative AI\nproblem, generative AI methods such as diffusion models (DMs) can efficiently\ndeal with rough initial estimations and have great potential to cooperate with\ntraditional signal processing methods. This paper focuses on the transceiver\ndesign of OFDM systems based on DMs, provides an illustration of the potential\nof DMs in wireless transceivers, and points out the related research directions\nbrought by DMs. We also provide a proof-of-concept case study of further\nadapting DMs for better wireless receiver performance.",
      "pdf_url": "http://arxiv.org/pdf/2510.24495v1",
      "published": "2025-10-28T15:10:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24495v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "A word association network methodology for evaluating implicit biases in LLMs compared to humans",
      "authors": [
        "Katherine Abramski",
        "Giulio Rossetti",
        "Massimo Stella"
      ],
      "abstract": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies.",
      "pdf_url": "http://arxiv.org/pdf/2510.24488v1",
      "published": "2025-10-28T15:03:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24488v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Sample-efficient and Scalable Exploration in Continuous-Time RL",
      "authors": [
        "Klemens Iten",
        "Lenart Treven",
        "Bhavya Sukhija",
        "Florian Dörfler",
        "Andreas Krause"
      ],
      "abstract": "Reinforcement learning algorithms are typically designed for discrete-time\ndynamics, even though the underlying real-world control systems are often\ncontinuous in time. In this paper, we study the problem of continuous-time\nreinforcement learning, where the unknown system dynamics are represented using\nnonlinear ordinary differential equations (ODEs). We leverage probabilistic\nmodels, such as Gaussian processes and Bayesian neural networks, to learn an\nuncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily\nmaximizes a weighted sum of the extrinsic reward and model epistemic\nuncertainty. This yields a scalable and sample-efficient approach to\ncontinuous-time model-based RL. We show that COMBRL achieves sublinear regret\nin the reward-driven setting, and in the unsupervised RL setting (i.e., without\nextrinsic rewards), we provide a sample complexity bound. In our experiments,\nwe evaluate COMBRL in both standard and unsupervised RL settings and\ndemonstrate that it scales better, is more sample-efficient than prior methods,\nand outperforms baselines across several deep RL tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.24482v1",
      "published": "2025-10-28T14:54:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24482v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems",
      "authors": [
        "Yihan Li",
        "Xiyuan Fu",
        "Ghanshyam Verma",
        "Paul Buitelaar",
        "Mingming Liu"
      ],
      "abstract": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2510.24476v1",
      "published": "2025-10-28T14:48:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24476v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization",
      "authors": [
        "Durga Prasad Maram",
        "Dhruvin Gandhi",
        "Zonghai Yao",
        "Gayathri Akkinapalli",
        "Franck Dernoncourt",
        "Yu Wang",
        "Ryan A. Rossi",
        "Nesreen K. Ahmed"
      ],
      "abstract": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.",
      "pdf_url": "http://arxiv.org/pdf/2510.24469v1",
      "published": "2025-10-28T14:36:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24469v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks",
      "authors": [
        "Korneel Van den Berghe",
        "Stein Stroobants",
        "Vijay Janapa Reddi",
        "G. C. H. E. de Croon"
      ],
      "abstract": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.",
      "pdf_url": "http://arxiv.org/pdf/2510.24461v1",
      "published": "2025-10-28T14:28:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24461v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Affordance Representation and Recognition for Autonomous Agents",
      "authors": [
        "Habtom Kahsay Gidey",
        "Niklas Huber",
        "Alexander Lenz",
        "Alois Knoll"
      ],
      "abstract": "The autonomy of software agents is fundamentally dependent on their ability\nto construct an actionable internal world model from the structured data that\ndefines their digital environment, such as the Document Object Model (DOM) of\nweb pages and the semantic descriptions of web services. However, constructing\nthis world model from raw structured data presents two critical challenges: the\nverbosity of raw HTML makes it computationally intractable for direct use by\nfoundation models, while the static nature of hardcoded API integrations\nprevents agents from adapting to evolving services.\n  This paper introduces a pattern language for world modeling from structured\ndata, presenting two complementary architectural patterns. The DOM Transduction\nPattern addresses the challenge of web page complexity by distilling} a\nverbose, raw DOM into a compact, task-relevant representation or world model\noptimized for an agent's reasoning core. Concurrently, the Hypermedia\nAffordances Recognition Pattern enables the agent to dynamically enrich its\nworld model by parsing standardized semantic descriptions to discover and\nintegrate the capabilities of unknown web services at runtime. Together, these\npatterns provide a robust framework for engineering agents that can efficiently\nconstruct and maintain an accurate world model, enabling scalable, adaptive,\nand interoperable automation across the web and its extended resources.",
      "pdf_url": "http://arxiv.org/pdf/2510.24459v1",
      "published": "2025-10-28T14:27:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24459v1",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.SE"
      ]
    },
    {
      "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices",
      "authors": [
        "Špela Vintar",
        "Taja Kuzman Pungeršek",
        "Mojca Brglez",
        "Nikola Ljubešić"
      ],
      "abstract": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.",
      "pdf_url": "http://arxiv.org/pdf/2510.24450v1",
      "published": "2025-10-28T14:13:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24450v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
      "authors": [
        "Pablo Acuaviva",
        "Aram Davtyan",
        "Mariam Hassan",
        "Sebastian Stapf",
        "Ahmad Rahimi",
        "Alexandre Alahi",
        "Paolo Favaro"
      ],
      "abstract": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
      "pdf_url": "http://arxiv.org/pdf/2510.24448v1",
      "published": "2025-10-28T14:12:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24448v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07, 68T45, 68T20",
        "I.2.10; I.4.8; I.5.1; I.2.6"
      ]
    },
    {
      "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents",
      "authors": [
        "Yiding Wang",
        "Yuxuan Chen",
        "Fanxu Meng",
        "Xifan Chen",
        "Xiaolei Yang",
        "Muhan Zhang"
      ],
      "abstract": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.",
      "pdf_url": "http://arxiv.org/pdf/2510.24442v1",
      "published": "2025-10-28T14:07:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24442v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.MA"
      ]
    },
    {
      "title": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content",
      "authors": [
        "Abdullah Mushtaq",
        "Rafay Naeem",
        "Ezieddin Elmahjub",
        "Ibrahim Ghaznavi",
        "Shawqi Al-Maliki",
        "Mohamed Abdallah",
        "Ala Al-Fuqaha",
        "Junaid Qadir"
      ],
      "abstract": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism.",
      "pdf_url": "http://arxiv.org/pdf/2510.24438v1",
      "published": "2025-10-28T14:05:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24438v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.MA"
      ]
    },
    {
      "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning",
      "authors": [
        "Benjamin Grando Moreira"
      ],
      "abstract": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.",
      "pdf_url": "http://arxiv.org/pdf/2510.24435v1",
      "published": "2025-10-28T14:02:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24435v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation",
      "authors": [
        "Xiaoyu Kong",
        "Leheng Sheng",
        "Junfei Tan",
        "Yuxin Chen",
        "Jiancan Wu",
        "An Zhang",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "abstract": "The recent success of large language models (LLMs) has renewed interest in\nwhether recommender systems can achieve similar scaling benefits. Conventional\nrecommenders, dominated by massive embedding tables, tend to plateau as\nembedding dimensions grow. In contrast, the emerging generative paradigm\nreplaces embeddings with compact Semantic ID (SID) sequences produced by\nautoregressive Transformers. Yet most industrial deployments remain\nproprietary, leaving two fundamental questions open: (1) Do the expected\nscaling laws hold on public benchmarks? (2) What is the minimal post-training\nrecipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully\nopen-source generative recommendation framework, which provides an end-to-end\nworkflow spanning SID construction, supervised fine-tuning, and\nrecommendation-oriented reinforcement learning. We generate SIDs via a Residual\nQuantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters\non the Amazon Review dataset. Our experiments reveal a consistent downward\ntrend in both training and evaluation losses with increasing model size,\nvalidating the parameter efficiency of the generative approach. To further\nenhance performance, we propose a lightweight yet effective post-training\npipeline that (1) enforces full-process SID alignment and (2) applies\nreinforcement learning with constrained decoding and hybrid rewards. Together,\nthese techniques yield significant improvements in both ranking accuracy and\ncandidate diversity.",
      "pdf_url": "http://arxiv.org/pdf/2510.24431v1",
      "published": "2025-10-28T13:58:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24431v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows",
      "authors": [
        "Qiushi Sun",
        "Mukai Li",
        "Zhoumianze Liu",
        "Zhihui Xie",
        "Fangzhi Xu",
        "Zhangyue Yin",
        "Kanzhi Cheng",
        "Zehao Li",
        "Zichen Ding",
        "Qi Liu",
        "Zhiyong Wu",
        "Zhuosheng Zhang",
        "Ben Kao",
        "Lingpeng Kong"
      ],
      "abstract": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.",
      "pdf_url": "http://arxiv.org/pdf/2510.24411v1",
      "published": "2025-10-28T13:22:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24411v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ]
    },
    {
      "title": "Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering",
      "authors": [
        "Michail Dadopoulos",
        "Anestis Ladas",
        "Stratos Moschidis",
        "Ioannis Negkakis"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) struggles on long, structured financial\nfilings where relevant evidence is sparse and cross-referenced. This paper\npresents a systematic investigation of advanced metadata-driven\nRetrieval-Augmented Generation (RAG) techniques, proposing and evaluating a\nnovel, multi-stage RAG architecture that leverages LLM-generated metadata. We\nintroduce a sophisticated indexing pipeline to create contextually rich\ndocument chunks and benchmark a spectrum of enhancements, including\npre-retrieval filtering, post-retrieval reranking, and enriched embeddings,\nbenchmarked on the FinanceBench dataset. Our results reveal that while a\npowerful reranker is essential for precision, the most significant performance\ngains come from embedding chunk metadata directly with text (\"contextual\nchunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval\noptimizations with these contextual embeddings to achieve superior performance.\nAdditionally, we present a custom metadata reranker that offers a compelling,\ncost-effective alternative to commercial solutions, highlighting a practical\ntrade-off between peak performance and operational efficiency. This study\nprovides a blueprint for building robust, metadata-aware RAG systems for\nfinancial document analysis.",
      "pdf_url": "http://arxiv.org/pdf/2510.24402v1",
      "published": "2025-10-28T13:16:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24402v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CE"
      ]
    },
    {
      "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training",
      "authors": [
        "Jiarui Qin",
        "Yunjia Xi",
        "Junjie Huang",
        "Renting Rui",
        "Di Yin",
        "Weiwen Liu",
        "Yong Yu",
        "Weinan Zhang",
        "Xing Sun"
      ],
      "abstract": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.",
      "pdf_url": "http://arxiv.org/pdf/2510.24397v1",
      "published": "2025-10-28T13:11:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24397v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion",
      "authors": [
        "Xianjun Gao",
        "Jianchun Liu",
        "Hongli Xu",
        "Liusheng Huang"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.",
      "pdf_url": "http://arxiv.org/pdf/2510.24390v1",
      "published": "2025-10-28T13:05:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24390v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents",
      "authors": [
        "Juraj Mavračić"
      ],
      "abstract": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.",
      "pdf_url": "http://arxiv.org/pdf/2510.24383v1",
      "published": "2025-10-28T12:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24383v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.MA",
        "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3"
      ]
    },
    {
      "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
      "authors": [
        "Pedram Fard",
        "Alaleh Azhir",
        "Neguine Rezaii",
        "Jiazi Tian",
        "Hossein Estiri"
      ],
      "abstract": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
      "pdf_url": "http://arxiv.org/pdf/2510.24359v1",
      "published": "2025-10-28T12:28:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24359v1",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "q-bio.QM",
        "stat.AP"
      ]
    },
    {
      "title": "Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning",
      "authors": [
        "Suman Sanyal"
      ],
      "abstract": "We introduce Perception Learning (PeL), a paradigm that optimizes an agent's\nsensory interface $f_\\phi:\\mathcal{X}\\to\\mathcal{Z}$ using task-agnostic\nsignals, decoupled from downstream decision learning\n$g_\\theta:\\mathcal{Z}\\to\\mathcal{Y}$. PeL directly targets label-free\nperceptual properties, such as stability to nuisances, informativeness without\ncollapse, and controlled geometry, assessed via objective\nrepresentation-invariant metrics. We formalize the separation of perception and\ndecision, define perceptual properties independent of objectives or\nreparameterizations, and prove that PeL updates preserving sufficient\ninvariants are orthogonal to Bayes task-risk gradients. Additionally, we\nprovide a suite of task-agnostic evaluation metrics to certify perceptual\nquality.",
      "pdf_url": "http://arxiv.org/pdf/2510.24356v1",
      "published": "2025-10-28T12:19:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.24356v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    }
  ]
}