{
  "last_updated": "2025-06-27T00:54:16.745824",
  "papers": [
    {
      "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs",
      "authors": [
        "Sonia K. Murthy",
        "Rosie Zhao",
        "Jennifer Hu",
        "Sham Kakade",
        "Markus Wulfmeier",
        "Peng Qian",
        "Tomer Ullman"
      ],
      "abstract": "Navigating everyday social situations often requires juggling conflicting\ngoals, such as conveying a harsh truth, maintaining trust, all while still\nbeing mindful of another person's feelings. These value trade-offs are an\nintegral part of human decision-making and language use, however, current tools\nfor interpreting such dynamic and multi-faceted notions of values in LLMs are\nlimited. In cognitive science, so-called \"cognitive models\" provide formal\naccounts of these trade-offs in humans, by modeling the weighting of a\nspeaker's competing utility functions in choosing an action or utterance. In\nthis work, we use a leading cognitive model of polite speech to interpret the\nextent to which LLMs represent human-like trade-offs. We apply this lens to\nsystematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models, and in\nopen-source models shown to be stronger in mathematical reasoning. Our findings\nfrom LLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. We show that our method\nis responsive to diverse aspects of the rapidly evolving LLM landscape, with\ninsights for forming hypotheses about other high-level behaviors, shaping\ntraining regimes for reasoning models, and better controlling trade-offs\nbetween values during model training.",
      "pdf_url": "http://arxiv.org/pdf/2506.20666v1",
      "published": "2025-06-25T17:58:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20666v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind",
      "authors": [
        "Andrei Lupu",
        "Timon Willi",
        "Jakob Foerster"
      ],
      "abstract": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents.",
      "pdf_url": "http://arxiv.org/pdf/2506.20664v1",
      "published": "2025-06-25T17:55:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20664v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.MA"
      ]
    },
    {
      "title": "Disentangled representations of microscopy images",
      "authors": [
        "Jacopo Dapueto",
        "Vito Paolo Pastore",
        "Nicoletta Noceti",
        "Francesca Odone"
      ],
      "abstract": "Microscopy image analysis is fundamental for different applications, from\ndiagnosis to synthetic engineering and environmental monitoring. Modern\nacquisition systems have granted the possibility to acquire an escalating\namount of images, requiring a consequent development of a large collection of\ndeep learning-based automatic image analysis methods. Although deep neural\nnetworks have demonstrated great performance in this field, interpretability,\nan essential requirement for microscopy image analysis, remains an open\nchallenge.\n  This work proposes a Disentangled Representation Learning (DRL) methodology\nto enhance model interpretability for microscopy image classification.\nExploiting benchmark datasets from three different microscopic image domains\n(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based\non transferring a representation learnt from synthetic data, can provide a good\ntrade-off between accuracy and interpretability in this domain.",
      "pdf_url": "http://arxiv.org/pdf/2506.20649v1",
      "published": "2025-06-25T17:44:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20649v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Towards Community-Driven Agents for Machine Learning Engineering",
      "authors": [
        "Sijie Li",
        "Weiwei Sun",
        "Shanda Li",
        "Ameet Talwalkar",
        "Yiming Yang"
      ],
      "abstract": "Large language model-based machine learning (ML) agents have shown great\npromise in automating ML research. However, existing agents typically operate\nin isolation on a given research problem, without engaging with the broader\nresearch community, where human researchers often gain insights and contribute\nby sharing knowledge. To bridge this gap, we introduce MLE-Live, a live\nevaluation framework designed to assess an agent's ability to communicate with\nand leverage collective knowledge from a simulated Kaggle research community.\nBuilding on this framework, we propose CoMind, a novel agent that excels at\nexchanging insights and developing novel solutions within a community context.\nCoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%\nhuman competitors on average across four ongoing Kaggle competitions. Our code\nis released at https://github.com/comind-ml/CoMind.",
      "pdf_url": "http://arxiv.org/pdf/2506.20640v1",
      "published": "2025-06-25T17:36:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20640v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Define-ML: An Approach to Ideate Machine Learning-Enabled Systems",
      "authors": [
        "Silvio Alonso",
        "Antonio Pedro Santos Alves",
        "Lucas Romao",
        "Hélio Lopes",
        "Marcos Kalinowski"
      ],
      "abstract": "[Context] The increasing adoption of machine learning (ML) in software\nsystems demands specialized ideation approaches that address ML-specific\nchallenges, including data dependencies, technical feasibility, and alignment\nbetween business objectives and probabilistic system behavior. Traditional\nideation methods like Lean Inception lack structured support for these ML\nconsiderations, which can result in misaligned product visions and unrealistic\nexpectations. [Goal] This paper presents Define-ML, a framework that extends\nLean Inception with tailored activities - Data Source Mapping, Feature-to-Data\nSource Mapping, and ML Mapping - to systematically integrate data and technical\nconstraints into early-stage ML product ideation. [Method] We developed and\nvalidated Define-ML following the Technology Transfer Model, conducting both\nstatic validation (with a toy problem) and dynamic validation (in a real-world\nindustrial case study). The analysis combined quantitative surveys with\nqualitative feedback, assessing utility, ease of use, and intent of adoption.\n[Results] Participants found Define-ML effective for clarifying data concerns,\naligning ML capabilities with business goals, and fostering cross-functional\ncollaboration. The approach's structured activities reduced ideation ambiguity,\nthough some noted a learning curve for ML-specific components, which can be\nmitigated by expert facilitation. All participants expressed the intention to\nadopt Define-ML. [Conclusion] Define-ML provides an openly available, validated\napproach for ML product ideation, building on Lean Inception's agility while\naligning features with available data and increasing awareness of technical\nfeasibility.",
      "pdf_url": "http://arxiv.org/pdf/2506.20621v1",
      "published": "2025-06-25T17:11:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20621v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation",
      "authors": [
        "Simon Perrin",
        "Sébastien Levilly",
        "Huajun Sun",
        "Harold Mouchère",
        "Jean-Michel Serfaty"
      ],
      "abstract": "In recent decades, the use of 4D Flow MRI images has enabled the\nquantification of velocity fields within a volume of interest and along the\ncardiac cycle. However, the lack of resolution and the presence of noise in\nthese biomarkers are significant issues. As indicated by recent studies, it\nappears that biomarkers such as wall shear stress are particularly impacted by\nthe poor resolution of vessel segmentation. The Phase Contrast Magnetic\nResonance Angiography (PC-MRA) is the state-of-the-art method to facilitate\nsegmentation. The objective of this work is to introduce a new handcraft\nfeature that provides a novel visualisation of 4D Flow MRI images, which is\nuseful in the segmentation task. This feature, termed Weighted Mean Frequencies\n(WMF), is capable of revealing the region in three dimensions where a voxel has\nbeen passed by pulsatile flow. Indeed, this feature is representative of the\nhull of all pulsatile velocity voxels. The value of the feature under\ndiscussion is illustrated by two experiments. The experiments involved\nsegmenting 4D Flow MRI images using optimal thresholding and deep learning\nmethods. The results obtained demonstrate a substantial enhancement in terms of\nIoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with\nthe PC-MRA feature, as evidenced by the deep learning task. This feature has\nthe potential to yield valuable insights that could inform future segmentation\nprocesses in other vascular regions, such as the heart or the brain.",
      "pdf_url": "http://arxiv.org/pdf/2506.20614v1",
      "published": "2025-06-25T17:04:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20614v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings",
      "authors": [
        "Ankit Shah",
        "Rita Singh",
        "Bhiksha Raj",
        "Alexander Hauptmann"
      ],
      "abstract": "The escalating rates of gun-related violence and mass shootings represent a\nsignificant threat to public safety. Timely and accurate information for law\nenforcement agencies is crucial in mitigating these incidents. Current\ncommercial gunshot detection systems, while effective, often come with\nprohibitive costs. This research explores a cost-effective alternative by\nleveraging acoustic analysis of gunshot recordings, potentially obtainable from\nubiquitous devices like cell phones, to not only detect gunshots but also\nclassify the type of firearm used. This paper details a study on deciphering\ngun type hierarchies using a curated dataset of 3459 recordings. We investigate\nthe fundamental acoustic characteristics of gunshots, including muzzle blasts\nand shockwaves, which vary based on firearm type, ammunition, and shooting\ndirection. We propose and evaluate machine learning frameworks, including\nSupport Vector Machines (SVMs) as a baseline and a more advanced Convolutional\nNeural Network (CNN) architecture for joint gunshot detection and gun type\nclassification. Results indicate that our deep learning approach achieves a\nmean average precision (mAP) of 0.58 on clean labeled data, outperforming the\nSVM baseline (mAP 0.39). Challenges related to data quality, environmental\nnoise, and the generalization capabilities when using noisy web-sourced data\n(mAP 0.35) are also discussed. The long-term vision is to develop a highly\naccurate, real-time system deployable on common recording devices,\nsignificantly reducing detection costs and providing critical intelligence to\nfirst responders.",
      "pdf_url": "http://arxiv.org/pdf/2506.20609v1",
      "published": "2025-06-25T17:00:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20609v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ]
    },
    {
      "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
      "authors": [
        "Barry Smith",
        "Junchao Zhang",
        "Hong Zhang",
        "Lois Curfman McInnes",
        "Murat Keceli",
        "Archit Vasan",
        "Satish Balay",
        "Toby Isaac",
        "Le Chen",
        "Venkatram Vishwanath"
      ],
      "abstract": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.",
      "pdf_url": "http://arxiv.org/pdf/2506.20608v1",
      "published": "2025-06-25T17:00:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20608v1",
      "categories": [
        "cs.AI",
        "cs.NA",
        "math.NA"
      ]
    },
    {
      "title": "CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video",
      "authors": [
        "Wengxi Li",
        "Roy Pea",
        "Nick Haber",
        "Hari Subramonyam"
      ],
      "abstract": "We introduce CogGen, a learner-centered AI architecture that transforms\nprogramming videos into interactive, adaptive learning experiences by\nintegrating student modeling with generative AI tutoring based on the Cognitive\nApprenticeship framework. The architecture consists of three components: (1)\nvideo segmentation by learning goals, (2) a conversational tutoring engine\napplying Cognitive Apprenticeship strategies, and (3) a student model using\nBayesian Knowledge Tracing to adapt instruction. Our technical evaluation\ndemonstrates effective video segmentation accuracy and strong pedagogical\nalignment across knowledge, method, action, and interaction layers. Ablation\nstudies confirm the necessity of each component in generating effective\nguidance. This work advances AI-powered tutoring by bridging structured student\nmodeling with interactive AI conversations, offering a scalable approach to\nenhancing video-based programming education.",
      "pdf_url": "http://arxiv.org/pdf/2506.20600v1",
      "published": "2025-06-25T16:39:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20600v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges",
      "authors": [
        "Alexander D. Kalian",
        "Jaewook Lee",
        "Stefan P. Johannesson",
        "Lennart Otte",
        "Christer Hogstrand",
        "Miao Guo"
      ],
      "abstract": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities",
      "pdf_url": "http://arxiv.org/pdf/2506.20598v1",
      "published": "2025-06-25T16:37:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20598v1",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "AI in the Writing Process: How Purposeful AI Support Fosters Student Writing",
      "authors": [
        "Momin N. Siddiqui",
        "Roy Pea",
        "Hari Subramonyam"
      ],
      "abstract": "The ubiquity of technologies like ChatGPT has raised concerns about their\nimpact on student writing, particularly regarding reduced learner agency and\nsuperficial engagement with content. While standalone chat-based LLMs often\nproduce suboptimal writing outcomes, evidence suggests that purposefully\ndesigned AI writing support tools can enhance the writing process. This paper\ninvestigates how different AI support approaches affect writers' sense of\nagency and depth of knowledge transformation. Through a randomized control\ntrial with 90 undergraduate students, we compare three conditions: (1) a\nchat-based LLM writing assistant, (2) an integrated AI writing tool to support\ndiverse subprocesses, and (3) a standard writing interface (control). Our\nfindings demonstrate that, among AI-supported conditions, students using the\nintegrated AI writing tool exhibited greater agency over their writing process\nand engaged in deeper knowledge transformation overall. These results suggest\nthat thoughtfully designed AI writing support targeting specific aspects of the\nwriting process can help students maintain ownership of their work while\nfacilitating improved engagement with content.",
      "pdf_url": "http://arxiv.org/pdf/2506.20595v1",
      "published": "2025-06-25T16:34:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20595v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Dense Video Captioning using Graph-based Sentence Summarization",
      "authors": [
        "Zhiwang Zhang",
        "Dong Xu",
        "Wanli Ouyang",
        "Luping Zhou"
      ],
      "abstract": "Recently, dense video captioning has made attractive progress in detecting\nand captioning all events in a long untrimmed video. Despite promising results\nwere achieved, most existing methods do not sufficiently explore the scene\nevolution within an event temporal proposal for captioning, and therefore\nperform less satisfactorily when the scenes and objects change over a\nrelatively long proposal. To address this problem, we propose a graph-based\npartition-and-summarization (GPaS) framework for dense video captioning within\ntwo stages. For the ``partition\" stage, a whole event proposal is split into\nshort video segments for captioning at a finer level. For the ``summarization\"\nstage, the generated sentences carrying rich description information for each\nsegment are summarized into one sentence to describe the whole event. We\nparticularly focus on the ``summarization\" stage, and propose a framework that\neffectively exploits the relationship between semantic words for summarization.\nWe achieve this goal by treating semantic words as nodes in a graph and\nlearning their interactions by coupling Graph Convolutional Network (GCN) and\nLong Short Term Memory (LSTM), with the aid of visual cues. Two schemes of\nGCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN\nand LSTM. The effectiveness of our approach is demonstrated via an extensive\ncomparison with the state-of-the-arts methods on the two benchmarks ActivityNet\nCaptions dataset and YouCook II dataset.",
      "pdf_url": "http://arxiv.org/pdf/2506.20583v1",
      "published": "2025-06-25T16:23:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20583v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Causal Representation Learning with Observational Grouping for CXR Classification",
      "authors": [
        "Rajat Rasal",
        "Avinash Kori",
        "Ben Glocker"
      ],
      "abstract": "Identifiable causal representation learning seeks to uncover the true causal\nrelationships underlying a data generation process. In medical imaging, this\npresents opportunities to improve the generalisability and robustness of\ntask-specific latent features. This work introduces the concept of grouping\nobservations to learn identifiable representations for disease classification\nin chest X-rays via an end-to-end framework. Our experiments demonstrate that\nthese causal representations improve generalisability and robustness across\nmultiple classification tasks when grouping is used to enforce invariance w.r.t\nrace, sex, and imaging views.",
      "pdf_url": "http://arxiv.org/pdf/2506.20582v1",
      "published": "2025-06-25T16:17:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20582v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS",
      "authors": [
        "Sabrine Ennaji",
        "Elhadj Benkhelifa",
        "Luigi V. Mancini"
      ],
      "abstract": "Adversarial attacks, wherein slight inputs are carefully crafted to mislead\nintelligent models, have attracted increasing attention. However, a critical\ngap persists between theoretical advancements and practical application,\nparticularly in structured data like network traffic, where interdependent\nfeatures complicate effective adversarial manipulations. Moreover, ambiguity in\ncurrent approaches restricts reproducibility and limits progress in this field.\nHence, existing defenses often fail to handle evolving adversarial attacks.\nThis paper proposes a novel approach for black-box adversarial attacks, that\naddresses these limitations. Unlike prior work, which often assumes system\naccess or relies on repeated probing, our method strictly respect black-box\nconstraints, reducing interaction to avoid detection and better reflect\nreal-world scenarios. We present an adaptive feature selection strategy using\nchange-point detection and causality analysis to identify and target sensitive\nfeatures to perturbations. This lightweight design ensures low computational\ncost and high deployability. Our comprehensive experiments show the attack's\neffectiveness in evading detection with minimal interaction, enhancing its\nadaptability and applicability in real-world scenarios. By advancing the\nunderstanding of adversarial attacks in network traffic, this work lays a\nfoundation for developing robust defenses.",
      "pdf_url": "http://arxiv.org/pdf/2506.20576v1",
      "published": "2025-06-25T16:10:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20576v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization",
      "authors": [
        "Zhiwang Zhang",
        "Dong Xu",
        "Wanli Ouyang",
        "Chuanqi Tan"
      ],
      "abstract": "In this work, we propose a division-and-summarization (DaS) framework for\ndense video captioning. After partitioning each untrimmed long video as\nmultiple event proposals, where each event proposal consists of a set of short\nvideo segments, we extract visual feature (e.g., C3D feature) from each segment\nand use the existing image/video captioning approach to generate one sentence\ndescription for this segment. Considering that the generated sentences contain\nrich semantic descriptions about the whole event proposal, we formulate the\ndense video captioning task as a visual cue aided sentence summarization\nproblem and propose a new two stage Long Short Term Memory (LSTM) approach\nequipped with a new hierarchical attention mechanism to summarize all generated\nsentences as one descriptive sentence with the aid of visual features.\nSpecifically, the first-stage LSTM network takes all semantic words from the\ngenerated sentences and the visual features from all segments within one event\nproposal as the input, and acts as the encoder to effectively summarize both\nsemantic and visual information related to this event proposal. The\nsecond-stage LSTM network takes the output from the first-stage LSTM network\nand the visual features from all video segments within one event proposal as\nthe input, and acts as the decoder to generate one descriptive sentence for\nthis event proposal. Our comprehensive experiments on the ActivityNet Captions\ndataset demonstrate the effectiveness of our newly proposed DaS framework for\ndense video captioning.",
      "pdf_url": "http://arxiv.org/pdf/2506.20567v1",
      "published": "2025-06-25T16:02:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20567v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DeepQuark: deep-neural-network approach to multiquark bound states",
      "authors": [
        "Wei-Lin Wu",
        "Lu Meng",
        "Shi-Lin Zhu"
      ],
      "abstract": "For the first time, we implement the deep-neural-network-based variational\nMonte Carlo approach for the multiquark bound states, whose complexity\nsurpasses that of electron or nucleon systems due to strong SU(3) color\ninteractions. We design a novel and high-efficiency architecture, DeepQuark, to\naddress the unique challenges in multiquark systems such as stronger\ncorrelations, extra discrete quantum numbers, and intractable confinement\ninteraction. Our method demonstrates competitive performance with\nstate-of-the-art approaches, including diffusion Monte Carlo and Gaussian\nexpansion method, in the nucleon, doubly heavy tetraquark, and fully heavy\ntetraquark systems. Notably, it outperforms existing calculations for\npentaquarks, exemplified by the triply heavy pentaquark. For the nucleon, we\nsuccessfully incorporate three-body flux-tube confinement interactions without\nadditional computational costs. In tetraquark systems, we consistently describe\nhadronic molecule $T_{cc}$ and compact tetraquark $T_{bb}$ with an unbiased\nform of wave function ansatz. In the pentaquark sector, we obtain weakly bound\n$\\bar D^*\\Xi_{cc}^*$ molecule $P_{cc\\bar c}(5715)$ with $S=\\frac{5}{2}$ and its\nbottom partner $P_{bb\\bar b}(15569)$. They can be viewed as the analogs of the\nmolecular $T_{cc}$. We recommend experimental search of $P_{cc\\bar c}(5715)$ in\nthe D-wave $J/\\psi \\Lambda_c$ channel. DeepQuark holds great promise for\nextension to larger multiquark systems, overcoming the computational barriers\nin conventional methods. It also serves as a powerful framework for exploring\nconfining mechanism beyond two-body interactions in multiquark states, which\nmay offer valuable insights into nonperturbative QCD and general many-body\nphysics.",
      "pdf_url": "http://arxiv.org/pdf/2506.20555v1",
      "published": "2025-06-25T15:53:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20555v1",
      "categories": [
        "hep-ph",
        "cs.AI",
        "hep-ex",
        "hep-lat",
        "nucl-th"
      ]
    },
    {
      "title": "Large Language Model-Driven Code Compliance Checking in Building Information Modeling",
      "authors": [
        "Soumya Madireddy",
        "Lu Gao",
        "Zia Din",
        "Kinam Kim",
        "Ahmed Senouci",
        "Zhe Han",
        "Yunpeng Zhang"
      ],
      "abstract": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects.",
      "pdf_url": "http://arxiv.org/pdf/2506.20551v1",
      "published": "2025-06-25T15:50:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20551v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks",
      "authors": [
        "Manyi Li",
        "Renshuai Tao",
        "Yufan Liu",
        "Chuangchuang Tan",
        "Haotong Qin",
        "Bing Li",
        "Yunchao Wei",
        "Yao Zhao"
      ],
      "abstract": "With the rapid advancement of deep learning, particularly through generative\nadversarial networks (GANs) and diffusion models (DMs), AI-generated images, or\n``deepfakes\", have become nearly indistinguishable from real ones. These images\nare widely shared across Online Social Networks (OSNs), raising concerns about\ntheir misuse. Existing deepfake detection methods overlook the ``block effects\"\nintroduced by compression in OSNs, which obscure deepfake artifacts, and\nprimarily focus on raw images, rarely encountered in real-world scenarios. To\naddress these challenges, we propose PLADA (Pay Less Attention to Deceptive\nArtifacts), a novel framework designed to tackle the lack of paired data and\nthe ineffective use of compressed images. PLADA consists of two core modules:\nBlock Effect Eraser (B2E), which uses a dual-stage attention mechanism to\nhandle block effects, and Open Data Aggregation (ODA), which processes both\npaired and unpaired data to improve detection. Extensive experiments across 26\ndatasets demonstrate that PLADA achieves a remarkable balance in deepfake\ndetection, outperforming SoTA methods in detecting deepfakes on OSNs, even with\nlimited paired data and compression. More importantly, this work introduces the\n``block effect\" as a critical factor in deepfake detection, providing a robust\nsolution for open-world scenarios. Our code is available at\nhttps://github.com/ManyiLee/PLADA.",
      "pdf_url": "http://arxiv.org/pdf/2506.20548v1",
      "published": "2025-06-25T15:46:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20548v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs",
      "authors": [
        "Ammar Khairi",
        "Daniel D'souza",
        "Ye Shen",
        "Julia Kreutzer",
        "Sara Hooker"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
      "pdf_url": "http://arxiv.org/pdf/2506.20544v1",
      "published": "2025-06-25T15:37:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20544v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads",
      "authors": [
        "Hongzhen Huang",
        "Kunming Zhang",
        "Hanlong Liao",
        "Kui Wu",
        "Guoming Tang"
      ],
      "abstract": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.",
      "pdf_url": "http://arxiv.org/pdf/2506.20535v1",
      "published": "2025-06-25T15:24:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20535v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios",
      "authors": [
        "Wenbin Gan",
        "Minh-Son Dao",
        "Koji Zettsu"
      ],
      "abstract": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.20531v1",
      "published": "2025-06-25T15:19:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20531v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation",
      "authors": [
        "Christian Internò",
        "Andrea Castellani",
        "Sebastian Schmitt",
        "Fabio Stella",
        "Barbara Hammer"
      ],
      "abstract": "Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of\nhigh-quality datasets and the complex variability of industrial energy\nconsumption patterns. To address data scarcity and privacy issues, we introduce\nthe Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an\nopen-source dataset generated using Digital Twin simulations. SIDED includes\nthree types of industrial facilities across three different geographic\nlocations, capturing diverse appliance behaviors, weather conditions, and load\nprofiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)\nmethod, a computationally efficient technique that enhances NILM model\ngeneralization by intelligently scaling appliance power contributions based on\ntheir relative impact. We show in experiments that NILM models trained with\nAMDA-augmented data significantly improve the disaggregation of energy\nconsumption of complex industrial appliances like combined heat and power\nsystems. Specifically, in our out-of-sample scenarios, models trained with AMDA\nachieved a Normalized Disaggregation Error of 0.093, outperforming models\ntrained without data augmentation (0.451) and those trained with random data\naugmentation (0.290). Data distribution analyses confirm that AMDA effectively\naligns training and test data distributions, enhancing model generalization.",
      "pdf_url": "http://arxiv.org/pdf/2506.20525v1",
      "published": "2025-06-25T15:10:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20525v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling",
      "authors": [
        "Zengzhi Wang",
        "Fan Zhou",
        "Xuefeng Li",
        "Pengfei Liu"
      ],
      "abstract": "Different base language model families, such as Llama and Qwen, exhibit\ndivergent behaviors during post-training with reinforcement learning (RL),\nespecially on reasoning-intensive tasks. What makes a base language model\nsuitable for reinforcement learning? Gaining deeper insight into this question\nis essential for developing RL-scalable foundation models of the next\ngeneration. In this work, we investigate how mid-training strategies shape RL\ndynamics, focusing on two representative model families: Qwen and Llama. Our\nstudy reveals that (1) high-quality mathematical corpora, such as\nMegaMath-Web-Pro, significantly improve both base model and RL performance,\nwhile existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further\nadding QA-style data, particularly long chain-of-thought (CoT) reasoning\nexamples, enhances RL outcomes, and instruction data further unlocks this\neffect; (3) while long-CoT improves reasoning depth, it can also induce\nverbosity of model responses and unstability of RL training, underscoring the\nimportance of data formatting; (4) scaling mid-training consistently leads to\nstronger downstream RL performance. Building on these insights, we introduce a\ntwo-stage mid-training strategy, Stable-then-Decay, in which base models are\nfirst trained on 200B tokens with a constant learning rate, followed by 20B\ntokens across three CoT-focused branches with learning rate decay. This yields\nOctoThinker, a family of models demonstrating strong RL compatibility and\nclosing the performance gap with more RL-friendly model families, i.e., Qwen.\nWe hope our work will help shape pre-training strategies for foundation models\nin the RL era. To support further research, we release our open-source models\nalong with a curated math reasoning-intensive corpus of over 70 billion tokens\n(i.e., MegaMath-Web-Pro-Max).",
      "pdf_url": "http://arxiv.org/pdf/2506.20512v1",
      "published": "2025-06-25T14:58:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20512v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Engineering Sentience",
      "authors": [
        "Konstantin Demin",
        "Taylor Webb",
        "Eric Elmoznino",
        "Hakwan Lau"
      ],
      "abstract": "We spell out a definition of sentience that may be useful for designing and\nbuilding it in machines. We propose that for sentience to be meaningful for AI,\nit must be fleshed out in functional, computational terms, in enough detail to\nallow for implementation. Yet, this notion of sentience must also reflect\nsomething essentially 'subjective', beyond just having the general capacity to\nencode perceptual content. For this specific functional notion of sentience to\noccur, we propose that certain sensory signals need to be both assertoric\n(persistent) and qualitative. To illustrate the definition in more concrete\nterms, we sketch out some ways for potential implementation, given current\ntechnology. Understanding what it takes for artificial agents to be\nfunctionally sentient can also help us avoid creating them inadvertently, or at\nleast, realize that we have created them in a timely manner.",
      "pdf_url": "http://arxiv.org/pdf/2506.20504v1",
      "published": "2025-06-25T14:49:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20504v1",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ]
    },
    {
      "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
      "authors": [
        "Haoze Wu",
        "Yunzhi Yao",
        "Wenhao Yu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
      "pdf_url": "http://arxiv.org/pdf/2506.20495v1",
      "published": "2025-06-25T14:41:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20495v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "title": "Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization",
      "authors": [
        "Salvatore Milite",
        "Giulio Caravagna",
        "Andrea Sottoriva"
      ],
      "abstract": "Neural Cellular Automata (NCAs) are a promising new approach to model\nself-organizing processes, with potential applications in life science.\nHowever, their deterministic nature limits their ability to capture the\nstochasticity of real-world biological and physical systems.\n  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework\nincorporating the idea of mixture models into the NCA paradigm. By combining\nprobabilistic rule assignments with intrinsic noise, MNCAs can model diverse\nlocal behaviors and reproduce the stochastic dynamics observed in biological\nprocesses.\n  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic\nsimulations of tissue growth and differentiation, (2) image morphogenesis\nrobustness, and (3) microscopy image segmentation. Results show that MNCAs\nachieve superior robustness to perturbations, better recapitulate real\nbiological growth patterns, and provide interpretable rule segmentation. These\nfindings position MNCAs as a promising tool for modeling stochastic dynamical\nsystems and studying self-growth processes.",
      "pdf_url": "http://arxiv.org/pdf/2506.20486v1",
      "published": "2025-06-25T14:33:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20486v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Counterfactual Influence as a Distributional Quantity",
      "authors": [
        "Matthieu Meeus",
        "Igor Shilov",
        "Georgios Kaissis",
        "Yves-Alexandre de Montjoye"
      ],
      "abstract": "Machine learning models are known to memorize samples from their training\ndata, raising concerns around privacy and generalization. Counterfactual\nself-influence is a popular metric to study memorization, quantifying how the\nmodel's prediction for a sample changes depending on the sample's inclusion in\nthe training dataset. However, recent work has shown memorization to be\naffected by factors beyond self-influence, with other training samples, in\nparticular (near-)duplicates, having a large impact. We here study memorization\ntreating counterfactual influence as a distributional quantity, taking into\naccount how all training samples influence how a sample is memorized. For a\nsmall language model, we compute the full influence distribution of training\nsamples on each other and analyze its properties. We find that solely looking\nat self-influence can severely underestimate tangible risks associated with\nmemorization: the presence of (near-)duplicates seriously reduces\nself-influence, while we find these samples to be (near-)extractable. We\nobserve similar patterns for image classification, where simply looking at the\ninfluence distributions reveals the presence of near-duplicates in CIFAR-10.\nOur findings highlight that memorization stems from complex interactions across\ntraining data and is better captured by the full influence distribution than by\nself-influence alone.",
      "pdf_url": "http://arxiv.org/pdf/2506.20481v1",
      "published": "2025-06-25T14:25:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20481v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ]
    },
    {
      "title": "Automatic Demonstration Selection for LLM-based Tabular Data Classification",
      "authors": [
        "Shuchu Han",
        "Wolfgang Bruckner"
      ],
      "abstract": "A fundamental question in applying In-Context Learning (ICL) for tabular data\nclassification is how to determine the ideal number of demonstrations in the\nprompt. This work addresses this challenge by presenting an algorithm to\nautomatically select a reasonable number of required demonstrations. Our method\ndistinguishes itself by integrating not only the tabular data's distribution\nbut also the user's selected prompt template and the specific Large Language\nModel (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed\nalgorithm defines a novel metric to quantify the similarities between different\ndemonstrations. We then construct a similarity graph and analyze the\neigenvalues of its Laplacian to derive the minimum number of demonstrations\ncapable of representing the data within the LLM's intrinsic representation\nspace. We validate the efficacy of our approach through experiments comparing\nits performance against conventional random selection algorithms on diverse\ndatasets and LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2506.20451v1",
      "published": "2025-06-25T13:57:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20451v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
      "authors": [
        "Weike Zhao",
        "Chaoyi Wu",
        "Yanjie Fan",
        "Xiaoman Zhang",
        "Pengcheng Qiu",
        "Yuze Sun",
        "Xiao Zhou",
        "Yanfeng Wang",
        "Ya Zhang",
        "Yongguo Yu",
        "Kun Sun",
        "Weidi Xie"
      ],
      "abstract": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
      "pdf_url": "http://arxiv.org/pdf/2506.20430v1",
      "published": "2025-06-25T13:42:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20430v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.MA"
      ]
    },
    {
      "title": "Off-Policy Evaluation and Learning for the Future under Non-Stationarity",
      "authors": [
        "Tatsuhiro Shimizu",
        "Kazuki Kawamura",
        "Takanori Muroi",
        "Yusuke Narita",
        "Kei Tateno",
        "Takuma Udagawa",
        "Yuta Saito"
      ],
      "abstract": "We study the novel problem of future off-policy evaluation (F-OPE) and\nlearning (F-OPL) for estimating and optimizing the future value of policies in\nnon-stationary environments, where distributions vary over time. In e-commerce\nrecommendations, for instance, our goal is often to estimate and optimize the\npolicy value for the upcoming month using data collected by an old policy in\nthe previous month. A critical challenge is that data related to the future\nenvironment is not observed in the historical data. Existing methods assume\nstationarity or depend on restrictive reward-modeling assumptions, leading to\nsignificant bias. To address these limitations, we propose a novel estimator\nnamed \\textit{\\textbf{O}ff-\\textbf{P}olicy Estimator for the \\textbf{F}uture\n\\textbf{V}alue (\\textbf{\\textit{OPFV}})}, designed for accurately estimating\npolicy values at any future time point. The key feature of OPFV is its ability\nto leverage the useful structure within time-series data. While future data\nmight not be present in the historical log, we can leverage, for example,\nseasonal, weekly, or holiday effects that are consistent in both the historical\nand future data. Our estimator is the first to exploit these time-related\nstructures via a new type of importance weighting, enabling effective F-OPE.\nTheoretical analysis identifies the conditions under which OPFV becomes\nlow-bias. In addition, we extend our estimator to develop a new policy-gradient\nmethod to proactively learn a good future policy using only historical data.\nEmpirical results show that our methods substantially outperform existing\nmethods in estimating and optimizing the future policy value under\nnon-stationarity for various experimental setups.",
      "pdf_url": "http://arxiv.org/pdf/2506.20417v1",
      "published": "2025-06-25T13:31:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20417v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models",
      "authors": [
        "Dipayan Saha",
        "Shams Tarek",
        "Hasan Al Shaikh",
        "Khan Thamid Hasan",
        "Pavan Sai Nalluri",
        "Md. Ajoad Hasan",
        "Nashmin Alam",
        "Jingbo Zhou",
        "Sujan Kumar Saha",
        "Mark Tehranipoor",
        "Farimah Farahmandi"
      ],
      "abstract": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy.",
      "pdf_url": "http://arxiv.org/pdf/2506.20415v1",
      "published": "2025-06-25T13:31:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20415v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning",
      "authors": [
        "Mohammad Mahdi Maheri",
        "Denys Herasymuk",
        "Hamed Haddadi"
      ],
      "abstract": "The growing adoption of Artificial Intelligence (AI) in Internet of Things\n(IoT) ecosystems has intensified the need for personalized learning methods\nthat can operate efficiently and privately across heterogeneous,\nresource-constrained devices. However, enabling effective personalized learning\nin decentralized settings introduces several challenges, including efficient\nknowledge transfer between clients, protection of data privacy, and resilience\nagainst poisoning attacks. In this paper, we address these challenges by\ndeveloping P4 (Personalized, Private, Peer-to-Peer) -- a method designed to\ndeliver personalized models for resource-constrained IoT devices while ensuring\ndifferential privacy and robustness against poisoning attacks. Our solution\nemploys a lightweight, fully decentralized algorithm to privately detect client\nsimilarity and form collaborative groups. Within each group, clients leverage\ndifferentially private knowledge distillation to co-train their models,\nmaintaining high accuracy while ensuring robustness to the presence of\nmalicious clients. We evaluate P4 on popular benchmark datasets using both\nlinear and CNN-based architectures across various heterogeneity settings and\nattack scenarios. Experimental results show that P4 achieves 5% to 30% higher\naccuracy than leading differentially private peer-to-peer approaches and\nmaintains robustness with up to 30% malicious clients. Additionally, we\ndemonstrate its practicality by deploying it on resource-constrained devices,\nwhere collaborative training between two clients adds only ~7 seconds of\noverhead.",
      "pdf_url": "http://arxiv.org/pdf/2506.20413v1",
      "published": "2025-06-25T13:27:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20413v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "GymPN: A Library for Decision-Making in Process Management Systems",
      "authors": [
        "Riccardo Lo Bianco",
        "Willem van Jaarsveld",
        "Remco Dijkman"
      ],
      "abstract": "Process management systems support key decisions about the way work is\nallocated in organizations. This includes decisions on which task to perform\nnext, when to execute the task, and who to assign the task to. Suitable\nsoftware tools are required to support these decisions in a way that is optimal\nfor the organization. This paper presents a software library, called GymPN,\nthat supports optimal decision-making in business processes using Deep\nReinforcement Learning. GymPN builds on previous work that supports task\nassignment in business processes, introducing two key novelties: support for\npartial process observability and the ability to model multiple decisions in a\nbusiness process. These novel elements address fundamental limitations of\nprevious work and thus enable the representation of more realistic process\ndecisions. We evaluate the library on eight typical business process\ndecision-making problem patterns, showing that GymPN allows for easy modeling\nof the desired problems, as well as learning optimal decision policies.",
      "pdf_url": "http://arxiv.org/pdf/2506.20404v1",
      "published": "2025-06-25T13:19:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20404v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation",
      "authors": [
        "Jinchun Du",
        "Bojie Shen",
        "Muhammad Aamir Cheema",
        "Adel N. Toosi"
      ],
      "abstract": "With the rising popularity of electric vehicles (EVs), modern service\nsystems, such as ride-hailing delivery services, are increasingly integrating\nEVs into their operations. Unlike conventional vehicles, EVs often have a\nshorter driving range, necessitating careful consideration of charging when\nfulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -\nallowing EVs to also discharge energy back to the grid - new opportunities and\ncomplexities emerge. We introduce the Electric Vehicle Orienteering Problem\nwith V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select\ncustomer requests or orders while managing when and where to charge or\ndischarge. This involves navigating dynamic electricity prices, charging\nstation selection, and route constraints. We formulate the problem as a Mixed\nInteger Programming (MIP) model and propose two near-optimal metaheuristic\nalgorithms: one evolutionary (EA) and the other based on large neighborhood\nsearch (LNS). Experiments on real-world data show our methods can double driver\nprofits compared to baselines, while maintaining near-optimal performance on\nsmall instances and excellent scalability on larger ones. Our work highlights a\npromising path toward smarter, more profitable EV-based mobility systems that\nactively support the energy grid.",
      "pdf_url": "http://arxiv.org/pdf/2506.20401v2",
      "published": "2025-06-25T13:15:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20401v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios",
      "authors": [
        "Dror Ivry",
        "Oran Nahum"
      ],
      "abstract": "This paper introduces two significant contributions to address the issue of\ngrounding claims in a given context. Grounding means that given a context\n(document) and a claim, there's at least one supportive evidence for the claim\nin the document. We will introduce Paladin-mini, a compact (3.8B parameters)\nopen-source classifier model (used for labeling data as grounded or ungrounded)\nengineered for robust performance in real-world scenarios, and the\ngrounding-benchmark, a new evaluation dataset designed to assess performance on\ncritical reasoning tasks. We'll also demonstrate the results of Paladin-mini\nwith benchmarks against the current State-of-the-art and share clear and\nreproducible results.",
      "pdf_url": "http://arxiv.org/pdf/2506.20384v1",
      "published": "2025-06-25T12:50:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20384v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition",
      "authors": [
        "Joerg Deigmoeller",
        "Stephan Hasler",
        "Nakul Agarwal",
        "Daniel Tanneberg",
        "Anna Belardinelli",
        "Reza Ghoddoosian",
        "Chao Wang",
        "Felix Ocker",
        "Fan Zhang",
        "Behzad Dariush",
        "Michael Gienger"
      ],
      "abstract": "We introduce CARMA, a system for situational grounding in human-robot group\ninteractions. Effective collaboration in such group settings requires\nsituational awareness based on a consistent representation of present persons\nand objects coupled with an episodic abstraction of events regarding actors and\nmanipulated objects. This calls for a clear and consistent assignment of\ninstances, ensuring that robots correctly recognize and track actors, objects,\nand their interactions over time. To achieve this, CARMA uniquely identifies\nphysical instances of such entities in the real world and organizes them into\ngrounded triplets of actors, objects, and actions.\n  To validate our approach, we conducted three experiments, where multiple\nhumans and a robot interact: collaborative pouring, handovers, and sorting.\nThese scenarios allow the assessment of the system's capabilities as to role\ndistinction, multi-actor awareness, and consistent instance identification. Our\nexperiments demonstrate that the system can reliably generate accurate\nactor-action-object triplets, providing a structured and robust foundation for\napplications requiring spatiotemporal reasoning and situated decision-making in\ncollaborative settings.",
      "pdf_url": "http://arxiv.org/pdf/2506.20373v1",
      "published": "2025-06-25T12:36:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20373v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations",
      "authors": [
        "Lorenzo Bini",
        "Stephane Marchand-Maillet"
      ],
      "abstract": "We present LaplaceGNN, a novel self-supervised graph learning framework that\nbypasses the need for negative sampling by leveraging spectral bootstrapping\ntechniques. Our method integrates Laplacian-based signals into the learning\nprocess, allowing the model to effectively capture rich structural\nrepresentations without relying on contrastive objectives or handcrafted\naugmentations. By focusing on positive alignment, LaplaceGNN achieves linear\nscaling while offering a simpler, more efficient, self-supervised alternative\nfor graph neural networks, applicable across diverse domains. Our contributions\nare twofold: we precompute spectral augmentations through max-min\ncentrality-guided optimization, enabling rich structural supervision without\nrelying on handcrafted augmentations, then we integrate an adversarial\nbootstrapped training scheme that further strengthens feature learning and\nrobustness. Our extensive experiments on different benchmark datasets show that\nLaplaceGNN achieves superior performance compared to state-of-the-art\nself-supervised graph methods, offering a promising direction for efficiently\nlearning expressive graph representations.",
      "pdf_url": "http://arxiv.org/pdf/2506.20362v1",
      "published": "2025-06-25T12:23:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20362v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS"
      ]
    },
    {
      "title": "Tabular Feature Discovery With Reasoning Type Exploration",
      "authors": [
        "Sungwon Han",
        "Sungkyu Park",
        "Seungeon Lee"
      ],
      "abstract": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data.",
      "pdf_url": "http://arxiv.org/pdf/2506.20357v1",
      "published": "2025-06-25T12:18:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20357v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A foundation model with multi-variate parallel attention to generate neuronal activity",
      "authors": [
        "Francesco Carzaniga",
        "Michael Hersche",
        "Abu Sebastian",
        "Kaspar Schindler",
        "Abbas Rahimi"
      ],
      "abstract": "Learning from multi-variate time-series with heterogeneous channel\nconfigurations remains a fundamental challenge for deep neural networks (DNNs),\nparticularly in clinical domains such as intracranial electroencephalography\n(iEEG), where channel setups vary widely across subjects. In this work, we\nintroduce multi-variate parallel attention (MVPA), a novel self-attention\nmechanism that disentangles content, temporal, and spatial attention, enabling\nflexible, generalizable, and efficient modeling of time-series data with\nvarying channel counts and configurations. We use MVPA to build MVPFormer, a\ngenerative foundation model for human electrophysiology, trained to predict the\nevolution of iEEG signals across diverse subjects. To support this and future\neffort by the community, we release the SWEC iEEG dataset, the largest publicly\navailable iEEG dataset to date, comprising nearly 10,000 hours of recordings\nfrom heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong\ngeneralization across subjects, demonstrating expert-level performance in\nseizure detection and outperforming state-of-the-art Transformer baselines on\nour SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard\ntime-series forecasting and classification tasks, where it matches or exceeds\nexisting attention-based models. Together, our contributions establish MVPA as\na general-purpose attention mechanism for heterogeneous time-series and\nMVPFormer as the first open-source, open-weights, and open-data iEEG foundation\nmodel with state-of-the-art clinical performance. The code is available at\nhttps://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG\ndataset is available at\nhttps://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.",
      "pdf_url": "http://arxiv.org/pdf/2506.20354v1",
      "published": "2025-06-25T12:07:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20354v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "DipSVD: Dual-importance Protected SVD for Efficient LLM Compression",
      "authors": [
        "Xuan Ding",
        "Rui Sun",
        "Yunjian Zhang",
        "Xiu Yan",
        "Yueqi Zhou",
        "Kaihao Huang",
        "Suzhong Fu",
        "Chuanlong Xie",
        "Yao Zhu"
      ],
      "abstract": "The ever-increasing computational demands and deployment costs of large\nlanguage models (LLMs) have spurred numerous compressing methods. Compared to\nquantization and unstructured pruning, SVD compression offers superior hardware\ncompatibility and theoretical guarantees. However, existing SVD-based methods\nfocus on the overall discrepancy between the original and compressed matrices\nwhile overlooking the protection of critical components within the matrix,\nwhich leads to inferior performance in the compressed models. This paper\nproposes a dual-level importance protection mechanism to enhance SVD-based\ncompression methods: (1) local importance protection: preserving the most\ncritical singular vectors within each weight matrix through channel-weighted\ndata whitening; and (2) global importance protection: enabling less important\nlayers to bear a greater portion of the compression burden through either a\nheuristic or optimization-based approach, thereby minimizing the impact of\ncompression on critical layers. Extensive experiments demonstrate that DipSVD\noutperforms existing SVD-based compression approaches across multiple\nbenchmarks, achieving superior model performance especially at high model\ncompression ratios.",
      "pdf_url": "http://arxiv.org/pdf/2506.20353v1",
      "published": "2025-06-25T12:04:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20353v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Feature Hallucination for Self-supervised Action Recognition",
      "authors": [
        "Lei Wang",
        "Piotr Koniusz"
      ],
      "abstract": "Understanding human actions in videos requires more than raw pixel analysis;\nit relies on high-level semantic reasoning and effective integration of\nmultimodal features. We propose a deep translational action recognition\nframework that enhances recognition accuracy by jointly predicting action\nconcepts and auxiliary features from RGB video frames. At test time,\nhallucination streams infer missing cues, enriching feature representations\nwithout increasing computational overhead. To focus on action-relevant regions\nbeyond raw pixels, we introduce two novel domain-specific descriptors. Object\nDetection Features (ODF) aggregate outputs from multiple object detectors to\ncapture contextual cues, while Saliency Detection Features (SDF) highlight\nspatial and intensity patterns crucial for action recognition. Our framework\nseamlessly integrates these descriptors with auxiliary modalities such as\noptical flow, Improved Dense Trajectories, skeleton data, and audio cues. It\nremains compatible with state-of-the-art architectures, including I3D,\nAssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE\nV2 and InternVideo2. To handle uncertainty in auxiliary features, we\nincorporate aleatoric uncertainty modeling in the hallucination step and\nintroduce a robust loss function to mitigate feature noise. Our multimodal\nself-supervised action recognition framework achieves state-of-the-art\nperformance on multiple benchmarks, including Kinetics-400, Kinetics-600, and\nSomething-Something V2, demonstrating its effectiveness in capturing\nfine-grained action dynamics.",
      "pdf_url": "http://arxiv.org/pdf/2506.20342v1",
      "published": "2025-06-25T11:50:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20342v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards",
      "authors": [
        "Jihao Gu",
        "Qihang Ai",
        "Yingyao Wang",
        "Pi Bu",
        "Jingxuan Xing",
        "Zekun Zhu",
        "Wei Jiang",
        "Ziming Wang",
        "Yingxiu Zhao",
        "Ming-Liang Zhang",
        "Jun Song",
        "Yuning Jiang",
        "Bo Zheng"
      ],
      "abstract": "Vision-language model-based mobile agents have gained the ability to not only\nunderstand complex instructions and mobile screenshots, but also optimize their\naction outputs via thinking and reasoning, benefiting from reinforcement\nlearning, such as Group Relative Policy Optimization (GRPO). However, existing\nresearch centers on offline reinforcement learning training or online\noptimization using action-level rewards, which limits the agent's dynamic\ninteraction with the environment. This often results in agents settling into\nlocal optima, thereby weakening their ability for exploration and error action\ncorrection. To address these challenges, we introduce an approach called\nMobile-R1, which employs interactive multi-turn reinforcement learning with\ntask-level rewards for mobile agents. Our training framework consists of three\nstages: initial format finetuning, single-step online training via action-level\nreward, followed by online training via task-level reward based on multi-turn\ntrajectories. This strategy is designed to enhance the exploration and error\ncorrection capabilities of Mobile-R1, leading to significant performance\nimprovements. Moreover, we have collected a dataset covering 28 Chinese\napplications with 24,521 high-quality manual annotations and established a new\nbenchmark with 500 trajectories. We will open source all resources, including\nthe dataset, benchmark, model weight, and codes:\nhttps://mobile-r1.github.io/Mobile-R1/.",
      "pdf_url": "http://arxiv.org/pdf/2506.20332v1",
      "published": "2025-06-25T11:34:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20332v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach",
      "authors": [
        "Saundarya Subramaniam",
        "Shalini Majumdar",
        "Shantanu Nadar",
        "Kaustubh Kulkarni"
      ],
      "abstract": "This research presents the development of an Artificial Intelligence (AI) -\ndriven crop disease detection system designed to assist farmers in rural areas\nwith limited resources. We aim to compare different deep learning models for a\ncomparative analysis, focusing on their efficacy in transfer learning. By\nleveraging deep learning models, including EfficientNet, ResNet101,\nMobileNetV2, and our custom CNN, which achieved a validation accuracy of\n95.76%, the system effectively classifies plant diseases. This research\ndemonstrates the potential of transfer learning in reshaping agricultural\npractices, improving crop health management, and supporting sustainable farming\nin rural environments.",
      "pdf_url": "http://arxiv.org/pdf/2506.20323v1",
      "published": "2025-06-25T11:04:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20323v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration",
      "authors": [
        "Heyang Zhao",
        "Xingrui Yu",
        "David M. Bossens",
        "Ivor W. Tsang",
        "Quanquan Gu"
      ],
      "abstract": "Imitation learning is a central problem in reinforcement learning where the\ngoal is to learn a policy that mimics the expert's behavior. In practice, it is\noften challenging to learn the expert policy from a limited number of\ndemonstrations accurately due to the complexity of the state space. Moreover,\nit is essential to explore the environment and collect data to achieve\nbeyond-expert performance. To overcome these challenges, we propose a novel\nimitation learning algorithm called Imitation Learning with Double Exploration\n(ILDE), which implements exploration in two aspects: (1) optimistic policy\noptimization via an exploration bonus that rewards state-action pairs with high\nuncertainty to potentially improve the convergence to the expert policy, and\n(2) curiosity-driven exploration of the states that deviate from the\ndemonstration trajectories to potentially yield beyond-expert performance.\nEmpirically, we demonstrate that ILDE outperforms the state-of-the-art\nimitation learning algorithms in terms of sample efficiency and achieves\nbeyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations\nthan in previous work. We also provide a theoretical justification of ILDE as\nan uncertainty-regularized policy optimization method with optimistic\nexploration, leading to a regret growing sublinearly in the number of episodes.",
      "pdf_url": "http://arxiv.org/pdf/2506.20307v1",
      "published": "2025-06-25T10:39:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20307v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Enterprise Large Language Model Evaluation Benchmark",
      "authors": [
        "Liya Wang",
        "David Yi",
        "Damien Jose",
        "John Passarelli",
        "James Gao",
        "Jordan Leventis",
        "Kang Li"
      ],
      "abstract": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment.",
      "pdf_url": "http://arxiv.org/pdf/2506.20274v1",
      "published": "2025-06-25T09:34:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20274v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Argumentative Ensembling for Robust Recourse under Model Multiplicity",
      "authors": [
        "Junqi Jiang",
        "Antonio Rago",
        "Francesco Leofante",
        "Francesca Toni"
      ],
      "abstract": "In machine learning, it is common to obtain multiple equally performing\nmodels for the same prediction task, e.g., when training neural networks with\ndifferent random seeds. Model multiplicity (MM) is the situation which arises\nwhen these competing models differ in their predictions for the same input, for\nwhich ensembling is often employed to determine an aggregation of the outputs.\nProviding recourse recommendations via counterfactual explanations (CEs) under\nMM thus becomes complex, since the CE may not be valid across all models, i.e.,\nthe CEs are not robust under MM. In this work, we formalise the problem of\nproviding recourse under MM, which we name recourse-aware ensembling (RAE). We\npropose the idea that under MM, CEs for each individual model should be\nconsidered alongside their predictions so that the aggregated prediction and\nrecourse are decided in tandem. Centred around this intuition, we introduce six\ndesirable properties for solutions to this problem. For solving RAE, we propose\na novel argumentative ensembling method which guarantees the robustness of CEs\nunder MM. Specifically, our method leverages computational argumentation to\nexplicitly represent the conflicts between models and counterfactuals regarding\nprediction results and CE validity. It then uses argumentation semantics to\nresolve the conflicts and obtain the final solution, in a manner which is\nparametric to the chosen semantics. Our method also allows for the\nspecification of preferences over the models under MM, allowing further\ncustomisation of the ensemble. In a comprehensive theoretical analysis, we\ncharacterise the behaviour of argumentative ensembling with four different\nargumentation semantics. We then empirically demonstrate the effectiveness of\nour approach in satisfying desirable properties with eight instantiations of\nour method. (Abstract is shortened for arXiv.)",
      "pdf_url": "http://arxiv.org/pdf/2506.20260v1",
      "published": "2025-06-25T09:07:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20260v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Generating and Customizing Robotic Arm Trajectories using Neural Networks",
      "authors": [
        "Andrej Lúčny",
        "Matilde Antonj",
        "Carlo Mazzola",
        "Hana Hornáčková",
        "Igor Farkaš"
      ],
      "abstract": "We introduce a neural network approach for generating and customizing the\ntrajectory of a robotic arm, that guarantees precision and repeatability. To\nhighlight the potential of this novel method, we describe the design and\nimplementation of the technique and show its application in an experimental\nsetting of cognitive robotics. In this scenario, the NICO robot was\ncharacterized by the ability to point to specific points in space with precise\nlinear movements, increasing the predictability of the robotic action during\nits interaction with humans. To achieve this goal, the neural network computes\nthe forward kinematics of the robot arm. By integrating it with a generator of\njoint angles, another neural network was developed and trained on an artificial\ndataset created from suitable start and end poses of the robotic arm. Through\nthe computation of angular velocities, the robot was characterized by its\nability to perform the movement, and the quality of its action was evaluated in\nterms of shape and accuracy. Thanks to its broad applicability, our approach\nsuccessfully generates precise trajectories that could be customized in their\nshape and adapted to different settings.",
      "pdf_url": "http://arxiv.org/pdf/2506.20259v1",
      "published": "2025-06-25T09:05:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20259v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "68T40, 93C85, 70E60",
        "I.2.9"
      ]
    },
    {
      "title": "Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios",
      "authors": [
        "Ben Gerhards",
        "Nikita Popkov",
        "Annekatrin König",
        "Marcel Arpogaus",
        "Bastian Schäfermeier",
        "Leonie Riedl",
        "Stephan Vogt",
        "Philip Hehlert"
      ],
      "abstract": "Forecasting attracts a lot of research attention in the electricity value\nchain. However, most studies concentrate on short-term forecasting of\ngeneration or consumption with a focus on systems and less on individual\nconsumers. Even more neglected is the topic of long-term forecasting of\nindividual power consumption.\n  Here, we provide an in-depth comparative evaluation of data-driven methods\nfor generating synthetic time series data tailored to energy consumption\nlong-term forecasting. High-fidelity synthetic data is crucial for a wide range\nof applications, including state estimations in energy systems or power grid\nplanning. In this study, we assess and compare the performance of multiple\nstate-of-the-art but less common techniques: a hybrid Wasserstein Generative\nAdversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),\nHidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial\nnormalizing Flows (MABF). We analyze the ability of each method to replicate\nthe temporal dynamics, long-range dependencies, and probabilistic transitions\ncharacteristic of individual energy consumption profiles. Our comparative\nevaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and\nMABF aiding in selecting the most suitable approach for state estimations and\nother energy-related tasks. Our generation and analysis framework aims to\nenhance the accuracy and reliability of synthetic power consumption data while\ngenerating data that fulfills criteria like anonymisation - preserving privacy\nconcerns mitigating risks of specific profiling of single customers. This study\nutilizes an open-source dataset from households in Germany with 15min time\nresolution. The generated synthetic power profiles can readily be used in\napplications like state estimations or consumption forecasting.",
      "pdf_url": "http://arxiv.org/pdf/2506.20253v1",
      "published": "2025-06-25T08:54:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20253v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models",
      "authors": [
        "Kejia Chen",
        "Jiawen Zhang",
        "Jiacong Hu",
        "Yu Wang",
        "Jian Lou",
        "Zunlei Feng",
        "Mingli Song"
      ],
      "abstract": "Quantized large language models (LLMs) have gained increasing attention and\nsignificance for enabling deployment in resource-constrained environments.\nHowever, emerging studies on a few calibration dataset-free quantization\nmethods suggest that quantization may compromise the safety capabilities of\nLLMs, underscoring the urgent need for systematic safety evaluations and\neffective mitigation strategies. In this paper, we present comprehensive safety\nevaluations across various mainstream quantization techniques and diverse\ncalibration datasets, utilizing widely accepted safety benchmarks. To address\nthe identified safety vulnerabilities, we propose a quantization-aware safety\npatching framework, Q-resafe, to efficiently restore the safety capabilities of\nquantized LLMs while minimizing any adverse impact on utility. Extensive\nexperimental results demonstrate that Q-resafe successfully re-aligns the\nsafety of quantized LLMs with their pre-quantization counterparts, even under\nchallenging evaluation scenarios. Project page is available at:\nhttps://github.com/Thecommonirin/Qresafe.",
      "pdf_url": "http://arxiv.org/pdf/2506.20251v1",
      "published": "2025-06-25T08:52:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20251v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Language Modeling by Language Models",
      "authors": [
        "Junyan Cheng",
        "Peter Clark",
        "Kyle Richardson"
      ],
      "abstract": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.20249v1",
      "published": "2025-06-25T08:46:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.20249v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ]
    }
  ]
}