{
  "last_updated": "2025-03-13T00:46:32.918875",
  "papers": [
    {
      "title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents",
      "authors": [
        "Haoyu Wang",
        "Sunhao Dai",
        "Haiyuan Zhao",
        "Liang Pang",
        "Xiao Zhang",
        "Gang Wang",
        "Zhenhua Dong",
        "Jun Xu",
        "Ji-Rong Wen"
      ],
      "abstract": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap.",
      "pdf_url": "http://arxiv.org/pdf/2503.08684v1",
      "published": "2025-03-11T17:59:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08684v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving",
      "authors": [
        "Changxing Liu",
        "Genjia Liu",
        "Zijun Wang",
        "Jinchang Yang",
        "Siheng Chen"
      ],
      "abstract": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver.",
      "pdf_url": "http://arxiv.org/pdf/2503.08683v1",
      "published": "2025-03-11T17:58:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08683v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
      "authors": [
        "Iván Arcuschin",
        "Jett Janiak",
        "Robert Krzyzanowski",
        "Senthooran Rajamanoharan",
        "Neel Nanda",
        "Arthur Conmy"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal concerning rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (30.6%), DeepSeek R1 (15.8%) and\nChatGPT-4o (12.6%) all answer a high proportion of question pairs unfaithfully.\nSpecifically, we find that models rationalize their implicit biases in answers\nto binary questions (\"implicit post-hoc rationalization\"). For example, when\nseparately presented with the questions \"Is X bigger than Y?\" and \"Is Y bigger\nthan X?\", models sometimes produce superficially coherent arguments to justify\nanswering Yes to both questions or No to both questions, despite such responses\nbeing logically contradictory. We also investigate restoration errors (Dziri et\nal., 2023), where models make and then silently correct errors in their\nreasoning, and unfaithful shortcuts, where models use clearly illogical\nreasoning to simplify solving problems in Putnam questions (a hard benchmark).\nOur findings raise challenges for AI safety work that relies on monitoring CoT\nto detect undesired behavior.",
      "pdf_url": "http://arxiv.org/pdf/2503.08679v1",
      "published": "2025-03-11T17:56:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08679v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing",
      "authors": [
        "Yuanhao Wang",
        "Cheng Zhang",
        "Gonçalo Frazão",
        "Jinlong Yang",
        "Alexandru-Eugen Ichim",
        "Thabo Beeler",
        "Fernando De la Torre"
      ],
      "abstract": "We introduce GarmentCrafter, a new approach that enables non-professional\nusers to create and modify 3D garments from a single-view image. While recent\nadvances in image generation have facilitated 2D garment design, creating and\nediting 3D garments remains challenging for non-professional users. Existing\nmethods for single-view 3D reconstruction often rely on pre-trained generative\nmodels to synthesize novel views conditioning on the reference image and camera\npose, yet they lack cross-view consistency, failing to capture the internal\nrelationships across different views. In this paper, we tackle this challenge\nthrough progressive depth prediction and image warping to approximate novel\nviews. Subsequently, we train a multi-view diffusion model to complete occluded\nand unknown clothing regions, informed by the evolving camera pose. By jointly\ninferring RGB and depth, GarmentCrafter enforces inter-view coherence and\nreconstructs precise geometries and fine details. Extensive experiments\ndemonstrate that our method achieves superior visual fidelity and inter-view\ncoherence compared to state-of-the-art single-view 3D garment reconstruction\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2503.08678v1",
      "published": "2025-03-11T17:56:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08678v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence",
      "authors": [
        "Zekun Li",
        "Shinda Huang",
        "Jiangtian Wang",
        "Nathan Zhang",
        "Antonis Antoniades",
        "Wenyue Hua",
        "Kaijie Zhu",
        "Sirui Zeng",
        "William Yang Wang",
        "Xifeng Yan"
      ],
      "abstract": "As language agents progressively automate critical tasks across domains,\ntheir ability to operate within operational constraints and safety protocols\nbecomes essential. While extensive research has demonstrated these agents'\neffectiveness in downstream task completion, their reliability in following\noperational procedures and constraints remains largely unexplored. To this end,\nwe present AgentOrca, a dual-system framework for evaluating language agents'\ncompliance with operational constraints and routines. Our framework encodes\naction constraints and routines through both natural language prompts for\nagents and corresponding executable code serving as ground truth for automated\nverification. Through an automated pipeline of test case generation and\nevaluation across five real-world domains, we quantitatively assess current\nlanguage agents' adherence to operational constraints. Our findings reveal\nnotable performance gaps among state-of-the-art models, with large reasoning\nmodels like o1 demonstrating superior compliance while others show\nsignificantly lower performance, particularly when encountering complex\nconstraints or user persuasion attempts.",
      "pdf_url": "http://arxiv.org/pdf/2503.08669v1",
      "published": "2025-03-11T17:53:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08669v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder",
      "authors": [
        "Yitian Zhang",
        "Long Mai",
        "Aniruddha Mahapatra",
        "David Bourgin",
        "Yicong Hong",
        "Jonah Casebeer",
        "Feng Liu",
        "Yun Fu"
      ],
      "abstract": "We present a novel perspective on learning video embedders for generative\nmodeling: rather than requiring an exact reproduction of an input video, an\neffective embedder should focus on synthesizing visually plausible\nreconstructions. This relaxed criterion enables substantial improvements in\ncompression ratios without compromising the quality of downstream generative\nmodels. Specifically, we propose replacing the conventional encoder-decoder\nvideo embedder with an encoder-generator framework that employs a diffusion\ntransformer (DiT) to synthesize missing details from a compact latent space.\nTherein, we develop a dedicated latent conditioning module to condition the DiT\ndecoder on the encoded video latent embedding. Our experiments demonstrate that\nour approach enables superior encoding-decoding performance compared to\nstate-of-the-art methods, particularly as the compression ratio increases. To\ndemonstrate the efficacy of our approach, we report results from our video\nembedders achieving a temporal compression ratio of up to 32x (8x higher than\nleading video embedders) and validate the robustness of this ultra-compact\nlatent space for text-to-video generation, providing a significant efficiency\nboost in latent diffusion model training and inference.",
      "pdf_url": "http://arxiv.org/pdf/2503.08665v1",
      "published": "2025-03-11T17:51:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08665v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention",
      "authors": [
        "Yuhan Wang",
        "Fangzhou Hong",
        "Shuai Yang",
        "Liming Jiang",
        "Wayne Wu",
        "Chen Change Loy"
      ],
      "abstract": "Multiview diffusion models have shown considerable success in image-to-3D\ngeneration for general objects. However, when applied to human data, existing\nmethods have yet to deliver promising results, largely due to the challenges of\nscaling multiview attention to higher resolutions. In this paper, we explore\nhuman multiview diffusion models at the megapixel level and introduce a\nsolution called mesh attention to enable training at 1024x1024 resolution.\nUsing a clothed human mesh as a central coarse geometric representation, the\nproposed mesh attention leverages rasterization and projection to establish\ndirect cross-view coordinate correspondences. This approach significantly\nreduces the complexity of multiview attention while maintaining cross-view\nconsistency. Building on this foundation, we devise a mesh attention block and\ncombine it with keypoint conditioning to create our human-specific multiview\ndiffusion model, MEAT. In addition, we present valuable insights into applying\nmultiview human motion videos for diffusion training, addressing the\nlongstanding issue of data scarcity. Extensive experiments show that MEAT\neffectively generates dense, consistent multiview human images at the megapixel\nlevel, outperforming existing multiview diffusion methods.",
      "pdf_url": "http://arxiv.org/pdf/2503.08664v1",
      "published": "2025-03-11T17:50:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08664v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety",
      "authors": [
        "Pierre Sermanet",
        "Anirudha Majumdar",
        "Alex Irpan",
        "Dmitry Kalashnikov",
        "Vikas Sindhwani"
      ],
      "abstract": "Until recently, robotics safety research was predominantly about collision\navoidance and hazard reduction in the immediate vicinity of a robot. Since the\nadvent of large vision and language models (VLMs), robots are now also capable\nof higher-level semantic scene understanding and natural language interactions\nwith humans. Despite their known vulnerabilities (e.g. hallucinations or\njail-breaking), VLMs are being handed control of robots capable of physical\ncontact with the real world. This can lead to dangerous behaviors, making\nsemantic safety for robots a matter of immediate concern. Our contributions in\nthis paper are two fold: first, to address these emerging risks, we release the\nASIMOV Benchmark, a large-scale and comprehensive collection of datasets for\nevaluating and improving semantic safety of foundation models serving as robot\nbrains. Our data generation recipe is highly scalable: by leveraging text and\nimage generation techniques, we generate undesirable situations from real-world\nvisual scenes and human injury reports from hospitals. Secondly, we develop a\nframework to automatically generate robot constitutions from real-world data to\nsteer a robot's behavior using Constitutional AI mechanisms. We propose a novel\nauto-amending process that is able to introduce nuances in written rules of\nbehavior; this can lead to increased alignment with human preferences on\nbehavior desirability and safety. We explore trade-offs between generality and\nspecificity across a diverse set of constitutions of different lengths, and\ndemonstrate that a robot is able to effectively reject unconstitutional\nactions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark using\ngenerated constitutions, outperforming no-constitution baselines and\nhuman-written constitutions. Data is available at asimov-benchmark.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.08663v1",
      "published": "2025-03-11T17:50:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08663v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.CY",
        "cs.HC"
      ]
    },
    {
      "title": "Exploring the Word Sense Disambiguation Capabilities of Large Language Models",
      "authors": [
        "Pierpaolo Basile",
        "Lucia Siciliani",
        "Elio Musacchio",
        "Giovanni Semeraro"
      ],
      "abstract": "Word Sense Disambiguation (WSD) is a historical task in computational\nlinguistics that has received much attention over the years. However, with the\nadvent of Large Language Models (LLMs), interest in this task (in its classical\ndefinition) has decreased. In this study, we evaluate the performance of\nvarious LLMs on the WSD task. We extend a previous benchmark (XL-WSD) to\nre-design two subtasks suitable for LLM: 1) given a word in a sentence, the LLM\nmust generate the correct definition; 2) given a word in a sentence and a set\nof predefined meanings, the LLM must select the correct one. The extended\nbenchmark is built using the XL-WSD and BabelNet. The results indicate that\nLLMs perform well in zero-shot learning but cannot surpass current\nstate-of-the-art methods. However, a fine-tuned model with a medium number of\nparameters outperforms all other models, including the state-of-the-art.",
      "pdf_url": "http://arxiv.org/pdf/2503.08662v1",
      "published": "2025-03-11T17:50:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08662v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Exploiting Instruction-Following Retrievers for Malicious Information Retrieval",
      "authors": [
        "Parishad BehnamGhader",
        "Nicholas Meade",
        "Siva Reddy"
      ],
      "abstract": "Instruction-following retrievers have been widely adopted alongside LLMs in\nreal-world applications, but little work has investigated the safety risks\nsurrounding their increasing search capabilities. We empirically study the\nability of retrievers to satisfy malicious queries, both when used directly and\nwhen used in a retrieval augmented generation-based setup. Concretely, we\ninvestigate six leading retrievers, including NV-Embed and LLM2Vec, and find\nthat given malicious requests, most retrievers can (for >50% of queries) select\nrelevant harmful passages. For example, LLM2Vec correctly selects passages for\n61.35% of our malicious queries. We further uncover an emerging risk with\ninstruction-following retrievers, where highly relevant harmful information can\nbe surfaced by exploiting their instruction-following capabilities. Finally, we\nshow that even safety-aligned LLMs, such as Llama3, can satisfy malicious\nrequests when provided with harmful retrieved passages in-context. In summary,\nour findings underscore the malicious misuse risks associated with increasing\nretriever capability.",
      "pdf_url": "http://arxiv.org/pdf/2503.08644v1",
      "published": "2025-03-11T17:36:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08644v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking Diffusion Model in High Dimension",
      "authors": [
        "Zhenxin Zheng",
        "Zhenjie Zheng"
      ],
      "abstract": "Curse of Dimensionality is an unavoidable challenge in statistical\nprobability models, yet diffusion models seem to overcome this limitation,\nachieving impressive results in high-dimensional data generation. Diffusion\nmodels assume that they can learn the statistical properties of the underlying\nprobability distribution, enabling sampling from this distribution to generate\nrealistic samples. But is this really how they work? To address this question,\nthis paper conducts a detailed analysis of the objective function and inference\nmethods of diffusion models, leading to several important conclusions that help\nanswer the above question: 1) In high-dimensional sparse scenarios, the target\nof the objective function fitting degrades from a weighted sum of multiple\nsamples to a single sample. 2) The mainstream inference methods can all be\nrepresented within a simple unified framework, without requiring statistical\nconcepts such as Markov chains and SDEs. 3) Guided by this simple framework,\nmore efficient inference methods can be discovered.",
      "pdf_url": "http://arxiv.org/pdf/2503.08643v1",
      "published": "2025-03-11T17:36:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08643v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
      "authors": [
        "Ruibin Yuan",
        "Hanfeng Lin",
        "Shuyue Guo",
        "Ge Zhang",
        "Jiahao Pan",
        "Yongyi Zang",
        "Haohe Liu",
        "Yiming Liang",
        "Wenye Ma",
        "Xingjian Du",
        "Xinrun Du",
        "Zhen Ye",
        "Tianyu Zheng",
        "Yinghao Ma",
        "Minghao Liu",
        "Zeyue Tian",
        "Ziya Zhou",
        "Liumeng Xue",
        "Xingwei Qu",
        "Yizhi Li",
        "Shangda Wu",
        "Tianhao Shen",
        "Ziyang Ma",
        "Jun Zhan",
        "Chunhui Wang",
        "Yatian Wang",
        "Xiaowei Chi",
        "Xinyue Zhang",
        "Zhenzhu Yang",
        "Xiangzhou Wang",
        "Shansong Liu",
        "Lingrui Mei",
        "Peng Li",
        "Junjie Wang",
        "Jianwei Yu",
        "Guojian Pang",
        "Xu Li",
        "Zihao Wang",
        "Xiaohuan Zhou",
        "Lijun Yu",
        "Emmanouil Benetos",
        "Yong Chen",
        "Chenghua Lin",
        "Xie Chen",
        "Gus Xia",
        "Zhaoxiang Zhang",
        "Chao Zhang",
        "Wenhu Chen",
        "Xinyu Zhou",
        "Xipeng Qiu",
        "Roger Dannenberg",
        "Jiaheng Liu",
        "Jian Yang",
        "Wenhao Huang",
        "Wei Xue",
        "Xu Tan",
        "Yike Guo"
      ],
      "abstract": "We tackle the task of long-form music generation--particularly the\nchallenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
      "pdf_url": "http://arxiv.org/pdf/2503.08638v1",
      "published": "2025-03-11T17:26:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08638v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD"
      ]
    },
    {
      "title": "Vision Transformer for Intracranial Hemorrhage Classification in CT Scans Using an Entropy-Aware Fuzzy Integral Strategy for Adaptive Scan-Level Decision Fusion",
      "authors": [
        "Mehdi Hosseini Chagahi",
        "Niloufar Delfan",
        "Behzad Moshiri",
        "Md. Jalil Piran",
        "Jaber Hatam Parikhan"
      ],
      "abstract": "Intracranial hemorrhage (ICH) is a critical medical emergency caused by the\nrupture of cerebral blood vessels, leading to internal bleeding within the\nskull. Accurate and timely classification of hemorrhage subtypes is essential\nfor effective clinical decision-making. To address this challenge, we propose\nan advanced pyramid vision transformer (PVT)-based model, leveraging its\nhierarchical attention mechanisms to capture both local and global spatial\ndependencies in brain CT scans. Instead of processing all extracted features\nindiscriminately, A SHAP-based feature selection method is employed to identify\nthe most discriminative components, which are then used as a latent feature\nspace to train a boosting neural network, reducing computational complexity. We\nintroduce an entropy-aware aggregation strategy along with a fuzzy integral\noperator to fuse information across multiple CT slices, ensuring a more\ncomprehensive and reliable scan-level diagnosis by accounting for inter-slice\ndependencies. Experimental results show that our PVT-based framework\nsignificantly outperforms state-of-the-art deep learning architectures in terms\nof classification accuracy, precision, and robustness. By combining SHAP-driven\nfeature selection, transformer-based modeling, and an entropy-aware fuzzy\nintegral operator for decision fusion, our method offers a scalable and\ncomputationally efficient AI-driven solution for automated ICH subtype\nclassification.",
      "pdf_url": "http://arxiv.org/pdf/2503.08609v1",
      "published": "2025-03-11T16:47:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08609v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "A Grid Cell-Inspired Structured Vector Algebra for Cognitive Maps",
      "authors": [
        "Sven Krausse",
        "Emre Neftci",
        "Friedrich T. Sommer",
        "Alpha Renner"
      ],
      "abstract": "The entorhinal-hippocampal formation is the mammalian brain's navigation\nsystem, encoding both physical and abstract spaces via grid cells. This system\nis well-studied in neuroscience, and its efficiency and versatility make it\nattractive for applications in robotics and machine learning. While continuous\nattractor networks (CANs) successfully model entorhinal grid cells for encoding\nphysical space, integrating both continuous spatial and abstract spatial\ncomputations into a unified framework remains challenging. Here, we attempt to\nbridge this gap by proposing a mechanistic model for versatile information\nprocessing in the entorhinal-hippocampal formation inspired by CANs and Vector\nSymbolic Architectures (VSAs), a neuro-symbolic computing framework. The novel\ngrid-cell VSA (GC-VSA) model employs a spatially structured encoding scheme\nwith 3D neuronal modules mimicking the discrete scales and orientations of grid\ncell modules, reproducing their characteristic hexagonal receptive fields. In\nexperiments, the model demonstrates versatility in spatial and abstract tasks:\n(1) accurate path integration for tracking locations, (2) spatio-temporal\nrepresentation for querying object locations and temporal relations, and (3)\nsymbolic reasoning using family trees as a structured test case for\nhierarchical relationships.",
      "pdf_url": "http://arxiv.org/pdf/2503.08608v1",
      "published": "2025-03-11T16:45:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08608v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "q-bio.NC"
      ]
    },
    {
      "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling",
      "authors": [
        "Subin Kim",
        "Seoung Wug Oh",
        "Jui-Hsien Wang",
        "Joon-Young Lee",
        "Jinwoo Shin"
      ],
      "abstract": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
      "pdf_url": "http://arxiv.org/pdf/2503.08605v1",
      "published": "2025-03-11T16:43:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08605v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments",
      "authors": [
        "Dongping Li",
        "Tielong Cai",
        "Tianci Tang",
        "Wenhao Chai",
        "Katherine Rose Driggs-Campbell",
        "Gaoang Wang"
      ],
      "abstract": "Developing autonomous home robots controlled by natural language has long\nbeen a pursuit of human. While advancements in large language models (LLMs) and\nembodied intelligence make this goal closer, several challenges persist: the\nlack of a unified benchmark for more complex robot tasks, limited evaluation\nmethods and metrics, data incompatibility between LLMs and mobile manipulation\ntrajectories. To address these issues, we introduce Embodied Mobile\nManipulation in Open Environments (EMMOE), which requires agents to interpret\nuser instructions and execute long-horizon everyday tasks in continuous space.\nEMMOE seamlessly integrates high-level and low-level embodied tasks into a\nunified framework, along with three new metrics for more diverse assessment.\nAdditionally, we collect EMMOE-100, which features in various task attributes,\ndetailed process annotations, re-plans after failures, and two sub-datasets for\nLLM training. Furthermore, we design HomieBot, a sophisticated agent system\nconsists of LLM with Direct Preference Optimization (DPO), light weighted\nnavigation and manipulation models, and multiple error detection mechanisms.\nFinally, we demonstrate HomieBot's performance and the evaluation of different\nmodels and policies.",
      "pdf_url": "http://arxiv.org/pdf/2503.08604v1",
      "published": "2025-03-11T16:42:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08604v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
      "authors": [
        "Xin Xu",
        "Wei Xu",
        "Ningyu Zhang",
        "Julian McAuley"
      ],
      "abstract": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
      "pdf_url": "http://arxiv.org/pdf/2503.08588v1",
      "published": "2025-03-11T16:25:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08588v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "MsaMIL-Net: An End-to-End Multi-Scale Aware Multiple Instance Learning Network for Efficient Whole Slide Image Classification",
      "authors": [
        "Jiangping Wen",
        "Jinyu Wen",
        "Meie Fang"
      ],
      "abstract": "Bag-based Multiple Instance Learning (MIL) approaches have emerged as the\nmainstream methodology for Whole Slide Image (WSI) classification. However,\nmost existing methods adopt a segmented training strategy, which first extracts\nfeatures using a pre-trained feature extractor and then aggregates these\nfeatures through MIL. This segmented training approach leads to insufficient\ncollaborative optimization between the feature extraction network and the MIL\nnetwork, preventing end-to-end joint optimization and thereby limiting the\noverall performance of the model. Additionally, conventional methods typically\nextract features from all patches of fixed size, ignoring the multi-scale\nobservation characteristics of pathologists. This not only results in\nsignificant computational resource waste when tumor regions represent a minimal\nproportion (as in the Camelyon16 dataset) but may also lead the model to\nsuboptimal solutions.\n  To address these limitations, this paper proposes an end-to-end multi-scale\nWSI classification framework that integrates multi-scale feature extraction\nwith multiple instance learning. Specifically, our approach includes: (1) a\nsemantic feature filtering module to reduce interference from non-lesion areas;\n(2) a multi-scale feature extraction module to capture pathological information\nat different levels; and (3) a multi-scale fusion MIL module for global\nmodeling and feature integration. Through an end-to-end training strategy, we\nsimultaneously optimize both the feature extractor and MIL network, ensuring\nmaximum compatibility between them.\n  Experiments were conducted on three cross-center datasets (DigestPath2019,\nBCNB, and UBC-OCEAN). Results demonstrate that our proposed method outperforms\nexisting state-of-the-art approaches in terms of both accuracy (ACC) and AUC\nmetrics.",
      "pdf_url": "http://arxiv.org/pdf/2503.08581v2",
      "published": "2025-03-11T16:16:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08581v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "When Discourse Stalls: Moving Past Five Semantic Stopsigns about Generative AI in Design Research",
      "authors": [
        "Willem van der Maden",
        "Vera van der Burg",
        "Brett A. Halperin",
        "Petra Jääskeläinen",
        "Joseph Lindley",
        "Derek Lomas",
        "Timothy Merritt"
      ],
      "abstract": "This essay examines how Generative AI (GenAI) is rapidly transforming design\npractices and how discourse often falls into over-simplified narratives that\nimpede meaningful research and practical progress. We identify and deconstruct\nfive prevalent \"semantic stopsigns\" -- reductive framings about GenAI in design\nthat halt deeper inquiry and limit productive engagement. Reflecting upon two\nexpert workshops at ACM conferences and semi-structured interviews with design\npractitioners, we analyze how these stopsigns manifest in research and\npractice. Our analysis develops mid-level knowledge that bridges theoretical\ndiscourse and practical implementation, helping designers and researchers\ninterrogate common assumptions about GenAI in their own contexts. By recasting\nthese stopsigns into more nuanced frameworks, we provide the design research\ncommunity with practical approaches for thinking about and working with these\nemerging technologies.",
      "pdf_url": "http://arxiv.org/pdf/2503.08565v1",
      "published": "2025-03-11T15:54:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08565v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "MoE-Loco: Mixture of Experts for Multitask Locomotion",
      "authors": [
        "Runhan Huang",
        "Shaoting Zhu",
        "Yilun Du",
        "Hang Zhao"
      ],
      "abstract": "We present MoE-Loco, a Mixture of Experts (MoE) framework for multitask\nlocomotion for legged robots. Our method enables a single policy to handle\ndiverse terrains, including bars, pits, stairs, slopes, and baffles, while\nsupporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradient\nconflicts that typically arise in multitask reinforcement learning, improving\nboth training efficiency and performance. Our experiments demonstrate that\ndifferent experts naturally specialize in distinct locomotion behaviors, which\ncan be leveraged for task migration and skill composition. We further validate\nour approach in both simulation and real-world deployment, showcasing its\nrobustness and adaptability.",
      "pdf_url": "http://arxiv.org/pdf/2503.08564v1",
      "published": "2025-03-11T15:53:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08564v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
      "authors": [
        "Chen Xu",
        "Tony Khuong Nguyen",
        "Emma Dixon",
        "Christopher Rodriguez",
        "Patrick Miller",
        "Robert Lee",
        "Paarth Shah",
        "Rares Ambrus",
        "Haruki Nishimura",
        "Masha Itkina"
      ],
      "abstract": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment.",
      "pdf_url": "http://arxiv.org/pdf/2503.08558v1",
      "published": "2025-03-11T15:47:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08558v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs",
      "authors": [
        "Wanyong Feng",
        "Peter Tran",
        "Stephen Sireci",
        "Andrew Lan"
      ],
      "abstract": "The difficulty of multiple-choice questions (MCQs) is a crucial factor for\neducational assessments. Predicting MCQ difficulty is challenging since it\nrequires understanding both the complexity of reaching the correct option and\nthe plausibility of distractors, i.e., incorrect options. In this paper, we\npropose a novel, two-stage method to predict the difficulty of MCQs. First, to\nbetter estimate the complexity of each MCQ, we use large language models (LLMs)\nto augment the reasoning steps required to reach each option. We use not just\nthe MCQ itself but also these reasoning steps as input to predict the\ndifficulty. Second, to capture the plausibility of distractors, we sample\nknowledge levels from a distribution to account for variation among students\nresponding to the MCQ. This setup, inspired by item response theory (IRT),\nenable us to estimate the likelihood of students selecting each (both correct\nand incorrect) option. We align these predictions with their ground truth\nvalues, using a Kullback-Leibler (KL) divergence-based regularization\nobjective, and use estimated likelihoods to predict MCQ difficulty. We evaluate\nour method on two real-world \\emph{math} MCQ and response datasets with ground\ntruth difficulty values estimated using IRT. Experimental results show that our\nmethod outperforms all baselines, up to a 28.3\\% reduction in mean squared\nerror and a 34.6\\% improvement in the coefficient of determination. We also\nqualitatively discuss how our novel method results in higher accuracy in\npredicting MCQ difficulty.",
      "pdf_url": "http://arxiv.org/pdf/2503.08551v1",
      "published": "2025-03-11T15:39:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08551v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research Idea Generation",
      "authors": [
        "Xian Gao",
        "Zongyun Zhang",
        "Mingye Xie",
        "Ting Liu",
        "Yuzhuo Fu"
      ],
      "abstract": "Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas.",
      "pdf_url": "http://arxiv.org/pdf/2503.08549v1",
      "published": "2025-03-11T15:36:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08549v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form Question-Answering",
      "authors": [
        "Sher Badshah",
        "Hassan Sajjad"
      ],
      "abstract": "Evaluating Large Language Models (LLMs) free-form generated responses remains\na challenge due to their diverse and open-ended nature. Traditional supervised\nsignal-based automatic metrics fail to capture semantic equivalence or handle\nthe variability of open-ended responses, while human evaluation, though\nreliable, is resource-intensive. Leveraging LLMs as evaluators offers a\npromising alternative due to their strong language understanding and\ninstruction-following capabilities. Taking advantage of these capabilities, we\npropose the Dynamic Arbitration Framework for Evaluation (DAFE), which employs\ntwo primary LLM-as-judges and engages a third arbitrator only in cases of\ndisagreements. This selective arbitration prioritizes evaluation reliability\nwhile reducing unnecessary computational demands compared to conventional\nmajority voting. DAFE utilizes task-specific reference answers with dynamic\narbitration to enhance judgment accuracy, resulting in significant improvements\nin evaluation metrics such as Macro F1 and Cohen's Kappa. Through experiments,\nincluding a comprehensive human evaluation, we demonstrate DAFE's ability to\nprovide consistent, scalable, and resource-efficient assessments, establishing\nit as a robust framework for evaluating free-form model outputs.",
      "pdf_url": "http://arxiv.org/pdf/2503.08542v1",
      "published": "2025-03-11T15:29:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08542v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.0; I.2.7"
      ]
    },
    {
      "title": "Mellow: a small audio language model for reasoning",
      "authors": [
        "Soham Deshmukh",
        "Satvik Dixit",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "abstract": "Multimodal Audio-Language Models (ALMs) can understand and reason over both\naudio and text. Typically, reasoning performance correlates with model size,\nwith the best results achieved by models exceeding 8 billion parameters.\nHowever, no prior work has explored enabling small audio-language models to\nperform reasoning tasks, despite the potential applications for edge devices.\nTo address this gap, we introduce Mellow, a small Audio-Language Model\nspecifically designed for reasoning. Mellow achieves state-of-the-art\nperformance among existing small audio-language models and surpasses several\nlarger models in reasoning capabilities. For instance, Mellow scores 52.11 on\nMMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times\nfewer parameters and being trained on 60 times less data (audio hrs). To train\nMellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded\nreasoning in models. It consists of a mixture of existing datasets (30% of the\ndata) and synthetically generated data (70%). The synthetic dataset is derived\nfrom audio captioning datasets, where Large Language Models (LLMs) generate\ndetailed and multiple-choice questions focusing on audio events, objects,\nacoustic scenes, signal properties, semantics, and listener emotions. To\nevaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks,\nassessing on both in-distribution and out-of-distribution data, including audio\nunderstanding, deductive reasoning, and comparative reasoning. Finally, we\nconduct extensive ablation studies to explore the impact of projection layer\nchoices, synthetic data generation methods, and language model pretraining on\nreasoning performance. Our training dataset, findings, and baseline pave the\nway for developing small ALMs capable of reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2503.08540v1",
      "published": "2025-03-11T15:29:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08540v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Chemical reasoning in LLMs unlocks steerable synthesis planning and reaction mechanism elucidation",
      "authors": [
        "Andres M Bran",
        "Theo A Neukomm",
        "Daniel P Armstrong",
        "Zlatko Jončev",
        "Philippe Schwaller"
      ],
      "abstract": "While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems.",
      "pdf_url": "http://arxiv.org/pdf/2503.08537v1",
      "published": "2025-03-11T15:27:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08537v1",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ]
    },
    {
      "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training",
      "authors": [
        "Tong Wei",
        "Yijun Yang",
        "Junliang Xing",
        "Yuanchun Shi",
        "Zongqing Lu",
        "Deheng Ye"
      ],
      "abstract": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
      "pdf_url": "http://arxiv.org/pdf/2503.08525v1",
      "published": "2025-03-11T15:17:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08525v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Triple-Inertial Accelerated Alternating Optimization Method for Deep Learning Training",
      "authors": [
        "Chengcheng Yan",
        "Jiawei Xu",
        "Qingsong Wang",
        "Zheng Peng"
      ],
      "abstract": "The stochastic gradient descent (SGD) algorithm has achieved remarkable\nsuccess in training deep learning models. However, it has several limitations,\nincluding susceptibility to vanishing gradients, sensitivity to input data, and\na lack of robust theoretical guarantees. In recent years, alternating\nminimization (AM) methods have emerged as a promising alternative for model\ntraining by employing gradient-free approaches to iteratively update model\nparameters. Despite their potential, these methods often exhibit slow\nconvergence rates. To address this challenge, we propose a novel\nTriple-Inertial Accelerated Alternating Minimization (TIAM) framework for\nneural network training. The TIAM approach incorporates a triple-inertial\nacceleration strategy with a specialized approximation method, facilitating\ntargeted acceleration of different terms in each sub-problem optimization. This\nintegration improves the efficiency of convergence, achieving superior\nperformance with fewer iterations. Additionally, we provide a convergence\nanalysis of the TIAM algorithm, including its global convergence properties and\nconvergence rate. Extensive experiments validate the effectiveness of the TIAM\nmethod, showing significant improvements in generalization capability and\ncomputational efficiency compared to existing approaches, particularly when\napplied to the rectified linear unit (ReLU) and its variants.",
      "pdf_url": "http://arxiv.org/pdf/2503.08489v1",
      "published": "2025-03-11T14:42:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08489v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Optimizing Ride-Pooling Operations with Extended Pickup and Drop-Off Flexibility",
      "authors": [
        "Hao Jiang",
        "Yixing Xu",
        "Pradeep Varakantham"
      ],
      "abstract": "The Ride-Pool Matching Problem (RMP) is central to on-demand ride-pooling\nservices, where vehicles must be matched with multiple requests while adhering\nto service constraints such as pickup delays, detour limits, and vehicle\ncapacity. Most existing RMP solutions assume passengers are picked up and\ndropped off at their original locations, neglecting the potential for\npassengers to walk to nearby spots to meet vehicles. This assumption restricts\nthe optimization potential in ride-pooling operations. In this paper, we\npropose a novel matching method that incorporates extended pickup and drop-off\nareas for passengers. We first design a tree-based approach to efficiently\ngenerate feasible matches between passengers and vehicles. Next, we optimize\nvehicle routes to cover all designated pickup and drop-off locations while\nminimizing total travel distance. Finally, we employ dynamic assignment\nstrategies to achieve optimal matching outcomes. Experiments on city-scale taxi\ndatasets demonstrate that our method improves the number of served requests by\nup to 13\\% and average travel distance by up to 21\\% compared to leading\nexisting solutions, underscoring the potential of leveraging passenger mobility\nto significantly enhance ride-pooling service efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2503.08472v1",
      "published": "2025-03-11T14:17:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08472v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Accelerating MoE Model Inference with Expert Sharding",
      "authors": [
        "Oana Balmau",
        "Anne-Marie Kermarrec",
        "Rafael Pires",
        "André Loureiro Espírito Santo",
        "Martijn de Vos",
        "Milos Vujasinovic"
      ],
      "abstract": "Mixture of experts (MoE) models achieve state-of-the-art results in language\nmodeling but suffer from inefficient hardware utilization due to imbalanced\ntoken routing and communication overhead. While prior work has focused on\noptimizing MoE training and decoder architectures, inference for encoder-based\nMoE models in a multi-GPU with expert parallelism setting remains\nunderexplored. We introduce MoEShard, an inference system that achieves perfect\nload balancing through tensor sharding of MoE experts. Unlike existing\napproaches that rely on heuristic capacity factors or drop tokens, MoEShard\nevenly distributes computation across GPUs and ensures full token retention,\nmaximizing utilization regardless of routing skewness. We achieve this through\na strategic row- and column-wise decomposition of expert matrices. This reduces\nidle time and avoids bottlenecks caused by imbalanced expert assignments.\nFurthermore, MoEShard minimizes kernel launches by fusing decomposed expert\ncomputations, significantly improving throughput. We evaluate MoEShard against\nDeepSpeed on encoder-based architectures, demonstrating speedups of up to\n6.4$\\times$ in time to first token (TTFT). Our results show that tensor\nsharding, when properly applied to experts, is a viable and effective strategy\nfor efficient MoE inference.",
      "pdf_url": "http://arxiv.org/pdf/2503.08467v1",
      "published": "2025-03-11T14:15:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08467v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective",
      "authors": [
        "Olga Meyer",
        "Marvin Boell",
        "Christoph Legat"
      ],
      "abstract": "The rapid development of Industry 4.0 technologies requires robust and\ncomprehensive standardization to ensure interoperability, safety and efficiency\nin the Industry of the Future. This paper examines the fundamental role and\nfunctionality of standardization, with a particular focus on its importance in\nEurope's regulatory framework. Based on this, selected topics in context of\nstandardization activities in context intelligent manufacturing and digital\ntwins are highlighted and, by that, an overview of the Industry 4.0 standards\nframework is provided. This paper serves both as an informative guide to the\nexisting standards in Industry 4.0 with respect to Artificial Intelligence and\nDigital Twins, and as a call to action for increased cooperation between\nstandardization bodies and the research community. By fostering such\ncollaboration, we aim to facilitate the continued development and\nimplementation of standards that will drive innovation and progress in the\nmanufacturing sector.",
      "pdf_url": "http://arxiv.org/pdf/2503.08460v2",
      "published": "2025-03-11T14:08:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08460v2",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Controlling Latent Diffusion Using Latent CLIP",
      "authors": [
        "Jason Becker",
        "Chris Wendler",
        "Peter Baylies",
        "Robert West",
        "Christian Wressnegger"
      ],
      "abstract": "Instead of performing text-conditioned denoising in the image domain, latent\ndiffusion models (LDMs) operate in latent space of a variational autoencoder\n(VAE), enabling more efficient processing at reduced computational costs.\nHowever, while the diffusion process has moved to the latent space, the\ncontrastive language-image pre-training (CLIP) models, as used in many image\nprocessing tasks, still operate in pixel space. Doing so requires costly\nVAE-decoding of latent images before they can be processed. In this paper, we\nintroduce Latent-CLIP, a CLIP model that operates directly in the latent space.\nWe train Latent-CLIP on 2.7B pairs of latent images and descriptive texts, and\nshow that it matches zero-shot classification performance of similarly sized\nCLIP models on both the ImageNet benchmark and a LDM-generated version of it,\ndemonstrating its effectiveness in assessing both real and generated content.\nFurthermore, we construct Latent-CLIP rewards for reward-based noise\noptimization (ReNO) and show that they match the performance of their CLIP\ncounterparts on GenEval and T2I-CompBench while cutting the cost of the total\npipeline by 21%. Finally, we use Latent-CLIP to guide generation away from\nharmful content, achieving strong performance on the inappropriate image\nprompts (I2P) benchmark and a custom evaluation, without ever requiring the\ncostly step of decoding intermediate images.",
      "pdf_url": "http://arxiv.org/pdf/2503.08455v1",
      "published": "2025-03-11T14:04:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08455v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV",
        "stat.ML"
      ]
    },
    {
      "title": "ICPR 2024 Competition on Rider Intention Prediction",
      "authors": [
        "Shankar Gangisetty",
        "Abdul Wasi",
        "Shyam Nandan Rai",
        "C. V. Jawahar",
        "Sajay Raj",
        "Manish Prajapati",
        "Ayesha Choudhary",
        "Aaryadev Chandra",
        "Dev Chandan",
        "Shireen Chand",
        "Suvaditya Mukherjee"
      ],
      "abstract": "The recent surge in the vehicle market has led to an alarming increase in\nroad accidents. This underscores the critical importance of enhancing road\nsafety measures, particularly for vulnerable road users like motorcyclists.\nHence, we introduce the rider intention prediction (RIP) competition that aims\nto address challenges in rider safety by proactively predicting maneuvers\nbefore they occur, thereby strengthening rider safety. This capability enables\nthe riders to react to the potential incorrect maneuvers flagged by advanced\ndriver assistance systems (ADAS). We collect a new dataset, namely, rider\naction anticipation dataset (RAAD) for the competition consisting of two tasks:\nsingle-view RIP and multi-view RIP. The dataset incorporates a spectrum of\ntraffic conditions and challenging navigational maneuvers on roads with varying\nlighting conditions. For the competition, we received seventy-five\nregistrations and five team submissions for inference of which we compared the\nmethods of the top three performing teams on both the RIP tasks: one\nstate-space model (Mamba2) and two learning-based approaches (SVM and\nCNN-LSTM). The results indicate that the state-space model outperformed the\nother methods across the entire dataset, providing a balanced performance\nacross maneuver classes. The SVM-based RIP method showed the second-best\nperformance when using random sampling and SMOTE. However, the CNN-LSTM method\nunderperformed, primarily due to class imbalance issues, particularly\nstruggling with minority classes. This paper details the proposed RAAD dataset\nand provides a summary of the submissions for the RIP 2024 competition.",
      "pdf_url": "http://arxiv.org/pdf/2503.08437v1",
      "published": "2025-03-11T13:50:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08437v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ]
    },
    {
      "title": "AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models",
      "authors": [
        "Kwan Yun",
        "Seokhyeon Hong",
        "Chaelin Kim",
        "Junyong Noh"
      ],
      "abstract": "Despite recent advancements in learning-based motion in-betweening, a key\nlimitation has been overlooked: the requirement for character-specific\ndatasets. In this work, we introduce AnyMoLe, a novel method that addresses\nthis limitation by leveraging video diffusion models to generate motion\nin-between frames for arbitrary characters without external data. Our approach\nemploys a two-stage frame generation process to enhance contextual\nunderstanding. Furthermore, to bridge the domain gap between real-world and\nrendered character animations, we introduce ICAdapt, a fine-tuning technique\nfor video diffusion models. Additionally, we propose a ``motion-video\nmimicking'' optimization technique, enabling seamless motion generation for\ncharacters with arbitrary joint structures using 2D and 3D-aware features.\nAnyMoLe significantly reduces data dependency while generating smooth and\nrealistic transitions, making it applicable to a wide range of motion\nin-betweening tasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.08417v1",
      "published": "2025-03-11T13:28:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08417v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "68U05",
        "I.3.7; I.4.9"
      ]
    },
    {
      "title": "V-Max: Making RL practical for Autonomous Driving",
      "authors": [
        "Valentin Charraut",
        "Thomas Tournaire",
        "Waël Doulazmi",
        "Thibault Buhet"
      ],
      "abstract": "Learning-based decision-making has the potential to enable generalizable\nAutonomous Driving (AD) policies, reducing the engineering overhead of\nrule-based approaches. Imitation Learning (IL) remains the dominant paradigm,\nbenefiting from large-scale human demonstration datasets, but it suffers from\ninherent limitations such as distribution shift and imitation gaps.\nReinforcement Learning (RL) presents a promising alternative, yet its adoption\nin AD remains limited due to the lack of standardized and efficient research\nframeworks. To this end, we introduce V-Max, an open research framework\nproviding all the necessary tools to make RL practical for AD. V-Max is built\non Waymax, a hardware-accelerated AD simulator designed for large-scale\nexperimentation. We extend it using ScenarioNet's approach, enabling the fast\nsimulation of diverse AD datasets. V-Max integrates a set of observation and\nreward functions, transformer-based encoders, and training pipelines.\nAdditionally, it includes adversarial evaluation settings and an extensive set\nof evaluation metrics. Through a large-scale benchmark, we analyze how network\narchitectures, observation functions, training data, and reward shaping impact\nRL performance.",
      "pdf_url": "http://arxiv.org/pdf/2503.08388v1",
      "published": "2025-03-11T12:53:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08388v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "InfluenceNet: AI Models for Banzhaf and Shapley Value Prediction",
      "authors": [
        "Benjamin Kempinski",
        "Tal Kachman"
      ],
      "abstract": "Power indices are essential in assessing the contribution and influence of\nindividual agents in multi-agent systems, providing crucial insights into\ncollaborative dynamics and decision-making processes. While invaluable,\ntraditional computational methods for exact or estimated power indices values\nrequire significant time and computational constraints, especially for large\n$(n\\ge10)$ coalitions. These constraints have historically limited researchers'\nability to analyse complex multi-agent interactions comprehensively. To address\nthis limitation, we introduce a novel Neural Networks-based approach that\nefficiently estimates power indices for voting games, demonstrating comparable\nand often superiour performance to existing tools in terms of both speed and\naccuracy. This method not only addresses existing computational bottlenecks,\nbut also enables rapid analysis of large coalitions, opening new avenues for\nmulti-agent system research by overcoming previous computational limitations\nand providing researchers with a more accessible, scalable analytical tool.This\nincreased efficiency will allow for the analysis of more complex and realistic\nmulti-agent scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2503.08381v1",
      "published": "2025-03-11T12:40:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08381v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "I.2; F.2.1"
      ]
    },
    {
      "title": "Robust Latent Matters: Boosting Image Generation with Sampling Error",
      "authors": [
        "Kai Qiu",
        "Xiang Li",
        "Jason Kuen",
        "Hao Chen",
        "Xiaohao Xu",
        "Jiuxiang Gu",
        "Yinyi Luo",
        "Bhiksha Raj",
        "Zhe Lin",
        "Marios Savvides"
      ],
      "abstract": "Recent image generation schemes typically capture image distribution in a\npre-constructed latent space relying on a frozen image tokenizer. Though the\nperformance of tokenizer plays an essential role to the successful generation,\nits current evaluation metrics (e.g. rFID) fail to precisely assess the\ntokenizer and correlate its performance to the generation quality (e.g. gFID).\nIn this paper, we comprehensively analyze the reason for the discrepancy of\nreconstruction and generation qualities in a discrete latent space, and, from\nwhich, we propose a novel plug-and-play tokenizer training scheme to facilitate\nlatent space construction. Specifically, a latent perturbation approach is\nproposed to simulate sampling noises, i.e., the unexpected tokens sampled, from\nthe generative process. With the latent perturbation, we further propose (1) a\nnovel tokenizer evaluation metric, i.e., pFID, which successfully correlates\nthe tokenizer performance to generation quality and (2) a plug-and-play\ntokenizer training scheme, which significantly enhances the robustness of\ntokenizer thus boosting the generation quality and convergence speed. Extensive\nbenchmarking are conducted with 11 advanced discrete image tokenizers with 2\nautoregressive generation models to validate our approach. The tokenizer\ntrained with our proposed latent perturbation achieve a notable 1.60 gFID with\nclassifier-free guidance (CFG) and 3.45 gFID without CFG with a $\\sim$400M\ngenerator. Code: https://github.com/lxa9867/ImageFolder.",
      "pdf_url": "http://arxiv.org/pdf/2503.08354v1",
      "published": "2025-03-11T12:09:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08354v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MINT-Demo: Membership Inference Test Demonstrator",
      "authors": [
        "Daniel DeAlcala",
        "Aythami Morales",
        "Julian Fierrez",
        "Gonzalo Mancera",
        "Ruben Tolosana",
        "Ruben Vera-Rodriguez"
      ],
      "abstract": "We present the Membership Inference Test Demonstrator, to emphasize the need\nfor more transparent machine learning training processes. MINT is a technique\nfor experimentally determining whether certain data has been used during the\ntraining of machine learning models. We conduct experiments with popular face\nrecognition models and 5 public databases containing over 22M images. Promising\nresults, up to 89% accuracy are achieved, suggesting that it is possible to\nrecognize if an AI model has been trained with specific data. Finally, we\npresent a MINT platform as demonstrator of this technology aimed to promote\ntransparency in AI training.",
      "pdf_url": "http://arxiv.org/pdf/2503.08332v1",
      "published": "2025-03-11T11:45:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08332v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation",
      "authors": [
        "José Pombal",
        "Nuno M. Guerreiro",
        "Ricardo Rei",
        "André F. T. Martins"
      ],
      "abstract": "As automatic metrics become increasingly stronger and widely adopted, the\nrisk of unintentionally \"gaming the metric\" during model development rises.\nThis issue is caused by metric interference (Mint), i.e., the use of the same\nor related metrics for both model tuning and evaluation. Mint can misguide\npractitioners into being overoptimistic about the performance of their systems:\nas system outputs become a function of the interfering metric, their estimated\nquality loses correlation with human judgments. In this work, we analyze two\ncommon cases of Mint in machine translation-related tasks: filtering of\ntraining data, and decoding with quality signals. Importantly, we find that\nMint strongly distorts instance-level metric scores, even when metrics are not\ndirectly optimized for -- questioning the common strategy of leveraging a\ndifferent, yet related metric for evaluation that is not used for tuning. To\naddress this problem, we propose MintAdjust, a method for more reliable\nevaluation under Mint. On the WMT24 MT shared task test set, MintAdjust ranks\ntranslations and systems more accurately than state-of-the-art-metrics across a\nmajority of language pairs, especially for high-quality systems. Furthermore,\nMintAdjust outperforms AutoRank, the ensembling method used by the organizers.",
      "pdf_url": "http://arxiv.org/pdf/2503.08327v1",
      "published": "2025-03-11T11:40:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08327v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Prototype-based Heterogeneous Federated Learning for Blade Icing Detection in Wind Turbines with Class Imbalanced Data",
      "authors": [
        "Lele Qi",
        "Mengna Liu",
        "Xu Cheng",
        "Fan Shi",
        "Xiufeng Liu",
        "Shengyong Chen"
      ],
      "abstract": "Wind farms, typically in high-latitude regions, face a high risk of blade\nicing. Traditional centralized training methods raise serious privacy concerns.\nTo enhance data privacy in detecting wind turbine blade icing, traditional\nfederated learning (FL) is employed. However, data heterogeneity, resulting\nfrom collections across wind farms in varying environmental conditions, impacts\nthe model's optimization capabilities. Moreover, imbalances in wind turbine\ndata lead to models that tend to favor recognizing majority classes, thus\nneglecting critical icing anomalies. To tackle these challenges, we propose a\nfederated prototype learning model for class-imbalanced data in heterogeneous\nenvironments to detect wind turbine blade icing. We also propose a contrastive\nsupervised loss function to address the class imbalance problem. Experiments on\nreal data from 20 turbines across two wind farms show our method outperforms\nfive FL models and five class imbalance methods, with an average improvement of\n19.64\\% in \\( mF_{\\beta} \\) and 5.73\\% in \\( m \\)BA compared to the second-best\nmethod, BiFL.",
      "pdf_url": "http://arxiv.org/pdf/2503.08325v1",
      "published": "2025-03-11T11:37:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08325v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Interpretable Reinforcement Learning by Distilling Policies into Programs",
      "authors": [
        "Hector Kohler",
        "Quentin Delfosse",
        "Waris Radji",
        "Riad Akrour",
        "Philippe Preux"
      ],
      "abstract": "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability.",
      "pdf_url": "http://arxiv.org/pdf/2503.08322v1",
      "published": "2025-03-11T11:34:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08322v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with an Uncertainty-Aware Agentic Framework",
      "authors": [
        "Zhuo Zhi",
        "Chen Feng",
        "Adam Daneshmend",
        "Mine Orlu",
        "Andreas Demosthenous",
        "Lu Yin",
        "Da Li",
        "Ziquan Liu",
        "Miguel R. D. Rodrigues"
      ],
      "abstract": "Multimodal large language models (MLLMs) show promise in tasks like visual\nquestion answering (VQA) but still face challenges in multimodal reasoning.\nRecent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to\nimprove performance. However, CoT-based multimodal reasoning often demands\ncostly data annotation and fine-tuning, while agentic approaches relying on\nexternal tools risk introducing unreliable output from these tools. In this\npaper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free\nmultimodal reasoning framework that integrates external vision models with\nuncertainty quantification (UQ) into an MLLM to address these challenges.\nSpecifically, SRICE guides the inference process by allowing MLLM to\nautonomously select regions of interest through multi-stage interactions with\nthe help of external tools. We propose to use a conformal prediction-based\napproach to calibrate the output of external tools and select the optimal tool\nby estimating the uncertainty of an MLLM's output. Our experiment shows that\nthe average improvement of SRICE over the base MLLM is 4.6% on five datasets\nand the performance on some datasets even outperforms fine-tuning-based\nmethods, revealing the significance of ensuring reliable tool use in an MLLM\nagent.",
      "pdf_url": "http://arxiv.org/pdf/2503.08308v1",
      "published": "2025-03-11T11:18:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08308v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "General-Purpose Aerial Intelligent Agents Empowered by Large Language Models",
      "authors": [
        "Ji Zhao",
        "Xiao Lin"
      ],
      "abstract": "The emergence of large language models (LLMs) opens new frontiers for\nunmanned aerial vehicle (UAVs), yet existing systems remain confined to\npredefined tasks due to hardware-software co-design challenges. This paper\npresents the first aerial intelligent agent capable of open-world task\nexecution through tight integration of LLM-based reasoning and robotic\nautonomy. Our hardware-software co-designed system addresses two fundamental\nlimitations: (1) Onboard LLM operation via an edge-optimized computing\nplatform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W\npeak power; (2) A bidirectional cognitive architecture that synergizes slow\ndeliberative planning (LLM task planning) with fast reactive control (state\nestimation, mapping, obstacle avoidance, and motion planning). Validated\nthrough preliminary results using our prototype, the system demonstrates\nreliable task planning and scene understanding in communication-constrained\nenvironments, such as sugarcane monitoring, power grid inspection, mine tunnel\nexploration, and biological observation applications. This work establishes a\nnovel framework for embodied aerial artificial intelligence, bridging the gap\nbetween task planning and robotic autonomy in open environments.",
      "pdf_url": "http://arxiv.org/pdf/2503.08302v1",
      "published": "2025-03-11T11:13:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08302v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Large Language Model as Meta-Surrogate for Data-Driven Many-Task Optimization: A Proof-of-Principle Study",
      "authors": [
        "Xian-Rong Zhang",
        "Yue-Jiao Gong",
        "Jun Zhang"
      ],
      "abstract": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization.",
      "pdf_url": "http://arxiv.org/pdf/2503.08301v2",
      "published": "2025-03-11T11:13:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08301v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "D3PO: Preference-Based Alignment of Discrete Diffusion Models",
      "authors": [
        "Umberto Borso",
        "Davide Paglieri",
        "Jude Wells",
        "Tim Rocktäschel"
      ],
      "abstract": "Diffusion models have achieved state-of-the-art performance across multiple\ndomains, with recent advancements extending their applicability to discrete\ndata. However, aligning discrete diffusion models with task-specific\npreferences remains challenging, particularly in scenarios where explicit\nreward functions are unavailable. In this work, we introduce Discrete Diffusion\nDPO (D3PO), the first adaptation of Direct Preference Optimization (DPO) to\ndiscrete diffusion models formulated as continuous-time Markov chains. Our\napproach derives a novel loss function that directly fine-tunes the generative\nprocess using preference data while preserving fidelity to a reference\ndistribution. We validate D3PO on a structured binary sequence generation task,\ndemonstrating that the method effectively aligns model outputs with preferences\nwhile maintaining structural validity. Our results highlight that D3PO enables\ncontrolled fine-tuning without requiring explicit reward models, making it a\npractical alternative to reinforcement learning-based approaches. Future\nresearch will explore extending D3PO to more complex generative tasks,\nincluding language modeling and protein sequence generation, as well as\ninvestigating alternative noise schedules, such as uniform noising, to enhance\nflexibility across different applications.",
      "pdf_url": "http://arxiv.org/pdf/2503.08295v1",
      "published": "2025-03-11T11:07:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08295v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges",
      "authors": [
        "Xiaoxiao Liu",
        "Qingying Xiao",
        "Junying Chen",
        "Xiangyi Feng",
        "Xiangbo Wu",
        "Bairui Zhang",
        "Xiang Wan",
        "Jian Chang",
        "Guangjun Yu",
        "Yan Hu",
        "Benyou Wang"
      ],
      "abstract": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.",
      "pdf_url": "http://arxiv.org/pdf/2503.08292v1",
      "published": "2025-03-11T11:05:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08292v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "OminiControl2: Efficient Conditioning for Diffusion Transformers",
      "authors": [
        "Zhenxiong Tan",
        "Qiaochu Xue",
        "Xingyi Yang",
        "Songhua Liu",
        "Xinchao Wang"
      ],
      "abstract": "Fine-grained control of text-to-image diffusion transformer models (DiT)\nremains a critical challenge for practical deployment. While recent advances\nsuch as OminiControl and others have enabled a controllable generation of\ndiverse control signals, these methods face significant computational\ninefficiency when handling long conditional inputs. We present OminiControl2,\nan efficient framework that achieves efficient image-conditional image\ngeneration. OminiControl2 introduces two key innovations: (1) a dynamic\ncompression strategy that streamlines conditional inputs by preserving only the\nmost semantically relevant tokens during generation, and (2) a conditional\nfeature reuse mechanism that computes condition token features only once and\nreuses them across denoising steps. These architectural improvements preserve\nthe original framework's parameter efficiency and multi-modal versatility while\ndramatically reducing computational costs. Our experiments demonstrate that\nOminiControl2 reduces conditional processing overhead by over 90% compared to\nits predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional\ngeneration scenarios. This efficiency enables the practical implementation of\ncomplex, multi-modal control for high-quality image synthesis with DiT models.",
      "pdf_url": "http://arxiv.org/pdf/2503.08280v1",
      "published": "2025-03-11T10:50:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08280v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models",
      "authors": [
        "Ruibin Xiong",
        "Yimeng Chen",
        "Dmitrii Khizbullin",
        "Jürgen Schmidhuber"
      ],
      "abstract": "Long-form writing agents require flexible integration and interaction across\ninformation retrieval, reasoning, and composition. Current approaches rely on\npredetermined workflows and rigid thinking patterns to generate outlines before\nwriting, resulting in constrained adaptability during writing. In this paper we\npropose a general agent framework that achieves human-like adaptive writing\nthrough recursive task decomposition and dynamic integration of three\nfundamental task types, i.e. retrieval, reasoning, and composition. Our\nmethodology features: 1) a planning mechanism that interleaves recursive task\ndecomposition and execution, eliminating artificial restrictions on writing\nworkflow; and 2) integration of task types that facilitates heterogeneous task\ndecomposition. Evaluations on both fiction writing and technical report\ngeneration show that our method consistently outperforms state-of-the-art\napproaches across all automatic evaluation metrics, which demonstrate the\neffectiveness and broad applicability of our proposed framework.",
      "pdf_url": "http://arxiv.org/pdf/2503.08275v1",
      "published": "2025-03-11T10:43:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08275v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
      "authors": [
        "Junying Wang",
        "Hongyuan Zhang",
        "Yuan Yuan"
      ],
      "abstract": "Recent Customized Portrait Generation (CPG) methods, taking a facial image\nand a textual prompt as inputs, have attracted substantial attention. Although\nthese methods generate high-fidelity portraits, they fail to prevent the\ngenerated portraits from being tracked and misused by malicious face\nrecognition systems. To address this, this paper proposes a Customized Portrait\nGeneration framework with facial Adversarial attacks (Adv-CPG). Specifically,\nto achieve facial privacy protection, we devise a lightweight local ID\nencryptor and an encryption enhancer. They implement progressive double-layer\nencryption protection by directly injecting the target identity and adding\nadditional identity guidance, respectively. Furthermore, to accomplish\nfine-grained and personalized portrait generation, we develop a multi-modal\nimage customizer capable of generating controlled fine-grained facial features.\nTo the best of our knowledge, Adv-CPG is the first study that introduces facial\nadversarial attacks into CPG. Extensive experiments demonstrate the superiority\nof Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is\n28.1% and 2.86% higher compared to the SOTA noise-based attack methods and\nunconstrained attack methods, respectively.",
      "pdf_url": "http://arxiv.org/pdf/2503.08269v1",
      "published": "2025-03-11T10:34:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08269v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness",
      "authors": [
        "Yiming Zhong",
        "Qi Jiang",
        "Jingyi Yu",
        "Yuexin Ma"
      ],
      "abstract": "A dexterous hand capable of grasping any object is essential for the\ndevelopment of general-purpose embodied intelligent robots. However, due to the\nhigh degree of freedom in dexterous hands and the vast diversity of objects,\ngenerating high-quality, usable grasping poses in a robust manner is a\nsignificant challenge. In this paper, we introduce DexGrasp Anything, a method\nthat effectively integrates physical constraints into both the training and\nsampling phases of a diffusion-based generative model, achieving\nstate-of-the-art performance across nearly all open datasets. Additionally, we\npresent a new dexterous grasping dataset containing over 3.4 million diverse\ngrasping poses for more than 15k different objects, demonstrating its potential\nto advance universal dexterous grasping. The code of our method and our dataset\nwill be publicly released soon.",
      "pdf_url": "http://arxiv.org/pdf/2503.08257v1",
      "published": "2025-03-11T10:21:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.08257v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    }
  ]
}