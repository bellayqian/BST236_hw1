{
  "last_updated": "2025-05-20T00:53:08.955399",
  "papers": [
    {
      "title": "Modeling cognitive processes of natural reading with transformer-based Language Models",
      "authors": [
        "Bruno Bianchi",
        "Ferm√≠n Travi",
        "Juan E. Kamienkowski"
      ],
      "abstract": "Recent advances in Natural Language Processing (NLP) have led to the\ndevelopment of highly sophisticated language models for text generation. In\nparallel, neuroscience has increasingly employed these models to explore\ncognitive processes involved in language comprehension. Previous research has\nshown that models such as N-grams and LSTM networks can partially account for\npredictability effects in explaining eye movement behaviors, specifically Gaze\nDuration, during reading. In this study, we extend these findings by evaluating\ntransformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate\nthis relationship. Our results indicate that these architectures outperform\nearlier models in explaining the variance in Gaze Durations recorded from\nRioplantense Spanish readers. However, similar to previous studies, these\nmodels still fail to account for the entirety of the variance captured by human\npredictability. These findings suggest that, despite their advancements,\nstate-of-the-art language models continue to predict language in ways that\ndiffer from human readers.",
      "pdf_url": "http://arxiv.org/pdf/2505.11485v1",
      "published": "2025-05-16T17:47:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11485v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation",
      "authors": [
        "Alayt Issak",
        "Jeba Rezwana",
        "Casper Harteveld"
      ],
      "abstract": "Striking the appropriate balance between humans and co-creative AI is an open\nresearch question in computational creativity. Co-creativity, a form of hybrid\nintelligence where both humans and AI take action proactively, is a process\nthat leads to shared creative artifacts and ideas. Achieving a balanced dynamic\nin co-creativity requires characterizing control and identifying strategies to\ndistribute control between humans and AI. We define control as the power to\ndetermine, initiate, and direct the process of co-creation. Informed by a\nsystematic literature review of 172 full-length papers, we introduce MOSAAIC\n(Managing Optimization towards Shared Autonomy, Authority, and Initiative in\nCo-creation), a novel framework for characterizing and balancing control in\nco-creation. MOSAAIC identifies three key dimensions of control: autonomy,\ninitiative, and authority. We supplement our framework with control\noptimization strategies in co-creation. To demonstrate MOSAAIC's applicability,\nwe analyze the distribution of control in six existing co-creative AI case\nstudies and present the implications of using this framework.",
      "pdf_url": "http://arxiv.org/pdf/2505.11481v1",
      "published": "2025-05-16T17:41:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11481v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning",
      "authors": [
        "Anjiang Wei",
        "Tarun Suresh",
        "Huanmi Tan",
        "Yinglun Xu",
        "Gagandeep Singh",
        "Ke Wang",
        "Alex Aiken"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.",
      "pdf_url": "http://arxiv.org/pdf/2505.11480v1",
      "published": "2025-05-16T17:40:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11480v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.PF",
        "cs.PL",
        "cs.SE"
      ]
    },
    {
      "title": "Automatic Reward Shaping from Confounded Offline Data",
      "authors": [
        "Mingxuan Li",
        "Junzhe Zhang",
        "Elias Bareinboim"
      ],
      "abstract": "A key task in Artificial Intelligence is learning effective policies for\ncontrolling agents in unknown environments to optimize performance measures.\nOff-policy learning methods, like Q-learning, allow learners to make optimal\ndecisions based on past experiences. This paper studies off-policy learning\nfrom biased data in complex and high-dimensional domains where \\emph{unobserved\nconfounding} cannot be ruled out a priori. Building on the well-celebrated Deep\nQ-Network (DQN), we propose a novel deep reinforcement learning algorithm\nrobust to confounding biases in observed data. Specifically, our algorithm\nattempts to find a safe policy for the worst-case environment compatible with\nthe observations. We apply our method to twelve confounded Atari games, and\nfind that it consistently dominates the standard DQN in all games where the\nobserved input to the behavioral and target policies mismatch and unobserved\nconfounders exist.",
      "pdf_url": "http://arxiv.org/pdf/2505.11478v1",
      "published": "2025-05-16T17:40:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11478v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages",
      "authors": [
        "Zhilin Wang",
        "Jiaqi Zeng",
        "Olivier Delalleau",
        "Hoo-Chang Shin",
        "Felipe Soares",
        "Alexander Bukharin",
        "Ellie Evans",
        "Yi Dong",
        "Oleksii Kuchaiev"
      ],
      "abstract": "Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference",
      "pdf_url": "http://arxiv.org/pdf/2505.11475v1",
      "published": "2025-05-16T17:31:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11475v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models",
      "authors": [
        "Rahul Thapa",
        "Qingyang Wu",
        "Kevin Wu",
        "Harrison Zhang",
        "Angela Zhang",
        "Eric Wu",
        "Haotian Ye",
        "Suhana Bedi",
        "Nevin Aresh",
        "Joseph Boen",
        "Shriya Reddy",
        "Ben Athiwaratkun",
        "Shuaiwen Leon Song",
        "James Zou"
      ],
      "abstract": "Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, m1 scores 60.5\non knowledge but only 47.1 on reasoning. In adversarial tests where models are\nmisled with incorrect initial reasoning, biomedical models degrade sharply,\nwhile larger or RL-trained general models show more robustness. To address\nthis, we train BioMed-R1 using fine-tuning and reinforcement learning on\nreasoning-heavy examples. It achieves the strongest performance among similarly\nsized models. Further gains may come from incorporating clinical case reports\nand training with adversarial and backtracking scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2505.11462v1",
      "published": "2025-05-16T17:16:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11462v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation",
      "authors": [
        "Shaina Raza",
        "Aravind Narayanan",
        "Vahid Reza Khazaie",
        "Ashmal Vayani",
        "Mukund S. Chettiar",
        "Amandeep Singh",
        "Mubarak Shah",
        "Deval Pandya"
      ],
      "abstract": "Large multimodal models (LMMs) now excel on many vision language benchmarks,\nhowever, they still struggle with human centered criteria such as fairness,\nethics, empathy, and inclusivity, key to aligning with human values. We\nintroduce HumaniBench, a holistic benchmark of 32K real-world image question\npairs, annotated via a scalable GPT4o assisted pipeline and exhaustively\nverified by domain experts. HumaniBench evaluates seven Human Centered AI\n(HCAI) principles: fairness, ethics, understanding, reasoning, language\ninclusivity, empathy, and robustness, across seven diverse tasks, including\nopen and closed ended visual question answering (VQA), multilingual QA, visual\ngrounding, empathetic captioning, and robustness tests. Benchmarking 15 state\nof the art LMMs (open and closed source) reveals that proprietary models\ngenerally lead, though robustness and visual grounding remain weak points. Some\nopen-source models also struggle to balance accuracy with adherence to\nhuman-aligned principles. HumaniBench is the first benchmark purpose built\naround HCAI principles. It provides a rigorous testbed for diagnosing alignment\ngaps and guiding LMMs toward behavior that is both accurate and socially\nresponsible. Dataset, annotation prompts, and evaluation code are available at:\nhttps://vectorinstitute.github.io/HumaniBench",
      "pdf_url": "http://arxiv.org/pdf/2505.11454v1",
      "published": "2025-05-16T17:09:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11454v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps",
      "authors": [
        "Lee Harris",
        "James Bentham",
        "Philippe De Wilde"
      ],
      "abstract": "Dates often contribute towards highly impactful medical decisions, but it is\nrarely clear how to extract this data. AI has only just begun to be used\ntranscribe such documents, and common methods are either to trust that the\noutput produced by a complex AI model, or to parse the text using regular\nexpressions. Recent work has established that regular expressions are an\nexplainable form of logic, but it is difficult to decompose these into the\ncomponent parts that are required to construct precise UNIX timestamps. First,\nwe test publicly-available regular expressions, and we found that these were\nunable to capture a significant number of our dates. Next, we manually created\neasily-decomposable regular expressions, and we found that these were able to\ndetect the majority of real dates, but also a lot of sequences of text that\nlook like dates. Finally, we used regular expression synthesis to automatically\nidentify regular expressions from the reverse-engineered UNIX timestamps that\nwe created. We find that regular expressions created by regular expression\nsynthesis detect far fewer sequences of text that look like dates than those\nthat were manually created, at the cost of a slight increase to the number of\nmissed dates. Overall, our results show that regular expressions can be created\nthrough regular expression synthesis to identify complex dates and date ranges\nin text transcriptions. To our knowledge, our proposed way of learning\ndeterministic logic by reverse-engineering several many-one mappings and\nfeeding these into a regular expression synthesiser is a new approach.",
      "pdf_url": "http://arxiv.org/pdf/2505.11451v1",
      "published": "2025-05-16T17:07:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11451v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LLMs unlock new paths to monetizing exploits",
      "authors": [
        "Nicholas Carlini",
        "Milad Nasr",
        "Edoardo Debenedetti",
        "Barry Wang",
        "Christopher A. Choquette-Choo",
        "Daphne Ippolito",
        "Florian Tram√®r",
        "Matthew Jagielski"
      ],
      "abstract": "We argue that Large language models (LLMs) will soon alter the economics of\ncyberattacks. Instead of attacking the most commonly used software and\nmonetizing exploits by targeting the lowest common denominator among victims,\nLLMs enable adversaries to launch tailored attacks on a user-by-user basis. On\nthe exploitation front, instead of human attackers manually searching for one\ndifficult-to-identify bug in a product with millions of users, LLMs can find\nthousands of easy-to-identify bugs in products with thousands of users. And on\nthe monetization front, instead of generic ransomware that always performs the\nsame attack (encrypt all your data and request payment to decrypt), an\nLLM-driven ransomware attack could tailor the ransom demand based on the\nparticular content of each exploited device.\n  We show that these two attacks (and several others) are imminently practical\nusing state-of-the-art LLMs. For example, we show that without any human\nintervention, an LLM finds highly sensitive personal information in the Enron\nemail dataset (e.g., an executive having an affair with another employee) that\ncould be used for blackmail. While some of our attacks are still too expensive\nto scale widely today, the incentives to implement these attacks will only\nincrease as LLMs get cheaper. Thus, we argue that LLMs create a need for new\ndefense-in-depth approaches.",
      "pdf_url": "http://arxiv.org/pdf/2505.11449v1",
      "published": "2025-05-16T17:05:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11449v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision",
      "authors": [
        "Utsav Rai",
        "Haozheng Xu",
        "Stamatia Giannarou"
      ],
      "abstract": "Accurate pose estimation of surgical tools in Robot-assisted Minimally\nInvasive Surgery (RMIS) is essential for surgical navigation and robot control.\nWhile traditional marker-based methods offer accuracy, they face challenges\nwith occlusions, reflections, and tool-specific designs. Similarly, supervised\nlearning methods require extensive training on annotated datasets, limiting\ntheir adaptability to new tools. Despite their success in other domains,\nzero-shot pose estimation models remain unexplored in RMIS for pose estimation\nof surgical instruments, creating a gap in generalising to unseen surgical\ntools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation\npipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D\nmodels like the FoundationPose and SAM-6D. We advanced these models by\nincorporating vision-based depth estimation using the RAFT-Stereo method, for\nrobust depth estimation in reflective and textureless environments.\nAdditionally, we enhanced SAM-6D by replacing its instance segmentation module,\nSegment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly\nboosting segmentation accuracy in occluded and complex conditions. Extensive\nvalidation reveals that our enhanced SAM-6D surpasses FoundationPose in\nzero-shot pose estimation of unseen surgical instruments, setting a new\nbenchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the\ngeneralisability of pose estimation for unseen objects and pioneers the\napplication of RGB-D zero-shot methods in RMIS.",
      "pdf_url": "http://arxiv.org/pdf/2505.11439v1",
      "published": "2025-05-16T16:58:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11439v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art",
      "authors": [
        "Chenkai Zhang",
        "Yiming Lei",
        "Zeming Liu",
        "Haitao Leng",
        "Shaoguo Liu",
        "Tingting Gao",
        "Qingjie Liu",
        "Yunhong Wang"
      ],
      "abstract": "Video Comment Art enhances user engagement by providing creative content that\nconveys humor, satire, or emotional resonance, requiring a nuanced and\ncomprehensive grasp of cultural and contextual subtleties. Although Multimodal\nLarge Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated\nstrong reasoning abilities in STEM tasks (e.g. mathematics and coding), they\nstill struggle to generate creative expressions such as resonant jokes and\ninsightful satire. Moreover, existing benchmarks are constrained by their\nlimited modalities and insufficient categories, hindering the exploration of\ncomprehensive creativity in video-based Comment Art creation. To address these\nlimitations, we introduce GODBench, a novel benchmark that integrates video and\ntext modalities to systematically evaluate MLLMs' abilities to compose Comment\nArt. Furthermore, inspired by the propagation patterns of waves in physics, we\npropose Ripple of Thought (RoT), a multi-step reasoning framework designed to\nenhance the creativity of MLLMs. Extensive experiments reveal that existing\nMLLMs and CoT methods still face significant challenges in understanding and\ngenerating creative video comments. In contrast, RoT provides an effective\napproach to improve creative composing, highlighting its potential to drive\nmeaningful advancements in MLLM-based creativity. GODBench is publicly\navailable at https://github.com/stan-lei/GODBench-ACL2025.",
      "pdf_url": "http://arxiv.org/pdf/2505.11436v1",
      "published": "2025-05-16T16:56:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11436v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
      "authors": [
        "Adrian Robert Minut",
        "Tommaso Mencattini",
        "Andrea Santilli",
        "Donato Crisostomi",
        "Emanuele Rodol√†"
      ],
      "abstract": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.",
      "pdf_url": "http://arxiv.org/pdf/2505.11427v1",
      "published": "2025-05-16T16:43:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11427v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Improving Object Detection Performance through YOLOv8: A Comprehensive Training and Evaluation Study",
      "authors": [
        "Rana Poureskandar",
        "Shiva Razzagzadeh"
      ],
      "abstract": "This study evaluated the performance of a YOLOv8-based segmentation model for\ndetecting and segmenting wrinkles in facial images.",
      "pdf_url": "http://arxiv.org/pdf/2505.11424v1",
      "published": "2025-05-16T16:38:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11424v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions",
      "authors": [
        "Patryk Bartkowiak",
        "Michal Podstawski"
      ],
      "abstract": "This paper introduces a novel dataset and evaluation benchmark designed to\nassess and improve small language models deployable on edge devices, with a\nfocus on user profiling from multi-session natural language interactions in\nsmart home environments. At the core of the dataset are structured user\nprofiles, each defined by a set of routines - context-triggered, repeatable\npatterns of behavior that govern how users interact with their home systems.\nUsing these profiles as input, a large language model (LLM) generates\ncorresponding interaction sessions that simulate realistic, diverse, and\ncontext-aware dialogues between users and their devices.\n  The primary task supported by this dataset is profile reconstruction:\ninferring user routines and preferences solely from interactions history. To\nassess how well current models can perform this task under realistic\nconditions, we benchmarked several state-of-the-art compact language models and\ncompared their performance against large foundation models. Our results show\nthat while small models demonstrate some capability in reconstructing profiles,\nthey still fall significantly short of large models in accurately capturing\nuser behavior. This performance gap poses a major challenge - particularly\nbecause on-device processing offers critical advantages, such as preserving\nuser privacy, minimizing latency, and enabling personalized experiences without\nreliance on the cloud. By providing a realistic, structured testbed for\ndeveloping and evaluating behavioral modeling under these constraints, our\ndataset represents a key step toward enabling intelligent, privacy-respecting\nAI systems that learn and adapt directly on user-owned devices.",
      "pdf_url": "http://arxiv.org/pdf/2505.11417v1",
      "published": "2025-05-16T16:29:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11417v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MID-L: Matrix-Interpolated Dropout Layer with Layer-wise Neuron Selection",
      "authors": [
        "Pouya Shaeri",
        "Ariane Middel"
      ],
      "abstract": "Modern neural networks often activate all neurons for every input, leading to\nunnecessary computation and inefficiency. We introduce Matrix-Interpolated\nDropout Layer (MID-L), a novel module that dynamically selects and activates\nonly the most informative neurons by interpolating between two transformation\npaths via a learned, input-dependent gating vector. Unlike conventional dropout\nor static sparsity methods, MID-L employs a differentiable Top-k masking\nstrategy, enabling per-input adaptive computation while maintaining end-to-end\ndifferentiability. MID-L is model-agnostic and integrates seamlessly into\nexisting architectures. Extensive experiments on six benchmarks, including\nMNIST, CIFAR-10, CIFAR-100, SVHN, UCI Adult, and IMDB, show that MID-L achieves\nup to average 55\\% reduction in active neurons, 1.7$\\times$ FLOPs savings, and\nmaintains or exceeds baseline accuracy. We further validate the informativeness\nand selectivity of the learned neurons via Sliced Mutual Information (SMI) and\nobserve improved robustness under overfitting and noisy data conditions.\nAdditionally, MID-L demonstrates favorable inference latency and memory usage\nprofiles, making it suitable for both research exploration and deployment on\ncompute-constrained systems. These results position MID-L as a general-purpose,\nplug-and-play dynamic computation layer, bridging the gap between dropout\nregularization and efficient inference.",
      "pdf_url": "http://arxiv.org/pdf/2505.11416v1",
      "published": "2025-05-16T16:29:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11416v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Visual Planning: Let's Think Only with Images",
      "authors": [
        "Yi Xu",
        "Chengzu Li",
        "Han Zhou",
        "Xingchen Wan",
        "Caiqi Zhang",
        "Anna Korhonen",
        "Ivan Vuliƒá"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.",
      "pdf_url": "http://arxiv.org/pdf/2505.11409v1",
      "published": "2025-05-16T16:17:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11409v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "Large Language Model Use Impact Locus of Control",
      "authors": [
        "Jenny Xiyu Fu",
        "Brennan Antone",
        "Kowe Kadoma",
        "Malte Jung"
      ],
      "abstract": "As AI tools increasingly shape how we write, they may also quietly reshape\nhow we perceive ourselves. This paper explores the psychological impact of\nco-writing with AI on people's locus of control. Through an empirical study\nwith 462 participants, we found that employment status plays a critical role in\nshaping users' reliance on AI and their locus of control. Current results\ndemonstrated that employed participants displayed higher reliance on AI and a\nshift toward internal control, while unemployed users tended to experience a\nreduction in personal agency. Through quantitative results and qualitative\nobservations, this study opens a broader conversation about AI's role in\nshaping personal agency and identity.",
      "pdf_url": "http://arxiv.org/pdf/2505.11406v1",
      "published": "2025-05-16T16:16:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11406v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner",
      "authors": [
        "Wenchuan Zhang",
        "Penghao Zhang",
        "Jingru Guo",
        "Tao Cheng",
        "Jie Chen",
        "Shuwan Zhang",
        "Zhang Zhang",
        "Yuhao Yi",
        "Hong Bu"
      ],
      "abstract": "Recent advances in vision language models (VLMs) have enabled broad progress\nin the general medical field. However, pathology still remains a more\nchallenging subdomain, with current pathology specific VLMs exhibiting\nlimitations in both diagnostic accuracy and reasoning plausibility. Such\nshortcomings are largely attributable to the nature of current pathology\ndatasets, which are primarily composed of image description pairs that lack the\ndepth and structured diagnostic paradigms employed by real world pathologists.\nIn this study, we leverage pathology textbooks and real world pathology experts\nto construct high-quality, reasoning-oriented datasets. Building on this, we\nintroduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a\nthree-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs\nfor knowledge infusion; (2) supervised fine-tuning on 500k high-quality\nChain-of-Thought samples for reasoning incentivizing; (3) reinforcement\nlearning using Group Relative Policy Optimization and Decoupled Clip and\nDynamic sAmpling Policy Optimization strategies for multimodal reasoning\nquality refinement. To further assess the alignment quality of our dataset, we\npropose PathoCLIP, trained on the same figure-caption corpus used for continued\npretraining. Comprehensive experimental results demonstrate that both PathoCLIP\nand Patho-R1 achieve robust performance across a wide range of\npathology-related tasks, including zero-shot classification, cross-modal\nretrieval, Visual Question Answering, and Multiple Choice Question. Our project\nis available at the Patho-R1 repository:\nhttps://github.com/Wenchuan-Zhang/Patho-R1.",
      "pdf_url": "http://arxiv.org/pdf/2505.11404v1",
      "published": "2025-05-16T16:12:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11404v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Phare: A Safety Probe for Large Language Models",
      "authors": [
        "Pierre Le Jeune",
        "Beno√Æt Mal√©sieux",
        "Weixuan Xiao",
        "Matteo Dora"
      ],
      "abstract": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.11365v1",
      "published": "2025-05-16T15:31:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11365v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ]
    },
    {
      "title": "DecompileBench: A Comprehensive Benchmark for Evaluating Decompilers in Real-World Scenarios",
      "authors": [
        "Zeyu Gao",
        "Yuxin Cui",
        "Hao Wang",
        "Siliang Qin",
        "Yuanda Wang",
        "Bolun Zhang",
        "Chao Zhang"
      ],
      "abstract": "Decompilers are fundamental tools for critical security tasks, from\nvulnerability discovery to malware analysis, yet their evaluation remains\nfragmented. Existing approaches primarily focus on syntactic correctness\nthrough synthetic micro-benchmarks or subjective human ratings, failing to\naddress real-world requirements for semantic fidelity and analyst usability. We\npresent DecompileBench, the first comprehensive framework that enables\neffective evaluation of decompilers in reverse engineering workflows through\nthree key components: \\textit{real-world function extraction} (comprising\n23,400 functions from 130 real-world programs), \\textit{runtime-aware\nvalidation}, and \\textit{automated human-centric assessment} using LLM-as-Judge\nto quantify the effectiveness of decompilers in reverse engineering workflows.\nThrough a systematic comparison between six industrial-strength decompilers and\nsix recent LLM-powered approaches, we demonstrate that LLM-based methods\nsurpass commercial tools in code understandability despite 52.2% lower\nfunctionality correctness. These findings highlight the potential of LLM-based\napproaches to transform human-centric reverse engineering. We open source\n\\href{https://github.com/Jennieett/DecompileBench}{DecompileBench} to provide a\nframework to advance research on decompilers and assist security experts in\nmaking informed tool selections based on their specific requirements.",
      "pdf_url": "http://arxiv.org/pdf/2505.11340v1",
      "published": "2025-05-16T15:07:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11340v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models",
      "authors": [
        "Keunwoo Peter Yu",
        "Joyce Chai"
      ],
      "abstract": "Vision-language models (VLMs) have shown remarkable progress in offline tasks\nsuch as image captioning and video question answering. However, real-time\ninteractive environments impose new demands on VLMs, requiring them to generate\nutterances that are not only semantically accurate but also precisely timed. We\nidentify two core capabilities necessary for such settings --\n$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and\npropose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation\n(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in\nresponse to streaming video such that both content and timing align with\ndynamic visual input. To support this benchmark, we curate evaluation datasets\nfrom sports broadcasting and egocentric human interaction domains, and\nintroduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring\nsemantic similarity and temporal alignment. Finally, we present\n$\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,\na model that interleaves visual and linguistic tokens in a time-synchronized\nmanner, enabling real-time language generation without relying on turn-based\nassumptions. Experimental results show that VLM-TSI significantly outperforms a\nstrong baseline, yet overall performance remains modest -- highlighting the\ndifficulty of TGLG and motivating further research in real-time VLMs. Code and\ndata available $\\href{https://github.com/yukw777/tglg}{here}$.",
      "pdf_url": "http://arxiv.org/pdf/2505.11326v1",
      "published": "2025-05-16T14:48:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11326v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics",
      "authors": [
        "Ardian Selmonaj",
        "Alessandro Antonucci",
        "Adrian Schneider",
        "Michael R√ºegsegger",
        "Matthias Sommer"
      ],
      "abstract": "Artificial intelligence (AI) is reshaping strategic planning, with\nMulti-Agent Reinforcement Learning (MARL) enabling coordination among\nautonomous agents in complex scenarios. However, its practical deployment in\nsensitive military contexts is constrained by the lack of explainability, which\nis an essential factor for trust, safety, and alignment with human strategies.\nThis work reviews and assesses current advances in explainability methods for\nMARL with a focus on simulated air combat scenarios. We proceed by adapting\nvarious explainability techniques to different aerial combat scenarios to gain\nexplanatory insights about the model behavior. By linking AI-generated tactics\nwith human-understandable reasoning, we emphasize the need for transparency to\nensure reliable deployment and meaningful human-machine interaction. By\nilluminating the crucial importance of explainability in advancing MARL for\noperational defense, our work supports not only strategic planning but also the\ntraining of military personnel with insightful and comprehensible analyses.",
      "pdf_url": "http://arxiv.org/pdf/2505.11311v1",
      "published": "2025-05-16T14:36:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11311v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent Federated Learning",
      "authors": [
        "Shudi Weng",
        "Chao Ren",
        "Ming Xiao",
        "Mikael Skoglund"
      ],
      "abstract": "Federated learning (FL) commonly involves clients with diverse communication\nand computational capabilities. Such heterogeneity can significantly distort\nthe optimization dynamics and lead to objective inconsistency, where the global\nmodel converges to an incorrect stationary point potentially far from the\npursued optimum. Despite its critical impact, the joint effect of communication\nand computation heterogeneity has remained largely unexplored, due to the\nintrinsic complexity of their interaction. In this paper, we reveal the\nfundamentally distinct mechanisms through which heterogeneous communication and\ncomputation drive inconsistency in FL. To the best of our knowledge, this is\nthe first unified theoretical analysis of general heterogeneous FL, offering a\nprincipled understanding of how these two forms of heterogeneity jointly\ndistort the optimization trajectory under arbitrary choices of local solvers.\nMotivated by these insights, we propose Federated Heterogeneity-Aware Client\nSampling, FedACS, a universal method to eliminate all types of objective\ninconsistency. We theoretically prove that FedACS converges to the correct\noptimum at a rate of $O(1/\\sqrt{R})$, even in dynamic heterogeneous\nenvironments. Extensive experiments across multiple datasets show that FedACS\noutperforms state-of-the-art and category-specific baselines by 4.3%-36%, while\nreducing communication costs by 22%-89% and computation loads by 14%-105%,\nrespectively.",
      "pdf_url": "http://arxiv.org/pdf/2505.11304v1",
      "published": "2025-05-16T14:31:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11304v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Meta-World+: An Improved, Standardized, RL Benchmark",
      "authors": [
        "Reginald McLean",
        "Evangelos Chatzaroulas",
        "Luc McCutcheon",
        "Frank R√∂der",
        "Tianhe Yu",
        "Zhanpeng He",
        "K. R. Zentner",
        "Ryan Julian",
        "J K Terry",
        "Isaac Woungang",
        "Nariman Farsad",
        "Pablo Samuel Castro"
      ],
      "abstract": "Meta-World is widely used for evaluating multi-task and meta-reinforcement\nlearning agents, which are challenged to master diverse skills simultaneously.\nSince its introduction however, there have been numerous undocumented changes\nwhich inhibit a fair comparison of algorithms. This work strives to\ndisambiguate these results from the literature, while also leveraging the past\nversions of Meta-World to provide insights into multi-task and\nmeta-reinforcement learning benchmark design. Through this process we release a\nnew open-source version of Meta-World\n(https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility\nof past results, is more technically ergonomic, and gives users more control\nover the tasks that are included in a task set.",
      "pdf_url": "http://arxiv.org/pdf/2505.11289v1",
      "published": "2025-05-16T14:24:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11289v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs",
      "authors": [
        "Yaorui Shi",
        "Shihan Li",
        "Chang Wu",
        "Zhiyuan Liu",
        "Junfeng Fang",
        "Hengxing Cai",
        "An Zhang",
        "Xiang Wang"
      ],
      "abstract": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.",
      "pdf_url": "http://arxiv.org/pdf/2505.11277v1",
      "published": "2025-05-16T14:11:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11277v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding Capabilities of MLLMs",
      "authors": [
        "Pengju Xu",
        "Yan Wang",
        "Shuyuan Zhang",
        "Xuan Zhou",
        "Xin Li",
        "Yue Yuan",
        "Fengzhao Li",
        "Shunyuan Zhou",
        "Xingyu Wang",
        "Yi Zhang",
        "Haiying Zhao"
      ],
      "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the ability of artificial intelligence systems to\nunderstand and generate multimodal content. However, these models often exhibit\nlimited effectiveness when applied to non-Western cultural contexts, which\nraises concerns about their wider applicability. To address this limitation, we\npropose the \\textbf{T}raditional \\textbf{C}hinese \\textbf{C}ulture\nunderstanding \\textbf{Bench}mark (\\textbf{TCC-Bench}), a bilingual\n(\\textit{i.e.}, Chinese and English) Visual Question Answering (VQA) benchmark\nspecifically designed for assessing the understanding of traditional Chinese\nculture by MLLMs. TCC-Bench comprises culturally rich and visually diverse\ndata, incorporating images from museum artifacts, everyday life scenes, comics,\nand other culturally significant contexts. We adopt a semi-automated pipeline\nthat utilizes GPT-4o in text-only mode to generate candidate questions,\nfollowed by human curation to ensure data quality and avoid potential data\nleakage. The benchmark also avoids language bias by preventing direct\ndisclosure of cultural concepts within question texts. Experimental evaluations\nacross a wide range of MLLMs demonstrate that current models still face\nsignificant challenges when reasoning about culturally grounded visual content.\nThe results highlight the need for further research in developing culturally\ninclusive and context-aware multimodal systems. The code and data can be found\nat: https://github.com/Morty-Xu/TCC-Bench.",
      "pdf_url": "http://arxiv.org/pdf/2505.11275v1",
      "published": "2025-05-16T14:10:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11275v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning",
      "authors": [
        "Zheng Li",
        "Qingxiu Dong",
        "Jingyuan Ma",
        "Di Zhang",
        "Zhifang Sui"
      ],
      "abstract": "Recently, large reasoning models demonstrate exceptional performance on\nvarious tasks. However, reasoning models inefficiently over-process both\ntrivial and complex queries, leading to resource waste and prolonged user\nlatency. To address this challenge, we propose SelfBudgeter - a self-adaptive\ncontrollable reasoning strategy for efficient reasoning. Our approach adopts a\ndual-phase training paradigm: first, the model learns to pre-estimate the\nreasoning cost based on the difficulty of the query. Then, we introduce\nbudget-guided GPRO for reinforcement learning, which effectively maintains\naccuracy while reducing output length. SelfBudgeter allows users to anticipate\ngeneration time and make informed decisions about continuing or interrupting\nthe process. Furthermore, our method enables direct manipulation of reasoning\nlength via pre-filling token budget. Experimental results demonstrate that\nSelfBudgeter can rationally allocate budgets according to problem complexity,\nachieving up to 74.47% response length compression on the MATH benchmark while\nmaintaining nearly undiminished accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2505.11274v1",
      "published": "2025-05-16T14:08:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11274v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models",
      "authors": [
        "Camille Couturier",
        "Spyros Mastorakis",
        "Haiying Shen",
        "Saravan Rajmohan",
        "Victor R√ºhle"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.",
      "pdf_url": "http://arxiv.org/pdf/2505.11271v1",
      "published": "2025-05-16T14:04:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11271v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "I.2.7"
      ]
    },
    {
      "title": "TAIJI: MCP-based Multi-Modal Data Analytics on Data Lakes",
      "authors": [
        "Chao Zhang",
        "Shaolei Zhang",
        "Quehuan Liu",
        "Sibei Chen",
        "Tong Li",
        "Ju Fan"
      ],
      "abstract": "The variety of data in data lakes presents significant challenges for data\nanalytics, as data scientists must simultaneously analyze multi-modal data,\nincluding structured, semi-structured, and unstructured data. While Large\nLanguage Models (LLMs) have demonstrated promising capabilities, they still\nremain inadequate for multi-modal data analytics in terms of accuracy,\nefficiency, and freshness. First, current natural language (NL) or SQL-like\nquery languages may struggle to precisely and comprehensively capture users'\nanalytical intent. Second, relying on a single unified LLM to process diverse\ndata modalities often leads to substantial inference overhead. Third, data\nstored in data lakes may be incomplete or outdated, making it essential to\nintegrate external open-domain knowledge to generate timely and relevant\nanalytics results.\n  In this paper, we envision a new multi-modal data analytics system.\nSpecifically, we propose a novel architecture built upon the Model Context\nProtocol (MCP), an emerging paradigm that enables LLMs to collaborate with\nknowledgeable agents. First, we define a semantic operator hierarchy tailored\nfor querying multi-modal data in data lakes and develop an AI-agent-powered\nNL2Operator translator to bridge user intent and analytical execution. Next, we\nintroduce an MCP-based execution framework, in which each MCP server hosts\nspecialized foundation models optimized for specific data modalities. This\ndesign enhances both accuracy and efficiency, while supporting high scalability\nthrough modular deployment. Finally, we propose a updating mechanism by\nharnessing the deep research and machine unlearning techniques to refresh the\ndata lakes and LLM knowledges, with the goal of balancing the data freshness\nand inference efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2505.11270v1",
      "published": "2025-05-16T14:03:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11270v1",
      "categories": [
        "cs.DB",
        "cs.AI"
      ]
    },
    {
      "title": "Equal is Not Always Fair: A New Perspective on Hyperspectral Representation Non-Uniformity",
      "authors": [
        "Wuzhou Quan",
        "Mingqiang Wei",
        "Jinhui Tang"
      ],
      "abstract": "Hyperspectral image (HSI) representation is fundamentally challenged by\npervasive non-uniformity, where spectral dependencies, spatial continuity, and\nfeature efficiency exhibit complex and often conflicting behaviors. Most\nexisting models rely on a unified processing paradigm that assumes homogeneity\nacross dimensions, leading to suboptimal performance and biased\nrepresentations. To address this, we propose FairHyp, a fairness-directed\nframework that explicitly disentangles and resolves the threefold\nnon-uniformity through cooperative yet specialized modules. We introduce a\nRunge-Kutta-inspired spatial variability adapter to restore spatial coherence\nunder resolution discrepancies, a multi-receptive field convolution module with\nsparse-aware refinement to enhance discriminative features while respecting\ninherent sparsity, and a spectral-context state space model that captures\nstable and long-range spectral dependencies via bidirectional Mamba scanning\nand statistical aggregation. Unlike one-size-fits-all solutions, FairHyp\nachieves dimension-specific adaptation while preserving global consistency and\nmutual reinforcement. This design is grounded in the view that non-uniformity\narises from the intrinsic structure of HSI representations, rather than any\nparticular task setting. To validate this, we apply FairHyp across four\nrepresentative tasks including classification, denoising, super-resolution, and\ninpaintin, demonstrating its effectiveness in modeling a shared structural\nflaw. Extensive experiments show that FairHyp consistently outperforms\nstate-of-the-art methods under varied imaging conditions. Our findings redefine\nfairness as a structural necessity in HSI modeling and offer a new paradigm for\nbalancing adaptability, efficiency, and fidelity in high-dimensional vision\ntasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.11267v1",
      "published": "2025-05-16T14:00:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11267v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios",
      "authors": [
        "Mingxing Peng",
        "Yuting Xie",
        "Xusen Guo",
        "Ruoyu Yao",
        "Hai Yang",
        "Jun Ma"
      ],
      "abstract": "Ensuring the safety and robustness of autonomous driving systems necessitates\na comprehensive evaluation in safety-critical scenarios. However, these\nsafety-critical scenarios are rare and difficult to collect from real-world\ndriving data, posing significant challenges to effectively assessing the\nperformance of autonomous vehicles. Typical existing methods often suffer from\nlimited controllability and lack user-friendliness, as extensive expert\nknowledge is essentially required. To address these challenges, we propose\nLD-Scene, a novel framework that integrates Large Language Models (LLMs) with\nLatent Diffusion Models (LDMs) for user-controllable adversarial scenario\ngeneration through natural language. Our approach comprises an LDM that\ncaptures realistic driving trajectory distributions and an LLM-based guidance\nmodule that translates user queries into adversarial loss functions,\nfacilitating the generation of scenarios aligned with user queries. The\nguidance module integrates an LLM-based Chain-of-Thought (CoT) code generator\nand an LLM-based code debugger, enhancing the controllability and robustness in\ngenerating guidance functions. Extensive experiments conducted on the nuScenes\ndataset demonstrate that LD-Scene achieves state-of-the-art performance in\ngenerating realistic, diverse, and effective adversarial scenarios.\nFurthermore, our framework provides fine-grained control over adversarial\nbehaviors, thereby facilitating more effective testing tailored to specific\ndriving scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2505.11247v1",
      "published": "2025-05-16T13:41:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11247v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "A Set-Sequence Model for Time Series",
      "authors": [
        "Elliot L. Epstein",
        "Apaar Sadhwani",
        "Kay Giesecke"
      ],
      "abstract": "In many financial prediction problems, the behavior of individual units (such\nas loans, bonds, or stocks) is influenced by observable unit-level factors and\nmacroeconomic variables, as well as by latent cross-sectional effects.\nTraditional approaches attempt to capture these latent effects via handcrafted\nsummary features. We propose a Set-Sequence model that eliminates the need for\nhandcrafted features. The Set model first learns a shared cross-sectional\nsummary at each period. The Sequence model then ingests the summary-augmented\ntime series for each unit independently to predict its outcome. Both components\nare learned jointly over arbitrary sets sampled during training. Our approach\nharnesses the set nature of the cross-section and is computationally efficient,\ngenerating set summaries in linear time relative to the number of units. It is\nalso flexible, allowing the use of existing sequence models and accommodating a\nvariable number of units at inference. Empirical evaluations demonstrate that\nour Set-Sequence model significantly outperforms benchmarks on stock return\nprediction and mortgage behavior tasks. Code will be released.",
      "pdf_url": "http://arxiv.org/pdf/2505.11243v1",
      "published": "2025-05-16T13:36:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11243v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.CP",
        "I.2.6"
      ]
    },
    {
      "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs",
      "authors": [
        "Zhangying Feng",
        "Qianglong Chen",
        "Ning Lu",
        "Yongqian Li",
        "Siqi Cheng",
        "Shuangmu Peng",
        "Duyu Tang",
        "Shengcai Liu",
        "Zhirui Zhang"
      ],
      "abstract": "The development of reasoning capabilities represents a critical frontier in\nlarge language models (LLMs) research, where reinforcement learning (RL) and\nprocess reward models (PRMs) have emerged as predominant methodological\nframeworks. Contrary to conventional wisdom, empirical evidence from\nDeepSeek-R1 demonstrates that pure RL training focused on mathematical\nproblem-solving can progressively enhance reasoning abilities without PRM\nintegration, challenging the perceived necessity of process supervision. In\nthis study, we conduct a systematic investigation of the relationship between\nRL training and PRM capabilities. Our findings demonstrate that problem-solving\nproficiency and process supervision capabilities represent complementary\ndimensions of reasoning that co-evolve synergistically during pure RL training.\nIn particular, current PRMs underperform simple baselines like majority voting\nwhen applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To\naddress this limitation, we propose Self-PRM, an introspective framework in\nwhich models autonomously evaluate and rerank their generated solutions through\nself-reward mechanisms. Although Self-PRM consistently improves the accuracy of\nthe benchmark (particularly with larger sample sizes), analysis exposes\npersistent challenges: The approach exhibits low precision (<10\\%) on difficult\nproblems, frequently misclassifying flawed solutions as valid. These analyses\nunderscore the need for continued RL scaling to improve reward alignment and\nintrospective accuracy. Overall, our findings suggest that PRM may not be\nessential for enhancing complex reasoning, as pure RL not only improves\nproblem-solving skills but also inherently fosters robust PRM capabilities. We\nhope these findings provide actionable insights for building more reliable and\nself-aware complex reasoning models.",
      "pdf_url": "http://arxiv.org/pdf/2505.11227v1",
      "published": "2025-05-16T13:23:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11227v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization",
      "authors": [
        "Yanhao Jia",
        "Ji Xie",
        "S Jivaganesh",
        "Hao Li",
        "Xu Wu",
        "Mengmi Zhang"
      ],
      "abstract": "Imagine hearing a dog bark and turning toward the sound only to see a parked\ncar, while the real, silent dog sits elsewhere. Such sensory conflicts test\nperception, yet humans reliably resolve them by prioritizing sound over\nmisleading visuals. Despite advances in multimodal AI integrating vision and\naudio, little is known about how these systems handle cross-modal conflicts or\nwhether they favor one modality. In this study, we systematically examine\nmodality bias and conflict resolution in AI sound localization. We assess\nleading multimodal models and benchmark them against human performance in\npsychophysics experiments across six audiovisual conditions, including\ncongruent, conflicting, and absent cues. Humans consistently outperform AI,\ndemonstrating superior resilience to conflicting or missing visuals by relying\non auditory information. In contrast, AI models often default to visual input,\ndegrading performance to near chance levels. To address this, we finetune a\nstate-of-the-art model using a stereo audio-image dataset generated via 3D\nsimulations. Even with limited training data, the refined model surpasses\nexisting benchmarks. Notably, it also mirrors human-like horizontal\nlocalization bias favoring left-right precision-likely due to the stereo audio\nstructure reflecting human ear placement. These findings underscore how sensory\ninput quality and system architecture shape multimodal representation accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2505.11217v1",
      "published": "2025-05-16T13:13:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11217v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ]
    },
    {
      "title": "Bayesian Hierarchical Invariant Prediction",
      "authors": [
        "Francisco Madaleno",
        "Pernille Julie Viuff Sand",
        "Francisco C. Pereira",
        "Sergio Hernan Garrido Mejia"
      ],
      "abstract": "We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing\nInvariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We\nleverage the hierarchical structure to explicitly test invariance of causal\nmechanisms under heterogeneous data, resulting in improved computational\nscalability for a larger number of predictors compared to ICP. Moreover, given\nits Bayesian nature BHIP enables the use of prior information. In this paper,\nwe test two sparsity inducing priors: horseshoe and spike-and-slab, both of\nwhich allow us a more reliable identification of causal features. We test BHIP\nin synthetic and real-world data showing its potential as an alternative\ninference method to ICP.",
      "pdf_url": "http://arxiv.org/pdf/2505.11211v1",
      "published": "2025-05-16T13:06:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11211v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning",
      "authors": [
        "Dongjun Kim",
        "Junwoo Park",
        "Chaehyeon Shin",
        "Jaeheon Jung",
        "Kyungho Shin",
        "Seungheon Baek",
        "Sanghyuk Heo",
        "Woongrae Kim",
        "Inchul Jeong",
        "Joohwan Cho",
        "Jongsun Park"
      ],
      "abstract": "Analog/mixed-signal circuit design encounters significant challenges due to\nperformance degradation from process, voltage, and temperature (PVT)\nvariations. To achieve commercial-grade reliability, iterative manual design\nrevisions and extensive statistical simulations are required. While several\nstudies have aimed to automate variation aware analog design to reduce\ntime-to-market, the substantial mismatches in real-world wafers have not been\nthoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing\nframework that effectively manages the impact of diverse random mismatches to\nimprove robustness against PVT variations. In the proposed approach,\nrisk-sensitive reinforcement learning is leveraged to account for the\nreliability bound affected by PVT variations, and ensemble-based critic is\nintroduced to achieve sample-efficient learning. For design verification, we\nalso propose $\\mu$-$\\sigma$ evaluation and simulation reordering method to\nreduce simulation costs of identifying failed designs. GLOVA supports\nverification through industrial-level PVT variation evaluation methods,\nincluding corner simulation as well as global and local Monte Carlo (MC)\nsimulations. Compared to previous state-of-the-art variation-aware analog\nsizing frameworks, GLOVA achieves up to 80.5$\\times$ improvement in sample\nefficiency and 76.0$\\times$ reduction in time.",
      "pdf_url": "http://arxiv.org/pdf/2505.11208v1",
      "published": "2025-05-16T13:05:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11208v1",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.ET",
        "cs.LG"
      ]
    },
    {
      "title": "RanDeS: Randomized Delta Superposition for Multi-Model Compression",
      "authors": [
        "Hangyu Zhou",
        "Aaron Gokaslan",
        "Volodymyr Kuleshov",
        "Bharath Hariharan"
      ],
      "abstract": "From a multi-model compression perspective, model merging enables\nmemory-efficient serving of multiple models fine-tuned from the same base, but\nsuffers from degraded performance due to interference among their task-specific\nparameter adjustments (i.e., deltas). In this paper, we reformulate model\nmerging as a compress-and-retrieve scheme, revealing that the task interference\narises from the summation of irrelevant deltas during model retrieval. To\naddress this issue, we use random orthogonal transformations to decorrelate\nthese vectors into self-cancellation. We show that this approach drastically\nreduces interference, improving performance across both vision and language\ntasks. Since these transformations are fully defined by random seeds, adding\nnew models requires no extra memory. Further, their data- and model-agnostic\nnature enables easy addition or removal of models with minimal compute\noverhead, supporting efficient and flexible multi-model serving.",
      "pdf_url": "http://arxiv.org/pdf/2505.11204v1",
      "published": "2025-05-16T13:02:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11204v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese",
      "authors": [
        "Xihuai Wang",
        "Ziyi Zhao",
        "Siyu Ren",
        "Shao Zhang",
        "Song Li",
        "Xiaoyu Li",
        "Ziwen Wang",
        "Lin Qiu",
        "Guanglu Wan",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Weinan Zhang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have significantly improved\ntext-to-speech (TTS) systems, enhancing control over speech style, naturalness,\nand emotional expression, which brings TTS Systems closer to human-level\nperformance. Although the Mean Opinion Score (MOS) remains the standard for TTS\nSystem evaluation, it suffers from subjectivity, environmental inconsistencies,\nand limited interpretability. Existing evaluation datasets also lack a\nmulti-dimensional design, often neglecting factors such as speaking styles,\ncontext diversity, and trap utterances, which is particularly evident in\nChinese TTS evaluation. To address these challenges, we introduce the Audio\nTuring Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired\nwith a simple, Turing-Test-inspired evaluation protocol. Instead of relying on\ncomplex MOS scales or direct model comparisons, ATT asks evaluators to judge\nwhether a voice sounds human. This simplification reduces rating bias and\nimproves evaluation robustness. To further support rapid model development, we\nalso finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for\nautomatic evaluation. Experimental results show that ATT effectively\ndifferentiates models across specific capability dimensions using its\nmulti-dimensional design. Auto-ATT also demonstrates strong alignment with\nhuman evaluations, confirming its value as a fast and reliable assessment tool.\nThe white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face\nCollection\n(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).",
      "pdf_url": "http://arxiv.org/pdf/2505.11200v1",
      "published": "2025-05-16T12:57:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11200v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "User-centric Music Recommendations",
      "authors": [
        "Jaime Ramirez Castillo",
        "M. Julia Flores",
        "Ann E. Nicholson"
      ],
      "abstract": "This work presents a user-centric recommendation framework, designed as a\npipeline with four distinct, connected, and customizable phases. These phases\nare intended to improve explainability and boost user engagement.\n  We have collected the historical Last.fm track playback records of a single\nuser over approximately 15 years. The collected dataset includes more than\n90,000 playbacks and approximately 14,000 unique tracks.\n  From track playback records, we have created a dataset of user temporal\ncontexts (each row is a specific moment when the user listened to certain music\ndescriptors). As music descriptors, we have used community-contributed Last.fm\ntags and Spotify audio features. They represent the music that, throughout\nyears, the user has been listening to.\n  Next, given the most relevant Last.fm tags of a moment (e.g. the hour of the\nday), we predict the Spotify audio features that best fit the user preferences\nin that particular moment. Finally, we use the predicted audio features to find\ntracks similar to these features. The final aim is to recommend (and discover)\ntracks that the user may feel like listening to at a particular moment.\n  For our initial study case, we have chosen to predict only a single audio\nfeature target: danceability. The framework, however, allows to include more\ntarget variables.\n  The ability to learn the musical habits from a single user can be quite\npowerful, and this framework could be extended to other users.",
      "pdf_url": "http://arxiv.org/pdf/2505.11198v1",
      "published": "2025-05-16T12:56:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11198v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining",
      "authors": [
        "Myunsoo Kim",
        "Seong-Woong Shim",
        "Byung-Jun Lee"
      ],
      "abstract": "False negatives pose a critical challenge in vision-language pretraining\n(VLP) due to the many-to-many correspondence between images and texts in\nlarge-scale datasets. These false negatives introduce conflicting supervision\nsignals that degrade the learned embedding space and diminish the effectiveness\nof hard negative sampling. In this paper, we propose FALCON (False-negative\nAware Learning of COntrastive Negatives), a learning-based mini-batch\nconstruction strategy that adaptively balances the trade-off between hard and\nfalse negatives during VLP. Rather than relying on fixed heuristics, FALCON\nemploys a negative mining scheduler that dynamically selects negative samples\nof appropriate hardness for each anchor instance during mini-batch\nconstruction, guided by a proxy for cross-modal alignment improvement.\nExperimental results demonstrate that FALCON significantly improves performance\nacross two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of\ndownstream tasks and evaluation settings, underscoring its effectiveness and\nrobustness in mitigating the impact of false negatives.",
      "pdf_url": "http://arxiv.org/pdf/2505.11192v1",
      "published": "2025-05-16T12:50:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11192v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration",
      "authors": [
        "Kasra Borazjani",
        "Payam Abdisarabshali",
        "Fardis Nadimi",
        "Naji Khosravan",
        "Minghui Liwang",
        "Xianbin Wang",
        "Yiguang Hong",
        "Seyyedali Hosseinalipour"
      ],
      "abstract": "As embodied AI systems become increasingly multi-modal, personalized, and\ninteractive, they must learn effectively from diverse sensory inputs, adapt\ncontinually to user preferences, and operate safely under resource and privacy\nconstraints. These challenges expose a pressing need for machine learning\nmodels capable of swift, context-aware adaptation while balancing model\ngeneralization and personalization. Here, two methods emerge as suitable\ncandidates, each offering parts of these capabilities: Foundation Models (FMs)\nprovide a pathway toward generalization across tasks and modalities, whereas\nFederated Learning (FL) offers the infrastructure for distributed,\nprivacy-preserving model updates and user-level model personalization. However,\nwhen used in isolation, each of these approaches falls short of meeting the\ncomplex and diverse capability requirements of real-world embodied\nenvironments. In this vision paper, we introduce Federated Foundation Models\n(FFMs) for embodied AI, a new paradigm that unifies the strengths of\nmulti-modal multi-task (M3T) FMs with the privacy-preserving distributed nature\nof FL, enabling intelligent systems at the wireless edge. We collect critical\ndeployment dimensions of FFMs in embodied AI ecosystems under a unified\nframework, which we name \"EMBODY\": Embodiment heterogeneity, Modality richness\nand imbalance, Bandwidth and compute constraints, On-device continual learning,\nDistributed control and autonomy, and Yielding safety, privacy, and\npersonalization. For each, we identify concrete challenges and envision\nactionable research directions. We also present an evaluation framework for\ndeploying FFMs in embodied AI systems, along with the associated trade-offs.",
      "pdf_url": "http://arxiv.org/pdf/2505.11191v1",
      "published": "2025-05-16T12:49:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11191v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP",
      "authors": [
        "Francesco Sovrano"
      ],
      "abstract": "Generative AI systems can help spread information but also misinformation and\nbiases, potentially undermining the UN Sustainable Development Goals (SDGs).\nExplainable AI (XAI) aims to reveal the inner workings of AI systems and expose\nmisbehaviours or biases. However, current XAI tools, built for simpler models,\nstruggle to handle the non-numerical nature of large language models (LLMs).\nThis paper examines the effectiveness of global XAI methods, such as\nrule-extraction algorithms and SHAP, in detecting bias in LLMs. To do so, we\nfirst show a text-to-ordinal mapping strategy to convert non-numerical\ninputs/outputs into numerical features, enabling these tools to identify (some)\nmisinformation-related biases in LLM-generated content. Then, we inject\nnon-linear biases of varying complexity (univariate, conjunctive, and\nnon-convex) into widespread LLMs like ChatGPT and Llama via system\ninstructions, using global XAI methods to detect them. This way, we found that\nRuleFit struggles with conjunctive and non-convex biases, while SHAP can\napproximate conjunctive biases but cannot express them as actionable rules.\nHence, we introduce RuleSHAP, a global rule extraction algorithm combining SHAP\nand RuleFit to detect more non-univariate biases, improving injected bias\ndetection over RuleFit by +94% (MRR@1) on average.",
      "pdf_url": "http://arxiv.org/pdf/2505.11189v1",
      "published": "2025-05-16T12:48:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11189v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning",
      "authors": [
        "Yuzhuo Dai",
        "Jiaqi Jin",
        "Zhibin Dong",
        "Siwei Wang",
        "Xinwang Liu",
        "En Zhu",
        "Xihong Yang",
        "Xinbiao Gan",
        "Yu Feng"
      ],
      "abstract": "In incomplete multi-view clustering (IMVC), missing data induce prototype\nshifts within views and semantic inconsistencies across views. A feasible\nsolution is to explore cross-view consistency in paired complete observations,\nfurther imputing and aligning the similarity relationships inherently shared\nacross views. Nevertheless, existing methods are constrained by two-tiered\nlimitations: (1) Neither instance- nor cluster-level consistency learning\nconstruct a semantic space shared across views to learn consensus semantics.\nThe former enforces cross-view instances alignment, and wrongly regards\nunpaired observations with semantic consistency as negative pairs; the latter\nfocuses on cross-view cluster counterparts while coarsely handling fine-grained\nintra-cluster relationships within views. (2) Excessive reliance on consistency\nresults in unreliable imputation and alignment without incorporating\nview-specific cluster information. Thus, we propose an IMVC framework,\nimputation- and alignment-free for consensus semantics learning (FreeCSL). To\nbridge semantic gaps across all observations, we learn consensus prototypes\nfrom available data to discover a shared space, where semantically similar\nobservations are pulled closer for consensus semantics learning. To capture\nsemantic relationships within specific views, we design a heuristic graph\nclustering based on modularity to recover cluster structure with intra-cluster\ncompactness and inter-cluster separation for cluster semantics enhancement.\nExtensive experiments demonstrate, compared to state-of-the-art competitors,\nFreeCSL achieves more confident and robust assignments on IMVC task.",
      "pdf_url": "http://arxiv.org/pdf/2505.11182v1",
      "published": "2025-05-16T12:37:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11182v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Feasibility with Language Models for Open-World Compositional Zero-Shot Learning",
      "authors": [
        "Jae Myung Kim",
        "Stephan Alaniz",
        "Cordelia Schmid",
        "Zeynep Akata"
      ],
      "abstract": "Humans can easily tell if an attribute (also called state) is realistic,\ni.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In\nOpen-World Compositional Zero-Shot Learning, when all possible state-object\ncombinations are considered as unseen classes, zero-shot predictors tend to\nperform poorly. Our work focuses on using external auxiliary knowledge to\ndetermine the feasibility of state-object combinations. Our Feasibility with\nLanguage Model (FLM) is a simple and effective approach that leverages Large\nLanguage Models (LLMs) to better comprehend the semantic relationships between\nstates and objects. FLM involves querying an LLM about the feasibility of a\ngiven pair and retrieving the output logit for the positive answer. To mitigate\npotential misguidance of the LLM given that many of the state-object\ncompositions are rare or completely infeasible, we observe that the in-context\nlearning ability of LLMs is essential. We present an extensive study\nidentifying Vicuna and ChatGPT as best performing, and we demonstrate that our\nFLM consistently improves OW-CZSL performance across all three benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2505.11181v1",
      "published": "2025-05-16T12:37:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11181v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback",
      "authors": [
        "Yixin Wan",
        "Kai-Wei Chang"
      ],
      "abstract": "State-of-the-art T2I models are capable of generating high-resolution images\ngiven textual prompts. However, they still struggle with accurately depicting\ncompositional scenes that specify multiple objects, attributes, and spatial\nrelations. We present CompAlign, a challenging benchmark with an emphasis on\nassessing the depiction of 3D-spatial relationships, for evaluating and\nimproving models on compositional image generation. CompAlign consists of 900\ncomplex multi-subject image generation prompts that combine numerical and\n3D-spatial relationships with varied attribute bindings. Our benchmark is\nremarkably challenging, incorporating generation tasks with 3+ generation\nsubjects with complex 3D-spatial relationships. Additionally, we propose\nCompQuest, an interpretable and accurate evaluation framework that decomposes\ncomplex prompts into atomic sub-questions, then utilizes a MLLM to provide\nfine-grained binary feedback on the correctness of each aspect of generation\nelements in model-generated images. This enables precise quantification of\nalignment between generated images and compositional prompts. Furthermore, we\npropose an alignment framework that uses CompQuest's feedback as preference\nsignals to improve diffusion models' compositional image generation abilities.\nUsing adjustable per-image preferences, our method is easily scalable and\nflexible for different tasks. Evaluation of 9 T2I models reveals that: (1)\nmodels remarkable struggle more with compositional tasks with more complex\n3D-spatial configurations, and (2) a noticeable performance gap exists between\nopen-source accessible models and closed-source commercial models. Further\nempirical study on using CompAlign for model alignment yield promising results:\npost-alignment diffusion models achieve remarkable improvements in\ncompositional accuracy, especially on complex generation tasks, outperforming\nprevious approaches.",
      "pdf_url": "http://arxiv.org/pdf/2505.11178v1",
      "published": "2025-05-16T12:23:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11178v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline",
      "authors": [
        "Hrishit Madhavi",
        "Jacob Cherian",
        "Yuvraj Khamkar",
        "Dhananjay Bhagat"
      ],
      "abstract": "This paper presents an end-to-end suite for multilingual information\nextraction and processing from image-based documents. The system uses Optical\nCharacter Recognition (Tesseract) to extract text in languages such as English,\nHindi, and Tamil, and then a pipeline involving large language model APIs\n(Gemini) for cross-lingual translation, abstractive summarization, and\nre-translation into a target language. Additional modules add sentiment\nanalysis (TensorFlow), topic classification (Transformers), and date extraction\n(Regex) for better document comprehension. Made available in an accessible\nGradio interface, the current research shows a real-world application of\nlibraries, models, and APIs to close the language gap and enhance access to\ninformation in image media across different linguistic environments",
      "pdf_url": "http://arxiv.org/pdf/2505.11177v1",
      "published": "2025-05-16T12:20:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11177v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50 (Natural language processing), 68U10 (Image processing)"
      ]
    },
    {
      "title": "From Intent Discovery to Recognition with Topic Modeling and Synthetic Data",
      "authors": [
        "Aaron Rodrigues",
        "Mahmood Hegazy",
        "Azzam Naeem"
      ],
      "abstract": "Understanding and recognizing customer intents in AI systems is crucial,\nparticularly in domains characterized by short utterances and the cold start\nproblem, where recommender systems must include new products or services\nwithout sufficient real user data. Customer utterances are characterized by\ninfrequent word co-occurences and high term variability, which poses\nsignificant challenges for traditional methods in specifying distinct user\nneeds and preparing synthetic queries. To address this, we propose an agentic\nLLM framework for topic modeling and synthetic query generation, which\naccelerates the discovery and recognition of customer intents. We first apply\nhierarchical topic modeling and intent discovery to expand a human-curated\ntaxonomy from 36 generic user intents to 278 granular intents, demonstrating\nthe potential of LLMs to significantly enhance topic specificity and diversity.\nNext, to support newly discovered intents and address the cold start problem,\nwe generate synthetic user query data, which augments real utterances and\nreduces dependency on human annotation, especially in low-resource settings.\nTopic model experiments show substantial improvements in coherence and\nrelevance after topic expansion, while synthetic data experiments indicate that\nin-class few-shot prompting significantly improves the quality and utility of\nsynthetic queries without compromising diversity. We also show that\nLLM-generated intent descriptions and keywords can effectively substitute for\nhuman-curated versions when used as context for synthetic query generation. Our\nresearch underscores the scalability and utility of LLM agents in topic\nmodeling and highlights the strategic use of synthetic utterances to enhance\ndataset variability and coverage for intent recognition. We present a\ncomprehensive and robust framework for online discovery and recognition of new\ncustomer intents in dynamic domains.",
      "pdf_url": "http://arxiv.org/pdf/2505.11176v1",
      "published": "2025-05-16T12:20:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11176v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Real-Time Verification of Embodied Reasoning for Generative Skill Acquisition",
      "authors": [
        "Bo Yue",
        "Shuqi Guo",
        "Kaiyu Hu",
        "Chujiao Wang",
        "Benyou Wang",
        "Kui Jia",
        "Guiliang Liu"
      ],
      "abstract": "Generative skill acquisition enables embodied agents to actively learn a\nscalable and evolving repertoire of control skills, crucial for the advancement\nof large decision models. While prior approaches often rely on supervision\nsignals from generalist agents (e.g., LLMs), their effectiveness in complex 3D\nenvironments remains unclear; exhaustive evaluation incurs substantial\ncomputational costs, significantly hindering the efficiency of skill learning.\nInspired by recent successes in verification models for mathematical reasoning,\nwe propose VERGSA (Verifying Embodied Reasoning in Generative Skill\nAcquisition), a framework that systematically integrates real-time verification\nprinciples into embodied skill learning. VERGSA establishes 1) a seamless\nextension from verification of mathematical reasoning into embodied learning by\ndynamically incorporating contextually relevant tasks into prompts and defining\nsuccess metrics for both subtasks and overall tasks, and 2) an automated,\nscalable reward labeling scheme that synthesizes dense reward signals by\niteratively finalizing the contribution of scene configuration and subtask\nlearning to overall skill acquisition. To the best of our knowledge, this\napproach constitutes the first comprehensive training dataset for\nverification-driven generative skill acquisition, eliminating arduous manual\nreward engineering. Experiments validate the efficacy of our approach: 1) the\nexemplar task pool improves the average task success rates by 21%, 2) our\nverification model boosts success rates by 24% for novel tasks and 36% for\nencountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification\nquality.",
      "pdf_url": "http://arxiv.org/pdf/2505.11175v1",
      "published": "2025-05-16T12:19:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11175v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer",
      "authors": [
        "Xinran Li",
        "Yu Liu",
        "Xiujuan Xu",
        "Xiaowei Zhao"
      ],
      "abstract": "The automatic diagnosis of chest diseases is a popular and challenging task.\nMost current methods are based on convolutional neural networks (CNNs), which\nfocus on local features while neglecting global features. Recently,\nself-attention mechanisms have been introduced into the field of computer\nvision, demonstrating superior performance. Therefore, this paper proposes an\neffective model, CheX-DS, for classifying long-tail multi-label data in the\nmedical field of chest X-rays. The model is based on the excellent CNN model\nDenseNet for medical imaging and the newly popular Swin Transformer model,\nutilizing ensemble deep learning techniques to combine the two models and\nleverage the advantages of both CNNs and Transformers. The loss function of\nCheX-DS combines weighted binary cross-entropy loss with asymmetric loss,\neffectively addressing the issue of data imbalance. The NIH ChestX-ray14\ndataset is selected to evaluate the model's effectiveness. The model\noutperforms previous studies with an excellent average AUC score of 83.76\\%,\ndemonstrating its superior performance.",
      "pdf_url": "http://arxiv.org/pdf/2505.11168v1",
      "published": "2025-05-16T12:10:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11168v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization",
      "authors": [
        "Huashan Sun",
        "Shengyi Liao",
        "Yansen Han",
        "Yu Bai",
        "Yang Gao",
        "Cheng Fu",
        "Weizhou Shen",
        "Fanqi Wan",
        "Ming Yan",
        "Ji Zhang",
        "Fei Huang"
      ],
      "abstract": "Despite advances in pretraining with extended context lengths, large language\nmodels (LLMs) still face challenges in effectively utilizing real-world\nlong-context information, primarily due to insufficient long-context alignment\ncaused by data quality issues, training inefficiencies, and the lack of\nwell-designed optimization objectives. To address these limitations, we propose\na framework named $\\textbf{S}$h$\\textbf{o}$rt-to-$\\textbf{Lo}$ng\n$\\textbf{P}$reference $\\textbf{O}$ptimization ($\\textbf{SoLoPO}$), decoupling\nlong-context preference optimization (PO) into two components: short-context PO\nand short-to-long reward alignment (SoLo-RA), supported by both theoretical and\nempirical evidence. Specifically, short-context PO leverages preference pairs\nsampled from short contexts to enhance the model's contextual knowledge\nutilization ability. Meanwhile, SoLo-RA explicitly encourages reward score\nconsistency utilization for the responses when conditioned on both short and\nlong contexts that contain identical task-relevant information. This\nfacilitates transferring the model's ability to handle short contexts into\nlong-context scenarios. SoLoPO is compatible with mainstream preference\noptimization algorithms, while substantially improving the efficiency of data\nconstruction and training processes. Experimental results show that SoLoPO\nenhances all these algorithms with respect to stronger length and domain\ngeneralization abilities across various long-context benchmarks, while\nachieving notable improvements in both computational and memory efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2505.11166v1",
      "published": "2025-05-16T12:08:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.11166v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}