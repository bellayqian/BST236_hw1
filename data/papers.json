{
  "last_updated": "2025-11-01T00:54:18.303391",
  "papers": [
    {
      "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark",
      "authors": [
        "Ziyu Guo",
        "Xinyan Chen",
        "Renrui Zhang",
        "Ruichuan An",
        "Yu Qi",
        "Dongzhi Jiang",
        "Xiangtai Li",
        "Manyuan Zhang",
        "Hongsheng Li",
        "Pheng-Ann Heng"
      ],
      "abstract": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io",
      "pdf_url": "http://arxiv.org/pdf/2510.26802v1",
      "published": "2025-10-30T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26802v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Gistify! Codebase-Level Understanding via Runtime Execution",
      "authors": [
        "Hyunji Lee",
        "Minseon Kim",
        "Chinmay Singh",
        "Matheus Pereira",
        "Atharv Sonwane",
        "Isadora White",
        "Elias Stengel-Eskin",
        "Mohit Bansal",
        "Zhengyan Shi",
        "Alessandro Sordoni",
        "Marc-Alexandre Côté",
        "Xingdi Yuan",
        "Lucas Caccia"
      ],
      "abstract": "As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.",
      "pdf_url": "http://arxiv.org/pdf/2510.26790v1",
      "published": "2025-10-30T17:58:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26790v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Defeating the Training-Inference Mismatch via FP16",
      "authors": [
        "Penghui Qi",
        "Zichen Liu",
        "Xiangxin Zhou",
        "Tianyu Pang",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "abstract": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.",
      "pdf_url": "http://arxiv.org/pdf/2510.26788v1",
      "published": "2025-10-30T17:58:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26788v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
      "authors": [
        "Mantas Mazeika",
        "Alice Gatti",
        "Cristina Menghini",
        "Udari Madhushani Sehwag",
        "Shivam Singhal",
        "Yury Orlovskiy",
        "Steven Basart",
        "Manasi Sharma",
        "Denis Peskoff",
        "Elaine Lau",
        "Jaehyuk Lim",
        "Lachlan Carroll",
        "Alice Blair",
        "Vinaya Sivakumar",
        "Sumana Basu",
        "Brad Kenstler",
        "Yuntao Ma",
        "Julian Michael",
        "Xiaoke Li",
        "Oliver Ingebretsen",
        "Aditya Mehta",
        "Jean Mottola",
        "John Teichmann",
        "Kevin Yu",
        "Zaina Shaik",
        "Adam Khoja",
        "Richard Ren",
        "Jason Hausenloy",
        "Long Phan",
        "Ye Htet",
        "Ankit Aich",
        "Tahseen Rabbani",
        "Vivswan Shah",
        "Andriy Novykov",
        "Felix Binder",
        "Kirill Chugunov",
        "Luis Ramirez",
        "Matias Geralnik",
        "Hernán Mesura",
        "Dean Lee",
        "Ed-Yeremai Hernandez Cardona",
        "Annette Diamond",
        "Summer Yue",
        "Alexandr Wang",
        "Bing Liu",
        "Ernesto Hernandez",
        "Dan Hendrycks"
      ],
      "abstract": "AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.",
      "pdf_url": "http://arxiv.org/pdf/2510.26787v1",
      "published": "2025-10-30T17:58:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26787v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "LLMs Process Lists With General Filter Heads",
      "authors": [
        "Arnab Sen Sharma",
        "Giordano Rogers",
        "Natalie Shapira",
        "David Bau"
      ],
      "abstract": "We investigate the mechanisms underlying a range of list-processing tasks in\nLLMs, and we find that LLMs have learned to encode a compact, causal\nrepresentation of a general filtering operation that mirrors the generic\n\"filter\" function of functional programming. Using causal mediation analysis on\na diverse set of list-processing tasks, we find that a small number of\nattention heads, which we dub filter heads, encode a compact representation of\nthe filtering predicate in their query states at certain tokens. We demonstrate\nthat this predicate representation is general and portable: it can be extracted\nand reapplied to execute the same filtering operation on different collections,\npresented in different formats, languages, or even in tasks. However, we also\nidentify situations where transformer LMs can exploit a different strategy for\nfiltering: eagerly evaluating if an item satisfies the predicate and storing\nthis intermediate result as a flag directly in the item representations. Our\nresults reveal that transformer LMs can develop human-interpretable\nimplementations of abstract computational operations that generalize in ways\nthat are surprisingly similar to strategies used in traditional functional\nprogramming patterns.",
      "pdf_url": "http://arxiv.org/pdf/2510.26784v1",
      "published": "2025-10-30T17:57:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26784v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Clone Deterministic 3D Worlds with Geometrically-Regularized World Models",
      "authors": [
        "Zaishuo Xia",
        "Yukuan Lu",
        "Xinyi Li",
        "Yifan Xu",
        "Yubei Chen"
      ],
      "abstract": "A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.",
      "pdf_url": "http://arxiv.org/pdf/2510.26782v1",
      "published": "2025-10-30T17:56:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26782v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Faithful and Fast Influence Function via Advanced Sampling",
      "authors": [
        "Jungyeon Koh",
        "Hyeonsu Lyu",
        "Jonggyu Jang",
        "Hyun Jong Yang"
      ],
      "abstract": "How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.",
      "pdf_url": "http://arxiv.org/pdf/2510.26776v1",
      "published": "2025-10-30T17:55:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26776v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization",
      "authors": [
        "Marco Federici",
        "Riccardo Del Chiaro",
        "Boris van Breugel",
        "Paul Whatmough",
        "Markus Nagel"
      ],
      "abstract": "Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations.",
      "pdf_url": "http://arxiv.org/pdf/2510.26771v1",
      "published": "2025-10-30T17:53:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26771v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions",
      "authors": [
        "Shengnan An",
        "Xunliang Cai",
        "Xuezhi Cao",
        "Xiaoyu Li",
        "Yehao Lin",
        "Junlin Liu",
        "Xinxuan Lv",
        "Dan Ma",
        "Xuanlin Wang",
        "Ziwen Wang",
        "Shuang Zhou"
      ],
      "abstract": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2510.26768v1",
      "published": "2025-10-30T17:52:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26768v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy",
      "authors": [
        "William Overman",
        "Mohsen Bayati"
      ],
      "abstract": "As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment.",
      "pdf_url": "http://arxiv.org/pdf/2510.26752v1",
      "published": "2025-10-30T17:46:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26752v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Deep sequence models tend to memorize geometrically; it is unclear why",
      "authors": [
        "Shahriar Noroozizadeh",
        "Vaishnavh Nagarajan",
        "Elan Rosenfeld",
        "Sanjiv Kumar"
      ],
      "abstract": "In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.",
      "pdf_url": "http://arxiv.org/pdf/2510.26745v1",
      "published": "2025-10-30T17:40:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26745v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ]
    },
    {
      "title": "A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation",
      "authors": [
        "Ashwin Kumar",
        "William Yeoh"
      ],
      "abstract": "We introduce the General Incentives-based Framework for Fairness (GIFF), a\nnovel approach for fair multi-agent resource allocation that infers fair\ndecision-making from standard value functions. In resource-constrained\nsettings, agents optimizing for efficiency often create inequitable outcomes.\nOur approach leverages the action-value (Q-)function to balance efficiency and\nfairness without requiring additional training. Specifically, our method\ncomputes a local fairness gain for each action and introduces a counterfactual\nadvantage correction term to discourage over-allocation to already well-off\nagents. This approach is formalized within a centralized control setting, where\nan arbitrator uses the GIFF-modified Q-values to solve an allocation problem.\n  Empirical evaluations across diverse domains, including dynamic ridesharing,\nhomelessness prevention, and a complex job allocation task-demonstrate that our\nframework consistently outperforms strong baselines and can discover\nfar-sighted, equitable policies. The framework's effectiveness is supported by\na theoretical foundation; we prove its fairness surrogate is a principled lower\nbound on the true fairness improvement and that its trade-off parameter offers\nmonotonic tuning. Our findings establish GIFF as a robust and principled\nframework for leveraging standard reinforcement learning components to achieve\nmore equitable outcomes in complex multi-agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2510.26740v1",
      "published": "2025-10-30T17:37:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26740v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models",
      "authors": [
        "J. de Curtò",
        "I. de Zarzà",
        "Pablo García",
        "Jordi Cabot"
      ],
      "abstract": "This paper presents a comprehensive cross-platform evaluation of reasoning\ncapabilities in contemporary foundation models, establishing an\ninfrastructure-agnostic benchmark across three computational paradigms: HPC\nsupercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and\nuniversity clusters (a node with eight H200 GPUs).\n  We evaluate 15 foundation models across 79 problems spanning eight academic\ndomains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,\nCalculus, and Optimization) through three experimental phases: (1) Baseline\nestablishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,\nMistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing\nmethodology and reference performance; (2) Infrastructure validation: The\n19-problem benchmark repeated on university cluster (seven models including\nFalcon-Mamba state-space architecture) and Nebius AI Studio (nine\nstate-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3\n30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic\nreproducibility; (3) Extended evaluation: Full 79-problem assessment on both\nuniversity cluster and Nebius platforms, probing generalization at scale across\narchitectural diversity.\n  The findings challenge conventional scaling assumptions, establish training\ndata quality as more critical than model size, and provide actionable\nguidelines for model selection across educational, production, and research\ncontexts. The tri-infrastructure methodology and 79-problem benchmark enable\nlongitudinal tracking of reasoning capabilities as foundation models evolve.",
      "pdf_url": "http://arxiv.org/pdf/2510.26732v1",
      "published": "2025-10-30T17:31:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26732v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference",
      "authors": [
        "Zixu Shen",
        "Kexin Chu",
        "Yifan Zhang",
        "Dawei Xiang",
        "Runxin Wu",
        "Wei Zhang"
      ],
      "abstract": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.",
      "pdf_url": "http://arxiv.org/pdf/2510.26730v1",
      "published": "2025-10-30T17:29:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26730v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.PF"
      ]
    },
    {
      "title": "Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off",
      "authors": [
        "Muhammad Faraz Ul Abrar",
        "Nicolò Michelusi"
      ],
      "abstract": "Over-the-air (OTA) federated learning (FL) has been well recognized as a\nscalable paradigm that exploits the waveform superposition of the wireless\nmultiple-access channel to aggregate model updates in a single use. Existing\nOTA-FL designs largely enforce zero-bias model updates by either assuming\n\\emph{homogeneous} wireless conditions (equal path loss across devices) or\nforcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous}\nwireless scenarios, however, such designs are constrained by the weakest device\nand inflate the update variance. Moreover, prior analyses of biased OTA-FL\nlargely address convex objectives, while most modern AI models are highly\nnon-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient\ndescent (SGD) for general smooth non-convex objectives under wireless\nheterogeneity. We develop novel OTA-FL SGD updates that allow a structured,\ntime-invariant model bias while facilitating reduced variance updates. We\nderive a finite-time stationarity bound (expected time average squared gradient\nnorm) that explicitly reveals a bias-variance trade-off. To optimize this\ntrade-off, we pose a non-convex joint OTA power-control design and develop an\nefficient successive convex approximation (SCA) algorithm that requires only\nstatistical CSI at the base station. Experiments on a non-convex image\nclassification task validate the approach: the SCA-based design accelerates\nconvergence via an optimized bias and improves generalization over prior OTA-FL\nbaselines.",
      "pdf_url": "http://arxiv.org/pdf/2510.26722v1",
      "published": "2025-10-30T17:22:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26722v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.SY",
        "eess.SP",
        "eess.SY"
      ]
    },
    {
      "title": "Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis",
      "authors": [
        "Xinhan Zheng",
        "Huyu Wu",
        "Xueting Wang",
        "Haiyun Jiang"
      ],
      "abstract": "Multimodal large language models (MLLMs) exhibit a pronounced preference for\ntextual inputs when processing vision-language data, limiting their ability to\nreason effectively from visual evidence. Unlike prior studies that attribute\nthis text bias to external factors such as data imbalance or instruction\ntuning, we propose that the bias originates from the model's internal\narchitecture. Specifically, we hypothesize that visual key vectors (Visual\nKeys) are out-of-distribution (OOD) relative to the text key space learned\nduring language-only pretraining. Consequently, these visual keys receive\nsystematically lower similarity scores during attention computation, leading to\ntheir under-utilization in the context representation. To validate this\nhypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their\ndistributional structures using qualitative (t-SNE) and quantitative\n(Jensen-Shannon divergence) methods. The results provide direct evidence that\nvisual and textual keys occupy markedly distinct subspaces within the attention\nspace. The inter-modal divergence is statistically significant, exceeding\nintra-modal variation by several orders of magnitude. These findings reveal\nthat text bias arises from an intrinsic misalignment within the attention key\nspace rather than solely from external data factors.",
      "pdf_url": "http://arxiv.org/pdf/2510.26721v1",
      "published": "2025-10-30T17:22:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26721v1",
      "categories": [
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "On the limitation of evaluating machine unlearning using only a single training seed",
      "authors": [
        "Jamie Lanyon",
        "Axel Finke",
        "Petros Andreou",
        "Georgina Cosma"
      ],
      "abstract": "Machine unlearning (MU) aims to remove the influence of certain data points\nfrom a trained model without costly retraining. Most practical MU algorithms\nare only approximate and their performance can only be assessed empirically.\nCare must therefore be taken to make empirical comparisons as representative as\npossible. A common practice is to run the MU algorithm multiple times\nindependently starting from the same trained model. In this work, we\ndemonstrate that this practice can give highly non-representative results\nbecause -- even for the same architecture and same dataset -- some MU methods\ncan be highly sensitive to the choice of random number seed used for model\ntraining. We therefore recommend that empirical\ncomphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should\nalso reflect the variability across different model training seeds.",
      "pdf_url": "http://arxiv.org/pdf/2510.26714v1",
      "published": "2025-10-30T17:13:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26714v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching",
      "authors": [
        "Majed El Helou",
        "Chiara Troiani",
        "Benjamin Ryder",
        "Jean Diaconu",
        "Hervé Muyal",
        "Marcelo Yannuzzi"
      ],
      "abstract": "Authorizing Large Language Model driven agents to dynamically invoke tools\nand access protected resources introduces significant risks, since current\nmethods for delegating authorization grant overly broad permissions and give\naccess to tools allowing agents to operate beyond the intended task scope. We\nintroduce and assess a delegated authorization model enabling authorization\nservers to semantically inspect access requests to protected resources, and\nissue access tokens constrained to the minimal set of scopes necessary for the\nagents' assigned tasks. Given the unavailability of datasets centered on\ndelegated authorization flows, particularly including both semantically\nappropriate and inappropriate scope requests for a given task, we introduce\nASTRA, a dataset and data generation pipeline for benchmarking semantic\nmatching between tasks and scopes. Our experiments show both the potential and\ncurrent limitations of model-based matching, particularly as the number of\nscopes needed for task completion increases. Our results highlight the need for\nfurther research into semantic matching techniques enabling intent-aware\nauthorization for multi-agent and tool-augmented applications, including\nfine-grained control, such as Task-Based Access Control (TBAC).",
      "pdf_url": "http://arxiv.org/pdf/2510.26702v1",
      "published": "2025-10-30T17:07:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26702v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
      "authors": [
        "Zhichao Wang",
        "Dongyang Ma",
        "Xinting Huang",
        "Deng Cai",
        "Tian Lan",
        "Jiahao Xu",
        "Haitao Mi",
        "Xiaoying Tang",
        "Yan Wang"
      ],
      "abstract": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.",
      "pdf_url": "http://arxiv.org/pdf/2510.26697v1",
      "published": "2025-10-30T17:01:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26697v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill",
      "authors": [
        "Vaibhav Kurrey",
        "Sivakalyan Pujari",
        "Gagan Raj Gupta"
      ],
      "abstract": "We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments.",
      "pdf_url": "http://arxiv.org/pdf/2510.26684v1",
      "published": "2025-10-30T16:54:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26684v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models",
      "authors": [
        "Mingchen Tu",
        "Zhiqiang Liu",
        "Juan Li",
        "Liangyurui Liu",
        "Junjie Wang",
        "Lei Liang",
        "Wen Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.26683v1",
      "published": "2025-10-30T16:53:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26683v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "The Era of Agentic Organization: Learning to Organize with Language Models",
      "authors": [
        "Zewen Chi",
        "Li Dong",
        "Qingxiu Dong",
        "Yaru Hao",
        "Xun Wu",
        "Shaohan Huang",
        "Furu Wei"
      ],
      "abstract": "We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.",
      "pdf_url": "http://arxiv.org/pdf/2510.26658v1",
      "published": "2025-10-30T16:25:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26658v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments",
      "authors": [
        "Xiaoyi He",
        "Danggui Chen",
        "Zhenshuo Zhang",
        "Zimeng Bai"
      ],
      "abstract": "This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.",
      "pdf_url": "http://arxiv.org/pdf/2510.26646v1",
      "published": "2025-10-30T16:12:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26646v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Aeolus: A Multi-structural Flight Delay Dataset",
      "authors": [
        "Lin Xu",
        "Xinyun Yuan",
        "Yuxuan Liang",
        "Suwan Yin",
        "Yuankai Wu"
      ],
      "abstract": "We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed\nto advance research on flight delay prediction and support the development of\nfoundation models for tabular data. Existing datasets in this domain are\ntypically limited to flat tabular structures and fail to capture the\nspatiotemporal dynamics inherent in delay propagation. Aeolus addresses this\nlimitation by providing three aligned modalities: (i) a tabular dataset with\nrich operational, meteorological, and airportlevel features for over 50 million\nflights; (ii) a flight chain module that models delay propagation along\nsequential flight legs, capturing upstream and downstream dependencies; and\n(iii) a flight network graph that encodes shared aircraft, crew, and airport\nresource connections, enabling cross-flight relational reasoning. The dataset\nis carefully constructed with temporal splits, comprehensive features, and\nstrict leakage prevention to support realistic and reproducible machine\nlearning evaluation. Aeolus supports a broad range of tasks, including\nregression, classification, temporal structure modeling, and graph learning,\nserving as a unified benchmark across tabular, sequential, and graph\nmodalities. We release baseline experiments and preprocessing tools to\nfacilitate adoption. Aeolus fills a key gap for both domain-specific modeling\nand general-purpose structured data research.Our source code and data can be\naccessed at https://github.com/Flnny/Delay-data",
      "pdf_url": "http://arxiv.org/pdf/2510.26616v1",
      "published": "2025-10-30T15:41:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26616v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives",
      "authors": [
        "Kentaro Ozeki",
        "Risako Ando",
        "Takanobu Morishita",
        "Hirohiko Abe",
        "Koji Mineshima",
        "Mitsuhiro Okada"
      ],
      "abstract": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.",
      "pdf_url": "http://arxiv.org/pdf/2510.26606v1",
      "published": "2025-10-30T15:35:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26606v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling",
      "authors": [
        "Reda El Makroum",
        "Sebastian Zwickl-Bernhard",
        "Lukas Kranzl"
      ],
      "abstract": "The electricity sector transition requires substantial increases in\nresidential demand response capacity, yet Home Energy Management Systems (HEMS)\nadoption remains limited by user interaction barriers requiring translation of\neveryday preferences into technical parameters. While large language models\nhave been applied to energy systems as code generators and parameter\nextractors, no existing implementation deploys LLMs as autonomous coordinators\nmanaging the complete workflow from natural language input to multi-appliance\nscheduling. This paper presents an agentic AI HEMS where LLMs autonomously\ncoordinate multi-appliance scheduling from natural language requests to device\ncontrol, achieving optimal scheduling without example demonstrations. A\nhierarchical architecture combining one orchestrator with three specialist\nagents uses the ReAct pattern for iterative reasoning, enabling dynamic\ncoordination without hardcoded workflows while integrating Google Calendar for\ncontext-aware deadline extraction. Evaluation across three open-source models\nusing real Austrian day-ahead electricity prices reveals substantial capability\ndifferences. Llama-3.3-70B successfully coordinates all appliances across all\nscenarios to match cost-optimal benchmarks computed via mixed-integer linear\nprogramming, while other models achieve perfect single-appliance performance\nbut struggle to coordinate all appliances simultaneously. Progressive prompt\nengineering experiments demonstrate that analytical query handling without\nexplicit guidance remains unreliable despite models' general reasoning\ncapabilities. We open-source the complete system including orchestration logic,\nagent prompts, tools, and web interfaces to enable reproducibility, extension,\nand future research.",
      "pdf_url": "http://arxiv.org/pdf/2510.26603v1",
      "published": "2025-10-30T15:33:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26603v1",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching",
      "authors": [
        "Anirban Ray",
        "Vera Galinova",
        "Florian Jug"
      ],
      "abstract": "Computational Super-Resolution (CSR) in fluorescence microscopy has, despite\nbeing an ill-posed problem, a long history. At its very core, CSR is about\nfinding a prior that can be used to extrapolate frequencies in a micrograph\nthat have never been imaged by the image-generating microscope. It stands to\nreason that, with the advent of better data-driven machine learning techniques,\nstronger prior can be learned and hence CSR can lead to better results. Here,\nwe present ResMatching, a novel CSR method that uses guided conditional flow\nmatching to learn such improved data-priors. We evaluate ResMatching on 4\ndiverse biological structures from the BioSR dataset and compare its results\nagainst 7 baselines. ResMatching consistently achieves competitive results,\ndemonstrating in all cases the best trade-off between data fidelity and\nperceptual realism. We observe that CSR using ResMatching is particularly\neffective in cases where a strong prior is hard to learn, e.g. when the given\nlow-resolution images contain a lot of noise. Additionally, we show that\nResMatching can be used to sample from an implicitly learned posterior\ndistribution and that this distribution is calibrated for all tested use-cases,\nenabling our method to deliver a pixel-wise data-uncertainty term that can\nguide future users to reject uncertain predictions.",
      "pdf_url": "http://arxiv.org/pdf/2510.26601v1",
      "published": "2025-10-30T15:29:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26601v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems",
      "authors": [
        "Fulin Lin",
        "Shaowen Chen",
        "Ruishan Fang",
        "Hongwei Wang",
        "Tao Lin"
      ],
      "abstract": "While Multi-Agent Systems (MAS) excel at complex tasks, their growing\nautonomy with operational complexity often leads to critical inefficiencies,\nsuch as excessive token consumption and failures arising from misinformation.\nExisting methods primarily focus on post-hoc failure attribution, lacking\nproactive, real-time interventions to enhance robustness and efficiency. To\nthis end, we introduce SupervisorAgent, a lightweight and modular framework for\nruntime, adaptive supervision that operates without altering the base agent's\narchitecture. Triggered by an LLM-free adaptive filter, SupervisorAgent\nintervenes at critical junctures to proactively correct errors, guide\ninefficient behaviors, and purify observations. On the challenging GAIA\nbenchmark, SupervisorAgent reduces the token consumption of the Smolagent\nframework by an average of 29.45% without compromising its success rate.\nExtensive experiments across five additional benchmarks (math reasoning, code\ngeneration, and question answering) and various SoTA foundation models validate\nthe broad applicability and robustness of our approach. The code is available\nat https://github.com/LINs-lab/SupervisorAgent.",
      "pdf_url": "http://arxiv.org/pdf/2510.26585v1",
      "published": "2025-10-30T15:12:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26585v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization",
      "authors": [
        "Kun Luo",
        "Hongjin Qian",
        "Zheng Liu",
        "Ziyi Xia",
        "Shitao Xiao",
        "Siqi Bao",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.26575v1",
      "published": "2025-10-30T15:03:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26575v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Multiclass Local Calibration With the Jensen-Shannon Distance",
      "authors": [
        "Cesare Barbera",
        "Lorenzo Perini",
        "Giovanni De Toni",
        "Andrea Passerini",
        "Andrea Pugnana"
      ],
      "abstract": "Developing trustworthy Machine Learning (ML) models requires their predicted\nprobabilities to be well-calibrated, meaning they should reflect true-class\nfrequencies. Among calibration notions in multiclass classification, strong\ncalibration is the most stringent, as it requires all predicted probabilities\nto be simultaneously calibrated across all classes. However, existing\napproaches to multiclass calibration lack a notion of distance among inputs,\nwhich makes them vulnerable to proximity bias: predictions in sparse regions of\nthe feature space are systematically miscalibrated. This is especially relevant\nin high-stakes settings, such as healthcare, where the sparse instances are\nexactly those most at risk of biased treatment. In this work, we address this\nmain shortcoming by introducing a local perspective on multiclass calibration.\nFirst, we formally define multiclass local calibration and establish its\nrelationship with strong calibration. Second, we theoretically analyze the\npitfalls of existing evaluation metrics when applied to multiclass local\ncalibration. Third, we propose a practical method for enhancing local\ncalibration in Neural Networks, which enforces alignment between predicted\nprobabilities and local estimates of class frequencies using the Jensen-Shannon\ndistance. Finally, we empirically validate our approach against existing\nmulticlass calibration techniques.",
      "pdf_url": "http://arxiv.org/pdf/2510.26566v1",
      "published": "2025-10-30T14:56:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26566v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool Manipulation in Robotics",
      "authors": [
        "Prathamesh Kothavale",
        "Sravani Boddepalli"
      ],
      "abstract": "Conventional robots possess a limited understanding of their kinematics and\nare confined to preprogrammed tasks, hindering their ability to leverage tools\nefficiently. Driven by the essential components of tool usage - grasping the\ndesired outcome, selecting the most suitable tool, determining optimal tool\norientation, and executing precise manipulations - we introduce a pioneering\nframework. Our novel approach expands the capabilities of the robot's inverse\nkinematics solver, empowering it to acquire a sequential repertoire of actions\nusing tools of varying lengths. By integrating a simulation-learned action\ntrajectory with the tool, we showcase the practicality of transferring acquired\nskills from simulation to real-world scenarios through comprehensive\nexperimentation. Remarkably, our extended inverse kinematics solver\ndemonstrates an impressive error rate of less than 1 cm. Furthermore, our\ntrained policy achieves a mean error of 8 cm in simulation. Noteworthy, our\nmodel achieves virtually indistinguishable performance when employing two\ndistinct tools of different lengths. This research provides an indication of\npotential advances in the exploration of all four fundamental aspects of tool\nusage, enabling robots to master the intricate art of tool manipulation across\ndiverse tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.26551v1",
      "published": "2025-10-30T14:44:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26551v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge",
      "authors": [
        "Jack FitzGerald",
        "Aristotelis Lazaridis",
        "Dylan Bates",
        "Aman Sharma",
        "Jonnathan Castillo",
        "Yousif Azami",
        "Sean Bailey",
        "Jeremy Cao",
        "Peter Damianov",
        "Kevin de Haan",
        "Luke Kerbs",
        "Vincent Lu",
        "Joseph Madigan",
        "Jeremy McLaurin",
        "Jonathan Tainer",
        "Dave Anderson",
        "Jonathan Beck",
        "Jamie Cuticello",
        "Colton Malkerson",
        "Tyler Saltsman"
      ],
      "abstract": "We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for\nmilitary tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated\nfrom military documentation and websites. We also present four new tests sets:\n(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k\n(general military knowledge). On these military test sets, EdgeRunner 20B\nmatches or exceeds GPT-5 task performance with 95%+ statistical significance,\nexcept for the high reasoning setting on the combat medic test set and the low\nreasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no\nstatistically-significant regression on general-purpose benchmarks like ARC-C,\nGPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the\nlow reasoning setting. We also present analyses on hyperparameter settings,\ncost, and throughput. These findings show that small, locally-hosted models are\nideal solutions for data-sensitive operations such as in the military domain,\nallowing for deployment in air-gapped edge devices.",
      "pdf_url": "http://arxiv.org/pdf/2510.26550v1",
      "published": "2025-10-30T14:43:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26550v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Structure of Relation Decoding Linear Operators in Large Language Models",
      "authors": [
        "Miranda Anna Christ",
        "Adrián Csiszárik",
        "Gergely Becsó",
        "Dániel Varga"
      ],
      "abstract": "This paper investigates the structure of linear operators introduced in\nHernandez et al. [2023] that decode specific relational facts in transformer\nlanguage models. We extend their single-relation findings to a collection of\nrelations and systematically chart their organization. We show that such\ncollections of relation decoders can be highly compressed by simple order-3\ntensor networks without significant loss in decoding accuracy. To explain this\nsurprising redundancy, we develop a cross-evaluation protocol, in which we\napply each linear decoder operator to the subjects of every other relation. Our\nresults reveal that these linear maps do not encode distinct relations, but\nextract recurring, coarse-grained semantic properties (e.g., country of capital\ncity and country of food are both in the country-of-X property). This\nproperty-centric structure clarifies both the operators' compressibility and\nhighlights why they generalize only to new relations that are semantically\nclose. Our findings thus interpret linear relational decoding in transformer\nlanguage models as primarily property-based, rather than relation-specific.",
      "pdf_url": "http://arxiv.org/pdf/2510.26543v1",
      "published": "2025-10-30T14:36:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26543v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Human-AI Complementarity: A Goal for Amplified Oversight",
      "authors": [
        "Rishub Jain",
        "Sophie Bridgers",
        "Lili Janzer",
        "Rory Greig",
        "Tian Huey Teh",
        "Vladimir Mikulik"
      ],
      "abstract": "Human feedback is critical for aligning AI systems to human values. As AI\ncapabilities improve and AI is used to tackle more challenging tasks, verifying\nquality and safety becomes increasingly challenging. This paper explores how we\ncan leverage AI to improve the quality of human oversight. We focus on an\nimportant safety problem that is already challenging for humans:\nfact-verification of AI outputs. We find that combining AI ratings and human\nratings based on AI rater confidence is better than relying on either alone.\nGiving humans an AI fact-verification assistant further improves their\naccuracy, but the type of assistance matters. Displaying AI explanation,\nconfidence, and labels leads to over-reliance, but just showing search results\nand evidence fosters more appropriate trust. These results have implications\nfor Amplified Oversight -- the challenge of combining humans and AI to\nsupervise AI systems even as they surpass human expert performance.",
      "pdf_url": "http://arxiv.org/pdf/2510.26518v1",
      "published": "2025-10-30T14:11:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26518v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs",
      "authors": [
        "Dipak Meher",
        "Carlotta Domeniconi"
      ],
      "abstract": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.",
      "pdf_url": "http://arxiv.org/pdf/2510.26512v1",
      "published": "2025-10-30T14:05:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26512v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Simulating and Experimenting with Social Media Mobilization Using LLM Agents",
      "authors": [
        "Sadegh Shirani",
        "Mohsen Bayati"
      ],
      "abstract": "Online social networks have transformed the ways in which political\nmobilization messages are disseminated, raising new questions about how peer\ninfluence operates at scale. Building on the landmark 61-million-person\nFacebook experiment \\citep{bond201261}, we develop an agent-based simulation\nframework that integrates real U.S. Census demographic distributions, authentic\nTwitter network topology, and heterogeneous large language model (LLM) agents\nto examine the effect of mobilization messages on voter turnout. Each simulated\nagent is assigned demographic attributes, a personal political stance, and an\nLLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano})\nreflecting its political sophistication. Agents interact over realistic social\nnetwork structures, receiving personalized feeds and dynamically updating their\nengagement behaviors and voting intentions. Experimental conditions replicate\nthe informational and social mobilization treatments of the original Facebook\nstudy. Across scenarios, the simulator reproduces qualitative patterns observed\nin field experiments, including stronger mobilization effects under social\nmessage treatments and measurable peer spillovers. Our framework provides a\ncontrolled, reproducible environment for testing counterfactual designs and\nsensitivity analyses in political mobilization research, offering a bridge\nbetween high-validity field experiments and flexible computational\nmodeling.\\footnote{Code and data available at\nhttps://github.com/CausalMP/LLM-SocioPol}",
      "pdf_url": "http://arxiv.org/pdf/2510.26494v1",
      "published": "2025-10-30T13:43:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26494v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "Context Engineering 2.0: The Context of Context Engineering",
      "authors": [
        "Qishuo Hua",
        "Lyumanshan Ye",
        "Dayuan Fu",
        "Yang Xiao",
        "Xiaojie Cai",
        "Yunze Wu",
        "Jifan Lin",
        "Junfei Wang",
        "Pengfei Liu"
      ],
      "abstract": "Karl Marx once wrote that ``the human essence is the ensemble of social\nrelations'', suggesting that individuals are not isolated entities but are\nfundamentally shaped by their interactions with other entities, within which\ncontexts play a constitutive and essential role. With the advent of computers\nand artificial intelligence, these contexts are no longer limited to purely\nhuman--human interactions: human--machine interactions are included as well.\nThen a central question emerges: How can machines better understand our\nsituations and purposes? To address this challenge, researchers have recently\nintroduced the concept of context engineering. Although it is often regarded as\na recent innovation of the agent era, we argue that related practices can be\ntraced back more than twenty years. Since the early 1990s, the field has\nevolved through distinct historical phases, each shaped by the intelligence\nlevel of machines: from early human--computer interaction frameworks built\naround primitive computers, to today's human--agent interaction paradigms\ndriven by intelligent agents, and potentially to human--level or superhuman\nintelligence in the future. In this paper, we situate context engineering,\nprovide a systematic definition, outline its historical and conceptual\nlandscape, and examine key design considerations for practice. By addressing\nthese questions, we aim to offer a conceptual foundation for context\nengineering and sketch its promising future. This paper is a stepping stone for\na broader community effort toward systematic context engineering in AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2510.26493v1",
      "published": "2025-10-30T13:43:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26493v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks",
      "authors": [
        "Dipak Meher",
        "Carlotta Domeniconi",
        "Guadalupe Correa-Cabrera"
      ],
      "abstract": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
      "pdf_url": "http://arxiv.org/pdf/2510.26486v1",
      "published": "2025-10-30T13:39:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26486v1",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Bayesian Network Fusion of Large Language Models for Sentiment Analysis",
      "authors": [
        "Rasoul Amirzadeh",
        "Dhananjay Thiruvady",
        "Fatemeh Shiri"
      ],
      "abstract": "Large language models (LLMs) continue to advance, with an increasing number\nof domain-specific variants tailored for specialised tasks. However, these\nmodels often lack transparency and explainability, can be costly to fine-tune,\nrequire substantial prompt engineering, yield inconsistent results across\ndomains, and impose significant adverse environmental impact due to their high\ncomputational demands. To address these challenges, we propose the Bayesian\nnetwork LLM fusion (BNLF) framework, which integrates predictions from three\nLLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic\nmechanism for sentiment analysis. BNLF performs late fusion by modelling the\nsentiment predictions from multiple LLMs as probabilistic nodes within a\nBayesian network. Evaluated across three human-annotated financial corpora with\ndistinct linguistic and contextual characteristics, BNLF demonstrates\nconsistent gains of about six percent in accuracy over the baseline LLMs,\nunderscoring its robustness to dataset variability and the effectiveness of\nprobabilistic fusion for interpretable sentiment classification.",
      "pdf_url": "http://arxiv.org/pdf/2510.26484v1",
      "published": "2025-10-30T13:37:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26484v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections",
      "authors": [
        "Clarissa Sabrina Arlinghaus",
        "Tristan Kenneweg",
        "Barbara Hammer",
        "Günter W. Maier"
      ],
      "abstract": "Large language models (LLMs) such as ChatGPT are increasingly integrated into\nhigh-stakes decision-making, yet little is known about their susceptibility to\nsocial influence. We conducted three preregistered conformity experiments with\nGPT-4o in a hiring context. In a baseline study, GPT consistently favored the\nsame candidate (Profile C), reported moderate expertise (M = 3.01) and high\ncertainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT\nfaced unanimous opposition from eight simulated partners and almost always\nconformed (99.9%), reporting lower certainty and significantly elevated\nself-reported informational and normative conformity (p < .001). In Study 2\n(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of\ndisagreement trials, reporting less certainty and more normative conformity.\nAcross studies, results demonstrate that GPT does not act as an independent\nobserver but adapts to perceived social consensus. These findings highlight\nrisks of treating LLMs as neutral decision aids and underline the need to\nelicit AI judgments prior to exposing them to human opinions.",
      "pdf_url": "http://arxiv.org/pdf/2510.26481v1",
      "published": "2025-10-30T13:35:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26481v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing",
      "authors": [
        "Xin Guo",
        "Zhiheng Xi",
        "Yiwen Ding",
        "Yitao Zhai",
        "Xiaowei Shi",
        "Xunliang Cai",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.",
      "pdf_url": "http://arxiv.org/pdf/2510.26474v1",
      "published": "2025-10-30T13:26:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26474v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning",
      "authors": [
        "Fang Liu",
        "Simiao Liu",
        "Yinghao Zhu",
        "Xiaoli Lian",
        "Li Zhang"
      ],
      "abstract": "Identifying and addressing security issues during the early phase of the\ndevelopment lifecycle is critical for mitigating the long-term negative impacts\non software systems. Code review serves as an effective practice that enables\ndevelopers to check their teammates' code before integration into the codebase.\nTo streamline the generation of review comments, various automated code review\napproaches have been proposed, where LLM-based methods have significantly\nadvanced the capabilities of automated review generation. However, existing\nmodels primarily focus on general-purpose code review, their effectiveness in\nidentifying and addressing security-related issues remains underexplored.\nMoreover, adapting existing code review approaches to target security issues\nfaces substantial challenges, including data scarcity and inadequate evaluation\nmetrics. To address these limitations, we propose SecureReviewer, a new\napproach designed for enhancing LLMs' ability to identify and resolve\nsecurity-related issues during code review. Specifically, we first construct a\ndataset tailored for training and evaluating secure code review capabilities.\nLeveraging this dataset, we fine-tune LLMs to generate code review comments\nthat can effectively identify security issues and provide fix suggestions with\nour proposed secure-aware fine-tuning strategy. To mitigate hallucination in\nLLMs and enhance the reliability of their outputs, we integrate the RAG\ntechnique, which grounds the generated comments in domain-specific security\nknowledge. Additionally, we introduce SecureBLEU, a new evaluation metric\ndesigned to assess the effectiveness of review comments in addressing security\nissues. Experimental results demonstrate that SecureReviewer outperforms\nstate-of-the-art baselines in both security issue detection accuracy and the\noverall quality and practical utility of generated review comments.",
      "pdf_url": "http://arxiv.org/pdf/2510.26457v1",
      "published": "2025-10-30T13:06:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26457v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Robust Graph Condensation via Classification Complexity Mitigation",
      "authors": [
        "Jiayi Luo",
        "Qingyun Sun",
        "Beining Yang",
        "Haonan Yuan",
        "Xingcheng Fu",
        "Yanbiao Ma",
        "Jianxin Li",
        "Philip S. Yu"
      ],
      "abstract": "Graph condensation (GC) has gained significant attention for its ability to\nsynthesize smaller yet informative graphs. However, existing studies often\noverlook the robustness of GC in scenarios where the original graph is\ncorrupted. In such cases, we observe that the performance of GC deteriorates\nsignificantly, while existing robust graph learning technologies offer only\nlimited effectiveness. Through both empirical investigation and theoretical\nanalysis, we reveal that GC is inherently an intrinsic-dimension-reducing\nprocess, synthesizing a condensed graph with lower classification complexity.\nAlthough this property is critical for effective GC performance, it remains\nhighly vulnerable to adversarial perturbations. To tackle this vulnerability\nand improve GC robustness, we adopt the geometry perspective of graph data\nmanifold and propose a novel Manifold-constrained Robust Graph Condensation\nframework named MRGC. Specifically, we introduce three graph data manifold\nlearning modules that guide the condensed graph to lie within a smooth,\nlow-dimensional manifold with minimal class ambiguity, thereby preserving the\nclassification complexity reduction capability of GC and ensuring robust\nperformance under universal adversarial attacks. Extensive experiments\ndemonstrate the robustness of \\ModelName\\ across diverse attack scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2510.26451v1",
      "published": "2025-10-30T12:55:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26451v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Personalized Treatment Outcome Prediction from Scarce Data via Dual-Channel Knowledge Distillation and Adaptive Fusion",
      "authors": [
        "Wenjie Chen",
        "Li Zhuang",
        "Ziying Luo",
        "Yu Liu",
        "Jiahao Wu",
        "Shengcai Liu"
      ],
      "abstract": "Personalized treatment outcome prediction based on trial data for\nsmall-sample and rare patient groups is critical in precision medicine.\nHowever, the costly trial data limit the prediction performance. To address\nthis issue, we propose a cross-fidelity knowledge distillation and adaptive\nfusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation\ndata to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN\nincorporates a dual-channel knowledge distillation module to extract\ncomplementary knowledge from the low-fidelity model, along with an\nattention-guided fusion module to dynamically integrate multi-source\ninformation. Experiments on treatment outcome prediction for the chronic\nobstructive pulmonary disease demonstrates significant improvements of CFKD-AFN\nover state-of-the-art methods in prediction accuracy, ranging from 6.67\\% to\n74.55\\%, and strong robustness to varying high-fidelity dataset sizes.\nFurthermore, we extend CFKD-AFN to an interpretable variant, enabling the\nexploration of latent medical semantics to support clinical decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2510.26444v1",
      "published": "2025-10-30T12:50:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26444v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification",
      "authors": [
        "Yingjia Wang",
        "Ting Qiao",
        "Xing Liu",
        "Chongzuo Li",
        "Sixing Wu",
        "Jianbin Li"
      ],
      "abstract": "The rapid advancement of deep neural networks (DNNs) heavily relies on\nlarge-scale, high-quality datasets. However, unauthorized commercial use of\nthese datasets severely violates the intellectual property rights of dataset\nowners. Existing backdoor-based dataset ownership verification methods suffer\nfrom inherent limitations: poison-label watermarks are easily detectable due to\nlabel inconsistencies, while clean-label watermarks face high technical\ncomplexity and failure on high-resolution images. Moreover, both approaches\nemploy static watermark patterns that are vulnerable to detection and removal.\nTo address these issues, this paper proposes a sample-specific clean-label\nbackdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked\nsample generator, this method generates unique watermarks for each sample,\nfundamentally overcoming the vulnerability of static watermark patterns. The\ncore innovation lies in designing a composite loss function with three\ncomponents: target sample loss ensures watermark effectiveness, non-target\nsample loss guarantees trigger reliability, and perceptual similarity loss\nmaintains visual imperceptibility. During ownership verification, black-box\ntesting is employed to check whether suspicious models exhibit predefined\nbackdoor behaviors. Extensive experiments on benchmark datasets demonstrate the\neffectiveness of the proposed method and its robustness against potential\nwatermark removal attacks.",
      "pdf_url": "http://arxiv.org/pdf/2510.26420v1",
      "published": "2025-10-30T12:13:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26420v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Chain-of-Thought Hijacking",
      "authors": [
        "Jianli Zhao",
        "Tingchen Fu",
        "Rylan Schaeffer",
        "Mrinank Sharma",
        "Fazl Barez"
      ],
      "abstract": "Large reasoning models (LRMs) achieve higher task performance by allocating\nmore inference-time compute, and prior works suggest this scaled reasoning may\nalso strengthen safety by improving refusal. Yet we find the opposite: the same\nreasoning can be used to bypass safeguards. We introduce Chain-of-Thought\nHijacking, a jailbreak attack on reasoning models. The attack pads harmful\nrequests with long sequences of harmless puzzle reasoning. Across HarmBench,\nCoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on\nGemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -\nfar exceeding prior jailbreak methods for LRMs. To understand the effectiveness\nof our attack, we turn to a mechanistic analysis, which shows that mid layers\nencode the strength of safety checking, while late layers encode the\nverification outcome. Long benign CoT dilutes both signals by shifting\nattention away from harmful tokens. Targeted ablations of attention heads\nidentified by this analysis causally decrease refusal, confirming their role in\na safety subnetwork. These results show that the most interpretable form of\nreasoning - explicit CoT - can itself become a jailbreak vector when combined\nwith final-answer cues. We release prompts, outputs, and judge decisions to\nfacilitate replication.",
      "pdf_url": "http://arxiv.org/pdf/2510.26418v1",
      "published": "2025-10-30T12:10:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26418v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation",
      "authors": [
        "Xiangqing Zheng",
        "Chengyue Wu",
        "Kehai Chen",
        "Min Zhang"
      ],
      "abstract": "Recently text-to-video generation has made impressive progress in producing\nshort, high-quality clips, but evaluating long-form outputs remains a major\nchallenge especially when processing complex prompts. Existing benchmarks\nmostly rely on simplified prompts and focus on low-level metrics, overlooking\nfine-grained alignment with prompts and abstract dimensions such as narrative\ncoherence and thematic expression. To address these gaps, we propose\nLoCoT2V-Bench, a benchmark specifically designed for long video generation\n(LVG) under complex input conditions. Based on various real-world videos,\nLoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating\nelements like scene transitions and event dynamics. Moreover, it constructs a\nmulti-dimensional evaluation framework that includes our newly proposed metrics\nsuch as event-level alignment, fine-grained temporal consistency, content\nclarity, and the Human Expectation Realization Degree (HERD) that focuses on\nmore abstract attributes like narrative flow, emotional response, and character\ndevelopment. Using this framework, we conduct a comprehensive evaluation of\nnine representative LVG models, finding that while current methods perform well\non basic visual and temporal aspects, they struggle with inter-event\nconsistency, fine-grained alignment, and high-level thematic adherence, etc.\nOverall, LoCoT2V-Bench provides a comprehensive and reliable platform for\nevaluating long-form complex text-to-video generation and highlights critical\ndirections for future method improvement.",
      "pdf_url": "http://arxiv.org/pdf/2510.26412v1",
      "published": "2025-10-30T12:00:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26412v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders",
      "authors": [
        "Riccardo Renzulli",
        "Colas Lepoutre",
        "Enrico Cassano",
        "Marco Grangetto"
      ],
      "abstract": "Artificial intelligence in healthcare requires models that are accurate and\ninterpretable. We advance mechanistic interpretability in medical vision by\napplying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,\na vision-language model trained on chest radiographs and reports. To quantify\ninterpretability, we propose an evaluation framework that combines correlation\nmetrics, entropy analyzes, and automated neuron naming via the MedGEMMA\nfoundation model. Experiments on the CheXpert dataset show that MedSAE neurons\nachieve higher monosemanticity and interpretability than raw MedCLIP features.\nOur findings bridge high-performing medical AI and transparency, offering a\nscalable step toward clinically reliable representations.",
      "pdf_url": "http://arxiv.org/pdf/2510.26411v1",
      "published": "2025-10-30T11:58:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26411v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Human-in-the-loop Online Rejection Sampling for Robotic Manipulation",
      "authors": [
        "Guanxing Lu",
        "Rui Zhao",
        "Haitao Lin",
        "He Zhang",
        "Yansong Tang"
      ],
      "abstract": "Reinforcement learning (RL) is widely used to produce robust robotic\nmanipulation policies, but fine-tuning vision-language-action (VLA) models with\nRL can be unstable due to inaccurate value estimates and sparse supervision at\nintermediate steps. In contrast, imitation learning (IL) is easy to train but\noften underperforms due to its offline nature. In this paper, we propose\nHi-ORS, a simple yet effective post-training method that utilizes rejection\nsampling to achieve both training stability and high robustness. Hi-ORS\nstabilizes value estimation by filtering out negatively rewarded samples during\nonline fine-tuning, and adopts a reward-weighted supervised training objective\nto provide dense intermediate-step supervision. For systematic study, we\ndevelop an asynchronous inference-training framework that supports flexible\nonline human-in-the-loop corrections, which serve as explicit guidance for\nlearning error-recovery behaviors. Across three real-world tasks and two\nembodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich\nmanipulation in just 1.5 hours of real-world training, outperforming RL and IL\nbaselines by a substantial margin in both effectiveness and efficiency.\nNotably, the fine-tuned policy exhibits strong test-time scalability by\nreliably executing complex error-recovery behaviors to achieve better\nperformance.",
      "pdf_url": "http://arxiv.org/pdf/2510.26406v1",
      "published": "2025-10-30T11:53:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26406v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education",
      "authors": [
        "Vikrant Sahu",
        "Gagan Raj Gupta",
        "Raghav Borikar",
        "Nitin Mane"
      ],
      "abstract": "The rapid growth of programming education has outpaced traditional assessment\ntools, leaving faculty with limited means to provide meaningful, scalable\nfeedback. Conventional autograders, while efficient, act as black-box systems\nthat simply return pass/fail results, offering little insight into student\nthinking or learning needs.\n  Autograder+ is designed to shift autograding from a purely summative process\nto a formative learning experience. It introduces two key capabilities:\nautomated feedback generation using a fine-tuned Large Language Model, and\nvisualization of student code submissions to uncover learning patterns. The\nmodel is fine-tuned on curated student code and expert feedback to ensure\npedagogically aligned, context-aware guidance.\n  In evaluation across 600 student submissions from multiple programming tasks,\nthe system produced feedback with strong semantic alignment to instructor\ncomments. For visualization, contrastively learned code embeddings trained on\n1,000 annotated submissions enable grouping solutions into meaningful clusters\nbased on functionality and approach. The system also supports prompt-pooling,\nallowing instructors to guide feedback style through selected prompt templates.\n  By integrating AI-driven feedback, semantic clustering, and interactive\nvisualization, Autograder+ reduces instructor workload while supporting\ntargeted instruction and promoting stronger learning outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2510.26402v1",
      "published": "2025-10-30T11:41:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.26402v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    }
  ]
}