{
  "last_updated": "2025-10-11T00:44:52.872322",
  "papers": [
    {
      "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation",
      "authors": [
        "Rocktim Jyoti Das",
        "Harsh Singh",
        "Diana Turmakhan",
        "Muhammad Abdullah Sohail",
        "Mingfei Han",
        "Preslav Nakov",
        "Fabio Pizzati",
        "Ivan Laptev"
      ],
      "abstract": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page.",
      "pdf_url": "http://arxiv.org/pdf/2510.08572v1",
      "published": "2025-10-09T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08572v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos",
      "authors": [
        "Hongyu Li",
        "Lingfeng Sun",
        "Yafei Hu",
        "Duy Ta",
        "Jennifer Barry",
        "George Konidaris",
        "Jiahui Fu"
      ],
      "abstract": "Enabling robots to execute novel manipulation tasks zero-shot is a central\ngoal in robotics. Most existing methods assume in-distribution tasks or rely on\nfine-tuning with embodiment-matched data, limiting transfer across platforms.\nWe present NovaFlow, an autonomous manipulation framework that converts a task\ndescription into an actionable plan for a target robot without any\ndemonstrations. Given a task description, NovaFlow synthesizes a video using a\nvideo generation model and distills it into 3D actionable object flow using\noff-the-shelf perception modules. From the object flow, it computes relative\nposes for rigid objects and realizes them as robot actions via grasp proposals\nand trajectory optimization. For deformable objects, this flow serves as a\ntracking objective for model-based planning with a particle-based dynamics\nmodel. By decoupling task understanding from low-level control, NovaFlow\nnaturally transfers across embodiments. We validate on rigid, articulated, and\ndeformable object manipulation tasks using a table-top Franka arm and a Spot\nquadrupedal mobile robot, and achieve effective zero-shot execution without\ndemonstrations or embodiment-specific training. Project website:\nhttps://novaflow.lhy.xyz/.",
      "pdf_url": "http://arxiv.org/pdf/2510.08568v1",
      "published": "2025-10-09T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08568v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation",
      "authors": [
        "Qin Liu",
        "Jacob Dineen",
        "Yuxi Huang",
        "Sheng Zhang",
        "Hoifung Poon",
        "Ben Zhou",
        "Muhao Chen"
      ],
      "abstract": "Benchmarks are central to measuring the capabilities of large language models\nand guiding model development, yet widespread data leakage from pretraining\ncorpora undermines their validity. Models can match memorized content rather\nthan demonstrate true generalization, which inflates scores, distorts\ncross-model comparisons, and misrepresents progress. We introduce ArenaBencher,\na model-agnostic framework for automatic benchmark evolution that updates test\ncases while preserving comparability. Given an existing benchmark and a diverse\npool of models to be evaluated, ArenaBencher infers the core ability of each\ntest case, generates candidate question-answer pairs that preserve the original\nobjective, verifies correctness and intent with an LLM as a judge, and\naggregates feedback from multiple models to select candidates that expose\nshared weaknesses. The process runs iteratively with in-context demonstrations\nthat steer generation toward more challenging and diagnostic cases. We apply\nArenaBencher to math problem solving, commonsense reasoning, and safety domains\nand show that it produces verified, diverse, and fair updates that uncover new\nfailure modes, increase difficulty while preserving test objective alignment,\nand improve model separability. The framework provides a scalable path to\ncontinuously evolve benchmarks in step with the rapid progress of foundation\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2510.08569v1",
      "published": "2025-10-09T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08569v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning",
      "authors": [
        "Tajamul Ashraf",
        "Umair Nawaz",
        "Abdelrahman M. Shaker",
        "Rao Anwer",
        "Philip Torr",
        "Fahad Shahbaz Khan",
        "Salman Khan"
      ],
      "abstract": "Vision language models (VLMs) are increasingly deployed as controllers with\naccess to external tools for complex reasoning and decision-making, yet their\neffectiveness remains limited by the scarcity of high-quality multimodal\ntrajectories and the cost of manual annotation. We address this challenge with\na vision-centric agent tuning framework that automatically synthesizes\nmultimodal trajectories, generates step-wise preference pairs, and trains a VLM\ncontroller for robust tool-use reasoning. Our pipeline first constructs\nM-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified\ntrajectories, enabling imitation-based trajectory tuning. Building on this, we\ndevelop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool\nreasoning. To achieve finer alignment, we further introduce Pref-X, a set of\n11K automatically generated preference pairs, and optimize MATRIX on it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,\nMATRIX consistently surpasses both open- and closed-source VLMs, demonstrating\nscalable and effective multimodal tool use. Our data and code is avaliable at\nhttps://github.com/mbzuai-oryx/MATRIX.",
      "pdf_url": "http://arxiv.org/pdf/2510.08567v1",
      "published": "2025-10-09T17:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08567v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "How to Teach Large Multimodal Models New Skills",
      "authors": [
        "Zhen Zhu",
        "Yiming Gong",
        "Yao Xiao",
        "Yaoyao Liu",
        "Derek Hoiem"
      ],
      "abstract": "How can we teach large multimodal models (LMMs) new skills without erasing\nprior abilities? We study sequential fine-tuning on five target skills while\nmonitoring general ability on eight held-out benchmarks across three model\nfamilies. We observe that apparent \"forgetting\" on held-out tasks after narrow\nfine-tuning can partly recover at later stages. We trace this behavior to a\nmeasurable shift in the output token distribution, manifested through a simple\ncounting-bias probe that co-varies with forgetting. Guided by this picture, we\nidentify two simple, robust tuning recipes that learn strongly while limiting\ndrift: (i) updating only the self-attention projection layers, and (ii)\nupdating only the MLP Gate&Up while freezing the Down projection. Across models\nand tasks, these choices deliver strong target gains while largely preserving\nheld-out performance. Code is available at\nhttps://github.com/jessemelpolio/LMM_CL",
      "pdf_url": "http://arxiv.org/pdf/2510.08564v1",
      "published": "2025-10-09T17:59:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08564v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models",
      "authors": [
        "Andong Deng",
        "Taojiannan Yang",
        "Shoubin Yu",
        "Lincoln Spencer",
        "Mohit Bansal",
        "Chen Chen",
        "Serena Yeung-Levy",
        "Xiaohan Wang"
      ],
      "abstract": "Large Multimodal Models (LMMs) have achieved remarkable progress across\nvarious capabilities; however, complex video reasoning in the scientific domain\nremains a significant and challenging frontier. Current video benchmarks\npredominantly target general scenarios where perception/recognition is heavily\nrelied on, while with relatively simple reasoning tasks, leading to saturation\nand thus failing to effectively evaluate advanced multimodal cognitive skills.\nTo address this critical gap, we introduce SciVideoBench, a rigorous benchmark\nspecifically designed to assess advanced video reasoning in scientific\ncontexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice\nquestions derived from cutting-edge scientific experimental videos spanning\nover 25 specialized academic subjects and verified by a semi-automatic system.\nEach question demands sophisticated domain-specific knowledge, precise\nspatiotemporal perception, and intricate logical reasoning, effectively\nchallenging models' higher-order cognitive abilities. Our evaluation highlights\nsignificant performance deficits in state-of-the-art proprietary and\nopen-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating\nsubstantial room for advancement in video reasoning capabilities. Detailed\nanalyses of critical factors such as reasoning complexity and visual grounding\nprovide valuable insights and clear direction for future developments in LMMs,\ndriving the evolution of truly capable multimodal AI co-scientists. We hope\nSciVideoBench could fit the interests of the community and help to push the\nboundary of cutting-edge AI for border science.",
      "pdf_url": "http://arxiv.org/pdf/2510.08559v1",
      "published": "2025-10-09T17:59:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08559v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Agent Learning via Early Experience",
      "authors": [
        "Kai Zhang",
        "Xiangchao Chen",
        "Bo Liu",
        "Tianci Xue",
        "Zeyi Liao",
        "Zhihan Liu",
        "Xiyao Wang",
        "Yuting Ning",
        "Zhaorun Chen",
        "Xiaohan Fu",
        "Jian Xie",
        "Yuxuan Sun",
        "Boyu Gou",
        "Qi Qi",
        "Zihang Meng",
        "Jianwei Yang",
        "Ning Zhang",
        "Xian Li",
        "Ashish Shah",
        "Dat Huynh",
        "Hengduo Li",
        "Zi Yang",
        "Sara Cao",
        "Lawrence Jang",
        "Shuyan Zhou",
        "Jiacheng Zhu",
        "Huan Sun",
        "Jason Weston",
        "Yu Su",
        "Yifan Wu"
      ],
      "abstract": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data with reinforcement learning\nremains difficult in many environments, which either lack verifiable rewards\n(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn\ntool use). As a result, most current agents rely on supervised fine-tuning on\nexpert data, which is challenging to scale and generalizes poorly. This\nlimitation stems from the nature of expert demonstrations: they capture only a\nnarrow range of scenarios and expose the agent to limited environment\ndiversity. We address this limitation with a middle-ground paradigm we call\nearly experience: interaction data generated by the agent's own actions, where\nthe resulting future states serve as supervision without reward signals. Within\nthis paradigm we study two strategies of using such data: (1) Implicit world\nmodeling, which uses collected states to ground the policy in environment\ndynamics; and (2) Self-reflection, where the agent learns from its suboptimal\nactions to improve reasoning and decision-making. We evaluate across eight\ndiverse environments and multiple model families. Our approaches consistently\nimprove effectiveness and out-of-domain generalization, highlighting the value\nof early experience. Moreover, in environments with verifiable rewards, our\nresults provide promising signals that early experience offers a strong\nfoundation for subsequent reinforcement learning, positioning it as a practical\nbridge between imitation learning and fully experience-driven agents.",
      "pdf_url": "http://arxiv.org/pdf/2510.08558v1",
      "published": "2025-10-09T17:59:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08558v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation",
      "authors": [
        "Yunzhe Xu",
        "Yiyuan Pan",
        "Zhe Liu"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) requires agents to follow natural\nlanguage instructions through environments, with memory-persistent variants\ndemanding progressive improvement through accumulated experience. Existing\napproaches for memory-persistent VLN face critical limitations: they lack\neffective memory access mechanisms, instead relying on entire memory\nincorporation or fixed-horizon lookup, and predominantly store only\nenvironmental observations while neglecting navigation behavioral patterns that\nencode valuable decision-making strategies. We present Memoir, which employs\nimagination as a retrieval mechanism grounded by explicit memory: a world model\nimagines future navigation states as queries to selectively retrieve relevant\nenvironmental observations and behavioral histories. The approach comprises: 1)\na language-conditioned world model that imagines future states serving dual\npurposes: encoding experiences for storage and generating retrieval queries; 2)\nHybrid Viewpoint-Level Memory that anchors both observations and behavioral\npatterns to viewpoints, enabling hybrid retrieval; and 3) an\nexperience-augmented navigation model that integrates retrieved knowledge\nthrough specialized encoders. Extensive evaluation across diverse\nmemory-persistent VLN benchmarks with 10 distinctive testing scenarios\ndemonstrates Memoir's effectiveness: significant improvements across all\nscenarios, with 5.4% SPL gains on IR2R over the best memory-persistent\nbaseline, accompanied by 8.3x training speedup and 74% inference memory\nreduction. The results validate that predictive retrieval of both environmental\nand behavioral memories enables more effective navigation, with analysis\nindicating substantial headroom (73.3% vs 93.4% upper bound) for this\nimagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.",
      "pdf_url": "http://arxiv.org/pdf/2510.08553v1",
      "published": "2025-10-09T17:58:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08553v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "VideoNorms: Benchmarking Cultural Awareness of Video Language Models",
      "authors": [
        "Nikhil Reddy Varimalla",
        "Yunfei Xu",
        "Arkadiy Saakyan",
        "Meng Fan Wang",
        "Smaranda Muresan"
      ],
      "abstract": "As Video Large Language Models (VideoLLMs) are deployed globally, they\nrequire understanding of and grounding in the relevant cultural background. To\nproperly assess these models' cultural awareness, adequate benchmarks are\nneeded. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm)\npairs from US and Chinese cultures annotated with socio-cultural norms grounded\nin speech act theory, norm adherence and violations labels, and verbal and\nnon-verbal evidence. To build VideoNorms, we use a human-AI collaboration\nframework, where a teacher model using theoretically-grounded prompting\nprovides candidate annotations and a set of trained human experts validate and\ncorrect the annotations. We benchmark a variety of open-weight VideoLLMs on the\nnew dataset which highlight several common trends: 1) models performs worse on\nnorm violation than adherence; 2) models perform worse w.r.t Chinese culture\ncompared to the US culture; 3) models have more difficulty in providing\nnon-verbal evidence compared to verbal for the norm adhere/violation label and\nstruggle to identify the exact norm corresponding to a speech-act; and 4)\nunlike humans, models perform worse in formal, non-humorous contexts. Our\nfindings emphasize the need for culturally-grounded video language model\ntraining - a gap our benchmark and framework begin to address.",
      "pdf_url": "http://arxiv.org/pdf/2510.08543v1",
      "published": "2025-10-09T17:54:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08543v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "On the optimization dynamics of RLVR: Gradient gap and step size thresholds",
      "authors": [
        "Joe Suk",
        "Yaqi Duan"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple\nbinary feedback to post-train large language models, has shown significant\nempirical success. However, a principled understanding of why it works has been\nlacking. This paper builds a theoretical foundation for RLVR by analyzing its\ntraining process at both the full-response (trajectory) and token levels.\nCentral to our analysis is a quantity called the Gradient Gap, which formalizes\nthe direction of improvement from low-reward to high-reward regions of the\nresponse space. We prove that convergence critically depends on aligning the\nupdate direction with this Gradient Gap. Moreover, we derive a sharp step-size\nthreshold based on the magnitude of the Gradient Gap: below it, learning\nconverges, whereas above it, performance collapses. Our theory further predicts\nhow the critical step size must scale with response length and the success\nrate, thereby explaining why practical heuristics such as length normalization\nimprove stability and showing that, with a fixed learning rate, the success\nrate can stagnate strictly below $100\\%$. We validate these predictions through\ncontrolled bandit simulations and LLM experiments, including training\nQwen2.5-7B with GRPO.",
      "pdf_url": "http://arxiv.org/pdf/2510.08539v1",
      "published": "2025-10-09T17:53:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08539v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "title": "Kontinuous Kontext: Continuous Strength Control for Instruction-based Image Editing",
      "authors": [
        "Rishubh Parihar",
        "Or Patashnik",
        "Daniil Ostashev",
        "R. Venkatesh Babu",
        "Daniel Cohen-Or",
        "Kuan-Chieh Wang"
      ],
      "abstract": "Instruction-based image editing offers a powerful and intuitive way to\nmanipulate images through natural language. Yet, relying solely on text\ninstructions limits fine-grained control over the extent of edits. We introduce\nKontinuous Kontext, an instruction-driven editing model that provides a new\ndimension of control over edit strength, enabling users to adjust edits\ngradually from no change to a fully realized result in a smooth and continuous\nmanner. Kontinuous Kontext extends a state-of-the-art image editing model to\naccept an additional input, a scalar edit strength which is then paired with\nthe edit instruction, enabling explicit control over the extent of the edit. To\ninject this scalar information, we train a lightweight projector network that\nmaps the input scalar and the edit instruction to coefficients in the model's\nmodulation space. For training our model, we synthesize a diverse dataset of\nimage-edit-instruction-strength quadruplets using existing generative models,\nfollowed by a filtering stage to ensure quality and consistency. Kontinuous\nKontext provides a unified approach for fine-grained control over edit strength\nfor instruction driven editing from subtle to strong across diverse operations\nsuch as stylization, attribute, material, background, and shape changes,\nwithout requiring attribute-specific training.",
      "pdf_url": "http://arxiv.org/pdf/2510.08532v1",
      "published": "2025-10-09T17:51:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08532v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models",
      "authors": [
        "Hongxing Li",
        "Dingming Li",
        "Zixuan Wang",
        "Yuchen Yan",
        "Hang Wu",
        "Wenqi Zhang",
        "Yongliang Shen",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "abstract": "Spatial reasoning remains a fundamental challenge for Vision-Language Models\n(VLMs), with current approaches struggling to achieve robust performance\ndespite recent advances. We identify that this limitation stems from a critical\ngap: existing methods attempt to learn spatial reasoning directly without\nestablishing the hierarchical foundations of perception and understanding. To\naddress this challenge, we present a comprehensive methodology for building\nspatial intelligence progressively. We introduce SpatialLadder-26k, a\nmultimodal dataset containing 26,610 samples spanning object localization,\nsingle image, multi-view, and video spatial reasoning tasks, constructed\nthrough a standardized pipeline that ensures systematic coverage across\nmodalities. Building on this dataset, we design a three-stage progressive\ntraining framework that (1) establishes spatial perception through object\nlocalization, (2) develops spatial understanding through multi-dimensional\nspatial tasks, and (3) strengthens complex reasoning via reinforcement learning\nwith verifiable rewards. This approach yields SpatialLadder, a 3B-parameter\nmodel that achieves state-of-the-art performance on spatial reasoning\nbenchmarks, with 23.4% average improvement over the base model, surpassing\nGPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains\nstrong generalization with 7.2% improvement on out-of-domain benchmarks,\ndemonstrating that progressive training from perception to reasoning is\nessential for robust spatial intelligence.",
      "pdf_url": "http://arxiv.org/pdf/2510.08531v1",
      "published": "2025-10-09T17:50:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08531v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
      "authors": [
        "Xiangyuan Xue",
        "Yifan Zhou",
        "Guibin Zhang",
        "Zaibin Zhang",
        "Yijiang Li",
        "Chen Zhang",
        "Zhenfei Yin",
        "Philip Torr",
        "Wanli Ouyang",
        "Lei Bai"
      ],
      "abstract": "Self-evolution is a central research topic in enabling large language model\n(LLM)-based agents to continually improve their capabilities after pretraining.\nRecent research has witnessed a transition from reinforcement learning\n(RL)-free to RL-based methods. Current RL-based methods either rely on dense\nexternal reward signals or extract intrinsic reward signals from LLMs\nthemselves. However, these approaches diverge from the self-evolution\nmechanisms observed in human intelligence, where individuals learn and improve\nthrough mutual discussion and collaboration. In this work, we introduce\nCo-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents\nto improve autonomously by learning from inter-agent interactions without\nexternal supervision. CoMAS generates intrinsic rewards from rich discussion\ndynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and\noptimizes each agent's policy through RL, thereby enabling decentralized and\nscalable co-evolution. Experimental results demonstrate that CoMAS consistently\noutperforms untrained agents and achieves state-of-the-art performance across\nmost evaluation settings. Ablation studies confirm the necessity of\ninteraction-based reward signals and reveal promising scalability as the number\nand diversity of agents increase. These findings establish CoMAS as a novel and\neffective paradigm for self-evolution in LLM-based agents.",
      "pdf_url": "http://arxiv.org/pdf/2510.08529v1",
      "published": "2025-10-09T17:50:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08529v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "FlowSearch: Advancing deep research with dynamic structured knowledge flow",
      "authors": [
        "Yusong Hu",
        "Runmin Ma",
        "Yue Fan",
        "Jinxin Shi",
        "Zongsheng Cao",
        "Yuhao Zhou",
        "Jiakang Yuan",
        "Xiangchao Yan",
        "Wenlong Zhang",
        "Lei Bai",
        "Bo Zhang"
      ],
      "abstract": "Deep research is an inherently challenging task that demands both breadth and\ndepth of thinking. It involves navigating diverse knowledge spaces and\nreasoning over complex, multi-step dependencies, which presents substantial\nchallenges for agentic systems. To address this, we propose FlowSearch, a\nmulti-agent framework that actively constructs and evolves a dynamic structured\nknowledge flow to drive subtask execution and reasoning. FlowSearch is capable\nof strategically planning and expanding the knowledge flow to enable parallel\nexploration and hierarchical task decomposition, while also adjusting the\nknowledge flow in real time based on feedback from intermediate reasoning\noutcomes and insights. FlowSearch achieves state-of-the-art performance on both\ngeneral and scientific benchmarks, including GAIA, HLE, GPQA and TRQA,\ndemonstrating its effectiveness in multi-disciplinary research scenarios and\nits potential to advance scientific discovery. The code is available at\nhttps://github.com/Alpha-Innovator/InternAgent.",
      "pdf_url": "http://arxiv.org/pdf/2510.08521v1",
      "published": "2025-10-09T17:48:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08521v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CaRT: Teaching LLM Agents to Know When They Know Enough",
      "authors": [
        "Grace Liu",
        "Yuxiao Qu",
        "Jeff Schneider",
        "Aarti Singh",
        "Aviral Kumar"
      ],
      "abstract": "Many tasks require learned models to strategically gather relevant\ninformation over multiple rounds of interaction before actually acting on a\ntask. Strategic information gathering requires models to know not only how to\neffectively acquire information, but also when to stop gathering information\nand make a decision, in order to avoid overthinking or getting derailed when\nacting. In this paper, we formalize this problem and introduce Counterfactuals\nand Reasoning for Termination (CaRT), an approach for teaching LLMs when to\nstop seeking information. To appropriately learn when to terminate, CaRT\nfine-tunes LLMs using counterfactual pairs of trajectories, one where\ntermination is appropriate and a minimally modified version of the same\ntrajectory where it is not. It trains the LLM to explain the rationale for the\ntermination decision in either case via verbal reasoning, and imbues this\ncapability into the base LLM via fine-tuning. We instantiate CaRT in two\ndomains: interactive medical diagnosis and math problem solving. In both\ndomains, we find that CaRT improves the efficiency of information gathering and\ntask success rate compared to other fine-tuning methods.",
      "pdf_url": "http://arxiv.org/pdf/2510.08517v1",
      "published": "2025-10-09T17:46:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08517v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents",
      "authors": [
        "Shangheng Du",
        "Xiangchao Yan",
        "Dengyang Jiang",
        "Jiakang Yuan",
        "Yusong Hu",
        "Xin Li",
        "Liang He",
        "Bo Zhang",
        "Lei Bai"
      ],
      "abstract": "Large language models (LLMs) have shown impressive performance in general\nprogramming tasks. However, in Machine Learning Engineering (MLE) scenarios\nsuch as AutoML and Kaggle competitions, achieving high performance depends\nheavily on expert intervention and repeated adjustments rather than simply\ngenerating correct code. When applied directly to these tasks, LLMs often lack\nfine-grained domain priors, and existing MLE approaches that use linear or\ntree-structured searches limit knowledge transfer to adjacent hierarchical\nlinks. As a result, they cannot leverage past full trajectories or share\ninformation across branches, limiting self-evolving ability and search space\ndiversity. To address these limitations, we introduce AutoMLGen, an LLM-based\ncoding agent that integrates a domain knowledge base for high-quality prior\nguidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS\nretains the tree-guided exploration of MCTS while embedding a graph structure\ninto the expansion stage to enable dynamic path reorganization, historical\ntrajectory reuse, and multi-solution fusion to support both self-evolution and\ncollaborative learning. Combined with fine-grained operator sets, this design\nimproves stability and accelerates convergence. Evaluation on the MLE-Bench\nshows that AutoMLGen achieves state-of-the-art performance in numerous\ndimensions, such as the average medal rate and the valid submission rate, under\na 12-hour budget (half the standard runtime). The code is available at\nhttps://github.com/Alpha-Innovator/InternAgent.",
      "pdf_url": "http://arxiv.org/pdf/2510.08511v1",
      "published": "2025-10-09T17:45:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08511v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models",
      "authors": [
        "Jiayun Luo",
        "Wan-Cyuan Fan",
        "Lyuyang Wang",
        "Xiangteng He",
        "Tanzila Rahman",
        "Purang Abolmaesumi",
        "Leonid Sigal"
      ],
      "abstract": "Large Vision Language Models (LVLMs) have recently emerged as powerful\narchitectures capable of understanding and reasoning over both visual and\ntextual information. These models typically rely on two key components: a\nVision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual\ncontent into a sequence of image tokens and serves as the perceptual front-end\n-- the eyes of the model. In contrast, the LLM interprets these tokens to\nperform high-level reasoning, generates responses, and functions as the\ncognitive core -- the brain of the model. However, it remains unclear which\nvisual tokens contribute most significantly to understanding and reasoning, and\nhow effectively these signals are propagated from ViT to the LLM. While most\nexisting works have focused on identifying attention sinks, low-semantic tokens\nreceiving disproportionately high attention, within the LLM, we shift the focus\nto the vision encoder by identifying a class of high-norm visual tokens from\nViT, referred to as ViT attention sinks -- a problem that has been rarely\nstudied but is indeed very important for LVLMs. Our findings show that these\nViT sinks encapsulate high-level semantic concepts from images, allowing the\nLLM to perform more effective understanding and reasoning. Despite their\nimportance, these sink tokens are often overlooked in existing LVLM\narchitectures. To explore their contribution, we present both qualitative and\nquantitative analyses of the information embedded in these sink tokens. We also\npropose both training-free and training-based approaches to better leverage how\nthis information is interpreted by the LLM, and to what extent. By explicitly\nutilizing these tokens, we demonstrate substantial improvements across a range\nof LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT\nattention sinks in enhancing visual reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2510.08510v1",
      "published": "2025-10-09T17:44:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08510v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "AI-Driven Radiology Report Generation for Traumatic Brain Injuries",
      "authors": [
        "Riadh Bouslimi",
        "Houda Trabelsi",
        "Wahiba Ben Abdssalem Karaa",
        "Hana Hedhli"
      ],
      "abstract": "Traumatic brain injuries present significant diagnostic challenges in\nemergency medicine, where the timely interpretation of medical images is\ncrucial for patient outcomes. In this paper, we propose a novel AI-based\napproach for automatic radiology report generation tailored to cranial trauma\ncases. Our model integrates an AC-BiFPN with a Transformer architecture to\ncapture and process complex medical imaging data such as CT and MRI scans. The\nAC-BiFPN extracts multi-scale features, enabling the detection of intricate\nanomalies like intracranial hemorrhages, while the Transformer generates\ncoherent, contextually relevant diagnostic reports by modeling long-range\ndependencies. We evaluate the performance of our model on the RSNA Intracranial\nHemorrhage Detection dataset, where it outperforms traditional CNN-based models\nin both diagnostic accuracy and report generation. This solution not only\nsupports radiologists in high-pressure environments but also provides a\npowerful educational tool for trainee physicians, offering real-time feedback\nand enhancing their learning experience. Our findings demonstrate the potential\nof combining advanced feature extraction with transformer-based text generation\nto improve clinical decision-making in the diagnosis of traumatic brain\ninjuries.",
      "pdf_url": "http://arxiv.org/pdf/2510.08498v1",
      "published": "2025-10-09T17:39:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08498v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "68T07, 68U10",
        "I.2.10; I.2.7; I.4.5"
      ]
    },
    {
      "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
      "authors": [
        "Shangqing Tu",
        "Yaxuan Li",
        "Yushi Bai",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning\ncapabilities in large language models (LLMs) by generating multiple\nChain-of-Thought (CoT) traces simultaneously. However, this approach introduces\nsignificant computational inefficiency due to inter-trace redundancy -- our\nanalysis reveals that over 80% of parallel reasoning traces yield identical\nfinal answers, representing substantial wasted computation. To address this\ncritical efficiency bottleneck, we propose DeepPrune, a novel framework that\nenables efficient parallel scaling through dynamic pruning. Our method features\na specialized judge model trained with focal loss and oversampling techniques\nto accurately predict answer equivalence from partial reasoning traces which\nrealizes 0.87 AUROC on equivalence prediction, combined with an online greedy\nclustering algorithm that dynamically prunes redundant paths while preserving\nanswer diversity. Comprehensive evaluations across three challenging benchmarks\n(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that\nDeepPrune achieves remarkable token reduction by over 80% compared to\nconventional consensus sampling on most cases, while maintaining competitive\naccuracy within 3 percentage points. Our work establishes a new standard for\nefficient parallel reasoning, making high-performance reasoning more efficient.\nOur code and data are here: https://deepprune.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2510.08483v1",
      "published": "2025-10-09T17:24:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08483v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling",
      "authors": [
        "Bianca-Mihaela Ganescu",
        "Suchir Salhan",
        "Andrew Caines",
        "Paula Buttery"
      ],
      "abstract": "Training vision-language models on cognitively-plausible amounts of data\nrequires rethinking how models integrate multimodal information. Within the\nconstraints of the Vision track for the BabyLM Challenge 2025, we propose a\nlightweight decoder-based architecture with (1) token-wise dynamic gating for\nadaptive fusion of linguistic and visual cues, (2) feature modulation and\nchannel attention to maximise the utility of limited visual information and (3)\nauxiliary contrastive objectives for visual grounding. Evaluation on five\nbenchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows\ncompetitive or superior performance to multimodal baselines. More notably, our\ndynamic gate discovers interpretable patterns without explicit supervision,\nfavouring visual cues for content words and linguistic cues for function words.\nWhile we identify limitations in the Challenge constraints, such as the\ninformation bottleneck created by global image embeddings and training\ninstability from the dataset split, our findings establish dynamic gating as a\npowerful tool for efficient multimodal learning, offering both interpretability\nand performance even under severe constraints.",
      "pdf_url": "http://arxiv.org/pdf/2510.08470v1",
      "published": "2025-10-09T17:10:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08470v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Platform-Agnostic Modular Architecture for Quantum Benchmarking",
      "authors": [
        "Neer Patel",
        "Anish Giri",
        "Hrushikesh Pramod Patil",
        "Noah Siekierski",
        "Avimita Chatterjee",
        "Sonika Johri",
        "Timothy Proctor",
        "Thomas Lubinski",
        "Siyuan Niu"
      ],
      "abstract": "We present a platform-agnostic modular architecture that addresses the\nincreasingly fragmented landscape of quantum computing benchmarking by\ndecoupling problem generation, circuit execution, and results analysis into\nindependent, interoperable components. Supporting over 20 benchmark variants\nranging from simple algorithmic tests like Bernstein-Vazirani to complex\nHamiltonian simulation with observable calculations, the system integrates with\nmultiple circuit generation APIs (Qiskit, CUDA-Q, Cirq) and enables diverse\nworkflows. We validate the architecture through successful integration with\nSandia's $\\textit{pyGSTi}$ for advanced circuit analysis and CUDA-Q for\nmulti-GPU HPC simulations. Extensibility of the system is demonstrated by\nimplementing dynamic circuit variants of existing benchmarks and a new quantum\nreinforcement learning benchmark, which become readily available across\nmultiple execution and analysis modes. Our primary contribution is identifying\nand formalizing modular interfaces that enable interoperability between\nincompatible benchmarking frameworks, demonstrating that standardized\ninterfaces reduce ecosystem fragmentation while preserving optimization\nflexibility. This architecture has been developed as a key enhancement to the\ncontinually evolving QED-C Application-Oriented Performance Benchmarks for\nQuantum Computing suite.",
      "pdf_url": "http://arxiv.org/pdf/2510.08469v1",
      "published": "2025-10-09T17:09:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08469v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Integral Signatures of Activation Functions: A 9-Dimensional Taxonomy and Stability Theory for Deep Learning",
      "authors": [
        "Ankur Mali",
        "Lawrence Hall",
        "Jake Williams",
        "Gordon Richards"
      ],
      "abstract": "Activation functions govern the expressivity and stability of neural\nnetworks, yet existing comparisons remain largely heuristic. We propose a\nrigorous framework for their classification via a nine-dimensional integral\nsignature S_sigma(phi), combining Gaussian propagation statistics (m1, g1, g2,\nm2, eta), asymptotic slopes (alpha_plus, alpha_minus), and regularity measures\n(TV(phi'), C(phi)). This taxonomy establishes well-posedness, affine\nreparameterization laws with bias, and closure under bounded slope variation.\nDynamical analysis yields Lyapunov theorems with explicit descent constants and\nidentifies variance stability regions through (m2', g2). From a kernel\nperspective, we derive dimension-free Hessian bounds and connect smoothness to\nbounded variation of phi'. Applying the framework, we classify eight standard\nactivations (ReLU, leaky-ReLU, tanh, sigmoid, Swish, GELU, Mish, TeLU), proving\nsharp distinctions between saturating, linear-growth, and smooth families.\nNumerical Gauss-Hermite and Monte Carlo validation confirms theoretical\npredictions. Our framework provides principled design guidance, moving\nactivation choice from trial-and-error to provable stability and kernel\nconditioning.",
      "pdf_url": "http://arxiv.org/pdf/2510.08456v1",
      "published": "2025-10-09T17:03:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08456v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity",
      "authors": [
        "Hugh Blayney",
        "Álvaro Arroyo",
        "Xiaowen Dong",
        "Michael M. Bronstein"
      ],
      "abstract": "Graph Neural Networks (GNNs) leverage the graph structure to transmit\ninformation between nodes, typically through the message-passing mechanism.\nWhile these models have found a wide variety of applications, they are known to\nsuffer from over-squashing, where information from a large receptive field of\nnode representations is collapsed into a single fixed sized vector, resulting\nin an information bottleneck. In this paper, we re-examine the over-squashing\nphenomenon through the lens of model storage and retrieval capacity, which we\ndefine as the amount of information that can be stored in a node's\nrepresentation for later use. We study some of the limitations of existing\ntasks used to measure over-squashing and introduce a new synthetic task to\ndemonstrate that an information bottleneck can saturate this capacity.\nFurthermore, we adapt ideas from the sequence modeling literature on\nassociative memories, fast weight programmers, and the xLSTM model to develop a\nnovel GNN architecture with improved capacity. We demonstrate strong\nperformance of this architecture both on our capacity synthetic task, as well\nas a range of real-world graph benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2510.08450v1",
      "published": "2025-10-09T16:58:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08450v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Synthetic Series-Symbol Data Generation for Time Series Foundation Models",
      "authors": [
        "Wenxuan Wang",
        "Kai Wu",
        "Yujian Betterest Li",
        "Dan Wang",
        "Xiaoyu Zhang"
      ],
      "abstract": "Foundation models for time series analysis (TSA) have attracted significant\nattention. However, challenges such as training data scarcity and imbalance\ncontinue to hinder their development. Inspired by complex dynamic system\ntheories, we design a series-symbol data generation mechanism, enabling the\nunrestricted creation of high-quality time series data paired with\ncorresponding symbolic expressions. To leverage series-symbol data pairs with\nstrong correlations, we develop \\texttt{SymTime}, a pre-trained foundation\nmodel for enhancing time series representation using symbolic information.\n\\texttt{SymTime} demonstrates competitive performance across five major TSA\ntasks when fine-tunes with downstream tasks, rivaling foundation models\npre-trained on real-world datasets. This approach underscores the potential of\nseries-symbol data generation and pretraining mechanisms in overcoming data\nscarcity and enhancing task performance. The code is available at\nhttps://github.com/wwhenxuan/SymTime.",
      "pdf_url": "http://arxiv.org/pdf/2510.08445v1",
      "published": "2025-10-09T16:54:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08445v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning",
      "authors": [
        "Andrew Lee",
        "Ian Chuang",
        "Dechen Gao",
        "Kai Fukazawa",
        "Iman Soltani"
      ],
      "abstract": "Visual Reinforcement Learning (RL) agents must learn to act based on\nhigh-dimensional image data where only a small fraction of the pixels is\ntask-relevant. This forces agents to waste exploration and computational\nresources on irrelevant features, leading to sample-inefficient and unstable\nlearning. To address this, inspired by human visual foveation, we introduce\nGaze on the Prize. This framework augments visual RL with a learnable foveal\nattention mechanism (Gaze), guided by a self-supervised signal derived from the\nagent's experience pursuing higher returns (the Prize). Our key insight is that\nreturn differences reveal what matters most: If two similar representations\nproduce different outcomes, their distinguishing features are likely\ntask-relevant, and the gaze should focus on them accordingly. This is realized\nthrough return-guided contrastive learning that trains the attention to\ndistinguish between the features relevant to success and failure. We group\nsimilar visual representations into positives and negatives based on their\nreturn differences and use the resulting labels to construct contrastive\ntriplets. These triplets provide the training signal that teaches the attention\nmechanism to produce distinguishable representations for states associated with\ndifferent outcomes. Our method achieves up to 2.4x improvement in sample\nefficiency and can solve tasks that the baseline fails to learn, demonstrated\nacross a suite of manipulation tasks from the ManiSkill3 benchmark, all without\nmodifying the underlying algorithm or hyperparameters.",
      "pdf_url": "http://arxiv.org/pdf/2510.08442v1",
      "published": "2025-10-09T16:54:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08442v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning",
      "authors": [
        "Cheng Qian",
        "Zuxin Liu",
        "Shirley Kokane",
        "Akshara Prabhakar",
        "Jielin Qiu",
        "Haolin Chen",
        "Zhiwei Liu",
        "Heng Ji",
        "Weiran Yao",
        "Shelby Heinecke",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang"
      ],
      "abstract": "Modern LLM deployments confront a widening cost-performance spectrum: premium\nmodels deliver strong reasoning but are expensive, while lightweight models are\neconomical yet brittle on complex tasks. Static escalation rules and keyword\nheuristics under-utilize this spectrum and fail to adapt across task types. We\npresent xRouter, a tool-calling-based routing system in which a learned router\ncan either answer directly or invoke one or more external models. The router is\ntrained end-to-end with reinforcement learning using an explicit, cost-aware\nreward that encodes cost-performance trade-offs, eliminating the need for\nhand-engineered routing rules. Our implementation encompasses the full\nreinforcement learning framework, including reward and cost accounting, as well\nas the deployment and evaluation pipelines. Across diverse benchmarks, xRouter\nachieves strong cost-performance trade-offs (e.g., substantial cost reductions\nat comparable task completion rates), and provides empirical insights into what\nreliably helps learned routing and what does not, ranging from model\ntrainability to the difficulty of eliciting sophisticated orchestration\nbehaviors in small open models. We hope these findings and our open\nimplementation will serve as a practical substrate for advancing learned,\ncost-aware LLM orchestration.",
      "pdf_url": "http://arxiv.org/pdf/2510.08439v1",
      "published": "2025-10-09T16:52:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08439v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing",
      "authors": [
        "Stella C. Dong",
        "James R. Finlay"
      ],
      "abstract": "Reinsurance treaty pricing must satisfy stringent regulatory standards, yet\ncurrent quoting practices remain opaque and difficult to audit. We introduce\nClauseLens, a clause-grounded reinforcement learning framework that produces\ntransparent, regulation-compliant, and risk-aware treaty quotes.\n  ClauseLens models the quoting task as a Risk-Aware Constrained Markov\nDecision Process (RA-CMDP). Statutory and policy clauses are retrieved from\nlegal and underwriting corpora, embedded into the agent's observations, and\nused both to constrain feasible actions and to generate clause-grounded natural\nlanguage justifications.\n  Evaluated in a multi-agent treaty simulator calibrated to industry data,\nClauseLens reduces solvency violations by 51%, improves tail-risk performance\nby 27.9% (CVaR_0.10), and achieves 88.2% accuracy in clause-grounded\nexplanations with retrieval precision of 87.4% and recall of 91.1%.\n  These findings demonstrate that embedding legal context into both decision\nand explanation pathways yields interpretable, auditable, and\nregulation-aligned quoting behavior consistent with Solvency II, NAIC RBC, and\nthe EU AI Act.",
      "pdf_url": "http://arxiv.org/pdf/2510.08429v1",
      "published": "2025-10-09T16:43:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08429v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "68T05, 91G70",
        "I.2.6; I.2.7; J.1"
      ]
    },
    {
      "title": "Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors",
      "authors": [
        "David Madras",
        "Joshua Safyan",
        "Qiuyi",
        "Zhang"
      ],
      "abstract": "Many prompt engineering techniques have been successful in practice, even\nwhen optimizing over a large prompt space with with a small amount of\ntask-specific data. Recent work has partially explained this success by showing\ngeneralization bounds which apply PAC-Bayes theory to the discrete prompt\nspace, but they are non-vacuous only in data-rich scenarios. We argue that such\nwidespread success can be more fully explained through more carefully\nconsidering data- or distribution-dependent perplexity, which acts as an\neffective prior and steers the optimization towards prompts that are more\n``natural'' for the task at hand. We derive novel generalization bounds that\nare non-vacuous for data-scarce prompt optimization via more useful priors,\nformally analyzing how perplexity regularization tightens these bounds by\nlimiting exploration. Empirically, we explore both the bounds' effectiveness\nand the practical benefits of perplexity regularization in improving prompt\ngeneralization.",
      "pdf_url": "http://arxiv.org/pdf/2510.08413v1",
      "published": "2025-10-09T16:32:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08413v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT",
      "authors": [
        "Noor Ul Zain",
        "Mohsin Raza",
        "Ahsan Adeel"
      ],
      "abstract": "We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two\nheads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$\nis the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2\n(124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two\nepochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude\ngreater training efficiency on 10M tokens, demonstrating highly sample\nefficient pretraining. Using the BabyLM challenge evaluation pipeline across\ncomplex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning\nperformance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out\nof 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out\nof 7 metrics in both cases. These results suggest the need to rethink\nprevailing deep learning paradigms and associated scaling laws.",
      "pdf_url": "http://arxiv.org/pdf/2510.08404v1",
      "published": "2025-10-09T16:22:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08404v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts",
      "authors": [
        "Heming Zou",
        "Yunliang Zang",
        "Wutong Xu",
        "Yao Zhu",
        "Xiangyang Ji"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning\nmethod for foundation models, but it suffers from parameter interference,\nresulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based\nLoRA variants show promise in mitigating intra-task correlations in single-task\ninstruction tuning, they introduce additional router parameters and remain\nineffective in multi-task model merging where inter-task interference arises.\nInspired by the fly olfactory circuit, we propose FlyLoRA, an implicit\nMoE-based LoRA variant that introduces: (1) rank-wise expert activation in the\nup-projection matrix, and (2) an implicit router that unifies expert routing\nand down-projection, where a frozen sparse random projection matrix replaces\nthe traditional dense trainable version. This design resolves the trade-off\nbetween intra-task decorrelation and computational efficiency by eliminating\nthe need for an explicit router, while inherently mitigating inter-task\ninterference due to the orthogonality property of random matrices. Extensive\nexperiments across four domains -- general knowledge understanding, scientific\nquestion answering, mathematical reasoning, and code generation -- demonstrate\nconsistent performance improvements over existing methods. Beyond empirical\ngains, FlyLoRA highlights how biological structures can inspire innovations in\nAI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.",
      "pdf_url": "http://arxiv.org/pdf/2510.08396v1",
      "published": "2025-10-09T16:17:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08396v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Revisiting Hallucination Detection with Effective Rank-based Uncertainty",
      "authors": [
        "Rui Wang",
        "Zeming Wei",
        "Guanzhang Yue",
        "Meng Sun"
      ],
      "abstract": "Detecting hallucinations in large language models (LLMs) remains a\nfundamental challenge for their trustworthy deployment. Going beyond basic\nuncertainty-driven hallucination detection frameworks, we propose a simple yet\npowerful method that quantifies uncertainty by measuring the effective rank of\nhidden states derived from multiple model outputs and different layers.\nGrounded in the spectral analysis of representations, our approach provides\ninterpretable insights into the model's internal reasoning process through\nsemantic variations, while requiring no extra knowledge or additional modules,\nthus offering a combination of theoretical elegance and practical efficiency.\nMeanwhile, we theoretically demonstrate the necessity of quantifying\nuncertainty both internally (representations of a single response) and\nexternally (different responses), providing a justification for using\nrepresentations among different layers and responses from LLMs to detect\nhallucinations. Extensive experiments demonstrate that our method effectively\ndetects hallucinations and generalizes robustly across various scenarios,\ncontributing to a new paradigm of hallucination detection for LLM truthfulness.",
      "pdf_url": "http://arxiv.org/pdf/2510.08389v1",
      "published": "2025-10-09T16:12:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08389v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning",
      "authors": [
        "Sofia Kirsanova",
        "Yao-Yi Chiang",
        "Weiwei Duan"
      ],
      "abstract": "Historical map legends are critical for interpreting cartographic symbols.\nHowever, their inconsistent layouts and unstructured formats make automatic\nextraction challenging. Prior work focuses primarily on segmentation or general\noptical character recognition (OCR), with few methods effectively matching\nlegend symbols to their corresponding descriptions in a structured manner. We\npresent a method that combines LayoutLMv3 for layout detection with GPT-4o\nusing in-context learning to detect and link legend items and their\ndescriptions via bounding box predictions. Our experiments show that GPT-4 with\nstructured JSON prompts outperforms the baseline, achieving 88% F-1 and 85%\nIoU, and reveal how prompt design, example counts, and layout alignment affect\nperformance. This approach supports scalable, layout-aware legend parsing and\nimproves the indexing and searchability of historical maps across various\nvisual styles.",
      "pdf_url": "http://arxiv.org/pdf/2510.08385v1",
      "published": "2025-10-09T16:08:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08385v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "H.2.8; H.3.3; I.2.10; I.4.8"
      ]
    },
    {
      "title": "QAgent: A modular Search Agent with Interactive Query Understanding",
      "authors": [
        "Yi Jiang",
        "Lei Shen",
        "Lujie Niu",
        "Sendong Zhao",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "abstract": "Large language models (LLMs) excel at natural language tasks but are limited\nby their static parametric knowledge, especially in knowledge-intensive task.\nRetrieval-augmented generation (RAG) mitigates this by integrating external\ninformation. However, (1) traditional RAG struggles with complex query\nunderstanding, and (2) even search agents trained with reinforcement learning\n(RL), despite their promise, still face generalization and deployment\nchallenges. To address these limitations, we propose QAgent, a unified agentic\nRAG framework that employs a search agent for adaptive retrieval. This agent\noptimizes its understanding of the query through interactive reasoning and\nretrieval. To facilitate real-world application, we focus on modular search\nagent for query understanding that are plug-and-play in complex systems.\nSecifically, the agent follows a multi-step decision process trained with RL to\nmaximize retrieval quality and support accurate downstream answers. We further\nanalyze the strengths and weaknesses of end-to-end RL and propose a strategy\nthat focuses on effective retrieval, thereby enhancing generalization in LLM\napplications. Experiments show QAgent excels at QA and serves as a\nplug-and-play module for real-world deployment.",
      "pdf_url": "http://arxiv.org/pdf/2510.08383v1",
      "published": "2025-10-09T16:08:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08383v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Airy: Reading Robot Intent through Height and Sky",
      "authors": [
        "Baoyang Chen",
        "Xian Xu",
        "Huamin Qu"
      ],
      "abstract": "As industrial robots move into shared human spaces, their opaque decision\nmaking threatens safety, trust, and public oversight. This artwork, Airy, asks\nwhether complex multi agent AI can become intuitively understandable by staging\na competition between two reinforcement trained robot arms that snap a bedsheet\nskyward. Building on three design principles, competition as a clear metric\n(who lifts higher), embodied familiarity (audiences recognize fabric snapping),\nand sensor to sense mapping (robot cooperation or rivalry shown through forest\nand weather projections), the installation gives viewers a visceral way to read\nmachine intent. Observations from five international exhibitions indicate that\naudiences consistently read the robots' strategies, conflict, and cooperation\nin real time, with emotional reactions that mirror the system's internal state.\nThe project shows how sensory metaphors can turn a black box into a public\ninterface.",
      "pdf_url": "http://arxiv.org/pdf/2510.08381v1",
      "published": "2025-10-09T16:07:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08381v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception",
      "authors": [
        "Nikos Theodoridis",
        "Tim Brophy",
        "Reenu Mohandas",
        "Ganesh Sistu",
        "Fiachra Collins",
        "Anthony Scanlan",
        "Ciaran Eising"
      ],
      "abstract": "Vision-Language Models (VLMs) are becoming increasingly powerful,\ndemonstrating strong performance on a variety of tasks that require both visual\nand textual understanding. Their strong generalisation abilities make them a\npromising component for automated driving systems, which must handle unexpected\ncorner cases. However, to be trusted in such safety-critical applications, a\nmodel must first possess a reliable perception system. Moreover, since critical\nobjects and agents in traffic scenes are often at a distance, we require\nsystems that are not \"shortsighted\", i.e., systems with strong perception\ncapabilities at both close (up to 20 meters) and long (30+ meters) range. With\nthis in mind, we introduce Distance-Annotated Traffic Perception Question\nAnswering (DTPQA), the first Visual Question Answering (VQA) benchmark focused\nsolely on perception-based questions in traffic scenes, enriched with distance\nannotations. By excluding questions that require reasoning, we ensure that\nmodel performance reflects perception capabilities alone. Since automated\ndriving hardware has limited processing power and cannot support large VLMs,\nour study centers on smaller VLMs. More specifically, we evaluate several\nstate-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the\nsimplicity of the questions, these models significantly underperform compared\nto humans (~60% average accuracy for the best-performing small VLM versus ~85%\nhuman performance). However, it is important to note that the human sample size\nwas relatively small, which imposes statistical limitations. We also identify\nspecific perception tasks, such as distinguishing left from right, that remain\nparticularly challenging for these models.",
      "pdf_url": "http://arxiv.org/pdf/2510.08352v1",
      "published": "2025-10-09T15:38:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08352v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DeepEN: Personalized Enteral Nutrition for Critically Ill Patients using Deep Reinforcement Learning",
      "authors": [
        "Daniel Jason Tan",
        "Jiayang Chen",
        "Dilruk Perera",
        "Kay Choong See",
        "Mengling Feng"
      ],
      "abstract": "We introduce DeepEN, a deep reinforcement learning (RL) framework for\npersonalized enteral nutrition (EN) in critically ill patients. Trained offline\non over 11,000 ICU patients from the MIMIC-IV database, DeepEN generates\n4-hourly recommendations for caloric, protein, and fluid intake tailored to\neach patient's evolving physiology. The model integrates a curated, clinically\ninformed state space with a custom reward function that balances short-term\nphysiological and nutrition-related goals with long-term survival outcomes.\nUsing a dueling double deep Q-network with conservative Q-learning\nregularization, DeepEN learns clinically realistic policies that align with\nhigh-value clinician actions while discouraging unsafe deviations. Across\nvarious qualitative and quantitative metrics, DeepEN outperforms\nclinician-derived and guideline-based policies, achieving a 3.7 $\\pm$ 0.17\npercentage-point reduction in estimated mortality (18.8% vs 22.5%) and\nimprovements in key nutritional biomarkers. These findings highlight the\npotential of safe, data-driven personalization of EN therapy to improve\noutcomes beyond traditional guideline- or heuristic-based approaches.",
      "pdf_url": "http://arxiv.org/pdf/2510.08350v1",
      "published": "2025-10-09T15:37:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08350v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization",
      "authors": [
        "Pál Zsámboki",
        "Benjamin Levi",
        "David Ansel Josef Smith",
        "Mitansh Kagalwala",
        "Arlington Kell",
        "Samuel Liechty",
        "Cong Wang"
      ],
      "abstract": "We study length generalization in transformers through the set complement\ntask, where a model must predict a uniform distribution over tokens absent from\nan input sequence -- an ability central to board-game style reasoning. Our main\ntheoretical result establishes two statements. First, we prove tight bounds on\nembedding and value dimensions for single-layer attention-only transformers.\nSecond, we show that if such a model achieves balanced logit displacement at\nlengths 1 and 2, then it must generalize to longer sequences, though with\nreduced precision. A mechanistic reading of the proof explains this limitation:\nas more tokens are attended to, softmax compresses logit displacements, eroding\nseparation between valid and invalid outputs. Training dynamics also suggest a\nsecond obstacle: when many next tokens are possible, updates become noisy. We\nhypothesize that dropout can counteract the first effect and Exponential Moving\nAverage (EMA) the second. We validate these hypotheses through random\nhyperparameter search on the set complement task, which confirms both\nmechanisms. We then test OthelloGPT, a GPT-1 style model trained on random\nOthello moves, and find that EMA again improves length generalization in this\nmore complex setting.",
      "pdf_url": "http://arxiv.org/pdf/2510.08341v1",
      "published": "2025-10-09T15:26:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08341v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings",
      "authors": [
        "Benjamin F. Maier",
        "Ulf Aslak",
        "Luca Fiaschi",
        "Nina Rismal",
        "Kemble Fletcher",
        "Christian C. Luhmann",
        "Robbie Dow",
        "Kli Pappas",
        "Thomas V. Wiecki"
      ],
      "abstract": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability.",
      "pdf_url": "http://arxiv.org/pdf/2510.08338v1",
      "published": "2025-10-09T15:24:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08338v1",
      "categories": [
        "cs.AI",
        "I.2.7; J.4"
      ]
    },
    {
      "title": "Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries",
      "authors": [
        "Marius Dragoi",
        "Ioana Pintilie",
        "Florin Gogianu",
        "Florin Brad"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm to improve Large Language Models on reasoning tasks such as\ncoding, math or logic. To assess the reasoning boundary (the fraction of\nproblems a model can solve) researchers often report Pass@k at large sampling\nbudgets. Recent results reveal a crossover phenomenon: while RLVR models\noutperform the base model at small k values, the base model usually outperforms\nthem when sampling a very large number of completions. This has been\ninterpreted as evidence that base models have a larger reasoning boundary. We\nargue that on tasks with discrete answer spaces, such as math with numeric\noutputs, Pass@k at large k reflects the increasingly higher chance of success\nin the limit of the number of trials rather than genuine reasoning, and can\ntherefore be misleading. We propose Cover@tau, which measures the fraction of\nproblems that a model can solve for which at least a tau proportion of\ncompletions are correct. Unlike Pass@k, Cover@tau captures reasoning under an\nexplicit reliability threshold: models that rely on random guessing degrade\nrapidly as tau increases. We evaluate several RLVR models using Cover@tau-based\nmetrics and illustrate how the relative rankings of popular algorithms change\ncompared to Pass@1, offering a different perspective on reasoning boundaries.",
      "pdf_url": "http://arxiv.org/pdf/2510.08325v1",
      "published": "2025-10-09T15:14:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08325v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2.6; I.2.7"
      ]
    },
    {
      "title": "Iterated Agent for Symbolic Regression",
      "authors": [
        "Zhuo-Yang Song",
        "Zeyu Cai",
        "Shutao Zhang",
        "Jiashen Wei",
        "Jichen Pan",
        "Shi Qiu",
        "Qing-Hong Cao",
        "Tie-Jiun Hou",
        "Xiaohui Liu",
        "Ming-xing Luo",
        "Hua Xing Zhu"
      ],
      "abstract": "Symbolic regression (SR), the automated discovery of mathematical expressions\nfrom data, is a cornerstone of scientific inquiry. However, it is often\nhindered by the combinatorial explosion of the search space and a tendency to\noverfit. Popular methods, rooted in genetic programming, explore this space\nsyntactically, often yielding overly complex, uninterpretable models. This\npaper introduces IdeaSearchFitter, a framework that employs Large Language\nModels (LLMs) as semantic operators within an evolutionary search. By\ngenerating candidate expressions guided by natural-language rationales, our\nmethod biases discovery towards models that are not only accurate but also\nconceptually coherent and interpretable. We demonstrate IdeaSearchFitter's\nefficacy across diverse challenges: it achieves competitive, noise-robust\nperformance on the Feynman Symbolic Regression Database (FSReD), outperforming\nseveral strong baselines; discovers mechanistically aligned models with good\naccuracy-complexity trade-offs on real-world data; and derives compact,\nphysically-motivated parametrizations for Parton Distribution Functions in a\nfrontier high-energy physics application. IdeaSearchFitter is a specialized\nmodule within our broader iterated agent framework, IdeaSearch, which is\npublicly available at https://www.ideasearch.cn/.",
      "pdf_url": "http://arxiv.org/pdf/2510.08317v1",
      "published": "2025-10-09T15:02:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08317v1",
      "categories": [
        "physics.comp-ph",
        "astro-ph.IM",
        "cs.AI",
        "cs.LG",
        "hep-ph"
      ]
    },
    {
      "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
      "authors": [
        "Liwei Kang",
        "Yue Deng",
        "Yao Xiao",
        "Zhanfeng Mo",
        "Wee Sun Lee",
        "Lidong Bing"
      ],
      "abstract": "Large language models have recently demonstrated significant gains in\nreasoning ability, often attributed to their capacity to generate longer chains\nof thought and engage in reflective reasoning. However, the contribution of\nreflections to performance improvement remains unclear. In this paper, we\nsystematically analyze the rollouts of eight reasoning models on five\nmathematical datasets. We focus on reflective behaviours where the model has\nalready produced an answer but continues reflecting before finalizing its\noutput. Our analysis reveals that reflections are predominantly confirmatory\nand rarely alter the model's initial answer, a pattern consistent across models\nand datasets. To understand the role of reflections in training, we construct\nsupervised fine-tuning (SFT) datasets with varying amounts of reflection steps.\nWe observe that training models on rollouts with more reflection steps\nprimarily enhances first-answer correctness rather than the ability to correct\ninitially wrong answers through reflections. This motivates us to propose a\nquestion-aware early-stopping method that enhances inference-time token\nefficiency by stopping the reasoning process once a few plausible candidate\nanswers are generated, thereby reducing unnecessary reflection steps. Motivated\nby this, we further propose to dynamically truncate the reflections after a\ncandidate answer has appeared during generation, which reduces reasoning tokens\nby 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2510.08308v1",
      "published": "2025-10-09T14:57:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08308v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant Graph Metanetworks",
      "authors": [
        "Bart Kuipers",
        "Freek Byrman",
        "Daniel Uyterlinde",
        "Alejandro García-Castellanos"
      ],
      "abstract": "Amortized optimization accelerates the solution of related optimization\nproblems by learning mappings that exploit shared structure across problem\ninstances. We explore the use of Scale Equivariant Graph Metanetworks\n(ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs\nenable single-shot fine-tuning of existing models, reducing the need for\niterative optimization. We demonstrate the effectiveness of this approach\nempirically and provide a theoretical result: the gauge freedom induced by\nscaling symmetries is strictly smaller in convolutional neural networks than in\nmulti-layer perceptrons. This insight helps explain the performance differences\nobserved between architectures in both our work and that of Kalogeropoulos et\nal. (2024). Overall, our findings underscore the potential of symmetry-aware\nmetanetworks as a powerful approach for efficient and generalizable neural\nnetwork optimization. Open-source code:\nhttps://github.com/daniuyter/scalegmn_amortization",
      "pdf_url": "http://arxiv.org/pdf/2510.08300v1",
      "published": "2025-10-09T14:51:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08300v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Counterfactual Identifiability via Dynamic Optimal Transport",
      "authors": [
        "Fabio De Sousa Ribeiro",
        "Ainkaran Santhirasekaram",
        "Ben Glocker"
      ],
      "abstract": "We address the open question of counterfactual identification for\nhigh-dimensional multivariate outcomes from observational data. Pearl (2000)\nargues that counterfactuals must be identifiable (i.e., recoverable from the\nobserved data distribution) to justify causal claims. A recent line of work on\ncounterfactual inference shows promising results but lacks identification,\nundermining the causal validity of its estimates. To address this, we establish\na foundation for multivariate counterfactual identification using\ncontinuous-time flows, including non-Markovian settings under standard\ncriteria. We characterise the conditions under which flow matching yields a\nunique, monotone and rank-preserving counterfactual transport map with tools\nfrom dynamic optimal transport, ensuring consistent inference. Building on\nthis, we validate the theory in controlled scenarios with counterfactual\nground-truth and demonstrate improvements in axiomatic counterfactual soundness\non real images.",
      "pdf_url": "http://arxiv.org/pdf/2510.08294v1",
      "published": "2025-10-09T14:45:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08294v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Learning Neural Exposure Fields for View Synthesis",
      "authors": [
        "Michael Niemeyer",
        "Fabian Manhardt",
        "Marie-Julie Rakotosaona",
        "Michael Oechsle",
        "Christina Tsalicoglou",
        "Keisuke Tateno",
        "Jonathan T. Barron",
        "Federico Tombari"
      ],
      "abstract": "Recent advances in neural scene representations have led to unprecedented\nquality in 3D reconstruction and view synthesis. Despite achieving high-quality\nresults for common benchmarks with curated data, outputs often degrade for data\nthat contain per image variations such as strong exposure changes, present,\ne.g., in most scenes with indoor and outdoor areas or rooms with windows. In\nthis paper, we introduce Neural Exposure Fields (NExF), a novel technique for\nrobustly reconstructing 3D scenes with high quality and 3D-consistent\nappearance from challenging real-world captures. In the core, we propose to\nlearn a neural field predicting an optimal exposure value per 3D point,\nenabling us to optimize exposure along with the neural scene representation.\nWhile capture devices such as cameras select optimal exposure per image/pixel,\nwe generalize this concept and perform optimization in 3D instead. This enables\naccurate view synthesis in high dynamic range scenarios, bypassing the need of\npost-processing steps or multi-exposure captures. Our contributions include a\nnovel neural representation for exposure prediction, a system for joint\noptimization of the scene representation and the exposure field via a novel\nneural conditioning mechanism, and demonstrated superior performance on\nchallenging real-world data. We find that our approach trains faster than prior\nworks and produces state-of-the-art results on several benchmarks improving by\nover 55% over best-performing baselines.",
      "pdf_url": "http://arxiv.org/pdf/2510.08279v1",
      "published": "2025-10-09T14:32:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08279v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Co-TAP: Three-Layer Agent Interaction Protocol Technical Report",
      "authors": [
        "Shunyu An",
        "Miao Wang",
        "Yongchao Li",
        "Dong Wan",
        "Lina Wang",
        "Ling Qin",
        "Liqin Gao",
        "Congyao Fan",
        "Zhiyong Mao",
        "Jiange Pu",
        "Wenji Xia",
        "Dong Zhao",
        "Rui Hu",
        "Ji Lu",
        "Guiyue Zhou",
        "Baoyu Tang",
        "Yanqin Gao",
        "Yongsheng Du",
        "Daigang Xu",
        "Lingjun Huang",
        "Baoli Wang",
        "Xiwen Zhang",
        "Luyao Wang",
        "Shilong Liu"
      ],
      "abstract": "This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer\nagent interaction protocol designed to address the challenges faced by\nmulti-agent systems across the three core dimensions of Interoperability,\nInteraction and Collaboration, and Knowledge Sharing. We have designed and\nproposed a layered solution composed of three core protocols: the Human-Agent\nInteraction Protocol (HAI), the Unified Agent Protocol (UAP), and the\nMemory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction\nlayer, standardizing the flow of information between users, interfaces, and\nagents by defining a standardized, event-driven communication paradigm. This\nensures the real-time performance, reliability, and synergy of interactions. As\nthe core of the infrastructure layer, UAP is designed to break down\ncommunication barriers among heterogeneous agents through unified service\ndiscovery and protocol conversion mechanisms, thereby enabling seamless\ninterconnection and interoperability of the underlying network. MEK, in turn,\noperates at the cognitive layer. By establishing a standardized ''Memory (M) -\nExtraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the\nability to learn from individual experiences and form shareable knowledge,\nthereby laying the foundation for the realization of true collective\nintelligence. We believe this protocol framework will provide a solid\nengineering foundation and theoretical guidance for building the next\ngeneration of efficient, scalable, and intelligent multi-agent applications.",
      "pdf_url": "http://arxiv.org/pdf/2510.08263v1",
      "published": "2025-10-09T14:20:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08263v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A Distributed Emulation Environment for In-Memory Computing Systems",
      "authors": [
        "Eleni Bougioukou",
        "Anastasios Petropoulos",
        "Nikolaos Toulgaridis",
        "Theodoros Chatzimichail",
        "Theodore Antonakopoulos"
      ],
      "abstract": "In-memory computing technology is used extensively in artificial intelligence\ndevices due to lower power consumption and fast calculation of matrix-based\nfunctions. The development of such a device and its integration in a system\ntakes a significant amount of time and requires the use of a real-time\nemulation environment, where various system aspects are analyzed, microcode is\ntested, and applications are deployed, even before the real chip is available.\nIn this work, we present the architecture, the software development tools, and\nexperimental results of a distributed and expandable emulation system for rapid\nprototyping of integrated circuits based on in-memory computing technologies.\nPresented experimental results demonstrate the usefulness of the proposed\nemulator.",
      "pdf_url": "http://arxiv.org/pdf/2510.08257v1",
      "published": "2025-10-09T14:15:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08257v1",
      "categories": [
        "cs.ET",
        "cs.AI"
      ]
    },
    {
      "title": "Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization",
      "authors": [
        "Jason Bohne",
        "Pawel Polak",
        "David Rosenberg",
        "Brian Bloniarz",
        "Gary Kazantsev"
      ],
      "abstract": "Direct Preference Optimization (DPO) has recently emerged as a simple and\neffective alternative to reinforcement learning from human feedback (RLHF) for\naligning large language models (LLMs) with user preferences. However, existing\nDPO formulations rely on a single monolithic model, which limits their\nexpressivity in multi-task settings and their adaptability to heterogeneous or\ndiverse preference distributions. In this work, we propose Mix- and MoE-DPO, a\nframework that extends DPO with both soft mixture models and mixture-of-experts\n(MoE) architectures, using a stochastic variational inference approach. Our\nmethod introduces a latent-variable model over expert assignments and optimizes\na variational evidence lower bound (ELBO), enabling stable and efficient\nlearning of specialized expert policies from preference data. Mix- and MoE-DPO\nprovides three key advantages over standard DPO: (i) generalization via\nuniversal function approximation through mixtures; (ii) reward and policy\nspecialization through expert components tailored to distinct preference modes;\nand (iii) contextual alignment through input-dependent soft gating that enables\nuser-specific mixture policies. Our framework supports both shared base\narchitectures with expert-specific policy heads and fully independent expert\nmodels, allowing flexible trade-offs between parameter efficiency and\nspecialization. We validate our approach on a variety of model sizes and\nmulti-preference datasets, demonstrating that Mix- and MoE-DPO offers a\npowerful and scalable method for preference-based LLM alignment.",
      "pdf_url": "http://arxiv.org/pdf/2510.08256v1",
      "published": "2025-10-09T14:15:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08256v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Opponent Shaping in LLM Agents",
      "authors": [
        "Marta Emili Garcia Segura",
        "Stephen Hailes",
        "Mirco Musolesi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being deployed as autonomous\nagents in real-world environments. As these deployments scale, multi-agent\ninteractions become inevitable, making it essential to understand strategic\nbehavior in such systems. A central open question is whether LLM agents, like\nreinforcement learning agents, can shape the learning dynamics and influence\nthe behavior of others through interaction alone. In this paper, we present the\nfirst investigation of opponent shaping (OS) with LLM-based agents. Existing OS\nalgorithms cannot be directly applied to LLMs, as they require higher-order\nderivatives, face scalability constraints, or depend on architectural\ncomponents that are absent in transformers. To address this gap, we introduce\nShapeLLM, an adaptation of model-free OS methods tailored for transformer-based\nagents. Using ShapeLLM, we examine whether LLM agents can influence co-players'\nlearning dynamics across diverse game-theoretic environments. We demonstrate\nthat LLM agents can successfully guide opponents toward exploitable equilibria\nin competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and\nChicken) and promote coordination and improve collective welfare in cooperative\ngames (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma).\nOur findings show that LLM agents can both shape and be shaped through\ninteraction, establishing opponent shaping as a key dimension of multi-agent\nLLM research.",
      "pdf_url": "http://arxiv.org/pdf/2510.08255v1",
      "published": "2025-10-09T14:13:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08255v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ]
    },
    {
      "title": "Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling",
      "authors": [
        "Jannek Ulm",
        "Kevin Du",
        "Vésteinn Snæbjarnarson"
      ],
      "abstract": "Large language models (LLMs) are trained on huge amounts of textual data, and\nconcerns have been raised that the limits of such data may soon be reached. A\npotential solution is to train on synthetic data sampled from LLMs. In this\nwork, we build on this idea and investigate the benefits of contrastive\ndecoding for generating synthetic corpora. In a controlled setting, we\nexperiment with sampling corpora using the relative difference between a good\nand bad model trained on the same original corpus of 100 million words. By\namplifying the signal from a model that has better performance, we create a\nsynthetic corpus and mix it with the original training data. Our findings show\nthat training on a mixture of synthesized and real data improves performance on\nthe language modeling objective and a range of downstream tasks. In particular,\nwe see that training with a mix of synthetic data from contrastive decoding\nbenefits tasks that require more reasoning skills, while synthetic data from\ntraditional sampling helps more on tasks dependent on surface level linguistic\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2510.08245v1",
      "published": "2025-10-09T14:04:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08245v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness",
      "authors": [
        "Jiyang Qiu",
        "Xinbei Ma",
        "Yunqing Xu",
        "Zhuosheng Zhang",
        "Hai Zhao"
      ],
      "abstract": "The rapid deployment of large language model (LLM)-based agents in real-world\napplications has raised serious concerns about their trustworthiness. In this\nwork, we reveal the security and robustness vulnerabilities of these agents\nthrough backdoor attacks. Distinct from traditional backdoors limited to\nsingle-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a\nmulti-step backdoor attack designed for long-horizon agentic control. CoTri\nrelies on an ordered sequence. It starts with an initial trigger, and\nsubsequent ones are drawn from the environment, allowing multi-step\nmanipulation that diverts the agent from its intended task. Experimental\nresults show that CoTri achieves a near-perfect attack success rate (ASR) while\nmaintaining a near-zero false trigger rate (FTR). Due to training data modeling\nthe stochastic nature of the environment, the implantation of CoTri\nparadoxically enhances the agent's performance on benign tasks and even\nimproves its robustness against environmental distractions. We further validate\nCoTri on vision-language models (VLMs), confirming its scalability to\nmultimodal agents. Our work highlights that CoTri achieves stable, multi-step\ncontrol within agents, improving their inherent robustness and task\ncapabilities, which ultimately makes the attack more stealthy and raises\npotential safty risks.",
      "pdf_url": "http://arxiv.org/pdf/2510.08238v1",
      "published": "2025-10-09T14:01:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.08238v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}