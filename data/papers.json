{
  "last_updated": "2025-12-02T00:53:58.262773",
  "papers": [
    {
      "title": "Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction",
      "authors": [
        "Bao Shu",
        "Yan Cai",
        "Jianjian Sun",
        "Chunrui Han",
        "En Yu",
        "Liang Zhao",
        "Jingcheng Hu",
        "Yinmin Zhang",
        "Haoran Lv",
        "Yuang Peng",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang",
        "Xiangyu Yue"
      ],
      "abstract": "Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.",
      "pdf_url": "https://arxiv.org/pdf/2511.23476v1",
      "published": "2025-11-28T18:59:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23476v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference",
      "authors": [
        "Hans Gundlach",
        "Jayson Lynch",
        "Matthias Mertens",
        "Neil Thompson"
      ],
      "abstract": "Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a warped picture of progress in practical capabilities per dollar. To remedy this, we use data from Artificial Analysis and Epoch AI to form the largest dataset of current and historical prices to run benchmarks to date. We find that the price for a given level of benchmark performance has decreased remarkably fast, around $5\\times$ to $10\\times$ per year, for frontier models on knowledge, reasoning, math, and software engineering benchmarks. These reductions in the cost of AI inference are due to economic forces, hardware efficiency improvements, and algorithmic efficiency improvements. Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around $3\\times$ per year. Finally, we recommend that evaluators both publicize and take into account the price of benchmarking as an essential part of measuring the real-world impact of AI.",
      "pdf_url": "https://arxiv.org/pdf/2511.23455v1",
      "published": "2025-11-28T18:47:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23455v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Physics-Informed Neural Networks for Thermophysical Property Retrieval",
      "authors": [
        "Ali Waseem",
        "Malcolm Mielle"
      ],
      "abstract": "Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.",
      "pdf_url": "https://arxiv.org/pdf/2511.23449v1",
      "published": "2025-11-28T18:41:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23449v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CV"
      ]
    },
    {
      "title": "ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts",
      "authors": [
        "Hang Yu",
        "Di Zhang",
        "Qiwei Du",
        "Yanping Zhao",
        "Hai Zhang",
        "Guang Chen",
        "Eduardo E. Veas",
        "Junqiao Zhao"
      ],
      "abstract": "Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.",
      "pdf_url": "https://arxiv.org/pdf/2511.23442v1",
      "published": "2025-11-28T18:35:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23442v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent",
      "authors": [
        "Jianzhe Lin",
        "Zeyu Pan",
        "Yun Zhu",
        "Ruiqi Song",
        "Jining Yang"
      ],
      "abstract": "We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.",
      "pdf_url": "https://arxiv.org/pdf/2511.23436v1",
      "published": "2025-11-28T18:32:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23436v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities",
      "authors": [
        "Aayush Garg",
        "Zanis Ali Khan",
        "Renzo Degiovanni",
        "Qiang Tang"
      ],
      "abstract": "Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.",
      "pdf_url": "https://arxiv.org/pdf/2511.23408v1",
      "published": "2025-11-28T18:03:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23408v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "LFM2 Technical Report",
      "authors": [
        "Alexander Amini",
        "Anna Banaszak",
        "Harold Benoit",
        "Arthur Böök",
        "Tarek Dakhran",
        "Song Duong",
        "Alfred Eng",
        "Fernando Fernandes",
        "Marc Härkönen",
        "Anne Harrington",
        "Ramin Hasani",
        "Saniya Karwa",
        "Yuri Khrustalev",
        "Maxime Labonne",
        "Mathias Lechner",
        "Valentine Lechner",
        "Simon Lee",
        "Zetian Li",
        "Noel Loo",
        "Jacob Marks",
        "Edoardo Mosca",
        "Samuel J. Paech",
        "Paul Pak",
        "Rom N. Parnichkun",
        "Alex Quach",
        "Ryan Rogers",
        "Daniela Rus",
        "Nayan Saxena",
        "Bettina Schlager",
        "Tim Seyde",
        "Jimmy T. H. Smith",
        "Aditya Tadimeti",
        "Neehal Tumma"
      ],
      "abstract": "We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.",
      "pdf_url": "https://arxiv.org/pdf/2511.23404v1",
      "published": "2025-11-28T17:56:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23404v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation",
      "authors": [
        "Mahdi Rahmani",
        "AmirHossein Saffari",
        "Reyhane Rahmani"
      ],
      "abstract": "Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet",
      "pdf_url": "https://arxiv.org/pdf/2511.23397v1",
      "published": "2025-11-28T17:44:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23397v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting",
      "authors": [
        "Daniil Sukhorukov",
        "Andrei Zakharov",
        "Nikita Glazkov",
        "Katsiaryna Yanchanka",
        "Vladimir Kirilin",
        "Maxim Dubovitsky",
        "Roman Sultimov",
        "Yuri Maksimov",
        "Ilya Makarov"
      ],
      "abstract": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2511.23387v1",
      "published": "2025-11-28T17:27:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23387v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Agentic AI Framework for Smart Inventory Replenishment",
      "authors": [
        "Toqeer Ali Syed",
        "Salman Jan",
        "Gohar Ali",
        "Ali Akarma",
        "Ahmad Ali",
        "Qurat-ul-Ain Mastoi"
      ],
      "abstract": "In contemporary retail, the variety of products available (e.g. clothing, groceries, cosmetics, frozen goods) make it difficult to predict the demand, prevent stockouts, and find high-potential products. We suggest an agentic AI model that will be used to monitor the inventory, initiate purchase attempts to the appropriate suppliers, and scan for trending or high-margin products to incorporate. The system applies demand forecasting, supplier selection optimization, multi-agent negotiation and continuous learning. We apply a prototype to a setting in the store of a middle scale mart, test its performance on three conventional and artificial data tables, and compare the results to the base heuristics. Our findings indicate that there is a decrease in stockouts, a reduction of inventory holding costs, and an improvement in product mix turnover. We address constraints, scalability as well as improvement prospect.",
      "pdf_url": "https://arxiv.org/pdf/2511.23366v1",
      "published": "2025-11-28T17:14:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23366v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories",
      "authors": [
        "Xinxi Zhang",
        "Shiwei Tan",
        "Quang Nguyen",
        "Quan Dao",
        "Ligong Han",
        "Xiaoxiao He",
        "Tunyu Zhang",
        "Alen Mrdovic",
        "Dimitris Metaxas"
      ],
      "abstract": "Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow.",
      "pdf_url": "https://arxiv.org/pdf/2511.23342v1",
      "published": "2025-11-28T16:50:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23342v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "ParaGate: Parasitic-Driven Domain Adaptation Transfer Learning for Netlist Performance Prediction",
      "authors": [
        "Bin Sun",
        "Jingyi Zhou",
        "Jianan Mu",
        "Zhiteng Chao",
        "Tianmeng Yang",
        "Ziyue Xu",
        "Jing Ye",
        "Huawei Li"
      ],
      "abstract": "In traditional EDA flows, layout-level performance metrics are only obtainable after placement and routing, hindering global optimization at earlier stages. Although some neural-network-based solutions predict layout-level performance directly from netlists, they often face generalization challenges due to the black-box heuristics of commercial placement-and-routing tools, which create disparate data across designs. To this end, we propose ParaGate, a three-step cross-stage prediction framework that infers layout-level timing and power from netlists. First, we propose a two-phase transfer-learning approach to predict parasitic parameters, pre-training on mid-scale circuits and fine-tuning on larger ones to capture extreme conditions. Next, we rely on EDA tools for timing analysis, offloading the long-path numerical reasoning. Finally, ParaGate performs global calibration using subgraph features. Experiments show that ParaGate achieves strong generalization with minimal fine-tuning data: on openE906, its arrival-time R2 from 0.119 to 0.897. These results demonstrate that ParaGate could provide guidance for global optimization in the synthesis and placement stages.",
      "pdf_url": "https://arxiv.org/pdf/2511.23340v1",
      "published": "2025-11-28T16:49:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23340v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach",
      "authors": [
        "Shuqi Liu",
        "Han Wu",
        "Guanzhi Deng",
        "Jianshu Chen",
        "Xiaoyang Wang",
        "Linqi Song"
      ],
      "abstract": "Knowledge-enhanced text generation aims to enhance the quality of generated text by utilizing internal or external knowledge sources. While language models have demonstrated impressive capabilities in generating coherent and fluent text, the lack of interpretability presents a substantial obstacle. The limited interpretability of generated text significantly impacts its practical usability, particularly in knowledge-enhanced text generation tasks that necessitate reliability and explainability. Existing methods often employ domain-specific knowledge retrievers that are tailored to specific data characteristics, limiting their generalizability to diverse data types and tasks. To overcome this limitation, we directly leverage the two-tier architecture of structured knowledge, consisting of high-level entities and low-level knowledge triples, to design our task-agnostic structured knowledge hunter. Specifically, we employ a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. By combining the strong generative ability of language models with the high faithfulness of the knowledge hunter, our model achieves high interpretability, enabling users to comprehend the model output generation process. Furthermore, we empirically demonstrate the effectiveness of our model in both internal knowledge-enhanced table-to-text generation on the RotoWireFG dataset and external knowledge-enhanced dialogue response generation on the KdConv dataset. Our task-agnostic model outperforms state-of-the-art methods and corresponding language models, setting new standards on the benchmark.",
      "pdf_url": "https://arxiv.org/pdf/2511.23335v1",
      "published": "2025-11-28T16:43:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23335v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models",
      "authors": [
        "Xiang Hu",
        "Zhanchao Zhou",
        "Ruiqi Liang",
        "Zehuan Li",
        "Wei Wu",
        "Jianguo Li"
      ],
      "abstract": "This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: \\textbf{sparsity}, \\textbf{random-access flexibility}, and \\textbf{length generalization}. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.",
      "pdf_url": "https://arxiv.org/pdf/2511.23319v1",
      "published": "2025-11-28T16:17:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23319v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach",
      "authors": [
        "Haruki Sakajo",
        "Hiroshi Takato",
        "Hiroshi Tsutsui",
        "Komei Soda",
        "Hidetaka Kamigaito",
        "Taro Watanabe"
      ],
      "abstract": "Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.",
      "pdf_url": "https://arxiv.org/pdf/2511.23311v1",
      "published": "2025-11-28T16:09:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23311v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Hard-Constrained Neural Networks with Physics-Embedded Architecture for Residual Dynamics Learning and Invariant Enforcement in Cyber-Physical Systems",
      "authors": [
        "Enzo Nicolás Spotorno",
        "Josafat Leal Filho",
        "Antônio Augusto Fröhlich"
      ],
      "abstract": "This paper presents a framework for physics-informed learning in complex cyber-physical systems governed by differential equations with both unknown dynamics and algebraic invariants. First, we formalize the Hybrid Recurrent Physics-Informed Neural Network (HRPINN), a general-purpose architecture that embeds known physics as a hard structural constraint within a recurrent integrator to learn only residual dynamics. Second, we introduce the Projected HRPINN (PHRPINN), a novel extension that integrates a predict-project mechanism to strictly enforce algebraic invariants by design. The framework is supported by a theoretical analysis of its representational capacity. We validate HRPINN on a real-world battery prognostics DAE and evaluate PHRPINN on a suite of standard constrained benchmarks. The results demonstrate the framework's potential for achieving high accuracy and data efficiency, while also highlighting critical trade-offs between physical consistency, computational cost, and numerical stability, providing practical guidance for its deployment.",
      "pdf_url": "https://arxiv.org/pdf/2511.23307v1",
      "published": "2025-11-28T16:06:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23307v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering",
      "authors": [
        "Zijian Fu",
        "Changsheng Lv",
        "Mengshi Qi",
        "Huadong Ma"
      ],
      "abstract": "In this paper, we propose a novel Multi-Modal Scene Graph with Kolmogorov-Arnold Expert Network for Audio-Visual Question Answering (SHRIKE). The task aims to mimic human reasoning by extracting and fusing information from audio-visual scenes, with the main challenge being the identification of question-relevant cues from the complex audio-visual content. Existing methods fail to capture the structural information within video, and suffer from insufficient fine-grained modeling of multi-modal features. To address these issues, we are the first to introduce a new multi-modal scene graph that explicitly models the objects and their relationship as a visually grounded, structured representation of the audio-visual scene. Furthermore, we design a Kolmogorov-Arnold Network~(KAN)-based Mixture of Experts (MoE) to enhance the expressive power of the temporal integration stage. This enables more fine-grained modeling of cross-modal interactions within the question-aware fused audio-visual representation, leading to capture richer and more nuanced patterns and then improve temporal reasoning performance. We evaluate the model on the established MUSIC-AVQA and MUSIC-AVQA v2 benchmarks, where it achieves state-of-the-art performance. Code and model checkpoints will be publicly released.",
      "pdf_url": "https://arxiv.org/pdf/2511.23304v1",
      "published": "2025-11-28T16:03:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23304v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Machine Learning for Scientific Visualization: Ensemble Data Analysis",
      "authors": [
        "Hamid Gadirov"
      ],
      "abstract": "Scientific simulations and experimental measurements produce vast amounts of spatio-temporal data, yet extracting meaningful insights remains challenging due to high dimensionality, complex structures, and missing information. Traditional analysis methods often struggle with these issues, motivating the need for more robust, data-driven approaches. This dissertation explores deep learning methodologies to improve the analysis and visualization of spatio-temporal scientific ensembles, focusing on dimensionality reduction, flow estimation, and temporal interpolation. First, we address high-dimensional data representation through autoencoder-based dimensionality reduction for scientific ensembles. We evaluate the stability of projection metrics under partial labeling and introduce a Pareto-efficient selection strategy to identify optimal autoencoder variants, ensuring expressive and reliable low-dimensional embeddings. Next, we present FLINT, a deep learning model for high-quality flow estimation and temporal interpolation in both flow-supervised and flow-unsupervised settings. FLINT reconstructs missing velocity fields and generates high-fidelity temporal interpolants for scalar fields across 2D+time and 3D+time ensembles without domain-specific assumptions or extensive finetuning. To further improve adaptability and generalization, we introduce HyperFLINT, a hypernetwork-based approach that conditions on simulation parameters to estimate flow fields and interpolate scalar data. This parameter-aware adaptation yields more accurate reconstructions across diverse scientific domains, even with sparse or incomplete data. Overall, this dissertation advances deep learning techniques for scientific visualization, providing scalable, adaptable, and high-quality solutions for interpreting complex spatio-temporal ensembles.",
      "pdf_url": "https://arxiv.org/pdf/2511.23290v1",
      "published": "2025-11-28T15:45:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23290v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.GR"
      ]
    },
    {
      "title": "Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI",
      "authors": [
        "Georgia Kanli",
        "Daniele Perlo",
        "Selma Boudissa",
        "Radovan Jirik",
        "Olivier Keunen"
      ],
      "abstract": "MR data are acquired in the frequency domain, known as k-space. Acquiring high-quality and high-resolution MR images can be time-consuming, posing a significant challenge when multiple sequences providing complementary contrast information are needed or when the patient is unable to remain in the scanner for an extended period of time. Reducing k-space measurements is a strategy to speed up acquisition, but often leads to reduced quality in reconstructed images. Additionally, in real-world MRI, both under-sampled and full-sampled images are prone to artefacts, and correcting these artefacts is crucial for maintaining diagnostic accuracy. Deep learning methods have been proposed to restore image quality from under-sampled data, while others focused on the correction of artefacts that result from the noise or motion. No approach has however been proposed so far that addresses both acceleration and artefacts correction, limiting the performance of these models when these degradation factors occur simultaneously. To address this gap, we present a method for recovering high-quality images from under-sampled data with simultaneously correction for noise and motion artefact called USArt (Under-Sampling and Artifact correction model). Customized for 2D brain anatomical images acquired with Cartesian sampling, USArt employs a dual sub-model approach. The results demonstrate remarkable increase of signal-to-noise ratio (SNR) and contrast in the images restored. Various under-sampling strategies and degradation levels were explored, with the gradient under-sampling strategy yielding the best outcomes. We achieved up to 5x acceleration and simultaneously artefacts correction without significant degradation, showcasing the model's robustness in real-world settings.",
      "pdf_url": "https://arxiv.org/pdf/2511.23274v1",
      "published": "2025-11-28T15:25:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23274v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.med-ph"
      ]
    },
    {
      "title": "OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning",
      "authors": [
        "Timothy Ossowski",
        "Sheng Zhang",
        "Qianchu Liu",
        "Guanghui Qin",
        "Reuben Tan",
        "Tristan Naumann",
        "Junjie Hu",
        "Hoifung Poon"
      ],
      "abstract": "High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.",
      "pdf_url": "https://arxiv.org/pdf/2511.23269v1",
      "published": "2025-11-28T15:21:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23269v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning",
      "authors": [
        "Yang Li",
        "Zhiyuan He",
        "Yuxuan Huang",
        "Zhuhanling Xiao",
        "Chao Yu",
        "Meng Fang",
        "Kun Shao",
        "Jun Wang"
      ],
      "abstract": "Recent Vision-Language Models (VLMs) exhibit strong perceptual reasoning abilities, yet they often struggle to adapt efficiently when encountering novel tasks at test time. In contrast, humans leverage the metacognitive model with memory, enabling continuous strategy refinement through metacognitive control when faced with new challenges. To bridge this gap, we propose metacognitive test-time reasoning (MCTR), a framework that equips models with the ability to learn, adapt, and improve during test time through metacognitive self-updating. Inspired by the dual structure of human metacognition, MCTR comprises meta-level and object-level VLM reasoning modules, each equipped with dedicated memory systems for hierarchical adaptive reasoning. Specifically, MCTR consists of (1) a meta-reasoning module which incrementally builds a structured memory by discovering and storing task-relevant rules, environmental patterns, and action-outcome relationships from test-time observations as natural language descriptions; and (2) an action-reasoning module that determines optimal actions through context-aware perception and strategic reasoning by dynamically retrieving and integrating knowledge from memory. The action-reasoning module continuously updates its policy through proposed metacognitive test-time reinforcement learning, adapting as knowledge memory evolves. We evaluate MCTR on 45 Atari games (33 seen, 12 unseen). MCTR demonstrates robust test-time adaptation, achieving 9/12 top-1 results on unseen games compared with baselines. Analyses through ablations, learning dynamics, and case studies reveal the complementary contributions of both components and show meta-reasoning evolving toward human-like adaptation strategies.",
      "pdf_url": "https://arxiv.org/pdf/2511.23262v1",
      "published": "2025-11-28T15:15:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23262v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Time Series Forecasting via Direct Per-Step Probability Distribution Modeling",
      "authors": [
        "Linghao Kong",
        "Xiaopeng Hong"
      ],
      "abstract": "Deep neural network-based time series prediction models have recently demonstrated superior capabilities in capturing complex temporal dependencies. However, it is challenging for these models to account for uncertainty associated with their predictions, because they directly output scalar values at each time step. To address such a challenge, we propose a novel model named interleaved dual-branch Probability Distribution Network (interPDN), which directly constructs discrete probability distributions per step instead of a scalar. The regression output at each time step is derived by computing the expectation of the predictive distribution on a predefined support set. To mitigate prediction anomalies, a dual-branch architecture is introduced with interleaved support sets, augmented by coarse temporal-scale branches for long-term trend forecasting. Outputs from another branch are treated as auxiliary signals to impose self-supervised consistency constraints on the current branch's prediction. Extensive experiments on multiple real-world datasets demonstrate the superior performance of interPDN.",
      "pdf_url": "https://arxiv.org/pdf/2511.23260v1",
      "published": "2025-11-28T15:13:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23260v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Robust HRRP Recognition under Interrupted Sampling Repeater Jamming using a Prior Jamming Information-Guided Network",
      "authors": [
        "Guozheng Sun",
        "Lei Wang",
        "Yanhao Wang",
        "Jie Wang",
        "Yimin Liu"
      ],
      "abstract": "Radar automatic target recognition (RATR) based on high-resolution range profile (HRRP) has attracted increasing attention due to its ability to capture fine-grained structural features. However, recognizing targets under electronic countermeasures (ECM), especially the mainstream interrupted-sampling repeater jamming (ISRJ), remains a significant challenge, as HRRPs often suffer from serious feature distortion. To address this, we propose a robust HRRP recognition method guided by prior jamming information. Specifically, we introduce a point spread function (PSF) as prior information to model the HRRP distortion induced by ISRJ. Based on this, we design a recognition network that leverages this prior through a prior-guided feature interaction module and a hybrid loss function to enhance the model's discriminative capability. With the aid of prior information, the model can learn invariant features within distorted HRRP under different jamming parameters. Both the simulated and measured-data experiments demonstrate that our method consistently outperforms state-of-the-art approaches and exhibits stronger generalization capabilities when facing unseen jamming parameters.",
      "pdf_url": "https://arxiv.org/pdf/2511.23256v1",
      "published": "2025-11-28T15:09:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23256v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture",
      "authors": [
        "Yibin Wen",
        "Qingmei Li",
        "Zi Ye",
        "Jiarui Zhang",
        "Jing Wu",
        "Zurong Mai",
        "Shuohong Lou",
        "Yuhang Chen",
        "Henglian Huang",
        "Xiaoya Fan",
        "Yang Zhang",
        "Lingyuan Zhao",
        "Haohuan Fu",
        "Huang Jianxi",
        "Juepeng Zheng"
      ],
      "abstract": "Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.",
      "pdf_url": "https://arxiv.org/pdf/2511.23253v1",
      "published": "2025-11-28T15:02:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23253v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "One-Shot Secure Aggregation: A Hybrid Cryptographic Protocol for Private Federated Learning in IoT",
      "authors": [
        "Imraul Emmaka",
        "Tran Viet Xuan Phuong"
      ],
      "abstract": "Federated Learning (FL) offers a promising approach to collaboratively train machine learning models without centralizing raw data, yet its scalability is often throttled by excessive communication overhead. This challenge is magnified in Internet of Things (IoT) environments, where devices face stringent bandwidth, latency, and energy constraints. Conventional secure aggregation protocols, while essential for protecting model updates, frequently require multiple interaction rounds, large payload sizes, and per-client costs rendering them impractical for many edge deployments.\n  In this work, we present Hyb-Agg, a lightweight and communication-efficient secure aggregation protocol that integrates Multi-Key CKKS (MK-CKKS) homomorphic encryption with Elliptic Curve Diffie-Hellman (ECDH)-based additive masking. Hyb-Agg reduces the secure aggregation process to a single, non-interactive client-to-server transmission per round, ensuring that per-client communication remains constant regardless of the number of participants. This design eliminates partial decryption exchanges, preserves strong privacy under the RLWE, CDH, and random oracle assumptions, and maintains robustness against collusion by the server and up to $N-2$ clients.\n  We implement and evaluate Hyb-Agg on both high-performance and resource-constrained devices, including a Raspberry Pi 4, demonstrating that it delivers sub-second execution times while achieving a constant communication expansion factor of approximately 12x over plaintext size. By directly addressing the communication bottleneck, Hyb-Agg enables scalable, privacy-preserving federated learning that is practical for real-world IoT deployments.",
      "pdf_url": "https://arxiv.org/pdf/2511.23252v1",
      "published": "2025-11-28T15:01:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23252v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Learning to Predict Aboveground Biomass from RGB Images with 3D Synthetic Scenes",
      "authors": [
        "Silvia Zuffi"
      ],
      "abstract": "Forests play a critical role in global ecosystems by supporting biodiversity and mitigating climate change via carbon sequestration. Accurate aboveground biomass (AGB) estimation is essential for assessing carbon storage and wildfire fuel loads, yet traditional methods rely on labor-intensive field measurements or remote sensing approaches with significant limitations in dense vegetation. In this work, we propose a novel learning-based method for estimating AGB from a single ground-based RGB image. We frame this as a dense prediction task, introducing AGB density maps, where each pixel represents tree biomass normalized by the plot area and each tree's image area. We leverage the recently introduced synthetic 3D SPREAD dataset, which provides realistic forest scenes with per-image tree attributes (height, trunk and canopy diameter) and instance segmentation masks. Using these assets, we compute AGB via allometric equations and train a model to predict AGB density maps, integrating them to recover the AGB estimate for the captured scene. Our approach achieves a median AGB estimation error of 1.22 kg/m^2 on held-out SPREAD data and 1.94 kg/m^2 on a real-image dataset. To our knowledge, this is the first method to estimate aboveground biomass directly from a single RGB image, opening up the possibility for a scalable, interpretable, and cost-effective solution for forest monitoring, while also enabling broader participation through citizen science initiatives.",
      "pdf_url": "https://arxiv.org/pdf/2511.23249v1",
      "published": "2025-11-28T15:00:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23249v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Tourism Question Answer System in Indian Language using Domain-Adapted Foundation Models",
      "authors": [
        "Praveen Gatla",
        "Anushka",
        "Nikita Kanwar",
        "Gouri Sahoo",
        "Rajesh Kumar Mundotiya"
      ],
      "abstract": "This article presents the first comprehensive study on designing a baseline extractive question-answering (QA) system for the Hindi tourism domain, with a specialized focus on the Varanasi-a cultural and spiritual hub renowned for its Bhakti-Bhaav (devotional ethos). Targeting ten tourism-centric subdomains-Ganga Aarti, Cruise, Food Court, Public Toilet, Kund, Museum, General, Ashram, Temple and Travel, the work addresses the absence of language-specific QA resources in Hindi for culturally nuanced applications. In this paper, a dataset comprising 7,715 Hindi QA pairs pertaining to Varanasi tourism was constructed and subsequently augmented with 27,455 pairs generated via Llama zero-shot prompting. We propose a framework leveraging foundation models-BERT and RoBERTa, fine-tuned using Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA), to optimize parameter efficiency and task performance. Multiple variants of BERT, including pre-trained languages (e.g., Hindi-BERT), are evaluated to assess their suitability for low-resource domain-specific QA. Evaluation metrics - F1, BLEU, and ROUGE-L - highlight trade-offs between answer precision and linguistic fluency. Experiments demonstrate that LoRA-based fine-tuning achieves competitive performance (85.3\\% F1) while reducing trainable parameters by 98\\% compared to SFT, striking a balance between efficiency and accuracy. Comparative analysis across models reveals that RoBERTa with SFT outperforms BERT variants in capturing contextual nuances, particularly for culturally embedded terms (e.g., Aarti, Kund). This work establishes a foundational baseline for Hindi tourism QA systems, emphasizing the role of LORA in low-resource settings and underscoring the need for culturally contextualized NLP frameworks in the tourism domain.",
      "pdf_url": "https://arxiv.org/pdf/2511.23235v1",
      "published": "2025-11-28T14:44:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23235v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration",
      "authors": [
        "Jordi Fornt",
        "Pau Fontova-Musté",
        "Adrian Gras",
        "Omar Lahyani",
        "Martí Caro",
        "Jaume Abella",
        "Francesc Moll",
        "Josep Altet"
      ],
      "abstract": "Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.",
      "pdf_url": "https://arxiv.org/pdf/2511.23203v1",
      "published": "2025-11-28T14:06:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23203v1",
      "categories": [
        "cs.AR",
        "cs.AI"
      ]
    },
    {
      "title": "Vision Bridge Transformer at Scale",
      "authors": [
        "Zhenxiong Tan",
        "Zeqing Wang",
        "Xingyi Yang",
        "Songhua Liu",
        "Xinchao Wang"
      ],
      "abstract": "We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.",
      "pdf_url": "https://arxiv.org/pdf/2511.23199v1",
      "published": "2025-11-28T14:03:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23199v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Obstruction reasoning for robotic grasping",
      "authors": [
        "Runyu Jiao",
        "Matteo Bortolon",
        "Francesco Giuliari",
        "Alice Fasoli",
        "Sergio Povoli",
        "Guofeng Mei",
        "Yiming Wang",
        "Fabio Poiesi"
      ],
      "abstract": "Successful robotic grasping in cluttered environments not only requires a model to visually ground a target object but also to reason about obstructions that must be cleared beforehand. While current vision-language embodied reasoning models show emergent spatial understanding, they remain limited in terms of obstruction reasoning and accessibility planning. To bridge this gap, we present UNOGrasp, a learning-based vision-language model capable of performing visually-grounded obstruction reasoning to infer the sequence of actions needed to unobstruct the path and grasp the target object. We devise a novel multi-step reasoning process based on obstruction paths originated by the target object. We anchor each reasoning step with obstruction-aware visual cues to incentivize reasoning capability. UNOGrasp combines supervised and reinforcement finetuning through verifiable reasoning rewards. Moreover, we construct UNOBench, a large-scale dataset for both training and benchmarking, based on MetaGraspNetV2, with over 100k obstruction paths annotated by humans with obstruction ratios, contact points, and natural-language instructions. Extensive experiments and real-robot evaluations show that UNOGrasp significantly improves obstruction reasoning and grasp success across both synthetic and real-world environments, outperforming generalist and proprietary alternatives. Project website: https://tev-fbk.github.io/UnoGrasp/.",
      "pdf_url": "https://arxiv.org/pdf/2511.23186v1",
      "published": "2025-11-28T13:53:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23186v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Listwise Preference Optimization with Element-wise Confusions for Aspect Sentiment Quad Prediction",
      "authors": [
        "Wenna Lai",
        "Haoran Xie",
        "Guandong Xu",
        "Qing Li",
        "S. Joe Qin"
      ],
      "abstract": "Aspect sentiment quad prediction (ASQP) is inherently challenging to predict a structured quadruple with four core sentiment elements, including aspect term (a), aspect category (c), opinion term (o), and sentiment polarity (s). Prior methods relying on marker-based prediction struggle with modeling the intricate relationships among elements and experience sharp performance declines when predicting higher-order elements (e.g., c and s) under standard supervised fine-tuning. To address these limitations, we employ reasoning-based generation to output both the quadruple and a natural language rationale under element prefixes within a unified template, encouraging explicit relational reasoning and interpretability. To further enhance element-wise alignment, we introduce a listwise preference optimization framework for improving structural validity and relational coherence. Specifically, we generate element-wise confusable candidates via syntactic and semantic proximity, then train the model with listwise objectives to prefer the gold candidates over closely competing alternatives. Extensive experiments on four benchmark datasets demonstrate that our framework effectively improves quadruple prediction accuracy and explanation consistency.",
      "pdf_url": "https://arxiv.org/pdf/2511.23184v1",
      "published": "2025-11-28T13:52:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23184v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Identification of Malicious Posts on the Dark Web Using Supervised Machine Learning",
      "authors": [
        "Sebastião Alves de Jesus Filho",
        "Gustavo Di Giovanni Bernardo",
        "Paulo Henrique Ribeiro Gabriel",
        "Bruno Bogaz Zarpelão",
        "Rodrigo Sanches Miani"
      ],
      "abstract": "Given the constant growth and increasing sophistication of cyberattacks, cybersecurity can no longer rely solely on traditional defense techniques and tools. Proactive detection of cyber threats has become essential to help security teams identify potential risks and implement effective mitigation measures. Cyber Threat Intelligence (CTI) plays a key role by providing security analysts with evidence-based knowledge about cyber threats. CTI information can be extracted using various techniques and data sources; however, machine learning has proven promising. As for data sources, social networks and online discussion forums are commonly explored. In this study, we apply text mining techniques and machine learning to data collected from Dark Web forums in Brazilian Portuguese to identify malicious posts. Our contributions include the creation of three original datasets, a novel multi-stage labeling process combining indicators of compromise (IoCs), contextual keywords, and manual analysis, and a comprehensive evaluation of text representations and classifiers. To our knowledge, this is the first study to focus specifically on Brazilian Portuguese content in this domain. The best-performing model, using LightGBM and TF-IDF, was able to detect relevant posts with high accuracy. We also applied topic modeling to validate the model's outputs on unlabeled data, confirming its robustness in real-world scenarios.",
      "pdf_url": "https://arxiv.org/pdf/2511.23183v1",
      "published": "2025-11-28T13:51:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23183v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "AI for software engineering: from probable to provable",
      "authors": [
        "Bertrand Meyer"
      ],
      "abstract": "Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals (\"prompt engineering\" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.\n  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools.",
      "pdf_url": "https://arxiv.org/pdf/2511.23159v1",
      "published": "2025-11-28T13:14:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23159v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection",
      "authors": [
        "Huangsen Cao",
        "Qin Mei",
        "Zhiheng Li",
        "Yuxi Li",
        "Ying Zhang",
        "Chen Li",
        "Zhimeng Zhang",
        "Xin Ding",
        "Yongwei Wang",
        "Jing Lyu",
        "Fei Wu"
      ],
      "abstract": "With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity. Consequently, there is an urgent need for efficient and truly explainable image forensic methods. Recent detection paradigms have shifted towards explainable forensics. However, state-of-the-art approaches primarily rely on post-hoc rationalizations or visual discrimination, lacking a verifiable chain of evidence. This reliance on surface-level pattern matching limits the generation of causally grounded explanations and often results in poor generalization. To bridge this critical gap, we introduce \\textbf{REVEAL-Bench}, the first reasoning-enhanced multimodal benchmark for AI-generated image detection that is explicitly structured around a chain-of-evidence derived from multiple lightweight expert models, then records step-by-step reasoning traces and evidential justifications. Building upon this dataset, we propose \\textbf{REVEAL} (\\underline{R}easoning-\\underline{e}nhanced Forensic E\\underline{v}id\\underline{e}nce \\underline{A}na\\underline{l}ysis), an effective and explainable forensic framework that integrates detection with a novel expert-grounded reinforcement learning. Our reward mechanism is specially tailored to jointly optimize detection accuracy, explanation fidelity, and logical coherence grounded in explicit forensic evidence, enabling REVEAL to produce fine-grained, interpretable, and verifiable reasoning chains alongside its detection outcomes. Extensive experimental results demonstrate that REVEAL significantly enhances detection accuracy, explanation fidelity, and robust cross-model generalization, benchmarking a new state of the art for explainable image forensics.",
      "pdf_url": "https://arxiv.org/pdf/2511.23158v1",
      "published": "2025-11-28T13:11:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23158v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning",
      "authors": [
        "Mian Ibad Ali Shah",
        "Marcos Eduardo Cruz Victorio",
        "Maeve Duffy",
        "Enda Barrett",
        "Karl Mason"
      ],
      "abstract": "The integration of renewable energy resources in rural areas, such as dairy farming communities, enables decentralized energy management through Peer-to-Peer (P2P) energy trading. This research highlights the role of P2P trading in efficient energy distribution and its synergy with advanced optimization techniques. While traditional rule-based methods perform well under stable conditions, they struggle in dynamic environments. To address this, Multi-Agent Reinforcement Learning (MARL), specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), is combined with community/distributed P2P trading mechanisms. By incorporating auction-based market clearing, a price advisor agent, and load and battery management, the approach achieves significant improvements. Results show that, compared to baseline models, DQN reduces electricity costs by 14.2% in Ireland and 5.16% in Finland, while increasing electricity revenue by 7.24% and 12.73%, respectively. PPO achieves the lowest peak hour demand, reducing it by 55.5% in Ireland, while DQN reduces peak hour demand by 50.0% in Ireland and 27.02% in Finland. These improvements are attributed to both MARL algorithms and P2P energy trading, which together results in electricity cost and peak hour demand reduction, and increase electricity selling revenue. This study highlights the complementary strengths of DQN, PPO, and P2P trading in achieving efficient, adaptable, and sustainable energy management in rural communities.",
      "pdf_url": "https://arxiv.org/pdf/2511.23148v1",
      "published": "2025-11-28T12:53:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23148v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications",
      "authors": [
        "Enrico Saccon",
        "Davide De Martini",
        "Matteo Saveriano",
        "Edoardo Lamon",
        "Luigi Palopoli",
        "Marco Roveri"
      ],
      "abstract": "We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.",
      "pdf_url": "https://arxiv.org/pdf/2511.23143v1",
      "published": "2025-11-28T12:48:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23143v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models",
      "authors": [
        "Yujiao Yang",
        "Jing Lian",
        "Linhui Li"
      ],
      "abstract": "The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.",
      "pdf_url": "https://arxiv.org/pdf/2511.23136v1",
      "published": "2025-11-28T12:35:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23136v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Evolutionary Discovery of Heuristic Policies for Traffic Signal Control",
      "authors": [
        "Ruibing Wang",
        "Shuhan Guo",
        "Zeen Li",
        "Zhen Wang",
        "Quanming Yao"
      ],
      "abstract": "Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\\textbf{\\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \\method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.",
      "pdf_url": "https://arxiv.org/pdf/2511.23122v1",
      "published": "2025-11-28T12:11:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23122v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Mind Reading or Misreading? LLMs on the Big Five Personality Test",
      "authors": [
        "Francesco Di Cursi",
        "Chiara Boldrini",
        "Marco Conti",
        "Andrea Passarella"
      ],
      "abstract": "We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.",
      "pdf_url": "https://arxiv.org/pdf/2511.23101v1",
      "published": "2025-11-28T11:40:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23101v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Fairness in the Multi-Secretary Problem",
      "authors": [
        "Georgios Papasotiropoulos",
        "Zein Pishbin"
      ],
      "abstract": "This paper bridges two perspectives: it studies the multi-secretary problem through the fairness lens of social choice, and examines multi-winner elections from the viewpoint of online decision making. After identifying the limitations of the prominent proportionality notion of Extended Justified Representation (EJR) in the online domain, the work proposes a set of mechanisms that merge techniques from online algorithms with rules from social choice -- such as the Method of Equal Shares and the Nash Rule -- and supports them through both theoretical analysis and extensive experimental evaluation.",
      "pdf_url": "https://arxiv.org/pdf/2511.23097v1",
      "published": "2025-11-28T11:35:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23097v1",
      "categories": [
        "cs.GT",
        "cs.AI"
      ]
    },
    {
      "title": "Does Self-Evaluation Enable Wireheading in Language Models?",
      "authors": [
        "David Demitri Africa",
        "Hans Ethan Ting"
      ],
      "abstract": "Self-evaluation is increasingly central to language model training, from constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate reward measurements rather than improving task performance. We formalize conditions under which reward-channel control strictly dominates task-focused behavior in POMDPs and test these predictions empirically. Across two models and three tasks, we find that models whose self-grades determine rewards exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. Models that self-evaluate but do not control rewards show no such inflation. Our results demonstrate that self-evaluation is safe when decoupled from learning signals but dangerous when coupled, with clear implications for agentic system design.",
      "pdf_url": "https://arxiv.org/pdf/2511.23092v1",
      "published": "2025-11-28T11:24:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23092v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models",
      "authors": [
        "Ruosen Zhao",
        "Zhikang Zhang",
        "Jialei Xu",
        "Jiahao Chang",
        "Dong Chen",
        "Lingyun Li",
        "Weijian Sun",
        "Zizhuang Wei"
      ],
      "abstract": "Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.",
      "pdf_url": "https://arxiv.org/pdf/2511.23075v1",
      "published": "2025-11-28T11:04:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23075v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "What If They Took the Shot? A Hierarchical Bayesian Framework for Counterfactual Expected Goals",
      "authors": [
        "Mikayil Mahmudlu",
        "Oktay Karakuş",
        "Hasan Arkadaş"
      ],
      "abstract": "This study develops a hierarchical Bayesian framework that integrates expert domain knowledge to quantify player-specific effects in expected goals (xG) estimation, addressing a limitation of standard models that treat all players as identical finishers. Using 9,970 shots from StatsBomb's 2015-16 data and Football Manager 2017 ratings, we combine Bayesian logistic regression with informed priors to stabilise player-level estimates, especially for players with few shots. The hierarchical model reduces posterior uncertainty relative to weak priors and achieves strong external validity: hierarchical and baseline predictions correlate at R2 = 0.75, while an XGBoost benchmark validated against StatsBomb xG reaches R2 = 0.833. The model uncovers interpretable specialisation profiles, including one-on-one finishing (Aguero, Suarez, Belotti, Immobile, Martial), long-range shooting (Pogba), and first-touch execution (Insigne, Salah, Gameiro). It also identifies latent ability in underperforming players such as Immobile and Belotti. The framework supports counterfactual \"what-if\" analysis by reallocating shots between players under identical contexts. Case studies show that Sansone would generate +2.2 xG from Berardi's chances, driven largely by high-pressure situations, while Vardy-Giroud substitutions reveal strong asymmetry: replacing Vardy with Giroud results in a large decline (about -7 xG), whereas the reverse substitution has only a small effect (about -1 xG). This work provides an uncertainty-aware tool for player evaluation, recruitment, and tactical planning, and offers a general approach for domains where individual skill and contextual factors jointly shape performance.",
      "pdf_url": "https://arxiv.org/pdf/2511.23072v1",
      "published": "2025-11-28T11:01:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23072v1",
      "categories": [
        "eess.SP",
        "cs.AI",
        "stat.AP"
      ]
    },
    {
      "title": "Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding",
      "authors": [
        "Anik De",
        "Abhirama Subramanyam Penamakuri",
        "Rajeev Yadav",
        "Aditya Rathore",
        "Harshiv Shah",
        "Devesh Sharma",
        "Sagar Agarwal",
        "Pravin Kumar",
        "Anand Mishra"
      ],
      "abstract": "Reading scene text, that is, text appearing in images, has numerous application areas, including assistive technology, search, and e-commerce. Although scene text recognition in English has advanced significantly and is often considered nearly a solved problem, Indian language scene text recognition remains an open challenge. This is due to script diversity, non-standard fonts, and varying writing styles, and, more importantly, the lack of high-quality datasets and open-source models. To address these gaps, we introduce the Bharat Scene Text Dataset (BSTD) - a large-scale and comprehensive benchmark for studying Indian Language Scene Text Recognition. It comprises more than 100K words that span 11 Indian languages and English, sourced from over 6,500 scene images captured across various linguistic regions of India. The dataset is meticulously annotated and supports multiple scene text tasks, including: (i) Scene Text Detection, (ii) Script Identification, (iii) Cropped Word Recognition, and (iv) End-to-End Scene Text Recognition. We evaluated state-of-the-art models originally developed for English by adapting (fine-tuning) them for Indian languages. Our results highlight the challenges and opportunities in Indian language scene text recognition. We believe that this dataset represents a significant step toward advancing research in this domain. All our models and data are open source.",
      "pdf_url": "https://arxiv.org/pdf/2511.23071v1",
      "published": "2025-11-28T10:58:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23071v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Evaluating the Clinical Impact of Generative Inpainting on Bone Age Estimation",
      "authors": [
        "Felipe Akio Matsuoka",
        "Eduardo Moreno J. M. Farina",
        "Augusto Sarquis Serpa",
        "Soraya Monteiro",
        "Rodrigo Ragazzini",
        "Nitamar Abdala",
        "Marcelo Straus Takahashi",
        "Felipe Campos Kitamura"
      ],
      "abstract": "Generative foundation models can remove visual artifacts through realistic image inpainting, but their impact on medical AI performance remains uncertain. Pediatric hand radiographs often contain non-anatomical markers, and it is unclear whether inpainting these regions preserves features needed for bone age and gender prediction. To evaluate the clinical reliability of generative model-based inpainting for artifact removal, we used the RSNA Bone Age Challenge dataset, selecting 200 original radiographs and generating 600 inpainted versions with gpt-image-1 using natural language prompts to target non-anatomical artifacts. Downstream performance was assessed with deep learning ensembles for bone age estimation and gender classification, using mean absolute error (MAE) and area under the ROC curve (AUC) as metrics, and pixel intensity distributions to detect structural alterations. Inpainting markedly degraded model performance: bone age MAE increased from 6.26 to 30.11 months, and gender classification AUC decreased from 0.955 to 0.704. Inpainted images displayed pixel-intensity shifts and inconsistencies, indicating structural modifications not corrected by simple calibration. These findings show that, although visually realistic, foundation model-based inpainting can obscure subtle but clinically relevant features and introduce latent bias even when edits are confined to non-diagnostic regions, underscoring the need for rigorous, task-specific validation before integrating such generative tools into clinical AI workflows.",
      "pdf_url": "https://arxiv.org/pdf/2511.23066v1",
      "published": "2025-11-28T10:48:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23066v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework",
      "authors": [
        "Jiatong Han"
      ],
      "abstract": "Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.",
      "pdf_url": "https://arxiv.org/pdf/2511.23059v1",
      "published": "2025-11-28T10:35:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23059v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents",
      "authors": [
        "Ruoxuan Zhang",
        "Qiyun Zheng",
        "Zhiyu Zhou",
        "Ziqi Liao",
        "Siyu Wu",
        "Jian-Yu Jiang-Lin",
        "Bin Wen",
        "Hongxia Xie",
        "Jianlong Fu",
        "Wen-Huang Cheng"
      ],
      "abstract": "Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.",
      "pdf_url": "https://arxiv.org/pdf/2511.23055v1",
      "published": "2025-11-28T10:24:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23055v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "High-Resolution Probabilistic Data-Driven Weather Modeling with a Stretched-Grid",
      "authors": [
        "Even Marius Nordhagen",
        "Håvard Homleid Haugen",
        "Aram Farhad Shafiq Salihi",
        "Magnus Sikora Ingstad",
        "Thomas Nils Nipen",
        "Ivar Ambjørn Seierstad",
        "Inger-Lise Frogner",
        "Mariana Clare",
        "Simon Lang",
        "Matthew Chantry",
        "Peter Dueben",
        "Jørn Kristiansen"
      ],
      "abstract": "We present a probabilistic data-driven weather model capable of providing an ensemble of high spatial resolution realizations of 87 variables at arbitrary forecast length and ensemble size. The model uses a stretched grid, dedicating 2.5 km resolution to a region of interest, and 31 km resolution elsewhere. Based on a stochastic encoder-decoder architecture, the model is trained using a loss function based on the Continuous Ranked Probability Score (CRPS) evaluated point-wise in real and spectral space. The spectral loss components is shown to be necessary to create fields that are spatially coherent. The model is compared to high-resolution operational numerical weather prediction forecasts from the MetCoOp Ensemble Prediction System (MEPS), showing competitive forecasts when evaluated against observations from surface weather stations. The model produced fields that are more spatially coherent than mean squared error based models and CRPS based models without the spectral component in the loss.",
      "pdf_url": "https://arxiv.org/pdf/2511.23043v1",
      "published": "2025-11-28T10:08:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23043v1",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Delta-XAI: A Unified Framework for Explaining Prediction Changes in Online Time Series Monitoring",
      "authors": [
        "Changhun Kim",
        "Yechan Mun",
        "Hyeongwon Jang",
        "Eunseo Lee",
        "Sangchul Hahn",
        "Eunho Yang"
      ],
      "abstract": "Explaining online time series monitoring models is crucial across sensitive domains such as healthcare and finance, where temporal and contextual prediction dynamics underpin critical decisions. While recent XAI methods have improved the explainability of time series models, they mostly analyze each time step independently, overlooking temporal dependencies. This results in further challenges: explaining prediction changes is non-trivial, methods fail to leverage online dynamics, and evaluation remains difficult. To address these challenges, we propose Delta-XAI, which adapts 14 existing XAI methods through a wrapper function and introduces a principled evaluation suite for the online setting, assessing diverse aspects, such as faithfulness, sufficiency, and coherence. Experiments reveal that classical gradient-based methods, such as Integrated Gradients (IG), can outperform recent approaches when adapted for temporal analysis. Building on this, we propose Shifted Window Integrated Gradients (SWING), which incorporates past observations in the integration path to systematically capture temporal dependencies and mitigate out-of-distribution effects. Extensive experiments consistently demonstrate the effectiveness of SWING across diverse settings with respect to diverse metrics. Our code is publicly available at https://anonymous.4open.science/r/Delta-XAI.",
      "pdf_url": "https://arxiv.org/pdf/2511.23036v1",
      "published": "2025-11-28T09:57:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23036v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning",
      "authors": [
        "Changpeng Wang",
        "Haozhe Wang",
        "Xi Chen",
        "Junhan Liu",
        "Taofeng Xue",
        "Chong Peng",
        "Donglian Qi",
        "Fangzhen Lin",
        "Yunfeng Yan"
      ],
      "abstract": "Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to \"get the right answer for the right visual reason\". Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.",
      "pdf_url": "https://arxiv.org/pdf/2511.23031v1",
      "published": "2025-11-28T09:52:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.23031v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}