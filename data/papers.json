{
  "last_updated": "2025-10-08T00:46:50.255127",
  "papers": [
    {
      "title": "TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration",
      "authors": [
        "Cheng Xin",
        "Fan Xu",
        "Xin Ding",
        "Jie Gao",
        "Jiaxin Ding"
      ],
      "abstract": "Graph Neural Networks (GNNs) have shown remarkable success across various\nscientific fields, yet their adoption in critical decision-making is often\nhindered by a lack of interpretability. Recently, intrinsically interpretable\nGNNs have been studied to provide insights into model predictions by\nidentifying rationale substructures in graphs. However, existing methods face\nchallenges when the underlying rationale subgraphs are complex and varied. In\nthis work, we propose TopInG: Topologically Interpretable Graph Learning, a\nnovel topological framework that leverages persistent homology to identify\npersistent rationale subgraphs. TopInG employs a rationale filtration learning\napproach to model an autoregressive generation process of rationale subgraphs,\nand introduces a self-adjusted topological constraint, termed topological\ndiscrepancy, to enforce a persistent topological distinction between rationale\nsubgraphs and irrelevant counterparts. We provide theoretical guarantees that\nour loss function is uniquely optimized by the ground truth under specific\nconditions. Extensive experiments demonstrate TopInG's effectiveness in\ntackling key challenges, such as handling variform rationale subgraphs,\nbalancing predictive performance with interpretability, and mitigating spurious\ncorrelations. Results show that our approach improves upon state-of-the-art\nmethods on both predictive accuracy and interpretation quality.",
      "pdf_url": "http://arxiv.org/pdf/2510.05102v1",
      "published": "2025-10-06T17:59:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05102v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CG",
        "math.AT",
        "stat.ML",
        "55N31, 68T05, 62R40, 05C, 68R05",
        "I.2.6; G.2.2; I.5.1"
      ]
    },
    {
      "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
      "authors": [
        "Zeyu Zhu",
        "Kevin Qinghong Lin",
        "Mike Zheng Shou"
      ],
      "abstract": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
      "pdf_url": "http://arxiv.org/pdf/2510.05096v1",
      "published": "2025-10-06T17:58:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05096v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "cs.MM"
      ]
    },
    {
      "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models",
      "authors": [
        "Mingkang Zhu",
        "Xi Chen",
        "Bei Yu",
        "Hengshuang Zhao",
        "Jiaya Jia"
      ],
      "abstract": "Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance.",
      "pdf_url": "http://arxiv.org/pdf/2510.05095v1",
      "published": "2025-10-06T17:58:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05095v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Learning to Interpret Weight Differences in Language Models",
      "authors": [
        "Avichal Goel",
        "Yoon Kim",
        "Nir Shavit",
        "Tony T. Wang"
      ],
      "abstract": "Finetuning (pretrained) language models is a standard approach for updating\ntheir internal parametric knowledge and specializing them to new tasks and\ndomains. However, the corresponding model weight changes (\"weight diffs\") are\nnot generally interpretable. While inspecting the finetuning dataset can give a\nsense of how the model might have changed, these datasets are often not\npublicly available or are too large to work with directly. Towards the goal of\ncomprehensively understanding weight diffs in natural language, we introduce\nDiff Interpretation Tuning (DIT), a method that trains models to describe their\nown finetuning-induced modifications. Our approach uses synthetic, labeled\nweight diffs to train a DIT adapter, which can be applied to a compatible\nfinetuned model to make it describe how it has changed. We demonstrate in two\nproof-of-concept settings (reporting hidden behaviors and summarizing finetuned\nknowledge) that our method enables models to describe their finetuning-induced\nmodifications using accurate natural language descriptions.",
      "pdf_url": "http://arxiv.org/pdf/2510.05092v1",
      "published": "2025-10-06T17:57:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05092v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models",
      "authors": [
        "Runchu Tian",
        "Junxia Cui",
        "Xueqiang Xu",
        "Feng Yao",
        "Jingbo Shang"
      ],
      "abstract": "Diffusion large language models (dLLMs) have recently emerged as a promising\nalternative to autoregressive (AR) models, offering advantages such as\naccelerated parallel decoding and bidirectional context modeling. However, the\nvanilla decoding strategy in discrete dLLMs suffers from a critical limitation:\nonce a token is accepted, it can no longer be revised in subsequent steps. As a\nresult, early mistakes persist across iterations, harming both intermediate\npredictions and final output quality. To address this issue, we propose\nTolerator (Token-Level Cross-Validation Refinement), a training-free decoding\nstrategy that leverages cross-validation among predicted tokens. Unlike\nexisting methods that follow a single progressive unmasking procedure,\nTolerator introduces a two-stage process: (i) sequence fill-up and (ii)\niterative refinement by remasking and decoding a subset of tokens while\ntreating the remaining as context. This design enables previously accepted\ntokens to be reconsidered and corrected when necessary, leading to more\nreliable diffusion decoding outputs. We evaluate Tolerator on five standard\nbenchmarks covering language understanding, code generation, and mathematics.\nExperiments show that our method achieves consistent improvements over the\nbaselines under the same computational budget. These findings suggest that\ndecoding algorithms are crucial to realizing the full potential of diffusion\nlarge language models. Code and data are publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2510.05090v1",
      "published": "2025-10-06T17:56:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05090v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data",
      "authors": [
        "Janos Perczel",
        "Jin Chow",
        "Dorottya Demszky"
      ],
      "abstract": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction.",
      "pdf_url": "http://arxiv.org/pdf/2510.05087v1",
      "published": "2025-10-06T17:55:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05087v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder",
      "authors": [
        "Ronen Kamenetsky",
        "Sara Dorfman",
        "Daniel Garibi",
        "Roni Paiss",
        "Or Patashnik",
        "Daniel Cohen-Or"
      ],
      "abstract": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains.",
      "pdf_url": "http://arxiv.org/pdf/2510.05081v1",
      "published": "2025-10-06T17:51:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05081v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Slm-mux: Orchestrating small language models for reasoning",
      "authors": [
        "Chenyu Wang",
        "Zishen Wan",
        "Hao Kang",
        "Emma Chen",
        "Zhiqiang Xie",
        "Tushar Krishna",
        "Vijay Janapa Reddi",
        "Yilun Du"
      ],
      "abstract": "With the rapid development of language models, the number of small language\nmodels (SLMs) has grown significantly. Although they do not achieve\nstate-of-the-art accuracy, they are more efficient and often excel at specific\ntasks. This raises a natural question: can multiple SLMs be orchestrated into a\nsystem where each contributes effectively, achieving higher accuracy than any\nindividual model? Existing orchestration methods have primarily targeted\nfrontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To\naddress this gap, we propose a three-stage approach for orchestrating SLMs.\nFirst, we introduce SLM-MUX, a multi-model architecture that effectively\ncoordinates multiple SLMs. Building on this, we develop two optimization\nstrategies: (i) a model selection search that identifies the most complementary\nSLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our\napproach delivers strong results: Compared to existing orchestration methods,\nour approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%\non GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and\nGSM8K, and matches its performance on MATH. We further provide theoretical\nanalyses to substantiate the advantages of our method. In summary, we\ndemonstrate that SLMs can be effectively orchestrated into more accurate and\nefficient systems through the proposed approach.",
      "pdf_url": "http://arxiv.org/pdf/2510.05077v1",
      "published": "2025-10-06T17:49:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05077v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs",
      "authors": [
        "Dachuan Shi",
        "Abedelkadir Asi",
        "Keying Li",
        "Xiangchi Yuan",
        "Leyan Pan",
        "Wenke Lee",
        "Wen Xiao"
      ],
      "abstract": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.",
      "pdf_url": "http://arxiv.org/pdf/2510.05069v1",
      "published": "2025-10-06T17:46:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05069v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Staircase Streaming for Low-Latency Multi-Agent Inference",
      "authors": [
        "Junlin Wang",
        "Jue Wang",
        "Zhen",
        "Xu",
        "Ben Athiwaratkun",
        "Bhuwan Dhingra",
        "Ce Zhang",
        "James Zou"
      ],
      "abstract": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality.",
      "pdf_url": "http://arxiv.org/pdf/2510.05059v1",
      "published": "2025-10-06T17:37:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05059v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model",
      "authors": [
        "Peter Van Katwyk",
        "Karianne J. Bergen"
      ],
      "abstract": "Uncertainty quantification is critical for ensuring robustness in high-stakes\nmachine learning applications. We introduce HybridFlow, a modular hybrid\narchitecture that unifies the modeling of aleatoric and epistemic uncertainty\nby combining a Conditional Masked Autoregressive normalizing flow for\nestimating aleatoric uncertainty with a flexible probabilistic predictor for\nepistemic uncertainty. The framework supports integration with any\nprobabilistic model class, allowing users to easily adapt HybridFlow to\nexisting architectures without sacrificing predictive performance. HybridFlow\nimproves upon previous uncertainty quantification frameworks across a range of\nregression tasks, such as depth estimation, a collection of regression\nbenchmarks, and a scientific case study of ice sheet emulation. We also provide\nempirical results of the quantified uncertainty, showing that the uncertainty\nquantified by HybridFlow is calibrated and better aligns with model error than\nexisting methods for quantifying aleatoric and epistemic uncertainty.\nHybridFlow addresses a key challenge in Bayesian deep learning, unifying\naleatoric and epistemic uncertainty modeling in a single robust framework.",
      "pdf_url": "http://arxiv.org/pdf/2510.05054v1",
      "published": "2025-10-06T17:34:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05054v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games",
      "authors": [
        "Ondřej Kubíček",
        "Viliam Lisý"
      ],
      "abstract": "Test-time reasoning significantly enhances pre-trained AI agents'\nperformance. However, it requires an explicit environment model, often\nunavailable or overly complex in real-world scenarios. While MuZero enables\neffective model learning for search in perfect information games, extending\nthis paradigm to imperfect information games presents substantial challenges\ndue to more nuanced look-ahead reasoning techniques and large number of states\nrelevant for individual decisions. This paper introduces an algorithm LAMIR\nthat learns an abstracted model of an imperfect information game directly from\nthe agent-environment interaction. During test time, this trained model is used\nto perform look-ahead reasoning. The learned abstraction limits the size of\neach subgame to a manageable size, making theoretically principled look-ahead\nreasoning tractable even in games where previous methods could not scale. We\nempirically demonstrate that with sufficient capacity, LAMIR learns the exact\nunderlying game structure, and with limited capacity, it still learns a\nvaluable abstraction, which improves game playing performance of the\npre-trained agents even in large games.",
      "pdf_url": "http://arxiv.org/pdf/2510.05048v1",
      "published": "2025-10-06T17:26:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05048v1",
      "categories": [
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts",
      "authors": [
        "Jihoon Lee",
        "Hoyeon Moon",
        "Kevin Zhai",
        "Arun Kumar Chithanar",
        "Anit Kumar Sahu",
        "Soummya Kar",
        "Chul Lee",
        "Souradip Chakraborty",
        "Amrit Singh Bedi"
      ],
      "abstract": "Diffusion-based large language models (dLLMs) are trained flexibly to model\nextreme dependence in the data distribution; however, how to best utilize this\ninformation at inference time remains an open problem. In this work, we uncover\nan interesting property of these models: dLLMs trained on textual data\nimplicitly learn a mixture of semi-autoregressive experts, where different\ngeneration orders reveal different specialized behaviors. We show that\ncommitting to any single, fixed inference time schedule, a common practice,\ncollapses performance by failing to leverage this latent ensemble. To address\nthis, we introduce HEX (Hidden semiautoregressive EXperts for test-time\nscaling), a training-free inference method that ensembles across heterogeneous\nblock schedules. By doing a majority vote over diverse block-sized generation\npaths, HEX robustly avoids failure modes associated with any single fixed\nschedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to\n3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and\nspecialized fine-tuned methods like GRPO, without additional training. HEX even\nyields significant gains on MATH benchmark from 16.40% to 40.00%, scientific\nreasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.\nOur results establish a new paradigm for test-time scaling in diffusion-based\nLLMs (dLLMs), revealing that the sequence in which masking is performed plays a\ncritical role in determining performance during inference.",
      "pdf_url": "http://arxiv.org/pdf/2510.05040v1",
      "published": "2025-10-06T17:16:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05040v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Graph-Aware Diffusion for Signal Generation",
      "authors": [
        "Sergio Rozada",
        "Vimal K. B.",
        "Andrea Cavallo",
        "Antonio G. Marques",
        "Hadi Jamali-Rad",
        "Elvin Isufi"
      ],
      "abstract": "We study the problem of generating graph signals from unknown distributions\ndefined over given graphs, relevant to domains such as recommender systems or\nsensor networks. Our approach builds on generative diffusion models, which are\nwell established in vision and graph generation but remain underexplored for\ngraph signals. Existing methods lack generality, either ignoring the graph\nstructure in the forward process or designing graph-aware mechanisms tailored\nto specific domains. We adopt a forward process that incorporates the graph\nthrough the heat equation. Rather than relying on the standard formulation, we\nconsider a time-warped coefficient to mitigate the exponential decay of the\ndrift term, yielding a graph-aware generative diffusion model (GAD). We analyze\nits forward dynamics, proving convergence to a Gaussian Markov random field\nwith covariance parametrized by the graph Laplacian, and interpret the backward\ndynamics as a sequence of graph-signal denoising problems. Finally, we\ndemonstrate the advantages of GAD on synthetic data, real traffic speed\nmeasurements, and a temperature sensor network.",
      "pdf_url": "http://arxiv.org/pdf/2510.05036v1",
      "published": "2025-10-06T17:11:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05036v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Imperceptible Jailbreaking against Large Language Models",
      "authors": [
        "Kuofeng Gao",
        "Yiming Li",
        "Chao Du",
        "Xin Wang",
        "Xingjun Ma",
        "Shu-Tao Xia",
        "Tianyu Pang"
      ],
      "abstract": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.",
      "pdf_url": "http://arxiv.org/pdf/2510.05025v1",
      "published": "2025-10-06T17:03:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05025v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective",
      "authors": [
        "Weixin Wang",
        "Haoyang Zheng",
        "Guang Lin",
        "Wei Deng",
        "Pan Xu"
      ],
      "abstract": "Most existing approximate Thompson Sampling (TS) algorithms for multi-armed\nbandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in\neach round to sample from the posterior, relaxing the need for conjugacy\nassumptions between priors and reward distributions in vanilla TS. However,\nthey often require approximating a different posterior distribution in\ndifferent round of the bandit problem. This requires tricky, round-specific\ntuning of hyperparameters such as dynamic learning rates, causing challenges in\nboth theoretical analysis and practical implementation. To alleviate this\nnon-stationarity, we introduce TS-SA, which incorporates stochastic\napproximation (SA) within the TS framework. In each round, TS-SA constructs a\nposterior approximation only using the most recent reward(s), performs a\nLangevin Monte Carlo (LMC) update, and applies an SA step to average noisy\nproposals over time. This can be interpreted as approximating a stationary\nposterior target throughout the entire algorithm, which further yields a fixed\nstep-size, a unified convergence analysis framework, and improved posterior\nestimates through temporal averaging. We establish near-optimal regret bounds\nfor TS-SA, with a simplified and more intuitive theoretical analysis enabled by\ninterpreting the entire algorithm as a simulation of a stationary SGLD process.\nOur empirical results demonstrate that even a single-step Langevin update with\ncertain warm-up outperforms existing methods substantially on bandit tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.05023v1",
      "published": "2025-10-06T17:01:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05023v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad",
      "authors": [
        "Lucas Carrit Delgado Pinheiro",
        "Ziru Chen",
        "Bruno Caixeta Piazza",
        "Ness Shroff",
        "Yingbin Liang",
        "Yuan-Sen Ting",
        "Huan Sun"
      ],
      "abstract": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy.",
      "pdf_url": "http://arxiv.org/pdf/2510.05016v1",
      "published": "2025-10-06T16:58:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05016v1",
      "categories": [
        "astro-ph.IM",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Think Then Embed: Generative Context Improves Multimodal Embedding",
      "authors": [
        "Xuanming Cui",
        "Jianpeng Cheng",
        "Hong-you Chen",
        "Satya Narayan Shukla",
        "Abhijeet Awasthi",
        "Xichen Pan",
        "Chaitanya Ahuja",
        "Shlok Kumar Mishra",
        "Qi Guo",
        "Ser-Nam Lim",
        "Aashu Singh",
        "Xiangjun Fan"
      ],
      "abstract": "There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance.",
      "pdf_url": "http://arxiv.org/pdf/2510.05014v1",
      "published": "2025-10-06T16:53:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05014v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning",
      "authors": [
        "Imran Mansha"
      ],
      "abstract": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2510.05003v1",
      "published": "2025-10-06T16:42:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05003v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Bridging Text and Video Generation: A Survey",
      "authors": [
        "Nilay Kumar",
        "Priyansh Bhandari",
        "G. Maragatham"
      ],
      "abstract": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.",
      "pdf_url": "http://arxiv.org/pdf/2510.04999v1",
      "published": "2025-10-06T16:39:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04999v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis",
      "authors": [
        "Jiongchi Yu",
        "Weipeng Jiang",
        "Xiaoyu Zhang",
        "Qiang Hu",
        "Xiaofei Xie",
        "Chao Shen"
      ],
      "abstract": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis.",
      "pdf_url": "http://arxiv.org/pdf/2510.04997v1",
      "published": "2025-10-06T16:37:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04997v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training",
      "authors": [
        "Wei Xiong",
        "Chenlu Ye",
        "Baohao Liao",
        "Hanze Dong",
        "Xinxing Xu",
        "Christof Monz",
        "Jiang Bian",
        "Nan Jiang",
        "Tong Zhang"
      ],
      "abstract": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.",
      "pdf_url": "http://arxiv.org/pdf/2510.04996v1",
      "published": "2025-10-06T16:34:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04996v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ]
    },
    {
      "title": "AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives",
      "authors": [
        "Khalid Mehtab Khan",
        "Anagha Kulkarni"
      ],
      "abstract": "Identifying cultural capital (CC) themes in student reflections can offer\nvaluable insights that help foster equitable learning environments in\nclassrooms. However, themes such as aspirational goals or family support are\noften woven into narratives, rather than appearing as direct keywords. This\nmakes them difficult to detect for standard NLP models that process sentences\nin isolation. The core challenge stems from a lack of awareness, as standard\nmodels are pre-trained on general corpora, leaving them blind to the\ndomain-specific language and narrative context inherent to the data. To address\nthis, we introduce AWARE, a framework that systematically attempts to improve a\ntransformer model's awareness for this nuanced task. AWARE has three core\ncomponents: 1) Domain Awareness, adapting the model's vocabulary to the\nlinguistic style of student reflections; 2) Context Awareness, generating\nsentence embeddings that are aware of the full essay context; and 3) Class\nOverlap Awareness, employing a multi-label strategy to recognize the\ncoexistence of themes in a single sentence. Our results show that by making the\nmodel explicitly aware of the properties of the input, AWARE outperforms a\nstrong baseline by 2.1 percentage points in Macro-F1 and shows considerable\nimprovements across all themes. This work provides a robust and generalizable\nmethodology for any text classification task in which meaning depends on the\ncontext of the narrative.",
      "pdf_url": "http://arxiv.org/pdf/2510.04983v2",
      "published": "2025-10-06T16:19:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04983v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game",
      "authors": [
        "Fangzhou Liang",
        "Tianshi Zheng",
        "Chunkit Chan",
        "Yauwai Yim",
        "Yangqiu Song"
      ],
      "abstract": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.",
      "pdf_url": "http://arxiv.org/pdf/2510.04980v1",
      "published": "2025-10-06T16:17:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04980v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI",
      "authors": [
        "Kun Xiang",
        "Terry Jingchen Zhang",
        "Yinya Huang",
        "Jixi He",
        "Zirong Liu",
        "Yueling Tang",
        "Ruizhe Zhou",
        "Lijing Luo",
        "Youpeng Wen",
        "Xiuwei Chen",
        "Bingqian Lin",
        "Jianhua Han",
        "Hang Xu",
        "Hanhui Li",
        "Bin Dong",
        "Xiaodan Liang"
      ],
      "abstract": "The rapid advancement of embodied intelligence and world models has\nintensified efforts to integrate physical laws into AI systems, yet physical\nperception and symbolic physics reasoning have developed along separate\ntrajectories without a unified bridging framework. This work provides a\ncomprehensive overview of physical AI, establishing clear distinctions between\ntheoretical physics reasoning and applied physical understanding while\nsystematically examining how physics-grounded methods enhance AI's real-world\ncomprehension across structured symbolic reasoning, embodied systems, and\ngenerative models. Through rigorous analysis of recent advances, we advocate\nfor intelligent systems that ground learning in both physical principles and\nembodied reasoning processes, transcending pattern recognition toward genuine\nunderstanding of physical laws. Our synthesis envisions next-generation world\nmodels capable of explaining physical phenomena and predicting future states,\nadvancing safe, generalizable, and interpretable AI systems. We maintain a\ncontinuously updated resource at\nhttps://github.com/AI4Phys/Awesome-AI-for-Physics.",
      "pdf_url": "http://arxiv.org/pdf/2510.04978v1",
      "published": "2025-10-06T16:16:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04978v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning",
      "authors": [
        "Marcel Wienöbst",
        "Leonard Henckel",
        "Sebastian Weichwald"
      ],
      "abstract": "We present FLOP (Fast Learning of Order and Parents), a score-based causal\ndiscovery algorithm for linear models. It pairs fast parent selection with\niterative Cholesky-based score updates, cutting run-times over prior\nalgorithms. This makes it feasible to fully embrace discrete search, enabling\niterated local search with principled order initialization to find graphs with\nscores at or close to the global optimum. The resulting structures are highly\naccurate across benchmarks, with near-perfect recovery in standard settings.\nThis performance calls for revisiting discrete search over graphs as a\nreasonable approach to causal discovery.",
      "pdf_url": "http://arxiv.org/pdf/2510.04970v1",
      "published": "2025-10-06T16:04:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04970v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "ActiveMark: on watermarking of visual foundation models via massive activations",
      "authors": [
        "Anna Chistyakova",
        "Mikhail Pautov"
      ],
      "abstract": "Being trained on large and vast datasets, visual foundation models (VFMs) can\nbe fine-tuned for diverse downstream tasks, achieving remarkable performance\nand efficiency in various computer vision applications. The high computation\ncost of data collection and training motivates the owners of some VFMs to\ndistribute them alongside the license to protect their intellectual property\nrights. However, a dishonest user of the protected model's copy may illegally\nredistribute it, for example, to make a profit. As a consequence, the\ndevelopment of reliable ownership verification tools is of great importance\ntoday, since such methods can be used to differentiate between a redistributed\ncopy of the protected model and an independent model. In this paper, we propose\nan approach to ownership verification of visual foundation models by\nfine-tuning a small set of expressive layers of a VFM along with a small\nencoder-decoder network to embed digital watermarks into an internal\nrepresentation of a hold-out set of input images. Importantly, the watermarks\nembedded remain detectable in the functional copies of the protected model,\nobtained, for example, by fine-tuning the VFM for a particular downstream task.\nTheoretically and experimentally, we demonstrate that the proposed method\nyields a low probability of false detection of a non-watermarked model and a\nlow probability of false misdetection of a watermarked model.",
      "pdf_url": "http://arxiv.org/pdf/2510.04966v1",
      "published": "2025-10-06T15:58:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04966v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling",
      "authors": [
        "Bi-Cheng Yan",
        "Ming-Kang Tsai",
        "Berlin Chen"
      ],
      "abstract": "Computer-assisted pronunciation training (CAPT) manages to facilitate\nsecond-language (L2) learners to practice pronunciation skills by offering\ntimely and instructive feedback. To examine pronunciation proficiency from\nmultiple facets, existing methods for CAPT broadly fall into two categories:\nmispronunciation detection and diagnosis (MDD) as well as automatic\npronunciation assessment (APA). The former aims to pinpoint phonetic\npronunciation errors and provide diagnostic feedback, while the latter seeks\ninstead to quantify pronunciation proficiency pertaining to various aspects.\nDespite the natural complementarity between MDD and APA, researchers and\npractitioners, however, often treat them as independent tasks with disparate\nmodeling paradigms. In light of this, we in this paper first introduce MuFFIN,\na Multi-Faceted pronunciation Feedback model with an Interactive hierarchical\nNeural architecture, to jointly address the tasks of MDD and APA. To better\ncapture the nuanced distinctions between phonemes in the feature space, a novel\nphoneme-contrastive ordinal regularization mechanism is then put forward to\noptimize the proposed model to generate more phoneme-discriminative features\nwhile factoring in the ordinality of the aspect scores. In addition, to address\nthe intricate data imbalance problem in MDD, we design a simple yet effective\ntraining objective, which is specifically tailored to perturb the outputs of a\nphoneme classifier with the phoneme-specific variations, so as to better render\nthe distribution of predicted phonemes meanwhile considering their\nmispronunciation characteristics. A series of experiments conducted on the\nSpeechocean762 benchmark dataset demonstrates the efficacy of our method in\nrelation to several cutting-edge baselines, showing state-of-the-art\nperformance on both the APA and MDD tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.04956v1",
      "published": "2025-10-06T15:54:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04956v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits",
      "authors": [
        "Ailiya Borjigin",
        "Cong He"
      ],
      "abstract": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment.",
      "pdf_url": "http://arxiv.org/pdf/2510.04952v2",
      "published": "2025-10-06T15:52:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04952v2",
      "categories": [
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints",
      "authors": [
        "Jayanta Mandi",
        "Marianne Defresne",
        "Senne Berden",
        "Tias Guns"
      ],
      "abstract": "When some parameters of a constrained optimization problem (COP) are\nuncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising\ntwo stages -- the prediction of the unknown parameters from contextual\ninformation and the subsequent optimization using those predicted parameters.\nDecision-focused learning (DFL) implements the first stage by training a\nmachine learning (ML) model to optimize the quality of the decisions made using\nthe predicted parameters. When parameters in the constraints of a COP are\npredicted, the predicted parameters can lead to infeasible solutions.\nTherefore, it is important to simultaneously manage both feasibility and\ndecision quality. We develop a DFL framework for predicting constraint\nparameters in a generic COP. While prior works typically assume that the\nunderlying optimization problem is a linear program (LP) or integer linear\nprogram (ILP), our approach makes no such assumption. We derive two novel loss\nfunctions based on maximum likelihood estimation (MLE): the first one penalizes\ninfeasibility (by penalizing when the predicted parameters lead to infeasible\nsolutions), and the second one penalizes suboptimal decisions (by penalizing\nwhen the true optimal solution is infeasible under the predicted parameters).\nWe introduce a single tunable parameter to form a weighted average of the two\nlosses, allowing decision-makers to balance suboptimality and feasibility. We\nexperimentally demonstrate that adjusting this parameter provides a\ndecision-maker the control over the trade-off between the two. Moreover, across\nseveral COP instances, we find that for a single value of the tunable\nparameter, our method matches the performance of the existing baselines on\nsuboptimality and feasibility.",
      "pdf_url": "http://arxiv.org/pdf/2510.04951v1",
      "published": "2025-10-06T15:52:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04951v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)",
      "authors": [
        "Om Dobariya",
        "Akhil Kumar"
      ],
      "abstract": "The wording of natural language prompts has been shown to influence the\nperformance of large language models (LLMs), yet the role of politeness and\ntone remains underexplored. In this study, we investigate how varying levels of\nprompt politeness affect model accuracy on multiple-choice questions. We\ncreated a dataset of 50 base questions spanning mathematics, science, and\nhistory, each rewritten into five tone variants: Very Polite, Polite, Neutral,\nRude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we\nevaluated responses across these conditions and applied paired sample t-tests\nto assess statistical significance. Contrary to expectations, impolite prompts\nconsistently outperformed polite ones, with accuracy ranging from 80.8% for\nVery Polite prompts to 84.8% for Very Rude prompts. These findings differ from\nearlier studies that associated rudeness with poorer outcomes, suggesting that\nnewer LLMs may respond differently to tonal variation. Our results highlight\nthe importance of studying pragmatic aspects of prompting and raise broader\nquestions about the social dimensions of human-AI interaction.",
      "pdf_url": "http://arxiv.org/pdf/2510.04950v1",
      "published": "2025-10-06T15:50:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04950v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "stat.ME"
      ]
    },
    {
      "title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion",
      "authors": [
        "Xin Li",
        "Kaixiang Yang",
        "Qiang Li",
        "Zhiwei Wang"
      ],
      "abstract": "Dual-view mammography, including craniocaudal (CC) and mediolateral oblique\n(MLO) projections, offers complementary anatomical views crucial for breast\ncancer diagnosis. However, in real-world clinical workflows, one view may be\nmissing, corrupted, or degraded due to acquisition errors or compression\nartifacts, limiting the effectiveness of downstream analysis. View-to-view\ntranslation can help recover missing views and improve lesion alignment. Unlike\nnatural images, this task in mammography is highly challenging due to large\nnon-rigid deformations and severe tissue overlap in X-ray projections, which\nobscure pixel-level correspondences. In this paper, we propose Column-Aware and\nImplicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view\ntranslation framework based on conditional diffusion model. To address\ncross-view structural misalignment, we first design a column-aware\ncross-attention mechanism that leverages the geometric property that\nanatomically corresponding regions tend to lie in similar column positions\nacross views. A Gaussian-decayed bias is applied to emphasize local column-wise\ncorrelations while suppressing distant mismatches. Furthermore, we introduce an\nimplicit 3D structure reconstruction module that back-projects noisy 2D latents\ninto a coarse 3D feature volume based on breast-view projection geometry. The\nreconstructed 3D structure is refined and injected into the denoising UNet to\nguide cross-view generation with enhanced anatomical awareness. Extensive\nexperiments demonstrate that CA3D-Diff achieves superior performance in\nbidirectional tasks, outperforming state-of-the-art methods in visual fidelity\nand structural consistency. Furthermore, the synthesized views effectively\nimprove single-view malignancy classification in screening settings,\ndemonstrating the practical value of our method in real-world diagnostics.",
      "pdf_url": "http://arxiv.org/pdf/2510.04947v1",
      "published": "2025-10-06T15:48:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04947v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation",
      "authors": [
        "Juan-José Guzmán-Landa",
        "Juan-Manuel Torres-Moreno",
        "Miguel Figueroa-Saavedra",
        "Ligia Quintana-Torres",
        "Martha-Lorena Avendaño-Garrido",
        "Graham Ranger"
      ],
      "abstract": "In this article we introduce a context-free grammar (CFG) for the Nawatl\nlanguage. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language\ntype, i.e. a language with few digital resources, in which the corpora\navailable for machine learning are virtually non-existent. The objective here\nis to generate a significant number of grammatically correct artificial\nsentences, in order to increase the corpora available for language model\ntraining. We want to show that a grammar enables us significantly to expand a\ncorpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched,\nenables us to train algorithms such as FastText and to evaluate them on\nsentence-level semantic tasks. Preliminary results show that by using the\ngrammar, comparative improvements are achieved over some LLMs. However, it is\nobserved that to achieve more significant improvement, grammars that model the\nNawatl language even more effectively are required.",
      "pdf_url": "http://arxiv.org/pdf/2510.04945v1",
      "published": "2025-10-06T15:46:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04945v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Unsupervised Active Learning via Natural Feature Progressive Framework",
      "authors": [
        "Yuxi Liu",
        "Catherine Lalman",
        "Yimin Yang"
      ],
      "abstract": "The effectiveness of modern deep learning models is predicated on the\navailability of large-scale, human-annotated datasets, a process that is\nnotoriously expensive and time-consuming. While Active Learning (AL) offers a\nstrategic solution by labeling only the most informative and representative\ndata, its iterative nature still necessitates significant human involvement.\nUnsupervised Active Learning (UAL) presents an alternative by shifting the\nannotation burden to a single, post-selection step. Unfortunately, prevailing\nUAL methods struggle to achieve state-of-the-art performance. These approaches\ntypically rely on local, gradient-based scoring for sample importance\nestimation, which not only makes them vulnerable to ambiguous and noisy data\nbut also hinders their capacity to select samples that adequately represent the\nfull data distribution. Moreover, their use of shallow, one-shot linear\nselection falls short of a true UAL paradigm. In this paper, we propose the\nNatural Feature Progressive Framework (NFPF), a UAL method that revolutionizes\nhow sample importance is measured. At its core, NFPF employs a Specific Feature\nLearning Machine (SFLM) to effectively quantify each sample's contribution to\nmodel performance. We further utilize the SFLM to define a powerful\nReconstruction Difference metric for initial sample selection. Our\ncomprehensive experiments show that NFPF significantly outperforms all\nestablished UAL methods and achieves performance on par with supervised AL\nmethods on vision datasets. Detailed ablation studies and qualitative\nvisualizations provide compelling evidence for NFPF's superior performance,\nenhanced robustness, and improved data distribution coverage.",
      "pdf_url": "http://arxiv.org/pdf/2510.04939v1",
      "published": "2025-10-06T15:44:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04939v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures",
      "authors": [
        "Shiwen Qin",
        "Alexander Auras",
        "Shay B. Cohen",
        "Elliot J. Crowley",
        "Michael Moeller",
        "Linus Ericsson",
        "Jovita Lukasik"
      ],
      "abstract": "Neural architecture search (NAS) automates the design process of\nhigh-performing architectures, but remains bottlenecked by expensive\nperformance evaluation. Most existing studies that achieve faster evaluation\nare mostly tied to cell-based search spaces and graph encodings tailored to\nthose individual search spaces, limiting their flexibility and scalability when\napplied to more expressive search spaces. In this work, we aim to close the gap\nof individual search space restrictions and search space dependent network\nrepresentations. We present ONNX-Bench, a benchmark consisting of a collection\nof neural networks in a unified format based on ONNX files. ONNX-Bench includes\nall open-source NAS-bench-based neural networks, resulting in a total size of\nmore than 600k {architecture, accuracy} pairs. This benchmark allows creating a\nshared neural network representation, ONNX-Net, able to represent any neural\narchitecture using natural language descriptions acting as an input to a\nperformance predictor. This text-based encoding can accommodate arbitrary layer\ntypes, operation parameters, and heterogeneous topologies, enabling a single\nsurrogate to generalise across all neural architectures rather than being\nconfined to cell-based search spaces. Experiments show strong zero-shot\nperformance across disparate search spaces using only a small amount of\npretraining samples, enabling the unprecedented ability to evaluate any neural\nnetwork architecture instantly.",
      "pdf_url": "http://arxiv.org/pdf/2510.04938v1",
      "published": "2025-10-06T15:43:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04938v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning",
      "authors": [
        "Guoxin Chen",
        "Zile Qiao",
        "Wenqing Wang",
        "Donglei Yu",
        "Xuanzhong Chen",
        "Hao Sun",
        "Minpeng Liao",
        "Kai Fan",
        "Yong Jiang",
        "Penguin Xie",
        "Wayne Xin Zhao",
        "Ruihua Song",
        "Fei Huang"
      ],
      "abstract": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments.",
      "pdf_url": "http://arxiv.org/pdf/2510.04935v1",
      "published": "2025-10-06T15:42:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04935v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "AURA Score: A Metric For Holistic Audio Question Answering Evaluation",
      "authors": [
        "Satvik Dixit",
        "Soham Deshmukh",
        "Bhiksha Raj"
      ],
      "abstract": "Audio Question Answering (AQA) is a key task for evaluating Audio-Language\nModels (ALMs), yet assessing open-ended responses remains challenging. Existing\nmetrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from\nNLP and audio captioning, rely on surface similarity and fail to account for\nquestion context, reasoning, and partial correctness. To address the gap in\nliterature, we make three contributions in this work. First, we introduce\nAQEval to enable systematic benchmarking of AQA metrics. It is the first\nbenchmark of its kind, consisting of 10k model responses annotated by multiple\nhumans for their correctness and relevance. Second, we conduct a comprehensive\nanalysis of existing AQA metrics on AQEval, highlighting weak correlation with\nhuman judgment, especially for longer answers. Third, we propose a new metric -\nAURA score, to better evaluate open-ended model responses. On AQEval, AURA\nachieves state-of-the-art correlation with human ratings, significantly\noutperforming all baselines. Through this work, we aim to highlight the\nlimitations of current AQA evaluation methods and motivate better metrics. We\nrelease both the AQEval benchmark and the AURA metric to support future\nresearch in holistic AQA evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2510.04934v1",
      "published": "2025-10-06T15:41:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04934v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models",
      "authors": [
        "Amir Hameed Mir"
      ],
      "abstract": "Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models.",
      "pdf_url": "http://arxiv.org/pdf/2510.04933v1",
      "published": "2025-10-06T15:41:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04933v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "cs.NE",
        "math.IT",
        "68T50, 68T07, 62H30",
        "I.2.7; I.2.6; F.2.2; H.3.3"
      ]
    },
    {
      "title": "Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data",
      "authors": [
        "Usman Akram",
        "Yiyue Chen",
        "Haris Vikalo"
      ],
      "abstract": "Training automatic modulation classification (AMC) models on centrally\naggregated data raises privacy concerns, incurs communication overhead, and\noften fails to confer robustness to channel shifts. Federated learning (FL)\navoids central aggregation by training on distributed clients but remains\nsensitive to class imbalance, non-IID client distributions, and limited labeled\nsamples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with\ntriplet-loss self-supervision on unlabeled I/Q sequences across clients,\nfollowed by per-client SVMs on small labeled sets. We establish convergence of\nthe federated representation learning procedure and a separability guarantee\nfor the downstream classifier under feature noise. Experiments on synthetic and\nover-the-air datasets show consistent gains over supervised FL baselines under\nheterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.",
      "pdf_url": "http://arxiv.org/pdf/2510.04927v1",
      "published": "2025-10-06T15:37:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04927v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ]
    },
    {
      "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis",
      "authors": [
        "Alec K. Peltekian",
        "Halil Ertugrul Aktas",
        "Gorkem Durak",
        "Kevin Grudzinski",
        "Bradford C. Bemiss",
        "Carrie Richardson",
        "Jane E. Dematte",
        "G. R. Scott Budinger",
        "Anthony J. Esposito",
        "Alexander Misharin",
        "Alok Choudhary",
        "Ankit Agrawal",
        "Ulas Bagci"
      ],
      "abstract": "Mixture-of-Experts (MoE) architectures have significantly contributed to\nscalable machine learning by enabling specialized subnetworks to tackle complex\ntasks efficiently. However, traditional MoE systems lack domain-specific\nconstraints essential for medical imaging, where anatomical structure and\nregional disease heterogeneity strongly influence pathological patterns. Here,\nwe introduce Regional Expert Networks (REN), the first anatomically-informed\nMoE framework tailored specifically for medical image classification. REN\nleverages anatomical priors to train seven specialized experts, each dedicated\nto distinct lung lobes and bilateral lung combinations, enabling precise\nmodeling of region-specific pathological variations. Multi-modal gating\nmechanisms dynamically integrate radiomics biomarkers and deep learning (DL)\nfeatures (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to\ninterstitial lung disease (ILD) classification, REN achieves consistently\nsuperior performance: the radiomics-guided ensemble reached an average AUC of\n0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC\n0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe\nmodels achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)\nand aligning with known disease progression patterns. Through rigorous\npatient-level cross-validation, REN demonstrates strong generalizability and\nclinical interpretability, presenting a scalable, anatomically-guided approach\nreadily extensible to other structured medical imaging applications.",
      "pdf_url": "http://arxiv.org/pdf/2510.04923v1",
      "published": "2025-10-06T15:35:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04923v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment",
      "authors": [
        "Davood Rafiei",
        "Morgan Lindsay Heisler",
        "Weiwei Zhang",
        "Mohammadreza Pourreza",
        "Yong Zhang"
      ],
      "abstract": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.04919v1",
      "published": "2025-10-06T15:33:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04919v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ]
    },
    {
      "title": "Glocal Information Bottleneck for Time Series Imputation",
      "authors": [
        "Jie Yang",
        "Kexin Zhang",
        "Guibin Zhang",
        "Philip S. Yu",
        "Kaize Ding"
      ],
      "abstract": "Time Series Imputation (TSI), which aims to recover missing values in\ntemporal data, remains a fundamental challenge due to the complex and often\nhigh-rate missingness in real-world scenarios. Existing models typically\noptimize the point-wise reconstruction loss, focusing on recovering numerical\nvalues (local information). However, we observe that under high missing rates,\nthese models still perform well in the training phase yet produce poor\nimputations and distorted latent representation distributions (global\ninformation) in the inference phase. This reveals a critical optimization\ndilemma: current objectives lack global guidance, leading models to overfit\nlocal noise and fail to capture global information of the data. To address this\nissue, we propose a new training paradigm, Glocal Information Bottleneck\n(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework\nby introducing a Global Alignment loss, derived from a tractable mutual\ninformation approximation. This loss aligns the latent representations of\nmasked inputs with those of their originally observed counterparts. It helps\nthe model retain global structure and local details while suppressing noise\ncaused by missing values, giving rise to better generalization under high\nmissingness. Extensive experiments on nine datasets confirm that Glocal-IB\nleads to consistently improved performance and aligned latent representations\nunder missingness. Our code implementation is available in\nhttps://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.",
      "pdf_url": "http://arxiv.org/pdf/2510.04910v1",
      "published": "2025-10-06T15:24:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04910v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects",
      "authors": [
        "Jonathan Colaço Carr",
        "Qinyi Sun",
        "Cameron Allen"
      ],
      "abstract": "Skills are essential for unlocking higher levels of problem solving. A common\napproach to discovering these skills is to learn ones that reliably reach\ndifferent states, thus empowering the agent to control its environment.\nHowever, existing skill discovery algorithms often overlook the natural state\nvariables present in many reinforcement learning problems, meaning that the\ndiscovered skills lack control of specific state variables. This can\nsignificantly hamper exploration efficiency, make skills more challenging to\nlearn with, and lead to negative side effects in downstream tasks when the goal\nis under-specified. We introduce a general method that enables these skill\ndiscovery algorithms to learn focused skills -- skills that target and control\nspecific state variables. Our approach improves state space coverage by a\nfactor of three, unlocks new learning capabilities, and automatically avoids\nnegative side effects in downstream tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.04901v1",
      "published": "2025-10-06T15:17:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04901v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding",
      "authors": [
        "Keane Ong",
        "Wei Dai",
        "Carol Li",
        "Dewei Feng",
        "Hengzhi Li",
        "Jingyao Wu",
        "Jiaee Cheong",
        "Rui Mao",
        "Gianmarco Mengaldo",
        "Erik Cambria",
        "Paul Pu Liang"
      ],
      "abstract": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains.",
      "pdf_url": "http://arxiv.org/pdf/2510.04899v1",
      "published": "2025-10-06T15:16:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04899v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
      "authors": [
        "Zheng Xiong",
        "Kang Li",
        "Zilin Wang",
        "Matthew Jackson",
        "Jakob Foerster",
        "Shimon Whiteson"
      ],
      "abstract": "Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA",
      "pdf_url": "http://arxiv.org/pdf/2510.04898v1",
      "published": "2025-10-06T15:15:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04898v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests",
      "authors": [
        "Punya Syon Pandey",
        "Hai Son Le",
        "Devansh Bhardwaj",
        "Rada Mihalcea",
        "Zhijing Jin"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench.",
      "pdf_url": "http://arxiv.org/pdf/2510.04891v1",
      "published": "2025-10-06T15:11:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04891v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models",
      "authors": [
        "Alina Ermilova",
        "Dmitrii Kornilov",
        "Sofia Samoilova",
        "Ekaterina Laptenkova",
        "Anastasia Kolesnikova",
        "Ekaterina Podplutova",
        "Senotrusova Sofya",
        "Maksim G. Sharaev"
      ],
      "abstract": "Identifying disease interconnections through manual analysis of large-scale\nclinical data is labor-intensive, subjective, and prone to expert disagreement.\nWhile machine learning (ML) shows promise, three critical challenges remain:\n(1) selecting optimal methods from the vast ML landscape, (2) determining\nwhether real-world clinical data (e.g., electronic health records, EHRs) or\nstructured disease descriptions yield more reliable insights, (3) the lack of\n\"ground truth,\" as some disease interconnections remain unexplored in medicine.\nLarge language models (LLMs) demonstrate broad utility, yet they often lack\nspecialized medical knowledge. To address these gaps, we conduct a systematic\nevaluation of seven approaches for uncovering disease relationships based on\ntwo data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the\nfull set of ICD-10 codes, both with and without textual descriptions. Our\nframework integrates the following: (i) a statistical co-occurrence analysis\nand a masked language modeling (MLM) approach using real clinical data; (ii)\ndomain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a\ngeneral-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,\nDeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained\ninterconnection matrices shows that the LLM-based approach produces\ninterconnections with the lowest diversity of ICD code connections to different\ndiseases compared to other methods, including text-based and domain-based\napproaches. This suggests an important implication: LLMs have limited potential\nfor discovering new interconnections. In the absence of ground truth databases\nfor medical interconnections between ICD codes, our results constitute a\nvaluable medical disease ontology that can serve as a foundational resource for\nfuture clinical research and artificial intelligence applications in\nhealthcare.",
      "pdf_url": "http://arxiv.org/pdf/2510.04888v1",
      "published": "2025-10-06T15:09:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04888v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution",
      "authors": [
        "Adi Banerjee",
        "Anirudh Nair",
        "Tarik Borogovac"
      ],
      "abstract": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2510.04886v1",
      "published": "2025-10-06T15:07:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04886v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Less is More: Recursive Reasoning with Tiny Networks",
      "authors": [
        "Alexia Jolicoeur-Martineau"
      ],
      "abstract": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters.",
      "pdf_url": "http://arxiv.org/pdf/2510.04871v1",
      "published": "2025-10-06T14:58:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04871v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing",
      "authors": [
        "Seyed Soroush Karimi Madahi",
        "Kenneth Bruninx",
        "Bert Claessens",
        "Chris Develder"
      ],
      "abstract": "In Europe, profit-seeking balance responsible parties can deviate in real\ntime from their day-ahead nominations to assist transmission system operators\nin maintaining the supply-demand balance. Model predictive control (MPC)\nstrategies to exploit these implicit balancing strategies capture arbitrage\nopportunities, but fail to accurately capture the price-formation process in\nthe European imbalance markets and face high computational costs. Model-free\nreinforcement learning (RL) methods are fast to execute, but require\ndata-intensive training and usually rely on real-time and historical data for\ndecision-making. This paper proposes an MPC-guided RL method that combines the\ncomplementary strengths of both MPC and RL. The proposed method can effectively\nincorporate forecasts into the decision-making process (as in MPC), while\nmaintaining the fast inference capability of RL. The performance of the\nproposed method is evaluated on the implicit balancing battery control problem\nusing Belgian balancing data from 2023. First, we analyze the performance of\nthe standalone state-of-the-art RL and MPC methods from various angles, to\nhighlight their individual strengths and limitations. Next, we show an\narbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and\n54.36%, compared to standalone RL and MPC.",
      "pdf_url": "http://arxiv.org/pdf/2510.04868v1",
      "published": "2025-10-06T14:52:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.04868v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ]
    }
  ]
}