{
  "last_updated": "2025-11-26T00:52:37.426718",
  "papers": [
    {
      "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
      "authors": [
        "Qiang Wang",
        "Xinyuan Gao",
        "SongLin Dong",
        "Jizhou Han",
        "Jiangyang Li",
        "Yuhang He",
        "Yihong Gong"
      ],
      "abstract": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.",
      "pdf_url": "https://arxiv.org/pdf/2511.19436v1",
      "published": "2025-11-24T18:59:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19436v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "Mixture of Horizons in Action Chunking",
      "authors": [
        "Dong Jing",
        "Gang Wang",
        "Jiaqi Liu",
        "Weiliang Tang",
        "Zelong Sun",
        "Yunchao Yao",
        "Zhenyu Wei",
        "Yunhui Liu",
        "Zhiwu Lu",
        "Mingyu Ding"
      ],
      "abstract": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
      "pdf_url": "https://arxiv.org/pdf/2511.19433v1",
      "published": "2025-11-24T18:59:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19433v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering",
      "authors": [
        "Jayanaka L. Dantanarayana",
        "Savini Kashmira",
        "Thakee Nathees",
        "Zichen Zhang",
        "Krisztian Flautner",
        "Lingjia Tang",
        "Jason Mars"
      ],
      "abstract": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.",
      "pdf_url": "https://arxiv.org/pdf/2511.19427v1",
      "published": "2025-11-24T18:58:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19427v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design",
      "authors": [
        "Bruno Jacob",
        "Khushbu Agarwal",
        "Marcel Baer",
        "Peter Rice",
        "Simone Raugei"
      ],
      "abstract": "We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.",
      "pdf_url": "https://arxiv.org/pdf/2511.19423v1",
      "published": "2025-11-24T18:57:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19423v1",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ]
    },
    {
      "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning",
      "authors": [
        "David Jiahao Fu",
        "Aryan Gupta",
        "Aaron Councilman",
        "David Grove",
        "Yu-Xiong Wang",
        "Vikram Adve"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.",
      "pdf_url": "https://arxiv.org/pdf/2511.19422v1",
      "published": "2025-11-24T18:56:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19422v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ]
    },
    {
      "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
      "authors": [
        "Yiming Qin",
        "Bomin Wei",
        "Jiaxin Ge",
        "Konstantinos Kallidromitis",
        "Stephanie Fu",
        "Trevor Darrell",
        "Xudong Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
      "pdf_url": "https://arxiv.org/pdf/2511.19418v1",
      "published": "2025-11-24T18:55:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19418v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration",
      "authors": [
        "James Y. Huang",
        "Sheng Zhang",
        "Qianchu Liu",
        "Guanghui Qin",
        "Tinghui Zhu",
        "Tristan Naumann",
        "Muhao Chen",
        "Hoifung Poon"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.19417v1",
      "published": "2025-11-24T18:55:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19417v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary",
      "authors": [
        "Zhaolong Su",
        "Wang Lu",
        "Hao Chen",
        "Sharon Li",
        "Jindong Wang"
      ],
      "abstract": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame",
      "pdf_url": "https://arxiv.org/pdf/2511.19413v1",
      "published": "2025-11-24T18:50:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19413v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "In-Video Instructions: Visual Signals as Generative Control",
      "authors": [
        "Gongfan Fang",
        "Xinyin Ma",
        "Xinchao Wang"
      ],
      "abstract": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
      "pdf_url": "https://arxiv.org/pdf/2511.19401v1",
      "published": "2025-11-24T18:38:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19401v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
      "authors": [
        "Rulin Shao",
        "Akari Asai",
        "Shannon Zejiang Shen",
        "Hamish Ivison",
        "Varsha Kishore",
        "Jingming Zhuo",
        "Xinran Zhao",
        "Molly Park",
        "Samuel G. Finlayson",
        "David Sontag",
        "Tyler Murray",
        "Sewon Min",
        "Pradeep Dasigi",
        "Luca Soldaini",
        "Faeze Brahman",
        "Wen-tau Yih",
        "Tongshuang Wu",
        "Luke Zettlemoyer",
        "Yoon Kim",
        "Hannaneh Hajishirzi",
        "Pang Wei Koh"
      ],
      "abstract": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.19399v1",
      "published": "2025-11-24T18:35:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19399v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments",
      "authors": [
        "Jorge Ortigoso-Narro",
        "Jose A. Belloch",
        "Adrian Amor-Martin",
        "Sandra Roger",
        "Maximo Cobos"
      ],
      "abstract": "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.",
      "pdf_url": "https://arxiv.org/pdf/2511.19396v1",
      "published": "2025-11-24T18:33:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19396v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme",
      "authors": [
        "Rudy Morel",
        "Francesco Pio Ramunno",
        "Jeff Shen",
        "Alberto Bietti",
        "Kyunghyun Cho",
        "Miles Cranmer",
        "Siavash Golkar",
        "Olexandr Gugnin",
        "Geraud Krawezik",
        "Tanya Marwah",
        "Michael McCabe",
        "Lucas Meyer",
        "Payel Mukhopadhyay",
        "Ruben Ohana",
        "Liam Parker",
        "Helen Qu",
        "François Rozet",
        "K. D. Leka",
        "François Lanusse",
        "David Fouhey",
        "Shirley Ho"
      ],
      "abstract": "Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.",
      "pdf_url": "https://arxiv.org/pdf/2511.19390v1",
      "published": "2025-11-24T18:30:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19390v1",
      "categories": [
        "cs.LG",
        "astro-ph.SR",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification",
      "authors": [
        "Saniah Kayenat Chowdhury",
        "Rusab Sarmun",
        "Muhammad E. H. Chowdhury",
        "Sohaib Bassam Zoghoul",
        "Israa Al-Hashimi",
        "Adam Mushtak",
        "Amith Khandakar"
      ],
      "abstract": "Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable \"black box\" manner, our method offers both state-of-the-art performance and transparent decision support.",
      "pdf_url": "https://arxiv.org/pdf/2511.19367v1",
      "published": "2025-11-24T18:01:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19367v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
      "authors": [
        "Zehong Ma",
        "Longhui Wei",
        "Shuai Wang",
        "Shiliang Zhang",
        "Qi Tian"
      ],
      "abstract": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
      "pdf_url": "https://arxiv.org/pdf/2511.19365v1",
      "published": "2025-11-24T17:59:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19365v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks",
      "authors": [
        "Franklin Cardenoso",
        "Wouter Caarls"
      ],
      "abstract": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.",
      "pdf_url": "https://arxiv.org/pdf/2511.19355v1",
      "published": "2025-11-24T17:55:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19355v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation",
      "authors": [
        "Maral Ebrahimzadeh",
        "Gilberto Bernardes",
        "Sebastian Stober"
      ],
      "abstract": "State-of-the-art symbolic music generation models have recently achieved remarkable output quality, yet explicit control over compositional features, such as tonal tension, remains challenging. We propose a novel approach that integrates a computational tonal tension model, based on tonal interval vector analysis, into a Transformer framework. Our method employs a two-level beam search strategy during inference. At the token level, generated candidates are re-ranked using model probability and diversity metrics to maintain overall quality. At the bar level, a tension-based re-ranking is applied to ensure that the generated music aligns with a desired tension curve. Objective evaluations indicate that our approach effectively modulates tonal tension, and subjective listening tests confirm that the system produces outputs that align with the target tension. These results demonstrate that explicit tension conditioning through a dual-level beam search provides a powerful and intuitive tool to guide AI-generated music. Furthermore, our experiments demonstrate that our method can generate multiple distinct musical interpretations under the same tension condition.",
      "pdf_url": "https://arxiv.org/pdf/2511.19342v1",
      "published": "2025-11-24T17:41:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19342v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
      "authors": [
        "Olivia Macmillan-Scott",
        "Roksana Goworek",
        "Eda B. Özyiğit"
      ],
      "abstract": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
      "pdf_url": "https://arxiv.org/pdf/2511.19325v1",
      "published": "2025-11-24T17:18:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19325v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models",
      "authors": [
        "Roksana Goworek",
        "Olivia Macmillan-Scott",
        "Eda B. Özyiğit"
      ],
      "abstract": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.",
      "pdf_url": "https://arxiv.org/pdf/2511.19324v1",
      "published": "2025-11-24T17:17:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19324v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach",
      "authors": [
        "Xincheng Wang",
        "Hanchi Sun",
        "Wenjun Sun",
        "Kejun Xue",
        "Wangqiu Zhou",
        "Jianbo Zhang",
        "Wei Sun",
        "Dandan Zhu",
        "Xiongkuo Min",
        "Jun Jia",
        "Zhijun Fang"
      ],
      "abstract": "Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.",
      "pdf_url": "https://arxiv.org/pdf/2511.19316v1",
      "published": "2025-11-24T17:11:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19316v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
      "authors": [
        "Jaewoo Lee",
        "Archiki Prasad",
        "Justin Chih-Yao Chen",
        "Zaid Khan",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
      "pdf_url": "https://arxiv.org/pdf/2511.19314v1",
      "published": "2025-11-24T17:09:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19314v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
      "authors": [
        "Jiayi Zhang",
        "Yiran Peng",
        "Fanqi Kong",
        "Yang Cheng",
        "Yifan Wu",
        "Zhaoyang Yu",
        "Jinyu Xiang",
        "Jianhao Ruan",
        "Jinlin Wang",
        "Maojia Song",
        "HongZhang Liu",
        "Xiangru Tang",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo"
      ],
      "abstract": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
      "pdf_url": "https://arxiv.org/pdf/2511.19304v1",
      "published": "2025-11-24T16:54:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19304v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning",
      "authors": [
        "James R. M. Black",
        "Moritz S. Hanke",
        "Aaron Maiwald",
        "Tina Hernandez-Boussard",
        "Oliver M. Crook",
        "Jaspreet Pannu"
      ],
      "abstract": "Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.",
      "pdf_url": "https://arxiv.org/pdf/2511.19299v1",
      "published": "2025-11-24T16:46:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19299v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Data Flows and Colonial Regimes in Africa: A Critical Analysis of the Colonial Futurities Embedded in AI Ecosystems",
      "authors": [
        "Ndaka. A",
        "Avila-Acosta. F",
        "Mbula-Ndaka. H",
        "Amera. C",
        "Chauke. S",
        "Majiwa. E"
      ],
      "abstract": "This chapter seeks to frame the elemental and invisible problems of AI and big data in the African context by examining digital sites and infrastructure through the lens of power and interests. It will present reflections on how these sites are using AI recommendation algorithms to recreate new digital societies in the region, how they have the potential to propagate algorithmic colonialism and negative gender norms, and what this means for the regional sustainable development agenda. The chapter proposes adopting business models that embrace response-ability and consider the existence of alternative socio-material worlds of AI. These reflections will mainly come from ongoing discussions with Kenyan social media users in this authors' user space talks, personal experiences and six months of active participant observations done by the authors.",
      "pdf_url": "https://arxiv.org/pdf/2511.19283v1",
      "published": "2025-11-24T16:31:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19283v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization",
      "authors": [
        "Ellie L. Zhang",
        "Duoduo Liao",
        "Callie C. Liao"
      ],
      "abstract": "Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design. Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments. Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs. To address these challenges, we present a novel, fully algorithm-driven framework that generates dynamic multi-species bird soundscapes using DSP-based chirp generation and 3D spatialization, without relying on recordings or training data. Our approach simulates multiple independently-moving birds per species along different moving 3D trajectories, supporting controllable chirp sequences, overlapping choruses, and realistic 3D motion in scalable soundscapes while preserving species-specific acoustic patterns. A visualization interface provides bird trajectories, spectrograms, activity timelines, and sound waves for analytical and creative purposes. Both visual and audio evaluations demonstrate the ability of the system to generate dense, immersive, and ecologically inspired soundscapes, highlighting its potential for computer music, interactive virtual environments, and computational bioacoustics research.",
      "pdf_url": "https://arxiv.org/pdf/2511.19275v1",
      "published": "2025-11-24T16:25:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19275v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ]
    },
    {
      "title": "Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry",
      "authors": [
        "Amirtha Varshini A S",
        "Duminda S. Ranasinghe",
        "Hok Hei Tam"
      ],
      "abstract": "Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.",
      "pdf_url": "https://arxiv.org/pdf/2511.19264v1",
      "published": "2025-11-24T16:16:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19264v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ]
    },
    {
      "title": "Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention",
      "authors": [
        "Lucas Li",
        "Jean-Baptiste Puel",
        "Florence Carton",
        "Dounya Barrit",
        "Jhony H. Giraldo"
      ],
      "abstract": "Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.",
      "pdf_url": "https://arxiv.org/pdf/2511.19263v1",
      "published": "2025-11-24T16:15:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19263v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Psychometric Tests for AI Agents and Their Moduli Space",
      "authors": [
        "Przemyslaw Chojecki"
      ],
      "abstract": "We develop a moduli-theoretic view of psychometric test batteries for AI agents and connect it explicitly to the AAI score developed previously. First, we make precise the notion of an AAI functional on a battery and set out axioms that any reasonable autonomy/general intelligence score should satisfy. Second, we show that the composite index ('AAI-Index') defined previously is a special case of our AAI functional. Third, we introduce the notion of a cognitive core of an agent relative to a battery and define the associated AAI$_{\\textrm{core}}$ score as the restriction of an AAI functional to that core. Finally, we use these notions to describe invariants of batteries under evaluation-preserving symmetries and outline how moduli of equivalent batteries are organized.",
      "pdf_url": "https://arxiv.org/pdf/2511.19262v1",
      "published": "2025-11-24T16:15:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19262v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.ST"
      ]
    },
    {
      "title": "A Nutrition Multimodal Photoplethysmography Language Model",
      "authors": [
        "Kyle Verrier",
        "Achille Nazaret",
        "Joseph Futoma",
        "Andrew C. Miller",
        "Guillermo Sapiro"
      ],
      "abstract": "Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.",
      "pdf_url": "https://arxiv.org/pdf/2511.19260v1",
      "published": "2025-11-24T16:12:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19260v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation",
      "authors": [
        "Yingjia Shang",
        "Yi Liu",
        "Huimin Wang",
        "Furong Li",
        "Wenfang Sun",
        "Wu Chengyu",
        "Yefeng Zheng"
      ],
      "abstract": "With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.",
      "pdf_url": "https://arxiv.org/pdf/2511.19257v1",
      "published": "2025-11-24T16:11:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19257v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SimDiff: Simpler Yet Better Diffusion Model for Time Series Point Forecasting",
      "authors": [
        "Hang Ding",
        "Xue Wang",
        "Tian Zhou",
        "Tao Yao"
      ],
      "abstract": "Diffusion models have recently shown promise in time series forecasting, particularly for probabilistic predictions. However, they often fail to achieve state-of-the-art point estimation performance compared to regression-based methods. This limitation stems from difficulties in providing sufficient contextual bias to track distribution shifts and in balancing output diversity with the stability and precision required for point forecasts. Existing diffusion-based approaches mainly focus on full-distribution modeling under probabilistic frameworks, often with likelihood maximization objectives, while paying little attention to dedicated strategies for high-accuracy point estimation. Moreover, other existing point prediction diffusion methods frequently rely on pre-trained or jointly trained mature models for contextual bias, sacrificing the generative flexibility of diffusion models.\n  To address these challenges, we propose SimDiff, a single-stage, end-to-end framework. SimDiff employs a single unified Transformer network carefully tailored to serve as both denoiser and predictor, eliminating the need for external pre-trained or jointly trained regressors. It achieves state-of-the-art point estimation performance by leveraging intrinsic output diversity and improving mean squared error accuracy through multiple inference ensembling. Key innovations, including normalization independence and the median-of-means estimator, further enhance adaptability and stability. Extensive experiments demonstrate that SimDiff significantly outperforms existing methods in time series point forecasting.",
      "pdf_url": "https://arxiv.org/pdf/2511.19256v1",
      "published": "2025-11-24T16:09:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19256v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation",
      "authors": [
        "Mohamed Rissal Hedna",
        "Sesugh Samuel Nder"
      ],
      "abstract": "Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.",
      "pdf_url": "https://arxiv.org/pdf/2511.19254v1",
      "published": "2025-11-24T16:05:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19254v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization",
      "authors": [
        "Boyuan Wu"
      ],
      "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.",
      "pdf_url": "https://arxiv.org/pdf/2511.19253v1",
      "published": "2025-11-24T16:05:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19253v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Neural Architecture Search for Quantum Autoencoders",
      "authors": [
        "Hibah Agha",
        "Samuel Yen-Chi Chen",
        "Huan-Hsin Tseng",
        "Shinjae Yoo"
      ],
      "abstract": "In recent years, machine learning and deep learning have driven advances in domains such as image classification, speech recognition, and anomaly detection by leveraging multi-layer neural networks to model complex data. Simultaneously, quantum computing (QC) promises to address classically intractable problems via quantum parallelism, motivating research in quantum machine learning (QML). Among QML techniques, quantum autoencoders show promise for compressing high-dimensional quantum and classical data. However, designing effective quantum circuit architectures for quantum autoencoders remains challenging due to the complexity of selecting gates, arranging circuit layers, and tuning parameters.\n  This paper proposes a neural architecture search (NAS) framework that automates the design of quantum autoencoders using a genetic algorithm (GA). By systematically evolving variational quantum circuit (VQC) configurations, our method seeks to identify high-performing hybrid quantum-classical autoencoders for data reconstruction without becoming trapped in local minima. We demonstrate effectiveness on image datasets, highlighting the potential of quantum autoencoders for efficient feature extraction within a noise-prone, near-term quantum era. Our approach lays a foundation for broader application of genetic algorithms to quantum architecture search, aiming for a robust, automated method that can adapt to varied data and hardware constraints.",
      "pdf_url": "https://arxiv.org/pdf/2511.19246v1",
      "published": "2025-11-24T15:55:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19246v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ]
    },
    {
      "title": "Local Entropy Search over Descent Sequences for Bayesian Optimization",
      "authors": [
        "David Stenger",
        "Armin Lindicke",
        "Alexander von Rohr",
        "Sebastian Trimpe"
      ],
      "abstract": "Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.",
      "pdf_url": "https://arxiv.org/pdf/2511.19241v1",
      "published": "2025-11-24T15:52:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19241v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control",
      "authors": [
        "Yuxuan Wang",
        "Haobin Jiang",
        "Shiqing Yao",
        "Ziluo Ding",
        "Zongqing Lu"
      ],
      "abstract": "Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.",
      "pdf_url": "https://arxiv.org/pdf/2511.19236v1",
      "published": "2025-11-24T15:48:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19236v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations",
      "authors": [
        "Christos-Nikolaos Zacharopoulos",
        "Revekka Kyriakoglou"
      ],
      "abstract": "How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.",
      "pdf_url": "https://arxiv.org/pdf/2511.19232v1",
      "published": "2025-11-24T15:43:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19232v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Learning Plug-and-play Memory for Guiding Video Diffusion Models",
      "authors": [
        "Selena Song",
        "Ziming Xu",
        "Zijun Zhang",
        "Kun Zhou",
        "Jiaxian Guo",
        "Lianhui Qin",
        "Biwei Huang"
      ],
      "abstract": "Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.",
      "pdf_url": "https://arxiv.org/pdf/2511.19229v1",
      "published": "2025-11-24T15:42:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19229v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering",
      "authors": [
        "Federico Felizzi",
        "Olivia Riccomi",
        "Michele Ferramola",
        "Francesco Andrea Causio",
        "Manuel Del Medico",
        "Vittorio De Vita",
        "Lorenzo De Mori",
        "Alessandra Piscitelli Pietro Eric Risuleo",
        "Bianca Destro Castaniti",
        "Antonio Cristiano Alessia Longo",
        "Luigi De Angelis",
        "Mariapia Vassalli",
        "Marcello Di Pumpo"
      ],
      "abstract": "Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.",
      "pdf_url": "https://arxiv.org/pdf/2511.19220v1",
      "published": "2025-11-24T15:26:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19220v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization",
      "authors": [
        "Xurui Li",
        "Kaisong Song",
        "Rui Zhu",
        "Pin-Yu Chen",
        "Haixu Tang"
      ],
      "abstract": "Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems.",
      "pdf_url": "https://arxiv.org/pdf/2511.19218v1",
      "published": "2025-11-24T15:23:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19218v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "CLASH: A Benchmark for Cross-Modal Contradiction Detection",
      "authors": [
        "Teodora Popordanoska",
        "Jiameng Li",
        "Matthew B. Blaschko"
      ],
      "abstract": "Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.",
      "pdf_url": "https://arxiv.org/pdf/2511.19199v1",
      "published": "2025-11-24T15:09:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19199v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement",
      "authors": [
        "Lakshaditya Singh",
        "Adwait Shelke",
        "Divyansh Agrawal"
      ],
      "abstract": "Designing new protein structures is fundamental to computational biology, enabling advances in therapeutic molecule discovery and enzyme engineering. Existing diffusion-based generative models typically operate in Cartesian coordinate space, where adding noise disrupts strict geometric constraints such as fixed bond lengths and angles, often producing physically invalid structures. To address this limitation, we propose a Torsion-Space Diffusion Model that generates protein backbones by denoising torsion angles, ensuring perfect local geometry by construction. A differentiable forward-kinematics module reconstructs 3D coordinates with fixed 3.8 Angstrom backbone bond lengths while a constrained post-processing refinement optimizes global compactness via Radius of Gyration (Rg) correction, without violating bond constraints. Experiments on standard PDB proteins demonstrate 100% bond-length accuracy and significantly improved structural compactness, reducing Rg error from 70% to 18.6% compared to Cartesian diffusion baselines. Overall, this hybrid torsion-diffusion plus geometric-refinement framework generates physically valid and compact protein backbones, providing a promising path toward full-atom protein generation.",
      "pdf_url": "https://arxiv.org/pdf/2511.19184v1",
      "published": "2025-11-24T14:51:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19184v1",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ]
    },
    {
      "title": "LLM-Based Agentic Negotiation for 6G: Addressing Uncertainty Neglect and Tail-Event Risk",
      "authors": [
        "Hatim Chergui",
        "Farhad Rezazadeh",
        "Mehdi Bennis",
        "Merouane Debbah"
      ],
      "abstract": "A critical barrier to the trustworthiness of sixth-generation (6G) agentic autonomous networks is the uncertainty neglect bias; a cognitive tendency for large language model (LLM)-powered agents to make high-stakes decisions based on simple averages while ignoring the tail risk of extreme events. This paper proposes an unbiased, risk-aware framework for agentic negotiation, designed to ensure robust resource allocation in 6G network slicing. Specifically, agents leverage Digital Twins (DTs) to predict full latency distributions, which are then evaluated using a formal framework from extreme value theory, namely, Conditional Value-at-Risk (CVaR). This approach fundamentally shifts the agent's objective from reasoning over the mean to reasoning over the tail, thereby building a statistically-grounded buffer against worst-case outcomes. Furthermore, our framework ensures full uncertainty awareness by requiring agents to quantify epistemic uncertainty -- confidence in their own DTs predictions -- and propagate this meta-verification to make robust decisions, preventing them from acting on unreliable data. We validate this framework in a 6G inter-slice negotiation use-case between an eMBB and a URLLC agent. The results demonstrate the profound failure of the biased, mean-based baseline, which consistently fails its SLAs with a 25\\% rate. Our unbiased, CVaR-aware agent successfully mitigates this bias, eliminating SLA violations and reducing the URLLC and eMBB p99.999 latencies by around 11\\%. We show this reliability comes at the rational and quantifiable cost of slightly reduced energy savings to 17\\%, exposing the false economy of the biased approach. This work provides a concrete methodology for building the trustworthy autonomous systems required for 6G.",
      "pdf_url": "https://arxiv.org/pdf/2511.19175v1",
      "published": "2025-11-24T14:36:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19175v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Information Physics of Intelligence: Unifying Logical Depth and Entropy under Thermodynamic Constraints",
      "authors": [
        "Jianfeng Xu",
        "Zeyan Li"
      ],
      "abstract": "The rapid scaling of artificial intelligence models has revealed a fundamental tension between model capacity (storage) and inference efficiency (computation). While classical information theory focuses on transmission and storage limits, it lacks a unified physical framework to quantify the thermodynamic costs of generating information from compressed laws versus retrieving it from memory. In this paper, we propose a theoretical framework that treats information processing as an enabling mapping from ontological states to carrier states. We introduce a novel metric, Derivation Entropy, which quantifies the effective work required to compute a target state from a given logical depth. By analyzing the interplay between Shannon entropy (storage) and computational complexity (time/energy), we demonstrate the existence of a critical phase transition point. Below this threshold, memory retrieval is thermodynamically favorable; above it, generative computation becomes the optimal strategy. This \"Energy-Time-Space\" conservation law provides a physical explanation for the efficiency of generative models and offers a rigorous mathematical bound for designing next-generation, energy-efficient AI architectures. Our findings suggest that the minimization of Derivation Entropy is a governing principle for the evolution of both biological and artificial intelligence.",
      "pdf_url": "https://arxiv.org/pdf/2511.19156v1",
      "published": "2025-11-24T14:24:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19156v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction",
      "authors": [
        "Xihe Qiu",
        "Gengchen Ma",
        "Haoyu Wang",
        "Chen Zhan",
        "Xiaoyu Tan",
        "Shuo Li"
      ],
      "abstract": "Sleep stage classification based on electroencephalography (EEG) is fundamental for assessing sleep quality and diagnosing sleep-related disorders. However, most traditional machine learning methods rely heavily on prior knowledge and handcrafted features, while existing deep learning models still struggle to jointly capture fine-grained time-frequency patterns and achieve clinical interpretability. Recently, vision-language models (VLMs) have made significant progress in the medical domain, yet their performance remains constrained when applied to physiological waveform data, especially EEG signals, due to their limited visual understanding and insufficient reasoning capability. To address these challenges, we propose EEG-VLM, a hierarchical vision-language framework that integrates multi-level feature alignment with visually enhanced language-guided reasoning for interpretable EEG-based sleep stage classification. Specifically, a specialized visual enhancement module constructs high-level visual tokens from intermediate-layer features to extract rich semantic representations of EEG images. These tokens are further aligned with low-level CLIP features through a multi-level alignment mechanism, enhancing the VLM's image-processing capability. In addition, a Chain-of-Thought (CoT) reasoning strategy decomposes complex medical inference into interpretable logical steps, effectively simulating expert-like decision-making. Experimental results demonstrate that the proposed method significantly improves both the accuracy and interpretability of VLMs in EEG-based sleep stage classification, showing promising potential for automated and explainable EEG analysis in clinical settings.",
      "pdf_url": "https://arxiv.org/pdf/2511.19155v1",
      "published": "2025-11-24T14:23:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19155v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation",
      "authors": [
        "Moazzam Umer Gondal",
        "Hamad Ul Qudous",
        "Daniya Siddiqui",
        "Asma Ahmad Farhan"
      ],
      "abstract": "This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.",
      "pdf_url": "https://arxiv.org/pdf/2511.19149v1",
      "published": "2025-11-24T14:13:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19149v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty",
      "authors": [
        "Krishang Sharma"
      ],
      "abstract": "Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature.",
      "pdf_url": "https://arxiv.org/pdf/2511.19124v1",
      "published": "2025-11-24T13:53:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19124v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "On the Optimality of Discrete Object Naming: a Kinship Case Study",
      "authors": [
        "Phong Le",
        "Mees Lindeman",
        "Raquel G. Alhama"
      ],
      "abstract": "The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.19120v1",
      "published": "2025-11-24T13:49:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19120v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "AI Consciousness and Existential Risk",
      "authors": [
        "Rufin VanRullen"
      ],
      "abstract": "In AI, the existential risk denotes the hypothetical threat posed by an artificial system that would possess both the capability and the objective, either directly or indirectly, to eradicate humanity. This issue is gaining prominence in scientific debate due to recent technical advancements and increased media coverage. In parallel, AI progress has sparked speculation and studies about the potential emergence of artificial consciousness. The two questions, AI consciousness and existential risk, are sometimes conflated, as if the former entailed the latter. Here, I explain that this view stems from a common confusion between consciousness and intelligence. Yet these two properties are empirically and theoretically distinct. Arguably, while intelligence is a direct predictor of an AI system's existential threat, consciousness is not. There are, however, certain incidental scenarios in which consciousness could influence existential risk, in either direction. Consciousness could be viewed as a means towards AI alignment, thereby lowering existential risk; or, it could be a precondition for reaching certain capabilities or levels of intelligence, and thus positively related to existential risk. Recognizing these distinctions can help AI safety researchers and public policymakers focus on the most pressing issues.",
      "pdf_url": "https://arxiv.org/pdf/2511.19115v1",
      "published": "2025-11-24T13:48:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19115v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Physics-informed Neural Operator Learning for Nonlinear Grad-Shafranov Equation",
      "authors": [
        "Siqi Ding",
        "Zitong Zhang",
        "Guoyang Shi",
        "Xingyu Li",
        "Xiang Gu",
        "Yanan Xu",
        "Huasheng Xie",
        "Hanyue Zhao",
        "Yuejiang Shi",
        "Tianyuan Liu"
      ],
      "abstract": "As artificial intelligence emerges as a transformative enabler for fusion energy commercialization, fast and accurate solvers become increasingly critical. In magnetic confinement nuclear fusion, rapid and accurate solution of the Grad-Shafranov equation (GSE) is essential for real-time plasma control and analysis. Traditional numerical solvers achieve high precision but are computationally prohibitive, while data-driven surrogates infer quickly but fail to enforce physical laws and generalize poorly beyond training distributions. To address this challenge, we present a Physics-Informed Neural Operator (PINO) that directly learns the GSE solution operator, mapping shape parameters of last closed flux surface to equilibrium solutions for realistic nonlinear current profiles. Comprehensive benchmarking of five neural architectures identifies the novel Transformer-KAN (Kolmogorov-Arnold Network) Neural Operator (TKNO) as achieving highest accuracy (0.25% mean L2 relative error) under supervised training (only data-driven). However, all data-driven models exhibit large physics residuals, indicating poor physical consistency. Our unsupervised training can reduce the residuals by nearly four orders of magnitude through embedding physics-based loss terms without labeled data. Critically, semi-supervised learning--integrating sparse labeled data (100 interior points) with physics constraints--achieves optimal balance: 0.48% interpolation error and the most robust extrapolation performance (4.76% error, 8.9x degradation factor vs 39.8x for supervised models). Accelerated by TensorRT optimization, our models enable millisecond-level inference, establishing PINO as a promising pathway for next-generation fusion control systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.19114v1",
      "published": "2025-11-24T13:46:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19114v1",
      "categories": [
        "physics.plasm-ph",
        "cs.AI"
      ]
    },
    {
      "title": "The Core in Max-Loss Non-Centroid Clustering Can Be Empty",
      "authors": [
        "Robert Bredereck",
        "Eva Deltl",
        "Leon Kellerhals",
        "Jannik Peters"
      ],
      "abstract": "We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\\geq 3$ there exist metric instances with $n\\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $α$-core for any $α<2^{\\frac{1}{5}}\\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.",
      "pdf_url": "https://arxiv.org/pdf/2511.19107v1",
      "published": "2025-11-24T13:42:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.19107v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "stat.ML"
      ]
    }
  ]
}