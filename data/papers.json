{
  "last_updated": "2025-03-25T00:48:00.077949",
  "papers": [
    {
      "title": "HCAST: Human-Calibrated Autonomy Software Tasks",
      "authors": [
        "David Rein",
        "Joel Becker",
        "Amy Deng",
        "Seraphina Nix",
        "Chris Canal",
        "Daniel O'Connel",
        "Pip Arnott",
        "Ryan Bloom",
        "Thomas Broadley",
        "Katharyn Garcia",
        "Brian Goodrich",
        "Max Hasin",
        "Sami Jawhar",
        "Megan Kinniment",
        "Thomas Kwa",
        "Aron Lajko",
        "Nate Rush",
        "Lucas Jun Koba Sato",
        "Sydney Von Arx",
        "Ben West",
        "Lawrence Chan",
        "Elizabeth Barnes"
      ],
      "abstract": "To understand and predict the societal impacts of highly autonomous AI\nsystems, we need benchmarks with grounding, i.e., metrics that directly connect\nAI performance to real-world effects we care about. We present HCAST\n(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning\nengineering, cybersecurity, software engineering, and general reasoning tasks.\nWe collect 563 human baselines (totaling over 1500 hours) from people skilled\nin these domains, working under identical conditions as AI agents, which lets\nus estimate that HCAST tasks take humans between one minute and 8+ hours.\nMeasuring the time tasks take for humans provides an intuitive metric for\nevaluating AI capabilities, helping answer the question \"can an agent be\ntrusted to complete a task that would take a human X hours?\" We evaluate the\nsuccess rates of AI agents built on frontier foundation models, and we find\nthat current agents succeed 70-80% of the time on tasks that take humans less\nthan one hour, and less than 20% of the time on tasks that take humans more\nthan 4 hours.",
      "pdf_url": "http://arxiv.org/pdf/2503.17354v1",
      "published": "2025-03-21T17:54:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17354v1",
      "categories": [
        "cs.AI",
        "I.2.0"
      ]
    },
    {
      "title": "NdLinear Is All You Need for Representation Learning",
      "authors": [
        "Alex Reneau",
        "Jerry Yao-Chieh Hu",
        "Zhongfang Zhuang",
        "Ting-Chun Liu"
      ],
      "abstract": "Many high-impact machine learning tasks involve multi-dimensional data (e.g.,\nimages, volumetric medical scans, multivariate time-series). Yet, most neural\narchitectures flatten inputs, discarding critical cross-dimension information.\nWe introduce NdLinear, a novel linear transformation that preserves these\nstructures without extra overhead. By operating separately along each\ndimension, NdLinear captures dependencies that standard fully connected layers\noverlook. Extensive experiments across convolutional, recurrent, and\ntransformer-based networks show significant improvements in representational\npower and parameter efficiency. Crucially, NdLinear serves as a foundational\nbuilding block for large-scale foundation models by operating on any unimodal\nor multimodal data in its native form. This removes the need for flattening or\nmodality-specific preprocessing. Ndlinear rethinks core architectural\npriorities beyond attention, enabling more expressive, context-aware models at\nscale. We propose NdLinear as a drop-in replacement for standard linear layers\n-- marking an important step toward next-generation neural architectures.",
      "pdf_url": "http://arxiv.org/pdf/2503.17353v1",
      "published": "2025-03-21T17:52:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17353v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation",
      "authors": [
        "Congyi Fan",
        "Jian Guan",
        "Xuanjia Zhao",
        "Dongli Xu",
        "Youtian Lin",
        "Tong Ye",
        "Pengming Feng",
        "Haiwei Pan"
      ],
      "abstract": "Automatically generating natural, diverse and rhythmic human dance movements\ndriven by music is vital for virtual reality and film industries. However,\ngenerating dance that naturally follows music remains a challenge, as existing\nmethods lack proper beat alignment and exhibit unnatural motion dynamics. In\nthis paper, we propose Danceba, a novel framework that leverages gating\nmechanism to enhance rhythm-aware feature representation for music-driven dance\ngeneration, which achieves highly aligned dance poses with enhanced rhythmic\nsensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to\nprecisely extract rhythmic information from musical phase data, capitalizing on\nthe intrinsic periodicity and temporal structures of music. Additionally, we\npropose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic\nfeatures, ensuring that dance movements closely follow the musical rhythm. We\nalso introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately\nmodel upper and lower body motions along with musical features, thereby\nimproving the naturalness and diversity of generated dance movements. Extensive\nexperiments confirm that Danceba outperforms state-of-the-art methods,\nachieving significantly better rhythmic alignment and motion diversity. Project\npage: https://danceba.github.io/ .",
      "pdf_url": "http://arxiv.org/pdf/2503.17340v1",
      "published": "2025-03-21T17:42:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17340v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Can AI expose tax loopholes? Towards a new generation of legal policy assistants",
      "authors": [
        "Peter Fratrič",
        "Nils Holzenberger",
        "David Restrepo Amariles"
      ],
      "abstract": "The legislative process is the backbone of a state built on solid\ninstitutions. Yet, due to the complexity of laws -- particularly tax law --\npolicies may lead to inequality and social tensions. In this study, we\nintroduce a novel prototype system designed to address the issues of tax\nloopholes and tax avoidance. Our hybrid solution integrates a natural language\ninterface with a domain-specific language tailored for planning. We demonstrate\non a case study how tax loopholes and avoidance schemes can be exposed. We\nconclude that our prototype can help enhance social welfare by systematically\nidentifying and addressing tax gaps stemming from loopholes.",
      "pdf_url": "http://arxiv.org/pdf/2503.17339v1",
      "published": "2025-03-21T17:40:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17339v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Capturing Individual Human Preferences with Reward Features",
      "authors": [
        "André Barreto",
        "Vincent Dumoulin",
        "Yiran Mao",
        "Nicolas Perez-Nieves",
        "Bobak Shahriari",
        "Yann Dauphin",
        "Doina Precup",
        "Hugo Larochelle"
      ],
      "abstract": "Reinforcement learning from human feedback usually models preferences using a\nreward model that does not distinguish between people. We argue that this is\nunlikely to be a good design choice in contexts with high potential for\ndisagreement, like in the training of large language models. We propose a\nmethod to specialise a reward model to a person or group of people. Our\napproach builds on the observation that individual preferences can be captured\nas a linear combination of a set of general reward features. We show how to\nlearn such features and subsequently use them to quickly adapt the reward model\nto a specific individual, even if their preferences are not reflected in the\ntraining data. We present experiments with large language models comparing the\nproposed architecture with a non-adaptive reward model and also adaptive\ncounterparts, including models that do in-context personalisation. Depending on\nhow much disagreement there is in the training data, our model either\nsignificantly outperforms the baselines or matches their performance with a\nsimpler architecture and more stable training.",
      "pdf_url": "http://arxiv.org/pdf/2503.17338v1",
      "published": "2025-03-21T17:39:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17338v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs",
      "authors": [
        "Reem Gody",
        "Mohamed Abdelghaffar",
        "Mohammed Jabreel",
        "Ahmed Tawfik"
      ],
      "abstract": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments.",
      "pdf_url": "http://arxiv.org/pdf/2503.17336v1",
      "published": "2025-03-21T17:34:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17336v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities",
      "authors": [
        "Yuxuan Zhu",
        "Antony Kellermann",
        "Dylan Bowman",
        "Philip Li",
        "Akul Gupta",
        "Adarsh Danda",
        "Richard Fang",
        "Conner Jensen",
        "Eric Ihli",
        "Jason Benn",
        "Jet Geronimo",
        "Avi Dhir",
        "Sudhit Rao",
        "Kaicheng Yu",
        "Twm Stone",
        "Daniel Kang"
      ],
      "abstract": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities.",
      "pdf_url": "http://arxiv.org/pdf/2503.17332v1",
      "published": "2025-03-21T17:32:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17332v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "I.2.1; I.2.7"
      ]
    },
    {
      "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language",
      "authors": [
        "Kun Chu",
        "Xufeng Zhao",
        "Cornelius Weber",
        "Stefan Wermter"
      ],
      "abstract": "Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP.",
      "pdf_url": "http://arxiv.org/pdf/2503.17309v1",
      "published": "2025-03-21T17:04:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17309v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Preference-Guided Diffusion for Multi-Objective Offline Optimization",
      "authors": [
        "Yashas Annadani",
        "Syrine Belakaria",
        "Stefano Ermon",
        "Stefan Bauer",
        "Barbara E Engelhardt"
      ],
      "abstract": "Offline multi-objective optimization aims to identify Pareto-optimal\nsolutions given a dataset of designs and their objective values. In this work,\nwe propose a preference-guided diffusion model that generates Pareto-optimal\ndesigns by leveraging a classifier-based guidance mechanism. Our guidance\nclassifier is a preference model trained to predict the probability that one\ndesign dominates another, directing the diffusion model toward optimal regions\nof the design space. Crucially, this preference model generalizes beyond the\ntraining distribution, enabling the discovery of Pareto-optimal solutions\noutside the observed dataset. We introduce a novel diversity-aware preference\nguidance, augmenting Pareto dominance preference with diversity criteria. This\nensures that generated solutions are optimal and well-distributed across the\nobjective space, a capability absent in prior generative methods for offline\nmulti-objective optimization. We evaluate our approach on various continuous\noffline multi-objective optimization tasks and find that it consistently\noutperforms other inverse/generative approaches while remaining competitive\nwith forward/surrogate-based optimization methods. Our results highlight the\neffectiveness of classifier-guided diffusion models in generating diverse and\nhigh-quality solutions that approximate the Pareto front well.",
      "pdf_url": "http://arxiv.org/pdf/2503.17299v1",
      "published": "2025-03-21T16:49:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17299v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Breaking the Symmetries of Indistinguishable Objects",
      "authors": [
        "Ozgur Akgun",
        "Mun See Chang",
        "Ian P. Gent",
        "Christopher Jefferson"
      ],
      "abstract": "Indistinguishable objects often occur when modelling problems in constraint\nprogramming, as well as in other related paradigms. They occur when objects can\nbe viewed as being drawn from a set of unlabelled objects, and the only\noperation allowed on them is equality testing. For example, the golfers in the\nsocial golfer problem are indistinguishable. If we do label the golfers, then\nany relabelling of the golfers in one solution gives another valid solution.\nTherefore, we can regard the symmetric group of size $n$ as acting on a set of\n$n$ indistinguishable objects. In this paper, we show how we can break the\nsymmetries resulting from indistinguishable objects. We show how symmetries on\nindistinguishable objects can be defined properly in complex types, for example\nin a matrix indexed by indistinguishable objects. We then show how the\nresulting symmetries can be broken correctly. In Essence, a high-level\nmodelling language, indistinguishable objects are encapsulated in \"unnamed\ntypes\". We provide an implementation of complete symmetry breaking for unnamed\ntypes in Essence.",
      "pdf_url": "http://arxiv.org/pdf/2503.17251v1",
      "published": "2025-03-21T15:56:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17251v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "KL3M Tokenizers: A Family of Domain-Specific and Character-Level Tokenizers for Legal, Financial, and Preprocessing Applications",
      "authors": [
        "Michael J Bommarito",
        "Daniel Martin Katz",
        "Jillian Bommarito"
      ],
      "abstract": "We present the KL3M tokenizers, a family of specialized tokenizers for legal,\nfinancial, and governmental text. Despite established work on tokenization,\nspecialized tokenizers for professional domains remain understudied. Our paper\noffers two main contributions to this area.\n  First, we introduce domain-specific BPE tokenizers for legal, financial, and\ngovernmental text. Our kl3m-004-128k-cased tokenizer uses 9-17% fewer tokens\nthan GPT-4o and Llama3 for domain-specific documents, despite having a smaller\nvocabulary. For specialized terminology, our cased tokenizer is even more\nefficient, using up to 83% fewer tokens for legal terms and 39% fewer tokens\nfor financial terms.\n  Second, we develop character-level BPE tokenizers (4K, 8K, and 16K vocabulary\nsizes) for text correction tasks like OCR post-processing. These tokenizers\nkeep consistent token boundaries between error-containing and correct text,\nmaking it easier for models to learn correction patterns.\n  These tokenizers help professional applications by fitting more text in\ncontext windows, reducing computational needs, and preserving the meaning of\ndomain-specific terms. Our analysis shows these efficiency gains directly\nbenefit the processing of long legal and financial documents. We release all\ntokenizers and code through GitHub and Hugging Face to support further research\nin specialized tokenization.",
      "pdf_url": "http://arxiv.org/pdf/2503.17247v1",
      "published": "2025-03-21T15:51:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17247v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging",
      "authors": [
        "Aladin Djuhera",
        "Swanand Ravindra Kadhe",
        "Farhan Ahmed",
        "Syed Zawad",
        "Holger Boche"
      ],
      "abstract": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses.",
      "pdf_url": "http://arxiv.org/pdf/2503.17239v1",
      "published": "2025-03-21T15:44:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17239v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
      "authors": [
        "Yu-Hsi Chen"
      ],
      "abstract": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
      "pdf_url": "http://arxiv.org/pdf/2503.17237v1",
      "published": "2025-03-21T15:40:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17237v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
      "authors": [
        "Albert Sawczyn",
        "Jakub Binkowski",
        "Denis Janiak",
        "Bogdan Gabrys",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content.",
      "pdf_url": "http://arxiv.org/pdf/2503.17229v1",
      "published": "2025-03-21T15:32:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17229v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation",
      "authors": [
        "Giacomo Savazzi",
        "Eugenio Lomurno",
        "Cristian Sbrolli",
        "Agnese Chiatti",
        "Matteo Matteucci"
      ],
      "abstract": "As machine learning models increase in scale and complexity, obtaining\nsufficient training data has become a critical bottleneck due to acquisition\ncosts, privacy constraints, and data scarcity in specialised domains. While\nsynthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly\nas task complexity grows. Concurrently, Neuro-Symbolic methods, which combine\nneural networks' learning strengths with symbolic reasoning's structured\nrepresentations, have demonstrated significant potential across various\ncognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning\nfor synthetic image dataset generation, focusing specifically on improving the\nperformance of Scene Graph Generation models. The research investigates whether\nstructured symbolic representations in the form of scene graphs can enhance\nsynthetic data quality through explicit encoding of relational constraints. The\nresults demonstrate that Neuro-Symbolic conditioning yields significant\nimprovements of up to +2.59% in standard Recall metrics and +2.83% in No Graph\nConstraint Recall metrics when used for dataset augmentation. These findings\nestablish that merging Neuro-Symbolic and generative approaches produces\nsynthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to\novercome data scarcity limitations even for complex visual reasoning tasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.17224v1",
      "published": "2025-03-21T15:26:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17224v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models",
      "authors": [
        "Sonish Sivarajkumar",
        "Kimia Ameri",
        "Chuqin Li",
        "Yanshan Wang",
        "Min Jiang"
      ],
      "abstract": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies.",
      "pdf_url": "http://arxiv.org/pdf/2503.17222v1",
      "published": "2025-03-21T15:25:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17222v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction",
      "authors": [
        "Ting Sun",
        "Cheng Cui",
        "Yuning Du",
        "Yi Liu"
      ],
      "abstract": "Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX .",
      "pdf_url": "http://arxiv.org/pdf/2503.17213v1",
      "published": "2025-03-21T15:20:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17213v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
      "authors": [
        "Sheng Wang",
        "Pengan Chen",
        "Jingqi Zhou",
        "Qintong Li",
        "Jingwei Dong",
        "Jiahui Gao",
        "Boyang Xue",
        "Jiyue Jiang",
        "Lingpeng Kong",
        "Chuan Wu"
      ],
      "abstract": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention.",
      "pdf_url": "http://arxiv.org/pdf/2503.17195v1",
      "published": "2025-03-21T14:43:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17195v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection",
      "authors": [
        "Xueqi Qiu",
        "Xingyu Miao",
        "Fan Wan",
        "Haoran Duan",
        "Tejal Shah",
        "Varun Ojhab",
        "Yang Longa",
        "Rajiv Ranjan"
      ],
      "abstract": "Deepfake detection is crucial for curbing the harm it causes to society.\nHowever, current Deepfake detection methods fail to thoroughly explore artifact\ninformation across different domains due to insufficient intrinsic\ninteractions. These interactions refer to the fusion and coordination after\nfeature extraction processes across different domains, which are crucial for\nrecognizing complex forgery clues. Focusing on more generalized Deepfake\ndetection, in this work, we introduce a novel bi-directional attention module\nto capture the local positional information of artifact clues from the spatial\ndomain. This enables accurate artifact localization, thus addressing the coarse\nprocessing with artifact features. To further address the limitation that the\nproposed bi-directional attention module may not well capture global subtle\nforgery information in the artifact feature (e.g., textures or edges), we\nemploy a fine-grained frequency attention module in the frequency domain. By\ndoing so, we can obtain high-frequency information in the fine-grained\nfeatures, which contains the global and subtle forgery information. Although\nthese features from the diverse domains can be effectively and independently\nimproved, fusing them directly does not effectively improve the detection\nperformance. Therefore, we propose a feature superposition strategy that\ncomplements information from spatial and frequency domains. This strategy turns\nthe feature components into the form of wave-like tokens, which are updated\nbased on their phase, such that the distinctions between authentic and artifact\nfeatures can be amplified. Our method demonstrates significant improvements\nover state-of-the-art (SOTA) methods on five public Deepfake datasets in\ncapturing abnormalities across different manipulated operations and real-life.",
      "pdf_url": "http://arxiv.org/pdf/2503.17184v1",
      "published": "2025-03-21T14:31:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17184v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries",
      "authors": [
        "Lukas Twist",
        "Jie M. Zhang",
        "Mark Harman",
        "Don Syme",
        "Joost Noppen",
        "Detlef Nauck"
      ],
      "abstract": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias.",
      "pdf_url": "http://arxiv.org/pdf/2503.17181v1",
      "published": "2025-03-21T14:29:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17181v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "DiTEC-WDN: A Large-Scale Dataset of Water Distribution Network Scenarios under Diverse Hydraulic Conditions",
      "authors": [
        "Huy Truong",
        "Andrés Tello",
        "Alexander Lazovik",
        "Victoria Degeler"
      ],
      "abstract": "Privacy restrictions hinder the sharing of real-world Water Distribution\nNetwork (WDN) models, limiting the application of emerging data-driven machine\nlearning, which typically requires extensive observations. To address this\nchallenge, we propose the dataset DiTEC-WDN that comprises 36,000 unique\nscenarios simulated over either short-term (24 hours) or long-term (1 year)\nperiods. We constructed this dataset using an automated pipeline that optimizes\ncrucial parameters (e.g., pressure, flow rate, and demand patterns),\nfacilitates large-scale simulations, and records discrete, synthetic but\nhydraulically realistic states under standard conditions via rule validation\nand post-hoc analysis. With a total of 228 million generated graph-based\nstates, DiTEC-WDN can support a variety of machine-learning tasks, including\ngraph-level, node-level, and link-level regression, as well as time-series\nforecasting. This contribution, released under a public license, encourages\nopen scientific research in the critical water sector, eliminates the risk of\nexposing sensitive data, and fulfills the need for a large-scale water\ndistribution network benchmark for study comparisons and scenario analysis.",
      "pdf_url": "http://arxiv.org/pdf/2503.17167v1",
      "published": "2025-03-21T14:14:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17167v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition",
      "authors": [
        "Siyuan Yang",
        "Shilin Lu",
        "Shizheng Wang",
        "Meng Hwa Er",
        "Zengwei Zheng",
        "Alex C. Kot"
      ],
      "abstract": "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.",
      "pdf_url": "http://arxiv.org/pdf/2503.17132v1",
      "published": "2025-03-21T13:31:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17132v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.NE"
      ]
    },
    {
      "title": "Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning",
      "authors": [
        "Chan Kim",
        "Seung-Woo Seo",
        "Seong-Woo Kim"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at\n\\href{https://lamour-rl.github.io/}{https://lamour-rl.github.io/}.",
      "pdf_url": "http://arxiv.org/pdf/2503.17125v1",
      "published": "2025-03-21T13:20:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17125v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding",
      "authors": [
        "Luca Rossetto",
        "Werner Bailer",
        "Duc-Tien Dang-Nguyen",
        "Graham Healy",
        "Björn Þór Jónsson",
        "Onanong Kongmeesub",
        "Hoang-Bao Le",
        "Stevan Rudinac",
        "Klaus Schöffmann",
        "Florian Spiess",
        "Allie Tran",
        "Minh-Triet Tran",
        "Quang-Linh Tran",
        "Cathal Gurrin"
      ],
      "abstract": "Egocentric video has seen increased interest in recent years, as it is used\nin a range of areas. However, most existing datasets are limited to a single\nperspective. In this paper, we present the CASTLE 2024 dataset, a multimodal\ncollection containing ego- and exo-centric (i.e., first- and third-person\nperspective) video and audio from 15 time-aligned sources, as well as other\nsensor streams and auxiliary data. The dataset was recorded by volunteer\nparticipants over four days in a fixed location and includes the point of view\nof 10 participants, with an additional 5 fixed cameras providing an exocentric\nperspective. The entire dataset contains over 600 hours of UHD video recorded\nat 50 frames per second. In contrast to other datasets, CASTLE 2024 does not\ncontain any partial censoring, such as blurred faces or distorted audio. The\ndataset is available via https://castle-dataset.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2503.17116v1",
      "published": "2025-03-21T13:01:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17116v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.IR"
      ]
    },
    {
      "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
      "authors": [
        "Kwan Yun",
        "Chaelin Kim",
        "Hangyeul Shin",
        "Junyong Noh"
      ],
      "abstract": "Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{\\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.",
      "pdf_url": "http://arxiv.org/pdf/2503.17095v1",
      "published": "2025-03-21T12:24:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17095v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "68T45, 68U05",
        "I.3.3; I.3.8"
      ]
    },
    {
      "title": "Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR Segmentation",
      "authors": [
        "Tiarna Lee",
        "Esther Puyol-Antón",
        "Bram Ruijsink",
        "Miaojing Shi",
        "Andrew P. King"
      ],
      "abstract": "Artificial intelligence (AI) is increasingly being used for medical imaging\ntasks. However, there can be biases in the resulting models, particularly when\nthey were trained using imbalanced training datasets. One such example has been\nthe strong race bias effect in cardiac magnetic resonance (CMR) image\nsegmentation models. Although this phenomenon has been reported in a number of\npublications, little is known about the effectiveness of bias mitigation\nalgorithms in this domain. We aim to investigate the impact of common bias\nmitigation methods to address bias between Black and White subjects in AI-based\nCMR segmentation models. Specifically, we use oversampling, importance\nreweighing and Group DRO as well as combinations of these techniques to\nmitigate the race bias. Furthermore, motivated by recent findings on the root\ncauses of AI-based CMR segmentation bias, we evaluate the same methods using\nmodels trained and evaluated on cropped CMR images. We find that bias can be\nmitigated using oversampling, significantly improving performance for the\nunderrepresented Black subjects whilst not significantly reducing the majority\nWhite subjects' performance. Group DRO also improves performance for Black\nsubjects but not significantly, while reweighing decreases performance for\nBlack subjects. Using a combination of oversampling and Group DRO also improves\nperformance for Black subjects but not significantly. Using cropped images\nincreases performance for both races and reduces the bias, whilst adding\noversampling as a bias mitigation technique with cropped images reduces the\nbias further.",
      "pdf_url": "http://arxiv.org/pdf/2503.17089v1",
      "published": "2025-03-21T12:17:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17089v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
      "authors": [
        "J. M. Diederik Kruijssen",
        "Nicholas Emmons"
      ],
      "abstract": "Artificial intelligence (AI) systems powered by large language models have\nbecome increasingly prevalent in modern society, enabling a wide range of\napplications through natural language interaction. As AI agents proliferate in\nour daily lives, their generic and uniform expressiveness presents a\nsignificant limitation to their appeal and adoption. Personality expression\nrepresents a key prerequisite for creating more human-like and distinctive AI\nsystems. We show that AI models can express deterministic and consistent\npersonalities when instructed using established psychological frameworks, with\nvarying degrees of accuracy depending on model capabilities. We find that more\nadvanced models like GPT-4o and o1 demonstrate the highest accuracy in\nexpressing specified personalities across both Big Five and Myers-Briggs\nassessments, and further analysis suggests that personality expression emerges\nfrom a combination of intelligence and reasoning capabilities. Our results\nreveal that personality expression operates through holistic reasoning rather\nthan question-by-question optimization, with response-scale metrics showing\nhigher variance than test-scale metrics. Furthermore, we find that model\nfine-tuning affects communication style independently of personality expression\naccuracy. These findings establish a foundation for creating AI agents with\ndiverse and consistent personalities, which could significantly enhance\nhuman-AI interaction across applications from education to healthcare, while\nadditionally enabling a broader range of more unique AI agents. The ability to\nquantitatively assess and implement personality expression in AI systems opens\nnew avenues for research into more relatable, trustworthy, and ethically\ndesigned AI.",
      "pdf_url": "http://arxiv.org/pdf/2503.17085v1",
      "published": "2025-03-21T12:12:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17085v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ]
    },
    {
      "title": "A Thorough Assessment of the Non-IID Data Impact in Federated Learning",
      "authors": [
        "Daniel M. Jimenez-Gutierrez",
        "Mehrdad Hassanzadeh",
        "Aris Anagnostopoulos",
        "Ioannis Chatzigiannakis",
        "Andrea Vitaletti"
      ],
      "abstract": "Federated learning (FL) allows collaborative machine learning (ML) model\ntraining among decentralized clients' information, ensuring data privacy. The\ndecentralized nature of FL deals with non-independent and identically\ndistributed (non-IID) data. This open problem has notable consequences, such as\ndecreased model performance and more significant convergence times. Despite its\nimportance, experimental studies systematically addressing all types of data\nheterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by\nassessing and quantifying the non-IID effect through a thorough empirical\nanalysis. We use the Hellinger Distance (HD) to measure differences in\ndistribution among clients. Our study benchmarks four state-of-the-art\nstrategies for handling non-IID data, including label, feature, quantity, and\nspatiotemporal skewness, under realistic and controlled conditions. This is the\nfirst comprehensive analysis of the spatiotemporal skew effect in FL. Our\nfindings highlight the significant impact of label and spatiotemporal skew\nnon-IID types on FL model performance, with notable performance drops occurring\nat specific HD thresholds. Additionally, the FL performance is heavily affected\nmainly when the non-IIDness is extreme. Thus, we provide recommendations for FL\nresearch to tackle data heterogeneity effectively. Our work represents the most\nextensive examination of non-IIDness in FL, offering a robust foundation for\nfuture research.",
      "pdf_url": "http://arxiv.org/pdf/2503.17070v1",
      "published": "2025-03-21T11:53:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17070v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "PVChat: Personalized Video Chat with One-Shot Learning",
      "authors": [
        "Yufei Shi",
        "Weilong Yan",
        "Gang Xu",
        "Yumeng Li",
        "Yuchen Li",
        "Zhenxi Li",
        "Fei Richard Yu",
        "Ming Li",
        "Si Yong Yeo"
      ],
      "abstract": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2503.17069v1",
      "published": "2025-03-21T11:50:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17069v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Replay4NCL: An Efficient Memory Replay-based Methodology for Neuromorphic Continual Learning in Embedded AI Systems",
      "authors": [
        "Mishal Fatima Minhas",
        "Rachmad Vidya Wicaksana Putra",
        "Falah Awwad",
        "Osman Hasan",
        "Muhammad Shafique"
      ],
      "abstract": "Neuromorphic Continual Learning (NCL) paradigm leverages Spiking Neural\nNetworks (SNNs) to enable continual learning (CL) capabilities for AI systems\nto adapt to dynamically changing environments. Currently, the state-of-the-art\nemploy a memory replay-based method to maintain the old knowledge. However,\nthis technique relies on long timesteps and compression-decompression steps,\nthereby incurring significant latency and energy overheads, which are not\nsuitable for tightly-constrained embedded AI systems (e.g., mobile\nagents/robotics). To address this, we propose Replay4NCL, a novel efficient\nmemory replay-based methodology for enabling NCL in embedded AI systems.\nSpecifically, Replay4NCL compresses the latent data (old knowledge), then\nreplays them during the NCL training phase with small timesteps, to minimize\nthe processing latency and energy consumption. To compensate the information\nloss from reduced spikes, we adjust the neuron threshold potential and learning\nrate settings. Experimental results on the class-incremental scenario with the\nSpiking Heidelberg Digits (SHD) dataset show that Replay4NCL can preserve old\nknowledge with Top-1 accuracy of 90.43% compared to 86.22% from the\nstate-of-the-art, while effectively learning new tasks, achieving 4.88x latency\nspeed-up, 20% latent memory saving, and 36.43% energy saving. These results\nhighlight the potential of our Replay4NCL methodology to further advances NCL\ncapabilities for embedded AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2503.17061v1",
      "published": "2025-03-21T11:33:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17061v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Data-Driven Optimization of EV Charging Station Placement Using Causal Discovery",
      "authors": [
        "Julius Stephan Junker",
        "Rong Hu",
        "Ziyue Li",
        "Wolfgang Ketter"
      ],
      "abstract": "This paper addresses the critical challenge of optimizing electric vehicle\ncharging station placement through a novel data-driven methodology employing\ncausal discovery techniques. While traditional approaches prioritize economic\nfactors or power grid constraints, they often neglect empirical charging\npatterns that ultimately determine station utilization. We analyze extensive\ncharging data from Palo Alto and Boulder (337,344 events across 100 stations)\nto uncover latent relationships between station characteristics and\nutilization. Applying structural learning algorithms (NOTEARS and DAGMA) to\nthis data reveals that charging demand is primarily determined by three\nfactors: proximity to amenities, EV registration density, and adjacency to\nhigh-traffic routes. These findings, consistent across multiple algorithms and\nurban contexts, challenge conventional infrastructure distribution strategies.\nWe develop an optimization framework that translates these insights into\nactionable placement recommendations, identifying locations likely to\nexperience high utilization based on the discovered dependency structures. The\nresulting site selection model prioritizes strategic clustering in high-amenity\nareas with substantial EV populations rather than uniform spatial distribution.\nOur approach contributes a framework that integrates empirical charging\nbehavior into infrastructure planning, potentially enhancing both station\nutilization and user convenience. By focusing on data-driven insights instead\nof theoretical distribution models, we provide a more effective strategy for\nexpanding charging networks that can adjust to various stages of EV market\ndevelopment.",
      "pdf_url": "http://arxiv.org/pdf/2503.17055v1",
      "published": "2025-03-21T11:15:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17055v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences",
      "authors": [
        "Dongsheng Yang",
        "Qianying Liu",
        "Wataru Sato",
        "Takashi Minato",
        "Chaoran Liu",
        "Shin'ya Nishida"
      ],
      "abstract": "Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.",
      "pdf_url": "http://arxiv.org/pdf/2503.17046v1",
      "published": "2025-03-21T11:04:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17046v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?",
      "authors": [
        "Jeremy Barnes",
        "Naiara Perez",
        "Alba Bonet-Jover",
        "Begoña Altuna"
      ],
      "abstract": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.",
      "pdf_url": "http://arxiv.org/pdf/2503.17039v1",
      "published": "2025-03-21T10:52:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17039v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation",
      "authors": [
        "Stephen Lloyd-Brown",
        "Susan Francis",
        "Caroline Hoad",
        "Penny Gowland",
        "Karen Mullinger",
        "Andrew French",
        "Xin Chen"
      ],
      "abstract": "An often overlooked problem in medical image segmentation research is the\neffective selection of training subsets to annotate from a complete set of\nunlabelled data. Many studies select their training sets at random, which may\nlead to suboptimal model performance, especially in the minimal supervision\nsetting where each training image has a profound effect on performance\noutcomes. This work aims to address this issue. We use prototypical contrasting\nlearning and clustering to extract representative and diverse samples for\nannotation. We improve upon prior works with a bespoke cluster-based image\nselection process. Additionally, we introduce the concept of unsupervised\nbalanced batch dataloading to medical image segmentation, which aims to improve\nmodel learning with minimally annotated data. We evaluated our method on a\npublic skin lesion dataset (ISIC 2018) and compared it to another\nstate-of-the-art data sampling method. Our method achieved superior performance\nin a low annotation budget scenario.",
      "pdf_url": "http://arxiv.org/pdf/2503.17034v1",
      "published": "2025-03-21T10:42:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17034v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based Approaches and Handcrafted Feature Extraction Techniques",
      "authors": [
        "Snigdha Paul",
        "Sambit Mallick",
        "Anindya Sen"
      ],
      "abstract": "Computer vision has transformed medical diagnosis, treatment, and research\nthrough advanced image processing and machine learning techniques. Fracture\nclassification, a critical area in healthcare, has greatly benefited from these\nadvancements, yet accurate detection is challenged by complex patterns and\nimage noise. Bit plane slicing enhances medical images by reducing noise\ninterference and extracting informative features. This research explores\npartial denoising techniques to provide practical solutions for improved\nfracture analysis, ultimately enhancing patient care. The study explores deep\nlearning model DenseNet and handcrafted feature extraction. Decision Tree and\nRandom Forest, were employed to train and evaluate distinct image\nrepresentations. These include the original image, the concatenation of the\nfour bit planes from the LSB as well as MSB, the fully denoised image, and an\nimage consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.\nThe purpose of forming these diverse image representations is to analyze SNR as\nwell as classification accuracy and identify the bit planes that contain the\nmost informative features. Moreover, the study delves into the significance of\npartial denoising techniques in preserving crucial features, leading to\nimprovements in classification results. Notably, this study shows that\nemploying the Random Forest classifier, the partially denoised image\nrepresentation exhibited a testing accuracy of 95.61% surpassing the\nperformance of other image representations. The outcomes of this research\nprovide valuable insights into the development of efficient preprocessing,\nfeature extraction and classification approaches for fracture identification.\nBy enhancing diagnostic accuracy, these advancements hold the potential to\npositively impact patient care and overall medical outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2503.17030v1",
      "published": "2025-03-21T10:39:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17030v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "A Guide to Bayesian Networks Software Packages for Structure and Parameter Learning -- 2025 Edition",
      "authors": [
        "Joverlyn Gaudillo",
        "Nicole Astrologo",
        "Fabio Stella",
        "Enzo Acerbi",
        "Francesco Canonaco"
      ],
      "abstract": "A representation of the cause-effect mechanism is needed to enable artificial\nintelligence to represent how the world works. Bayesian Networks (BNs) have\nproven to be an effective and versatile tool for this task. BNs require\nconstructing a structure of dependencies among variables and learning the\nparameters that govern these relationships. These tasks, referred to as\nstructural learning and parameter learning, are actively investigated by the\nresearch community, with several algorithms proposed and no single method\nhaving established itself as standard. A wide range of software, tools, and\npackages have been developed for BNs analysis and made available to academic\nresearchers and industry practitioners. As a consequence of having no\none-size-fits-all solution, moving the first practical steps and getting\noriented into this field is proving to be challenging to outsiders and\nbeginners. In this paper, we review the most relevant tools and software for\nBNs structural and parameter learning to date, providing our subjective\nrecommendations directed to an audience of beginners. In addition, we provide\nan extensive easy-to-consult overview table summarizing all software packages\nand their main features. By improving the reader understanding of which\navailable software might best suit their needs, we improve accessibility to the\nfield and make it easier for beginners to take their first step into it.",
      "pdf_url": "http://arxiv.org/pdf/2503.17025v1",
      "published": "2025-03-21T10:36:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17025v1",
      "categories": [
        "cs.AI",
        "I.2"
      ]
    },
    {
      "title": "Symbolic Audio Classification via Modal Decision Tree Learning",
      "authors": [
        "Enrico Marzano",
        "Giovanni Pagliarini",
        "Riccardo Pasini",
        "Guido Sciavicco",
        "Ionel Eduard Stan"
      ],
      "abstract": "The range of potential applications of acoustic analysis is wide.\nClassification of sounds, in particular, is a typical machine learning task\nthat received a lot of attention in recent years. The most common approaches to\nsound classification are sub-symbolic, typically based on neural networks, and\nresult in black-box models with high performances but very low transparency. In\nthis work, we consider several audio tasks, namely, age and gender recognition,\nemotion classification, and respiratory disease diagnosis, and we approach them\nwith a symbolic technique, that is, (modal) decision tree learning. We prove\nthat such tasks can be solved using the same symbolic pipeline, that allows to\nextract simple rules with very high accuracy and low complexity. In principle,\nall such tasks could be associated to an autonomous conversation system, which\ncould be useful in different contexts, such as an automatic reservation agent\nfor an hospital or a clinic.",
      "pdf_url": "http://arxiv.org/pdf/2503.17018v1",
      "published": "2025-03-21T10:27:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17018v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "68T05",
        "I.2.6"
      ]
    },
    {
      "title": "Developing Critical Thinking in Second Language Learners: Exploring Generative AI like ChatGPT as a Tool for Argumentative Essay Writing",
      "authors": [
        "Simon Suh",
        "Jihyuk Bang",
        "Ji Woo Han"
      ],
      "abstract": "This study employs the Paul-Elder Critical Thinking Model and Tan's\nargumentative writing framework to create a structured methodology. This\nmethodology, ChatGPT Guideline for Critical Argumentative Writing (CGCAW)\nframework, integrates the models with ChatGPT's capabilities to guide L2\nlearners in utilizing ChatGPT to enhance their critical thinking skills. A\nquantitative experiment was conducted with 10 participants from a state\nuniversity, divided into experimental and control groups. The experimental\ngroup utilized the CGCAW framework, while the control group used ChatGPT\nwithout specific guidelines. Participants wrote an argumentative essay within a\n40-minute timeframe, and essays were evaluated by three assessors: ChatGPT,\nGrammarly, and a course instructor. Results indicated that the experimental\ngroup showed improvements in clarity, logical coherence, and use of evidence,\ndemonstrating ChatGPT's potential to enhance specific aspects of argumentative\nwriting. However, the control group performed better in overall language\nmechanics and articulation of main arguments, indicating areas where the CGCAW\nframework could be further refined. This study highlights the need for further\nresearch to optimize the use of AI tools like ChatGPT in L2 learning\nenvironments to enhance critical thinking and writing skills.",
      "pdf_url": "http://arxiv.org/pdf/2503.17013v1",
      "published": "2025-03-21T10:22:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17013v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2.7; K.3.1"
      ]
    },
    {
      "title": "Targetless 6DoF Calibration of LiDAR and 2D Scanning Radar Based on Cylindrical Occupancy",
      "authors": [
        "Weimin Wang",
        "Yu Du",
        "Ting Yang",
        "Yu Liu"
      ],
      "abstract": "Owing to the capability for reliable and all-weather long-range sensing, the\nfusion of LiDAR and Radar has been widely applied to autonomous vehicles for\nrobust perception. In practical operation, well manually calibrated extrinsic\nparameters, which are crucial for the fusion of multi-modal sensors, may drift\ndue to the vibration. To address this issue, we present a novel targetless\ncalibration approach, termed LiRaCo, for the extrinsic 6DoF calibration of\nLiDAR and Radar sensors. Although both types of sensors can obtain geometric\ninformation, bridging the geometric correspondences between multi-modal data\nwithout any clues of explicit artificial markers is nontrivial, mainly due to\nthe low vertical resolution of scanning Radar. To achieve the targetless\ncalibration, LiRaCo leverages a spatial occupancy consistency between LiDAR\npoint clouds and Radar scans in a common cylindrical representation,\nconsidering the increasing data sparsity with distance for both sensors.\nSpecifically, LiRaCo expands the valid Radar scanned pixels into 3D occupancy\ngrids to constrain LiDAR point clouds based on spatial consistency.\nConsequently, a cost function involving extrinsic calibration parameters is\nformulated based on the spatial overlap of 3D grids and LiDAR points. Extrinsic\nparameters are finally estimated by optimizing the cost function. Comprehensive\nquantitative and qualitative experiments on two real outdoor datasets with\ndifferent LiDAR sensors demonstrate the feasibility and accuracy of the\nproposed method. The source code will be publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2503.17002v1",
      "published": "2025-03-21T10:09:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.17002v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Enabling Versatile Controls for Video Diffusion Models",
      "authors": [
        "Xu Zhang",
        "Hao Zhou",
        "Haoming Qin",
        "Xiaobin Lu",
        "Jiaxing Yan",
        "Guanzhong Wang",
        "Zeyu Chen",
        "Yi Liu"
      ],
      "abstract": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
      "pdf_url": "http://arxiv.org/pdf/2503.16983v1",
      "published": "2025-03-21T09:48:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16983v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models",
      "authors": [
        "Haichao Zhang",
        "Zhuowei Li",
        "Dimitris Metaxas",
        "Yun Fu"
      ],
      "abstract": "Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2503.16980v1",
      "published": "2025-03-21T09:46:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16980v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies with Q-Ensembles",
      "authors": [
        "Ruoqi Zhang",
        "Ziwei Luo",
        "Jens Sjölund",
        "Per Mattsson",
        "Linus Gisslén",
        "Alessandro Sestini"
      ],
      "abstract": "Diffusion models have shown impressive performance in capturing complex and\nmulti-modal action distributions for game agents, but their slow inference\nspeed prevents practical deployment in real-time game environments. While\nconsistency models offer a promising approach for one-step generation, they\noften suffer from training instability and performance degradation when applied\nto policy learning. In this paper, we present CPQE (Consistency Policy with\nQ-Ensembles), which combines consistency models with Q-ensembles to address\nthese challenges.CPQE leverages uncertainty estimation through Q-ensembles to\nprovide more reliable value function approximations, resulting in better\ntraining stability and improved performance compared to classic double\nQ-network methods. Our extensive experiments across multiple game scenarios\ndemonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant\nimprovement over state-of-the-art diffusion policies that operate at only 20 Hz\n-- while maintaining comparable performance to multi-step diffusion approaches.\nCPQE consistently outperforms state-of-the-art consistency model approaches,\nshowing both higher rewards and enhanced training stability throughout the\nlearning process. These results indicate that CPQE offers a practical solution\nfor deploying diffusion-based policies in games and other real-time\napplications where both multi-modal behavior modeling and rapid inference are\ncritical requirements.",
      "pdf_url": "http://arxiv.org/pdf/2503.16978v1",
      "published": "2025-03-21T09:45:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16978v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation",
      "authors": [
        "Weihao Yu",
        "Xiaoqing Guo",
        "Chenxin Li",
        "Yifan Liu",
        "Yixuan Yuan"
      ],
      "abstract": "Achieving meticulous segmentation of tooth point clouds from intra-oral scans\nstands as an indispensable prerequisite for various orthodontic applications.\nGiven the labor-intensive nature of dental annotation, a significant amount of\ndata remains unlabeled, driving increasing interest in semi-supervised\napproaches. One primary challenge of existing semi-supervised medical\nsegmentation methods lies in noisy pseudo labels generated for unlabeled data.\nTo address this challenge, we propose GeoT, the first framework that employs\ninstance-dependent transition matrix (IDTM) to explicitly model noise in pseudo\nlabels for semi-supervised dental segmentation. Specifically, to handle the\nextensive solution space of IDTM arising from tens of thousands of dental\npoints, we introduce tooth geometric priors through two key components:\npoint-level geometric regularization (PLGR) to enhance consistency between\npoint adjacency relationships in 3D and IDTM spaces, and class-level geometric\nsmoothing (CLGS) to leverage the fixed spatial distribution of tooth categories\nfor optimal IDTM estimation. Extensive experiments performed on the public\nTeeth3DS dataset and private dataset demonstrate that our method can make full\nutilization of unlabeled data to facilitate segmentation, achieving performance\ncomparable to fully supervised methods with only $20\\%$ of the labeled data.",
      "pdf_url": "http://arxiv.org/pdf/2503.16976v1",
      "published": "2025-03-21T09:43:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16976v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks",
      "authors": [
        "Julian Junyan Wang",
        "Victor Xiaoqi Wang"
      ],
      "abstract": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. Simulation analysis reveals that despite measurable\ninconsistency in LLM outputs, downstream statistical inferences remain\nremarkably robust. These findings address concerns about what we term\n\"G-hacking,\" the selective reporting of favorable outcomes from multiple\nGenerative AI runs, by demonstrating that such risks are relatively low for\nfinance and accounting tasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.16974v1",
      "published": "2025-03-21T09:43:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16974v1",
      "categories": [
        "q-fin.GN",
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "ARFlow: Human Action-Reaction Flow Matching with Physical Guidance",
      "authors": [
        "Wentao Jiang",
        "Jingya Wang",
        "Haotao Lu",
        "Kaiyang Ji",
        "Baoxiong Jia",
        "Siyuan Huang",
        "Ye Shi"
      ],
      "abstract": "Human action-reaction synthesis, a fundamental challenge in modeling causal\nhuman interactions, plays a critical role in applications ranging from virtual\nreality to social robotics. While diffusion-based models have demonstrated\npromising performance, they exhibit two key limitations for interaction\nsynthesis: reliance on complex noise-to-reaction generators with intricate\nconditional mechanisms, and frequent physical violations in generated motions.\nTo address these issues, we propose Action-Reaction Flow Matching (ARFlow), a\nnovel framework that establishes direct action-to-reaction mappings,\neliminating the need for complex conditional mechanisms. Our approach\nintroduces two key innovations: an x1-prediction method that directly outputs\nhuman motions instead of velocity fields, enabling explicit constraint\nenforcement; and a training-free, gradient-based physical guidance mechanism\nthat effectively prevents body penetration artifacts during sampling. Extensive\nexperiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only\noutperforms existing methods in terms of Fr\\'echet Inception Distance and\nmotion diversity but also significantly reduces body collisions, as measured by\nour new Intersection Volume and Intersection Frequency metrics.",
      "pdf_url": "http://arxiv.org/pdf/2503.16973v1",
      "published": "2025-03-21T09:41:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16973v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech",
      "authors": [
        "Ji-Hoon Kim",
        "Jeongsoo Choi",
        "Jaehun Kim",
        "Chaeyoung Jung",
        "Joon Son Chung"
      ],
      "abstract": "The objective of this study is to generate high-quality speech from silent\ntalking face videos, a task also known as video-to-speech synthesis. A\nsignificant challenge in video-to-speech synthesis lies in the substantial\nmodality gap between silent video and multi-faceted speech. In this paper, we\npropose a novel video-to-speech system that effectively bridges this modality\ngap, significantly enhancing the quality of synthesized speech. This is\nachieved by learning of hierarchical representations from video to speech.\nSpecifically, we gradually transform silent video into acoustic feature spaces\nthrough three sequential stages -- content, timbre, and prosody modeling. In\neach stage, we align visual factors -- lip movements, face identity, and facial\nexpressions -- with corresponding acoustic counterparts to ensure the seamless\ntransformation. Additionally, to generate realistic and coherent speech from\nthe visual representations, we employ a flow matching model that estimates\ndirect trajectories from a simple prior distribution to the target speech\ndistribution. Extensive experiments demonstrate that our method achieves\nexceptional generation quality comparable to real utterances, outperforming\nexisting methods by a significant margin.",
      "pdf_url": "http://arxiv.org/pdf/2503.16956v1",
      "published": "2025-03-21T09:02:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16956v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.SD"
      ]
    },
    {
      "title": "Neural-Guided Equation Discovery",
      "authors": [
        "Jannis Brugger",
        "Mattia Cerrato",
        "David Richter",
        "Cedric Derstroff",
        "Daniel Maninger",
        "Mira Mezini",
        "Stefan Kramer"
      ],
      "abstract": "Deep learning approaches are becoming increasingly attractive for equation\ndiscovery. We show the advantages and disadvantages of using neural-guided\nequation discovery by giving an overview of recent papers and the results of\nexperiments using our modular equation discovery system MGMT\n($\\textbf{M}$ulti-Task $\\textbf{G}$rammar-Guided $\\textbf{M}$onte-Carlo\n$\\textbf{T}$ree Search for Equation Discovery). The system uses neural-guided\nMonte-Carlo Tree Search (MCTS) and supports both supervised and reinforcement\nlearning, with a search space defined by a context-free grammar. We summarize\nseven desirable properties of equation discovery systems, emphasizing the\nimportance of embedding tabular data sets for such learning approaches. Using\nthe modular structure of MGMT, we compare seven architectures (among them,\nRNNs, CNNs, and Transformers) for embedding tabular datasets on the auxiliary\ntask of contrastive learning for tabular data sets on an equation discovery\ntask. For almost all combinations of modules, supervised learning outperforms\nreinforcement learning. Moreover, our experiments indicate an advantage of\nusing grammar rules as action space instead of tokens. Two adaptations of MCTS\n-- risk-seeking MCTS and AmEx-MCTS -- can improve equation discovery with that\nkind of search.",
      "pdf_url": "http://arxiv.org/pdf/2503.16953v1",
      "published": "2025-03-21T08:55:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16953v1",
      "categories": [
        "cs.AI",
        "I.2.6; I.1.1; G.3"
      ]
    },
    {
      "title": "On-Sensor Convolutional Neural Networks with Early-Exits",
      "authors": [
        "Hazem Hesham Yousef Shalby",
        "Arianna De Vecchi",
        "Alice Scandelli",
        "Pietro Bartoli",
        "Diana Trojaniello",
        "Manuel Roveri",
        "Federica Villa"
      ],
      "abstract": "Tiny Machine Learning (TinyML) is a novel research field aiming at\nintegrating Machine Learning (ML) within embedded devices with limited memory,\ncomputation, and energy. Recently, a new branch of TinyML has emerged, focusing\non integrating ML directly into the sensors to further reduce the power\nconsumption of embedded devices. Interestingly, despite their state-of-the-art\nperformance in many tasks, none of the current solutions in the literature aims\nto optimize the implementation of Convolutional Neural Networks (CNNs)\noperating directly into sensors. In this paper, we introduce for the first time\nin the literature the optimized design and implementation of Depth-First CNNs\noperating on the Intelligent Sensor Processing Unit (ISPU) within an Inertial\nMeasurement Unit (IMU) by STMicroelectronics. Our approach partitions the CNN\nbetween the ISPU and the microcontroller (MCU) and employs an Early-Exit\nmechanism to stop the computations on the IMU when enough confidence about the\nresults is achieved, hence significantly reducing power consumption. When using\na NUCLEO-F411RE board, this solution achieved an average current consumption of\n4.8 mA, marking an 11% reduction compared to the regular inference pipeline on\nthe MCU, while having equal accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2503.16939v1",
      "published": "2025-03-21T08:31:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16939v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Interpretable Machine Learning for Oral Lesion Diagnosis through Prototypical Instances Identification",
      "authors": [
        "Alessio Cascione",
        "Mattia Setzu",
        "Federico A. Galatolo",
        "Mario G. C. A. Cimino",
        "Riccardo Guidotti"
      ],
      "abstract": "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts.",
      "pdf_url": "http://arxiv.org/pdf/2503.16938v1",
      "published": "2025-03-21T08:25:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16938v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Rude Humans and Vengeful Robots: Examining Human Perceptions of Robot Retaliatory Intentions in Professional Settings",
      "authors": [
        "Kate Letheren",
        "Nicole Robinson"
      ],
      "abstract": "Humans and robots are increasingly working in personal and professional\nsettings. In workplace settings, humans and robots may work together as\ncolleagues, potentially leading to social expectations, or violation thereof.\nExtant research has primarily sought to understand social interactions and\nexpectations in personal rather than professional settings, and none of these\nstudies have examined negative outcomes arising from violations of social\nexpectations. This paper reports the results of a 2x3 online experiment that\nused a unique first-person perspective video to immerse participants in a\ncollaborative workplace setting. The results are nuanced and reveal that while\nrobots are expected to act in accordance with social expectations despite human\nbehavior, there are benefits for robots perceived as being the bigger person in\nthe face of human rudeness. Theoretical and practical implications are provided\nwhich discuss the import of these findings for the design of social robots.",
      "pdf_url": "http://arxiv.org/pdf/2503.16932v1",
      "published": "2025-03-21T08:12:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16932v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ]
    }
  ]
}