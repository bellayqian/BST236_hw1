{
  "last_updated": "2025-08-29T00:49:20.071813",
  "papers": [
    {
      "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning",
      "authors": [
        "Zeyi Sun",
        "Yuhang Cao",
        "Jianze Liang",
        "Qiushi Sun",
        "Ziyu Liu",
        "Zhixiong Zhang",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Kai Chen",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.",
      "pdf_url": "http://arxiv.org/pdf/2508.20096v1",
      "published": "2025-08-27T17:59:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20096v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning",
      "authors": [
        "Jinhao Liang",
        "Sven Koenig",
        "Ferdinando Fioretto"
      ],
      "abstract": "Multi-Robot Motion Planning (MRMP) involves generating collision-free\ntrajectories for multiple robots operating in a shared continuous workspace.\nWhile discrete multi-agent path finding (MAPF) methods are broadly adopted due\nto their scalability, their coarse discretization severely limits trajectory\nquality. In contrast, continuous optimization-based planners offer\nhigher-quality paths but suffer from the curse of dimensionality, resulting in\npoor scalability with respect to the number of robots. This paper tackles the\nlimitations of these two approaches by introducing a novel framework that\nintegrates discrete MAPF solvers with constrained generative diffusion models.\nThe resulting framework, called Discrete-Guided Diffusion (DGD), has three key\ncharacteristics: (1) it decomposes the original nonconvex MRMP problem into\ntractable subproblems with convex configuration spaces, (2) it combines\ndiscrete MAPF solutions with constrained optimization techniques to guide\ndiffusion models capture complex spatiotemporal dependencies among robots, and\n(3) it incorporates a lightweight constraint repair mechanism to ensure\ntrajectory feasibility. The proposed method sets a new state-of-the-art\nperformance in large-scale, complex environments, scaling to 100 robots while\nachieving planning efficiency and high success rates.",
      "pdf_url": "http://arxiv.org/pdf/2508.20095v1",
      "published": "2025-08-27T17:59:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20095v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices",
      "authors": [
        "Philippe Zhang",
        "Weili Jiang",
        "Yihao Li",
        "Jing Zhang",
        "Sarah Matta",
        "Yubo Tan",
        "Hui Lin",
        "Haoshen Wang",
        "Jiangtian Pan",
        "Hui Xu",
        "Laurent Borderie",
        "Alexandre Le Guilcher",
        "Béatrice Cochener",
        "Chubin Ou",
        "Gwenolé Quellec",
        "Mathieu Lamard"
      ],
      "abstract": "Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting\nvisual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments\nhave been effective in slowing the progression of neovascular AMD, with better\noutcomes achieved through timely diagnosis and consistent monitoring. Tracking\nthe progression of neovascular activity in OCT scans of patients with exudative\nAMD allows for the development of more personalized and effective treatment\nplans. This was the focus of the Monitoring Age-related Macular Degeneration\nProgression in Optical Coherence Tomography (MARIO) challenge, in which we\nparticipated. In Task 1, which involved classifying the evolution between two\npairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN\nnetwork with model ensembling to further enhance the model's performance. For\nTask 2, which focused on predicting progression over the next three months\nbased on current exam data, we proposed the Patch Progression Masked\nAutoencoder that generates an OCT for the next exam and then classifies the\nevolution between the current OCT and the one generated using our solution from\nTask 1. The results we achieved allowed us to place in the Top 10 for both\ntasks. Some team members are part of the same organization as the challenge\norganizers; therefore, we are not eligible to compete for the prize.",
      "pdf_url": "http://arxiv.org/pdf/2508.20064v1",
      "published": "2025-08-27T17:18:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20064v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Model Science: getting serious about verification, explanation and control of AI systems",
      "authors": [
        "Przemyslaw Biecek",
        "Wojciech Samek"
      ],
      "abstract": "The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.20040v1",
      "published": "2025-08-27T16:50:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20040v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis",
      "authors": [
        "Liana Patel",
        "Negar Arabzadeh",
        "Harshit Gupta",
        "Ankita Sundar",
        "Ion Stoica",
        "Matei Zaharia",
        "Carlos Guestrin"
      ],
      "abstract": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of $19\\%$ across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.",
      "pdf_url": "http://arxiv.org/pdf/2508.20033v1",
      "published": "2025-08-27T16:36:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20033v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Large Language Models (LLMs) for Electronic Design Automation (EDA)",
      "authors": [
        "Kangwei Xu",
        "Denis Schwachhofer",
        "Jason Blocklove",
        "Ilia Polian",
        "Peter Domanski",
        "Dirk Pflüger",
        "Siddharth Garg",
        "Ramesh Karri",
        "Ozgur Sinanoglu",
        "Johann Knechtel",
        "Zhuorui Zhao",
        "Ulf Schlichtmann",
        "Bing Li"
      ],
      "abstract": "With the growing complexity of modern integrated circuits, hardware engineers\nare required to devote more effort to the full design-to-manufacturing\nworkflow. This workflow involves numerous iterations, making it both\nlabor-intensive and error-prone. Therefore, there is an urgent demand for more\nefficient Electronic Design Automation (EDA) solutions to accelerate hardware\ndevelopment. Recently, large language models (LLMs) have shown remarkable\nadvancements in contextual comprehension, logical reasoning, and generative\ncapabilities. Since hardware designs and intermediate scripts can be\nrepresented as text, integrating LLM for EDA offers a promising opportunity to\nsimplify and even automate the entire workflow. Accordingly, this paper\nprovides a comprehensive overview of incorporating LLMs into EDA, with emphasis\non their capabilities, limitations, and future opportunities. Three case\nstudies, along with their outlook, are introduced to demonstrate the\ncapabilities of LLMs in hardware design, testing, and optimization. Finally,\nfuture directions and challenges are highlighted to further explore the\npotential of LLMs in shaping the next-generation EDA, providing valuable\ninsights for researchers interested in leveraging advanced AI technologies for\nEDA.",
      "pdf_url": "http://arxiv.org/pdf/2508.20030v1",
      "published": "2025-08-27T16:33:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20030v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.AR",
        "cs.LG",
        "cs.SY"
      ]
    },
    {
      "title": "Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence",
      "authors": [
        "Ji Wang",
        "Kashing Chen",
        "Xinyuan Song",
        "Ke Zhang",
        "Lynn Ai",
        "Eric Yang",
        "Bill Shi"
      ],
      "abstract": "Most existing Large Language Model (LLM)-based agent frameworks rely on\ncentralized orchestration, incurring high deployment costs, rigid communication\ntopologies, and limited adaptability. To address these challenges, we introduce\nSymphony, a decentralized multi-agent system which enables lightweight LLMs on\nconsumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:\n(1) a decentralized ledger that records capabilities, (2) a Beacon-selection\nprotocol for dynamic task allocation, and (3) weighted result voting based on\nCoTs. This design forms a privacy-saving, scalable, and fault-tolerant\norchestration with low overhead. Empirically, Symphony outperforms existing\nbaselines on reasoning benchmarks, achieving substantial accuracy gains and\ndemonstrating robustness across models of varying capacities.",
      "pdf_url": "http://arxiv.org/pdf/2508.20019v1",
      "published": "2025-08-27T16:27:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20019v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ]
    },
    {
      "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control",
      "authors": [
        "Quanfeng Lu",
        "Zhantao Ma",
        "Shuai Zhong",
        "Jin Wang",
        "Dahai Yu",
        "Michael K. Ng",
        "Ping Luo"
      ],
      "abstract": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.20018v1",
      "published": "2025-08-27T16:27:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20018v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.MA"
      ]
    },
    {
      "title": "HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling",
      "authors": [
        "Matthias Maiterth",
        "Wesley H. Brewer",
        "Jaya S. Kuruvella",
        "Arunavo Dey",
        "Tanzima Z. Islam",
        "Kevin Menear",
        "Dmitry Duplyakin",
        "Rashadul Kabir",
        "Tapasya Patki",
        "Terry Jones",
        "Feiyi Wang"
      ],
      "abstract": "Schedulers are critical for optimal resource utilization in high-performance\ncomputing. Traditional methods to evaluate schedulers are limited to\npost-deployment analysis, or simulators, which do not model associated\ninfrastructure. In this work, we present the first-of-its-kind integration of\nscheduling and digital twins in HPC. This enables what-if studies to understand\nthe impact of parameter configurations and scheduling decisions on the physical\nassets, even before deployment, or regarching changes not easily realizable in\nproduction. We (1) provide the first digital twin framework extended with\nscheduling capabilities, (2) integrate various top-tier HPC systems given their\npublicly available datasets, (3) implement extensions to integrate external\nscheduling simulators. Finally, we show how to (4) implement and evaluate\nincentive structures, as-well-as (5) evaluate machine learning based\nscheduling, in such novel digital-twin based meta-framework to prototype\nscheduling. Our work enables what-if scenarios of HPC systems to evaluate\nsustainability, and the impact on the simulated system.",
      "pdf_url": "http://arxiv.org/pdf/2508.20016v2",
      "published": "2025-08-27T16:21:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20016v2",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment",
      "authors": [
        "Julian Arnold",
        "Niels Lörch"
      ],
      "abstract": "Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is\nbroadly misaligned with respect to human values. To understand when and how\nthis emergent misalignment occurs, we develop a comprehensive framework for\ndetecting and characterizing rapid transitions during fine-tuning using both\ndistributional change detection methods as well as order parameters that are\nformulated in plain English and evaluated by an LLM judge. Using an objective\nstatistical dissimilarity measure, we quantify how the phase transition that\noccurs during fine-tuning affects multiple aspects of the model. In particular,\nwe assess what percentage of the total distributional change in model outputs\nis captured by different aspects, such as alignment or verbosity, providing a\ndecomposition of the overall transition. We also find that the actual\nbehavioral transition occurs later in training than indicated by the peak in\nthe gradient norm alone. Our framework enables the automated discovery and\nquantification of language-based order parameters, which we demonstrate on\nexamples ranging from knowledge questions to politics and ethics.",
      "pdf_url": "http://arxiv.org/pdf/2508.20015v1",
      "published": "2025-08-27T16:19:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20015v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach",
      "authors": [
        "Lotte Gross",
        "Rebecca Walter",
        "Nicole Zoppi",
        "Adrien Justus",
        "Alessandro Gambetti",
        "Qiwei Han",
        "Maximilian Kaiser"
      ],
      "abstract": "This study addresses critical industrial challenges in e-commerce product\ncategorization, namely platform heterogeneity and the structural limitations of\nexisting taxonomies, by developing and deploying a multimodal hierarchical\nclassification framework. Using a dataset of 271,700 products from 40\ninternational fashion e-commerce platforms, we integrate textual features\n(RoBERTa), visual features (ViT), and joint vision--language representations\n(CLIP). We investigate fusion strategies, including early, late, and\nattention-based fusion within a hierarchical architecture enhanced by dynamic\nmasking to ensure taxonomic consistency. Results show that CLIP embeddings\ncombined via an MLP-based late-fusion strategy achieve the highest hierarchical\nF1 (98.59\\%), outperforming unimodal baselines. To address shallow or\ninconsistent categories, we further introduce a self-supervised ``product\nrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which\ndiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with\ncluster purities above 86\\%. Cross-platform experiments reveal a\ndeployment-relevant trade-off: complex late-fusion methods maximize accuracy\nwith diverse training data, while simpler early-fusion methods generalize more\neffectively to unseen platforms. Finally, we demonstrate the framework's\nindustrial scalability through deployment in EURWEB's commercial transaction\nintelligence platform via a two-stage inference pipeline, combining a\nlightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance\ncost and accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.20013v1",
      "published": "2025-08-27T16:16:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20013v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation",
      "authors": [
        "Ziniu Zhang",
        "Zhenshuo Zhang",
        "Dongyue Li",
        "Lu Wang",
        "Jennifer Dy",
        "Hongyang R. Zhang"
      ],
      "abstract": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average.",
      "pdf_url": "http://arxiv.org/pdf/2508.19999v1",
      "published": "2025-08-27T15:59:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19999v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MathBuddy: A Multimodal System for Affective Math Tutoring",
      "authors": [
        "Debanjana Kar",
        "Leopold Böss",
        "Dacia Braca",
        "Sebastian Maximilian Dennerlein",
        "Nina Christine Hubig",
        "Philipp Wintersberger",
        "Yufang Hou"
      ],
      "abstract": "The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions.",
      "pdf_url": "http://arxiv.org/pdf/2508.19993v1",
      "published": "2025-08-27T15:50:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19993v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Diffusion Language Models Know the Answer Before Decoding",
      "authors": [
        "Pengxiang Li",
        "Yefan Zhou",
        "Dilxat Muhtar",
        "Lu Yin",
        "Shilin Yan",
        "Li Shen",
        "Yi Liang",
        "Soroush Vosoughi",
        "Shiwei Liu"
      ],
      "abstract": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.",
      "pdf_url": "http://arxiv.org/pdf/2508.19982v1",
      "published": "2025-08-27T15:40:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19982v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity",
      "authors": [
        "Seongheon Park",
        "Yixuan Li"
      ],
      "abstract": "Object hallucination in large vision-language models presents a significant\nchallenge to their safe deployment in real-world applications. Recent works\nhave proposed object-level hallucination scores to estimate the likelihood of\nobject hallucination; however, these methods typically adopt either a global or\nlocal perspective in isolation, which may limit detection reliability. In this\npaper, we introduce GLSim, a novel training-free object hallucination detection\nframework that leverages complementary global and local embedding similarity\nsignals between image and text modalities, enabling more accurate and reliable\nhallucination detection in diverse scenarios. We comprehensively benchmark\nexisting object hallucination detection methods and demonstrate that GLSim\nachieves superior detection performance, outperforming competitive baselines by\na significant margin.",
      "pdf_url": "http://arxiv.org/pdf/2508.19972v1",
      "published": "2025-08-27T15:30:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19972v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation",
      "authors": [
        "Slimane Bellaouar",
        "Attia Nehar",
        "Soumia Souffi",
        "Mounia Bouameur"
      ],
      "abstract": "Despite its significance, Arabic, a linguistically rich and morphologically\ncomplex language, faces the challenge of being under-resourced. The scarcity of\nlarge annotated datasets hampers the development of accurate tools for\nsubjectivity analysis in Arabic. Recent advances in deep learning and\nTransformers have proven highly effective for text classification in English\nand French. This paper proposes a new approach for subjectivity assessment in\nArabic textual data. To address the dearth of specialized annotated datasets,\nwe developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic\ndatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we\nfine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and\nArabianGPT) on AraDhati+ for effective subjectivity classification.\nFurthermore, we experimented with an ensemble decision approach to harness the\nstrengths of individual models. Our approach achieves a remarkable accuracy of\n97.79\\,\\% for Arabic subjectivity classification. Results demonstrate the\neffectiveness of the proposed approach in addressing the challenges posed by\nlimited resources in Arabic language processing.",
      "pdf_url": "http://arxiv.org/pdf/2508.19966v1",
      "published": "2025-08-27T15:20:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19966v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants",
      "authors": [
        "M. Umlauft",
        "M. Schranz"
      ],
      "abstract": "Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course.",
      "pdf_url": "http://arxiv.org/pdf/2508.19963v1",
      "published": "2025-08-27T15:17:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19963v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments",
      "authors": [
        "Nitish Jaipuria",
        "Lorenzo Gatto",
        "Zijun Kan",
        "Shankey Poddar",
        "Bill Cheung",
        "Diksha Bansal",
        "Ramanan Balakrishnan",
        "Aviral Suri",
        "Jose Estevez"
      ],
      "abstract": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains.",
      "pdf_url": "http://arxiv.org/pdf/2508.19932v1",
      "published": "2025-08-27T14:47:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19932v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution",
      "authors": [
        "Fayaz Ali",
        "Muhammad Zawish",
        "Steven Davy",
        "Radu Timofte"
      ],
      "abstract": "Transformers have demonstrated promising performance in computer vision\ntasks, including image super-resolution (SR). The quadratic computational\ncomplexity of window self-attention mechanisms in many transformer-based SR\nmethods forces the use of small, fixed windows, limiting the receptive field.\nIn this paper, we propose a new approach by embedding the wavelet transform\nwithin a hierarchical transformer framework, called (WaveHiT-SR). First, using\nadaptive hierarchical windows instead of static small windows allows to capture\nfeatures across different levels and greatly improve the ability to model\nlong-range dependencies. Secondly, the proposed model utilizes wavelet\ntransforms to decompose images into multiple frequency subbands, allowing the\nnetwork to focus on both global and local features while preserving structural\ndetails. By progressively reconstructing high-resolution images through\nhierarchical processing, the network reduces computational complexity without\nsacrificing performance. The multi-level decomposition strategy enables the\nnetwork to capture fine-grained information in lowfrequency components while\nenhancing high-frequency textures. Through extensive experimentation, we\nconfirm the effectiveness and efficiency of our WaveHiT-SR. Our refined\nversions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR\nresults, achieving higher efficiency with fewer parameters, lower FLOPs, and\nfaster speeds.",
      "pdf_url": "http://arxiv.org/pdf/2508.19927v1",
      "published": "2025-08-27T14:37:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19927v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology",
      "authors": [
        "Muhammad Waqas",
        "Rukhmini Bandyopadhyay",
        "Eman Showkatian",
        "Amgad Muneer",
        "Anas Zafar",
        "Frank Rojas Alvarez",
        "Maricel Corredor Marin",
        "Wentao Li",
        "David Jaffray",
        "Cara Haymaker",
        "John Heymach",
        "Natalie I Vokes",
        "Luisa Maren Solis Soto",
        "Jianjun Zhang",
        "Jia Wu"
      ],
      "abstract": "Foundation models have recently emerged as powerful feature extractors in\ncomputational pathology, yet they typically omit mechanisms for leveraging the\nglobal spatial structure of tissues and the local contextual relationships\namong diagnostically relevant regions - key elements for understanding the\ntumor microenvironment. Multiple instance learning (MIL) remains an essential\nnext step following foundation model, designing a framework to aggregate\npatch-level features into slide-level predictions. We present EAGLE-Net, a\nstructure-preserving, attention-guided MIL architecture designed to augment\nprediction and interpretability. EAGLE-Net integrates multi-scale absolute\nspatial encoding to capture global tissue architecture, a top-K\nneighborhood-aware loss to focus attention on local microenvironments, and\nbackground suppression loss to minimize false positives. We benchmarked\nEAGLE-Net on large pan-cancer datasets, including three cancer types for\nclassification (10,260 slides) and seven cancer types for survival prediction\n(4,172 slides), using three distinct histology foundation backbones (REMEDIES,\nUni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher\nclassification accuracy and the top concordance indices in 6 of 7 cancer types,\nproducing smooth, biologically coherent attention maps that aligned with expert\nannotations and highlighted invasive fronts, necrosis, and immune infiltration.\nThese results position EAGLE-Net as a generalizable, interpretable framework\nthat complements foundation models, enabling improved biomarker discovery,\nprognostic modeling, and clinical decision support",
      "pdf_url": "http://arxiv.org/pdf/2508.19914v1",
      "published": "2025-08-27T14:19:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19914v1",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling",
      "authors": [
        "Ramya Keerthy Thatikonda",
        "Wray Buntine",
        "Ehsan Shareghi"
      ],
      "abstract": "Logical reasoning is a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs), as it reflects their ability to derive valid\nconclusions from given premises. While the combination of test-time scaling\nwith dedicated outcome or process reward models has opened up new avenues to\nenhance LLMs performance in complex reasoning tasks, this space is\nunder-explored in deductive logical reasoning. We present a set of Outcome\nReward Models (ORMs) for deductive reasoning. To train the ORMs we mainly\ngenerate data using Chain-of-Thought (CoT) with single and multiple samples.\nAdditionally, we propose a novel tactic to further expand the type of errors\ncovered in the training dataset of the ORM. In particular, we propose an echo\ngeneration technique that leverages LLMs' tendency to reflect incorrect\nassumptions made in prompts to extract additional training data, covering\npreviously unexplored error types. While a standard CoT chain may contain\nerrors likely to be made by the reasoner, the echo strategy deliberately steers\nthe model toward incorrect reasoning. We show that ORMs trained on CoT and\necho-augmented data demonstrate improved performance on the FOLIO, JustLogic,\nand ProverQA datasets across four different LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2508.19903v1",
      "published": "2025-08-27T14:08:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19903v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "The Information Dynamics of Generative Diffusion",
      "authors": [
        "Luca Ambrogioni"
      ],
      "abstract": "Generative diffusion models have emerged as a powerful class of models in\nmachine learning, yet a unified theoretical understanding of their operation is\nstill developing. This perspective paper provides an integrated perspective on\ngenerative diffusion by connecting their dynamic, information-theoretic, and\nthermodynamic properties under a unified mathematical framework. We demonstrate\nthat the rate of conditional entropy production during generation (i.e. the\ngenerative bandwidth) is directly governed by the expected divergence of the\nscore function's vector field. This divergence, in turn, is linked to the\nbranching of trajectories and generative bifurcations, which we characterize as\nsymmetry-breaking phase transitions in the energy landscape. This synthesis\noffers a powerful insight: the process of generation is fundamentally driven by\nthe controlled, noise-induced breaking of (approximate) symmetries, where peaks\nin information transfer correspond to critical transitions between possible\noutcomes. The score function acts as a dynamic non-linear filter that regulates\nthe bandwidth of the noise by suppressing fluctuations that are incompatible\nwith the data.",
      "pdf_url": "http://arxiv.org/pdf/2508.19897v1",
      "published": "2025-08-27T13:53:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19897v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AI-Powered Detection of Inappropriate Language in Medical School Curricula",
      "authors": [
        "Chiman Salavati",
        "Shannon Song",
        "Scott A. Hale",
        "Roberto E. Montenegro",
        "Shiri Dori-Hacohen",
        "Fabricio Murai"
      ],
      "abstract": "The use of inappropriate language -- such as outdated, exclusionary, or\nnon-patient-centered terms -- medical instructional materials can significantly\ninfluence clinical training, patient interactions, and health outcomes. Despite\ntheir reputability, many materials developed over past decades contain examples\nnow considered inappropriate by current medical standards. Given the volume of\ncurricular content, manually identifying instances of inappropriate use of\nlanguage (IUL) and its subcategories for systematic review is prohibitively\ncostly and impractical. To address this challenge, we conduct a first-in-class\nevaluation of small language models (SLMs) fine-tuned on labeled data and\npre-trained LLMs with in-context learning on a dataset containing approximately\n500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL\nclassifier, (2) subcategory-specific binary classifiers, (3) a multilabel\nclassifier, and (4) a two-stage hierarchical pipeline for general IUL detection\nfollowed by multilabel classification. For LLMs, we consider variations of\nprompts that include subcategory definitions and/or shots. We found that both\nLLama-3 8B and 70B, even with carefully curated shots, are largely outperformed\nby SLMs. While the multilabel classifier performs best on annotated data,\nsupplementing training with unflagged excerpts as negative examples boosts the\nspecific classifiers' AUC by up to 25%, making them most effective models for\nmitigating harmful language in medical curricula.",
      "pdf_url": "http://arxiv.org/pdf/2508.19883v1",
      "published": "2025-08-27T13:40:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19883v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "I.2.1; I.2.7"
      ]
    },
    {
      "title": "Generative AI for Testing of Autonomous Driving Systems: A Survey",
      "authors": [
        "Qunying Song",
        "He Ye",
        "Mark Harman",
        "Federica Sarro"
      ],
      "abstract": "Autonomous driving systems (ADS) have been an active area of research, with\nthe potential to deliver significant benefits to society. However, before\nlarge-scale deployment on public roads, extensive testing is necessary to\nvalidate their functionality and safety under diverse driving conditions.\nTherefore, different testing approaches are required, and achieving effective\nand efficient testing of ADS remains an open challenge. Recently, generative AI\nhas emerged as a powerful tool across many domains, and it is increasingly\nbeing applied to ADS testing due to its ability to interpret context, reason\nabout complex tasks, and generate diverse outputs. To gain a deeper\nunderstanding of its role in ADS testing, we systematically analyzed 91\nrelevant studies and synthesized their findings into six major application\ncategories, primarily centered on scenario-based testing of ADS. We also\nreviewed their effectiveness and compiled a wide range of datasets, simulators,\nADS, metrics, and benchmarks used for evaluation, while identifying 27\nlimitations. This survey provides an overview and practical insights into the\nuse of generative AI for testing ADS, highlights existing challenges, and\noutlines directions for future research in this rapidly evolving field.",
      "pdf_url": "http://arxiv.org/pdf/2508.19882v1",
      "published": "2025-08-27T13:40:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19882v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Multispectral LiDAR data for extracting tree points in urban and suburban areas",
      "authors": [
        "Narges Takhtkeshha",
        "Gabriele Mazzacca",
        "Fabio Remondino",
        "Juha Hyyppä",
        "Gottfried Mandlburger"
      ],
      "abstract": "Monitoring urban tree dynamics is vital for supporting greening policies and\nreducing risks to electrical infrastructure. Airborne laser scanning has\nadvanced large-scale tree management, but challenges remain due to complex\nurban environments and tree variability. Multispectral (MS) light detection and\nranging (LiDAR) improves this by capturing both 3D spatial and spectral data,\nenabling detailed mapping. This study explores tree point extraction using\nMS-LiDAR and deep learning (DL) models. Three state-of-the-art models are\nevaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point\nTransformer V1 (PTv1). Results show the notable time efficiency and accuracy of\nSPT, with a mean intersection over union (mIoU) of 85.28%. The highest\ndetection accuracy is achieved by incorporating pseudo normalized difference\nvegetation index (pNDVI) with spatial data, reducing error rate by 10.61\npercentage points (pp) compared to using spatial information alone. These\nfindings highlight the potential of MS-LiDAR and DL to improve tree extraction\nand further tree inventories.",
      "pdf_url": "http://arxiv.org/pdf/2508.19881v1",
      "published": "2025-08-27T13:39:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19881v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Tracking World States with Language Models: State-Based Evaluation Using Chess",
      "authors": [
        "Romain Harang",
        "Jason Naradowsky",
        "Yaswitha Gujju",
        "Yusuke Miyao"
      ],
      "abstract": "Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments.",
      "pdf_url": "http://arxiv.org/pdf/2508.19851v1",
      "published": "2025-08-27T13:08:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19851v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SoK: Large Language Model Copyright Auditing via Fingerprinting",
      "authors": [
        "Shuo Shao",
        "Yiming Li",
        "Yu He",
        "Hongwei Yao",
        "Wenyuan Yang",
        "Dacheng Tao",
        "Zhan Qin"
      ],
      "abstract": "The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench.",
      "pdf_url": "http://arxiv.org/pdf/2508.19843v1",
      "published": "2025-08-27T12:56:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19843v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "PSO-Merging: Merging Models Based on Particle Swarm Optimization",
      "authors": [
        "Kehao Zhang",
        "Shaolei Zhang",
        "Yang Feng"
      ],
      "abstract": "Model merging has emerged as an efficient strategy for constructing multitask\nmodels by integrating the strengths of multiple available expert models,\nthereby reducing the need to fine-tune a pre-trained model for all the tasks\nfrom scratch. Existing data-independent methods struggle with performance\nlimitations due to the lack of data-driven guidance. Data-driven approaches\nalso face key challenges: gradient-based methods are computationally expensive,\nlimiting their practicality for merging large expert models, whereas existing\ngradient-free methods often fail to achieve satisfactory results within a\nlimited number of optimization steps. To address these limitations, this paper\nintroduces PSO-Merging, a novel data-driven merging method based on the\nParticle Swarm Optimization (PSO). In this approach, we initialize the particle\nswarm with a pre-trained model, expert models, and sparsified expert models. We\nthen perform multiple iterations, with the final global best particle serving\nas the merged model. Experimental results on different language models show\nthat PSO-Merging generally outperforms baseline merging methods, offering a\nmore efficient and scalable solution for model merging.",
      "pdf_url": "http://arxiv.org/pdf/2508.19839v1",
      "published": "2025-08-27T12:52:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19839v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Gradient Rectification for Robust Calibration under Distribution Shift",
      "authors": [
        "Yilin Zhang",
        "Cai Xu",
        "You Wu",
        "Ziyu Guan",
        "Wei Zhao"
      ],
      "abstract": "Deep neural networks often produce overconfident predictions, undermining\ntheir reliability in safety-critical applications. This miscalibration is\nfurther exacerbated under distribution shift, where test data deviates from the\ntraining distribution due to environmental or acquisition changes. While\nexisting approaches improve calibration through training-time regularization or\npost-hoc adjustment, their reliance on access to or simulation of target\ndomains limits their practicality in real-world scenarios. In this paper, we\npropose a novel calibration framework that operates without access to target\ndomain information. From a frequency-domain perspective, we identify that\ndistribution shifts often distort high-frequency visual cues exploited by deep\nmodels, and introduce a low-frequency filtering strategy to encourage reliance\non domain-invariant features. However, such information loss may degrade\nIn-Distribution (ID) calibration performance. Therefore, we further propose a\ngradient-based rectification mechanism that enforces ID calibration as a hard\nconstraint during optimization. Experiments on synthetic and real-world shifted\ndatasets, including CIFAR-10/100-C and WILDS, demonstrate that our method\nsignificantly improves calibration under distribution shift while maintaining\nstrong in-distribution performance.",
      "pdf_url": "http://arxiv.org/pdf/2508.19830v1",
      "published": "2025-08-27T12:28:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19830v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?",
      "authors": [
        "Samuel Lewis-Lim",
        "Xingwei Tan",
        "Zhixue Zhao",
        "Nikolaos Aletras"
      ],
      "abstract": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.",
      "pdf_url": "http://arxiv.org/pdf/2508.19827v1",
      "published": "2025-08-27T12:25:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19827v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning",
      "authors": [
        "Viktor Valadi",
        "Mattias Åkesson",
        "Johan Östman",
        "Salman Toor",
        "Andreas Hellander"
      ],
      "abstract": "Gradient inversion attacks have garnered attention for their ability to\ncompromise privacy in federated learning. However, many studies consider\nattacks with the model in inference mode, where training-time behaviors like\ndropout are disabled and batch normalization relies on fixed statistics. In\nthis work, we systematically analyze how architecture and training behavior\naffect vulnerability, including the first in-depth study of inference-mode\nclients, which we show dramatically simplifies inversion. To assess attack\nfeasibility under more realistic conditions, we turn to clients operating in\nstandard training mode. In this setting, we find that successful attacks are\nonly possible when several architectural conditions are met simultaneously:\nmodels must be shallow and wide, use skip connections, and, critically, employ\npre-activation normalization. We introduce two novel attacks against models in\ntraining-mode with varying attacker knowledge, achieving state-of-the-art\nperformance under realistic training conditions. We extend these efforts by\npresenting the first attack on a production-grade object-detection model. Here,\nto enable any visibly identifiable leakage, we revert to the lenient inference\nmode setting and make multiple architectural modifications to increase model\nvulnerability, with the extent of required changes highlighting the strong\ninherent robustness of such architectures. We conclude this work by offering\nthe first comprehensive mapping of settings, clarifying which combinations of\narchitectural choices and operational modes meaningfully impact privacy. Our\nanalysis provides actionable insight into when models are likely vulnerable,\nwhen they appear robust, and where subtle leakage may persist. Together, these\nfindings reframe how gradient inversion risk should be assessed in future\nresearch and deployment scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2508.19819v1",
      "published": "2025-08-27T12:07:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19819v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images",
      "authors": [
        "Linkuan Zhou",
        "Zhexin Chen",
        "Yufei Shen",
        "Junlin Xu",
        "Ping Xuan",
        "Yixin Zhu",
        "Yuqi Fang",
        "Cong Cong",
        "Leyi Wei",
        "Ran Su",
        "Jia Zhou",
        "Qiangguo Jin"
      ],
      "abstract": "Automated segmentation of the fetal head in ultrasound images is critical for\nprenatal monitoring. However, achieving robust segmentation remains challenging\ndue to the poor quality of ultrasound images and the lack of annotated data.\nSemi-supervised methods alleviate the lack of annotated data but struggle with\nthe unique characteristics of fetal head ultrasound images, making it\nchallenging to generate reliable pseudo-labels and enforce effective\nconsistency regularization constraints. To address this issue, we propose a\nnovel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.\nOur framework consists of the dual-scoring adaptive filtering strategy, the\nellipse-constrained pseudo-label refinement, and the symmetry-based multiple\nconsistency regularization. The dual-scoring adaptive filtering strategy uses\nboundary consistency and contour regularity criteria to evaluate and filter\nteacher outputs. The ellipse-constrained pseudo-label refinement refines these\nfiltered outputs by fitting least-squares ellipses, which strengthens pixels\nnear the center of the fitted ellipse and suppresses noise simultaneously. The\nsymmetry-based multiple consistency regularization enforces multi-level\nconsistency across perturbed images, symmetric regions, and between original\npredictions and pseudo-labels, enabling the model to capture robust and stable\nshape representations. Our method achieves state-of-the-art performance on two\nbenchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%\nwith 10% and 20% labeled data, respectively. On the PSFH dataset, the scores\nare 91.68% and 93.70% under the same settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.19815v1",
      "published": "2025-08-27T12:01:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19815v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Bootstrapping Learned Cost Models with Synthetic SQL Queries",
      "authors": [
        "Michael Nidd",
        "Christoph Miksovic",
        "Thomas Gschwind",
        "Francesco Fusco",
        "Andrea Giovannini",
        "Ioana Giurgiu"
      ],
      "abstract": "Having access to realistic workloads for a given database instance is\nextremely important to enable stress and vulnerability testing, as well as to\noptimize for cost and performance. Recent advances in learned cost models have\nshown that when enough diverse SQL queries are available, one can effectively\nand efficiently predict the cost of running a given query against a specific\ndatabase engine. In this paper, we describe our experience in exploiting modern\nsynthetic data generation techniques, inspired by the generative AI and LLM\ncommunity, to create high-quality datasets enabling the effective training of\nsuch learned cost models. Initial results show that we can improve a learned\ncost model's predictive accuracy by training it with 45% fewer queries than\nwhen using competitive generation approaches.",
      "pdf_url": "http://arxiv.org/pdf/2508.19807v1",
      "published": "2025-08-27T11:50:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19807v1",
      "categories": [
        "cs.DB",
        "cs.AI"
      ]
    },
    {
      "title": "A bag of tricks for real-time Mitotic Figure detection",
      "authors": [
        "Christian Marzahl",
        "Brian Napora"
      ],
      "abstract": "Mitotic figure (MF) detection in histopathology images is challenging due to\nlarge variations in slide scanners, staining protocols, tissue types, and the\npresence of artifacts. This paper presents a collection of training techniques\n- a bag of tricks - that enable robust, real-time MF detection across diverse\ndomains. We build on the efficient RTMDet single stage object detector to\nachieve high inference speed suitable for clinical deployment. Our method\naddresses scanner variability and tumor heterogeneity via extensive\nmulti-domain training data, balanced sampling, and careful augmentation.\nAdditionally, we employ targeted, hard negative mining on necrotic and debris\ntissue to reduce false positives. In a grouped 5-fold cross-validation across\nmultiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On\nthe preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025\nchallenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,\noutperforming larger models and demonstrating adaptability to new, unfamiliar\ndomains. The proposed solution offers a practical trade-off between accuracy\nand speed, making it attractive for real-world clinical adoption.",
      "pdf_url": "http://arxiv.org/pdf/2508.19804v1",
      "published": "2025-08-27T11:45:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19804v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks",
      "authors": [
        "Aritra Dutta",
        "Swapnanil Mukherjee",
        "Deepanway Ghosal",
        "Somak Aditya"
      ],
      "abstract": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.",
      "pdf_url": "http://arxiv.org/pdf/2508.19724v2",
      "published": "2025-08-27T09:34:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19724v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Attention is also needed for form design",
      "authors": [
        "B. Sankar",
        "Dibakar Sen"
      ],
      "abstract": "Conventional product design is a cognitively demanding process, limited by\nits time-consuming nature, reliance on subjective expertise, and the opaque\ntranslation of inspiration into tangible concepts. This research introduces a\nnovel, attention-aware framework that integrates two synergistic systems:\nEUPHORIA, an immersive Virtual Reality environment using eye-tracking to\nimplicitly capture a designer's aesthetic preferences, and RETINA, an agentic\nAI pipeline that translates these implicit preferences into concrete design\noutputs. The foundational principles were validated in a two-part study. An\ninitial study correlated user's implicit attention with explicit preference and\nthe next one correlated mood to attention. A comparative study where 4\ndesigners solved challenging design problems using 4 distinct workflows, from a\nmanual process to an end-to-end automated pipeline, showed the integrated\nEUPHORIA-RETINA workflow was over 4 times more time-efficient than the\nconventional method. A panel of 50 design experts evaluated the 16 final\nrenderings. Designs generated by the fully automated system consistently\nreceived the highest Worthiness (calculated by an inverse Plackett-Luce model\nbased on gradient descent optimization) and Design Effectiveness scores,\nindicating superior quality across 8 criteria: novelty, visual appeal,\nemotional resonance, clarity of purpose, distinctiveness of silhouette, implied\nmateriality, proportional balance, & adherence to the brief. This research\npresents a validated paradigm shift from traditional Computer-Assisted Design\n(CAD) to a collaborative model of Designer-Assisting Computers (DAC). By\nautomating logistical and skill-dependent generative tasks, the proposed\nframework elevates the designer's role to that of a creative director,\nsynergizing human intuition with the generative power of agentic AI to produce\nhigher-quality designs more efficiently.",
      "pdf_url": "http://arxiv.org/pdf/2508.19708v1",
      "published": "2025-08-27T09:15:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19708v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "68T07, 68T42, 68T50",
        "I.2; J.5; J.6"
      ]
    },
    {
      "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads",
      "authors": [
        "Chao Huang",
        "Zefeng Zhang",
        "Juewei Yue",
        "Quangang Li",
        "Chuang Zhang",
        "Tingwen Liu"
      ],
      "abstract": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility.",
      "pdf_url": "http://arxiv.org/pdf/2508.19697v1",
      "published": "2025-08-27T09:06:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19697v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Topological Uncertainty for Anomaly Detection in the Neural-network EoS Inference with Neutron Star Data",
      "authors": [
        "Kenji Fukushima",
        "Syo Kamata"
      ],
      "abstract": "We study the performance of the Topological Uncertainty (TU) constructed with\na trained feedforward neural network (FNN) for Anomaly Detection. Generally,\nmeaningful information can be stored in the hidden layers of the trained FNN,\nand the TU implementation is one tractable recipe to extract buried information\nby means of the Topological Data Analysis. We explicate the concept of the TU\nand the numerical procedures. Then, for a concrete demonstration of the\nperformance test, we employ the Neutron Star data used for inference of the\nequation of state (EoS). For the training dataset consisting of the input\n(Neutron Star data) and the output (EoS parameters), we can compare the\ninferred EoSs and the exact answers to classify the data with the label $k$.\nThe subdataset with $k=0$ leads to the normal inference for which the inferred\nEoS approximates the answer well, while the subdataset with $k=1$ ends up with\nthe unsuccessful inference. Once the TU is prepared based on the $k$-labled\nsubdatasets, we introduce the cross-TU to quantify the uncertainty of\ncharacterizing the $k$-labeled data with the label $j$. The anomaly or\nunsuccessful inference is correctly detected if the cross-TU for $j=k=1$ is\nsmaller than that for $j=0$ and $k=1$. In our numerical experiment, for various\ninput data, we calculate the cross-TU and estimate the performance of Anomaly\nDetection. We find that performance depends on FNN hyperparameters, and the\nsuccess rate of Anomaly Detection exceeds $90\\%$ in the best case. We finally\ndiscuss further potential of the TU application to retrieve the information\nhidden in the trained FNN.",
      "pdf_url": "http://arxiv.org/pdf/2508.19683v1",
      "published": "2025-08-27T08:44:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19683v1",
      "categories": [
        "nucl-th",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning",
      "authors": [
        "Qihang Ai",
        "Pi Bu",
        "Yue Cao",
        "Yingyao Wang",
        "Jihao Gu",
        "Jingxuan Xing",
        "Zekun Zhu",
        "Wei Jiang",
        "Zhicheng Zheng",
        "Jun Song",
        "Yuning Jiang",
        "Bo Zheng"
      ],
      "abstract": "Recent advances in Vision-Language Models (VLMs) have enabled mobile agents\nto perceive and interact with real-world mobile environments based on human\ninstructions. However, the current fully autonomous paradigm poses potential\nsafety risks when model understanding or reasoning capabilities are\ninsufficient. To address this challenge, we first introduce\n\\textbf{InquireBench}, a comprehensive benchmark specifically designed to\nevaluate mobile agents' capabilities in safe interaction and proactive inquiry\nwith users, encompassing 5 categories and 22 sub-categories, where most\nexisting VLM-based agents demonstrate near-zero performance. In this paper, we\naim to develop an interactive system that actively seeks human confirmation at\ncritical decision points. To achieve this, we propose \\textbf{InquireMobile}, a\nnovel model inspired by reinforcement learning, featuring a two-stage training\nstrategy and an interactive pre-action reasoning mechanism. Finally, our model\nachieves an 46.8% improvement in inquiry success rate and the best overall\nsuccess rate among existing baselines on InquireBench. We will open-source all\ndatasets, models, and evaluation codes to facilitate development in both\nacademia and industry.",
      "pdf_url": "http://arxiv.org/pdf/2508.19679v1",
      "published": "2025-08-27T08:40:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19679v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Survey of Specialized Large Language Model",
      "authors": [
        "Chenghan Yang",
        "Ruiyu Zhao",
        "Yang Liu",
        "Ling Jiang"
      ],
      "abstract": "The rapid evolution of specialized large language models (LLMs) has\ntransitioned from simple domain adaptation to sophisticated native\narchitectures, marking a paradigm shift in AI development. This survey\nsystematically examines this progression across healthcare, finance, legal, and\ntechnical domains. Besides the wide use of specialized LLMs, technical\nbreakthrough such as the emergence of domain-native designs beyond fine-tuning,\ngrowing emphasis on parameter efficiency through sparse computation and\nquantization, increasing integration of multimodal capabilities and so on are\napplied to recent LLM agent. Our analysis reveals how these innovations address\nfundamental limitations of general-purpose LLMs in professional applications,\nwith specialized models consistently performance gains on domain-specific\nbenchmarks. The survey further highlights the implications for E-Commerce field\nto fill gaps in the field.",
      "pdf_url": "http://arxiv.org/pdf/2508.19667v1",
      "published": "2025-08-27T08:27:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19667v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation",
      "authors": [
        "Vojtech Mrazek",
        "Konstantinos Balaskas",
        "Paula Carolina Lozano Duarte",
        "Zdenek Vasicek",
        "Mehdi B. Tahoori",
        "Georgios Zervakis"
      ],
      "abstract": "Printed electronics offer a promising alternative for applications beyond\nsilicon-based systems, requiring properties like flexibility, stretchability,\nconformality, and ultra-low fabrication costs. Despite the large feature sizes\nin printed electronics, printed neural networks have attracted attention for\nmeeting target application requirements, though realizing complex circuits\nremains challenging. This work bridges the gap between classification accuracy\nand area efficiency in printed neural networks, covering the entire\nprocessing-near-sensor system design and co-optimization from the\nanalog-to-digital interface-a major area and power bottleneck-to the digital\nclassifier. We propose an automated framework for designing printed Ternary\nNeural Networks with arbitrary input precision, utilizing multi-objective\noptimization and holistic approximation. Our circuits outperform existing\napproximate printed neural networks by 17x in area and 59x in power on average,\nbeing the first to enable printed-battery-powered operation with under 5%\naccuracy loss while accounting for analog-to-digital interfacing costs.",
      "pdf_url": "http://arxiv.org/pdf/2508.19660v1",
      "published": "2025-08-27T08:19:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19660v1",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses",
      "authors": [
        "Lincan Li",
        "Bolin Shen",
        "Chenxi Zhao",
        "Yuxiang Sun",
        "Kaixiang Zhao",
        "Shirui Pan",
        "Yushun Dong"
      ],
      "abstract": "Graph-structured data, which captures non-Euclidean relationships and\ninteractions between entities, is growing in scale and complexity. As a result,\ntraining state-of-the-art graph machine learning (GML) models have become\nincreasingly resource-intensive, turning these models and data into invaluable\nIntellectual Property (IP). To address the resource-intensive nature of model\ntraining, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an\nefficient solution by leveraging third-party cloud services for model\ndevelopment and management. However, deploying such models in GMLaaS also\nexposes them to potential threats from attackers. Specifically, while the APIs\nwithin a GMLaaS system provide interfaces for users to query the model and\nreceive outputs, they also allow attackers to exploit and steal model\nfunctionalities or sensitive training data, posing severe threats to the safety\nof these GML models and the underlying graph data. To address these challenges,\nthis survey systematically introduces the first taxonomy of threats and\ndefenses at the level of both GML model and graph-structured data. Such a\ntailored taxonomy facilitates an in-depth understanding of GML IP protection.\nFurthermore, we present a systematic evaluation framework to assess the\neffectiveness of IP protection methods, introduce a curated set of benchmark\ndatasets across various domains, and discuss their application scopes and\nfuture challenges. Finally, we establish an open-sourced versatile library\nnamed PyGIP, which evaluates various attack and defense techniques in GMLaaS\nscenarios and facilitates the implementation of existing benchmark methods. The\nlibrary resource can be accessed at: https://labrai.github.io/PyGIP. We believe\nthis survey will play a fundamental role in intellectual property protection\nfor GML and provide practical recipes for the GML community.",
      "pdf_url": "http://arxiv.org/pdf/2508.19641v1",
      "published": "2025-08-27T07:37:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19641v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception",
      "authors": [
        "Yang Li",
        "Quan Yuan",
        "Guiyang Luo",
        "Xiaoyuan Fu",
        "Rui Pan",
        "Yujia Yang",
        "Congzhang Shao",
        "Yuewen Liu",
        "Jinglin Li"
      ],
      "abstract": "Collaborative perception allows agents to enhance their perceptual\ncapabilities by exchanging intermediate features. Existing methods typically\norganize these intermediate features as 2D bird's-eye-view (BEV)\nrepresentations, which discard critical fine-grained 3D structural cues\nessential for accurate object recognition and localization. To this end, we\nfirst introduce point-level tokens as intermediate representations for\ncollaborative perception. However, point-cloud data are inherently unordered,\nmassive, and position-sensitive, making it challenging to produce compact and\naligned point-level token sequences that preserve detailed structural\ninformation. Therefore, we present CoPLOT, a novel Collaborative perception\nframework that utilizes Point-Level Optimized Tokens. It incorporates a\npoint-native processing pipeline, including token reordering, sequence\nmodeling, and multi-agent spatial alignment. A semantic-aware token reordering\nmodule generates adaptive 1D reorderings by leveraging scene-level and\ntoken-level semantic information. A frequency-enhanced state space model\ncaptures long-range sequence dependencies across both spatial and spectral\ndomains, improving the differentiation between foreground tokens and background\nclutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop\nprocess, combining global agent-level correction with local token-level\nrefinement to mitigate localization noise. Extensive experiments on both\nsimulated and real-world datasets show that CoPLOT outperforms state-of-the-art\nmodels, with even lower communication and computation overhead. Code will be\navailable at https://github.com/CheeryLeeyy/CoPLOT.",
      "pdf_url": "http://arxiv.org/pdf/2508.19638v1",
      "published": "2025-08-27T07:27:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19638v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge",
      "authors": [
        "Maha Shatta",
        "Konstantinos Balaskas",
        "Paula Carolina Lozano Duarte",
        "Georgios Panagopoulos",
        "Mehdi B. Tahoori",
        "Georgios Zervakis"
      ],
      "abstract": "Flexible Electronics (FE) offer a promising alternative to rigid\nsilicon-based hardware for wearable healthcare devices, enabling lightweight,\nconformable, and low-cost systems. However, their limited integration density\nand large feature sizes impose strict area and power constraints, making\nML-based healthcare systems-integrating analog frontend, feature extraction and\nclassifier-particularly challenging. Existing FE solutions often neglect\npotential system-wide solutions and focus on the classifier, overlooking the\nsubstantial hardware cost of feature extraction and Analog-to-Digital\nConverters (ADCs)-both major contributors to area and power consumption. In\nthis work, we present a holistic mixed-signal feature-to-classifier co-design\nframework for flexible smart wearable systems. To the best of our knowledge, we\ndesign the first analog feature extractors in FE, significantly reducing\nfeature extraction cost. We further propose an hardware-aware NAS-inspired\nfeature selection strategy within ML training, enabling efficient,\napplication-specific designs. Our evaluation on healthcare benchmarks shows our\napproach delivers highly accurate, ultra-area-efficient flexible systems-ideal\nfor disposable, low-power wearable monitoring.",
      "pdf_url": "http://arxiv.org/pdf/2508.19637v1",
      "published": "2025-08-27T07:26:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19637v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition",
      "authors": [
        "Xiaolei Wei",
        "Yi Ouyang",
        "Haibo Ye"
      ],
      "abstract": "Long-tailed visual recognition is challenging not only due to class imbalance\nbut also because of varying classification difficulty across categories. Simply\nreweighting classes by frequency often overlooks those that are intrinsically\nhard to learn. To address this, we propose \\textbf{DQRoute}, a modular\nframework that combines difficulty-aware optimization with dynamic expert\ncollaboration. DQRoute first estimates class-wise difficulty based on\nprediction uncertainty and historical performance, and uses this signal to\nguide training with adaptive loss weighting. On the architectural side, DQRoute\nemploys a mixture-of-experts design, where each expert specializes in a\ndifferent region of the class distribution. At inference time, expert\npredictions are weighted by confidence scores derived from expert-specific OOD\ndetectors, enabling input-adaptive routing without the need for a centralized\nrouter. All components are trained jointly in an end-to-end manner. Experiments\non standard long-tailed benchmarks demonstrate that DQRoute significantly\nimproves performance, particularly on rare and difficult classes, highlighting\nthe benefit of integrating difficulty modeling with decentralized expert\nrouting.",
      "pdf_url": "http://arxiv.org/pdf/2508.19630v1",
      "published": "2025-08-27T07:09:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19630v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Training for Obsolescence? The AI-Driven Education Trap",
      "authors": [
        "Andrew J. Peterson"
      ],
      "abstract": "Artificial intelligence simultaneously transforms human capital production in\nschools and its demand in labor markets. Analyzing these effects in isolation\ncan lead to a significant misallocation of educational resources. We model an\neducational planner whose decision to adopt AI is driven by its teaching\nproductivity, failing to internalize AI's future wage-suppressing effect on\nthose same skills. Our core assumption, motivated by a pilot survey, is that\nthere is a positive correlation between these two effects. This drives our\ncentral proposition: this information failure creates a skill mismatch that\nmonotonically increases with AI prevalence. Extensions show the mismatch is\nexacerbated by the neglect of unpriced non-cognitive skills and by a school's\nendogenous over-investment in AI. Our findings caution that policies promoting\nAI in education, if not paired with forward-looking labor market signals, may\nparadoxically undermine students' long-term human capital, especially if\nreliance on AI crowds out the development of unpriced non-cognitive skills,\nsuch as persistence, that are forged through intellectual struggle.",
      "pdf_url": "http://arxiv.org/pdf/2508.19625v1",
      "published": "2025-08-27T07:04:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19625v1",
      "categories": [
        "econ.GN",
        "cs.AI",
        "q-fin.EC"
      ]
    },
    {
      "title": "Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning",
      "authors": [
        "Tiandi Ye",
        "Wenyan Liu",
        "Kai Yao",
        "Lichun Li",
        "Shangchao Su",
        "Cen Chen",
        "Xiang Li",
        "Shan Yin",
        "Ming Gao"
      ],
      "abstract": "Federated learning (FL) is a privacy-preserving machine learning paradigm\nthat enables collaborative model training across multiple distributed clients\nwithout disclosing their raw data. Personalized federated learning (pFL) has\ngained increasing attention for its ability to address data heterogeneity.\nHowever, most existing pFL methods assume that each client's data follows a\nsingle distribution and learn one client-level personalized model for each\nclient. This assumption often fails in practice, where a single client may\npossess data from multiple sources or domains, resulting in significant\nintra-client heterogeneity and suboptimal performance. To tackle this\nchallenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework\nbased on visual prompt tuning. Specifically, we formulate instance-wise prompt\ngeneration from a Bayesian perspective and model the prompt posterior as an\nimplicit distribution to capture diverse visual semantics. We derive a\nvariational training objective under the semi-implicit variational inference\nframework. Extensive experiments on benchmark datasets demonstrate that\npFedBayesPT consistently outperforms existing pFL methods under both feature\nand label heterogeneity settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.19621v1",
      "published": "2025-08-27T06:59:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19621v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Scenario-Oriented Survey of Federated Recommender Systems: Techniques, Challenges, and Future Directions",
      "authors": [
        "Yunqi Mi",
        "Jiakui Shen",
        "Guoshuai Zhao",
        "Jialie Shen",
        "Xueming Qian"
      ],
      "abstract": "Extending recommender systems to federated learning (FL) frameworks to\nprotect the privacy of users or platforms while making recommendations has\nrecently gained widespread attention in academia. This is due to the natural\ncoupling of recommender systems and federated learning architectures: the data\noriginates from distributed clients (mostly mobile devices held by users),\nwhich are highly related to privacy. In a centralized recommender system\n(CenRec), the central server collects clients' data, trains the model, and\nprovides the service. Whereas in federated recommender systems (FedRec), the\nstep of data collecting is omitted, and the step of model training is offloaded\nto each client. The server only aggregates the model and other knowledge, thus\navoiding client privacy leakage. Some surveys of federated recommender systems\ndiscuss and analyze related work from the perspective of designing FL systems.\nHowever, their utility drops by ignoring specific recommendation scenarios'\nunique characteristics and practical challenges. For example, the statistical\nheterogeneity issue in cross-domain FedRec originates from the label drift of\nthe data held by different platforms, which is mainly caused by the recommender\nitself, but not the federated architecture. Therefore, it should focus more on\nsolving specific problems in real-world recommendation scenarios to encourage\nthe deployment FedRec. To this end, this review comprehensively analyzes the\ncoupling of recommender systems and federated learning from the perspective of\nrecommendation researchers and practitioners. We establish a clear link between\nrecommendation scenarios and FL frameworks, systematically analyzing\nscenario-specific approaches, practical challenges, and potential\nopportunities. We aim to develop guidance for the real-world deployment of\nFedRec, bridging the gap between existing research and applications.",
      "pdf_url": "http://arxiv.org/pdf/2508.19620v1",
      "published": "2025-08-27T06:57:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19620v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation",
      "authors": [
        "Yang Sun",
        "Lixin Zou",
        "Dan Luo",
        "Zhiyong Xie",
        "Long Zhang",
        "Liming Dong",
        "Yunwei Zhao",
        "Xixun Lin",
        "Yanxiong Lu",
        "Chenliang Li"
      ],
      "abstract": "Retrieval-augmented generation (RAG) incorporates external knowledge into\nlarge language models (LLMs), improving their adaptability to downstream tasks\nand enabling information updates. Surprisingly, recent empirical evidence\ndemonstrates that injecting noise into retrieved relevant documents\nparadoxically facilitates exploitation of external knowledge and improves\ngeneration quality. Although counterintuitive and challenging to apply in\npractice, this phenomenon enables granular control and rigorous analysis of how\nLLMs integrate external knowledge. Therefore, in this paper, we intervene on\nnoise injection and establish a layer-specific functional demarcation within\nthe LLM: shallow layers specialize in local context modeling, intermediate\nlayers focus on integrating long-range external factual knowledge, and deeper\nlayers primarily rely on parametric internal knowledge. Building on this\ninsight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that\ndirectly combines representations from an intermediate layer with final-layer\ndecoding outputs to fully exploit the external factual knowledge. To identify\nthe optimal intermediate layer, we introduce an internal knowledge score (IKS)\ncriterion that selects the layer with the lowest IKS value in the latter half\nof layers. Experimental results across multiple benchmarks demonstrate that LFD\nhelps RAG systems more effectively surface retrieved context knowledge with\nminimal cost.",
      "pdf_url": "http://arxiv.org/pdf/2508.19614v1",
      "published": "2025-08-27T06:48:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19614v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties",
      "authors": [
        "Huaiyuan Yao",
        "Wanpeng Xu",
        "Justin Turnau",
        "Nadia Kellam",
        "Hua Wei"
      ],
      "abstract": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.19611v1",
      "published": "2025-08-27T06:45:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.19611v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "I.2.7"
      ]
    }
  ]
}