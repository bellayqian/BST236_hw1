{
  "last_updated": "2025-08-13T00:54:26.289836",
  "papers": [
    {
      "title": "Cut2Next: Generating Next Shot via In-Context Tuning",
      "authors": [
        "Jingwen He",
        "Hongbo Liu",
        "Jiajun Li",
        "Ziqi Huang",
        "Yu Qiao",
        "Wanli Ouyang",
        "Ziwei Liu"
      ],
      "abstract": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots.",
      "pdf_url": "http://arxiv.org/pdf/2508.08244v2",
      "published": "2025-08-11T17:56:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08244v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VGGSounder: Audio-Visual Evaluations for Foundation Models",
      "authors": [
        "Daniil Zverev",
        "Thaddäus Wiedemer",
        "Ameya Prabhu",
        "Matthias Bethge",
        "Wieland Brendel",
        "A. Sophia Koepke"
      ],
      "abstract": "The emergence of audio-visual foundation models underscores the importance of\nreliably assessing their multi-modal understanding. The VGGSounder dataset is\ncommonly used as a benchmark for evaluation audio-visual classification.\nHowever, our analysis identifies several limitations of VGGSounder, including\nincomplete labelling, partially overlapping classes, and misaligned modalities.\nThese lead to distorted evaluations of auditory and visual capabilities. To\naddress these limitations, we introduce VGGSounder, a comprehensively\nre-annotated, multi-label test set that extends VGGSound and is specifically\ndesigned to evaluate audio-visual foundation models. VGGSounder features\ndetailed modality annotations, enabling precise analyses of modality-specific\nperformance. Furthermore, we reveal model limitations by analysing performance\ndegradation when adding another input modality with our new modality confusion\nmetric.",
      "pdf_url": "http://arxiv.org/pdf/2508.08237v1",
      "published": "2025-08-11T17:53:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08237v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "title": "LL3M: Large Language 3D Modelers",
      "authors": [
        "Sining Lu",
        "Guan Chen",
        "Nam Anh Dinh",
        "Itai Lang",
        "Ari Holtzman",
        "Rana Hanocka"
      ],
      "abstract": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.",
      "pdf_url": "http://arxiv.org/pdf/2508.08228v1",
      "published": "2025-08-11T17:48:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08228v1",
      "categories": [
        "cs.GR",
        "cs.AI"
      ]
    },
    {
      "title": "OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution",
      "authors": [
        "Zhiqiang Wu",
        "Zhaomang Sun",
        "Tong Zhou",
        "Bingtao Fu",
        "Ji Cong",
        "Yitong Dong",
        "Huaqi Zhang",
        "Xuan Tang",
        "Mingsong Chen",
        "Xian Wei"
      ],
      "abstract": "Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)\ngenerative models show promising potential for one-step Real-World Image\nSuper-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a\nLow-Quality (LQ) image latent distribution at the initial timestep. However, a\nfundamental gap exists between the LQ image latent distribution and the\nGaussian noisy latent distribution, limiting the effective utilization of\ngenerative priors. We observe that the noisy latent distribution at DDPM/FM\nmid-timesteps aligns more closely with the LQ image latent distribution. Based\non this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a\nuniversal framework applicable to DDPM/FM-based generative models. OMGSR\ninjects the LQ image latent distribution at a pre-computed mid-timestep,\nincorporating the proposed Latent Distribution Refinement loss to alleviate the\nlatent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to\neliminate checkerboard artifacts in image generation. Within this framework, we\ninstantiate OMGSR for DDPM/FM-based generative models with two variants:\nOMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate\nthat OMGSR-S/F achieves balanced/excellent performance across quantitative and\nqualitative metrics at 512-resolution. Notably, OMGSR-F establishes\noverwhelming dominance in all reference metrics. We further train a\n1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which\nyields excellent results, especially in the details of the image generation. We\nalso generate 2k-resolution images by the 1k-resolution OMGSR-F using our\ntwo-stage Tiled VAE & Diffusion.",
      "pdf_url": "http://arxiv.org/pdf/2508.08227v1",
      "published": "2025-08-11T17:44:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08227v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning",
      "authors": [
        "Shansong Wang",
        "Mingzhe Hu",
        "Qiang Li",
        "Mojtaba Safari",
        "Xiaofeng Yang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.08224v1",
      "published": "2025-08-11T17:43:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08224v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent",
      "authors": [
        "Tong Yang",
        "Yu Huang",
        "Yingbin Liang",
        "Yuejie Chi"
      ],
      "abstract": "Transformers have demonstrated remarkable capabilities in multi-step\nreasoning tasks. However, understandings of the underlying mechanisms by which\nthey acquire these abilities through training remain limited, particularly from\na theoretical standpoint. This work investigates how transformers learn to\nsolve symbolic multi-step reasoning problems through chain-of-thought\nprocesses, focusing on path-finding in trees. We analyze two intertwined tasks:\na backward reasoning task, where the model outputs a path from a goal node to\nthe root, and a more complex forward reasoning task, where the model implements\ntwo-stage reasoning by first identifying the goal-to-root path and then\nreversing it to produce the root-to-goal path. Our theoretical analysis,\ngrounded in the dynamics of gradient descent, shows that trained one-layer\ntransformers can provably solve both tasks with generalization guarantees to\nunseen trees. In particular, our multi-phase training dynamics for forward\nreasoning elucidate how different attention heads learn to specialize and\ncoordinate autonomously to solve the two subtasks in a single autoregressive\npath. These results provide a mechanistic explanation of how trained\ntransformers can implement sequential algorithmic procedures. Moreover, they\noffer insights into the emergence of reasoning abilities, suggesting that when\ntasks are structured to take intermediate chain-of-thought steps, even shallow\nmulti-head transformers can effectively solve problems that would otherwise\nrequire deeper architectures.",
      "pdf_url": "http://arxiv.org/pdf/2508.08222v1",
      "published": "2025-08-11T17:40:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08222v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling",
      "authors": [
        "Zhuohao Yu",
        "Xingru Jiang",
        "Weizheng Gu",
        "Yidong Wang",
        "Shikun Zhang",
        "Wei Ye"
      ],
      "abstract": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.",
      "pdf_url": "http://arxiv.org/pdf/2508.08211v1",
      "published": "2025-08-11T17:33:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08211v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models",
      "authors": [
        "Kyle Moore",
        "Jesse Roberts",
        "Daryl Watson"
      ],
      "abstract": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis.",
      "pdf_url": "http://arxiv.org/pdf/2508.08204v1",
      "published": "2025-08-11T17:22:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08204v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Street-Level AI: Are Large Language Models Ready for Real-World Judgments?",
      "authors": [
        "Gaurab Pokharel",
        "Shafkat Farabi",
        "Patrick J. Fowler",
        "Sanmay Das"
      ],
      "abstract": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2508.08193v1",
      "published": "2025-08-11T17:12:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08193v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "RedDino: A foundation model for red blood cell analysis",
      "authors": [
        "Luca Zedda",
        "Andrea Loddo",
        "Cecilia Di Ruberto",
        "Carsten Marr"
      ],
      "abstract": "Red blood cells (RBCs) are essential to human health, and their precise\nmorphological analysis is important for diagnosing hematological disorders.\nDespite the promise of foundation models in medical diagnostics, comprehensive\nAI solutions for RBC analysis remain scarce. We present RedDino, a\nself-supervised foundation model designed for RBC image analysis. RedDino uses\nan RBC-specific adaptation of the DINOv2 self-supervised learning framework and\nis trained on a curated dataset of 1.25 million RBC images from diverse\nacquisition modalities and sources. Extensive evaluations show that RedDino\noutperforms existing state-of-the-art models on RBC shape classification.\nThrough assessments including linear probing and nearest neighbor\nclassification, we confirm its strong feature representations and\ngeneralization ability. Our main contributions are: (1) a foundation model\ntailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations\nfor RBC modeling, and (3) a detailed evaluation of generalization performance.\nRedDino addresses key challenges in computational hematology by capturing\nnuanced morphological features, advancing the development of reliable\ndiagnostic tools. The source code and pretrained models for RedDino are\navailable at https://github.com/Snarci/RedDino, and the pretrained models can\nbe downloaded from our Hugging Face collection at\nhttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc",
      "pdf_url": "http://arxiv.org/pdf/2508.08180v1",
      "published": "2025-08-11T16:59:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08180v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision",
      "authors": [
        "Zhonghao Yan",
        "Muxi Diao",
        "Yuxuan Yang",
        "Jiayuan Xu",
        "Kaizhou Zhang",
        "Ruoyan Jing",
        "Lele Yang",
        "Yanxi Liu",
        "Kongming Liang",
        "Zhanyu Ma"
      ],
      "abstract": "Accurately grounding regions of interest (ROIs) is critical for diagnosis and\ntreatment planning in medical imaging. While multimodal large language models\n(MLLMs) combine visual perception with natural language, current\nmedical-grounding pipelines still rely on supervised fine-tuning with explicit\nspatial hints, making them ill-equipped to handle the implicit queries common\nin clinical practice. This work makes three core contributions. We first define\nUnified Medical Reasoning Grounding (UMRG), a novel vision-language task that\ndemands clinical reasoning and pixel-level grounding. Second, we release\nU-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside\nimplicit clinical queries and reasoning traces, spanning 10 modalities, 15\nsuper-categories, and 108 specific categories. Finally, we introduce\nMedReasoner, a modular framework that distinctly separates reasoning from\nsegmentation: an MLLM reasoner is optimized with reinforcement learning, while\na frozen segmentation expert converts spatial prompts into masks, with\nalignment achieved through format and accuracy rewards. MedReasoner achieves\nstate-of-the-art performance on U-MRG-14K and demonstrates strong\ngeneralization to unseen clinical queries, underscoring the significant promise\nof reinforcement learning for interpretable medical grounding.",
      "pdf_url": "http://arxiv.org/pdf/2508.08177v1",
      "published": "2025-08-11T16:59:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08177v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Neural Logic Networks for Interpretable Classification",
      "authors": [
        "Vincent Perreault",
        "Katsumi Inoue",
        "Richard Labib",
        "Alain Hertz"
      ],
      "abstract": "Traditional neural networks have an impressive classification performance,\nbut what they learn cannot be inspected, verified or extracted. Neural Logic\nNetworks on the other hand have an interpretable structure that enables them to\nlearn a logical mechanism relating the inputs and outputs with AND and OR\noperations. We generalize these networks with NOT operations and biases that\ntake into account unobserved data and develop a rigorous logical and\nprobabilistic modeling in terms of concept combinations to motivate their use.\nWe also propose a novel factorized IF-THEN rule structure for the model as well\nas a modified learning algorithm. Our method improves the state-of-the-art in\nBoolean networks discovery and is able to learn relevant, interpretable rules\nin tabular classification, notably on an example from the medical field where\ninterpretability has tangible value.",
      "pdf_url": "http://arxiv.org/pdf/2508.08172v1",
      "published": "2025-08-11T16:49:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08172v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C",
      "authors": [
        "Pedro Orvalho",
        "Marta Kwiatkowska"
      ],
      "abstract": "Python has become the dominant language for general-purpose programming, yet\nit lacks robust tools for formal verification. In contrast, programmers working\nin languages such as C benefit from mature model checkers, for example CBMC,\nwhich enable exhaustive symbolic reasoning and fault localisation. The inherent\ncomplexity of Python, coupled with the verbosity and low-level nature of\nexisting transpilers (e.g., Cython), have historically limited the\napplicability of formal verification to Python programs.\n  In this paper, we propose PyVeritas, a novel framework that leverages Large\nLanguage Models (LLMs) for high-level transpilation from Python to C, followed\nby bounded model checking and MaxSAT-based fault localisation in the generated\nC code. PyVeritas enables verification and bug localisation for Python code\nusing existing model checking tools for C. Our empirical evaluation on two\nPython benchmarks demonstrates that LLM-based transpilation can achieve a high\ndegree of accuracy, up to 80--90% for some LLMs, enabling effective development\nenvironment that supports assertion-based verification and interpretable fault\ndiagnosis for small yet non-trivial Python programs.",
      "pdf_url": "http://arxiv.org/pdf/2508.08171v1",
      "published": "2025-08-11T16:49:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08171v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo",
      "authors": [
        "Mandira Sawkar",
        "Samay U. Shetty",
        "Deepak Pandita",
        "Tharindu Cyril Weerasooriya",
        "Christopher M. Homan"
      ],
      "abstract": "The Learning With Disagreements (LeWiDi) 2025 shared task is to model\nannotator disagreement through soft label distribution prediction and\nperspectivist evaluation, modeling annotators. We adapt DisCo (Distribution\nfrom Context), a neural architecture that jointly models item-level and\nannotator-level label distributions, and present detailed analysis and\nimprovements. In this paper, we extend the DisCo by incorporating annotator\nmetadata, enhancing input representations, and modifying the loss functions to\ncapture disagreement patterns better. Through extensive experiments, we\ndemonstrate substantial improvements in both soft and perspectivist evaluation\nmetrics across three datasets. We also conduct in-depth error and calibration\nanalyses, highlighting the conditions under which improvements occur. Our\nfindings underscore the value of disagreement-aware modeling and offer insights\ninto how system components interact with the complexity of human-annotated\ndata.",
      "pdf_url": "http://arxiv.org/pdf/2508.08163v1",
      "published": "2025-08-11T16:39:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08163v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Can AI Explanations Make You Change Your Mind?",
      "authors": [
        "Laura Spillner",
        "Rachel Ringe",
        "Robert Porzel",
        "Rainer Malaka"
      ],
      "abstract": "In the context of AI-based decision support systems, explanations can help\nusers to judge when to trust the AI's suggestion, and when to question it. In\nthis way, human oversight can prevent AI errors and biased decision-making.\nHowever, this rests on the assumption that users will consider explanations in\nenough detail to be able to catch such errors. We conducted an online study on\ntrust in explainable DSS, and were surprised to find that in many cases,\nparticipants spent little time on the explanation and did not always consider\nit in detail. We present an exploratory analysis of this data, investigating\nwhat factors impact how carefully study participants consider AI explanations,\nand how this in turn impacts whether they are open to changing their mind based\non what the AI suggests.",
      "pdf_url": "http://arxiv.org/pdf/2508.08158v1",
      "published": "2025-08-11T16:36:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08158v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework",
      "authors": [
        "Yunkai Hu",
        "Tianqiao Zhao",
        "Meng Yue"
      ],
      "abstract": "This paper introduces a novel Large Language Models (LLMs)-assisted agent\nthat automatically converts natural-language descriptions of power system\noptimization scenarios into compact, solver-ready formulations and generates\ncorresponding solutions. In contrast to approaches that rely solely on LLM to\nproduce solutions directly, the proposed method focuses on discovering a\nmathematically compatible formulation that can be efficiently solved by\noff-the-shelf optimization solvers. Directly using LLMs to produce solutions\noften leads to infeasible or suboptimal results, as these models lack the\nnumerical precision and constraint-handling capabilities of established\noptimization solvers. The pipeline integrates a domain-aware prompt and schema\nwith an LLM, enforces feasibility through systematic validation and iterative\nrepair, and returns both solver-ready models and user-facing results. Using the\nunit commitment problem as a representative case study, the agent produces\noptimal or near-optimal schedules along with the associated objective costs.\nResults demonstrate that coupling the solver with task-specific validation\nsignificantly enhances solution reliability. This work shows that combining AI\nwith established optimization frameworks bridges high-level problem\ndescriptions and executable mathematical models, enabling more efficient\ndecision-making in energy systems",
      "pdf_url": "http://arxiv.org/pdf/2508.08147v1",
      "published": "2025-08-11T16:22:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08147v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models",
      "authors": [
        "Ganesh Sundaram",
        "Jonas Ulmen",
        "Amjad Haider",
        "Daniel Görges"
      ],
      "abstract": "The rapid growth of resource-constrained mobile platforms, including mobile\nrobots, wearable systems, and Internet-of-Things devices, has increased the\ndemand for computationally efficient neural network controllers (NNCs) that can\noperate within strict hardware limitations. While deep neural networks (DNNs)\ndemonstrate superior performance in control applications, their substantial\ncomputational complexity and memory requirements present significant barriers\nto practical deployment on edge devices. This paper introduces a comprehensive\nmodel compression methodology that leverages component-aware structured pruning\nto determine the optimal pruning magnitude for each pruning group, ensuring a\nbalance between compression and stability for NNC deployment. Our approach is\nrigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),\na state-of-the-art model-based reinforcement learning algorithm, with a\nsystematic integration of mathematical stability guarantee properties,\nspecifically Lyapunov criteria. The key contribution of this work lies in\nproviding a principled framework for determining the theoretical limits of\nmodel compression while preserving controller stability. Experimental\nvalidation demonstrates that our methodology successfully reduces model\ncomplexity while maintaining requisite control performance and stability\ncharacteristics. Furthermore, our approach establishes a quantitative boundary\nfor safe compression ratios, enabling practitioners to systematically determine\nthe maximum permissible model reduction before violating critical stability\nproperties, thereby facilitating the confident deployment of compressed NNCs in\nresource-limited environments.",
      "pdf_url": "http://arxiv.org/pdf/2508.08144v1",
      "published": "2025-08-11T16:16:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08144v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models",
      "authors": [
        "Tianyi Zhou",
        "Johanne Medina",
        "Sanjay Chawla"
      ],
      "abstract": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.",
      "pdf_url": "http://arxiv.org/pdf/2508.08139v1",
      "published": "2025-08-11T16:12:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08139v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation",
      "authors": [
        "Pravallika Abbineni",
        "Saoud Aldowaish",
        "Colin Liechty",
        "Soroosh Noorzad",
        "Ali Ghazizadeh",
        "Morteza Fayazi"
      ],
      "abstract": "Conducting a comprehensive literature review is crucial for advancing circuit\ndesign methodologies. However, the rapid influx of state-of-the-art research,\ninconsistent data representation, and the complexity of optimizing circuit\ndesign objectives make this task significantly challenging. In this paper, we\npropose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for\ncircuit design assistance that integrates a hybrid Retrieval-Augmented\nGeneration (RAG) framework with an adaptive vector database of circuit design\nresearch papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +\nAct (ReAct) workflow for iterative reasoning, goal-setting, and multi-step\ninformation retrieval. It functions as a question-answering design assistant,\ncapable of interpreting complex queries and providing reasoned responses\ngrounded in circuit literature. Its multimodal capabilities enable processing\nof both textual and visual data, facilitating more efficient and comprehensive\nanalysis. The system dynamically adapts using intelligent search tools,\nautomated document retrieval from the internet, and real-time database updates.\nUnlike conventional approaches constrained by model context limits, MuaLLM\ndecouples retrieval from inference, enabling scalable reasoning over\narbitrarily large corpora. At the maximum context length supported by standard\nLLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining\nthe same accuracy. This allows rapid, no-human-in-the-loop database generation,\novercoming the bottleneck of simulation-based dataset creation for circuits. To\nevaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval\nand citation performance, and Reasoning-100 (Reas-100), focused on multistep\nreasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%\naccuracy on Reas-100.",
      "pdf_url": "http://arxiv.org/pdf/2508.08137v1",
      "published": "2025-08-11T16:11:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08137v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models",
      "authors": [
        "Wenze Xu",
        "Chun Wang",
        "Jiazhen Yu",
        "Sheng Chen",
        "Liang Gao",
        "Weihong Deng"
      ],
      "abstract": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets.",
      "pdf_url": "http://arxiv.org/pdf/2508.08131v1",
      "published": "2025-08-11T16:06:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08131v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks",
      "authors": [
        "Rui Miao",
        "Yixin Liu",
        "Yili Wang",
        "Xu Shen",
        "Yue Tan",
        "Yiwei Dai",
        "Shirui Pan",
        "Xin Wang"
      ],
      "abstract": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard.",
      "pdf_url": "http://arxiv.org/pdf/2508.08127v1",
      "published": "2025-08-11T16:04:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08127v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing",
      "authors": [
        "Mingrong Lin",
        "Ke Deng",
        "Zhengyang Wu",
        "Zetao Zheng",
        "Jie Li"
      ],
      "abstract": "Knowledge Tracing (KT) is committed to capturing students' knowledge mastery\nfrom their historical interactions. Simulating students' memory states is a\npromising approach to enhance both the performance and interpretability of\nknowledge tracing models. Memory consists of three fundamental processes:\nencoding, storage, and retrieval. Although forgetting primarily manifests\nduring the storage stage, most existing studies rely on a single,\nundifferentiated forgetting mechanism, overlooking other memory processes as\nwell as personalized forgetting patterns. To address this, this paper proposes\nmemoryKT, a knowledge tracing model based on a novel temporal variational\nautoencoder. The model simulates memory dynamics through a three-stage process:\n(i) Learning the distribution of students' knowledge memory features, (ii)\nReconstructing their exercise feedback, while (iii) Embedding a personalized\nforgetting module within the temporal workflow to dynamically modulate memory\nstorage strength. This jointly models the complete encoding-storage-retrieval\ncycle, significantly enhancing the model's perception capability for individual\ndifferences. Extensive experiments on four public datasets demonstrate that our\nproposed approach significantly outperforms state-of-the-art baselines.",
      "pdf_url": "http://arxiv.org/pdf/2508.08122v1",
      "published": "2025-08-11T15:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08122v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Vision-Based Localization and LLM-based Navigation for Indoor Environments",
      "authors": [
        "Keyan Rahimi",
        "Md. Wasiul Haque",
        "Sagar Dasgupta",
        "Mizanur Rahman"
      ],
      "abstract": "Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions.",
      "pdf_url": "http://arxiv.org/pdf/2508.08120v1",
      "published": "2025-08-11T15:59:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08120v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking",
      "authors": [
        "Xudong Han",
        "Pengcheng Fang",
        "Yueying Tian",
        "Jianhui Yu",
        "Xiaohao Cai",
        "Daniel Roggen",
        "Philip Birch"
      ],
      "abstract": "Multi-object tracking (MOT) in monocular videos is fundamentally challenged\nby occlusions and depth ambiguity, issues that conventional\ntracking-by-detection (TBD) methods struggle to resolve owing to a lack of\ngeometric awareness. To address these limitations, we introduce GRASPTrack, a\nnovel depth-aware MOT framework that integrates monocular depth estimation and\ninstance segmentation into a standard TBD pipeline to generate high-fidelity 3D\npoint clouds from 2D detections, thereby enabling explicit 3D geometric\nreasoning. These 3D point clouds are then voxelized to enable a precise and\nrobust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To\nfurther enhance tracking robustness, our approach incorporates Depth-aware\nAdaptive Noise Compensation, which dynamically adjusts the Kalman filter\nprocess noise based on occlusion severity for more reliable state estimation.\nAdditionally, we propose a Depth-enhanced Observation-Centric Momentum, which\nextends the motion direction consistency from the image plane into 3D space to\nimprove motion-based association cues, particularly for objects with complex\ntrajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack\nbenchmarks demonstrate that our method achieves competitive performance,\nsignificantly improving tracking robustness in complex scenes with frequent\nocclusions and intricate motion patterns.",
      "pdf_url": "http://arxiv.org/pdf/2508.08117v1",
      "published": "2025-08-11T15:56:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08117v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork",
      "authors": [
        "Pranav Pushkar Mishra",
        "Mohammad Arvan",
        "Mohan Zalake"
      ],
      "abstract": "We present TeamMedAgents, a novel multi-agent approach that systematically\nintegrates evidence-based teamwork components from human-human collaboration\ninto medical decision-making with large language models (LLMs). Our approach\nvalidates an organizational psychology teamwork model from human collaboration\nto computational multi-agent medical systems by operationalizing six core\nteamwork components derived from Salas et al.'s \"Big Five\" model: team\nleadership, mutual performance monitoring, team orientation, shared mental\nmodels, closed-loop communication, and mutual trust. We implement and evaluate\nthese components as modular, configurable mechanisms within an adaptive\ncollaboration architecture while assessing the effect of the number of agents\ninvolved based on the task's requirements and domain. Systematic evaluation of\ncomputational implementations of teamwork behaviors across eight medical\nbenchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,\nPath-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8\nevaluated datasets. Controlled ablation studies conducted on 50 questions per\nconfiguration across 3 independent runs provide mechanistic insights into\nindividual component contributions, revealing optimal teamwork configurations\nthat vary by reasoning task complexity and domain-specific requirements. Our\nablation analyses reveal dataset-specific optimal teamwork configurations,\nindicating that different medical reasoning modalities benefit from distinct\ncollaborative patterns. TeamMedAgents represents an advancement in\ncollaborative AI by providing a systematic translation of established teamwork\ntheories from human collaboration into agentic collaboration, establishing a\nfoundation for evidence-based multi-agent system design in critical\ndecision-making domains.",
      "pdf_url": "http://arxiv.org/pdf/2508.08115v1",
      "published": "2025-08-11T15:55:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08115v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Hyperspectral Imaging",
      "authors": [
        "Danfeng Hong",
        "Chenyu Li",
        "Naoto Yokoya",
        "Bing Zhang",
        "Xiuping Jia",
        "Antonio Plaza",
        "Paolo Gamba",
        "Jon Atli Benediktsson",
        "Jocelyn Chanussot"
      ],
      "abstract": "Hyperspectral imaging (HSI) is an advanced sensing modality that\nsimultaneously captures spatial and spectral information, enabling\nnon-invasive, label-free analysis of material, chemical, and biological\nproperties. This Primer presents a comprehensive overview of HSI, from the\nunderlying physical principles and sensor architectures to key steps in data\nacquisition, calibration, and correction. We summarize common data structures\nand highlight classical and modern analysis methods, including dimensionality\nreduction, classification, spectral unmixing, and AI-driven techniques such as\ndeep learning. Representative applications across Earth observation, precision\nagriculture, biomedicine, industrial inspection, cultural heritage, and\nsecurity are also discussed, emphasizing HSI's ability to uncover sub-visual\nfeatures for advanced monitoring, diagnostics, and decision-making. Persistent\nchallenges, such as hardware trade-offs, acquisition variability, and the\ncomplexity of high-dimensional data, are examined alongside emerging solutions,\nincluding computational imaging, physics-informed modeling, cross-modal fusion,\nand self-supervised learning. Best practices for dataset sharing,\nreproducibility, and metadata documentation are further highlighted to support\ntransparency and reuse. Looking ahead, we explore future directions toward\nscalable, real-time, and embedded HSI systems, driven by sensor\nminiaturization, self-supervised learning, and foundation models. As HSI\nevolves into a general-purpose, cross-disciplinary platform, it holds promise\nfor transformative applications in science, technology, and society.",
      "pdf_url": "http://arxiv.org/pdf/2508.08107v1",
      "published": "2025-08-11T15:47:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08107v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience",
      "authors": [
        "Yeana Lee Bond",
        "Mungyeong Choe",
        "Baker Kasim Hasan",
        "Arsh Siddiqui",
        "Myounghoon Jeon"
      ],
      "abstract": "Studies on in-vehicle conversational agents have traditionally relied on\npre-scripted prompts or limited voice commands, constraining natural\ndriver-agent interaction. To resolve this issue, the present study explored the\npotential of a ChatGPT-based in-vehicle agent capable of carrying continuous,\nmulti-turn dialogues. Forty drivers participated in our experiment using a\nmotion-based driving simulator, comparing three conditions (No agent,\nPre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.\nResults showed that the ChatGPT-based agent condition led to more stable\ndriving performance across multiple metrics. Participants demonstrated lower\nvariability in longitudinal acceleration, lateral acceleration, and lane\ndeviation compared to the other two conditions. In subjective evaluations, the\nChatGPT-based agent also received significantly higher ratings in competence,\nanimacy, affective trust, and preference compared to the Pre-scripted agent.\nOur thematic analysis of driver-agent conversations revealed diverse\ninteraction patterns in topics, including driving assistance/questions,\nentertainment requests, and anthropomorphic interactions. Our results highlight\nthe potential of LLM-powered in-vehicle conversational agents to enhance\ndriving safety and user experience through natural, context-rich interactions.",
      "pdf_url": "http://arxiv.org/pdf/2508.08101v1",
      "published": "2025-08-11T15:40:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08101v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Grid2Guide: A* Enabled Small Language Model for Indoor Navigation",
      "authors": [
        "Md. Wasiul Haque",
        "Sagar Dasgupta",
        "Mizanur Rahman"
      ],
      "abstract": "Reliable indoor navigation remains a significant challenge in complex\nenvironments, particularly where external positioning signals and dedicated\ninfrastructures are unavailable. This research presents Grid2Guide, a hybrid\nnavigation framework that combines the A* search algorithm with a Small\nLanguage Model (SLM) to generate clear, human-readable route instructions. The\nframework first conducts a binary occupancy matrix from a given indoor map.\nUsing this matrix, the A* algorithm computes the optimal path between origin\nand destination, producing concise textual navigation steps. These steps are\nthen transformed into natural language instructions by the SLM, enhancing\ninterpretability for end users. Experimental evaluations across various indoor\nscenarios demonstrate the method's effectiveness in producing accurate and\ntimely navigation guidance. The results validate the proposed approach as a\nlightweight, infrastructure-free solution for real-time indoor navigation\nsupport.",
      "pdf_url": "http://arxiv.org/pdf/2508.08100v1",
      "published": "2025-08-11T15:39:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08100v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Dual Information Speech Language Models for Emotional Conversations",
      "authors": [
        "Chun Wang",
        "Chenyang Liu",
        "Wenze Xu",
        "Weihong Deng"
      ],
      "abstract": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.08095v1",
      "published": "2025-08-11T15:33:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08095v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Growing Reservoirs with Developmental Graph Cellular Automata",
      "authors": [
        "Matias Barandiaran",
        "James Stovold"
      ],
      "abstract": "Developmental Graph Cellular Automata (DGCA) are a novel model for\nmorphogenesis, capable of growing directed graphs from single-node seeds. In\nthis paper, we show that DGCAs can be trained to grow reservoirs. Reservoirs\nare grown with two types of targets: task-driven (using the NARMA family of\ntasks) and task-independent (using reservoir metrics).\n  Results show that DGCAs are able to grow into a variety of specialized,\nlife-like structures capable of effectively solving benchmark tasks,\nstatistically outperforming `typical' reservoirs on the same task. Overall,\nthese lay the foundation for the development of DGCA systems that produce\nplastic reservoirs and for modeling functional, adaptive morphogenesis.",
      "pdf_url": "http://arxiv.org/pdf/2508.08091v1",
      "published": "2025-08-11T15:32:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08091v1",
      "categories": [
        "cs.NE",
        "cs.AI"
      ]
    },
    {
      "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches",
      "authors": [
        "Jiejun Tan",
        "Zhicheng Dou",
        "Yan Yu",
        "Jiehan Cheng",
        "Qiang Ju",
        "Jian Xie",
        "Ji-Rong Wen"
      ],
      "abstract": "Recently, large reasoning models have demonstrated strong mathematical and\ncoding abilities, and deep search leverages their reasoning capabilities in\nchallenging information retrieval tasks. Existing deep search works are\ngenerally limited to a single knowledge source, either local or the Web.\nHowever, enterprises often require private deep search systems that can\nleverage search tools over both local and the Web corpus. Simply training an\nagent equipped with multiple search tools using flat reinforcement learning\n(RL) is a straightforward idea, but it has problems such as low training data\nefficiency and poor mastery of complex tools. To address the above issue, we\npropose a hierarchical agentic deep search framework, HierSearch, trained with\nhierarchical RL. At the low level, a local deep search agent and a Web deep\nsearch agent are trained to retrieve evidence from their corresponding domains.\nAt the high level, a planner agent coordinates low-level agents and provides\nthe final answer. Moreover, to prevent direct answer copying and error\npropagation, we design a knowledge refiner that filters out hallucinations and\nirrelevant evidence returned by low-level agents. Experiments show that\nHierSearch achieves better performance compared to flat RL, and outperforms\nvarious deep search and multi-source retrieval-augmented generation baselines\nin six benchmarks across general, finance, and medical domains.",
      "pdf_url": "http://arxiv.org/pdf/2508.08088v1",
      "published": "2025-08-11T15:31:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08088v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence",
      "authors": [
        "Meishen He",
        "Wenjun Ma",
        "Jiao Wang",
        "Huijun Yue",
        "Xiaoma Fan"
      ],
      "abstract": "The Dempster-Shafer theory of evidence has been widely applied in the field\nof information fusion under uncertainty. Most existing research focuses on\ncombining evidence within the same frame of discernment. However, in real-world\nscenarios, trained algorithms or data often originate from different regions or\norganizations, where data silos are prevalent. As a result, using different\ndata sources or models to generate basic probability assignments may lead to\nheterogeneous frames, for which traditional fusion methods often yield\nunsatisfactory results. To address this challenge, this study proposes an\nopen-world information fusion method, termed Full Negation Belief\nTransformation (FNBT), based on the Dempster-Shafer theory. More specially, a\ncriterion is introduced to determine whether a given fusion task belongs to the\nopen-world setting. Then, by extending the frames, the method can accommodate\nelements from heterogeneous frames. Finally, a full negation mechanism is\nemployed to transform the mass functions, so that existing combination rules\ncan be applied to the transformed mass functions for such information fusion.\nTheoretically, the proposed method satisfies three desirable properties, which\nare formally proven: mass function invariance, heritability, and essential\nconflict elimination. Empirically, FNBT demonstrates superior performance in\npattern classification tasks on real-world datasets and successfully resolves\nZadeh's counterexample, thereby validating its practical effectiveness.",
      "pdf_url": "http://arxiv.org/pdf/2508.08075v1",
      "published": "2025-08-11T15:21:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08075v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction",
      "authors": [
        "Yunqing Li",
        "Zixiang Tang",
        "Jiaying Zhuang",
        "Zhenyu Yang",
        "Farhad Ameri",
        "Jianbang Zhang"
      ],
      "abstract": "Connecting an ever-expanding catalogue of products with suitable\nmanufacturers and suppliers is critical for resilient, efficient global supply\nchains, yet traditional methods struggle to capture complex capabilities,\ncertifications, geographic constraints, and rich multimodal data of real-world\nmanufacturer profiles. To address these gaps, we introduce PMGraph, a public\nbenchmark of bipartite and heterogeneous multimodal supply-chain graphs linking\n8,888 manufacturers, over 70k products, more than 110k manufacturer-product\nedges, and over 29k product images. Building on this benchmark, we propose the\nCascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first\naligns and aggregates textual and visual attributes into intermediate group\nembeddings, then propagates them through a manufacturer-product hetero-graph\nvia multiscale message passing to enhance link prediction accuracy. C-MAG also\nprovides practical guidelines for modality-aware fusion, preserving predictive\nperformance in noisy, real-world settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.08071v1",
      "published": "2025-08-11T15:14:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08071v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "J.1; I.2.4; H.2.8"
      ]
    },
    {
      "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model",
      "authors": [
        "Weitai Kang",
        "Weiming Zhuang",
        "Zhizhong Li",
        "Yan Yan",
        "Lingjuan Lyu"
      ],
      "abstract": "Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.",
      "pdf_url": "http://arxiv.org/pdf/2508.08066v1",
      "published": "2025-08-11T15:10:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08066v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning",
      "authors": [
        "Runchuan Zhu",
        "Bowen Jiang",
        "Lingrui Mei",
        "Fangkai Yang",
        "Lu Wang",
        "Haoxiang Gao",
        "Fengshuo Bai",
        "Pu Zhao",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.",
      "pdf_url": "http://arxiv.org/pdf/2508.08053v1",
      "published": "2025-08-11T14:52:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08053v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "On Understanding of the Dynamics of Model Capacity in Continual Learning",
      "authors": [
        "Supriyo Chakraborty",
        "Krishnan Raghavan"
      ],
      "abstract": "The stability-plasticity dilemma, closely related to a neural network's (NN)\ncapacity-its ability to represent tasks-is a fundamental challenge in continual\nlearning (CL). Within this context, we introduce CL's effective model capacity\n(CLEMC) that characterizes the dynamic behavior of the stability-plasticity\nbalance point. We develop a difference equation to model the evolution of the\ninterplay between the NN, task data, and optimization procedure. We then\nleverage CLEMC to demonstrate that the effective capacity-and, by extension,\nthe stability-plasticity balance point is inherently non-stationary. We show\nthat regardless of the NN architecture or optimization method, a NN's ability\nto represent new tasks diminishes when incoming task distributions differ from\nprevious ones. We conduct extensive experiments to support our theoretical\nfindings, spanning a range of architectures-from small feedforward network and\nconvolutional networks to medium-sized graph neural networks and\ntransformer-based large language models with millions of parameters.",
      "pdf_url": "http://arxiv.org/pdf/2508.08052v1",
      "published": "2025-08-11T14:52:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08052v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking Self-Replication: Detecting Distributed Selfhood in the Outlier Cellular Automaton",
      "authors": [
        "Arend Hintze",
        "Clifford Bohm"
      ],
      "abstract": "Spontaneous self-replication in cellular automata has long been considered\nrare, with most known examples requiring careful design or artificial\ninitialization. In this paper, we present formal, causal evidence that such\nreplication can emerge unassisted -- and that it can do so in a distributed,\nmulti-component form. Building on prior work identifying complex dynamics in\nthe Outlier rule, we introduce a data-driven framework that reconstructs the\nfull causal ancestry of patterns in a deterministic cellular automaton. This\nallows us to rigorously identify self-replicating structures via explicit\ncausal lineages. Our results show definitively that self-replicators in the\nOutlier CA are not only spontaneous and robust, but are also often composed of\nmultiple disjoint clusters working in coordination, raising questions about\nsome conventional notions of individuality and replication in artificial life\nsystems.",
      "pdf_url": "http://arxiv.org/pdf/2508.08047v1",
      "published": "2025-08-11T14:49:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08047v1",
      "categories": [
        "nlin.CG",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation",
      "authors": [
        "Van-Khang Nguyen",
        "Duc-Hoang Pham",
        "Huy-Son Nguyen",
        "Cam-Van Thi Nguyen",
        "Hoang-Quynh Le",
        "Duc-Trong Le"
      ],
      "abstract": "Recommendation systems have faced significant challenges in cold-start\nscenarios, where new items with a limited history of interaction need to be\neffectively recommended to users. Though multimodal data (e.g., images, text,\naudio, etc.) offer rich information to address this issue, existing approaches\noften employ simplistic integration methods such as concatenation, average\npooling, or fixed weighting schemes, which fail to capture the complex\nrelationships between modalities. Our study proposes a novel Mixture of Experts\n(MoE) framework for multimodal cold-start recommendation, named MAMEX, which\ndynamically leverages latent representation from different modalities. MAMEX\nutilizes modality-specific expert networks and introduces a learnable gating\nmechanism that adaptively weights the contribution of each modality based on\nits content characteristics. This approach enables MAMEX to emphasize the most\ninformative modalities for each item while maintaining robustness when certain\nmodalities are less relevant or missing. Extensive experiments on benchmark\ndatasets show that MAMEX outperforms state-of-the-art methods in cold-start\nscenarios, with superior accuracy and adaptability. For reproducibility, the\ncode has been made available on Github https://github.com/L2R-UET/MAMEX.",
      "pdf_url": "http://arxiv.org/pdf/2508.08042v1",
      "published": "2025-08-11T14:47:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08042v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models",
      "authors": [
        "Maozhen Zhang",
        "Mengnan Zhao",
        "Bo Wang"
      ],
      "abstract": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments.",
      "pdf_url": "http://arxiv.org/pdf/2508.08040v1",
      "published": "2025-08-11T14:42:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08040v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Strategies for Personalized Radiation Therapy: Part III Identifying genetic determinants for Radiation Response with Meta Learning",
      "authors": [
        "Hao Peng",
        "Yuanyuan Zhang",
        "Steve Jiang",
        "Robert Timmerman",
        "John Minna"
      ],
      "abstract": "Radiation response in cancer is shaped by complex, patient specific biology,\nyet current treatment strategies often rely on uniform dose prescriptions\nwithout accounting for tumor heterogeneity. In this study, we introduce a meta\nlearning framework for one-shot prediction of radiosensitivity measured by SF2\nusing cell line level gene expression data. Unlike the widely used\nRadiosensitivity Index RSI a rank-based linear model trained on a fixed 10-gene\nsignature, our proposed meta-learned model allows the importance of each gene\nto vary by sample through fine tuning. This flexibility addresses key\nlimitations of static models like RSI, which assume uniform gene contributions\nacross tumor types and discard expression magnitude and gene gene interactions.\nOur results show that meta learning offers robust generalization to unseen\nsamples and performs well in tumor subgroups with high radiosensitivity\nvariability, such as adenocarcinoma and large cell carcinoma. By learning\ntransferable structure across tasks while preserving sample specific\nadaptability, our approach enables rapid adaptation to individual samples,\nimproving predictive accuracy across diverse tumor subtypes while uncovering\ncontext dependent patterns of gene influence that may inform personalized\ntherapy.",
      "pdf_url": "http://arxiv.org/pdf/2508.08030v1",
      "published": "2025-08-11T14:34:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08030v1",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches",
      "authors": [
        "Ahmed Aboeitta",
        "Ahmed Sharshar",
        "Youssef Nafea",
        "Shady Shehata"
      ],
      "abstract": "Speech Recognition (ASR) due to phoneme distortions and high variability.\nWhile self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown\npromise, their effectiveness in dysarthric speech remains unclear. This study\nsystematically benchmarks these models with different decoding strategies,\nincluding CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our\ncontributions include (1) benchmarking ASR architectures for dysarthric speech,\n(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing\ngeneralization across datasets, and (4) providing insights into recognition\nerrors across severity levels. Findings highlight that LLM-enhanced decoding\nimproves dysarthric ASR by leveraging linguistic constraints for phoneme\nrestoration and grammatical correction.",
      "pdf_url": "http://arxiv.org/pdf/2508.08027v1",
      "published": "2025-08-11T14:31:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08027v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Advancing Knowledge Tracing by Exploring Follow-up Performance Trends",
      "authors": [
        "Hengyu Liu",
        "Yushuai Li",
        "Minghe Yu",
        "Tiancheng Zhang",
        "Ge Yu",
        "Torben Bach Pedersen",
        "Kristian Torp",
        "Christian S. Jensen",
        "Tianyi Li"
      ],
      "abstract": "Intelligent Tutoring Systems (ITS), such as Massive Open Online Courses,\noffer new opportunities for human learning. At the core of such systems,\nknowledge tracing (KT) predicts students' future performance by analyzing their\nhistorical learning activities, enabling an accurate evaluation of students'\nknowledge states over time. We show that existing KT methods often encounter\ncorrelation conflicts when analyzing the relationships between historical\nlearning sequences and future performance. To address such conflicts, we\npropose to extract so-called Follow-up Performance Trends (FPTs) from\nhistorical ITS data and to incorporate them into KT. We propose a method called\nForward-Looking Knowledge Tracing (FINER) that combines historical learning\nsequences with FPTs to enhance student performance prediction accuracy. FINER\nconstructs learning patterns that facilitate the retrieval of FPTs from\nhistorical ITS data in linear time; FINER includes a novel similarity-aware\nattention mechanism that aggregates FPTs based on both frequency and contextual\nsimilarity; and FINER offers means of combining FPTs and historical learning\nsequences to enable more accurate prediction of student future performance.\nExperiments on six real-world datasets show that FINER can outperform ten\nstate-of-the-art KT methods, increasing accuracy by 8.74% to 84.85%.",
      "pdf_url": "http://arxiv.org/pdf/2508.08019v1",
      "published": "2025-08-11T14:26:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08019v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Fitting Description Logic Ontologies to ABox and Query Examples",
      "authors": [
        "Maurice Funk",
        "Marvin Grosser",
        "Carsten Lutz"
      ],
      "abstract": "We study a fitting problem inspired by ontology-mediated querying: given a\ncollection of positive and negative examples of the form $(\\mathcal{A},q)$ with\n$\\mathcal{A}$ an ABox and $q$ a Boolean query, we seek an ontology\n$\\mathcal{O}$ that satisfies $\\mathcal{A} \\cup \\mathcal{O} \\vDash q$ for all\npositive examples and $\\mathcal{A} \\cup \\mathcal{O}\\not\\vDash q$ for all\nnegative examples. We consider the description logics $\\mathcal{ALC}$ and\n$\\mathcal{ALCI}$ as ontology languages and a range of query languages that\nincludes atomic queries (AQs), conjunctive queries (CQs), and unions thereof\n(UCQs). For all of the resulting fitting problems, we provide effective\ncharacterizations and determine the computational complexity of deciding\nwhether a fitting ontology exists. This problem turns out to be ${\\scriptsize\nCO}NP$ for AQs and full CQs and $2E{\\scriptsize XP}T{\\scriptsize IME}$-complete\nfor CQs and UCQs. These results hold for both $\\mathcal{ALC}$ and\n$\\mathcal{ALCI}$.",
      "pdf_url": "http://arxiv.org/pdf/2508.08007v2",
      "published": "2025-08-11T14:11:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08007v2",
      "categories": [
        "cs.AI",
        "Computing methodologies~Description logics, Computing\n  methodologies~Ontology engineering"
      ]
    },
    {
      "title": "Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP",
      "authors": [
        "Xiang Li",
        "Shanshan Wang",
        "Chenglong Xiao"
      ],
      "abstract": "Extensive experiments and prior studies show that no single maximum clique\nalgorithm consistently performs best across all instances, highlighting the\nimportance of selecting suitable algorithms based on instance features. Through\nan extensive analysis of relevant studies, it is found that there is a lack of\nresearch work concerning algorithm selection oriented toward the Maximum Clique\nProblem (MCP). In this work, we propose a learning-based framework that\nintegrates both traditional machine learning and graph neural networks to\naddress this gap. We construct a labeled dataset by running four exact MCP\nalgorithms on a diverse collection of graph instances, accompanied by\nstructural and global statistical features extracted from each graph. We first\nevaluate four conventional classifiers: Support Vector Machine (SVM), Random\nForest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple\ndataset variants. Experimental results show that RF consistently shows strong\nperformance across metrics and dataset variants, making it a reliable baseline.\nIn addition, feature importance analysis indicates that connectivity and\ntopological structure are strong predictors of algorithm performance. Building\non these findings, we develop a dual-channel model named GAT-MLP, which\ncombines a Graph Attention Network (GAT) for local structural encoding with a\nMultilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model\nshows strong and consistent performance across all metrics. Our results\nhighlight the effectiveness of dual-channel architectures and the promise of\ngraph neural networks in combinatorial algorithm selection.",
      "pdf_url": "http://arxiv.org/pdf/2508.08005v1",
      "published": "2025-08-11T14:09:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08005v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths",
      "authors": [
        "Rui Yao",
        "Qi Chai",
        "Jinhai Yao",
        "Siyuan Li",
        "Junhao Chen",
        "Qi Zhang",
        "Hao Wang"
      ],
      "abstract": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal\nReserve, encodes implicit policy signals and strategic stances. The Federal\nOpen Market Committee strategically employs Fedspeak as a communication tool to\nshape market expectations and influence both domestic and global economic\nconditions. As such, automatically parsing and interpreting Fedspeak presents a\nhigh-impact challenge, with significant implications for financial forecasting,\nalgorithmic trading, and data-driven policy analysis. In this paper, we propose\nan LLM-based, uncertainty-aware framework for deciphering Fedspeak and\nclassifying its underlying monetary policy stance. Technically, to enrich the\nsemantic and contextual representation of Fedspeak texts, we incorporate\ndomain-specific reasoning grounded in the monetary policy transmission\nmechanism. We further introduce a dynamic uncertainty decoding module to assess\nthe confidence of model predictions, thereby enhancing both classification\naccuracy and model reliability. Experimental results demonstrate that our\nframework achieves state-of-the-art performance on the policy stance analysis\ntask. Moreover, statistical analysis reveals a significant positive correlation\nbetween perceptual uncertainty and model error rates, validating the\neffectiveness of perceptual uncertainty as a diagnostic signal.",
      "pdf_url": "http://arxiv.org/pdf/2508.08001v2",
      "published": "2025-08-11T14:04:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08001v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval",
      "authors": [
        "Meixiu Long",
        "Duolin Sun",
        "Dan Yang",
        "Junjie Wang",
        "Yue Shen",
        "Jian Wang",
        "Peng Wei",
        "Jinjie Gu",
        "Jiahai Wang"
      ],
      "abstract": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon.",
      "pdf_url": "http://arxiv.org/pdf/2508.07995v2",
      "published": "2025-08-11T13:57:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.07995v2",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation",
      "authors": [
        "Fangyuan Mao",
        "Aiming Hao",
        "Jintao Chen",
        "Dongxia Liu",
        "Xiaokun Feng",
        "Jiashu Zhu",
        "Meiqi Wu",
        "Chubin Chen",
        "Jiahong Wu",
        "Xiangxiang Chu"
      ],
      "abstract": "Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.",
      "pdf_url": "http://arxiv.org/pdf/2508.07981v2",
      "published": "2025-08-11T13:41:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.07981v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL",
      "authors": [
        "Jiaxuan Gao",
        "Wei Fu",
        "Minyang Xie",
        "Shusheng Xu",
        "Chuyi He",
        "Zhiyu Mei",
        "Banghua Zhu",
        "Yi Wu"
      ],
      "abstract": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
      "pdf_url": "http://arxiv.org/pdf/2508.07976v1",
      "published": "2025-08-11T13:36:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.07976v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer",
      "authors": [
        "Junyu Wu",
        "Weiming Chang",
        "Xiaotao Liu",
        "Guanyou He",
        "Tingfeng Xian",
        "Haoqiang Hong",
        "Boqi Chen",
        "Haotao Tian",
        "Tao Yang",
        "Yunsheng Shi",
        "Feng Lin",
        "Ting Yao"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent\nparadigm for training large language models and multimodal systems. Despite\nnotable advances enabled by existing RLHF training frameworks, significant\nchallenges remain in scaling to complex multimodal workflows and adapting to\ndynamic workloads. In particular, current systems often encounter limitations\nrelated to controller scalability when managing large models, as well as\ninefficiencies in orchestrating intricate RLHF pipelines, especially in\nscenarios that require dynamic sampling and resource allocation. In this paper,\nwe introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,\nscalable, and balanced RLHF training framework specifically designed to address\nthese challenges. WeChat-YATT features a parallel controller programming model\nthat enables flexible and efficient orchestration of complex RLHF workflows,\neffectively mitigating the bottlenecks associated with centralized controller\narchitectures and facilitating scalability in large-scale data scenarios. In\naddition, we propose a dynamic placement schema that adaptively partitions\ncomputational resources and schedules workloads, thereby significantly reducing\nhardware idle time and improving GPU utilization under variable training\nconditions. We evaluate WeChat-YATT across a range of experimental scenarios,\ndemonstrating that it achieves substantial improvements in throughput compared\nto state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been\nsuccessfully deployed to train models supporting WeChat product features for a\nlarge-scale user base, underscoring its effectiveness and robustness in\nreal-world applications.",
      "pdf_url": "http://arxiv.org/pdf/2508.07970v1",
      "published": "2025-08-11T13:31:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.07970v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring the Challenges and Opportunities of AI-assisted Codebase Generation",
      "authors": [
        "Philipp Eibl",
        "Sadra Sabouri",
        "Souti Chattopadhyay"
      ],
      "abstract": "Recent AI code assistants have significantly improved their ability to\nprocess more complex contexts and generate entire codebases based on a textual\ndescription, compared to the popular snippet-level generation. These codebase\nAI assistants (CBAs) can also extend or adapt codebases, allowing users to\nfocus on higher-level design and deployment decisions. While prior work has\nextensively studied the impact of snippet-level code generation, this new class\nof codebase generation models is relatively unexplored. Despite initial\nanecdotal reports of excitement about these agents, they remain less frequently\nadopted compared to snippet-level code assistants. To utilize CBAs better, we\nneed to understand how developers interact with CBAs, and how and why CBAs fall\nshort of developers' needs. In this paper, we explored these gaps through a\ncounterbalanced user study and interview with (n = 16) students and developers\nworking on coding tasks with CBAs. We found that participants varied the\ninformation in their prompts, like problem description (48% of prompts),\nrequired functionality (98% of prompts), code structure (48% of prompts), and\ntheir prompt writing process. Despite various strategies, the overall\nsatisfaction score with generated codebases remained low (mean = 2.8, median =\n3, on a scale of one to five). Participants mentioned functionality as the most\ncommon factor for dissatisfaction (77% of instances), alongside poor code\nquality (42% of instances) and communication issues (25% of instances). We\ndelve deeper into participants' dissatisfaction to identify six underlying\nchallenges that participants faced when using CBAs, and extracted five barriers\nto incorporating CBAs into their workflows. Finally, we surveyed 21 commercial\nCBAs to compare their capabilities with participant challenges and present\ndesign opportunities for more efficient and useful CBAs.",
      "pdf_url": "http://arxiv.org/pdf/2508.07966v1",
      "published": "2025-08-11T13:26:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.07966v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    }
  ]
}