{
  "last_updated": "2025-09-01T00:59:29.532128",
  "papers": [
    {
      "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation",
      "authors": [
        "Ruixuan Liu",
        "Philip Huang",
        "Ava Pun",
        "Kangle Deng",
        "Shobhit Aggarwal",
        "Kevin Tang",
        "Michelle Liu",
        "Deva Ramanan",
        "Jun-Yan Zhu",
        "Jiaoyang Li",
        "Changliu Liu"
      ],
      "abstract": "Creating assembly products demands significant manual effort and expert\nknowledge in 1) designing the assembly and 2) constructing the product. This\npaper introduces Prompt-to-Product, an automated pipeline that generates\nreal-world assembly products from natural language prompts. Specifically, we\nleverage LEGO bricks as the assembly platform and automate the process of\ncreating brick assembly structures. Given the user design requirements,\nPrompt-to-Product generates physically buildable brick designs, and then\nleverages a bimanual robotic system to construct the real assembly products,\nbringing user imaginations into the real world. We conduct a comprehensive user\nstudy, and the results demonstrate that Prompt-to-Product significantly lowers\nthe barrier and reduces manual effort in creating assembly products from\nimaginative ideas.",
      "pdf_url": "http://arxiv.org/pdf/2508.21063v1",
      "published": "2025-08-28T17:59:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21063v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models",
      "authors": [
        "Adam Coscia",
        "Shunan Guo",
        "Eunyee Koh",
        "Alex Endert"
      ],
      "abstract": "As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance.",
      "pdf_url": "http://arxiv.org/pdf/2508.21061v1",
      "published": "2025-08-28T17:58:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21061v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Mixture of Contexts for Long Video Generation",
      "authors": [
        "Shengqu Cai",
        "Ceyuan Yang",
        "Lvmin Zhang",
        "Yuwei Guo",
        "Junfei Xiao",
        "Ziyan Yang",
        "Yinghao Xu",
        "Zhenheng Yang",
        "Alan Yuille",
        "Leonidas Guibas",
        "Maneesh Agrawala",
        "Lu Jiang",
        "Gordon Wetzstein"
      ],
      "abstract": "Long video generation is fundamentally a long context memory problem: models\nmust retain and retrieve salient events across a long range without collapsing\nor drifting. However, scaling diffusion transformers to generate long-context\nvideos is fundamentally limited by the quadratic cost of self-attention, which\nmakes memory and computation intractable and difficult to optimize for long\nsequences. We recast long-context video generation as an internal information\nretrieval task and propose a simple, learnable sparse attention routing module,\nMixture of Contexts (MoC), as an effective long-term memory retrieval engine.\nIn MoC, each query dynamically selects a few informative chunks plus mandatory\nanchors (caption, local windows) to attend to, with causal routing that\nprevents loop closures. As we scale the data and gradually sparsify the\nrouting, the model allocates compute to salient history, preserving identities,\nactions, and scenes over minutes of content. Efficiency follows as a byproduct\nof retrieval (near-linear scaling), which enables practical training and\nsynthesis, and the emergence of memory and consistency at the scale of minutes.",
      "pdf_url": "http://arxiv.org/pdf/2508.21058v1",
      "published": "2025-08-28T17:57:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21058v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "FakeParts: a New Family of AI-Generated DeepFakes",
      "authors": [
        "Gaetan Brison",
        "Soobash Daiboo",
        "Samy Aimeur",
        "Awais Hussain Sani",
        "Xi Wang",
        "Gianni Franchi",
        "Vicky Kalogeiton"
      ],
      "abstract": "We introduce FakeParts, a new class of deepfakes characterized by subtle,\nlocalized manipulations to specific spatial regions or temporal segments of\notherwise authentic videos. Unlike fully synthetic content, these partial\nmanipulations, ranging from altered facial expressions to object substitutions\nand background modifications, blend seamlessly with real elements, making them\nparticularly deceptive and difficult to detect. To address the critical gap in\ndetection capabilities, we present FakePartsBench, the first large-scale\nbenchmark dataset specifically designed to capture the full spectrum of partial\ndeepfakes. Comprising over 25K videos with pixel-level and frame-level\nmanipulation annotations, our dataset enables comprehensive evaluation of\ndetection methods. Our user studies demonstrate that FakeParts reduces human\ndetection accuracy by over 30% compared to traditional deepfakes, with similar\nperformance degradation observed in state-of-the-art detection models. This\nwork identifies an urgent vulnerability in current deepfake detection\napproaches and provides the necessary resources to develop more robust methods\nfor partial video manipulations.",
      "pdf_url": "http://arxiv.org/pdf/2508.21052v1",
      "published": "2025-08-28T17:55:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21052v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Enabling Equitable Access to Trustworthy Financial Reasoning",
      "authors": [
        "William Jurayj",
        "Nils Holzenberger",
        "Benjamin Van Durme"
      ],
      "abstract": "According to the United States Internal Revenue Service, ''the average\nAmerican spends $\\$270$ and 13 hours filing their taxes''. Even beyond the\nU.S., tax filing requires complex reasoning, combining application of\noverlapping rules with numerical calculations. Because errors can incur costly\npenalties, any automated system must deliver high accuracy and auditability,\nmaking modern large language models (LLMs) poorly suited for this task. We\npropose an approach that integrates LLMs with a symbolic solver to calculate\ntax obligations. We evaluate variants of this system on the challenging\nStAtutory Reasoning Assessment (SARA) dataset, and include a novel method for\nestimating the cost of deploying such a system based on real-world penalties\nfor tax errors. We further show how combining up-front translation of\nplain-text rules into formal logic programs, combined with intelligently\nretrieved exemplars for formal case representations, can dramatically improve\nperformance on this task and reduce costs to well below real-world averages.\nOur results demonstrate the promise and economic feasibility of neuro-symbolic\narchitectures for increasing equitable access to reliable tax assistance.",
      "pdf_url": "http://arxiv.org/pdf/2508.21051v1",
      "published": "2025-08-28T17:55:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21051v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning",
      "authors": [
        "Hao Tan",
        "Jun Lan",
        "Zichang Tan",
        "Ajian Liu",
        "Chuanbiao Song",
        "Senyuan Shi",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Jun Wan",
        "Zhen Lei"
      ],
      "abstract": "Deepfake detection remains a formidable challenge due to the complex and\nevolving nature of fake content in real-world scenarios. However, existing\nacademic benchmarks suffer from severe discrepancies from industrial practice,\ntypically featuring homogeneous training sources and low-quality testing\nimages, which hinder the practical deployments of current detectors. To\nmitigate this gap, we introduce HydraFake, a dataset that simulates real-world\nchallenges with hierarchical generalization testing. Specifically, HydraFake\ninvolves diversified deepfake techniques and in-the-wild forgeries, along with\nrigorous training and evaluation protocol, covering unseen model architectures,\nemerging forgery techniques and novel data domains. Building on this resource,\nwe propose Veritas, a multi-modal large language model (MLLM) based deepfake\ndetector. Different from vanilla chain-of-thought (CoT), we introduce\npattern-aware reasoning that involves critical reasoning patterns such as\n\"planning\" and \"self-reflection\" to emulate human forensic process. We further\npropose a two-stage training pipeline to seamlessly internalize such deepfake\nreasoning capacities into current MLLMs. Experiments on HydraFake dataset\nreveal that although previous detectors show great generalization on\ncross-model scenarios, they fall short on unseen forgeries and data domains.\nOur Veritas achieves significant gains across different OOD scenarios, and is\ncapable of delivering transparent and faithful detection outputs.",
      "pdf_url": "http://arxiv.org/pdf/2508.21048v1",
      "published": "2025-08-28T17:53:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21048v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop",
      "authors": [
        "Lev Tankelevitch",
        "Elena L. Glassman",
        "Jessica He",
        "Aniket Kittur",
        "Mina Lee",
        "Srishti Palani",
        "Advait Sarkar",
        "Gonzalo Ramos",
        "Yvonne Rogers",
        "Hari Subramonyam"
      ],
      "abstract": "Generative AI (GenAI) radically expands the scope and capability of\nautomation for work, education, and everyday tasks, a transformation posing\nboth risks and opportunities for human cognition. How will human cognition\nchange, and what opportunities are there for GenAI to augment it? Which\ntheories, metrics, and other tools are needed to address these questions? The\nCHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of\nhow the use of GenAI affects human thought, from metacognition to critical\nthinking, memory, and creativity, with an emerging design practice for building\nGenAI tools that both protect and augment human thought. Fifty-six researchers,\ndesigners, and thinkers from across disciplines as well as industry and\nacademia, along with 34 papers and portfolios, seeded a day of discussion,\nideation, and community-building. We synthesize this material here to begin\nmapping the space of research and design opportunities and to catalyze a\nmultidisciplinary community around this pressing area of research.",
      "pdf_url": "http://arxiv.org/pdf/2508.21036v1",
      "published": "2025-08-28T17:40:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21036v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance",
      "authors": [
        "Luozhijie Jin",
        "Zijie Qiu",
        "Jie Liu",
        "Zijie Diao",
        "Lifeng Qiao",
        "Ning Ding",
        "Alex Lamb",
        "Xipeng Qiu"
      ],
      "abstract": "Denoising-based generative models, particularly diffusion and flow matching\nalgorithms, have achieved remarkable success. However, aligning their output\ndistributions with complex downstream objectives, such as human preferences,\ncompositional accuracy, or data compressibility, remains challenging. While\nreinforcement learning (RL) fine-tuning methods, inspired by advances in RL\nfrom human feedback (RLHF) for large language models, have been adapted to\nthese generative frameworks, current RL approaches are suboptimal for diffusion\nmodels and offer limited flexibility in controlling alignment strength after\nfine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models\nthrough the lens of stochastic differential equations and implicit reward\nconditioning. We introduce Reinforcement Learning Guidance (RLG), an\ninference-time method that adapts Classifier-Free Guidance (CFG) by combining\nthe outputs of the base and RL fine-tuned models via a geometric average. Our\ntheoretical analysis shows that RLG's guidance scale is mathematically\nequivalent to adjusting the KL-regularization coefficient in standard RL\nobjectives, enabling dynamic control over the alignment-quality trade-off\nwithout further training. Extensive experiments demonstrate that RLG\nconsistently improves the performance of RL fine-tuned models across various\narchitectures, RL algorithms, and downstream tasks, including human\npreferences, compositional control, compressibility, and text rendering.\nFurthermore, RLG supports both interpolation and extrapolation, thereby\noffering unprecedented flexibility in controlling generative alignment. Our\napproach provides a practical and theoretically sound solution for enhancing\nand controlling diffusion model alignment at inference. The source code for RLG\nis publicly available at the Github:\nhttps://github.com/jinluo12345/Reinforcement-learning-guidance.",
      "pdf_url": "http://arxiv.org/pdf/2508.21016v1",
      "published": "2025-08-28T17:18:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21016v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
      "authors": [
        "Paritosh Parmar",
        "Eric Peh",
        "Basura Fernando"
      ],
      "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle\nwith higher-order reasoning, relying on opaque, monolithic pipelines that\nentangle video understanding, causal inference, and answer generation. These\nblack-box approaches offer limited interpretability and tend to depend on\nshallow heuristics. We propose a novel, modular framework that explicitly\ndecouples causal reasoning from answer generation, introducing natural language\ncausal chains as interpretable intermediate representations. Inspired by human\ncognitive models, these structured cause-effect sequences bridge low-level\nvideo content with high-level causal reasoning, enabling transparent and\nlogically coherent inference. Our two-stage architecture comprises a Causal\nChain Extractor (CCE) that generates causal chains from video-question pairs,\nand a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in\nthese chains. To address the lack of annotated reasoning traces, we introduce a\nscalable method for generating high-quality causal chains from existing\ndatasets using large language models. We also propose CauCo, a new evaluation\nmetric for causality-oriented captioning. Experiments on three large-scale\nbenchmarks demonstrate that our approach not only outperforms state-of-the-art\nmodels, but also yields substantial gains in explainability, user trust, and\ngeneralization -- positioning the CCE as a reusable causal reasoning engine\nacross diverse domains. Project page:\nhttps://paritoshparmar.github.io/chainreaction/",
      "pdf_url": "http://arxiv.org/pdf/2508.21010v1",
      "published": "2025-08-28T17:10:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21010v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees",
      "authors": [
        "Yaniv Hassidof",
        "Tom Jurgenson",
        "Kiril Solovey"
      ],
      "abstract": "Kinodynamic motion planning is concerned with computing collision-free\ntrajectories while abiding by the robot's dynamic constraints. This critical\nproblem is often tackled using sampling-based planners (SBPs) that explore the\nrobot's high-dimensional state space by constructing a search tree via action\npropagations. Although SBPs can offer global guarantees on completeness and\nsolution quality, their performance is often hindered by slow exploration due\nto uninformed action sampling. Learning-based approaches can yield\nsignificantly faster runtimes, yet they fail to generalize to\nout-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,\nthus limiting their deployment on physical robots. We present Diffusion Tree\n(DiTree): a \\emph{provably-generalizable} framework leveraging diffusion\npolicies (DPs) as informed samplers to efficiently guide state-space search\nwithin SBPs. DiTree combines DP's ability to model complex distributions of\nexpert trajectories, conditioned on local observations, with the completeness\nof SBPs to yield \\emph{provably-safe} solutions within a few action propagation\niterations for complex dynamical systems. We demonstrate DiTree's power with an\nimplementation combining the popular RRT planner with a DP action sampler\ntrained on a \\emph{single environment}. In comprehensive evaluations on OOD\nscenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than\nclassical SBPs), while improving the average success rate over DP and SBPs.\nDiTree is on average 3x faster than classical SBPs, and outperforms all other\napproaches by achieving roughly 30\\% higher success rate. Project webpage:\nhttps://sites.google.com/view/ditree.",
      "pdf_url": "http://arxiv.org/pdf/2508.21001v1",
      "published": "2025-08-28T17:04:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21001v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery",
      "authors": [
        "Junda Wang",
        "Zonghai Yao",
        "Zhichao Yang",
        "Lingxi Li",
        "Junhui Qian",
        "Hong Yu"
      ],
      "abstract": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation.",
      "pdf_url": "http://arxiv.org/pdf/2508.20996v1",
      "published": "2025-08-28T16:57:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20996v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts",
      "authors": [
        "Patryk Będkowski",
        "Jan Dubiński",
        "Filip Szatkowski",
        "Kamil Deja",
        "Przemysław Rokita",
        "Tomasz Trzciński"
      ],
      "abstract": "Simulating detector responses is a crucial part of understanding the inner\nworkings of particle collisions in the Large Hadron Collider at CERN. Such\nsimulations are currently performed with statistical Monte Carlo methods, which\nare computationally expensive and put a significant strain on CERN's\ncomputational grid. Therefore, recent proposals advocate for generative machine\nlearning methods to enable more efficient simulations. However, the\ndistribution of the data varies significantly across the simulations, which is\nhard to capture with out-of-the-box methods. In this study, we present\nExpertSim - a deep learning simulation approach tailored for the Zero Degree\nCalorimeter in the ALICE experiment. Our method utilizes a\nMixture-of-Generative-Experts architecture, where each expert specializes in\nsimulating a different subset of the data. This allows for a more precise and\nefficient generation process, as each expert focuses on a specific aspect of\nthe calorimeter response. ExpertSim not only improves accuracy, but also\nprovides a significant speedup compared to the traditional Monte-Carlo methods,\noffering a promising solution for high-efficiency detector simulations in\nparticle physics experiments at CERN. We make the code available at\nhttps://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.",
      "pdf_url": "http://arxiv.org/pdf/2508.20991v1",
      "published": "2025-08-28T16:53:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20991v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Efficient Neuro-Symbolic Learning of Constraints and Objective",
      "authors": [
        "Marianne Defresne",
        "Romain Gambardella",
        "Sophie Barbe",
        "Thomas Schiex"
      ],
      "abstract": "In the ongoing quest for hybridizing discrete reasoning with neural nets,\nthere is an increasing interest in neural architectures that can learn how to\nsolve discrete reasoning or optimization problems from natural inputs, a task\nthat Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a\nloss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints\nand the objective, thus delivering a complete model that can be scrutinized and\ncompleted with side constraints. By pushing the combinatorial solver out of the\ntraining loop, our architecture also offers scalable training while exact\ninference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve\nNP-hard reasoning problems from natural inputs. On three variants of the Sudoku\nbenchmark -- symbolic, visual, and many-solution --, our approach requires a\nfraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut\ntask, it optimizes the regret better than a Decision-Focused-Learning\nregret-dedicated loss. Finally, it efficiently learns the energy optimization\nformulation of the large real-world problem of designing proteins.",
      "pdf_url": "http://arxiv.org/pdf/2508.20978v1",
      "published": "2025-08-28T16:33:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20978v1",
      "categories": [
        "cs.AI",
        "cs.LO",
        "cs.SC"
      ]
    },
    {
      "title": "WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations",
      "authors": [
        "Jaeyeon Kim",
        "Heeseung Yun",
        "Sang Hoon Woo",
        "Chao-Han Huck Yang",
        "Gunhee Kim"
      ],
      "abstract": "Large audio language models (LALMs) extend language understanding into the\nauditory domain, yet their ability to perform low-level listening, such as\npitch and duration detection, remains underexplored. However, low-level\nlistening is critical for real-world, out-of-distribution tasks where models\nmust reason about unfamiliar sounds based on fine-grained acoustic cues. To\naddress this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to\nevaluate low-level auditory perception and cognition using marine mammal\nvocalizations. WoW-bench is composed of a Perception benchmark for categorizing\nnovel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess\nthe abilities to remember, understand, apply, and analyze sound events. For the\nCognition benchmark, we additionally introduce distractor questions to evaluate\nwhether models are truly solving problems through listening rather than relying\non other heuristics. Experiments with state-of-the-art LALMs show performance\nfar below human levels, indicating a need for stronger auditory grounding in\nLALMs.",
      "pdf_url": "http://arxiv.org/pdf/2508.20976v1",
      "published": "2025-08-28T16:29:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20976v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents",
      "authors": [
        "Tianjian Liu",
        "Fanqi Wan",
        "Jiajian Guo",
        "Xiaojun Quan"
      ],
      "abstract": "Proactive dialogue has emerged as a critical and challenging research problem\nin advancing large language models (LLMs). Existing works predominantly focus\non domain-specific or task-oriented scenarios, which leads to fragmented\nevaluations and limits the comprehensive exploration of models' proactive\nconversation abilities. In this work, we propose ProactiveEval, a unified\nframework designed for evaluating proactive dialogue capabilities of LLMs. This\nframework decomposes proactive dialogue into target planning and dialogue\nguidance, establishing evaluation metrics across various domains. Moreover, it\nalso enables the automatic generation of diverse and challenging evaluation\ndata. Based on the proposed framework, we develop 328 evaluation environments\nspanning 6 distinct domains. Through experiments with 22 different types of\nLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional\nperformance on target planning and dialogue guidance tasks, respectively.\nFinally, we investigate how reasoning capabilities influence proactive\nbehaviors and discuss their implications for future model development.",
      "pdf_url": "http://arxiv.org/pdf/2508.20973v1",
      "published": "2025-08-28T16:26:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20973v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling",
      "authors": [
        "Vipul Patel",
        "Anirudh Deodhar",
        "Dagnachew Birru"
      ],
      "abstract": "Workforce scheduling in the healthcare sector is a significant operational\nchallenge, characterized by fluctuating patient loads, diverse clinical skills,\nand the critical need to control labor costs while upholding high standards of\npatient care. This problem is inherently multi-objective, demanding a delicate\nbalance between competing goals: minimizing payroll, ensuring adequate staffing\nfor patient needs, and accommodating staff preferences to mitigate burnout. We\npropose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital\nunit workforce scheduling problem as a multi-objective optimization task. Our\nmodel incorporates real-world complexities, including hourly appointment-driven\ndemand and the use of modular shifts for a multi-skilled workforce. By defining\nobjective functions for cost, patient care coverage, and staff satisfaction,\nthe GA navigates the vast search space to identify a set of high-quality,\nnon-dominated solutions. Demonstrated on datasets representing a typical\nhospital unit, the results show that our MOO-GA generates robust and balanced\nschedules. On average, the schedules produced by our algorithm showed a 66\\%\nperformance improvement over a baseline that simulates a conventional, manual\nscheduling process. This approach effectively manages trade-offs between\ncritical operational and staff-centric objectives, providing a practical\ndecision support tool for nurse managers and hospital administrators.",
      "pdf_url": "http://arxiv.org/pdf/2508.20953v1",
      "published": "2025-08-28T16:16:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20953v1",
      "categories": [
        "cs.AI",
        "cs.DM"
      ]
    },
    {
      "title": "Research Challenges in Relational Database Management Systems for LLM Queries",
      "authors": [
        "Kerem Akillioglu",
        "Anurag Chakraborty",
        "Sairaj Voruganti",
        "M. Tamer Özsu"
      ],
      "abstract": "Large language models (LLMs) have become essential for applications such as\ntext summarization, sentiment analysis, and automated question-answering.\nRecently, LLMs have also been integrated into relational database management\nsystems to enhance querying and support advanced data processing. Companies\nsuch as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly\nwithin SQL, denoted as LLM queries, to boost data insights. However,\nopen-source solutions currently have limited functionality and poor\nperformance. In this work, we present an early exploration of two open-source\nsystems and one enterprise platform, using five representative queries to\nexpose functional, performance, and scalability limits in today's SQL-invoked\nLLM integrations. We identify three main issues: enforcing structured outputs,\noptimizing resource utilization, and improving query planning. We implemented\ninitial solutions and observed improvements in accommodating LLM powered SQL\nqueries. These early gains demonstrate that tighter integration of LLM+DBMS is\nthe key to scalable and efficient processing of LLM queries.",
      "pdf_url": "http://arxiv.org/pdf/2508.20912v1",
      "published": "2025-08-28T15:41:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20912v1",
      "categories": [
        "cs.DB",
        "cs.AI"
      ]
    },
    {
      "title": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant",
      "authors": [
        "Nicolas Dupuis",
        "Adarsh Tiwari",
        "Youssef Mroueh",
        "David Kremer",
        "Ismael Faro",
        "Juan Cruz-Benito"
      ],
      "abstract": "Qiskit is an open-source quantum computing framework that allows users to\ndesign, simulate, and run quantum circuits on real quantum hardware. We explore\npost-training techniques for LLMs to assist in writing Qiskit code. We\nintroduce quantum verification as an effective method for ensuring code quality\nand executability on quantum hardware. To support this, we developed a\nsynthetic data pipeline that generates quantum problem-unit test pairs and used\nit to create preference data for aligning LLMs with DPO. Additionally, we\ntrained models using GRPO, leveraging quantum-verifiable rewards provided by\nthe quantum hardware. Our best-performing model, combining DPO and GRPO,\nsurpasses the strongest open-source baselines on the challenging\nQiskit-HumanEval-hard benchmark.",
      "pdf_url": "http://arxiv.org/pdf/2508.20907v1",
      "published": "2025-08-28T15:37:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20907v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning",
      "authors": [
        "Amine Lbath",
        "Massih-Reza Amini",
        "Aurelien Delaitre",
        "Vadim Okun"
      ],
      "abstract": "The increasing complexity of software systems and the sophistication of\ncyber-attacks have underscored the critical need for effective automated\nvulnerability detection and repair systems. Traditional methods, such as static\nprogram analysis, face significant challenges related to scalability,\nadaptability, and high false-positive and false-negative rates. AI-driven\napproaches, particularly those using machine learning and deep learning models,\nshow promise but are heavily reliant on the quality and quantity of training\ndata. This paper introduces a novel framework designed to automatically\nintroduce realistic, category-specific vulnerabilities into secure C/C++\ncodebases to generate datasets. The proposed approach coordinates multiple AI\nagents that simulate expert reasoning, along with function agents and\ntraditional code analysis tools. It leverages Retrieval-Augmented Generation\nfor contextual grounding and employs Low-Rank approximation of weights for\nefficient model fine-tuning. Our experimental study on 116 code samples from\nthree different benchmarks suggests that our approach outperforms other\ntechniques with regard to dataset accuracy, achieving between 89\\% and 95\\%\nsuccess rates in injecting vulnerabilities at function level.",
      "pdf_url": "http://arxiv.org/pdf/2508.20866v1",
      "published": "2025-08-28T14:59:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20866v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring",
      "authors": [
        "Junjie Chu",
        "Mingjie Li",
        "Ziqing Yang",
        "Ye Leng",
        "Chenhao Lin",
        "Chao Shen",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "abstract": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks.",
      "pdf_url": "http://arxiv.org/pdf/2508.20848v1",
      "published": "2025-08-28T14:40:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20848v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Learning Primitive Embodied World Models: Towards Scalable Robotic Learning",
      "authors": [
        "Qiao Sun",
        "Liujia Yang",
        "Wei Tang",
        "Wei Huang",
        "Kaixin Xu",
        "Yongchao Chen",
        "Mingyu Liu",
        "Jiange Yang",
        "Haoyi Zhu",
        "Yating Wang",
        "Tong He",
        "Yilun Chen",
        "Xili Dai",
        "Nanyang Ye",
        "Qinying Gu"
      ],
      "abstract": "While video-generation-based embodied world models have gained increasing\nattention, their reliance on large-scale embodied interaction data remains a\nkey bottleneck. The scarcity, difficulty of collection, and high dimensionality\nof embodied data fundamentally limit the alignment granularity between language\nand actions and exacerbate the challenge of long-horizon video\ngeneration--hindering generative models from achieving a \"GPT moment\" in the\nembodied domain. There is a naive observation: the diversity of embodied data\nfar exceeds the relatively small space of possible primitive motions. Based on\nthis insight, we propose a novel paradigm for world modeling--Primitive\nEmbodied World Models (PEWM). By restricting video generation to fixed short\nhorizons, our approach 1) enables fine-grained alignment between linguistic\nconcepts and visual representations of robotic actions, 2) reduces learning\ncomplexity, 3) improves data efficiency in embodied data collection, and 4)\ndecreases inference latency. By equipping with a modular Vision-Language Model\n(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further\nenables flexible closed-loop control and supports compositional generalization\nof primitive-level policies over extended, complex tasks. Our framework\nleverages the spatiotemporal vision priors in video models and the semantic\nawareness of VLMs to bridge the gap between fine-grained physical interaction\nand high-level reasoning, paving the way toward scalable, interpretable, and\ngeneral-purpose embodied intelligence.",
      "pdf_url": "http://arxiv.org/pdf/2508.20840v1",
      "published": "2025-08-28T14:31:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20840v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Multi-Agent Penetration Testing AI for the Web",
      "authors": [
        "Isaac David",
        "Arthur Gervais"
      ],
      "abstract": "AI-powered development platforms are making software creation accessible to a\nbroader audience, but this democratization has triggered a scalability crisis\nin security auditing. With studies showing that up to 40% of AI-generated code\ncontains vulnerabilities, the pace of development now vastly outstrips the\ncapacity for thorough security assessment.\n  We present MAPTA, a multi-agent system for autonomous web application\nsecurity assessment that combines large language model orchestration with\ntool-grounded execution and end-to-end exploit validation. On the 104-challenge\nXBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance\non SSRF and misconfiguration vulnerabilities, 83% success on broken\nauthorization, and strong results on injection attacks including server-side\ntemplate injection (85%) and SQL injection (83%). Cross-site scripting (57%)\nand blind SQL injection (0%) remain challenging. Our comprehensive cost\nanalysis across all challenges totals $21.38 with a median cost of $0.073 for\nsuccessful attempts versus $0.357 for failures. Success correlates strongly\nwith resource efficiency, enabling practical early-stopping thresholds at\napproximately 40 tool calls or $0.30 per challenge.\n  MAPTA's real-world findings are impactful given both the popularity of the\nrespective scanned GitHub repositories (8K-70K stars) and MAPTA's low average\noperating cost of $3.67 per open-source assessment: MAPTA discovered critical\nvulnerabilities including RCEs, command injections, secret exposure, and\narbitrary file write vulnerabilities. Findings are responsibly disclosed, 10\nfindings are under CVE review.",
      "pdf_url": "http://arxiv.org/pdf/2508.20816v1",
      "published": "2025-08-28T14:14:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20816v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting",
      "authors": [
        "Lorenzo Busellato",
        "Federico Cunico",
        "Diego Dall'Alba",
        "Marco Emporio",
        "Andrea Giachetti",
        "Riccardo Muradore",
        "Marco Cristani"
      ],
      "abstract": "To enable flexible, high-throughput automation in settings where people and\nrobots share workspaces, collaborative robotic cells must reconcile stringent\nsafety guarantees with the need for responsive and effective behavior. A\ndynamic obstacle is the stochastic, task-dependent variability of human motion:\nwhen robots fall back on purely reactive or worst-case envelopes, they brake\nunnecessarily, stall task progress, and tamper with the fluidity that true\nHuman-Robot Interaction demands. In recent years, learning-based human-motion\nprediction has rapidly advanced, although most approaches produce worst-case\nscenario forecasts that often do not treat prediction uncertainty in a\nwell-structured way, resulting in over-conservative planning algorithms,\nlimiting their flexibility. We introduce Uncertainty-Aware Predictive Control\nBarrier Functions (UA-PCBFs), a unified framework that fuses probabilistic\nhuman hand motion forecasting with the formal safety guarantees of Control\nBarrier Functions. In contrast to other variants, our framework allows for\ndynamic adjustment of the safety margin thanks to the human motion uncertainty\nestimation provided by a forecasting module. Thanks to uncertainty estimation,\nUA-PCBFs empower collaborative robots with a deeper understanding of future\nhuman states, facilitating more fluid and intelligent interactions through\ninformed motion planning. We validate UA-PCBFs through comprehensive real-world\nexperiments with an increasing level of realism, including automated setups (to\nperform exactly repeatable motions) with a robotic hand and direct human-robot\ninteractions (to validate promptness, usability, and human confidence).\nRelative to state-of-the-art HRI architectures, UA-PCBFs show better\nperformance in task-critical metrics, significantly reducing the number of\nviolations of the robot's safe space during interaction with respect to the\nstate-of-the-art.",
      "pdf_url": "http://arxiv.org/pdf/2508.20812v1",
      "published": "2025-08-28T14:11:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20812v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A Graph-Based Test-Harness for LLM Evaluation",
      "authors": [
        "Jessica Lundin",
        "Guillaume Chabot-Couture"
      ],
      "abstract": "We present a first known prototype of a dynamic, systematic benchmark of\nmedical guidelines for 400+ questions, with 3.3+ trillion possible\ncombinations, covering 100\\% of guideline relationships. We transformed the WHO\nIMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,\ntreatments, follow-ups, severities) and 300+ edges, then used graph traversal\nto generate questions that incorporated age-specific scenarios and contextual\ndistractors to ensure clinical relevance. Our graph-based approach enables\nsystematic evaluation across clinical tasks (45-67\\% accuracy), and we find\nmodels excel at symptom recognition but struggle with triaging severity,\ntreatment protocols and follow-up care, demonstrating how customized benchmarks\ncan identify specific capability gaps that general-domain evaluations miss.\nBeyond evaluation, this dynamic MCQA methodology enhances LLM post-training\n(supervised finetuning, GRPO, DPO), where correct answers provide high-reward\nsamples without expensive human annotation. The graph-based approach\nsuccessfully addresses the coverage limitations of manually curated benchmarks.\nThis methodology is a step toward scalable, contamination-resistant solution\nfor creating comprehensive benchmarks that can be dynamically generated,\nincluding when the guidelines are updated. Code and datasets are available at\nhttps://github.com/jessicalundin/graph_testing_harness",
      "pdf_url": "http://arxiv.org/pdf/2508.20810v1",
      "published": "2025-08-28T14:10:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20810v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Exploring Machine Learning and Language Models for Multimodal Depression Detection",
      "authors": [
        "Javier Si Zhao Hong",
        "Timothy Zoe Delaya",
        "Sherwyn Chan Yin Kit",
        "Pai Chet Ng",
        "Xiaoxiao Miao"
      ],
      "abstract": "This paper presents our approach to the first Multimodal Personality-Aware\nDepression Detection Challenge, focusing on multimodal depression detection\nusing machine learning and deep learning models. We explore and compare the\nperformance of XGBoost, transformer-based architectures, and large language\nmodels (LLMs) on audio, video, and text features. Our results highlight the\nstrengths and limitations of each type of model in capturing depression-related\nsignals across modalities, offering insights into effective multimodal\nrepresentation strategies for mental health prediction.",
      "pdf_url": "http://arxiv.org/pdf/2508.20805v1",
      "published": "2025-08-28T14:07:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20805v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "title": "Speech Emotion Recognition via Entropy-Aware Score Selection",
      "authors": [
        "ChenYi Chua",
        "JunKai Wong",
        "Chengxin Chen",
        "Xiaoxiao Miao"
      ],
      "abstract": "In this paper, we propose a multimodal framework for speech emotion\nrecognition that leverages entropy-aware score selection to combine speech and\ntextual predictions. The proposed method integrates a primary pipeline that\nconsists of an acoustic model based on wav2vec2.0 and a secondary pipeline that\nconsists of a sentiment analysis model using RoBERTa-XLM, with transcriptions\ngenerated via Whisper-large-v3. We propose a late score fusion approach based\non entropy and varentropy thresholds to overcome the confidence constraints of\nprimary pipeline predictions. A sentiment mapping strategy translates three\nsentiment categories into four target emotion classes, enabling coherent\nintegration of multimodal predictions. The results on the IEMOCAP and\nMSP-IMPROV datasets show that the proposed method offers a practical and\nreliable enhancement over traditional single-modality systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.20796v1",
      "published": "2025-08-28T13:58:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20796v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Surfel-based 3D Registration with Equivariant SE(3) Features",
      "authors": [
        "Xueyang Kang",
        "Hang Zhao",
        "Kourosh Khoshelham",
        "Patrick Vandewalle"
      ],
      "abstract": "Point cloud registration is crucial for ensuring 3D alignment consistency of\nmultiple local point clouds in 3D reconstruction for remote sensing or digital\nheritage. While various point cloud-based registration methods exist, both\nnon-learning and learning-based, they ignore point orientations and point\nuncertainties, making the model susceptible to noisy input and aggressive\nrotations of the input point cloud like orthogonal transformation; thus, it\nnecessitates extensive training point clouds with transformation augmentations.\nTo address these issues, we propose a novel surfel-based pose learning\nregression approach. Our method can initialize surfels from Lidar point cloud\nusing virtual perspective camera parameters, and learns explicit\n$\\mathbf{SE(3)}$ equivariant features, including both position and rotation\nthrough $\\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative\ntransformation between source and target scans. The model comprises an\nequivariant convolutional encoder, a cross-attention mechanism for similarity\ncomputation, a fully-connected decoder, and a non-linear Huber loss.\nExperimental results on indoor and outdoor datasets demonstrate our model\nsuperiority and robust performance on real point-cloud scans compared to\nstate-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.20789v1",
      "published": "2025-08-28T13:53:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20789v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control",
      "authors": [
        "Yifan Zhang"
      ],
      "abstract": "Bus bunching remains a challenge for urban transit due to stochastic traffic\nand passenger demand. Traditional solutions rely on multi-agent reinforcement\nlearning (MARL) in loop-line settings, which overlook realistic operations\ncharacterized by heterogeneous routes, timetables, fluctuating demand, and\nvarying fleet sizes. We propose a novel single-agent reinforcement learning\n(RL) framework for bus holding control that avoids the data imbalance and\nconvergence issues of MARL under near-realistic simulation. A bidirectional\ntimetabled network with dynamic passenger demand is constructed. The key\ninnovation is reformulating the multi-agent problem into a single-agent one by\naugmenting the state space with categorical identifiers (vehicle ID, station\nID, time period) in addition to numerical features (headway, occupancy,\nvelocity). This high-dimensional encoding enables single-agent policies to\ncapture inter-agent dependencies, analogous to projecting non-separable inputs\ninto a higher-dimensional space. We further design a structured reward function\naligned with operational goals: instead of exponential penalties on headway\ndeviations, a ridge-shaped reward balances uniform headways and schedule\nadherence. Experiments show that our modified soft actor-critic (SAC) achieves\nmore stable and superior performance than benchmarks, including MADDPG (e.g.,\n-430k vs. -530k under stochastic conditions). These results demonstrate that\nsingle-agent deep RL, when enhanced with categorical structuring and\nschedule-aware rewards, can effectively manage bus holding in non-loop,\nreal-world contexts. This paradigm offers a robust, scalable alternative to\nMARL frameworks, particularly where agent-specific experiences are imbalanced.",
      "pdf_url": "http://arxiv.org/pdf/2508.20784v1",
      "published": "2025-08-28T13:47:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20784v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Compositional Generalisation in VLMs and Diffusion Models",
      "authors": [
        "Beth Pearson",
        "Bilal Boulbarss",
        "Michael Wray",
        "Martha Lewis"
      ],
      "abstract": "A fundamental aspect of the semantics of natural language is that novel\nmeanings can be formed from the composition of previously known parts.\nVision-language models (VLMs) have made significant progress in recent years,\nhowever, there is evidence that they are unable to perform this kind of\ncomposition. For example, given an image of a red cube and a blue cylinder, a\nVLM such as CLIP is likely to incorrectly label the image as a red cylinder or\na blue cube, indicating it represents the image as a `bag-of-words' and fails\nto capture compositional semantics. Diffusion models have recently gained\nsignificant attention for their impressive generative abilities, and zero-shot\nclassifiers based on diffusion models have been shown to perform competitively\nwith CLIP in certain compositional tasks. In this work we explore whether the\ngenerative Diffusion Classifier has improved compositional generalisation\nabilities compared to discriminative models. We assess three models --\nDiffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with\nattributes and relations in both zero-shot learning (ZSL) and generalised\nzero-shot learning (GZSL) settings. Our results show that the Diffusion\nClassifier and ViLT perform well at concept binding tasks, but that all models\nstruggle significantly with the relational GZSL task, underscoring the broader\nchallenges VLMs face with relational reasoning. Analysis of CLIP embeddings\nsuggests that the difficulty may stem from overly similar representations of\nrelational concepts such as left and right. Code and dataset are available at:\nhttps://github.com/otmive/diffusion_classifier_clip",
      "pdf_url": "http://arxiv.org/pdf/2508.20783v1",
      "published": "2025-08-28T13:45:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20783v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML",
      "authors": [
        "Kuniko Paxton",
        "Koorosh Aslansefat",
        "Amila Akagić",
        "Dhavalkumar Thakker",
        "Yiannis Papadopoulos"
      ],
      "abstract": "Recent advancements in skin lesion classification models have significantly\nimproved accuracy, with some models even surpassing dermatologists' diagnostic\nperformance. However, in medical practice, distrust in AI models remains a\nchallenge. Beyond high accuracy, trustworthy, explainable diagnoses are\nessential. Existing explainability methods have reliability issues, with\nLIME-based methods suffering from inconsistency, while CAM-based methods\nfailing to consider all classes. To address these limitations, we propose\nGlobal Class Activation Probabilistic Map Evaluation, a method that analyses\nall classes' activation probability maps probabilistically and at a pixel\nlevel. By visualizing the diagnostic process in a unified manner, it helps\nreduce the risk of misdiagnosis. Furthermore, the application of SafeML\nenhances the detection of false diagnoses and issues warnings to doctors and\npatients as needed, improving diagnostic reliability and ultimately patient\nsafety. We evaluated our method using the ISIC datasets with MobileNetV2 and\nVision Transformers.",
      "pdf_url": "http://arxiv.org/pdf/2508.20776v1",
      "published": "2025-08-28T13:32:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20776v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI",
      "authors": [
        "Christoforos N. Spartalis",
        "Theodoros Semertzidis",
        "Petros Daras",
        "Efstratios Gavves"
      ],
      "abstract": "We introduce SAFEMax, a novel method for Machine Unlearning in diffusion\nmodels. Grounded in information-theoretic principles, SAFEMax maximizes the\nentropy in generated images, causing the model to generate Gaussian noise when\nconditioned on impermissible classes by ultimately halting its denoising\nprocess. Also, our method controls the balance between forgetting and retention\nby selectively focusing on the early diffusion steps, where class-specific\ninformation is prominent. Our results demonstrate the effectiveness of SAFEMax\nand highlight its substantial efficiency gains over state-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.20773v1",
      "published": "2025-08-28T13:29:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20773v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Signs of Struggle: Spotting Cognitive Distortions across Language and Register",
      "authors": [
        "Abhishek Kuber",
        "Enrico Liscio",
        "Ruixuan Zhang",
        "Caroline Figueroa",
        "Pradeep K. Murukannaiah"
      ],
      "abstract": "Rising mental health issues among youth have increased interest in automated\napproaches for detecting early signs of psychological distress in digital text.\nOne key focus is the identification of cognitive distortions, irrational\nthought patterns that have a role in aggravating mental distress. Early\ndetection of these distortions may enable timely, low-cost interventions. While\nprior work has focused on English clinical data, we present the first in-depth\nstudy of cross-lingual and cross-register generalization of cognitive\ndistortion detection, analyzing forum posts written by Dutch adolescents. Our\nfindings show that while changes in language and writing style can\nsignificantly affect model performance, domain adaptation methods show the most\npromise.",
      "pdf_url": "http://arxiv.org/pdf/2508.20771v1",
      "published": "2025-08-28T13:28:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20771v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection",
      "authors": [
        "Harethah Abu Shairah",
        "Hasan Abed Al Kader Hammoud",
        "George Turkiyyah",
        "Bernard Ghanem"
      ],
      "abstract": "Safety alignment in Large Language Models (LLMs) often involves mediating\ninternal representations to refuse harmful requests. Recent research has\ndemonstrated that these safety mechanisms can be bypassed by ablating or\nremoving specific representational directions within the model. In this paper,\nwe propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box\nmethod that amplifies a model's safety alignment by permanently steering its\nactivations toward the refusal-mediating subspace. ROSI operates as a simple,\nfine-tuning-free rank-one weight modification applied to all residual stream\nwrite matrices. The required safety direction can be computed from a small set\nof harmful and harmless instruction pairs. We show that ROSI consistently\nincreases safety refusal rates - as evaluated by Llama Guard 3 - while\npreserving the utility of the model on standard benchmarks such as MMLU,\nHellaSwag, and Arc. Furthermore, we show that ROSI can also re-align\n'uncensored' models by amplifying their own latent safety directions,\ndemonstrating its utility as an effective last-mile safety procedure. Our\nresults suggest that targeted, interpretable weight steering is a cheap and\npotent mechanism to improve LLM safety, complementing more resource-intensive\nfine-tuning paradigms.",
      "pdf_url": "http://arxiv.org/pdf/2508.20766v1",
      "published": "2025-08-28T13:22:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20766v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding",
      "authors": [
        "Gowreesh Mago",
        "Pascal Mettes",
        "Stevan Rudinac"
      ],
      "abstract": "The automatic understanding of video content is advancing rapidly. Empowered\nby deeper neural networks and large datasets, machines are increasingly capable\nof understanding what is concretely visible in video frames, whether it be\nobjects, actions, events, or scenes. In comparison, humans retain a unique\nability to also look beyond concrete entities and recognize abstract concepts\nlike justice, freedom, and togetherness. Abstract concept recognition forms a\ncrucial open challenge in video understanding, where reasoning on multiple\nsemantic levels based on contextual information is key. In this paper, we argue\nthat the recent advances in foundation models make for an ideal setting to\naddress abstract concept understanding in videos. Automated understanding of\nhigh-level abstract concepts is imperative as it enables models to be more\naligned with human reasoning and values. In this survey, we study different\ntasks and datasets used to understand abstract concepts in video content. We\nobserve that, periodically and over a long period, researchers have attempted\nto solve these tasks, making the best use of the tools available at their\ndisposal. We advocate that drawing on decades of community experience will help\nus shed light on this important open grand challenge and avoid ``re-inventing\nthe wheel'' as we start revisiting it in the era of multi-modal foundation\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2508.20765v1",
      "published": "2025-08-28T13:19:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20765v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer",
      "authors": [
        "Fachri Najm Noer Kartiman",
        "Rasim",
        "Yaya Wihardi",
        "Nurul Hasanah",
        "Oskar Natan",
        "Bambang Wahono",
        "Taufik Ibnu Salim"
      ],
      "abstract": "Focusing on the development of an end-to-end autonomous vehicle model with\npixel-to-pixel context awareness, this research proposes the SKGE-Swin\narchitecture. This architecture utilizes the Swin Transformer with a skip-stage\nmechanism to broaden feature representation globally and at various network\nlevels. This approach enables the model to extract information from distant\npixels by leveraging the Swin Transformer's Shifted Window-based Multi-head\nSelf-Attention (SW-MSA) mechanism and to retain critical information from the\ninitial to the final stages of feature extraction, thereby enhancing its\ncapability to comprehend complex patterns in the vehicle's surroundings. The\nmodel is evaluated on the CARLA platform using adversarial scenarios to\nsimulate real-world conditions. Experimental results demonstrate that the\nSKGE-Swin architecture achieves a superior Driving Score compared to previous\nmethods. Furthermore, an ablation study will be conducted to evaluate the\ncontribution of each architectural component, including the influence of skip\nconnections and the use of the Swin Transformer, in improving model\nperformance.",
      "pdf_url": "http://arxiv.org/pdf/2508.20762v1",
      "published": "2025-08-28T13:17:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20762v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Occlusion Robustness of CLIP for Military Vehicle Classification",
      "authors": [
        "Jan Erik van Woerden",
        "Gertjan Burghouts",
        "Lotte Nijskens",
        "Alma M. Liezenga",
        "Sabina van Rooij",
        "Frank Ruis",
        "Hugo J. Kuijf"
      ],
      "abstract": "Vision-language models (VLMs) like CLIP enable zero-shot classification by\naligning images and text in a shared embedding space, offering advantages for\ndefense applications with scarce labeled data. However, CLIP's robustness in\nchallenging military environments, with partial occlusion and degraded\nsignal-to-noise ratio (SNR), remains underexplored. We investigate CLIP\nvariants' robustness to occlusion using a custom dataset of 18 military vehicle\nclasses and evaluate using Normalized Area Under the Curve (NAUC) across\nocclusion percentages. Four key insights emerge: (1) Transformer-based CLIP\nmodels consistently outperform CNNs, (2) fine-grained, dispersed occlusions\ndegrade performance more than larger contiguous occlusions, (3) despite\nimproved accuracy, performance of linear-probed models sharply drops at around\n35% occlusion, (4) by finetuning the model's backbone, this performance drop\noccurs at more than 60% occlusion. These results underscore the importance of\nocclusion-specific augmentations during training and the need for further\nexploration into patch-level sensitivity and architectural resilience for\nreal-world deployment of CLIP.",
      "pdf_url": "http://arxiv.org/pdf/2508.20760v1",
      "published": "2025-08-28T13:16:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20760v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding",
      "authors": [
        "Jiawen Lin",
        "Shiran Bian",
        "Yihang Zhu",
        "Wenbin Tan",
        "Yachao Zhang",
        "Yuan Xie",
        "Yanyun Qu"
      ],
      "abstract": "3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using\nnatural language descriptions. Although supervised methods achieve higher\naccuracy in constrained settings, zero-shot 3DVG holds greater promise for\nreal-world applications since eliminating scene-specific training requirements.\nHowever, existing zero-shot methods face challenges of spatial-limited\nreasoning due to reliance on single-view localization, and contextual omissions\nor detail degradation. To address these issues, we propose SeqVLM, a novel\nzero-shot 3DVG framework that leverages multi-view real-world scene images with\nspatial information for target object reasoning. Specifically, SeqVLM first\ngenerates 3D instance proposals via a 3D semantic segmentation network and\nrefines them through semantic filtering, retaining only semantic-relevant\ncandidates. A proposal-guided multi-view projection strategy then projects\nthese candidate proposals onto real scene image sequences, preserving spatial\nrelationships and contextual details in the conversion process of 3D point\ncloud to images. Furthermore, to mitigate VLM computational overload, we\nimplement a dynamic scheduling mechanism that iteratively processes\nsequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to\nidentify textually specified objects. Experiments on the ScanRefer and Nr3D\nbenchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores\nof 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,\nrespectively, which advance 3DVG toward greater generalization and real-world\napplicability. The code is available at https://github.com/JiawLin/SeqVLM.",
      "pdf_url": "http://arxiv.org/pdf/2508.20758v1",
      "published": "2025-08-28T13:15:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20758v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Provable Benefits of In-Tool Learning for Large Language Models",
      "authors": [
        "Sam Houliston",
        "Ambroise Odonnat",
        "Charles Arnal",
        "Vivien Cabannes"
      ],
      "abstract": "Tool-augmented language models, equipped with retrieval, memory, or external\nAPIs, are reshaping AI, yet their theoretical advantages remain underexplored.\nIn this paper, we address this question by demonstrating the benefits of\nin-tool learning (external retrieval) over in-weight learning (memorization)\nfor factual recall. We show that the number of facts a model can memorize\nsolely in its weights is fundamentally limited by its parameter count. In\ncontrast, we prove that tool-use enables unbounded factual recall via a simple\nand efficient circuit construction. These results are validated in controlled\nexperiments, where tool-using models consistently outperform memorizing ones.\nWe further show that for pretrained large language models, teaching tool-use\nand general rules is more effective than finetuning facts into memory. Our work\nprovides both a theoretical and empirical foundation, establishing why\ntool-augmented workflows are not just practical, but provably more scalable.",
      "pdf_url": "http://arxiv.org/pdf/2508.20755v1",
      "published": "2025-08-28T13:12:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20755v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting",
      "authors": [
        "Yuxi Hu",
        "Jun Zhang",
        "Kuangyi Chen",
        "Zhe Zhang",
        "Friedrich Fraundorfer"
      ],
      "abstract": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen\nscenes without per-scene optimization. In particular, recent advancements\nutilize feed-forward networks to predict per-pixel Gaussian parameters,\nenabling high-quality synthesis from sparse input views. However, existing\napproaches fall short in encoding discriminative, multi-view consistent\nfeatures for Gaussian predictions, which struggle to construct accurate\ngeometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a\nframework that enhances feature learning by incorporating context-aware,\ncross-dimension, and cross-scale constraints. Our architecture integrates three\nlightweight modules into a unified rendering pipeline, improving feature fusion\nand enabling photorealistic synthesis without requiring additional supervision.\nExtensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS\nachieves state-of-the-art rendering quality and generalization ability. Code is\navailable at: https://github.com/YuhsiHu/C3-GS.",
      "pdf_url": "http://arxiv.org/pdf/2508.20754v1",
      "published": "2025-08-28T13:12:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20754v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol",
      "authors": [
        "Wei Ma",
        "Yixiao Yang",
        "Qiang Hu",
        "Shi Ying",
        "Zhi Jin",
        "Bo Du",
        "Zhenchang Xing",
        "Tianlin Li",
        "Junjie Shi",
        "Yang Liu",
        "Linxiao Jiang"
      ],
      "abstract": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.",
      "pdf_url": "http://arxiv.org/pdf/2508.20737v1",
      "published": "2025-08-28T13:00:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20737v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision",
      "authors": [
        "Ao Cheng",
        "Lei Zhang",
        "Guowei He"
      ],
      "abstract": "Large language models (LLMs) serve as an active and promising field of\ngenerative artificial intelligence and have demonstrated abilities to perform\ncomplex tasks in multiple domains, including mathematical and scientific\nreasoning. In this work, we construct a novel agent framework for solving\nrepresentative problems in scientific computing. The proposed agent,\nincorporating a \"rewriting-resolution-review-revision\" logical chain via three\nreasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,\nrespectively), is integrated in a collaborative and interactive manner. The\nConsultant module endows the agent with knowledge transfer capabilities to link\nproblems to professional domain insights, thereby rewriting problem\ndescriptions through text augmentation. The Programmer module is responsible\nfor generating and executing well-structured code to deliver the problem\nresolution. The Reviewer module equips the agent with the capacity for\nself-debugging and self-refinement through interactive feedback with code\nruntime outputs. By leveraging the end-to-end review mechanism, the executable\ncode provided by the Programmer attains the iterative revision. A comprehensive\nevaluation is conducted on the performance of the proposed agent framework in\nsolving PDEs, ill-conditioned linear systems, and data-driven physical analysis\nproblems. Compared to single-model, this collaborative framework significantly\nimproves the bug-free code generation rate and reduces the occurrence of\nnon-physical solutions, thereby establishing a highly reliable framework for\nautonomous code generation based on natural language descriptions. The review\nmechanism improved the average execution success (bug-free code and non-NaN\nsolutions) rate of the latest reasoning models. In summary, our agent framework\nestablishes automatic code generation and review as a promising scientific\ncomputing paradigm.",
      "pdf_url": "http://arxiv.org/pdf/2508.20729v1",
      "published": "2025-08-28T12:50:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20729v1",
      "categories": [
        "cs.AI",
        "physics.comp-ph"
      ]
    },
    {
      "title": "EEGDM: Learning EEG Representation with Latent Diffusion Model",
      "authors": [
        "Shaocong Wang",
        "Tong Liu",
        "Ming Li",
        "Minjing Yu",
        "Yong-Jin Liu"
      ],
      "abstract": "While electroencephalography (EEG) signal analysis using deep learning has\nshown great promise, existing approaches still face significant challenges in\nlearning generalizable representations that perform well across diverse tasks,\nparticularly when training data is limited. Current EEG representation learning\nmethods including EEGPT and LaBraM typically rely on simple masked\nreconstruction objective, which may not fully capture the rich semantic\ninformation and complex patterns inherent in EEG signals. In this paper, we\npropose EEGDM, a novel self-supervised EEG representation learning method based\non the latent diffusion model, which leverages EEG signal generation as a\nself-supervised objective, turning the diffusion model into a strong\nrepresentation learner capable of capturing EEG semantics. EEGDM incorporates\nan EEG encoder that distills EEG signals and their channel augmentations into a\ncompact representation, acting as conditional information to guide the\ndiffusion model for generating EEG signals. This design endows EEGDM with a\ncompact latent space, which not only offers ample control over the generative\nprocess but also can be leveraged for downstream tasks. Experimental results\nshow that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively\nlearns robust representations, and (3) achieves competitive performance with\nmodest pre-training data size across diverse downstream tasks, underscoring its\ngeneralizability and practical utility.",
      "pdf_url": "http://arxiv.org/pdf/2508.20705v1",
      "published": "2025-08-28T12:23:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20705v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings",
      "authors": [
        "Ares Fabregat-Hernández",
        "Javier Palanca",
        "Vicent Botti"
      ],
      "abstract": "The paper introduces a novel framework based on category theory to enhance\nthe explainability of artificial intelligence systems, particularly focusing on\nword embeddings. Key topics include the construction of categories\n$\\mathcal{L}_T$ and $\\mathcal{P}_T$, providing schematic representations of the\nsemantics of a text $ T $, and reframing the selection of the element with\nmaximum probability as a categorical notion. Additionally, the monoidal\ncategory $\\mathcal{P}_T$ is constructed to visualize various methods of\nextracting semantic information from $T$, offering a dimension-agnostic\ndefinition of semantic spaces reliant solely on information within the text.\n  Furthermore, the paper defines the categories of configurations Conf and word\nembeddings $\\mathcal{Emb}$, accompanied by the concept of divergence as a\ndecoration on $\\mathcal{Emb}$. It establishes a mathematically precise method\nfor comparing word embeddings, demonstrating the equivalence between the GloVe\nand Word2Vec algorithms and the metric MDS algorithm, transitioning from neural\nnetwork algorithms (black box) to a transparent framework. Finally, the paper\npresents a mathematical approach to computing biases before embedding and\noffers insights on mitigating biases at the semantic space level, advancing the\nfield of explainable artificial intelligence.",
      "pdf_url": "http://arxiv.org/pdf/2508.20701v1",
      "published": "2025-08-28T12:19:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20701v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "math.CT"
      ]
    },
    {
      "title": "Generative Annotation for ASR Named Entity Correction",
      "authors": [
        "Yuanchang Luo",
        "Daimeng Wei",
        "Shaojun Li",
        "Hengchao Shang",
        "Jiaxin Guo",
        "Zongyao Li",
        "Zhanglin Wu",
        "Xiaoyu Chen",
        "Zhiqiang Rao",
        "Jinlong Yang",
        "Hao Yang"
      ],
      "abstract": "End-to-end automatic speech recognition systems often fail to transcribe\ndomain-specific named entities, causing catastrophic failures in downstream\ntasks. Numerous fast and lightweight named entity correction (NEC) models have\nbeen proposed in recent years. These models, mainly leveraging phonetic-level\nedit distance algorithms, have shown impressive performances. However, when the\nforms of the wrongly-transcribed words(s) and the ground-truth entity are\nsignificantly different, these methods often fail to locate the wrongly\ntranscribed words in hypothesis, thus limiting their usage. We propose a novel\nNEC method that utilizes speech sound features to retrieve candidate entities.\nWith speech sound features and candidate entities, we inovatively design a\ngenerative method to annotate entity errors in ASR transcripts and replace the\ntext with correct entities. This method is effective in scenarios of word form\ndifference. We test our method using open-source and self-constructed test\nsets. The results demonstrate that our NEC method can bring significant\nimprovement to entity accuracy. We will open source our self-constructed test\nset and training data.",
      "pdf_url": "http://arxiv.org/pdf/2508.20700v1",
      "published": "2025-08-28T12:18:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20700v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MobileCLIP2: Improving Multi-Modal Reinforced Training",
      "authors": [
        "Fartash Faghri",
        "Pavan Kumar Anasosalu Vasu",
        "Cem Koc",
        "Vaishaal Shankar",
        "Alexander Toshev",
        "Oncel Tuzel",
        "Hadi Pouransari"
      ],
      "abstract": "Foundation image-text models such as CLIP with zero-shot capabilities enable\na wide array of applications. MobileCLIP is a recent family of image-text\nmodels at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot\naccuracy. The main ingredients in MobileCLIP were its low-latency and light\narchitectures and a novel multi-modal reinforced training that made knowledge\ndistillation from multiple caption-generators and CLIP teachers efficient,\nscalable, and reproducible. In this paper, we improve the multi-modal\nreinforced training of MobileCLIP through: 1) better CLIP teacher ensembles\ntrained on the DFN dataset, 2) improved captioner teachers trained on the DFN\ndataset and fine-tuned on a diverse selection of high-quality image-caption\ndatasets. We discover new insights through ablations such as the importance of\ntemperature tuning in contrastive knowledge distillation, the effectiveness of\ncaption-generator fine-tuning for caption diversity, and the additive\nimprovement from combining synthetic captions generated by multiple models. We\ntrain a new family of models called MobileCLIP2 and achieve state-of-the-art\nImageNet-1k zero-shot accuracies at low latencies. In particular, we observe\n2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with\nMobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot\naccuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\\times$ smaller and\nimproves on DFN ViT-L/14 at 2.5$\\times$ lower latency. We release our\npretrained models (https://github.com/apple/ml-mobileclip) and the data\ngeneration code (https://github.com/apple/ml-mobileclip-dr). The data\ngeneration code makes it easy to create new reinforced datasets with arbitrary\nteachers using distributed scalable processing.",
      "pdf_url": "http://arxiv.org/pdf/2508.20691v1",
      "published": "2025-08-28T11:50:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20691v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning",
      "authors": [
        "Thanh Thi Nguyen",
        "Quoc Viet Hung Nguyen",
        "Jonathan Kua",
        "Imran Razzak",
        "Dung Nguyen",
        "Saeid Nahavandi"
      ],
      "abstract": "Enabling multiple autonomous machines to perform reliably requires the\ndevelopment of efficient cooperative control algorithms. This paper presents a\nsurvey of algorithms that have been developed for controlling and coordinating\nautonomous machines in complex environments. We especially focus on task\nallocation methods using computational intelligence (CI) and deep reinforcement\nlearning (RL). The advantages and disadvantages of the surveyed methods are\nanalysed thoroughly. We also propose and discuss in detail various future\nresearch directions that shed light on how to improve existing algorithms or\ncreate new methods to enhance the employability and performance of autonomous\nmachines in real-world applications. The findings indicate that CI and deep RL\nmethods provide viable approaches to addressing complex task allocation\nproblems in dynamic and uncertain environments. The recent development of deep\nRL has greatly contributed to the literature on controlling and coordinating\nautonomous machines, and it has become a growing trend in this area. It is\nenvisaged that this paper will provide researchers and engineers with a\ncomprehensive overview of progress in machine learning research related to\nautonomous machines. It also highlights underexplored areas, identifies\nemerging methodologies, and suggests new avenues for exploration in future\nresearch within this domain.",
      "pdf_url": "http://arxiv.org/pdf/2508.20688v1",
      "published": "2025-08-28T11:48:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20688v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science",
      "authors": [
        "Rui Mao",
        "Qian Liu",
        "Xiao Li",
        "Erik Cambria",
        "Amir Hussain"
      ],
      "abstract": "Cognitive Science has profoundly shaped disciplines such as Artificial\nIntelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and\nCulture. Many breakthroughs in AI trace their roots to cognitive theories,\nwhile AI itself has become an indispensable tool for advancing cognitive\nresearch. This reciprocal relationship motivates a comprehensive review of the\nintersections between AI and Cognitive Science. By synthesizing key\ncontributions from both perspectives, we observe that AI progress has largely\nemphasized practical task performance, whereas its cognitive foundations remain\nconceptually fragmented. We argue that the future of AI within Cognitive\nScience lies not only in improving performance but also in constructing systems\nthat deepen our understanding of the human mind. Promising directions include\naligning AI behaviors with cognitive frameworks, situating AI in embodiment and\nculture, developing personalized cognitive models, and rethinking AI ethics\nthrough cognitive co-evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2508.20674v1",
      "published": "2025-08-28T11:26:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20674v1",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ]
    },
    {
      "title": "Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music",
      "authors": [
        "Hongju Su",
        "Ke Li",
        "Lan Yang",
        "Honggang Zhang",
        "Yi-Zhe Song"
      ],
      "abstract": "Existing state-of-the-art symbolic music generation models predominantly\nadopt autoregressive or hierarchical autoregressive architectures, modelling\nsymbolic music as a sequence of attribute tokens with unidirectional temporal\ndependencies, under the assumption of a fixed, strict dependency structure\namong these attributes. However, we observe that using different attributes as\nthe initial token in these models leads to comparable performance. This\nsuggests that the attributes of a musical note are, in essence, a concurrent\nand unordered set, rather than a temporally dependent sequence. Based on this\ninsight, we introduce Amadeus, a novel symbolic music generation framework.\nAmadeus adopts a two-level architecture: an autoregressive model for note\nsequences and a bidirectional discrete diffusion model for attributes. To\nenhance performance, we propose Music Latent Space Discriminability Enhancement\nStrategy(MLSDES), incorporating contrastive learning constraints that amplify\ndiscriminability of intermediate music representations. The Conditional\nInformation Enhancement Module (CIEM) simultaneously strengthens note latent\nvector representation via attention mechanisms, enabling more precise note\ndecoding. We conduct extensive experiments on unconditional and\ntext-conditioned generation tasks. Amadeus significantly outperforms SOTA\nmodels across multiple metrics while achieving at least 4$\\times$ speed-up.\nFurthermore, we demonstrate training-free, fine-grained note attribute control\nfeasibility using our model. To explore the upper performance bound of the\nAmadeus architecture, we compile the largest open-source symbolic music dataset\nto date, AMD (Amadeus MIDI Dataset), supporting both pre-training and\nfine-tuning.",
      "pdf_url": "http://arxiv.org/pdf/2508.20665v1",
      "published": "2025-08-28T11:15:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20665v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse",
      "authors": [
        "Kan Chen",
        "Zhen Meng",
        "Xiangmin Xu",
        "Jiaming Yang",
        "Emma Li",
        "Philip G. Zhao"
      ],
      "abstract": "Real-time human-device interaction in industrial Metaverse faces challenges\nsuch as high computational load, limited bandwidth, and strict latency. This\npaper proposes a task-oriented edge-assisted cross-system framework using\ndigital twins (DTs) to enable responsive interactions. By predicting operator\nmotions, the system supports: 1) proactive Metaverse rendering for visual\nfeedback, and 2) preemptive control of remote devices. The DTs are decoupled\ninto two virtual functions-visual display and robotic control-optimizing both\nperformance and adaptability. To enhance generalizability, we introduce the\nHuman-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which\ndynamically adjusts prediction horizons. Evaluation on two tasks demonstrates\nthe framework's effectiveness: in a Trajectory-Based Drawing Control task, it\nreduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene\nrepresentation task for nuclear decommissioning, it achieves a PSNR of 22.11,\nSSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's\ncapability to ensure spatial precision and visual fidelity in real-time,\nhigh-risk industrial environments.",
      "pdf_url": "http://arxiv.org/pdf/2508.20664v1",
      "published": "2025-08-28T11:10:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20664v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "GDS Agent: A Graph Algorithmic Reasoning Agent",
      "authors": [
        "Borun Shi",
        "Ioannis Panagiotas"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We also\nintroduce a new benchmark that evaluates intermediate tool calls as well as\nfinal responses. The results indicate that GDS agent is able to solve a wide\nspectrum of graph tasks. We also provide detailed case studies for more\nopen-ended tasks and study scenarios where the agent struggles. Finally, we\ndiscuss the remaining challenges and the future roadmap.",
      "pdf_url": "http://arxiv.org/pdf/2508.20637v1",
      "published": "2025-08-28T10:35:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.20637v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    }
  ]
}