{
  "last_updated": "2025-06-18T00:53:31.850642",
  "papers": [
    {
      "title": "Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value",
      "authors": [
        "Yixian Xu",
        "Shengjie Luo",
        "Liwei Wang",
        "Di He",
        "Chang Liu"
      ],
      "abstract": "Diffusion models have achieved remarkable success in generative modeling.\nDespite more stable training, the loss of diffusion models is not indicative of\nabsolute data-fitting quality, since its optimal value is typically not zero\nbut unknown, leading to confusion between large optimal loss and insufficient\nmodel capacity. In this work, we advocate the need to estimate the optimal loss\nvalue for diagnosing and improving diffusion models. We first derive the\noptimal loss in closed form under a unified formulation of diffusion models,\nand develop effective estimators for it, including a stochastic variant\nscalable to large datasets with proper control of variance and bias. With this\ntool, we unlock the inherent metric for diagnosing the training quality of\nmainstream diffusion model variants, and develop a more performant training\nschedule based on the optimal loss. Moreover, using models with 120M to 1.5B\nparameters, we find that the power law is better demonstrated after subtracting\nthe optimal loss from the actual training loss, suggesting a more principled\nsetting for investigating the scaling law for diffusion models.",
      "pdf_url": "http://arxiv.org/pdf/2506.13763v1",
      "published": "2025-06-16T17:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13763v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ]
    },
    {
      "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
      "authors": [
        "Runpeng Yu",
        "Qi Li",
        "Xinchao Wang"
      ],
      "abstract": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
      "pdf_url": "http://arxiv.org/pdf/2506.13759v1",
      "published": "2025-06-16T17:59:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13759v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models",
      "authors": [
        "Edward Li",
        "Zichen Wang",
        "Jiahe Huang",
        "Jeong Joon Park"
      ],
      "abstract": "We present a unified framework for solving partial differential equations\n(PDEs) using video-inpainting diffusion transformer models. Unlike existing\nmethods that devise specialized strategies for either forward or inverse\nproblems under full or partial observation, our approach unifies these tasks\nunder a single, flexible generative framework. Specifically, we recast\nPDE-solving as a generalized inpainting problem, e.g., treating forward\nprediction as inferring missing spatiotemporal information of future states\nfrom initial conditions. To this end, we design a transformer-based\narchitecture that conditions on arbitrary patterns of known data to infer\nmissing values across time and space. Our method proposes pixel-space video\ndiffusion models for fine-grained, high-fidelity inpainting and conditioning,\nwhile enhancing computational efficiency through hierarchical modeling.\nExtensive experiments show that our video inpainting-based diffusion model\noffers an accurate and versatile solution across a wide range of PDEs and\nproblem setups, outperforming state-of-the-art baselines.",
      "pdf_url": "http://arxiv.org/pdf/2506.13754v2",
      "published": "2025-06-16T17:58:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13754v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Steering LLM Thinking with Budget Guidance",
      "authors": [
        "Junyan Li",
        "Wenshuo Zhao",
        "Yang Zhang",
        "Chuang Gan"
      ],
      "abstract": "Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget without requiring any LLM\nfine-tuning. Our approach introduces a lightweight predictor that models a\nGamma distribution over the remaining thinking length during next-token\ngeneration. This signal is then used to guide generation in a soft, token-level\nmanner, ensuring that the overall reasoning trace adheres to the specified\nthinking budget. Budget guidance enables natural control of the thinking\nlength, along with significant token efficiency improvements over baseline\nmethods on challenging math benchmarks. For instance, it achieves up to a 26%\naccuracy gain on the MATH-500 benchmark under tight budgets compared to\nbaseline methods, while maintaining competitive accuracy with only 63% of the\nthinking tokens used by the full-thinking model. Budget guidance also\ngeneralizes to broader task domains and exhibits emergent capabilities, such as\nestimating question difficulty. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/BudgetGuidance.",
      "pdf_url": "http://arxiv.org/pdf/2506.13752v1",
      "published": "2025-06-16T17:57:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13752v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction",
      "authors": [
        "Haoru Xue",
        "Xiaoyu Huang",
        "Dantong Niu",
        "Qiayuan Liao",
        "Thomas Kragerud",
        "Jan Tommy Gravdahl",
        "Xue Bin Peng",
        "Guanya Shi",
        "Trevor Darrell",
        "Koushil Screenath",
        "Shankar Sastry"
      ],
      "abstract": "Vision-language-action (VLA) models have demonstrated strong semantic\nunderstanding and zero-shot generalization, yet most existing systems assume an\naccurate low-level controller with hand-crafted action \"vocabulary\" such as\nend-effector pose or root velocity. This assumption confines prior work to\nquasi-static tasks and precludes the agile, whole-body behaviors required by\nhumanoid whole-body control (WBC) tasks. To capture this gap in the literature,\nwe start by introducing the first sim-to-real-ready, vision-language,\nclosed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10\ncategories. We then propose LeVERB: Latent Vision-Language-Encoded Robot\nBehavior, a hierarchical latent instruction-following framework for humanoid\nvision-language WBC, the first of its kind. At the top level, a vision-language\npolicy learns a latent action vocabulary from synthetically rendered kinematic\ndemonstrations; at the low level, a reinforcement-learned WBC policy consumes\nthese latent verbs to generate dynamics-level commands. In our benchmark,\nLeVERB can zero-shot attain a 80% success rate on simple visual navigation\ntasks, and 58.5% success rate overall, outperforming naive hierarchical\nwhole-body VLA implementation by 7.8 times.",
      "pdf_url": "http://arxiv.org/pdf/2506.13751v1",
      "published": "2025-06-16T17:56:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13751v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability",
      "authors": [
        "Shova Kuikel",
        "Aritran Piplai",
        "Palvi Aggarwal"
      ],
      "abstract": "Phishing attacks remain one of the most prevalent and persistent\ncybersecurity threat with attackers continuously evolving and intensifying\ntactics to evade the general detection system. Despite significant advances in\nartificial intelligence and machine learning, faithfully reproducing the\ninterpretable reasoning with classification and explainability that underpin\nphishing judgments remains challenging. Due to recent advancement in Natural\nLanguage Processing, Large Language Models (LLMs) show a promising direction\nand potential for improving domain specific phishing classification tasks.\nHowever, enhancing the reliability and robustness of classification models\nrequires not only accurate predictions from LLMs but also consistent and\ntrustworthy explanations aligning with those predictions. Therefore, a key\nquestion remains: can LLMs not only classify phishing emails accurately but\nalso generate explanations that are reliably aligned with their predictions and\ninternally self-consistent? To answer these questions, we have fine-tuned\ntransformer based models, including BERT, Llama models, and Wizard, to improve\ndomain relevance and make them more tailored to phishing specific distinctions,\nusing Binary Sequence Classification, Contrastive Learning (CL) and Direct\nPreference Optimization (DPO). To that end, we examined their performance in\nphishing classification and explainability by applying the ConsistenCy measure\nbased on SHAPley values (CC SHAP), which measures prediction explanation token\nalignment to test the model's internal faithfulness and consistency and uncover\nthe rationale behind its predictions and reasoning. Overall, our findings show\nthat Llama models exhibit stronger prediction explanation token alignment with\nhigher CC SHAP scores despite lacking reliable decision making accuracy,\nwhereas Wizard achieves better prediction accuracy but lower CC SHAP scores.",
      "pdf_url": "http://arxiv.org/pdf/2506.13746v1",
      "published": "2025-06-16T17:54:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13746v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning",
      "authors": [
        "Brahim Driss",
        "Alex Davey",
        "Riad Akrour"
      ],
      "abstract": "Preference-based reinforcement learning (PbRL) has emerged as a promising\napproach for learning behaviors from human feedback without predefined reward\nfunctions. However, current PbRL methods face a critical challenge in\neffectively exploring the preference space, often converging prematurely to\nsuboptimal policies that satisfy only a narrow subset of human preferences. In\nthis work, we identify and address this preference exploration problem through\npopulation-based methods. We demonstrate that maintaining a diverse population\nof agents enables more comprehensive exploration of the preference landscape\ncompared to single-agent approaches. Crucially, this diversity improves reward\nmodel learning by generating preference queries with clearly distinguishable\nbehaviors, a key factor in real-world scenarios where humans must easily\ndifferentiate between options to provide meaningful feedback. Our experiments\nreveal that current methods may fail by getting stuck in local optima,\nrequiring excessive feedback, or degrading significantly when human evaluators\nmake errors on similar trajectories, a realistic scenario often overlooked by\nmethods relying on perfect oracle teachers. Our population-based approach\ndemonstrates robust performance when teachers mislabel similar trajectory\nsegments and shows significantly enhanced preference exploration\ncapabilities,particularly in environments with complex reward landscapes.",
      "pdf_url": "http://arxiv.org/pdf/2506.13741v1",
      "published": "2025-06-16T17:51:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13741v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Instruction Following by Boosting Attention of Large Language Models",
      "authors": [
        "Vitoria Guardieiro",
        "Adam Stein",
        "Avishree Khare",
        "Eric Wong"
      ],
      "abstract": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering.",
      "pdf_url": "http://arxiv.org/pdf/2506.13734v1",
      "published": "2025-06-16T17:42:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13734v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "BanditWare: A Contextual Bandit-based Framework for Hardware Prediction",
      "authors": [
        "Tainã Coleman",
        "Hena Ahmed",
        "Ravi Shende",
        "Ismael Perez",
        "Ïlkay Altintaş"
      ],
      "abstract": "Distributed computing systems are essential for meeting the demands of modern\napplications, yet transitioning from single-system to distributed environments\npresents significant challenges. Misallocating resources in shared systems can\nlead to resource contention, system instability, degraded performance, priority\ninversion, inefficient utilization, increased latency, and environmental\nimpact.\n  We present BanditWare, an online recommendation system that dynamically\nselects the most suitable hardware for applications using a contextual\nmulti-armed bandit algorithm. BanditWare balances exploration and exploitation,\ngradually refining its hardware recommendations based on observed application\nperformance while continuing to explore potentially better options. Unlike\ntraditional statistical and machine learning approaches that rely heavily on\nlarge historical datasets, BanditWare operates online, learning and adapting in\nreal-time as new workloads arrive.\n  We evaluated BanditWare on three workflow applications: Cycles (an\nagricultural science scientific workflow) BurnPro3D (a web-based platform for\nfire science) and a matrix multiplication application. Designed for seamless\nintegration with the National Data Platform (NDP), BanditWare enables users of\nall experience levels to optimize resource allocation efficiently.",
      "pdf_url": "http://arxiv.org/pdf/2506.13730v1",
      "published": "2025-06-16T17:40:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13730v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs",
      "authors": [
        "Sayed Mohammad Vakilzadeh Hatefi",
        "Maximilian Dreyer",
        "Reduan Achtibat",
        "Patrick Kahardipraja",
        "Thomas Wiegand",
        "Wojciech Samek",
        "Sebastian Lapuschkin"
      ],
      "abstract": "Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3.",
      "pdf_url": "http://arxiv.org/pdf/2506.13727v1",
      "published": "2025-06-16T17:38:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13727v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models",
      "authors": [
        "Arjun Krishna",
        "Aaditya Rastogi",
        "Erick Galinkin"
      ],
      "abstract": "The introduction of advanced reasoning capabilities have improved the\nproblem-solving performance of large language models, particularly on math and\ncoding benchmarks. However, it remains unclear whether these reasoning models\nare more or less vulnerable to adversarial prompt attacks than their\nnon-reasoning counterparts. In this work, we present a systematic evaluation of\nweaknesses in advanced reasoning models compared to similar non-reasoning\nmodels across a diverse set of prompt-based attack categories. Using\nexperimental data, we find that on average the reasoning-augmented models are\n\\emph{slightly more robust} than non-reasoning models (42.51\\% vs 45.53\\%\nattack success rate, lower is better). However, this overall trend masks\nsignificant category-specific differences: for certain attack types the\nreasoning models are substantially \\emph{more vulnerable} (e.g., up to 32\npercentage points worse on a tree-of-attacks prompt), while for others they are\nmarkedly \\emph{more robust} (e.g., 29.8 points better on cross-site scripting\ninjection). Our findings highlight the nuanced security implications of\nadvanced reasoning in language models and emphasize the importance of\nstress-testing safety across diverse adversarial techniques.",
      "pdf_url": "http://arxiv.org/pdf/2506.13726v1",
      "published": "2025-06-16T17:32:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13726v1",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "Contrastive Self-Supervised Learning As Neural Manifold Packing",
      "authors": [
        "Guanming Zhang",
        "David J. Heeger",
        "Stefano Martiniani"
      ],
      "abstract": "Contrastive self-supervised learning based on point-wise comparisons has been\nwidely studied for vision tasks. In the visual cortex of the brain, neuronal\nresponses to distinct stimulus classes are organized into geometric structures\nknown as neural manifolds. Accurate classification of stimuli can be achieved\nby effectively separating these manifolds, akin to solving a packing problem.\nWe introduce Contrastive Learning As Manifold Packing (CLAMP), a\nself-supervised framework that recasts representation learning as a manifold\npacking problem. CLAMP introduces a loss function inspired by the potential\nenergy of short-range repulsive particle systems, such as those encountered in\nthe physics of simple liquids and jammed packings. In this framework, each\nclass consists of sub-manifolds embedding multiple augmented views of a single\nimage. The sizes and positions of the sub-manifolds are dynamically optimized\nby following the gradient of a packing loss. This approach yields interpretable\ndynamics in the embedding space that parallel jamming physics, and introduces\ngeometrically meaningful hyperparameters within the loss function. Under the\nstandard linear evaluation protocol, which freezes the backbone and trains only\na linear classifier, CLAMP achieves competitive performance with\nstate-of-the-art self-supervised models. Furthermore, our analysis reveals that\nneural manifolds corresponding to different categories emerge naturally and are\neffectively separated in the learned representation space, highlighting the\npotential of CLAMP to bridge insights from physics, neural science, and machine\nlearning.",
      "pdf_url": "http://arxiv.org/pdf/2506.13717v1",
      "published": "2025-06-16T17:24:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13717v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "stat.ML"
      ]
    },
    {
      "title": "TimeMaster: Training Time-Series Multimodal LLMs to Reason via Reinforcement Learning",
      "authors": [
        "Junru Zhang",
        "Lang Feng",
        "Xu Guo",
        "Yuhan Wu",
        "Yabo Dong",
        "Duanqing Xu"
      ],
      "abstract": "Time-series reasoning remains a significant challenge in multimodal large\nlanguage models (MLLMs) due to the dynamic temporal patterns, ambiguous\nsemantics, and lack of temporal priors. In this work, we introduce TimeMaster,\na reinforcement learning (RL)-based method that enables time-series MLLMs to\nperform structured, interpretable reasoning directly over visualized\ntime-series inputs and task prompts. TimeMaster adopts a three-part structured\noutput format, reasoning, classification, and domain-specific extension, and is\noptimized via a composite reward function that aligns format adherence,\nprediction accuracy, and open-ended insight quality. The model is trained using\na two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish\na good initialization, followed by Group Relative Policy Optimization (GRPO) at\nthe token level to enable stable and targeted reward-driven improvement in\ntime-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across\nsix real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster\nachieves state-of-the-art performance, outperforming both classical time-series\nmodels and few-shot GPT-4o by over 14.6% and 7.3% performance gain,\nrespectively. Notably, TimeMaster goes beyond time-series classification: it\nalso exhibits expert-like reasoning behavior, generates context-aware\nexplanations, and delivers domain-aligned insights. Our results highlight that\nreward-driven RL can be a scalable and promising path toward integrating\ntemporal understanding into time-series MLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2506.13705v1",
      "published": "2025-06-16T17:12:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13705v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Value-Free Policy Optimization via Reward Partitioning",
      "authors": [
        "Bilal Faye",
        "Hanane Azzag",
        "Mustapha Lebbah"
      ],
      "abstract": "Single-trajectory reinforcement learning (RL) methods aim to optimize\npolicies from datasets consisting of (prompt, response, reward) triplets, where\nscalar rewards are directly available. This supervision format is highly\npractical, as it mirrors real-world human feedback, such as thumbs-up/down\nsignals, and avoids the need for structured preference annotations. In\ncontrast, pairwise preference-based methods like Direct Preference Optimization\n(DPO) rely on datasets with both preferred and dispreferred responses, which\nare harder to construct and less natural to collect. Among single-trajectory\napproaches, Direct Reward Optimization (DRO) has shown strong empirical\nperformance due to its simplicity and stability. However, DRO requires\napproximating a value function, which introduces several limitations: high\noff-policy variance, coupling between policy and value learning, and a lack of\nabsolute supervision on the policy itself. We introduce Reward Partitioning\nOptimization (RPO), a new method that resolves these limitations by removing\nthe need to model the value function. Instead, RPO normalizes observed rewards\nusing a partitioning approach estimated directly from data. This leads to a\nstraightforward supervised learning objective on the policy, with no auxiliary\nmodels and no joint optimization. RPO provides direct and stable supervision on\nthe policy, making it robust and easy to implement in practice. We validate RPO\non scalar-feedback language modeling tasks using Flan-T5 encoder-decoder\nmodels. Our results demonstrate that RPO outperforms existing single-trajectory\nbaselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings\nconfirm that RPO is a simple, effective, and theoretically grounded method for\nsingle-trajectory policy optimization.",
      "pdf_url": "http://arxiv.org/pdf/2506.13702v1",
      "published": "2025-06-16T17:06:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13702v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems",
      "authors": [
        "Shang-Chi Tsai",
        "Yun-Nung Chen"
      ],
      "abstract": "With the advancement of large language models, many dialogue systems are now\ncapable of providing reasonable and informative responses to patients' medical\nconditions. However, when patients consult their doctor, they may experience\nnegative emotions due to the severity and urgency of their situation. If the\nmodel can provide appropriate comfort and empathy based on the patient's\nnegative emotions while answering medical questions, it will likely offer a\nmore reassuring experience during the medical consultation process. To address\nthis issue, our paper explores the balance between knowledge sharing and\nemotional support in the healthcare dialogue process. We utilize a large\nlanguage model to rewrite a real-world interactive medical dialogue dataset,\ngenerating patient queries with negative emotions and corresponding medical\nresponses aimed at soothing the patient's emotions while addressing their\nconcerns. The modified data serves to refine the latest large language models\nwith various fine-tuning methods, enabling them to accurately provide sentences\nwith both emotional reassurance and constructive suggestions in response to\npatients' questions. Compared to the original LLM model, our experimental\nresults demonstrate that our methodology significantly enhances the model's\nability to generate emotional responses while maintaining its original\ncapability to provide accurate knowledge-based answers.",
      "pdf_url": "http://arxiv.org/pdf/2506.13692v1",
      "published": "2025-06-16T16:54:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13692v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Meta-learning how to Share Credit among Macro-Actions",
      "authors": [
        "Ionel-Alexandru Hosu",
        "Traian Rebedea",
        "Razvan Pascanu"
      ],
      "abstract": "One proposed mechanism to improve exploration in reinforcement learning is\nthrough the use of macro-actions. Paradoxically though, in many scenarios the\nnaive addition of macro-actions does not lead to better exploration, but rather\nthe opposite. It has been argued that this was caused by adding non-useful\nmacros and multiple works have focused on mechanisms to discover effectively\nenvironment-specific useful macros. In this work, we take a slightly different\nperspective. We argue that the difficulty stems from the trade-offs between\nreducing the average number of decisions per episode versus increasing the size\nof the action space. Namely, one typically treats each potential macro-action\nas independent and atomic, hence strictly increasing the search space and\nmaking typical exploration strategies inefficient. To address this problem we\npropose a novel regularization term that exploits the relationship between\nactions and macro-actions to improve the credit assignment mechanism by\nreducing the effective dimension of the action space and, therefore, improving\nexploration. The term relies on a similarity matrix that is meta-learned\njointly with learning the desired policy. We empirically validate our strategy\nlooking at macro-actions in Atari games, and the StreetFighter II environment.\nOur results show significant improvements over the Rainbow-DQN baseline in all\nenvironments. Additionally, we show that the macro-action similarity is\ntransferable to related environments. We believe this work is a small but\nimportant step towards understanding how the similarity-imposed geometry on the\naction space can be exploited to improve credit assignment and exploration,\ntherefore making learning more effective.",
      "pdf_url": "http://arxiv.org/pdf/2506.13690v1",
      "published": "2025-06-16T16:52:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13690v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ROSA: Harnessing Robot States for Vision-Language and Action Alignment",
      "authors": [
        "Yuqing Wen",
        "Kefan Gu",
        "Haoxuan Liu",
        "Yucheng Zhao",
        "Tiancai Wang",
        "Haoqiang Fan",
        "Xiaoyan Sun"
      ],
      "abstract": "Vision-Language-Action (VLA) models have recently made significant advance in\nmulti-task, end-to-end robotic control, due to the strong generalization\ncapabilities of Vision-Language Models (VLMs). A fundamental challenge in\ndeveloping such models is effectively aligning the vision-language space with\nthe robotic action space. Existing approaches typically rely on directly\nfine-tuning VLMs using expert demonstrations. However, this strategy suffers\nfrom a spatio-temporal gap, resulting in considerable data inefficiency and\nheavy reliance on human labor. Spatially, VLMs operate within a high-level\nsemantic space, whereas robotic actions are grounded in low-level 3D physical\nspace; temporally, VLMs primarily interpret the present, while VLA models\nanticipate future actions. To overcome these challenges, we propose a novel\ntraining paradigm, ROSA, which leverages robot state estimation to improve\nalignment between vision-language and action spaces. By integrating robot state\nestimation data obtained via an automated process, ROSA enables the VLA model\nto gain enhanced spatial understanding and self-awareness, thereby boosting\nperformance and generalization. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of ROSA, particularly in\nlow-data regimes.",
      "pdf_url": "http://arxiv.org/pdf/2506.13679v1",
      "published": "2025-06-16T16:34:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13679v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention",
      "authors": [
        "Haonan Wang",
        "Brian Chen",
        "Siquan Li",
        "Xinhe Liang",
        "Hwee Kuan Lee",
        "Kenji Kawaguchi",
        "Tianyang Hu"
      ],
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation.",
      "pdf_url": "http://arxiv.org/pdf/2506.13674v2",
      "published": "2025-06-16T16:30:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13674v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems",
      "authors": [
        "Junfeng Fang",
        "Zijun Yao",
        "Ruipeng Wang",
        "Haokai Ma",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "abstract": "The development of large language models (LLMs) has entered in a\nexperience-driven era, flagged by the emergence of environment feedback-driven\nlearning via reinforcement learning and tool-using agents. This encourages the\nemergenece of model context protocol (MCP), which defines the standard on how\nshould a LLM interact with external services, such as \\api and data. However,\nas MCP becomes the de facto standard for LLM agent systems, it also introduces\nnew safety risks. In particular, MCP introduces third-party services, which are\nnot controlled by the LLM developers, into the agent systems. These third-party\nMCP services provider are potentially malicious and have the economic\nincentives to exploit vulnerabilities and sabotage user-agent interactions. In\nthis position paper, we advocate the research community in LLM safety to pay\nclose attention to the new safety risks issues introduced by MCP, and develop\nnew techniques to build safe MCP-powered agent systems. To establish our\nposition, we argue with three key parts. (1) We first construct \\framework, a\ncontrolled framework to examine safety issues in MCP-powered agent systems. (2)\nWe then conduct a series of pilot experiments to demonstrate the safety risks\nin MCP-powered agent systems is a real threat and its defense is not trivial.\n(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered\nagent systems. In particular, we would call for researchers to persue the\nfollowing research directions: red teaming, MCP safe LLM development, MCP\nsafety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP\nsafe ecosystem construction. We hope this position paper can raise the\nawareness of the research community in MCP safety and encourage more\nresearchers to join this important research direction. Our code is available at\nhttps://github.com/littlelittlenine/SafeMCP.git.",
      "pdf_url": "http://arxiv.org/pdf/2506.13666v1",
      "published": "2025-06-16T16:24:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13666v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning",
      "authors": [
        "Shulin Tian",
        "Ruiqi Wang",
        "Hongming Guo",
        "Penghao Wu",
        "Yuhao Dong",
        "Xiuying Wang",
        "Jingkang Yang",
        "Hao Zhang",
        "Hongyuan Zhu",
        "Ziwei Liu"
      ],
      "abstract": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,\nin days and weeks) egocentric videos, which leverages a structured\nChain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained\nvia reinforcement learning (RL). Inspired by human problem-solving strategies,\nCoTT decomposes complex reasoning into modular steps, with the RL agent\ninvoking specific tools, one per step, to iteratively and collaboratively\nanswer sub-questions tackling such tasks as temporal retrieval and multi-modal\nunderstanding. We design a two-stage training paradigm involving supervised\nfinetuning (SFT) of a pretrained language model using CoTT data and RL to\nenable our agent to dynamically propose step-by-step tools for long-range\nreasoning. To facilitate training, we construct a dataset called Ego-R1 Data,\nwhich consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our\nEgo-R1 agent is evaluated on a newly curated week-long video QA benchmark,\nEgo-R1 Bench, which contains human-verified QA pairs from hybrid sources.\nExtensive results demonstrate that the dynamic, tool-augmented chain-of-thought\nreasoning by our Ego-R1 Agent can effectively tackle the unique challenges of\nunderstanding ultra-long egocentric videos, significantly extending the time\ncoverage from few hours to a week.",
      "pdf_url": "http://arxiv.org/pdf/2506.13654v1",
      "published": "2025-06-16T16:17:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13654v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model",
      "authors": [
        "Shaolei Zhang",
        "Shoutao Guo",
        "Qingkai Fang",
        "Yan Zhou",
        "Yang Feng"
      ],
      "abstract": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
      "pdf_url": "http://arxiv.org/pdf/2506.13642v1",
      "published": "2025-06-16T16:06:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13642v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models",
      "authors": [
        "Zhiyi Shi",
        "Binjie Wang",
        "Chongjie Si",
        "Yichen Wu",
        "Junsik Kim",
        "Hanspeter Pfister"
      ],
      "abstract": "Model editing aims to efficiently update a pre-trained model's knowledge\nwithout the need for time-consuming full retraining. While existing pioneering\nediting methods achieve promising results, they primarily focus on editing\nsingle-modal language models (LLMs). However, for vision-language models\n(VLMs), which involve multiple modalities, the role and impact of each modality\non editing performance remain largely unexplored. To address this gap, we\nexplore the impact of textual and visual modalities on model editing and find\nthat: (1) textual and visual representations reach peak sensitivity at\ndifferent layers, reflecting their varying importance; and (2) editing both\nmodalities can efficiently update knowledge, but this comes at the cost of\ncompromising the model's original capabilities. Based on our findings, we\npropose DualEdit, an editor that modifies both textual and visual modalities at\ntheir respective key layers. Additionally, we introduce a gating module within\nthe more sensitive textual modality, allowing DualEdit to efficiently update\nnew knowledge while preserving the model's original information. We evaluate\nDualEdit across multiple VLM backbones and benchmark datasets, demonstrating\nits superiority over state-of-the-art VLM editing baselines as well as adapted\nLLM editing methods on different evaluation metrics.",
      "pdf_url": "http://arxiv.org/pdf/2506.13638v1",
      "published": "2025-06-16T16:04:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13638v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Graph-Convolutional-Beta-VAE for Synthetic Abdominal Aorta Aneurysm Generation",
      "authors": [
        "Francesco Fabbri",
        "Martino Andrea Scarpolini",
        "Angelo Iollo",
        "Francesco Viola",
        "Francesco Tudisco"
      ],
      "abstract": "Synthetic data generation plays a crucial role in medical research by\nmitigating privacy concerns and enabling large-scale patient data analysis.\nThis study presents a beta-Variational Autoencoder Graph Convolutional Neural\nNetwork framework for generating synthetic Abdominal Aorta Aneurysms (AAA).\nUsing a small real-world dataset, our approach extracts key anatomical features\nand captures complex statistical relationships within a compact disentangled\nlatent space. To address data limitations, low-impact data augmentation based\non Procrustes analysis was employed, preserving anatomical integrity. The\ngeneration strategies, both deterministic and stochastic, manage to enhance\ndata diversity while ensuring realism. Compared to PCA-based approaches, our\nmodel performs more robustly on unseen data by capturing complex, nonlinear\nanatomical variations. This enables more comprehensive clinical and statistical\nanalyses than the original dataset alone. The resulting synthetic AAA dataset\npreserves patient privacy while providing a scalable foundation for medical\nresearch, device testing, and computational modeling.",
      "pdf_url": "http://arxiv.org/pdf/2506.13628v2",
      "published": "2025-06-16T15:55:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13628v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.TO"
      ]
    },
    {
      "title": "EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning",
      "authors": [
        "Zhiqiang Li",
        "Haiyong Bao",
        "Menghong Guan",
        "Hao Pan",
        "Cheng Huang",
        "Hong-Ning Dai"
      ],
      "abstract": "Despite federated learning (FL)'s potential in collaborative learning, its\nperformance has deteriorated due to the data heterogeneity of distributed\nusers. Recently, clustered federated learning (CFL) has emerged to address this\nchallenge by partitioning users into clusters according to their similarity.\nHowever, CFL faces difficulties in training when users are unwilling to share\ntheir cluster identities due to privacy concerns. To address these issues, we\npresent an innovative Efficient and Robust Secure Aggregation scheme for CFL,\ndubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while\nmaintaining users' cluster identity confidentially. Moreover, it detects\npotential poisonous attacks without compromising individual client gradients by\ndiscarding negatively correlated gradients and aggregating positively\ncorrelated ones using a weighted approach. The server also authenticates\ncorrect gradient encoding by clients. EBS-CFL has high efficiency with\nclient-side overhead O(ml + m^2) for communication and O(m^2l) for computation,\nwhere m is the number of cluster identities, and l is the gradient size. When m\n= 1, EBS-CFL's computational efficiency of client is at least O(log n) times\nbetter than comparison schemes, where n is the number of clients.In addition,\nwe validate the scheme through extensive experiments. Finally, we theoretically\nprove the scheme's security.",
      "pdf_url": "http://arxiv.org/pdf/2506.13612v1",
      "published": "2025-06-16T15:39:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13612v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "A Hybrid Artificial Intelligence Method for Estimating Flicker in Power Systems",
      "authors": [
        "Javad Enayati",
        "Pedram Asef",
        "Alexandre Benoit"
      ],
      "abstract": "This paper introduces a novel hybrid AI method combining H filtering and an\nadaptive linear neuron network for flicker component estimation in power\ndistribution systems.The proposed method leverages the robustness of the H\nfilter to extract the voltage envelope under uncertain and noisy conditions\nfollowed by the use of ADALINE to accurately identify flicker frequencies\nembedded in the envelope.This synergy enables efficient time domain estimation\nwith rapid convergence and noise resilience addressing key limitations of\nexisting frequency domain approaches.Unlike conventional techniques this hybrid\nAI model handles complex power disturbances without prior knowledge of noise\ncharacteristics or extensive training.To validate the method performance we\nconduct simulation studies based on IEC Standard 61000 4 15 supported by\nstatistical analysis Monte Carlo simulations and real world data.Results\ndemonstrate superior accuracy robustness and reduced computational load\ncompared to Fast Fourier Transform and Discrete Wavelet Transform based\nestimators.",
      "pdf_url": "http://arxiv.org/pdf/2506.13611v1",
      "published": "2025-06-16T15:38:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13611v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY",
        "stat.AP"
      ]
    },
    {
      "title": "Avoiding Obfuscation with Prover-Estimator Debate",
      "authors": [
        "Jonah Brown-Cohen",
        "Geoffrey Irving",
        "Georgios Piliouras"
      ],
      "abstract": "Training powerful AI systems to exhibit desired behaviors hinges on the\nability to provide accurate human supervision on increasingly complex tasks. A\npromising approach to this problem is to amplify human judgement by leveraging\nthe power of two competing AIs in a debate about the correct solution to a\ngiven problem. Prior theoretical work has provided a complexity-theoretic\nformalization of AI debate, and posed the problem of designing protocols for AI\ndebate that guarantee the correctness of human judgements for as complex a\nclass of problems as possible. Recursive debates, in which debaters decompose a\ncomplex problem into simpler subproblems, hold promise for growing the class of\nproblems that can be accurately judged in a debate. However, existing protocols\nfor recursive debate run into the obfuscated arguments problem: a dishonest\ndebater can use a computationally efficient strategy that forces an honest\nopponent to solve a computationally intractable problem to win. We mitigate\nthis problem with a new recursive debate protocol that, under certain stability\nassumptions, ensures that an honest debater can win with a strategy requiring\ncomputational efficiency comparable to their opponent.",
      "pdf_url": "http://arxiv.org/pdf/2506.13609v1",
      "published": "2025-06-16T15:37:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13609v1",
      "categories": [
        "cs.AI",
        "cs.CC",
        "cs.DS"
      ]
    },
    {
      "title": "The ASP-based Nurse Scheduling System at the University of Yamanashi Hospital",
      "authors": [
        "Hidetomo Nabeshima",
        "Mutsunori Banbara",
        "Torsten Schaub",
        "Takehide Soh"
      ],
      "abstract": "We present the design principles of a nurse scheduling system built using\nAnswer Set Programming (ASP) and successfully deployed at the University of\nYamanashi Hospital. Nurse scheduling is a complex optimization problem\nrequiring the reconciliation of individual nurse preferences with hospital\nstaffing needs across various wards. This involves balancing hard and soft\nconstraints and the flexibility of interactive adjustments. While extensively\nstudied in academia, real-world nurse scheduling presents unique challenges\nthat go beyond typical benchmark problems and competitions. This paper details\nthe practical application of ASP to address these challenges at the University\nof Yamanashi Hospital, focusing on the insights gained and the advancements in\nASP technology necessary to effectively manage the complexities of real-world\ndeployment.",
      "pdf_url": "http://arxiv.org/pdf/2506.13600v1",
      "published": "2025-06-16T15:25:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13600v1",
      "categories": [
        "cs.AI",
        "68T30"
      ]
    },
    {
      "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation",
      "authors": [
        "Yuwei Du",
        "Jie Feng",
        "Jian Yuan",
        "Yong Li"
      ],
      "abstract": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
      "pdf_url": "http://arxiv.org/pdf/2506.13599v1",
      "published": "2025-06-16T15:24:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13599v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Agent Capability Negotiation and Binding Protocol (ACNBP)",
      "authors": [
        "Ken Huang",
        "Akram Sheriff",
        "Vineeth Sai Narajala",
        "Idan Habler"
      ],
      "abstract": "As multi-agent systems evolve to encompass increasingly diverse and\nspecialized agents, the challenge of enabling effective collaboration between\nheterogeneous agents has become paramount, with traditional agent communication\nprotocols often assuming homogeneous environments or predefined interaction\npatterns that limit their applicability in dynamic, open-world scenarios. This\npaper presents the Agent Capability Negotiation and Binding Protocol (ACNBP), a\nnovel framework designed to facilitate secure, efficient, and verifiable\ninteractions between agents in heterogeneous multi-agent systems through\nintegration with an Agent Name Service (ANS) infrastructure that provides\ncomprehensive discovery, negotiation, and binding mechanisms. The protocol\nintroduces a structured 10-step process encompassing capability discovery,\ncandidate pre-screening and selection, secure negotiation phases, and binding\ncommitment with built-in security measures including digital signatures,\ncapability attestation, and comprehensive threat mitigation strategies, while a\nkey innovation of ACNBP is its protocolExtension mechanism that enables\nbackward-compatible protocol evolution and supports diverse agent architectures\nwhile maintaining security and interoperability. We demonstrate ACNBP's\neffectiveness through a comprehensive security analysis using the MAESTRO\nthreat modeling framework, practical implementation considerations, and a\ndetailed example showcasing the protocol's application in a document\ntranslation scenario, with the protocol addressing critical challenges in agent\nautonomy, capability verification, secure communication, and scalable agent\necosystem management.",
      "pdf_url": "http://arxiv.org/pdf/2506.13590v1",
      "published": "2025-06-16T15:18:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13590v1",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.MA"
      ]
    },
    {
      "title": "From Data-Driven to Purpose-Driven Artificial Intelligence: Systems Thinking for Data-Analytic Automation of Patient Care",
      "authors": [
        "Daniel Anadria",
        "Roel Dobbe",
        "Anastasia Giachanou",
        "Ruurd Kuiper",
        "Richard Bartels",
        "Íñigo Martínez de Rituerto de Troya",
        "Carmen Zürcher",
        "Daniel Oberski"
      ],
      "abstract": "In this work, we reflect on the data-driven modeling paradigm that is gaining\nground in AI-driven automation of patient care. We argue that the repurposing\nof existing real-world patient datasets for machine learning may not always\nrepresent an optimal approach to model development as it could lead to\nundesirable outcomes in patient care. We reflect on the history of data\nanalysis to explain how the data-driven paradigm rose to popularity, and we\nenvision ways in which systems thinking and clinical domain theory could\ncomplement the existing model development approaches in reaching human-centric\noutcomes. We call for a purpose-driven machine learning paradigm that is\ngrounded in clinical theory and the sociotechnical realities of real-world\noperational contexts. We argue that understanding the utility of existing\npatient datasets requires looking in two directions: upstream towards the data\ngeneration, and downstream towards the automation objectives. This\npurpose-driven perspective to AI system development opens up new methodological\nopportunities and holds promise for AI automation of patient care.",
      "pdf_url": "http://arxiv.org/pdf/2506.13584v1",
      "published": "2025-06-16T15:07:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13584v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ]
    },
    {
      "title": "Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes",
      "authors": [
        "Bernhard Hilpert",
        "Muhan Hou",
        "Kim Baraka",
        "Joost Broekens"
      ],
      "abstract": "Reinforcement Learning (RL) agents often exhibit learning behaviors that are\nnot intuitively interpretable by human observers, which can result in\nsuboptimal feedback in collaborative teaching settings. Yet, how humans\nperceive and interpret RL agent's learning behavior is largely unknown. In a\nbottom-up approach with two experiments, this work provides a data-driven\nunderstanding of the factors of human observers' understanding of the agent's\nlearning process. A novel, observation-based paradigm to directly assess human\ninferences about agent learning was developed. In an exploratory interview\nstudy (\\textit{N}=9), we identify four core themes in human interpretations:\nAgent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second\nconfirmatory study (\\textit{N}=34) applied an expanded version of the paradigm\nacross two tasks (navigation/manipulation) and two RL algorithms\n(tabular/function approximation). Analyses of 816 responses confirmed the\nreliability of the paradigm and refined the thematic framework, revealing how\nthese themes evolve over time and interrelate. Our findings provide a\nhuman-centered understanding of how people make sense of agent learning,\noffering actionable insights for designing interpretable RL systems and\nimproving transparency in Human-Robot Interaction.",
      "pdf_url": "http://arxiv.org/pdf/2506.13583v1",
      "published": "2025-06-16T15:04:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13583v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Flexible-length Text Infilling for Discrete Diffusion Models",
      "authors": [
        "Andrew Zhang",
        "Anushka Sivakumar",
        "Chiawei Tang",
        "Chris Thomas"
      ],
      "abstract": "Discrete diffusion models are a new class of text generators that offer\nadvantages such as bidirectional context use, parallelizable generation, and\nflexible prompting compared to autoregressive models. However, a critical\nlimitation of discrete diffusion models is their inability to perform\nflexible-length or flexible-position text infilling without access to\nground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete\n\\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling),\nthe first discrete diffusion model to overcome this challenge. DDOT jointly\ndenoises token values and token positions, employing a novel sample-level\nOptimal Transport (OT) coupling. This coupling preserves relative token\nordering while dynamically adjusting the positions and length of infilled\nsegments, a capability previously missing in text diffusion. Our method is\northogonal to existing discrete text diffusion methods and is compatible with\nvarious pretrained text denoisers. Extensive experiments on text infilling\nbenchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms\nnaive diffusion baselines. Furthermore, DDOT achieves performance on par with\nstate-of-the-art non-autoregressive models and enables significant improvements\nin training efficiency and flexibility.",
      "pdf_url": "http://arxiv.org/pdf/2506.13579v1",
      "published": "2025-06-16T15:02:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13579v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "A Production Scheduling Framework for Reinforcement Learning Under Real-World Constraints",
      "authors": [
        "Jonathan Hoss",
        "Felix Schelling",
        "Noah Klarmann"
      ],
      "abstract": "The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing\nmakespan under deterministic constraints. Real-world production environments\nintroduce additional complexities that cause traditional scheduling approaches\nto be less effective. Reinforcement learning (RL) holds potential in addressing\nthese challenges, as it allows agents to learn adaptive scheduling strategies.\nHowever, there is a lack of a comprehensive, general-purpose frameworks for\neffectively training and evaluating RL agents under real-world constraints. To\naddress this gap, we propose a modular framework that extends classical JSSP\nformulations by incorporating key real-world constraints inherent to the\nshopfloor, including transport logistics, buffer management, machine\nbreakdowns, setup times, and stochastic processing conditions, while also\nsupporting multi-objective optimization. The framework is a customizable\nsolution that offers flexibility in defining problem instances and configuring\nsimulation parameters, enabling adaptation to diverse production scenarios. A\nstandardized interface ensures compatibility with various RL approaches,\nproviding a robust environment for training RL agents and facilitating the\nstandardized comparison of different scheduling methods under dynamic and\nuncertain conditions. We release JobShopLab as an open-source tool for both\nresearch and industrial applications, accessible at:\nhttps://github.com/proto-lab-ro/jobshoplab",
      "pdf_url": "http://arxiv.org/pdf/2506.13566v2",
      "published": "2025-06-16T14:50:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13566v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Understand the Implication: Learning to Think for Pragmatic Understanding",
      "authors": [
        "Settaluri Lakshmi Sravanthi",
        "Kishan Maharaj",
        "Sravani Gunnu",
        "Abhijit Mishra",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "Pragmatics, the ability to infer meaning beyond literal interpretation, is\ncrucial for social cognition and communication. While LLMs have been\nbenchmarked for their pragmatic understanding, improving their performance\nremains underexplored. Existing methods rely on annotated labels but overlook\nthe reasoning process humans naturally use to interpret implicit meaning. To\nbridge this gap, we introduce a novel pragmatic dataset,\nImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both\ncorrect and incorrect interpretations. Through preference-tuning and supervised\nfine-tuning, we demonstrate that thought-based learning significantly enhances\nLLMs' pragmatic understanding, improving accuracy by 11.12% across model\nfamilies. We further discuss a transfer-learning study where we evaluate the\nperformance of thought-based training for the other tasks of pragmatics\n(presupposition, deixis) that are not seen during the training time and observe\nan improvement of 16.10% compared to label-trained models.",
      "pdf_url": "http://arxiv.org/pdf/2506.13559v1",
      "published": "2025-06-16T14:45:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13559v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Seismic Acoustic Impedance Inversion Framework Based on Conditional Latent Generative Diffusion Model",
      "authors": [
        "Jie Chen",
        "Hongling Chen",
        "Jinghuai Gao",
        "Chuangji Meng",
        "Tao Yang",
        "XinXin Liang"
      ],
      "abstract": "Seismic acoustic impedance plays a crucial role in lithological\nidentification and subsurface structure interpretation. However, due to the\ninherently ill-posed nature of the inversion problem, directly estimating\nimpedance from post-stack seismic data remains highly challenging. Recently,\ndiffusion models have shown great potential in addressing such inverse problems\ndue to their strong prior learning and generative capabilities. Nevertheless,\nmost existing methods operate in the pixel domain and require multiple\niterations, limiting their applicability to field data. To alleviate these\nlimitations, we propose a novel seismic acoustic impedance inversion framework\nbased on a conditional latent generative diffusion model, where the inversion\nprocess is made in latent space. To avoid introducing additional training\noverhead when embedding conditional inputs, we design a lightweight\nwavelet-based module into the framework to project seismic data and reuse an\nencoder trained on impedance to embed low-frequency impedance into the latent\nspace. Furthermore, we propose a model-driven sampling strategy during the\ninversion process of this framework to enhance accuracy and reduce the number\nof required diffusion steps. Numerical experiments on a synthetic model\ndemonstrate that the proposed method achieves high inversion accuracy and\nstrong generalization capability within only a few diffusion steps. Moreover,\napplication to field data reveals enhanced geological detail and higher\nconsistency with well-log measurements, validating the effectiveness and\npracticality of the proposed approach.",
      "pdf_url": "http://arxiv.org/pdf/2506.13529v1",
      "published": "2025-06-16T14:19:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13529v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The Price of Freedom: Exploring Expressivity and Runtime Tradeoffs in Equivariant Tensor Products",
      "authors": [
        "YuQing Xie",
        "Ameya Daigavane",
        "Mit Kotak",
        "Tess Smidt"
      ],
      "abstract": "$E(3)$-equivariant neural networks have demonstrated success across a wide\nrange of 3D modelling tasks. A fundamental operation in these networks is the\ntensor product, which interacts two geometric features in an equivariant manner\nto create new features. Due to the high computational complexity of the tensor\nproduct, significant effort has been invested to optimize the runtime of this\noperation. For example, Luo et al. (2024) recently proposed the Gaunt tensor\nproduct (GTP) which promises a significant speedup. In this work, we provide a\ncareful, systematic analysis of a number of tensor product operations. In\nparticular, we emphasize that different tensor products are not performing the\nsame operation. The reported speedups typically come at the cost of\nexpressivity. We introduce measures of expressivity and interactability to\ncharacterize these differences. In addition, we realized the original\nimplementation of GTP can be greatly simplified by directly using a spherical\ngrid at no cost in asymptotic runtime. This spherical grid approach is faster\non our benchmarks and in actual training of the MACE interatomic potential by\n30\\%. Finally, we provide the first systematic microbenchmarks of the various\ntensor product operations. We find that the theoretical runtime guarantees can\ndiffer wildly from empirical performance, demonstrating the need for careful\napplication-specific benchmarking. Code is available at\n\\href{https://github.com/atomicarchitects/PriceofFreedom}{https://github.com/atomicarchitects/PriceofFreedom}",
      "pdf_url": "http://arxiv.org/pdf/2506.13523v1",
      "published": "2025-06-16T14:15:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13523v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data",
      "authors": [
        "Vasiliki Balaska",
        "Ioannis Tsampikos Papapetros",
        "Katerina Maria Oikonomou",
        "Loukas Bampis",
        "Antonios Gasteratos"
      ],
      "abstract": "The mining sector increasingly adopts digital tools to improve operational\nefficiency, safety, and data-driven decision-making. One of the key challenges\nremains the reliable acquisition of high-resolution, geo-referenced spatial\ninformation to support core activities such as extraction planning and on-site\nmonitoring. This work presents an integrated system architecture that combines\nUAV-based sensing, LiDAR terrain modeling, and deep learning-based object\ndetection to generate spatially accurate information for open-pit mining\nenvironments. The proposed pipeline includes geo-referencing, 3D\nreconstruction, and object localization, enabling structured spatial outputs to\nbe integrated into an industrial digital twin platform. Unlike traditional\nstatic surveying methods, the system offers higher coverage and automation\npotential, with modular components suitable for deployment in real-world\nindustrial contexts. While the current implementation operates in post-flight\nbatch mode, it lays the foundation for real-time extensions. The system\ncontributes to the development of AI-enhanced remote sensing in mining by\ndemonstrating a scalable and field-validated geospatial data workflow that\nsupports situational awareness and infrastructure safety.",
      "pdf_url": "http://arxiv.org/pdf/2506.13505v1",
      "published": "2025-06-16T13:59:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13505v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.ET",
        "cs.RO"
      ]
    },
    {
      "title": "Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness",
      "authors": [
        "Mei-Yen Chen",
        "Thi Thu Uyen Hoang",
        "Michael Hahn",
        "M. Saquib Sarfraz"
      ],
      "abstract": "Merging or routing low-rank adapters (LoRAs) has emerged as a popular\nsolution for enhancing large language models, particularly when data access is\nrestricted by regulatory or domain-specific constraints. This position paper\nargues that the research community should shift its focus from developing new\nmerging or routing algorithms to understanding the conditions under which\nreusing LoRAs is truly effective. Through theoretical analysis and synthetic\ntwo-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs\nenables genuine compositional generalization or merely reflects shallow pattern\nmatching. Evaluating two data-agnostic methods--parameter averaging and dynamic\nadapter selection--we found that reusing LoRAs often fails to logically\nintegrate knowledge across disjoint fine-tuning datasets, especially when such\nknowledge is underrepresented during pretraining. Our empirical results,\nsupported by theoretical insights into LoRA's limited expressiveness, highlight\nthe preconditions and constraints of reusing them for unseen tasks and cast\ndoubt on its feasibility as a truly data-free approach. We advocate for pausing\nthe pursuit of novel methods for recycling LoRAs and emphasize the need for\nrigorous mechanisms to guide future academic research in adapter-based model\nmerging and practical system designs for practitioners.",
      "pdf_url": "http://arxiv.org/pdf/2506.13479v1",
      "published": "2025-06-16T13:35:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13479v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ESRPCB: an Edge guided Super-Resolution model and Ensemble learning for tiny Printed Circuit Board Defect detection",
      "authors": [
        "Xiem HoangVan",
        "Dang Bui Dinh",
        "Thanh Nguyen Canh",
        "Van-Truong Nguyen"
      ],
      "abstract": "Printed Circuit Boards (PCBs) are critical components in modern electronics,\nwhich require stringent quality control to ensure proper functionality.\nHowever, the detection of defects in small-scale PCBs images poses significant\nchallenges as a result of the low resolution of the captured images, leading to\npotential confusion between defects and noise. To overcome these challenges,\nthis paper proposes a novel framework, named ESRPCB (edgeguided\nsuper-resolution for PCBs defect detection), which combines edgeguided\nsuper-resolution with ensemble learning to enhance PCBs defect detection. The\nframework leverages the edge information to guide the EDSR (Enhanced Deep\nSuper-Resolution) model with a novel ResCat (Residual Concatenation) structure,\nenabling it to reconstruct high-resolution images from small PCBs inputs. By\nincorporating edge features, the super-resolution process preserves critical\nstructural details, ensuring that tiny defects remain distinguishable in the\nenhanced image. Following this, a multi-modal defect detection model employs\nensemble learning to analyze the super-resolved",
      "pdf_url": "http://arxiv.org/pdf/2506.13476v1",
      "published": "2025-06-16T13:34:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13476v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning",
      "authors": [
        "David Bani-Harouni",
        "Chantal Pellegrini",
        "Ege Özsoy",
        "Matthias Keicher",
        "Nassir Navab"
      ],
      "abstract": "Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2506.13474v1",
      "published": "2025-06-16T13:32:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13474v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models",
      "authors": [
        "Junho Yoon",
        "Geom Lee",
        "Donghyeon Jeon",
        "Inho Kang",
        "Seung-Hoon Na"
      ],
      "abstract": "Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64.",
      "pdf_url": "http://arxiv.org/pdf/2506.13472v2",
      "published": "2025-06-16T13:30:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13472v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Two-stage Optimization Method for Wide-range Single-electron Quantum Magnetic Sensing",
      "authors": [
        "Shiqian Guo",
        "Jianqing Liu",
        "Thinh Le",
        "Huaiyu Dai"
      ],
      "abstract": "Quantum magnetic sensing based on spin systems has emerged as a new paradigm\nfor detecting ultra-weak magnetic fields with unprecedented sensitivity,\nrevitalizing applications in navigation, geo-localization, biology, and beyond.\nAt the heart of quantum magnetic sensing, from the protocol perspective, lies\nthe design of optimal sensing parameters to manifest and then estimate the\nunderlying signals of interest (SoI). Existing studies on this front mainly\nrely on adaptive algorithms based on black-box AI models or formula-driven\nprincipled searches. However, when the SoI spans a wide range and the quantum\nsensor has physical constraints, these methods may fail to converge efficiently\nor optimally, resulting in prolonged interrogation times and reduced sensing\naccuracy. In this work, we report the design of a new protocol using a\ntwo-stage optimization method. In the 1st Stage, a Bayesian neural network with\na fixed set of sensing parameters is used to narrow the range of SoI. In the\n2nd Stage, a federated reinforcement learning agent is designed to fine-tune\nthe sensing parameters within a reduced search space. The proposed protocol is\ndeveloped and evaluated in a challenging context of single-shot readout of an\nNV-center electron spin under a constrained total sensing time budget; and yet\nit achieves significant improvements in both accuracy and resource efficiency\nfor wide-range D.C. magnetic field estimation compared to the state of the art.",
      "pdf_url": "http://arxiv.org/pdf/2506.13469v1",
      "published": "2025-06-16T13:28:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13469v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "An Interdisciplinary Approach to Human-Centered Machine Translation",
      "authors": [
        "Marine Carpuat",
        "Omri Asscher",
        "Kalika Bali",
        "Luisa Bentivogli",
        "Frédéric Blain",
        "Lynne Bowker",
        "Monojit Choudhury",
        "Hal Daumé III",
        "Kevin Duh",
        "Ge Gao",
        "Alvin Grissom II",
        "Marzena Karpinska",
        "Elaine C. Khoong",
        "William D. Lewis",
        "André F. T. Martins",
        "Mary Nurminen",
        "Douglas W. Oard",
        "Maja Popovic",
        "Michel Simard",
        "François Yvon"
      ],
      "abstract": "Machine Translation (MT) tools are widely used today, often in contexts where\nprofessional translators are not present. Despite progress in MT technology, a\ngap persists between system development and real-world usage, particularly for\nnon-expert users who may struggle to assess translation reliability. This paper\nadvocates for a human-centered approach to MT, emphasizing the alignment of\nsystem design with diverse communicative goals and contexts of use. We survey\nthe literature in Translation Studies and Human-Computer Interaction to\nrecontextualize MT evaluation and design to address the diverse real-world\nscenarios in which MT is used today.",
      "pdf_url": "http://arxiv.org/pdf/2506.13468v1",
      "published": "2025-06-16T13:27:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13468v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study",
      "authors": [
        "Zhengyu Hu",
        "Jianxun Lian",
        "Zheyuan Xiao",
        "Seraphina Zhang",
        "Tianfu Wang",
        "Nicholas Jing Yuan",
        "Xing Xie",
        "Hui Xiong"
      ],
      "abstract": "Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models.",
      "pdf_url": "http://arxiv.org/pdf/2506.13464v1",
      "published": "2025-06-16T13:24:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13464v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
      "authors": [
        "Kangye Ji",
        "Yuan Meng",
        "Hanyun Cui",
        "Ye Li",
        "Shengjia Hua",
        "Lei Chen",
        "Zhi Wang"
      ],
      "abstract": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
      "pdf_url": "http://arxiv.org/pdf/2506.13456v1",
      "published": "2025-06-16T13:14:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13456v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Towards a Formal Specification for Self-organized Shape Formation in Swarm Robotics",
      "authors": [
        "YR Darr",
        "MA Niazi"
      ],
      "abstract": "The self-organization of robots for the formation of structures and shapes is\na stimulating application of the swarm robotic system. It involves a large\nnumber of autonomous robots of heterogeneous behavior, coordination among them,\nand their interaction with the dynamic environment. This process of complex\nstructure formation is considered a complex system, which needs to be modeled\nby using any modeling approach. Although the formal specification approach\nalong with other formal methods has been used to model the behavior of robots\nin a swarm. However, to the best of our knowledge, the formal specification\napproach has not been used to model the self-organization process in swarm\nrobotic systems for shape formation. In this paper, we use a formal\nspecification approach to model the shape formation task of swarm robots. We\nuse Z (Zed) language of formal specification, which is a state-based language,\nto model the states of the entities of the systems. We demonstrate the\neffectiveness of Z for the self-organized shape formation. The presented formal\nspecification model gives the outlines for designing and implementing the swarm\nrobotic system for the formation of complex shapes and structures. It also\nprovides the foundation for modeling the complex shape formation process for\nswarm robotics using a multi-agent system in a simulation-based environment.\nKeywords: Swarm robotics, Self-organization, Formal specification, Complex\nsystems",
      "pdf_url": "http://arxiv.org/pdf/2506.13453v1",
      "published": "2025-06-16T13:13:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13453v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A Neural Model for Word Repetition",
      "authors": [
        "Daniel Dager",
        "Robin Sobczyk",
        "Emmanuel Chemla",
        "Yair Lakretz"
      ],
      "abstract": "It takes several years for the developing brain of a baby to fully master\nword repetition-the task of hearing a word and repeating it aloud. Repeating a\nnew word, such as from a new language, can be a challenging task also for\nadults. Additionally, brain damage, such as from a stroke, may lead to\nsystematic speech errors with specific characteristics dependent on the\nlocation of the brain damage. Cognitive sciences suggest a model with various\ncomponents for the different processing stages involved in word repetition.\nWhile some studies have begun to localize the corresponding regions in the\nbrain, the neural mechanisms and how exactly the brain performs word repetition\nremain largely unknown. We propose to bridge the gap between the cognitive\nmodel of word repetition and neural mechanisms in the human brain by modeling\nthe task using deep neural networks. Neural models are fully observable,\nallowing us to study the detailed mechanisms in their various substructures and\nmake comparisons with human behavior and, ultimately, the brain. Here, we make\nfirst steps in this direction by: (1) training a large set of models to\nsimulate the word repetition task; (2) creating a battery of tests to probe the\nmodels for known effects from behavioral studies in humans, and (3) simulating\nbrain damage through ablation studies, where we systematically remove neurons\nfrom the model, and repeat the behavioral study to examine the resulting speech\nerrors in the \"patient\" model. Our results show that neural models can mimic\nseveral effects known from human research, but might diverge in other aspects,\nhighlighting both the potential and the challenges for future research aimed at\ndeveloping human-like neural models.",
      "pdf_url": "http://arxiv.org/pdf/2506.13450v1",
      "published": "2025-06-16T13:09:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13450v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Simple is what you need for efficient and accurate medical image segmentation",
      "authors": [
        "Xiang Yu",
        "Yayan Chen",
        "Guannan He",
        "Qing Zeng",
        "Yue Qin",
        "Meiling Liang",
        "Dandan Luo",
        "Yimei Liao",
        "Zeyu Ren",
        "Cheng Kang",
        "Delong Yang",
        "Bocheng Liang",
        "Bin Pu",
        "Ying Yuan",
        "Shengli Li"
      ],
      "abstract": "While modern segmentation models often prioritize performance over\npracticality, we advocate a design philosophy prioritizing simplicity and\nefficiency, and attempted high performance segmentation model design. This\npaper presents SimpleUNet, a scalable ultra-lightweight medical image\nsegmentation model with three key innovations: (1) A partial feature selection\nmechanism in skip connections for redundancy reduction while enhancing\nsegmentation performance; (2) A fixed-width architecture that prevents\nexponential parameter growth across network stages; (3) An adaptive feature\nfusion module achieving enhanced representation with minimal computational\noverhead. With a record-breaking 16 KB parameter configuration, SimpleUNet\noutperforms LBUNet and other lightweight benchmarks across multiple public\ndatasets. The 0.67 MB variant achieves superior efficiency (8.60 GFLOPs) and\naccuracy, attaining a mean DSC/IoU of 85.76%/75.60% on multi-center breast\nlesion datasets, surpassing both U-Net and TransUNet. Evaluations on skin\nlesion datasets (ISIC 2017/2018: mDice 84.86%/88.77%) and endoscopic polyp\nsegmentation (KVASIR-SEG: 86.46%/76.48% mDice/mIoU) confirm consistent\ndominance over state-of-the-art models. This work demonstrates that extreme\nmodel compression need not compromise performance, providing new insights for\nefficient and accurate medical image segmentation. Codes can be found at\nhttps://github.com/Frankyu5666666/SimpleUNet.",
      "pdf_url": "http://arxiv.org/pdf/2506.13415v1",
      "published": "2025-06-16T12:31:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13415v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "I.4.6"
      ]
    },
    {
      "title": "CALM: Consensus-Aware Localized Merging for Multi-Task Learning",
      "authors": [
        "Kunda Yan",
        "Min Zhang",
        "Sen Cui",
        "Zikun Qu",
        "Bo Jiang",
        "Feng Liu",
        "Changshui Zhang"
      ],
      "abstract": "Model merging aims to integrate the strengths of multiple fine-tuned models\ninto a unified model while preserving task-specific capabilities. Existing\nmethods, represented by task arithmetic, are typically classified into global-\nand local-aware methods. However, global-aware methods inevitably cause\nparameter interference, while local-aware methods struggle to maintain the\neffectiveness of task-specific details in the merged model. To address these\nlimitations, we propose a Consensus-Aware Localized Merging (CALM) method which\nincorporates localized information aligned with global task consensus, ensuring\nits effectiveness post-merging. CALM consists of three key components: (1)\nclass-balanced entropy minimization sampling, providing a more flexible and\nreliable way to leverage unsupervised data; (2) an efficient-aware framework,\nselecting a small set of tasks for sequential merging with high scalability;\n(3) a consensus-aware mask optimization, aligning localized binary masks with\nglobal task consensus and merging them conflict-free. Experiments demonstrate\nthe superiority and robustness of our CALM, significantly outperforming\nexisting methods and achieving performance close to traditional MTL.",
      "pdf_url": "http://arxiv.org/pdf/2506.13406v1",
      "published": "2025-06-16T12:19:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13406v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Technical Study into Small Reasoning Language Models",
      "authors": [
        "Xialie Zhuang",
        "Peixian Ma",
        "Zhikai Jia",
        "Zheng Cao",
        "Shiwei Liu"
      ],
      "abstract": "The ongoing evolution of language models has led to the development of\nlarge-scale architectures that demonstrate exceptional performance across a\nwide range of tasks. However, these models come with significant computational\nand energy demands, as well as potential privacy implications. In this context,\nSmall Reasoning Language Models (SRLMs) with approximately 0.5 billion\nparameters present a compelling alternative due to their remarkable\ncomputational efficiency and cost effectiveness, particularly in\nresource-constrained environments. Despite these advantages, the limited\ncapacity of 0.5 billion parameter models poses challenges in handling complex\ntasks such as mathematical reasoning and code generation. This research\ninvestigates various training strategies, including supervised fine-tuning\n(SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as\ntheir hybrid implementations, to enhance the performance of 0.5B SRLMs. We\nanalyze effective methodologies to bridge the performance gap between SRLMS and\nlarger models and present insights into optimal training pipelines tailored for\nthese smaller architectures. Through extensive experimental validation and\nanalysis, our work aims to provide actionable recommendations for maximizing\nthe reasoning capabilities of 0.5B models.",
      "pdf_url": "http://arxiv.org/pdf/2506.13404v1",
      "published": "2025-06-16T12:18:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.13404v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}