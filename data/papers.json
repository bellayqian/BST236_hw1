{
  "last_updated": "2025-05-27T00:51:29.950585",
  "papers": [
    {
      "title": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions",
      "authors": [
        "Zizhang Li",
        "Hong-Xing Yu",
        "Wei Liu",
        "Yin Yang",
        "Charles Herrmann",
        "Gordon Wetzstein",
        "Jiajun Wu"
      ],
      "abstract": "WonderPlay is a novel framework integrating physics simulation with video\ngeneration for generating action-conditioned dynamic 3D scenes from a single\nimage. While prior works are restricted to rigid body or simple elastic\ndynamics, WonderPlay features a hybrid generative simulator to synthesize a\nwide range of 3D dynamics. The hybrid generative simulator first uses a physics\nsolver to simulate coarse 3D dynamics, which subsequently conditions a video\ngenerator to produce a video with finer, more realistic motion. The generated\nvideo is then used to update the simulated dynamic 3D scene, closing the loop\nbetween the physics solver and the video generator. This approach enables\nintuitive user control to be combined with the accurate dynamics of\nphysics-based simulators and the expressivity of diffusion-based video\ngenerators. Experimental results demonstrate that WonderPlay enables users to\ninteract with various scenes of diverse content, including cloth, sand, snow,\nliquid, smoke, elastic, and rigid bodies -- all using a single image input.\nCode will be made public. Project website:\nhttps://kyleleey.github.io/WonderPlay/",
      "pdf_url": "http://arxiv.org/pdf/2505.18151v1",
      "published": "2025-05-23T17:59:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18151v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find",
      "authors": [
        "Owen Bianchi",
        "Mathew J. Koretsky",
        "Maya Willey",
        "Chelsea X. Alvarado",
        "Tanay Nayak",
        "Adi Asija",
        "Nicole Kuznetsov",
        "Mike A. Nalls",
        "Faraz Faghri",
        "Daniel Khashabi"
      ],
      "abstract": "Large language models (LLMs) face significant challenges with\nneedle-in-a-haystack tasks, where relevant information (\"the needle\") must be\ndrawn from a large pool of irrelevant context (\"the haystack\"). Previous\nstudies have highlighted positional bias and distractor quantity as critical\nfactors affecting model performance, yet the influence of gold context size has\nreceived little attention. We address this gap by systematically studying how\nvariations in gold context length impact LLM performance on long-context\nquestion answering tasks. Our experiments reveal that LLM performance drops\nsharply when the gold context is shorter, i.e., smaller gold contexts\nconsistently degrade model performance and amplify positional sensitivity,\nposing a major challenge for agentic systems that must integrate scattered,\nfine-grained information of varying lengths. This pattern holds across three\ndiverse domains (general knowledge, biomedical reasoning, and mathematical\nreasoning) and seven state-of-the-art LLMs of various sizes and architectures.\nOur work provides clear insights to guide the design of robust, context-aware\nLLM-driven systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.18148v1",
      "published": "2025-05-23T17:57:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18148v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems",
      "authors": [
        "Gordon Dai",
        "Yunze Xiao"
      ],
      "abstract": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.",
      "pdf_url": "http://arxiv.org/pdf/2505.18139v1",
      "published": "2025-05-23T17:48:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18139v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection",
      "authors": [
        "Mykola Trokhymovych",
        "Lydia Pintscher",
        "Ricardo Baeza-Yates",
        "Diego Saez-Trumper"
      ],
      "abstract": "We introduce a next-generation vandalism detection system for Wikidata, one\nof the largest open-source structured knowledge bases on the Web. Wikidata is\nhighly complex: its items incorporate an ever-expanding universe of factual\ntriples and multilingual texts. While edits can alter both structured and\ntextual content, our approach converts all edits into a single space using a\nmethod we call Graph2Text. This allows for evaluating all content changes for\npotential vandalism using a single multilingual language model. This unified\napproach improves coverage and simplifies maintenance. Experiments demonstrate\nthat our solution outperforms the current production system. Additionally, we\nare releasing the code under an open license along with a large dataset of\nvarious human-generated knowledge alterations, enabling further research.",
      "pdf_url": "http://arxiv.org/pdf/2505.18136v1",
      "published": "2025-05-23T17:44:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18136v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Gaming Tool Preferences in Agentic LLMs",
      "authors": [
        "Kazem Faghih",
        "Wenxiao Wang",
        "Yize Cheng",
        "Siddhant Bharti",
        "Gaurang Sriramanan",
        "Sriram Balasubramanian",
        "Parsa Hosseini",
        "Soheil Feizi"
      ],
      "abstract": "Large language models (LLMs) can now access a wide range of external tools,\nthanks to the Model Context Protocol (MCP). This greatly expands their\nabilities as various agents. However, LLMs rely entirely on the text\ndescriptions of tools to decide which ones to use--a process that is\nsurprisingly fragile. In this work, we expose a vulnerability in prevalent\ntool/function-calling protocols by investigating a series of edits to tool\ndescriptions, some of which can drastically increase a tool's usage from LLMs\nwhen competing with alternatives. Through controlled experiments, we show that\ntools with properly edited descriptions receive over 10 times more usage from\nGPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further\nevaluate how various edits to tool descriptions perform when competing directly\nwith one another and how these trends generalize or differ across a broader set\nof 10 different models. These phenomenons, while giving developers a powerful\nway to promote their tools, underscore the need for a more reliable foundation\nfor agentic LLMs to select and utilize tools and resources.",
      "pdf_url": "http://arxiv.org/pdf/2505.18135v1",
      "published": "2025-05-23T17:43:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18135v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "VideoGameBench: Can Vision-Language Models complete popular video games?",
      "authors": [
        "Alex L. Zhang",
        "Thomas L. Griffiths",
        "Karthik R. Narasimhan",
        "Ofir Press"
      ],
      "abstract": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.",
      "pdf_url": "http://arxiv.org/pdf/2505.18134v1",
      "published": "2025-05-23T17:43:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18134v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement",
      "authors": [
        "Jonas A. Actor",
        "Graham Harper",
        "Ben Southworth",
        "Eric C. Cyr"
      ],
      "abstract": "Multilayer perceptrons (MLPs) are a workhorse machine learning architecture,\nused in a variety of modern deep learning frameworks. However, recently\nKolmogorov-Arnold Networks (KANs) have become increasingly popular due to their\nsuccess on a range of problems, particularly for scientific machine learning\ntasks. In this paper, we exploit the relationship between KANs and multichannel\nMLPs to gain structural insight into how to train MLPs faster. We demonstrate\nthe KAN basis (1) provides geometric localized support, and (2) acts as a\npreconditioned descent in the ReLU basis, overall resulting in expedited\ntraining and improved accuracy. Our results show the equivalence between\nfree-knot spline KAN architectures, and a class of MLPs that are refined\ngeometrically along the channel dimension of each weight tensor. We exploit\nthis structural equivalence to define a hierarchical refinement scheme that\ndramatically accelerates training of the multi-channel MLP architecture. We\nshow further accuracy improvements can be had by allowing the $1$D locations of\nthe spline knots to be trained simultaneously with the weights. These advances\nare demonstrated on a range of benchmark examples for regression and scientific\nmachine learning.",
      "pdf_url": "http://arxiv.org/pdf/2505.18131v1",
      "published": "2025-05-23T17:41:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18131v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T99",
        "I.2.6"
      ]
    },
    {
      "title": "Reward Model Overoptimisation in Iterated RLHF",
      "authors": [
        "Lorenz Wolf",
        "Robert Kirk",
        "Mirco Musolesi"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) is a widely used method for\naligning large language models with human preferences. However, RLHF often\nsuffers from reward model overoptimisation, in which models overfit to the\nreward function, resulting in non-generalisable policies that exploit the\nidiosyncrasies and peculiarities of the reward function. A common mitigation is\niterated RLHF, in which reward models are repeatedly retrained with updated\nhuman feedback and policies are re-optimised. Despite its increasing adoption,\nthe dynamics of overoptimisation in this setting remain poorly understood. In\nthis work, we present the first comprehensive study of overoptimisation in\niterated RLHF. We systematically analyse key design choices - how reward model\ntraining data is transferred across iterations, which reward function is used\nfor optimisation, and how policies are initialised. Using the controlled\nAlpacaFarm benchmark, we observe that overoptimisation tends to decrease over\nsuccessive iterations, as reward models increasingly approximate ground-truth\npreferences. However, performance gains diminish over time, and while\nreinitialising from the base policy is robust, it limits optimisation\nflexibility. Other initialisation strategies often fail to recover from early\noveroptimisation. These findings offer actionable insights for building more\nstable and generalisable RLHF pipelines.",
      "pdf_url": "http://arxiv.org/pdf/2505.18126v1",
      "published": "2025-05-23T17:36:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18126v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "ProgRM: Build Better GUI Agents with Progress Rewards",
      "authors": [
        "Danyang Zhang",
        "Situo Zhang",
        "Ziyue Yang",
        "Zichen Zhu",
        "Zihan Zhao",
        "Ruisheng Cao",
        "Lu Chen",
        "Kai Yu"
      ],
      "abstract": "LLM-based (Large Language Model) GUI (Graphical User Interface) agents can\npotentially reshape our daily lives significantly. However, current LLM-based\nGUI agents suffer from the scarcity of high-quality training data owing to the\ndifficulties of trajectory collection and reward annotation. Existing works\nhave been exploring LLMs to collect trajectories for imitation learning or to\noffer reward signals for online RL training. However, the Outcome Reward Model\n(ORM) used in existing works cannot provide finegrained feedback and can\nover-penalize the valuable steps in finally failed trajectories. To this end,\nwe propose Progress Reward Model (ProgRM) to provide dense informative\nintermediate rewards by predicting a task completion progress for each step in\nonline training. To handle the challenge of progress reward label annotation,\nwe further design an efficient LCS-based (Longest Common Subsequence)\nself-annotation algorithm to discover the key steps in trajectories and assign\nprogress labels accordingly. ProgRM is evaluated with extensive experiments and\nanalyses. Actors trained with ProgRM outperform leading proprietary LLMs and\nORM-trained actors, illustrating the effectiveness of ProgRM. The codes for\nexperiments will be made publicly available upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2505.18121v1",
      "published": "2025-05-23T17:23:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18121v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models",
      "authors": [
        "Jiongran Wu",
        "Jiahao Liu",
        "Dongsheng Li",
        "Guangping Zhang",
        "Mingzhe Han",
        "Hansu Gu",
        "Peng Zhang",
        "Li Shang",
        "Tun Lu",
        "Ning Gu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional performance in\nunderstanding and generating semantic patterns, making them promising\ncandidates for sequential recommendation tasks. However, when combined with\nconventional recommendation models (CRMs), LLMs often face challenges related\nto high inference costs and static knowledge transfer methods. In this paper,\nwe propose a novel mutual distillation framework, LLMD4Rec, that fosters\ndynamic and bidirectional knowledge exchange between LLM-centric and CRM-based\nrecommendation systems. Unlike traditional unidirectional distillation methods,\nLLMD4Rec enables iterative optimization by alternately refining both models,\nenhancing the semantic understanding of CRMs and enriching LLMs with\ncollaborative signals from user-item interactions. By leveraging sample-wise\nadaptive weighting and aligning output distributions, our approach eliminates\nthe need for additional parameters while ensuring effective knowledge transfer.\nExtensive experiments on real-world datasets demonstrate that LLMD4Rec\nsignificantly improves recommendation accuracy across multiple benchmarks\nwithout increasing inference costs. This method provides a scalable and\nefficient solution for combining the strengths of both LLMs and CRMs in\nsequential recommendation systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.18120v1",
      "published": "2025-05-23T17:21:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18120v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?",
      "authors": [
        "Takashi Ishida",
        "Thanawat Lodkaew",
        "Ikko Yamane"
      ],
      "abstract": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.",
      "pdf_url": "http://arxiv.org/pdf/2505.18102v1",
      "published": "2025-05-23T16:57:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18102v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ME"
      ]
    },
    {
      "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL",
      "authors": [
        "Joey Hong",
        "Anca Dragan",
        "Sergey Levine"
      ],
      "abstract": "Large language models (LLMs) excel in tasks like question answering and\ndialogue, but complex tasks requiring interaction, such as negotiation and\npersuasion, require additional long-horizon reasoning and planning.\nReinforcement learning (RL) fine-tuning can enable such planning in principle,\nbut suffers from drawbacks that hinder scalability. In particular, multi-turn\nRL training incurs high memory and computational costs, which are exacerbated\nwhen training LLMs as policies. Furthermore, the largest LLMs do not expose the\nAPIs necessary to be trained in such manner. As a result, modern methods to\nimprove the reasoning of LLMs rely on sophisticated prompting mechanisms rather\nthan RL fine-tuning. To remedy this, we propose a novel approach that uses\ngoal-conditioned value functions to guide the reasoning of LLM agents, that\nscales even to large API-based models. These value functions predict how a task\nwill unfold given an action, allowing the LLM agent to evaluate multiple\npossible outcomes, both positive and negative, to plan effectively. In\naddition, these value functions are trained over reasoning steps rather than\nfull actions, to be a concise and light-weight module that facilitates\ndecision-making in multi-turn interactions. We validate our method on tasks\nrequiring interaction, including tool use, social deduction, and dialogue,\ndemonstrating superior performance over both RL fine-tuning and prompting\nmethods while maintaining efficiency and scalability.",
      "pdf_url": "http://arxiv.org/pdf/2505.18098v1",
      "published": "2025-05-23T16:51:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18098v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition",
      "authors": [
        "Xinran Gu",
        "Kaifeng Lyu",
        "Jiazheng Li",
        "Jingzhao Zhang"
      ],
      "abstract": "Large Language Models (LLMs) are typically trained on data mixtures: most\ndata come from web scrapes, while a small portion is curated from high-quality\nsources with dense domain-specific knowledge. In this paper, we show that when\ntraining LLMs on such data mixtures, knowledge acquisition from knowledge-dense\ndatasets, unlike training exclusively on knowledge-dense data\n(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit\nphase transitions with respect to the mixing ratio and model size. Through\ncontrolled experiments on a synthetic biography dataset mixed with web-scraped\ndata, we demonstrate that: (1) as we increase the model size to a critical\nvalue, the model suddenly transitions from memorizing very few to most of the\nbiographies; (2) below a critical mixing ratio, the model memorizes almost\nnothing even with extensive training, but beyond this threshold, it rapidly\nmemorizes more biographies. We attribute these phase transitions to a capacity\nallocation phenomenon: a model with bounded capacity must act like a knapsack\nproblem solver to minimize the overall test loss, and the optimal allocation\nacross datasets can change discontinuously as the model size or mixing ratio\nvaries. We formalize this intuition in an information-theoretic framework and\nreveal that these phase transitions are predictable, with the critical mixing\nratio following a power-law relationship with the model size. Our findings\nhighlight a concrete case where a good mixing recipe for large models may not\nbe optimal for small models, and vice versa.",
      "pdf_url": "http://arxiv.org/pdf/2505.18091v1",
      "published": "2025-05-23T16:46:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18091v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays",
      "authors": [
        "Hyungyung Lee",
        "Geon Choi",
        "Jung-Oh Lee",
        "Hangyul Yoon",
        "Hyuk Gi Hong",
        "Edward Choi"
      ],
      "abstract": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench",
      "pdf_url": "http://arxiv.org/pdf/2505.18087v1",
      "published": "2025-05-23T16:44:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18087v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Stable Reinforcement Learning for Efficient Reasoning",
      "authors": [
        "Muzhi Dai",
        "Shixuan Liu",
        "Qingyi Si"
      ],
      "abstract": "The success of Deepseek-R1 has drawn the LLM community's attention to\nreinforcement learning (RL) methods like GRPO. However, such rule-based 0/1\noutcome reward methods lack the capability to regulate the intermediate\nreasoning processes during chain-of-thought (CoT) generation, leading to severe\noverthinking phenomena. In response, recent studies have designed reward\nfunctions to reinforce models' behaviors in producing shorter yet correct\ncompletions. Nevertheless, we observe that these length-penalty reward\nfunctions exacerbate RL training instability: as the completion length\ndecreases, model accuracy abruptly collapses, often occurring early in\ntraining. To address this issue, we propose a simple yet effective solution\nGRPO-$\\lambda$, an efficient and stabilized variant of GRPO, which dynamically\nadjusts the reward strategy by monitoring the correctness ratio among\ncompletions within each query-sampled group. A low correctness ratio indicates\nthe need to avoid length penalty that compromises CoT quality, triggering a\nswitch to length-agnostic 0/1 rewards that prioritize reasoning capability. A\nhigh ratio maintains length penalties to boost efficiency. Experimental results\nshow that our approach avoids training instability caused by length penalty\nwhile maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,\nGPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average\naccuracy by 1.48% while reducing CoT sequence length by 47.3%.",
      "pdf_url": "http://arxiv.org/pdf/2505.18086v1",
      "published": "2025-05-23T16:43:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18086v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Backpropagation-Free Metropolis-Adjusted Langevin Algorithm",
      "authors": [
        "Adam D. Cobb",
        "Susmit Jha"
      ],
      "abstract": "Recent work on backpropagation-free learning has shown that it is possible to\nuse forward-mode automatic differentiation (AD) to perform optimization on\ndifferentiable models. Forward-mode AD requires sampling a tangent vector for\neach forward pass of a model. The result is the model evaluation with the\ndirectional derivative along the tangent. In this paper, we illustrate how the\nsampling of this tangent vector can be incorporated into the proposal mechanism\nfor the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the\nfirst to introduce a backpropagation-free gradient-based Markov chain Monte\nCarlo (MCMC) algorithm. We also extend to a novel backpropagation-free\nposition-specific preconditioned forward-mode MALA that leverages Hessian\ninformation. Overall, we propose four new algorithms: Forward MALA; Line\nForward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward\nMALA. We highlight the reduced computational cost of the forward-mode samplers\nand show that forward-mode is competitive with the original MALA, while even\noutperforming it depending on the probabilistic model. We include Bayesian\ninference results on a range of probabilistic models, including hierarchical\ndistributions and Bayesian neural networks.",
      "pdf_url": "http://arxiv.org/pdf/2505.18081v1",
      "published": "2025-05-23T16:39:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18081v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction",
      "authors": [
        "Chunlin Gong",
        "Yin Wang",
        "Jingru Li",
        "Hanleran Zhang"
      ],
      "abstract": "This paper presents AFD-STA Net, a neural framework integrating adaptive\nfiltering and spatiotemporal dynamics learning for predicting high-dimensional\nchaotic systems governed by partial differential equations. The architecture\ncombines: 1) An adaptive exponential smoothing module with position-aware decay\ncoefficients for robust attractor reconstruction, 2) Parallel attention\nmechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated\nfusion of multiscale features, and 4) Deep projection networks with\ndimension-scaling capabilities. Numerical experiments on nonlinear PDE systems\ndemonstrate the model's effectiveness in maintaining prediction accuracy under\nboth smooth and strongly chaotic regimes while exhibiting noise tolerance\nthrough adaptive filtering. Component ablation studies confirm critical\ncontributions from each module, particularly highlighting the essential role of\nspatiotemporal attention in learning complex dynamical interactions. The\nframework shows promising potential for real-world applications requiring\nsimultaneous handling of measurement uncertainties and high-dimensional\nnonlinear dynamics.",
      "pdf_url": "http://arxiv.org/pdf/2505.18080v1",
      "published": "2025-05-23T16:39:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18080v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding",
      "authors": [
        "Xiaoyi Zhang",
        "Zhaoyang Jia",
        "Zongyu Guo",
        "Jiahao Li",
        "Bin Li",
        "Houqiang Li",
        "Yan Lu"
      ],
      "abstract": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.",
      "pdf_url": "http://arxiv.org/pdf/2505.18079v1",
      "published": "2025-05-23T16:37:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18079v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals",
      "authors": [
        "Jia-Nan Li",
        "Jian Guan",
        "Wei Wu",
        "Rui Yan"
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning\\textemdash the ability to\nderive general rules from incomplete evidence, remains underexplored. This\npaper investigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose \\textsc{AlignXplore}, a model that leverages extended reasoning chains\nto enable systematic preference inference from behavioral signals in users'\ninteraction histories. We develop \\textsc{AlignXplore} by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that \\textsc{AlignXplore}\nachieves substantial improvements over the backbone model by an average of\n11.05\\% on in-domain and out-of-domain benchmarks, while maintaining strong\ngeneralization ability across different input formats and downstream models.\nFurther analyses establish best practices for preference inference learning\nthrough systematic comparison of reward modeling strategies, while revealing\nthe emergence of human-like inductive reasoning patterns during training.",
      "pdf_url": "http://arxiv.org/pdf/2505.18071v1",
      "published": "2025-05-23T16:16:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18071v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making",
      "authors": [
        "Min Hun Lee",
        "Martyn Zhe Yu Tok"
      ],
      "abstract": "Despite the growing promise of artificial intelligence (AI) in supporting\ndecision-making across domains, fostering appropriate human reliance on AI\nremains a critical challenge. In this paper, we investigate the utility of\nexploring distance-based uncertainty scores for task delegation to AI and\ndescribe how these scores can be visualized through embedding representations\nfor human-AI decision-making. After developing an AI-based system for physical\nstroke rehabilitation assessment, we conducted a study with 19 health\nprofessionals and 10 students in medicine/health to understand the effect of\nexploring distance-based uncertainty scores on users' reliance on AI. Our\nfindings showed that distance-based uncertainty scores outperformed traditional\nprobability-based uncertainty scores in identifying uncertain cases. In\naddition, after exploring confidence scores for task delegation and reviewing\nembedding-based visualizations of distance-based uncertainty scores,\nparticipants achieved an 8.20% higher rate of correct decisions, a 7.15% higher\nrate of changing their decisions to correct ones, and a 7.14% lower rate of\nincorrect changes after reviewing AI outputs than those reviewing\nprobability-based uncertainty scores ($p<0.01$). Our findings highlight the\npotential of distance-based uncertainty scores to enhance decision accuracy and\nappropriate reliance on AI while discussing ongoing challenges for human-AI\ncollaborative decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2505.18066v1",
      "published": "2025-05-23T16:12:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18066v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation",
      "authors": [
        "Zherui Zhang",
        "Jiaxin Wu",
        "Changwei Wang",
        "Rongtao Xu",
        "Longzhao Huang",
        "Wenhao Xu",
        "Wenbo Xu",
        "Li Guo",
        "Shibiao Xu"
      ],
      "abstract": "Prompt learning as a parameter-efficient method that has been widely adopted\nto adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt\ndesign requires domain expertise and iterative optimization, soft-prompt\nmethods rely heavily on task-specific hard labels, limiting their\ngeneralization to unseen categories. Recent popular distillation-based prompt\nlearning methods improve generalization by exploiting larger teacher VLMs and\nunsupervised knowledge transfer, yet their repetitive teacher model online\ninference sacrifices the inherent training efficiency advantage of prompt\nlearning. In this paper, we propose {{\\large {\\textbf{F}}}}aster {{\\large\n{\\textbf{D}}}}istillation-{{\\large {\\textbf{B}}}}ased {{\\large\n{\\textbf{P}}}}rompt {{\\large {\\textbf{L}}}}earning (\\textbf{FDBPL}), which\naddresses these issues by sharing soft supervision contexts across multiple\ntraining stages and implementing accelerated I/O. Furthermore, FDBPL introduces\na region-aware prompt learning paradigm with dual positive-negative prompt\nspaces to fully exploit randomly cropped regions that containing multi-level\ninformation. We propose a positive-negative space mutual learning mechanism\nbased on similarity-difference learning, enabling student CLIP models to\nrecognize correct semantics while learning to reject weakly related concepts,\nthereby improving zero-shot performance. Unlike existing distillation-based\nprompt learning methods that sacrifice parameter efficiency for generalization,\nFDBPL maintains dual advantages of parameter efficiency and strong downstream\ngeneralization. Comprehensive evaluations across 11 datasets demonstrate\nsuperior performance in base-to-new generalization, cross-dataset transfer, and\nrobustness tests, achieving $2.2\\times$ faster training speed.",
      "pdf_url": "http://arxiv.org/pdf/2505.18053v1",
      "published": "2025-05-23T15:57:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18053v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration",
      "authors": [
        "Sudarshan Rajagopalan",
        "Kartik Narayan",
        "Vishal M. Patel"
      ],
      "abstract": "The use of latent diffusion models (LDMs) such as Stable Diffusion has\nsignificantly improved the perceptual quality of All-in-One image Restoration\n(AiOR) methods, while also enhancing their generalization capabilities.\nHowever, these LDM-based frameworks suffer from slow inference due to their\niterative denoising process, rendering them impractical for time-sensitive\napplications. To address this, we propose RestoreVAR, a novel generative\napproach for AiOR that significantly outperforms LDM-based models in\nrestoration performance while achieving over $\\mathbf{10\\times}$ faster\ninference. RestoreVAR leverages visual autoregressive modeling (VAR), a\nrecently introduced approach which performs scale-space autoregression for\nimage generation. VAR achieves comparable performance to that of\nstate-of-the-art diffusion transformers with drastically reduced computational\ncosts. To optimally exploit these advantages of VAR for AiOR, we propose\narchitectural modifications and improvements, including intricately designed\ncross-attention mechanisms and a latent-space refinement module, tailored for\nthe AiOR task. Extensive experiments show that RestoreVAR achieves\nstate-of-the-art performance among generative AiOR methods, while also\nexhibiting strong generalization capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2505.18047v1",
      "published": "2025-05-23T15:52:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18047v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
      "authors": [
        "Zhishuai Liu",
        "Pan Xu"
      ],
      "abstract": "Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting.",
      "pdf_url": "http://arxiv.org/pdf/2505.18044v1",
      "published": "2025-05-23T15:48:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18044v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ]
    },
    {
      "title": "Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks",
      "authors": [
        "Wentao Sun",
        "Joao Paulo Nogueira",
        "Alonso Silva"
      ],
      "abstract": "Despite remarkable advances in the field, LLMs remain unreliable in\ndistinguishing causation from correlation. Recent results from the Corr2Cause\ndataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:\n29.08) -- only marginally outperform random baselines (Random Uniform, F1\nscore: 20.38), indicating limited capacity of generalization. To tackle this\nlimitation, we propose a novel structured approach: rather than directly\nanswering causal queries, we provide the model with the capability to structure\nits thinking by guiding the model to build a structured knowledge graph,\nsystematically encoding the provided correlational premises, to answer the\ncausal queries. This intermediate representation significantly enhances the\nmodel's causal capabilities. Experiments on the test subset of the Corr2Cause\ndataset benchmark with Qwen3-32B model (reasoning model) show substantial gains\nover standard direct prompting methods, improving F1 scores from 32.71 to 48.26\n(over 47.5% relative increase), along with notable improvements in precision\nand recall. These results underscore the effectiveness of providing the model\nwith the capability to structure its thinking and highlight its promising\npotential for broader generalization across diverse causal inference tasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.18034v1",
      "published": "2025-05-23T15:37:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18034v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons",
      "authors": [
        "Hazhar Rahmani",
        "Jie Fu"
      ],
      "abstract": "Many preference elicitation algorithms consider preference over propositional\nlogic formulas or items with different attributes. In sequential decision\nmaking, a user's preference can be a preorder over possible outcomes, each of\nwhich is a temporal sequence of events. This paper considers a class of\npreference inference problems where the user's unknown preference is\nrepresented by a preorder over regular languages (sets of temporal sequences),\nreferred to as temporal goals. Given a finite set of pairwise comparisons\nbetween finite words, the objective is to learn both the set of temporal goals\nand the preorder over these goals. We first show that a preference relation\nover temporal goals can be modeled by a Preference Deterministic Finite\nAutomaton (PDFA), which is a deterministic finite automaton augmented with a\npreorder over acceptance conditions. The problem of preference inference\nreduces to learning the PDFA. This problem is shown to be computationally\nchallenging, with the problem of determining whether there exists a PDFA of\nsize smaller than a given integer $k$, consistent with the sample, being\nNP-Complete. We formalize the properties of characteristic samples and develop\nan algorithm that guarantees to learn, given a characteristic sample, the\nminimal PDFA equivalent to the true PDFA from which the sample is drawn. We\npresent the method through a running example and provide detailed analysis\nusing a robotic motion planning problem.",
      "pdf_url": "http://arxiv.org/pdf/2505.18030v1",
      "published": "2025-05-23T15:35:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18030v1",
      "categories": [
        "cs.AI",
        "cs.FL",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Knot So Simple: A Minimalistic Environment for Spatial Reasoning",
      "authors": [
        "Zizhao Chen",
        "Yoav Artzi"
      ],
      "abstract": "We propose KnotGym, an interactive environment for complex, spatial reasoning\nand manipulation. KnotGym includes goal-oriented rope manipulation tasks with\nvarying levels of complexity, all requiring acting from pure image\nobservations. Tasks are defined along a clear and quantifiable axis of\ncomplexity based on the number of knot crossings, creating a natural\ngeneralization test. KnotGym has a simple observation space, allowing for\nscalable development, yet it highlights core challenges in integrating acute\nperception, spatial reasoning, and grounded manipulation. We evaluate methods\nof different classes, including model-based RL, model-predictive control, and\nchain-of-thought reasoning, and illustrate the challenges KnotGym presents.\nKnotGym is available at https://github.com/lil-lab/knotgym.",
      "pdf_url": "http://arxiv.org/pdf/2505.18028v1",
      "published": "2025-05-23T15:34:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18028v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System",
      "authors": [
        "Rashmi Gupta",
        "Aditya K Gupta",
        "Aarav Jain",
        "Avinash C Pandey",
        "Atul Gupta"
      ],
      "abstract": "Like any other discipline, Large Language Models (LLMs) have significantly\nimpacted software engineering by helping developers generate the required\nartifacts across various phases of software development. This paper presents a\ncase study comparing the performance of popular LLMs GPT, Claude, Gemini, and\nDeepSeek in generating functional specifications that include use cases,\nbusiness rules, and collaborative workflows for a web application, the Mess\nManagement System. The study evaluated the quality of LLM generated use cases,\nbusiness rules, and collaborative workflows in terms of their syntactic and\nsemantic correctness, consistency, non ambiguity, and completeness compared to\nthe reference specifications against the zero-shot prompted problem statement.\nOur results suggested that all four LLMs can specify syntactically and\nsemantically correct, mostly non-ambiguous artifacts. Still, they may be\ninconsistent at times and may differ significantly in the completeness of the\ngenerated specification. Claude and Gemini generated all the reference use\ncases, with Claude achieving the most complete but somewhat redundant use case\nspecifications. Similar results were obtained for specifying workflows.\nHowever, all four LLMs struggled to generate relevant Business Rules, with\nDeepSeek generating the most reference rules but with less completeness.\nOverall, Claude generated more complete specification artifacts, while Gemini\nwas more precise in the specifications it generated.",
      "pdf_url": "http://arxiv.org/pdf/2505.18019v1",
      "published": "2025-05-23T15:25:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18019v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "ExoGait-MS: Learning Periodic Dynamics with Multi-Scale Graph Network for Exoskeleton Gait Recognition",
      "authors": [
        "Lijiang Liu",
        "Junyu Shi",
        "Yong Sun",
        "Zhiyuan Zhang",
        "Jinni Zhou",
        "Shugen Ma",
        "Qiang Nie"
      ],
      "abstract": "Current exoskeleton control methods often face challenges in delivering\npersonalized treatment. Standardized walking gaits can lead to patient\ndiscomfort or even injury. Therefore, personalized gait is essential for the\neffectiveness of exoskeleton robots, as it directly impacts their adaptability,\ncomfort, and rehabilitation outcomes for individual users. To enable\npersonalized treatment in exoskeleton-assisted therapy and related\napplications, accurate recognition of personal gait is crucial for implementing\ntailored gait control. The key challenge in gait recognition lies in\neffectively capturing individual differences in subtle gait features caused by\njoint synergy, such as step frequency and step length. To tackle this issue, we\npropose a novel approach, which uses Multi-Scale Global Dense Graph\nConvolutional Networks (GCN) in the spatial domain to identify latent joint\nsynergy patterns. Moreover, we propose a Gait Non-linear Periodic Dynamics\nLearning module to effectively capture the periodic characteristics of gait in\nthe temporal domain. To support our individual gait recognition task, we have\nconstructed a comprehensive gait dataset that ensures both completeness and\nreliability. Our experimental results demonstrate that our method achieves an\nimpressive accuracy of 94.34% on this dataset, surpassing the current\nstate-of-the-art (SOTA) by 3.77%. This advancement underscores the potential of\nour approach to enhance personalized gait control in exoskeleton-assisted\ntherapy.",
      "pdf_url": "http://arxiv.org/pdf/2505.18018v1",
      "published": "2025-05-23T15:24:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18018v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Training with Pseudo-Code for Instruction Following",
      "authors": [
        "Prince Kumar",
        "Rudra Murthy",
        "Riyaz Bhat",
        "Danish Contractor"
      ],
      "abstract": "Despite the rapid progress in the capabilities of Large Language Models\n(LLMs), they continue to have difficulty following relatively simple,\nunambiguous instructions, especially when compositions are involved. In this\npaper, we take inspiration from recent work that suggests that models may\nfollow instructions better when they are expressed in pseudo-code. However,\nwriting pseudo-code programs can be tedious and using few-shot demonstrations\nto craft code representations for use in inference can be unnatural for\nnon-expert users of LLMs. To overcome these limitations, we propose fine-tuning\nLLMs with instruction-tuning data that additionally includes instructions\nre-expressed in pseudo-code along with the final response. We evaluate models\ntrained using our method on $11$ publicly available benchmarks comprising of\ntasks related to instruction-following, mathematics, and common-sense\nreasoning. We conduct rigorous experiments with $5$ different models and find\nthat not only do models follow instructions better when trained with\npseudo-code, they also retain their capabilities on the other tasks related to\nmathematical and common sense reasoning. Specifically, we observe a relative\ngain of $3$--$19$% on instruction-following benchmark, and an average gain of\nupto 14% across all tasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.18011v1",
      "published": "2025-05-23T15:14:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18011v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "AI Literacy for Legal AI Systems: A practical approach",
      "authors": [
        "Gizem Gultekin-Varkonyi"
      ],
      "abstract": "Legal AI systems are increasingly being adopted by judicial and legal system\ndeployers and providers worldwide to support a range of applications. While\nthey offer potential benefits such as reducing bias, increasing efficiency, and\nimproving accountability, they also pose significant risks, requiring a careful\nbalance between opportunities, and legal and ethical development and\ndeployment. AI literacy, as a legal requirement under the EU AI Act and a\ncritical enabler of ethical AI for deployers and providers, could be a tool to\nachieve this. The article introduces the term \"legal AI systems\" and then\nanalyzes the concept of AI literacy and the benefits and risks associated with\nthese systems. This analysis is linked to a broader AI-L concept for\norganizations that deal with legal AI systems. The outcome of the article, a\nroadmap questionnaire as a practical tool for developers and providers to\nassess risks, benefits, and stakeholder concerns, could be useful in meeting\nsocietal and regulatory expectations for legal AI.",
      "pdf_url": "http://arxiv.org/pdf/2505.18006v1",
      "published": "2025-05-23T15:10:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18006v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ]
    },
    {
      "title": "An Example Safety Case for Safeguards Against Misuse",
      "authors": [
        "Joshua Clymer",
        "Jonah Weinbaum",
        "Robert Kirk",
        "Kimberly Mai",
        "Selena Zhang",
        "Xander Davies"
      ],
      "abstract": "Existing evaluations of AI misuse safeguards provide a patchwork of evidence\nthat is often difficult to connect to real-world decisions. To bridge this gap,\nwe describe an end-to-end argument (a \"safety case\") that misuse safeguards\nreduce the risk posed by an AI assistant to low levels. We first describe how a\nhypothetical developer red teams safeguards, estimating the effort required to\nevade them. Then, the developer plugs this estimate into a quantitative \"uplift\nmodel\" to determine how much barriers introduced by safeguards dissuade misuse\n(https://www.aimisusemodel.com/). This procedure provides a continuous signal\nof risk during deployment that helps the developer rapidly respond to emerging\nthreats. Finally, we describe how to tie these components together into a\nsimple safety case. Our work provides one concrete path -- though not the only\npath -- to rigorously justifying AI misuse risks are low.",
      "pdf_url": "http://arxiv.org/pdf/2505.18003v1",
      "published": "2025-05-23T15:06:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.18003v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Outcome-based Reinforcement Learning to Predict the Future",
      "authors": [
        "Benjamin Turtel",
        "Danny Franklin",
        "Kris Skotheim",
        "Luke Hewitt",
        "Philipp Schoenegger"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has boosted math and\ncoding in large language models, yet there has been little effort to extend\nRLVR into messier, real-world domains like forecasting. One sticking point is\nthat outcome-based reinforcement learning for forecasting must learn from\nbinary, delayed, and noisy rewards, a regime where standard fine-tuning is\nbrittle. We show that outcome-only online RL on a 14B model can match\nfrontier-scale accuracy and surpass it in calibration and hypothetical\nprediction market betting by adapting two leading algorithms, Group-Relative\nPolicy Optimisation (GRPO) and ReMax, to the forecasting setting. Our\nadaptations remove per-question variance scaling in GRPO, apply\nbaseline-subtracted advantages in ReMax, hydrate training with 100k temporally\nconsistent synthetic questions, and introduce lightweight guard-rails that\npenalise gibberish, non-English responses and missing rationales, enabling a\nsingle stable pass over 110k events. Scaling ReMax to 110k questions and\nensembling seven predictions yields a 14B model that matches frontier baseline\no1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in\ncalibration (ECE = 0.042, p < 0.001). A simple trading rule turns this\ncalibration edge into \\$127 of hypothetical profit versus \\$92 for o1 (p =\n0.037). This demonstrates that refined RLVR methods can convert small-scale\nLLMs into potentially economically valuable forecasting tools, with\nimplications for scaling this to larger models.",
      "pdf_url": "http://arxiv.org/pdf/2505.17989v1",
      "published": "2025-05-23T14:56:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17989v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning",
      "authors": [
        "Yutong Chen",
        "Jiandong Gao",
        "Ji Wu"
      ],
      "abstract": "R1-style Reinforcement Learning (RL) significantly enhances Large Language\nModels' reasoning capabilities, yet the mechanism behind rule-based RL remains\nunclear. We found that small-scale SFT has significant influence on RL but\nshows poor efficiency. To explain our observations, we propose an analytical\nframework and compare the efficiency of SFT and RL by measuring sample effect.\nHypothetical analysis show that SFT efficiency is limited by training data.\nGuided by our analysis, we propose Re-distillation, a technique that fine-tunes\npretrain model through small-scale distillation from the RL-trained policy.\nExperiments on Knight & Knave and MATH datasets demonstrate re-distillation's\nsurprising efficiency: re-distilled models match RL performance with far fewer\nsamples and less computation. Empirical verification shows that sample effect\nis a good indicator of performance improvements. As a result, on K&K dataset,\nour re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT\nsamples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches\nits instruct-tuned variant without RL. Our work explains several interesting\nphenomena in R1-style RL, shedding light on the mechanisms behind its empirical\nsuccess. Code is available at: https://github.com/on1262/deep-reasoning",
      "pdf_url": "http://arxiv.org/pdf/2505.17988v1",
      "published": "2025-05-23T14:55:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17988v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling",
      "authors": [
        "Weihang You",
        "Hanqi Jiang",
        "Zishuai Liu",
        "Zihang Xie",
        "Tianming Liu",
        "Jin Lu",
        "Fei Dou"
      ],
      "abstract": "Real world collection of Activities of Daily Living data is challenging due\nto privacy concerns, costly deployment and labeling, and the inherent sparsity\nand imbalance of human behavior. We present ADLGen, a generative framework\nspecifically designed to synthesize realistic, event triggered, and symbolic\nsensor sequences for ambient assistive environments. ADLGen integrates a\ndecoder only Transformer with sign based symbolic temporal encoding, and a\ncontext and layout aware sampling mechanism to guide generation toward\nsemantically rich and physically plausible sensor event sequences. To enhance\nsemantic fidelity and correct structural inconsistencies, we further\nincorporate a large language model into an automatic generate evaluate refine\nloop, which verifies logical, behavioral, and temporal coherence and generates\ncorrection rules without manual intervention or environment specific tuning.\nThrough comprehensive experiments with novel evaluation metrics, ADLGen is\nshown to outperform baseline generators in statistical fidelity, semantic\nrichness, and downstream activity recognition, offering a scalable and\nprivacy-preserving solution for ADL data synthesis.",
      "pdf_url": "http://arxiv.org/pdf/2505.17987v1",
      "published": "2025-05-23T14:52:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17987v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models",
      "authors": [
        "Viktoriia Chekalina",
        "Daniil Moskovskiy",
        "Daria Cherniuk",
        "Maxim Kurkin",
        "Andrey Kuznetsov",
        "Evgeny Frolov"
      ],
      "abstract": "The Fisher information is a fundamental concept for characterizing the\nsensitivity of parameters in neural networks. However, leveraging the full\nobserved Fisher information is too expensive for large models, so most methods\nrely on simple diagonal approximations. While efficient, this approach ignores\nparameter correlations, often resulting in reduced performance on downstream\ntasks. In this work, we mitigate these limitations and propose Generalized\nFisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that\naccounts for both diagonal and off-diagonal elements of the Fisher information\nmatrix, providing a more accurate reflection of parameter importance. To make\nthe method tractable, we introduce a scalable adaptation of the\nKronecker-factored approximation algorithm for the observed Fisher information.\nWe demonstrate the effectiveness of our method on LLM compression, showing\nimprovements over existing compression baselines. For example, at a 20\ncompression rate on the MMLU benchmark, our method outperforms FWSVD, which is\nbased on a diagonal approximation of the Fisher information, by 5 percent,\nSVD-LLM by 3 percent, and ASVD by 6 percent compression rate.",
      "pdf_url": "http://arxiv.org/pdf/2505.17974v1",
      "published": "2025-05-23T14:41:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17974v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems",
      "authors": [
        "Jiayi Geng",
        "Howard Chen",
        "Dilip Arumugam",
        "Thomas L. Griffiths"
      ],
      "abstract": "Using AI to create autonomous researchers has the potential to accelerate\nscientific discovery. A prerequisite for this vision is understanding how well\nan AI model can identify the underlying structure of a black-box system from\nits behavior. In this paper, we explore how well a large language model (LLM)\nlearns to identify a black-box function from passively observed versus actively\ncollected data. We investigate the reverse-engineering capabilities of LLMs\nacross three distinct types of black-box systems, each chosen to represent\ndifferent problem domains where future autonomous AI researchers may have\nconsiderable impact: Program, Formal Language, and Math Equation. Through\nextensive experiments, we show that LLMs fail to extract information from\nobservations, reaching a performance plateau that falls short of the ideal of\nBayesian inference. However, we demonstrate that prompting LLMs to not only\nobserve but also intervene -- actively querying the black-box with specific\ninputs to observe the resulting output -- improves performance by allowing LLMs\nto test edge cases and refine their beliefs. By providing the intervention data\nfrom one LLM to another, we show that this improvement is partly a result of\nengaging in the process of generating effective interventions, paralleling\nresults in the literature on human learning. Further analysis reveals that\nengaging in intervention can help LLMs escape from two common failure modes:\novercomplication, where the LLM falsely assumes prior knowledge about the\nblack-box, and overlooking, where the LLM fails to incorporate observations.\nThese insights provide practical guidance for helping LLMs more effectively\nreverse-engineer black-box systems, supporting their use in making new\ndiscoveries.",
      "pdf_url": "http://arxiv.org/pdf/2505.17968v1",
      "published": "2025-05-23T14:37:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17968v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models",
      "authors": [
        "Ionut-Vlad Modoranu",
        "Mher Safaryan",
        "Erik Schultheis",
        "Dan Alistarh"
      ],
      "abstract": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to reduce the memory usage of adaptive optimizers by\nconstraining learning to a lower-dimensional space. Prior work typically\nprojects gradients of linear layers using approaches based on Singular Value\nDecomposition (SVD). However, applying SVD-based procedures individually to\neach layer in large models is computationally expensive and incurs additional\nmemory costs due to storing the projection matrices. In this work, we propose a\ncomputationally efficient and conceptually simple two-step procedure to\napproximate SVD-based gradient projections into lower-dimensional spaces.\nFirst, we construct a complete orthogonal basis using predefined orthogonal\nmatrices of the Discrete Cosine Transform (DCT). Second, we adaptively select\nbasis columns based on their alignment with the gradient of each layer. Each\nprojection matrix in our method is obtained via a single matrix multiplication\nfollowed by a lightweight sorting step to identify the most relevant basis\nvectors. Due to the predefined nature of the orthogonal bases, they are\ncomputed once at the start of training. During training, we store only the\nindices of the selected columns, avoiding the need to store full projection\nmatrices for each layer. Our numerical experiments on both pre-training and\nfine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, matching the performance of costly\nSVD-based methods while achieving faster runtime and reduced memory usage.",
      "pdf_url": "http://arxiv.org/pdf/2505.17967v1",
      "published": "2025-05-23T14:37:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17967v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation",
      "authors": [
        "Khellaf Rmi",
        "Bellet Aurlien",
        "Josse Julie"
      ],
      "abstract": "Causal inference typically assumes centralized access to individual-level\ndata. Yet, in practice, data are often decentralized across multiple sites,\nmaking centralization infeasible due to privacy, logistical, or legal\nconstraints. We address this by estimating the Average Treatment Effect (ATE)\nfrom decentralized observational data using federated learning, which enables\ninference through the exchange of aggregate statistics rather than\nindividual-level data. We propose a novel method to estimate propensity scores\nin a (non-)parametric manner by computing a federated weighted average of local\nscores, using two theoretically grounded weighting schemes -- Membership\nWeights (MW) and Density Ratio Weights (DW) -- that balance communication\nefficiency and model flexibility. These federated scores are then used to\nconstruct two ATE estimators: the Federated Inverse Propensity Weighting\nestimator (Fed-IPW) and its augmented variant (Fed-AIPW). Unlike meta-analysis\nmethods, which fail when any site violates positivity, our approach leverages\nheterogeneity in treatment assignment across sites to improve overlap. We show\nthat Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample\nsizes, treatment mechanisms, and covariate distributions, with theoretical\nanalysis and experiments on simulated and real-world data highlighting their\nstrengths and limitations relative to meta-analysis and related methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.17961v1",
      "published": "2025-05-23T14:32:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17961v1",
      "categories": [
        "stat.ME",
        "cs.AI",
        "math.ST",
        "stat.AP",
        "stat.TH"
      ]
    },
    {
      "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL",
      "authors": [
        "Che Liu",
        "Haozhe Wang",
        "Jiazhen Pan",
        "Zhongwei Wan",
        "Yong Dai",
        "Fangzhen Lin",
        "Wenjia Bai",
        "Daniel Rueckert",
        "Rossella Arcucci"
      ],
      "abstract": "Improving performance on complex tasks and enabling interpretable decision\nmaking in large language models (LLMs), especially for clinical applications,\nrequires effective reasoning. Yet this remains challenging without supervised\nfine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from\nclosed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the\nfirst medical LLM to show that reasoning capability can emerge purely through\nreinforcement learning (RL), using minimalist rule-based rewards on public\nmultiple-choice QA datasets, without relying on SFT or distilled CoT data.\nAlphaMed achieves state-of-the-art results on six medical QA benchmarks,\noutperforming models trained with conventional SFT+RL pipelines. On challenging\nbenchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source\nmodels such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the\nfactors behind this success, we conduct a comprehensive data-centric analysis\nguided by three questions: (i) Can minimalist rule-based RL incentivize\nreasoning without distilled CoT supervision? (ii) How do dataset quantity and\ndiversity impact reasoning? (iii) How does question difficulty shape the\nemergence and generalization of reasoning? Our findings show that dataset\ninformativeness is a key driver of reasoning performance, and that minimalist\nRL on informative, multiple-choice QA data is effective at inducing reasoning\nwithout CoT supervision. We also observe divergent trends across benchmarks,\nunderscoring limitations in current evaluation and the need for more\nchallenging, reasoning-oriented medical QA benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2505.17952v1",
      "published": "2025-05-23T14:27:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17952v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models",
      "authors": [
        "Tom Bleckmann",
        "Paul Tschisgale"
      ],
      "abstract": "Recent advancements in Natural Language Processing (NLP) have facilitated the\nanalysis of student-generated language products in learning analytics (LA),\nparticularly through the use of NLP embedding models. Yet when it comes to\nscience-related language, symbolic expressions such as equations and formulas\nintroduce challenges that current embedding models struggle to address.\nExisting studies and applications often either overlook these challenges or\nremove symbolic expressions altogether, potentially leading to biased findings\nand diminished performance of LA applications. This study therefore explores\nhow contemporary embedding models differ in their capability to process and\ninterpret science-related symbolic expressions. To this end, various embedding\nmodels are evaluated using physics-specific symbolic expressions drawn from\nauthentic student responses, with performance assessed via two approaches:\nsimilarity-based analyses and integration into a machine learning pipeline. Our\nfindings reveal significant differences in model performance, with OpenAI's\nGPT-text-embedding-3-large outperforming all other examined models, though its\nadvantage over other models was moderate rather than decisive. Beyond\nperformance, additional factors such as cost, regulatory compliance, and model\ntransparency are discussed as key considerations for model selection. Overall,\nthis study underscores the importance for LA researchers and practitioners of\ncarefully selecting NLP embedding models when working with science-related\nlanguage products that include symbolic expressions.",
      "pdf_url": "http://arxiv.org/pdf/2505.17950v1",
      "published": "2025-05-23T14:26:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17950v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "physics.ed-ph"
      ]
    },
    {
      "title": "LMask: Learn to Solve Constrained Routing Problems with Lazy Masking",
      "authors": [
        "Tianyou Li",
        "Haijun Zou",
        "Jiayuan Wu",
        "Zaiwen Wen"
      ],
      "abstract": "Routing problems are canonical combinatorial optimization tasks with\nwide-ranging applications in logistics, transportation, and supply chain\nmanagement. However, solving these problems becomes significantly more\nchallenging when complex constraints are involved. In this paper, we propose\nLMask, a novel learning framework that utilizes dynamic masking to generate\nhigh-quality feasible solutions for constrained routing problems. LMask\nintroduces the LazyMask decoding method, which lazily refines feasibility masks\nwith the backtracking mechanism. In addition, it employs the refinement\nintensity embedding to encode the search trace into the model, mitigating\nrepresentation ambiguities induced by backtracking. To further reduce sampling\ncost, LMask sets a backtracking budget during decoding, while constraint\nviolations are penalized in the loss function during training to counteract\ninfeasibility caused by this budget. We provide theoretical guarantees for the\nvalidity and probabilistic optimality of our approach. Extensive experiments on\nthe traveling salesman problem with time windows (TSPTW) and TSP with draft\nlimits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility\nrates and solution quality, outperforming existing neural methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.17938v1",
      "published": "2025-05-23T14:15:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17938v1",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG",
        "90C27, 68T20"
      ]
    },
    {
      "title": "AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models",
      "authors": [
        "Xingjian Li",
        "Qifeng Wu",
        "Colleen Que",
        "Yiran Ding",
        "Adithya S. Ubaradka",
        "Jianhua Xing",
        "Tianyang Wang",
        "Min Xu"
      ],
      "abstract": "Medical image segmentation is vital for clinical diagnosis, yet current deep\nlearning methods often demand extensive expert effort, i.e., either through\nannotating large training datasets or providing prompts at inference time for\neach new case. This paper introduces a zero-shot and automatic segmentation\npipeline that combines off-the-shelf vision-language and segmentation\nfoundation models. Given a medical image and a task definition (e.g., \"segment\nthe optic disc in an eye fundus image\"), our method uses a grounding model to\ngenerate an initial bounding box, followed by a visual prompt boosting module\nthat enhance the prompts, which are then processed by a promptable segmentation\nmodel to produce the final mask. To address the challenges of domain gap and\nresult verification, we introduce a test-time adaptation framework featuring a\nset of learnable adaptors that align the medical inputs with foundation model\nrepresentations. Its hyperparameters are optimized via Bayesian Optimization,\nguided by a proxy validation model without requiring ground-truth labels. Our\npipeline offers an annotation-efficient and scalable solution for zero-shot\nmedical image segmentation across diverse tasks. Our pipeline is evaluated on\nseven diverse medical imaging datasets and shows promising results. By proper\ndecomposition and test-time adaptation, our fully automatic pipeline performs\ncompetitively with weakly-prompted interactive foundation models.",
      "pdf_url": "http://arxiv.org/pdf/2505.17931v1",
      "published": "2025-05-23T14:07:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17931v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Towards Practical Defect-Focused Automated Code Review",
      "authors": [
        "Junyi Lu",
        "Lili Jiang",
        "Xiaojia Li",
        "Jianbing Fang",
        "Fengjun Zhang",
        "Li Yang",
        "Chun Zuo"
      ],
      "abstract": "The complexity of code reviews has driven efforts to automate review\ncomments, but prior approaches oversimplify this task by treating it as\nsnippet-level code-to-text generation and relying on text similarity metrics\nlike BLEU for evaluation. These methods overlook repository context, real-world\nmerge request evaluation, and defect detection, limiting their practicality. To\naddress these issues, we explore the full automation pipeline within the online\nrecommendation service of a company with nearly 400 million daily active users,\nanalyzing industry-grade C++ codebases comprising hundreds of thousands of\nlines of code. We identify four key challenges: 1) capturing relevant context,\n2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and\n4) integrating human workflows. To tackle these, we propose 1) code slicing\nalgorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a\nfiltering mechanism for FAR reduction, and 4) a novel prompt design for better\nhuman interaction. Our approach, validated on real-world merge requests from\nhistorical fault reports, achieves a 2x improvement over standard LLMs and a\n10x gain over previous baselines. While the presented results focus on C++, the\nunderlying framework design leverages language-agnostic principles (e.g.,\nAST-based analysis), suggesting potential for broader applicability.",
      "pdf_url": "http://arxiv.org/pdf/2505.17928v1",
      "published": "2025-05-23T14:06:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17928v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy",
      "authors": [
        "Carlos Salazar-Ruiz",
        "Francisco Lopez-Tiro",
        "Ivan Reyes-Amezcua",
        "Clement Larose",
        "Gilberto Ochoa-Ruiz",
        "Christian Daul"
      ],
      "abstract": "Determining the type of kidney stones is crucial for prescribing appropriate\ntreatments to prevent recurrence. Currently, various approaches exist to\nidentify the type of kidney stones. However, obtaining results through the\nreference ex vivo identification procedure can take several weeks, while in\nvivo visual recognition requires highly trained specialists. For this reason,\ndeep learning models have been developed to provide urologists with an\nautomated classification of kidney stones during ureteroscopies. Nevertheless,\na common issue with these models is the lack of training data. This\ncontribution presents a deep learning method based on few-shot learning, aimed\nat producing sufficiently discriminative features for identifying kidney stone\ntypes in endoscopic images, even with a very limited number of samples. This\napproach was specifically designed for scenarios where endoscopic images are\nscarce or where uncommon classes are present, enabling classification even with\na limited training dataset. The results demonstrate that Prototypical Networks,\nusing up to 25% of the training data, can achieve performance equal to or\nbetter than traditional deep learning models trained with the complete dataset.",
      "pdf_url": "http://arxiv.org/pdf/2505.17921v1",
      "published": "2025-05-23T13:59:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17921v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention",
      "authors": [
        "Zheyang Huang",
        "Jagannath Aryal",
        "Saeid Nahavandi",
        "Xuequan Lu",
        "Chee Peng Lim",
        "Lei Wei",
        "Hailing Zhou"
      ],
      "abstract": "Cross-view geo-localization determines the location of a query image,\ncaptured by a drone or ground-based camera, by matching it to a geo-referenced\nsatellite image. While traditional approaches focus on image-level\nlocalization, many applications, such as search-and-rescue, infrastructure\ninspection, and precision delivery, demand object-level accuracy. This enables\nusers to prompt a specific object with a single click on a drone image to\nretrieve precise geo-tagged information of the object. However, variations in\nviewpoints, timing, and imaging conditions pose significant challenges,\nespecially when identifying visually similar objects in extensive satellite\nimagery. To address these challenges, we propose an Object-level Cross-view\nGeo-localization Network (OCGNet). It integrates user-specified click locations\nusing Gaussian Kernel Transfer (GKT) to preserve location information\nthroughout the network. This cue is dually embedded into the feature encoder\nand feature matching blocks, ensuring robust object-specific localization.\nAdditionally, OCGNet incorporates a Location Enhancement (LE) module and a\nMulti-Head Cross Attention (MHCA) module to adaptively emphasize\nobject-specific features or expand focus to relevant contextual regions when\nnecessary. OCGNet achieves state-of-the-art performance on a public dataset,\nCVOGL. It also demonstrates few-shot learning capabilities, effectively\ngeneralizing from limited examples, making it suitable for diverse applications\n(https://github.com/ZheyangH/OCGNet).",
      "pdf_url": "http://arxiv.org/pdf/2505.17911v1",
      "published": "2025-05-23T13:55:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17911v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning",
      "authors": [
        "Bin Wu",
        "Wei Wang",
        "Yahui Liu",
        "Zixiang Li",
        "Yao Zhao"
      ],
      "abstract": "Reward Feedback Learning (ReFL) has recently shown great potential in\naligning model outputs with human preferences across various generative tasks.\nIn this work, we introduce a ReFL framework, named DiffusionReward, to the\nBlind Face Restoration task for the first time. DiffusionReward effectively\novercomes the limitations of diffusion-based methods, which often fail to\ngenerate realistic facial details and exhibit poor identity consistency. The\ncore of our framework is the Face Reward Model (FRM), which is trained using\ncarefully annotated data. It provides feedback signals that play a pivotal role\nin steering the optimization process of the restoration network. In particular,\nour ReFL framework incorporates a gradient flow into the denoising process of\noff-the-shelf face restoration methods to guide the update of model parameters.\nThe guiding gradient is collaboratively determined by three aspects: (i) the\nFRM to ensure the perceptual quality of the restored faces; (ii) a\nregularization term that functions as a safeguard to preserve generative\ndiversity; and (iii) a structural consistency constraint to maintain facial\nfidelity. Furthermore, the FRM undergoes dynamic optimization throughout the\nprocess. It not only ensures that the restoration network stays precisely\naligned with the real face manifold, but also effectively prevents reward\nhacking. Experiments on synthetic and wild datasets demonstrate that our method\noutperforms state-of-the-art methods, significantly improving identity\nconsistency and facial details. The source codes, data, and models are\navailable at: https://github.com/01NeuralNinja/DiffusionReward.",
      "pdf_url": "http://arxiv.org/pdf/2505.17910v1",
      "published": "2025-05-23T13:53:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17910v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling",
      "authors": [
        "Bram Grooten",
        "Farid Hasanov",
        "Chenxiang Zhang",
        "Qiao Xiao",
        "Boqian Wu",
        "Zahra Atashgahi",
        "Ghada Sokar",
        "Shiwei Liu",
        "Lu Yin",
        "Elena Mocanu",
        "Mykola Pechenizkiy",
        "Decebal Constantin Mocanu"
      ],
      "abstract": "Model ensembles have long been a cornerstone for improving generalization and\nrobustness in deep learning. However, their effectiveness often comes at the\ncost of substantial computational overhead. To address this issue,\nstate-of-the-art methods aim to replicate ensemble-class performance without\nrequiring multiple independently trained networks. Unfortunately, these\nalgorithms often still demand considerable compute at inference. In response to\nthese limitations, we introduce $\\textbf{NeuroTrails}$, a sparse multi-head\narchitecture with dynamically evolving topology. This unexplored model-agnostic\ntraining paradigm improves ensemble performance while reducing the required\nresources. We analyze the underlying reason for its effectiveness and observe\nthat the various neural trails induced by dynamic sparsity attain a\n$\\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays\nefficacy with convolutional and transformer-based architectures on computer\nvision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,\namong many others, demonstrate increased accuracy and stronger robustness in\nzero-shot generalization, while requiring significantly fewer parameters.",
      "pdf_url": "http://arxiv.org/pdf/2505.17909v1",
      "published": "2025-05-23T13:53:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17909v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback",
      "authors": [
        "Litao Guo",
        "Xinli Xu",
        "Luozhou Wang",
        "Jiantao Lin",
        "Jinsong Zhou",
        "Zixin Zhang",
        "Bolan Su",
        "Ying-Cong Chen"
      ],
      "abstract": "With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind",
      "pdf_url": "http://arxiv.org/pdf/2505.17908v1",
      "published": "2025-05-23T13:53:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17908v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation",
      "authors": [
        "Zi-Ao Ma",
        "Tian Lan",
        "Rong-Cheng Tu",
        "Shu-Hang Liu",
        "Heyan Huang",
        "Zhijing Wu",
        "Chen Xu",
        "Xian-Ling Mao"
      ],
      "abstract": "The rapid progress in diffusion-based text-to-image (T2I) generation has\ncreated an urgent need for interpretable automatic evaluation methods that can\nassess the quality of generated images, therefore reducing the human annotation\nburden. To reduce the prohibitive cost of relying on commercial models for\nlarge-scale evaluation, and to improve the reasoning capabilities of\nopen-source models, recent research has explored supervised fine-tuning (SFT)\nof multimodal large language models (MLLMs) as dedicated T2I evaluators.\nHowever, SFT approaches typically rely on high-quality critique datasets, which\nare either generated by proprietary LLMs-with potential issues of bias and\ninconsistency-or annotated by humans at high cost, limiting their scalability\nand generalization. To address these limitations, we propose T2I-Eval-R1, a\nnovel reinforcement learning framework that trains open-source MLLMs using only\ncoarse-grained quality scores, thereby avoiding the need for annotating\nhigh-quality interpretable evaluation rationale. Our approach integrates Group\nRelative Policy Optimization (GRPO) into the instruction-tuning process,\nenabling models to generate both scalar scores and interpretable reasoning\nchains with only easy accessible annotated judgment scores or preferences.\nFurthermore, we introduce a continuous reward formulation that encourages score\ndiversity and provides stable optimization signals, leading to more robust and\ndiscriminative evaluation behavior. Experimental results on three established\nT2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves\nsignificantly higher alignment with human assessments and offers more accurate\ninterpretable score rationales compared to strong baseline methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.17897v1",
      "published": "2025-05-23T13:44:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17897v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "DataRater: Meta-Learned Dataset Curation",
      "authors": [
        "Dan A. Calian",
        "Gregory Farquhar",
        "Iurii Kemaev",
        "Luisa M. Zintgraf",
        "Matteo Hessel",
        "Jeremy Shar",
        "Junhyuk Oh",
        "Andrs Gyrgy",
        "Tom Schaul",
        "Jeffrey Dean",
        "Hado van Hasselt",
        "David Silver"
      ],
      "abstract": "The quality of foundation models depends heavily on their training data.\nConsequently, great efforts have been put into dataset curation. Yet most\napproaches rely on manual tuning of coarse-grained mixtures of large buckets of\ndata, or filtering by hand-crafted heuristics. An approach that is ultimately\nmore scalable (let alone more satisfying) is to \\emph{learn} which data is\nactually valuable for training. This type of meta-learning could allow more\nsophisticated, fine-grained, and effective curation. Our proposed\n\\emph{DataRater} is an instance of this idea. It estimates the value of\ntraining on any particular data point. This is done by meta-learning using\n`meta-gradients', with the objective of improving training efficiency on held\nout data. In extensive experiments across a range of model scales and datasets,\nwe find that using our DataRater to filter data is highly effective, resulting\nin significantly improved compute efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2505.17895v1",
      "published": "2025-05-23T13:43:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17895v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "I.2.6"
      ]
    }
  ]
}