{
  "last_updated": "2026-02-16T01:11:07.357117",
  "papers": [
    {
      "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
      "authors": [
        "Jacky Kwok",
        "Xilun Zhang",
        "Mengdi Xu",
        "Yuejiang Liu",
        "Azalia Mirhoseini",
        "Chelsea Finn",
        "Marco Pavone"
      ],
      "abstract": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
      "pdf_url": "https://arxiv.org/pdf/2602.12281v1",
      "published": "2026-02-12T18:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12281v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ]
    },
    {
      "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
      "authors": [
        "Leon Liangyu Chen",
        "Haoyu Ma",
        "Zhipeng Fan",
        "Ziqi Huang",
        "Animesh Sinha",
        "Xiaoliang Dai",
        "Jialiang Wang",
        "Zecheng He",
        "Jianwei Yang",
        "Chunyuan Li",
        "Junzhe Sun",
        "Chu Wang",
        "Serena Yeung-Levy",
        "Felix Juefei-Xu"
      ],
      "abstract": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
      "pdf_url": "https://arxiv.org/pdf/2602.12279v1",
      "published": "2026-02-12T18:59:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12279v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
      "authors": [
        "David Jiahao Fu",
        "Lam Thanh Do",
        "Jiayu Li",
        "Kevin Chen-Chuan Chang"
      ],
      "abstract": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.",
      "pdf_url": "https://arxiv.org/pdf/2602.12278v1",
      "published": "2026-02-12T18:59:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12278v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Agentic Test-Time Scaling for WebAgents",
      "authors": [
        "Nicholas Lee",
        "Lutfi Eren Erdogan",
        "Chris Joseph John",
        "Surya Krishnapillai",
        "Michael W. Mahoney",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "abstract": "Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.",
      "pdf_url": "https://arxiv.org/pdf/2602.12276v1",
      "published": "2026-02-12T18:58:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12276v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Creative Ownership in the Age of AI",
      "authors": [
        "Annie Liang",
        "Jay Lu"
      ],
      "abstract": "Copyright law focuses on whether a new work is \"substantially similar\" to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been generated without that work in its training corpus. To operationalize this definition, we model generative systems as closure operators mapping a corpus of existing works to an output of new works. AI generated outputs are \\emph{permissible} if they do not infringe on any existing work according to our criterion. Our results characterize structural properties of permissible generation and reveal a sharp asymptotic dichotomy: when the process of organic creations is light-tailed, dependence on individual works eventually vanishes, so that regulation imposes no limits on AI generation; with heavy-tailed creations, regulation can be persistently constraining.",
      "pdf_url": "https://arxiv.org/pdf/2602.12270v1",
      "published": "2026-02-12T18:56:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12270v1",
      "categories": [
        "econ.TH",
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use",
      "authors": [
        "Zhen Zhang",
        "Kaiqiang Song",
        "Xun Wang",
        "Yebowen Hu",
        "Weixiang Yan",
        "Chenyang Zhao",
        "Henry Peng Zou",
        "Haoyun Deng",
        "Sathish Reddy Indurthi",
        "Shujian Liu",
        "Simin Ma",
        "Xiaoyang Wang",
        "Xin Eric Wang",
        "Song Wang"
      ],
      "abstract": "AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.",
      "pdf_url": "https://arxiv.org/pdf/2602.12268v1",
      "published": "2026-02-12T18:55:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12268v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
      "authors": [
        "Jianke Yang",
        "Ohm Venkatachalam",
        "Mohammad Kianezhad",
        "Sharvaree Vadgama",
        "Rose Yu"
      ],
      "abstract": "Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.",
      "pdf_url": "https://arxiv.org/pdf/2602.12259v1",
      "published": "2026-02-12T18:49:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12259v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "On the implicit regularization of Langevin dynamics with projected noise",
      "authors": [
        "Govind Menon",
        "Austin J. Stromme",
        "Adrien Vacher"
      ],
      "abstract": "We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, Langevin dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic diffusion but with an additional drift term proportional to the negative log volume of the group orbit. We prove this result by constructing a coupling of the two processes via a third process on the group itself, and identify the additional drift as the mean curvature of the orbits.",
      "pdf_url": "https://arxiv.org/pdf/2602.12257v1",
      "published": "2026-02-12T18:45:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12257v1",
      "categories": [
        "math.PR",
        "cs.AI"
      ]
    },
    {
      "title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication",
      "authors": [
        "Ralph Krüger"
      ],
      "abstract": "This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.",
      "pdf_url": "https://arxiv.org/pdf/2602.12251v1",
      "published": "2026-02-12T18:37:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12251v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "\"Sorry, I Didn't Catch That\": How Speech Models Miss What Matters Most",
      "authors": [
        "Kaitlyn Zhou",
        "Martijn Bartelds",
        "Federico Bianchi",
        "James Zou"
      ],
      "abstract": "Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.",
      "pdf_url": "https://arxiv.org/pdf/2602.12249v1",
      "published": "2026-02-12T18:36:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12249v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction",
      "authors": [
        "Nick Ferguson",
        "Josh Pennington",
        "Narek Beghian",
        "Aravind Mohan",
        "Douwe Kiela",
        "Sheshansh Agrawal",
        "Thien Hang Nguyen"
      ],
      "abstract": "Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.",
      "pdf_url": "https://arxiv.org/pdf/2602.12247v1",
      "published": "2026-02-12T18:31:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12247v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces",
      "authors": [
        "Anthony Kobanda",
        "Waris Radji"
      ],
      "abstract": "Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.",
      "pdf_url": "https://arxiv.org/pdf/2602.12245v1",
      "published": "2026-02-12T18:30:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12245v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
      "authors": [
        "Mayee F. Chen",
        "Tyler Murray",
        "David Heineman",
        "Matt Jordan",
        "Hannaneh Hajishirzi",
        "Christopher Ré",
        "Luca Soldaini",
        "Kyle Lo"
      ],
      "abstract": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.",
      "pdf_url": "https://arxiv.org/pdf/2602.12237v1",
      "published": "2026-02-12T18:16:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12237v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision",
      "authors": [
        "Anika Tabassum Meem",
        "Muntasir Hossain Nadid",
        "Md Zesun Ahmed Mia"
      ],
      "abstract": "Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.",
      "pdf_url": "https://arxiv.org/pdf/2602.12236v1",
      "published": "2026-02-12T18:15:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12236v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Bandit Learning in Matching Markets with Interviews",
      "authors": [
        "Amirmahdi Mirfakhar",
        "Xuchuang Wang",
        "Mengfan Xu",
        "Hedyeh Beyhaghi",
        "Mohammad Hajiesmaili"
      ],
      "abstract": "Two-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with interviews, modeling interviews as \\textit{low-cost hints} that reveal partial preference information to both sides. Our framework departs from existing work by allowing firm-side uncertainty: firms, like agents, may be unsure of their own preferences and can make early hiring mistakes by hiring less preferred agents. To handle this, we extend the firm's action space to allow \\emph{strategic deferral} (choosing not to hire in a round), enabling recovery from suboptimal hires and supporting decentralized learning without coordination. We design novel algorithms for (i) a centralized setting with an omniscient interview allocator and (ii) decentralized settings with two types of firm-side feedback. Across all settings, our algorithms achieve time-independent regret, a substantial improvement over the $O(\\log T)$ regret bounds known for learning stable matchings without interviews. Also, under mild structured markets, decentralized performance matches the centralized counterpart up to polynomial factors in the number of agents and firms.",
      "pdf_url": "https://arxiv.org/pdf/2602.12224v1",
      "published": "2026-02-12T18:03:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12224v1",
      "categories": [
        "cs.GT",
        "cs.AI",
        "econ.TH"
      ]
    },
    {
      "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training",
      "authors": [
        "Miaosen Zhang",
        "Yishan Liu",
        "Shuxia Lin",
        "Xu Yang",
        "Qi Dai",
        "Chong Luo",
        "Weihao Jiang",
        "Peng Hou",
        "Anxiang Zeng",
        "Xin Geng",
        "Baining Guo"
      ],
      "abstract": "Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \\textbf{\\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \\textbf{\\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT",
      "pdf_url": "https://arxiv.org/pdf/2602.12222v1",
      "published": "2026-02-12T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12222v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics",
      "authors": [
        "Christian Internò",
        "Jumpei Yamaguchi",
        "Loren Amdahl-Culleton",
        "Markus Olhofer",
        "David Klindt",
        "Barbara Hammer"
      ],
      "abstract": "Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.",
      "pdf_url": "https://arxiv.org/pdf/2602.12218v1",
      "published": "2026-02-12T17:56:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12218v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "VIRENA: Virtual Arena for Research, Education, and Democratic Innovation",
      "authors": [
        "Emma Hoes",
        "K. Jonathan Klueser",
        "Fabrizio Gilardi"
      ],
      "abstract": "Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA's no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.",
      "pdf_url": "https://arxiv.org/pdf/2602.12207v1",
      "published": "2026-02-12T17:46:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12207v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SI"
      ]
    },
    {
      "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
      "authors": [
        "Dianyi Wang",
        "Ruihang Li",
        "Feng Han",
        "Chaofan Ma",
        "Wei Song",
        "Siyuan Wang",
        "Yibin Wang",
        "Yi Xin",
        "Hongjian Liu",
        "Zhixiong Zhang",
        "Shengyuan Ding",
        "Tianhang Wang",
        "Zhenglin Cheng",
        "Tao Lin",
        "Cheng Jin",
        "Kaicheng Yu",
        "Jingjing Chen",
        "Wenjie Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "abstract": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
      "pdf_url": "https://arxiv.org/pdf/2602.12205v1",
      "published": "2026-02-12T17:44:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12205v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education",
      "authors": [
        "Mohamed Huti",
        "Alasdair Mackintosh",
        "Amy Waldock",
        "Dominic Andrews",
        "Maxime Lelièvre",
        "Moritz Boos",
        "Tobias Murray",
        "Paul Atherton",
        "Robin A. A. Ince",
        "Oliver G. B. Garrod"
      ],
      "abstract": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.",
      "pdf_url": "https://arxiv.org/pdf/2602.12196v1",
      "published": "2026-02-12T17:29:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12196v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization",
      "authors": [
        "Sunghwan Kim",
        "Wooseok Jeong",
        "Serin Kim",
        "Sangam Lee",
        "Dongha Lee"
      ],
      "abstract": "Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.",
      "pdf_url": "https://arxiv.org/pdf/2602.12187v1",
      "published": "2026-02-12T17:18:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12187v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation",
      "authors": [
        "Chengxi Zeng",
        "Yuxuan Jiang",
        "Ge Gao",
        "Shuai Wang",
        "Duolikun Danier",
        "Bin Zhu",
        "Stevan Rudinac",
        "David Bull",
        "Fan Zhang"
      ],
      "abstract": "Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.",
      "pdf_url": "https://arxiv.org/pdf/2602.12173v1",
      "published": "2026-02-12T17:01:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12173v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation",
      "authors": [
        "Bowei He",
        "Yankai Chen",
        "Xiaokun Zhang",
        "Linghe Kong",
        "Philip S. Yu",
        "Xue Liu",
        "Chen Ma"
      ],
      "abstract": "Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline -- Knowledge Identifier, Organizer, and Adapter (IOA) -- that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.",
      "pdf_url": "https://arxiv.org/pdf/2602.12172v1",
      "published": "2026-02-12T17:00:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12172v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Statistical Parsing for Logical Information Retrieval",
      "authors": [
        "Greg Coppola"
      ],
      "abstract": "In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language.\n  This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive reasoning (modus tollens) via backward lambda messages, completing Prawitz's simple elimination rules. The engine handles 44/44 test cases spanning 22 reasoning patterns. For semantics, we present a typed logical language with role-labeled predicates, modal quantifiers, and three tiers of expressiveness following Prawitz: first-order quantification, propositions as arguments, and predicate quantification via lambda abstraction. For syntax, we present a typed slot grammar that deterministically compiles sentences to logical form (33/33 correct, zero ambiguity). LLMs handle disambiguation (95% PP attachment accuracy) but cannot produce structured parses directly (12.4% UAS), confirming grammars are necessary. The architecture: LLM preprocesses, grammar parses, LLM reranks, QBBN infers.\n  We argue this reconciles formal semantics with Sutton's \"bitter lesson\" (2019): LLMs eliminate the annotation bottleneck that killed formal NLP, serving as annotator while the QBBN serves as verifier. Code: https://github.com/gregorycoppola/world",
      "pdf_url": "https://arxiv.org/pdf/2602.12170v1",
      "published": "2026-02-12T16:57:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12170v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
      "authors": [
        "Xiaohan He",
        "Shiyang Feng",
        "Songtao Huang",
        "Lei Bai",
        "Bin Wang",
        "Bo Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.",
      "pdf_url": "https://arxiv.org/pdf/2602.12164v1",
      "published": "2026-02-12T16:46:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12164v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting",
      "authors": [
        "Wancai Zheng",
        "Hao Chen",
        "Xianlong Lu",
        "Linlin Ou",
        "Xinyi Yu"
      ],
      "abstract": "Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2602.12159v1",
      "published": "2026-02-12T16:41:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12159v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "dVoting: Fast Voting for dLLMs",
      "authors": [
        "Sicheng Feng",
        "Zigeng Chen",
        "Xinyin Ma",
        "Gongfan Fang",
        "Xinchao Wang"
      ],
      "abstract": "Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting",
      "pdf_url": "https://arxiv.org/pdf/2602.12153v1",
      "published": "2026-02-12T16:35:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12153v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GPT-4o Lacks Core Features of Theory of Mind",
      "authors": [
        "John Muchovej",
        "Amanda Royka",
        "Shane Lee",
        "Julian Jara-Ettinger"
      ],
      "abstract": "Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.",
      "pdf_url": "https://arxiv.org/pdf/2602.12150v1",
      "published": "2026-02-12T16:33:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12150v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning",
      "authors": [
        "Mahdi Khodabandeh",
        "Ghazal Shabani",
        "Arash Yousefi Jordehi",
        "Seyed Abolghasem Mirroshandel"
      ],
      "abstract": "Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.",
      "pdf_url": "https://arxiv.org/pdf/2602.12146v1",
      "published": "2026-02-12T16:30:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12146v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IT"
      ]
    },
    {
      "title": "On the Adoption of AI Coding Agents in Open-source Android and iOS Development",
      "authors": [
        "Muhammad Ahmad Khan",
        "Hasnain Ali",
        "Muneeb Rana",
        "Muhammad Saqib Ilyas",
        "Abdul Ali Bangash"
      ],
      "abstract": "AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.",
      "pdf_url": "https://arxiv.org/pdf/2602.12144v1",
      "published": "2026-02-12T16:30:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12144v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction",
      "authors": [
        "Xiaoxiao Wang",
        "Chunxiao Li",
        "Junying Wang",
        "Yijin Guo",
        "Zijian Chen",
        "Chunyi Li",
        "Xiaohong Liu",
        "Zicheng Zhang",
        "Guangtao Zhai"
      ],
      "abstract": "As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We propose STAR, a framework that bridges data-driven STatistical expectations with knowledge-driven Agentic Reasoning. STAR leverages specialized retrievers to gather external knowledge and embeds semantic features into Constrained Probabilistic Matrix Factorization (CPMF) to generate statistical expectations with uncertainty. A reasoning module guided by Expectation Violation Theory (EVT) then refines predictions through intra-family analysis, cross-model comparison, and credibility-aware aggregation, producing adjustments with traceable explanations. Extensive experiments show that STAR consistently outperforms all baselines on both score-based and rank-based metrics, delivering a 14.46% gain in total score over the strongest statistical method under extreme sparsity, with only 1--2 observed scores per test model.",
      "pdf_url": "https://arxiv.org/pdf/2602.12143v1",
      "published": "2026-02-12T16:30:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12143v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment",
      "authors": [
        "Jiajun Chen",
        "Hua Shen"
      ],
      "abstract": "Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2602.12134v1",
      "published": "2026-02-12T16:21:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12134v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini Flash 2.5 Image and GPT Image 1.5",
      "authors": [
        "Roberto Balestri"
      ],
      "abstract": "This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantically neutral prompts. The analysis employed a rigorous pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification using the Monk (MST), PERLA, and Fitzpatrick scales. Neutral prompts produced highly polarized defaults. Both models exhibited a strong \"default white\" bias (>96% of outputs). However, they diverged sharply on gender: Gemini favored female-presenting subjects, while GPT favored male-presenting subjects with lighter skin tones. This research provides a large-scale, comparative audit of state-of-the-art models using an illumination-aware colorimetric methodology, distinguishing aesthetic rendering from underlying pigmentation in synthetic imagery. The study demonstrates that neutral prompts function as diagnostic probes rather than neutral instructions. It offers a robust framework for auditing algorithmic visual culture and challenges the sociolinguistic assumption that unmarked language results in inclusive representation.",
      "pdf_url": "https://arxiv.org/pdf/2602.12133v1",
      "published": "2026-02-12T16:21:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12133v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC"
      ]
    },
    {
      "title": "HLA: Hadamard Linear Attention",
      "authors": [
        "Hanno Ackermann",
        "Hong Cai",
        "Mohsen Ghafoorian",
        "Amirhossein Habibian"
      ],
      "abstract": "The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.\n  We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.",
      "pdf_url": "https://arxiv.org/pdf/2602.12128v1",
      "published": "2026-02-12T16:16:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12128v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
      "authors": [
        "Wenkai Yang",
        "Weijie Liu",
        "Ruobing Xie",
        "Kai Yang",
        "Saiyong Yang",
        "Yankai Lin"
      ],
      "abstract": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
      "pdf_url": "https://arxiv.org/pdf/2602.12125v1",
      "published": "2026-02-12T16:14:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12125v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning",
      "authors": [
        "Xubin Wang",
        "Weijia Jia"
      ],
      "abstract": "Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.\n  Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.\n  Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods -- spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches -- across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.",
      "pdf_url": "https://arxiv.org/pdf/2602.12123v1",
      "published": "2026-02-12T16:11:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12123v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models",
      "authors": [
        "Jittarin Jetwiriyanon",
        "Teo Susnjak",
        "Surangika Ranathunga"
      ],
      "abstract": "Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreliable, as parameter estimation and model selection are unstable with short samples, and structural breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors, delivering strong gains in annual, data-sparse institutional forecasting under leakage-disciplined covariate construction. We benchmark multiple TSFM families in a zero-shot setting and test a compact, leakage-safe covariate set and introduce the Institutional Operating Conditions Index (IOCI), a transferable 0-100 regime covariate derived from time-stamped documentary evidence available at each forecast origin, alongside Google Trends demand proxies with stabilising feature engineering. Using an expanding-window backtest with strict vintage alignment, covariate-conditioned TSFMs perform on par with classical benchmarks without institution-specific training, with performance differences varying by cohort and model.",
      "pdf_url": "https://arxiv.org/pdf/2602.12120v1",
      "published": "2026-02-12T16:10:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12120v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation on Meteorological Satellite",
      "authors": [
        "Jiakang Shen",
        "Qinghui Chen",
        "Runtong Wang",
        "Chenrui Xu",
        "Jinglin Zhang",
        "Cong Bai",
        "Feng Zhang"
      ],
      "abstract": "Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computational inefficiency and high parameter counts of existing methods on resource-constrained edge devices. Current physics-guided models suffer from linear feature interactions that fail to capture high-order polynomial relationships between TC attributes, leading to inflated model sizes and hardware incompatibility. To overcome these challenges, this study introduces the Kolmogorov-Arnold Network-based Feature Interaction Framework (KAN-FIF), a lightweight multimodal architecture that integrates MLP and CNN layers with spline-parameterized KAN layers. For Maximum Sustained Wind (MSW) prediction, experiments demonstrate that the KAN-FIF framework achieves a $94.8\\%$ reduction in parameters (0.99MB vs 19MB) and $68.7\\%$ faster inference per sample (2.3ms vs 7.35ms) compared to baseline model Phy-CoCo, while maintaining superior accuracy with $32.5\\%$ lower MAE. The offline deployment experiment of the FY-4 series meteorological satellite processor on the Qingyun-1000 development board achieved a 14.41ms per-sample inference latency with the KAN-FIF framework, demonstrating promising feasibility for operational TC monitoring and extending deployability to edge-device AI applications. The code is released at https://github.com/Jinglin-Zhang/KAN-FIF.",
      "pdf_url": "https://arxiv.org/pdf/2602.12117v1",
      "published": "2026-02-12T16:07:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12117v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty",
      "authors": [
        "Zewei Yu",
        "Lirong Gao",
        "Yuke Zhu",
        "Bo Zheng",
        "Sheng Guo",
        "Haobo Wang",
        "Junbo Zhao"
      ],
      "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity induces more excessive and unnecessary reflection, which in turn reduces accuracy and increases token overhead. To address this challenge, we propose Adaptive Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning framework designed to dynamically balance reasoning efficiency and solution accuracy. ARLCP introduces two key innovations: (1) a reflection penalty that adaptively curtails unnecessary reflective steps while preserving essential reasoning, and (2) a length penalty calibrated to the estimated complexity of the problem. By coordinating these penalties, ARLCP encourages the model to generate more concise and effective reasoning paths. We evaluate our method on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models. Experimental results show that ARLCP achieves a superior efficiency-accuracy trade-off compared to existing approaches. For the 1.5B model, it reduces the average response length by 53.1% while simultaneously improving accuracy by 5.8%. For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy gain. The code is released at https://github.com/ZeweiYu1/ARLCP .",
      "pdf_url": "https://arxiv.org/pdf/2602.12113v1",
      "published": "2026-02-12T16:04:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12113v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context",
      "authors": [
        "Xiaoyuan Liu",
        "Tian Liang",
        "Dongyang Ma",
        "Deyu Zhou",
        "Haitao Mi",
        "Pinjia He",
        "Yan Wang"
      ],
      "abstract": "In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.",
      "pdf_url": "https://arxiv.org/pdf/2602.12108v1",
      "published": "2026-02-12T16:00:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12108v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "On the Complexity of Offline Reinforcement Learning with $Q^\\star$-Approximation and Partial Coverage",
      "authors": [
        "Haolin Liu",
        "Braham Snyder",
        "Chen-Yu Wei"
      ],
      "abstract": "We study offline reinforcement learning under $Q^\\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: \"Are $Q^\\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?\"\n  We answer in the negative by establishing an information-theoretic lower bound. Going substantially beyond this, we introduce a general framework that characterizes the intrinsic complexity of a given $Q^\\star$ function class, inspired by model-free decision-estimation coefficients (DEC) for online RL (Foster et al., 2023b; Liu et al., 2025b). This complexity recovers and improves the quantities underlying the guarantees of Chen and Jiang (2022) and Uehara et al. (2023), and extends to broader settings. Our decision-estimation decomposition can be combined with a wide range of $Q^\\star$ estimation procedures, modularizing and generalizing existing approaches.\n  Beyond the general framework, we make further contributions: By developing a novel second-order performance difference lemma, we obtain the first $ε^{-2}$ sample complexity under partial coverage for soft $Q$-learning, improving the $ε^{-4}$ bound of Uehara et al. (2023). We remove Chen and Jiang's (2022) need for additional online interaction when the value gap of $Q^\\star$ is unknown. We also give the first characterization of offline learnability for general low-Bellman-rank MDPs without Bellman completeness (Jiang et al., 2017; Du et al., 2021; Jin et al., 2021), a canonical setting in online RL that remains unexplored in offline RL except for special cases. Finally, we provide the first analysis for CQL under $Q^\\star$-realizability and Bellman completeness beyond the tabular case.",
      "pdf_url": "https://arxiv.org/pdf/2602.12107v1",
      "published": "2026-02-12T15:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12107v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Multi Graph Search for High-Dimensional Robot Motion Planning",
      "authors": [
        "Itamar Mishani",
        "Maxim Likhachev"
      ],
      "abstract": "Efficient motion planning for high-dimensional robotic systems, such as manipulators and mobile manipulators, is critical for real-time operation and reliable deployment. Although advances in planning algorithms have enhanced scalability to high-dimensional state spaces, these improvements often come at the cost of generating unpredictable, inconsistent motions or requiring excessive computational resources and memory. In this work, we introduce Multi-Graph Search (MGS), a search-based motion planning algorithm that generalizes classical unidirectional and bidirectional search to a multi-graph setting. MGS maintains and incrementally expands multiple implicit graphs over the state space, focusing exploration on high-potential regions while allowing initially disconnected subgraphs to be merged through feasible transitions as the search progresses. We prove that MGS is complete and bounded-suboptimal, and empirically demonstrate its effectiveness on a range of manipulation and mobile manipulation tasks. Demonstrations, benchmarks and code are available at https://multi-graph-search.github.io/.",
      "pdf_url": "https://arxiv.org/pdf/2602.12096v1",
      "published": "2026-02-12T15:50:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12096v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "DeepSight: An All-in-One LM Safety Toolkit",
      "authors": [
        "Bo Zhang",
        "Jiaxuan Guo",
        "Lijun Li",
        "Dongrui Liu",
        "Sujin Chen",
        "Guanxu Chen",
        "Zhijie Zheng",
        "Qihao Lin",
        "Lewen Yan",
        "Chen Qian",
        "Yijin Zhou",
        "Yuyao Wu",
        "Shaoxiong Guo",
        "Tianyi Du",
        "Jingyi Yang",
        "Xuhao Hu",
        "Ziqi Miao",
        "Xiaoya Lu",
        "Jing Shao",
        "Xia Hu"
      ],
      "abstract": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
      "pdf_url": "https://arxiv.org/pdf/2602.12092v1",
      "published": "2026-02-12T15:43:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12092v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ]
    },
    {
      "title": "Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation",
      "authors": [
        "Kehang Zhu",
        "Lithium Thain",
        "Vivian Tsai",
        "James Wexler",
        "Crystal Qian"
      ],
      "abstract": "As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants \\textit{access to} a single LLM assistance modality: proactive recommendations from an \\textit{Advisor}, reactive feedback from a \\textit{Coach}, or autonomous execution by a \\textit{Delegate}; all modalities are powered by an underlying LLM that achieves superhuman performance in an all-agent environment. On each turn, participants privately decide whether to act manually or use the AI modality available in that game. Despite preferring the \\textit{Advisor} modality, participants achieve the highest mean individual gains with the \\textit{Delegate}, demonstrating a preference-performance misalignment. Moreover, delegation generates positive externalities; even non-adopting users in \\textit{access-to-delegate} treatment groups benefit by receiving higher-quality offers. Mechanism analysis reveals that the \\textit{Delegate} agent acts as a market maker, injecting rational, Pareto-improving proposals that restructure the trading environment. Our research reveals a gap between agent capabilities and realized group welfare. While autonomous agents can exhibit super-human strategic performance, their impact on realized welfare gains can be constrained by interfaces, user perceptions, and adoption barriers. Assistance modalities should be designed as mechanisms with endogenous participation; adoption-compatible interaction rules are a prerequisite to improving human welfare with automated assistance.",
      "pdf_url": "https://arxiv.org/pdf/2602.12089v1",
      "published": "2026-02-12T15:41:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12089v1",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication",
      "authors": [
        "Antonin Sulc"
      ],
      "abstract": "As multi-agent AI systems evolve from simple chatbots to autonomous swarms, debugging semantic failures requires reasoning about knowledge, belief, causality, and obligation, precisely what modal logic was designed to formalize. However, traditional modal logic requires manual specification of relationship structures that are unknown or dynamic in real systems. This tutorial demonstrates differentiable modal logic (DML), implemented via Modal Logical Neural Networks (MLNNs), enabling systems to learn trust networks, causal chains, and regulatory boundaries from behavioral data alone.\n  We present a unified neurosymbolic debugging framework through four modalities: epistemic (who to trust), temporal (when events cause failures), deontic (what actions are permitted), and doxastic (how to interpret agent confidence). Each modality is demonstrated on concrete multi-agent scenarios, from discovering deceptive alliances in diplomacy games to detecting LLM hallucinations, with complete implementations showing how logical contradictions become learnable optimization objectives. Key contributions for the neurosymbolic community: (1) interpretable learned structures where trust and causality are explicit parameters, not opaque embeddings; (2) knowledge injection via differentiable axioms that guide learning with sparse data (3) compositional multi-modal reasoning that combines epistemic, temporal, and deontic constraints; and (4) practical deployment patterns for monitoring, active control and communication of multi-agent systems. All code provided as executable Jupyter notebooks.",
      "pdf_url": "https://arxiv.org/pdf/2602.12083v1",
      "published": "2026-02-12T15:39:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12083v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "Tiny Recursive Reasoning with Mamba-2 Attention Hybrid",
      "authors": [
        "Wenlong Wang",
        "Fergal Reid"
      ],
      "abstract": "Recent work on recursive reasoning models like TRM demonstrates that tiny networks (7M parameters) can achieve strong performance on abstract reasoning tasks through latent recursion -- iterative refinement in hidden representation space without emitting intermediate tokens. This raises a natural question about operator choice: Mamba-2's state space recurrence is itself a form of iterative refinement, making it a natural candidate for recursive reasoning -- but does introducing Mamba-2 into the recursive scaffold preserve reasoning capability? We investigate this by replacing the Transformer blocks in TRM with Mamba-2 hybrid operators while maintaining parameter parity (6.83M vs 6.86M parameters). On ARC-AGI-1, we find that the hybrid improves pass@2 (the official metric) by +2.0\\% (45.88\\% vs 43.88\\%) and consistently outperforms at higher K values (+4.75\\% at pass@100), whilst maintaining pass@1 parity. This suggests improved candidate coverage -- the model generates correct solutions more reliably -- with similar top-1 selection. Our results validate that Mamba-2 hybrid operators preserve reasoning capability within the recursive scaffold, establishing SSM-based operators as viable candidates in the recursive operator design space and taking a first step towards understanding the best mixing strategies for recursive reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2602.12078v1",
      "published": "2026-02-12T15:36:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12078v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair",
      "authors": [
        "Zhiyong Chen",
        "Jialun Cao",
        "Chang Xu",
        "Shing-Chi Cheung"
      ],
      "abstract": "Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.",
      "pdf_url": "https://arxiv.org/pdf/2602.12058v1",
      "published": "2026-02-12T15:19:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12058v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.FL"
      ]
    },
    {
      "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
      "authors": [
        "Xinyu Yang",
        "Chenlong Deng",
        "Tongyu Wen",
        "Binyu Xie",
        "Zhicheng Dou"
      ],
      "abstract": "Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .",
      "pdf_url": "https://arxiv.org/pdf/2602.12056v1",
      "published": "2026-02-12T15:19:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12056v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Multi UAVs Preflight Planning in a Shared and Dynamic Airspace",
      "authors": [
        "Amath Sow",
        "Mauricio Rodriguez Cesen",
        "Fabiola Martins Campos de Oliveira",
        "Mariusz Wzorek",
        "Daniel de Leng",
        "Mattias Tiger",
        "Fredrik Heintz",
        "Christian Esteve Rothenberg"
      ],
      "abstract": "Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with Incremental and Iterative Conflict Resolution. Our framework first generates an initial solution by prioritizing missions based on urgency. Secondly, it computes roundtrip trajectories using SFIPP-ST, a novel 4D single-agent planner (Safe Flight Interval Path Planning with Soft and Temporal Constraints). SFIPP-ST handles heterogeneous UAVs, strictly enforces temporal NFZs, and models inter-agent conflicts as soft constraints. Subsequently, an iterative Large Neighborhood Search, guided by a geometric conflict graph, efficiently resolves any residual conflicts. A completeness-preserving directional pruning technique further accelerates the 3D search. On benchmarks with temporal NFZs, DTAPP-IICR achieves near-100% success with fleets of up to 1,000 UAVs and gains up to 50% runtime reduction from pruning, outperforming batch Enhanced Conflict-Based Search in the UTM context. Scaling successfully in realistic city-scale operations where other priority-based methods fail even at moderate deployments, DTAPP-IICR is positioned as a practical and scalable solution for preflight planning in dense, dynamic urban airspace.",
      "pdf_url": "https://arxiv.org/pdf/2602.12055v1",
      "published": "2026-02-12T15:18:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12055v1",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ]
    },
    {
      "title": "Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling",
      "authors": [
        "Jed A. Duersch",
        "Elohan Veillon",
        "Astrid Klipfel",
        "Adlane Sayede",
        "Zied Bouraoui"
      ],
      "abstract": "The discovery of new crystalline materials calls for generative models that handle periodic boundary conditions, crystallographic symmetries, and physical constraints, while scaling to large and structurally diverse unit cells. We propose a reciprocal-space generative pipeline that represents crystals through a truncated Fourier transform of the species-resolved unit-cell density, rather than modeling atomic coordinates directly. This representation is periodicity-native, admits simple algebraic actions of space-group symmetries, and naturally supports variable atomic multiplicities during generation, addressing a common limitation of particle-based approaches. Using only nine Fourier basis functions per spatial dimension, our approach reconstructs unit cells containing up to 108 atoms per chemical species. We instantiate this pipeline with a transformer variational autoencoder over complex-valued Fourier coefficients, and a latent diffusion model that generates in the compressed latent space. We evaluate reconstruction and latent diffusion on the LeMaterial benchmark and compare unconditional generation against coordinate-based baselines in the small-cell regime ($\\leq 16$ atoms per unit cell).",
      "pdf_url": "https://arxiv.org/pdf/2602.12045v1",
      "published": "2026-02-12T15:11:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.12045v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}