{
  "last_updated": "2025-04-17T00:48:56.771090",
  "papers": [
    {
      "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning",
      "authors": [
        "Zhiwei He",
        "Tian Liang",
        "Jiahao Xu",
        "Qiuzhi Liu",
        "Xingyu Chen",
        "Yue Wang",
        "Linfeng Song",
        "Dian Yu",
        "Zhenwen Liang",
        "Wenxuan Wang",
        "Zhuosheng Zhang",
        "Rui Wang",
        "Zhaopeng Tu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.",
      "pdf_url": "http://arxiv.org/pdf/2504.11456v1",
      "published": "2025-04-15T17:59:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11456v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Elucidating the Design Space of Multimodal Protein Language Models",
      "authors": [
        "Cheng-Yen Hsieh",
        "Xinyou Wang",
        "Daiheng Zhang",
        "Dongyu Xue",
        "Fei Ye",
        "Shujian Huang",
        "Zaixiang Zheng",
        "Quanquan Gu"
      ],
      "abstract": "Multimodal protein language models (PLMs) integrate sequence and token-based\nstructural information, serving as a powerful foundation for protein modeling,\ngeneration, and design. However, the reliance on tokenizing 3D structures into\ndiscrete tokens causes substantial loss of fidelity about fine-grained\nstructural details and correlations. In this paper, we systematically elucidate\nthe design space of multimodal PLMs to overcome their limitations. We identify\ntokenization loss and inaccurate structure token predictions by the PLMs as\nmajor bottlenecks. To address these, our proposed design space covers improved\ngenerative modeling, structure-aware architectures and representation learning,\nand data exploration. Our advancements approach finer-grained supervision,\ndemonstrating that token-based multimodal PLMs can achieve robust structural\nmodeling. The effective design methods dramatically improve the structure\ngeneration diversity, and notably, folding abilities of our 650M model by\nreducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B\nbaselines and on par with the specialized folding models.",
      "pdf_url": "http://arxiv.org/pdf/2504.11454v2",
      "published": "2025-04-15T17:59:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11454v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ]
    },
    {
      "title": "A Clean Slate for Offline Reinforcement Learning",
      "authors": [
        "Matthew Thomas Jackson",
        "Uljad Berdica",
        "Jarek Liesen",
        "Shimon Whiteson",
        "Jakob Nicolaus Foerster"
      ],
      "abstract": "Progress in offline reinforcement learning (RL) has been impeded by ambiguous\nproblem definitions and entangled algorithmic designs, resulting in\ninconsistent implementations, insufficient ablations, and unfair evaluations.\nAlthough offline RL explicitly avoids environment interaction, prior methods\nfrequently employ extensive, undocumented online evaluation for hyperparameter\ntuning, complicating method comparisons. Moreover, existing reference\nimplementations differ significantly in boilerplate code, obscuring their core\nalgorithmic contributions. We address these challenges by first introducing a\nrigorous taxonomy and a transparent evaluation protocol that explicitly\nquantifies online tuning budgets. To resolve opaque algorithmic design, we\nprovide clean, minimalistic, single-file implementations of various model-free\nand model-based offline RL methods, significantly enhancing clarity and\nachieving substantial speed-ups. Leveraging these streamlined implementations,\nwe propose Unifloral, a unified algorithm that encapsulates diverse prior\napproaches within a single, comprehensive hyperparameter space, enabling\nalgorithm development in a shared hyperparameter space. Using Unifloral with\nour rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR\n(model-free) and MoBRAC (model-based) - which substantially outperform\nestablished baselines. Our implementation is publicly available at\nhttps://github.com/EmptyJackson/unifloral.",
      "pdf_url": "http://arxiv.org/pdf/2504.11453v1",
      "published": "2025-04-15T17:59:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11453v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "TextArena",
      "authors": [
        "Leon Guertler",
        "Bobby Cheng",
        "Simon Yu",
        "Bo Liu",
        "Leshem Choshen",
        "Cheston Tan"
      ],
      "abstract": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
      "pdf_url": "http://arxiv.org/pdf/2504.11442v1",
      "published": "2025-04-15T17:55:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11442v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Greedy Restart Schedules: A Baseline for Dynamic Algorithm Selection on Numerical Black-box Optimization Problems",
      "authors": [
        "Lennart Sch√§permeier"
      ],
      "abstract": "In many optimization domains, there are multiple different solvers that\ncontribute to the overall state-of-the-art, each performing better on some, and\nworse on other types of problem instances. Meta-algorithmic approaches, such as\ninstance-based algorithm selection, configuration and scheduling, aim to close\nthis gap by extracting the most performance possible from a set of\n(configurable) optimizers. In this context, the best performing individual\nalgorithms are often hand-crafted hybrid heuristics which perform many restarts\nof fast local optimization approaches. However, data-driven techniques to\ncreate optimized restart schedules have not yet been extensively studied.\n  Here, we present a simple scheduling approach that iteratively selects the\nalgorithm performing best on the distribution of unsolved training problems at\ntime of selection, resulting in a problem-independent solver schedule. We\ndemonstrate our approach using well-known optimizers from numerical black-box\noptimization on the BBOB testbed, bridging much of the gap between single and\nvirtual best solver from the original portfolio across various evaluation\nprotocols. Our greedy restart schedule presents a powerful baseline for more\ncomplex dynamic algorithm selection models.",
      "pdf_url": "http://arxiv.org/pdf/2504.11440v1",
      "published": "2025-04-15T17:54:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11440v1",
      "categories": [
        "math.OC",
        "cs.AI"
      ]
    },
    {
      "title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models",
      "authors": [
        "Maria Teleki",
        "Xiangjue Dong",
        "Haoran Liu",
        "James Caverlee"
      ],
      "abstract": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault.",
      "pdf_url": "http://arxiv.org/pdf/2504.11431v1",
      "published": "2025-04-15T17:41:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11431v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.SI"
      ]
    },
    {
      "title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models",
      "authors": [
        "Xue Zhang",
        "Songming Zhang",
        "Yunlong Liang",
        "Fandong Meng",
        "Yufeng Chen",
        "Jinan Xu",
        "Jie Zhou"
      ],
      "abstract": "Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies.",
      "pdf_url": "http://arxiv.org/pdf/2504.11426v1",
      "published": "2025-04-15T17:38:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11426v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "ADT: Tuning Diffusion Models with Adversarial Supervision",
      "authors": [
        "Dazhong Shen",
        "Guanglu Song",
        "Yi Zhang",
        "Bingqi Ma",
        "Lujundong Li",
        "Dongzhi Jiang",
        "Zhuofan Zong",
        "Yu Liu"
      ],
      "abstract": "Diffusion models have achieved outstanding image generation by reversing a\nforward noising process to approximate true data distributions. During\ntraining, these models predict diffusion scores from noised versions of true\nsamples in a single forward pass, while inference requires iterative denoising\nstarting from white noise. This training-inference divergences hinder the\nalignment between inference and training data distributions, due to potential\nprediction biases and cumulative error accumulation. To address this problem,\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\nDiffusion Tuning (ADT), by stimulating the inference process during\noptimization and aligning the final outputs with training data by adversarial\nsupervision. Specifically, to achieve robust adversarial training, ADT features\na siamese-network discriminator with a fixed pre-trained backbone and\nlightweight trainable parameters, incorporates an image-to-image sampling\nstrategy to smooth discriminative difficulties, and preserves the original\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\nconstrain the backward-flowing path for back-propagating gradients along the\ninference path without incurring memory overload or gradient explosion.\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\ndemonstrate that ADT significantly improves both distribution alignment and\nimage quality.",
      "pdf_url": "http://arxiv.org/pdf/2504.11423v1",
      "published": "2025-04-15T17:37:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11423v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Embodied World Models Emerge from Navigational Task in Open-Ended Environments",
      "authors": [
        "Li Jin",
        "Liu Jia"
      ],
      "abstract": "Understanding how artificial systems can develop spatial awareness and\nreasoning has long been a challenge in AI research. Traditional models often\nrely on passive observation, but embodied cognition theory suggests that deeper\nunderstanding emerges from active interaction with the environment. This study\ninvestigates whether neural networks can autonomously internalize spatial\nconcepts through interaction, focusing on planar navigation tasks. Using Gated\nRecurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we\nshow that agents can learn to encode spatial properties like direction,\ndistance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS)\nto model the agent-environment interaction as a closed dynamical system,\nrevealing stable limit cycles that correspond to optimal navigation strategies.\nRidge Representation allows us to map navigation paths into a fixed-dimensional\nbehavioral space, enabling comparison with neural states. Canonical Correlation\nAnalysis (CCA) confirms strong alignment between these representations,\nsuggesting that the agent's neural states actively encode spatial knowledge.\nIntervention experiments further show that specific neural dimensions are\ncausally linked to navigation performance. This work provides an approach to\nbridging the gap between action and perception in AI, offering new insights\ninto building adaptive, interpretable models that can generalize across complex\nenvironments. The causal validation of neural representations also opens new\navenues for understanding and controlling the internal mechanisms of AI\nsystems, pushing the boundaries of how machines learn and reason in dynamic,\nreal-world scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2504.11419v1",
      "published": "2025-04-15T17:35:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11419v1",
      "categories": [
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Measures of Variability for Risk-averse Policy Gradient",
      "authors": [
        "Yudong Luo",
        "Yangchen Pan",
        "Jiaqi Tan",
        "Pascal Poupart"
      ],
      "abstract": "Risk-averse reinforcement learning (RARL) is critical for decision-making\nunder uncertainty, which is especially valuable in high-stake applications.\nHowever, most existing works focus on risk measures, e.g., conditional\nvalue-at-risk (CVaR), while measures of variability remain underexplored. In\nthis paper, we comprehensively study nine common measures of variability,\nnamely Variance, Gini Deviation, Mean Deviation, Mean-Median Deviation,\nStandard Deviation, Inter-Quantile Range, CVaR Deviation, Semi_Variance, and\nSemi_Standard Deviation. Among them, four metrics have not been previously\nstudied in RARL. We derive policy gradient formulas for these unstudied\nmetrics, improve gradient estimation for Gini Deviation, analyze their gradient\nproperties, and incorporate them with the REINFORCE and PPO frameworks to\npenalize the dispersion of returns.\n  Our empirical study reveals that variance-based metrics lead to unstable\npolicy updates. In contrast, CVaR Deviation and Gini Deviation show consistent\nperformance across different randomness and evaluation domains, achieving high\nreturns while effectively learning risk-averse policies. Mean Deviation and\nSemi_Standard Deviation are also competitive across different scenarios. This\nwork provides a comprehensive overview of variability measures in RARL,\noffering practical insights for risk-aware decision-making and guiding future\nresearch on risk metrics and RARL algorithms.",
      "pdf_url": "http://arxiv.org/pdf/2504.11412v1",
      "published": "2025-04-15T17:28:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11412v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-level Cellular Automata for FLIM networks",
      "authors": [
        "Felipe Crispim Salvagnini",
        "Jancarlo F. Gomes",
        "Cid A. N. Santos",
        "Silvio Jamil F. Guimar√£es",
        "Alexandre X. Falc√£o"
      ],
      "abstract": "The necessity of abundant annotated data and complex network architectures\npresents a significant challenge in deep-learning Salient Object Detection\n(deep SOD) and across the broader deep-learning landscape. This challenge is\nparticularly acute in medical applications in developing countries with limited\ncomputational resources. Combining modern and classical techniques offers a\npath to maintaining competitive performance while enabling practical\napplications. Feature Learning from Image Markers (FLIM) methodology empowers\nexperts to design convolutional encoders through user-drawn markers, with\nfilters learned directly from these annotations. Recent findings demonstrate\nthat coupling a FLIM encoder with an adaptive decoder creates a flyweight\nnetwork suitable for SOD, requiring significantly fewer parameters than\nlightweight models and eliminating the need for backpropagation. Cellular\nAutomata (CA) methods have proven successful in data-scarce scenarios but\nrequire proper initialization -- typically through user input, priors, or\nrandomness. We propose a practical intersection of these approaches: using FLIM\nnetworks to initialize CA states with expert knowledge without requiring user\ninteraction for each image. By decoding features from each level of a FLIM\nnetwork, we can initialize multiple CAs simultaneously, creating a multi-level\nframework. Our method leverages the hierarchical knowledge encoded across\ndifferent network layers, merging multiple saliency maps into a high-quality\nfinal output that functions as a CA ensemble. Benchmarks across two challenging\nmedical datasets demonstrate the competitiveness of our multi-level CA approach\ncompared to established models in the deep SOD literature.",
      "pdf_url": "http://arxiv.org/pdf/2504.11406v1",
      "published": "2025-04-15T17:22:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11406v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VideoPanda: Video Panoramic Diffusion with Multi-view Attention",
      "authors": [
        "Kevin Xie",
        "Amirmojtaba Sabour",
        "Jiahui Huang",
        "Despoina Paschalidou",
        "Greg Klar",
        "Umar Iqbal",
        "Sanja Fidler",
        "Xiaohui Zeng"
      ],
      "abstract": "High resolution panoramic video content is paramount for immersive\nexperiences in Virtual Reality, but is non-trivial to collect as it requires\nspecialized equipment and intricate camera setups. In this work, we introduce\nVideoPanda, a novel approach for synthesizing 360$^\\circ$ videos conditioned on\ntext or single-view video data. VideoPanda leverages multi-view attention\nlayers to augment a video diffusion model, enabling it to generate consistent\nmulti-view videos that can be combined into immersive panoramic content.\nVideoPanda is trained jointly using two conditions: text-only and single-view\nvideo, and supports autoregressive generation of long-videos. To overcome the\ncomputational burden of multi-view video generation, we randomly subsample the\nduration and camera views used during training and show that the model is able\nto gracefully generalize to generating more frames during inference. Extensive\nevaluations on both real-world and synthetic video datasets demonstrate that\nVideoPanda generates more realistic and coherent 360$^\\circ$ panoramas across\nall input conditions compared to existing methods. Visit the project website at\nhttps://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results.",
      "pdf_url": "http://arxiv.org/pdf/2504.11389v1",
      "published": "2025-04-15T16:58:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11389v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Trajectory Encoding Temporal Graph Networks",
      "authors": [
        "Jiafeng Xiong",
        "Rizos Sakellariou"
      ],
      "abstract": "Temporal Graph Networks (TGNs) have demonstrated significant success in\ndynamic graph tasks such as link prediction and node classification. Both tasks\ncomprise transductive settings, where the model predicts links among known\nnodes, and in inductive settings, where it generalises learned patterns to\npreviously unseen nodes. Existing TGN designs face a dilemma under these dual\nscenarios. Anonymous TGNs, which rely solely on temporal and structural\ninformation, offer strong inductive generalisation but struggle to distinguish\nknown nodes. In contrast, non-anonymous TGNs leverage node features to excel in\ntransductive tasks yet fail to adapt to new nodes. To address this challenge,\nwe propose Trajectory Encoding TGN (TETGN). Our approach introduces\nautomatically expandable node identifiers (IDs) as learnable temporal\npositional features and performs message passing over these IDs to capture each\nnode's historical context. By integrating this trajectory-aware module with a\nstandard TGN using multi-head attention, TETGN effectively balances\ntransductive accuracy with inductive generalisation. Experimental results on\nthree real-world datasets show that TETGN significantly outperforms strong\nbaselines on both link prediction and node classification tasks, demonstrating\nits ability to unify the advantages of anonymous and non-anonymous models for\ndynamic graph learning.",
      "pdf_url": "http://arxiv.org/pdf/2504.11386v1",
      "published": "2025-04-15T16:57:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11386v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Winner-Takes-All Mechanism for Event Generation",
      "authors": [
        "Yongkang Huo",
        "Fuvio Forni",
        "Rodolphe Sepulchre"
      ],
      "abstract": "We present a novel framework for central pattern generator design that\nleverages the intrinsic rebound excitability of neurons in combination with\nwinner-takes-all computation. Our approach unifies decision-making and rhythmic\npattern generation within a simple yet powerful network architecture that\nemploys all-to-all inhibitory connections enhanced by designable excitatory\ninteractions. This design offers significant advantages regarding ease of\nimplementation, adaptability, and robustness. We demonstrate its efficacy\nthrough a ring oscillator model, which exhibits adaptive phase and frequency\nmodulation, making the framework particularly promising for applications in\nneuromorphic systems and robotics.",
      "pdf_url": "http://arxiv.org/pdf/2504.11374v1",
      "published": "2025-04-15T16:40:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11374v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ]
    },
    {
      "title": "OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution",
      "authors": [
        "Lucio La Cava",
        "Andrea Tagarelli"
      ],
      "abstract": "Open Large Language Models (OLLMs) are increasingly leveraged in generative\nAI applications, posing new challenges for detecting their outputs. We propose\nOpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate\nmachine-generated text detectors on the Turing Test and Authorship Attribution\nproblems. OpenTuringBench focuses on a representative set of OLLMs, and\nfeatures a number of challenging evaluation tasks, including\nhuman/machine-manipulated texts, out-of-domain texts, and texts from previously\nunseen models. We also provide OTBDetector, a contrastive learning framework to\ndetect and attribute OLLM-based machine-generated texts. Results highlight the\nrelevance and varying degrees of difficulty of the OpenTuringBench tasks, with\nour detector achieving remarkable capabilities across the various tasks and\noutperforming most existing detectors. Resources are available on the\nOpenTuringBench Hugging Face repository at\nhttps://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench",
      "pdf_url": "http://arxiv.org/pdf/2504.11369v1",
      "published": "2025-04-15T16:36:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11369v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "physics.soc-ph"
      ]
    },
    {
      "title": "Teaching Large Language Models to Reason through Learning and Forgetting",
      "authors": [
        "Tianwei Ni",
        "Allen Nie",
        "Sapana Chaudhary",
        "Yao Liu",
        "Huzefa Rangwala",
        "Rasool Fakoor"
      ],
      "abstract": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$.",
      "pdf_url": "http://arxiv.org/pdf/2504.11364v1",
      "published": "2025-04-15T16:30:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11364v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks",
      "authors": [
        "Yupei Liu",
        "Yuqi Jia",
        "Jinyuan Jia",
        "Dawn Song",
        "Neil Zhenqiang Gong"
      ],
      "abstract": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, where an attacker injects prompts into their inputs to induce\nattacker-desired outputs. A detection method aims to determine whether a given\ninput is contaminated by an injected prompt. However, existing detection\nmethods have limited effectiveness against state-of-the-art attacks, let alone\nadaptive ones. In this work, we propose DataSentinel, a game-theoretic method\nto detect prompt injection attacks. Specifically, DataSentinel fine-tunes an\nLLM to detect inputs contaminated with injected prompts that are strategically\nadapted to evade detection. We formulate this as a minimax optimization\nproblem, with the objective of fine-tuning the LLM to detect strong adaptive\nattacks. Furthermore, we propose a gradient-based method to solve the minimax\noptimization problem by alternating between the inner max and outer min\nproblems. Our evaluation results on multiple benchmark datasets and LLMs show\nthat DataSentinel effectively detects both existing and adaptive prompt\ninjection attacks.",
      "pdf_url": "http://arxiv.org/pdf/2504.11358v1",
      "published": "2025-04-15T16:26:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11358v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Neural Networks for on-chip Model Predictive Control: a Method to Build Optimized Training Datasets and its application to Type-1 Diabetes",
      "authors": [
        "Alberto Castillo",
        "Elliot Pryor",
        "Anas El Fathi",
        "Boris Kovatchev",
        "Marc Breton"
      ],
      "abstract": "Training Neural Networks (NNs) to behave as Model Predictive Control (MPC)\nalgorithms is an effective way to implement them in constrained embedded\ndevices. By collecting large amounts of input-output data, where inputs\nrepresent system states and outputs are MPC-generated control actions, NNs can\nbe trained to replicate MPC behavior at a fraction of the computational cost.\nHowever, although the composition of the training data critically influences\nthe final NN accuracy, methods for systematically optimizing it remain\nunderexplored. In this paper, we introduce the concept of Optimally-Sampled\nDatasets (OSDs) as ideal training sets and present an efficient algorithm for\ngenerating them. An OSD is a parametrized subset of all the available data that\n(i) preserves existing MPC information up to a certain numerical resolution,\n(ii) avoids duplicate or near-duplicate states, and (iii) becomes saturated or\ncomplete. We demonstrate the effectiveness of OSDs by training NNs to replicate\nthe University of Virginia's MPC algorithm for automated insulin delivery in\nType-1 Diabetes, achieving a four-fold improvement in final accuracy. Notably,\ntwo OSD-trained NNs received regulatory clearance for clinical testing as the\nfirst NN-based control algorithm for direct human insulin dosing. This\nmethodology opens new pathways for implementing advanced optimizations on\nresource-constrained embedded platforms, potentially revolutionizing how\ncomplex algorithms are deployed.",
      "pdf_url": "http://arxiv.org/pdf/2504.11355v1",
      "published": "2025-04-15T16:25:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11355v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ]
    },
    {
      "title": "Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning",
      "authors": [
        "Haiming Wang",
        "Mert Unsal",
        "Xiaohan Lin",
        "Mantas Baksys",
        "Junqi Liu",
        "Marco Dos Santos",
        "Flood Sung",
        "Marina Vinyes",
        "Zhenzhe Ying",
        "Zekai Zhu",
        "Jianqiao Lu",
        "Hugues de Saxc√©",
        "Bolton Bailey",
        "Chendong Song",
        "Chenjun Xiao",
        "Dehao Zhang",
        "Ebony Zhang",
        "Frederick Pu",
        "Han Zhu",
        "Jiawei Liu",
        "Jonas Bayer",
        "Julien Michel",
        "Longhui Yu",
        "L√©o Dreyfus-Schmidt",
        "Lewis Tunstall",
        "Luigi Pagani",
        "Moreira Machado",
        "Pauline Bourigault",
        "Ran Wang",
        "Stanislas Polu",
        "Thibaut Barroyer",
        "Wen-Ding Li",
        "Yazhe Niu",
        "Yann Fleureau",
        "Yangyang Hu",
        "Zhouliang Yu",
        "Zihan Wang",
        "Zhilin Yang",
        "Zhengying Liu",
        "Jia Li"
      ],
      "abstract": "We introduce Kimina-Prover Preview, a large language model that pioneers a\nnovel reasoning-driven exploration paradigm for formal theorem proving, as\nshowcased in this preview release. Trained with a large-scale reinforcement\nlearning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong\nperformance in Lean 4 proof generation by employing a structured reasoning\npattern we term \\textit{formal reasoning pattern}. This approach allows the\nmodel to emulate human problem-solving strategies in Lean, iteratively\ngenerating and refining proof steps. Kimina-Prover sets a new state-of-the-art\non the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved\nbenchmark performance, our work yields several key insights: (1) Kimina-Prover\nexhibits high sample efficiency, delivering strong results even with minimal\nsampling (pass@1) and scaling effectively with computational budget, stemming\nfrom its unique reasoning pattern and RL training; (2) we demonstrate clear\nperformance scaling with model size, a trend previously unobserved for neural\ntheorem provers in formal mathematics; (3) the learned reasoning style,\ndistinct from traditional search algorithms, shows potential to bridge the gap\nbetween formal verification and informal mathematical intuition. We open source\ndistilled versions with 1.5B and 7B parameters of Kimina-Prover",
      "pdf_url": "http://arxiv.org/pdf/2504.11354v1",
      "published": "2025-04-15T16:23:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11354v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Explicit and Implicit Representations in AI-based 3D Reconstruction for Radiology: A systematic literature review",
      "authors": [
        "Yuezhe Yang",
        "Boyu Yang",
        "Yaqian Wang",
        "Yang He",
        "Xingbo Dong",
        "Zhe Jin"
      ],
      "abstract": "The demand for high-quality medical imaging in clinical practice and assisted\ndiagnosis has made 3D reconstruction in radiological imaging a key research\nfocus. Artificial intelligence (AI) has emerged as a promising approach to\nenhancing reconstruction accuracy while reducing acquisition and processing\ntime, thereby minimizing patient radiation exposure and discomfort and\nultimately benefiting clinical diagnosis. This review explores state-of-the-art\nAI-based 3D reconstruction algorithms in radiological imaging, categorizing\nthem into explicit and implicit approaches based on their underlying\nprinciples. Explicit methods include point-based, volume-based, and Gaussian\nrepresentations, while implicit methods encompass implicit prior embedding and\nneural radiance fields. Additionally, we examine commonly used evaluation\nmetrics and benchmark datasets. Finally, we discuss the current state of\ndevelopment, key challenges, and future research directions in this evolving\nfield. Our project available on: https://github.com/Bean-Young/AI4Med.",
      "pdf_url": "http://arxiv.org/pdf/2504.11349v1",
      "published": "2025-04-15T16:21:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11349v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "68T45",
        "I.4.5"
      ]
    },
    {
      "title": "Interpretable Hybrid-Rule Temporal Point Processes",
      "authors": [
        "Yunyang Cao",
        "Juekai Lin",
        "Hongye Wang",
        "Wenhao Li",
        "Bo Jin"
      ],
      "abstract": "Temporal Point Processes (TPPs) are widely used for modeling event sequences\nin various medical domains, such as disease onset prediction, progression\nanalysis, and clinical decision support. Although TPPs effectively capture\ntemporal dynamics, their lack of interpretability remains a critical challenge.\nRecent advancements have introduced interpretable TPPs. However, these methods\nfail to incorporate numerical features, thereby limiting their ability to\ngenerate precise predictions. To address this issue, we propose Hybrid-Rule\nTemporal Point Processes (HRTPP), a novel framework that integrates temporal\nlogic rules with numerical features, improving both interpretability and\npredictive accuracy in event modeling. HRTPP comprises three key components:\nbasic intensity for intrinsic event likelihood, rule-based intensity for\nstructured temporal dependencies, and numerical feature intensity for dynamic\nprobability modulation. To effectively discover valid rules, we introduce a\ntwo-phase rule mining strategy with Bayesian optimization. To evaluate our\nmethod, we establish a multi-criteria assessment framework, incorporating rule\nvalidity, model fitting, and temporal predictive accuracy. Experimental results\non real-world medical datasets demonstrate that HRTPP outperforms\nstate-of-the-art interpretable TPPs in terms of predictive performance and\nclinical interpretability. In case studies, the rules extracted by HRTPP\nexplain the disease progression, offering valuable contributions to medical\ndiagnosis.",
      "pdf_url": "http://arxiv.org/pdf/2504.11344v1",
      "published": "2025-04-15T16:15:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11344v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce",
      "authors": [
        "Wei Xiong",
        "Jiarui Yao",
        "Yuhui Xu",
        "Bo Pang",
        "Lei Wang",
        "Doyen Sahoo",
        "Junnan Li",
        "Nan Jiang",
        "Tong Zhang",
        "Caiming Xiong",
        "Hanze Dong"
      ],
      "abstract": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
      "pdf_url": "http://arxiv.org/pdf/2504.11343v1",
      "published": "2025-04-15T16:15:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11343v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ]
    },
    {
      "title": "Transformer-Based Model for Cold Start Mitigation in FaaS Architecture",
      "authors": [
        "Alexandre Savi Fayam Mbala Mouen",
        "Jerry Lacmou Zeutouo",
        "Vianney Kengne Tchendji"
      ],
      "abstract": "Serverless architectures, particularly the Function as a Service (FaaS)\nmodel, have become a cornerstone of modern cloud computing due to their ability\nto simplify resource management and enhance application deployment agility.\nHowever, a significant challenge remains: the cold start problem. This\nphenomenon occurs when an idle FaaS function is invoked, requiring a full\ninitialization process, which increases latency and degrades user experience.\nExisting solutions for cold start mitigation are limited in terms of invocation\npattern generalization and implementation complexity. In this study, we propose\nan innovative approach leveraging Transformer models to mitigate the impact of\ncold starts in FaaS architectures. Our solution excels in accurately modeling\nfunction initialization delays and optimizing serverless system performance.\nExperimental evaluation using a public dataset provided by Azure demonstrates a\nsignificant reduction in cold start times, reaching up to 79\\% compared to\nconventional methods.",
      "pdf_url": "http://arxiv.org/pdf/2504.11338v1",
      "published": "2025-04-15T16:12:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11338v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "Looking beyond the next token",
      "authors": [
        "Abitha Thankaraj",
        "Yiding Jiang",
        "J. Zico Kolter",
        "Yonatan Bisk"
      ],
      "abstract": "The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm.",
      "pdf_url": "http://arxiv.org/pdf/2504.11336v1",
      "published": "2025-04-15T16:09:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11336v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Code Reborn AI-Driven Legacy Systems Modernization from COBOL to Java",
      "authors": [
        "Gopichand Bandarupalli"
      ],
      "abstract": "This study investigates AI-driven modernization of legacy COBOL code into\nJava, addressing a critical challenge in aging software systems. Leveraging the\nLegacy COBOL 2024 Corpus -- 50,000 COBOL files from public and enterprise\nsources -- Java parses the code, AI suggests upgrades, and React visualizes\ngains. Achieving 93% accuracy, complexity drops 35% (from 18 to 11.7) and\ncoupling 33% (from 8 to 5.4), surpassing manual efforts (75%) and rule-based\ntools (82%). The approach offers a scalable path to rejuvenate COBOL systems,\nvital for industries like banking and insurance.",
      "pdf_url": "http://arxiv.org/pdf/2504.11335v1",
      "published": "2025-04-15T16:07:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11335v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints",
      "authors": [
        "Ruicheng Ao",
        "Gan Luo",
        "David Simchi-Levi",
        "Xinshang Wang"
      ],
      "abstract": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
      "pdf_url": "http://arxiv.org/pdf/2504.11320v1",
      "published": "2025-04-15T16:00:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11320v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "title": "CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection",
      "authors": [
        "Jincheng Kang",
        "Yi Cen",
        "Yigang Cen",
        "Ke Wang",
        "Yuhan Liu"
      ],
      "abstract": "Wood defect detection is critical for ensuring quality control in the wood\nprocessing industry. However, current industrial applications face two major\nchallenges: traditional methods are costly, subjective, and labor-intensive,\nwhile mainstream deep learning models often struggle to balance detection\naccuracy and computational efficiency for edge deployment. To address these\nissues, this study proposes CFIS-YOLO, a lightweight object detection model\noptimized for edge devices. The model introduces an enhanced C2f structure, a\ndynamic feature recombination module, and a novel loss function that\nincorporates auxiliary bounding boxes and angular constraints. These\ninnovations improve multi-scale feature fusion and small object localization\nwhile significantly reducing computational overhead. Evaluated on a public wood\ndefect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of\n77.5\\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON\nBM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to\n17.3\\% of the original implementation, and incurs only a 0.5 percentage point\ndrop in mAP. These results demonstrate that CFIS-YOLO is a practical and\neffective solution for real-world wood defect detection in resource-constrained\nenvironments.",
      "pdf_url": "http://arxiv.org/pdf/2504.11305v1",
      "published": "2025-04-15T15:45:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11305v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Learning to Be A Doctor: Searching for Effective Medical Agent Architectures",
      "authors": [
        "Yangyang Zhuang",
        "Wenjia Jiang",
        "Jiayu Zhang",
        "Ze Yang",
        "Joey Tianyi Zhou",
        "Chi Zhang"
      ],
      "abstract": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments.",
      "pdf_url": "http://arxiv.org/pdf/2504.11301v1",
      "published": "2025-04-15T15:44:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11301v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation",
      "authors": [
        "Michal Lukasik",
        "Lin Chen",
        "Harikrishna Narasimhan",
        "Aditya Krishna Menon",
        "Wittawat Jitkrittum",
        "Felix X. Yu",
        "Sashank J. Reddi",
        "Gang Fu",
        "Mohammadhossein Bateni",
        "Sanjiv Kumar"
      ],
      "abstract": "Bipartite ranking is a fundamental supervised learning problem, with the goal\nof learning a ranking over instances with maximal area under the ROC curve\n(AUC) against a single binary target label. However, one may often observe\nmultiple binary target labels, e.g., from distinct human annotators. How can\none synthesize such labels into a single coherent ranking? In this work, we\nformally analyze two approaches to this problem -- loss aggregation and label\naggregation -- by characterizing their Bayes-optimal solutions. Based on this,\nwe show that while both methods can yield Pareto-optimal solutions, loss\naggregation can exhibit label dictatorship: one can inadvertently (and\nundesirably) favor one label over others. This suggests that label aggregation\ncan be preferable to loss aggregation, which we empirically verify.",
      "pdf_url": "http://arxiv.org/pdf/2504.11284v1",
      "published": "2025-04-15T15:25:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11284v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "stat.ML"
      ]
    },
    {
      "title": "Single-Input Multi-Output Model Merging: Leveraging Foundation Models for Dense Multi-Task Learning",
      "authors": [
        "Juan Garcia Giraldo",
        "Nikolaos Dimitriadis",
        "Ke Wang",
        "Pascal Frossard"
      ],
      "abstract": "Model merging is a flexible and computationally tractable approach to merge\nsingle-task checkpoints into a multi-task model. Prior work has solely focused\non constrained multi-task settings where there is a one-to-one mapping between\na sample and a task, overlooking the paradigm where multiple tasks may operate\non the same sample, e.g., scene understanding. In this paper, we focus on the\nmulti-task setting with single-input-multiple-outputs (SIMO) and show that it\nqualitatively differs from the single-input-single-output model merging\nsettings studied in the literature due to the existence of task-specific\ndecoders and diverse loss objectives. We identify that existing model merging\nmethods lead to significant performance degradation, primarily due to\nrepresentation misalignment between the merged encoder and task-specific\ndecoders. We propose two simple and efficient fixes for the SIMO setting to\nre-align the feature representation after merging. Compared to joint\nfine-tuning, our approach is computationally effective and flexible, and sheds\nlight into identifying task relationships in an offline manner. Experiments on\nNYUv2, Cityscapes, and a subset of the Taskonomy dataset demonstrate: (1) task\narithmetic suffices to enable multi-task capabilities; however, the\nrepresentations generated by the merged encoder has to be re-aligned with the\ntask-specific heads; (2) the proposed architecture rivals traditional\nmulti-task learning in performance but requires fewer samples and training\nsteps by leveraging the existence of task-specific models.",
      "pdf_url": "http://arxiv.org/pdf/2504.11268v1",
      "published": "2025-04-15T15:10:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11268v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DeepSelective: Feature Gating and Representation Matching for Interpretable Clinical Prediction",
      "authors": [
        "Ruochi Zhang",
        "Qian Yang",
        "Xiaoyang Wang",
        "Haoran Wu",
        "Qiong Zhou",
        "Yu Wang",
        "Kewei Li",
        "Yueying Wang",
        "Yusi Fan",
        "Jiale Zhang",
        "Lan Huang",
        "Chang Liu",
        "Fengfeng Zhou"
      ],
      "abstract": "The rapid accumulation of Electronic Health Records (EHRs) has transformed\nhealthcare by providing valuable data that enhance clinical predictions and\ndiagnoses. While conventional machine learning models have proven effective,\nthey often lack robust representation learning and depend heavily on\nexpert-crafted features. Although deep learning offers powerful solutions, it\nis often criticized for its lack of interpretability. To address these\nchallenges, we propose DeepSelective, a novel end to end deep learning\nframework for predicting patient prognosis using EHR data, with a strong\nemphasis on enhancing model interpretability. DeepSelective combines data\ncompression techniques with an innovative feature selection approach,\nintegrating custom-designed modules that work together to improve both accuracy\nand interpretability. Our experiments demonstrate that DeepSelective not only\nenhances predictive accuracy but also significantly improves interpretability,\nmaking it a valuable tool for clinical decision-making. The source code is\nfreely available at http://www.healthinformaticslab.org/supp/resources.php .",
      "pdf_url": "http://arxiv.org/pdf/2504.11264v1",
      "published": "2025-04-15T15:04:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11264v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Rollout-Based Algorithm and Reward Function for Efficient Resource Allocation in Business Processes",
      "authors": [
        "Jeroen Middelhuis",
        "Zaharah Bukhsh",
        "Ivo Adan",
        "Remco Dijkman"
      ],
      "abstract": "Resource allocation plays a critical role in minimizing cycle time and\nimproving the efficiency of business processes. Recently, Deep Reinforcement\nLearning (DRL) has emerged as a powerful tool to optimize resource allocation\npolicies in business processes. In the DRL framework, an agent learns a policy\nthrough interaction with the environment, guided solely by reward signals that\nindicate the quality of its decisions. However, existing algorithms are not\nsuitable for dynamic environments such as business processes. Furthermore,\nexisting DRL-based methods rely on engineered reward functions that approximate\nthe desired objective, but a misalignment between reward and objective can lead\nto undesired decisions or suboptimal policies. To address these issues, we\npropose a rollout-based DRL algorithm and a reward function to optimize the\nobjective directly. Our algorithm iteratively improves the policy by evaluating\nexecution trajectories following different actions. Our reward function\ndirectly decomposes the objective function of minimizing the mean cycle time.\nMaximizing our reward function guarantees that the objective function is\nminimized without requiring extensive reward engineering. The results show that\nour method consistently learns the optimal policy in all six evaluated business\nprocesses, outperforming the state-of-the-art algorithm that can only learn the\noptimal policy in two of the evaluated processes.",
      "pdf_url": "http://arxiv.org/pdf/2504.11250v1",
      "published": "2025-04-15T14:46:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11250v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Respiratory Inhaler Sound Event Classification Using Self-Supervised Learning",
      "authors": [
        "Davoud Shariat Panah",
        "Alessandro N Franciosi",
        "Cormac McCarthy",
        "Andrew Hines"
      ],
      "abstract": "Asthma is a chronic respiratory condition that affects millions of people\nworldwide. While this condition can be managed by administering controller\nmedications through handheld inhalers, clinical studies have shown low\nadherence to the correct inhaler usage technique. Consequently, many patients\nmay not receive the full benefit of their medication. Automated classification\nof inhaler sounds has recently been studied to assess medication adherence.\nHowever, the existing classification models were typically trained using data\nfrom specific inhaler types, and their ability to generalize to sounds from\ndifferent inhalers remains unexplored. In this study, we adapted the wav2vec\n2.0 self-supervised learning model for inhaler sound classification by\npre-training and fine-tuning this model on inhaler sounds. The proposed model\nshows a balanced accuracy of 98% on a dataset collected using a dry powder\ninhaler and smartwatch device. The results also demonstrate that re-finetuning\nthis model on minimal data from a target inhaler is a promising approach to\nadapting a generic inhaler sound classification model to a different inhaler\ndevice and audio capture hardware. This is the first study in the field to\ndemonstrate the potential of smartwatches as assistive technologies for the\npersonalized monitoring of inhaler adherence using machine learning models.",
      "pdf_url": "http://arxiv.org/pdf/2504.11246v1",
      "published": "2025-04-15T14:44:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11246v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Influence Maximization in Temporal Social Networks with a Cold-Start Problem: A Supervised Approach",
      "authors": [
        "Laixin Xie",
        "Ying Zhang",
        "Xiyuan Wang",
        "Shiyi Liu",
        "Shenghan Gao",
        "Xingxing Xing",
        "Wei Wan",
        "Haipeng Zhang",
        "Quan Li"
      ],
      "abstract": "Influence Maximization (IM) in temporal graphs focuses on identifying\ninfluential \"seeds\" that are pivotal for maximizing network expansion. We\nadvocate defining these seeds through Influence Propagation Paths (IPPs), which\nis essential for scaling up the network. Our focus lies in efficiently labeling\nIPPs and accurately predicting these seeds, while addressing the\noften-overlooked cold-start issue prevalent in temporal networks. Our strategy\nintroduces a motif-based labeling method and a tensorized Temporal Graph\nNetwork (TGN) tailored for multi-relational temporal graphs, bolstering\nprediction accuracy and computational efficiency. Moreover, we augment\ncold-start nodes with new neighbors from historical data sharing similar IPPs.\nThe recommendation system within an online team-based gaming environment\npresents subtle impact on the social network, forming multi-relational (i.e.,\nweak and strong) temporal graphs for our empirical IM study. We conduct offline\nexperiments to assess prediction accuracy and model training efficiency,\ncomplemented by online A/B testing to validate practical network growth and the\neffectiveness in addressing the cold-start issue.",
      "pdf_url": "http://arxiv.org/pdf/2504.11245v1",
      "published": "2025-04-15T14:44:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11245v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG",
      "authors": [
        "Balahari Vignesh Balu",
        "Florian Geissler",
        "Francesco Carella",
        "Joao-Vitor Zacchi",
        "Josef Jiru",
        "Nuria Mata",
        "Reinhard Stolle"
      ],
      "abstract": "We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods.",
      "pdf_url": "http://arxiv.org/pdf/2504.11243v1",
      "published": "2025-04-15T14:43:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11243v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs",
      "authors": [
        "Chang Yang",
        "Ruiyu Wang",
        "Junzhe Jiang",
        "Qi Jiang",
        "Qinggang Zhang",
        "Yanchen Deng",
        "Shuxin Li",
        "Shuyue Hu",
        "Bo Li",
        "Florian T. Pokorny",
        "Xiao Huang",
        "Xinrun Wang"
      ],
      "abstract": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI).",
      "pdf_url": "http://arxiv.org/pdf/2504.11239v1",
      "published": "2025-04-15T14:40:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11239v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Diversity-Driven Learning: Tackling Spurious Correlations and Data Heterogeneity in Federated Models",
      "authors": [
        "Gergely D. N√©meth",
        "Eros Fan√¨",
        "Yeat Jeng Ng",
        "Barbara Caputo",
        "Miguel √Ångel Lozano",
        "Nuria Oliver",
        "Novi Quadrianto"
      ],
      "abstract": "Federated Learning (FL) enables decentralized training of machine learning\nmodels on distributed data while preserving privacy. However, in real-world FL\nsettings, client data is often non-identically distributed and imbalanced,\nresulting in statistical data heterogeneity which impacts the generalization\ncapabilities of the server's model across clients, slows convergence and\nreduces performance. In this paper, we address this challenge by first\nproposing a characterization of statistical data heterogeneity by means of 6\nmetrics of global and client attribute imbalance, class imbalance, and spurious\ncorrelations. Next, we create and share 7 computer vision datasets for binary\nand multiclass image classification tasks in Federated Learning that cover a\nbroad range of statistical data heterogeneity and hence simulate real-world\nsituations. Finally, we propose FedDiverse, a novel client selection algorithm\nin FL which is designed to manage and leverage data heterogeneity across\nclients by promoting collaboration between clients with complementary data\ndistributions. Experiments on the seven proposed FL datasets demonstrate\nFedDiverse's effectiveness in enhancing the performance and robustness of a\nvariety of FL methods while having low communication and computational\noverhead.",
      "pdf_url": "http://arxiv.org/pdf/2504.11216v1",
      "published": "2025-04-15T14:20:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11216v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Mutual Understanding between People and Systems via Neurosymbolic AI and Knowledge Graphs",
      "authors": [
        "Irene Celino",
        "Mario Scrocca",
        "Agnese Chiatti"
      ],
      "abstract": "This chapter investigates the concept of mutual understanding between humans\nand systems, positing that Neuro-symbolic Artificial Intelligence (NeSy AI)\nmethods can significantly enhance this mutual understanding by leveraging\nexplicit symbolic knowledge representations with data-driven learning models.\nWe start by introducing three critical dimensions to characterize mutual\nunderstanding: sharing knowledge, exchanging knowledge, and governing\nknowledge. Sharing knowledge involves aligning the conceptual models of\ndifferent agents to enable a shared understanding of the domain of interest.\nExchanging knowledge relates to ensuring the effective and accurate\ncommunication between agents. Governing knowledge concerns establishing rules\nand processes to regulate the interaction between agents. Then, we present\nseveral different use case scenarios that demonstrate the application of NeSy\nAI and Knowledge Graphs to aid meaningful exchanges between human, artificial,\nand robotic agents. These scenarios highlight both the potential and the\nchallenges of combining top-down symbolic reasoning with bottom-up neural\nlearning, guiding the discussion of the coverage provided by current solutions\nalong the dimensions of sharing, exchanging, and governing knowledge.\nConcurrently, this analysis facilitates the identification of gaps and less\ndeveloped aspects in mutual understanding to address in future research.",
      "pdf_url": "http://arxiv.org/pdf/2504.11200v1",
      "published": "2025-04-15T13:57:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11200v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Efficient Distributed Retrieval-Augmented Generation for Enhancing Language Model Performance",
      "authors": [
        "Shangyu Liu",
        "Zhenzhe Zheng",
        "Xiaoyao Huang",
        "Fan Wu",
        "Guihai Chen",
        "Jie Wu"
      ],
      "abstract": "Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead.",
      "pdf_url": "http://arxiv.org/pdf/2504.11197v2",
      "published": "2025-04-15T13:53:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11197v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.IR"
      ]
    },
    {
      "title": "Enhancing multimodal analogical reasoning with Logic Augmented Generation",
      "authors": [
        "Anna Sofia Lippolis",
        "Andrea Giovanni Nuzzolese",
        "Aldo Gangemi"
      ],
      "abstract": "Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods.",
      "pdf_url": "http://arxiv.org/pdf/2504.11190v1",
      "published": "2025-04-15T13:47:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11190v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items",
      "authors": [
        "Minjie Zou",
        "Sahana Srinivasan",
        "Thaddaeus Wai Soon Lo",
        "Ke Zou",
        "Gabriel Dawei Yang",
        "Xuguang Ai",
        "Hyunjae Kim",
        "Maxwell Singer",
        "Fares Antaki",
        "Kelvin Li",
        "Robert Chang",
        "Marcus Tan",
        "David Ziyou Chen",
        "Dianbo Liu",
        "Qingyu Chen",
        "Yih Chung Tham"
      ],
      "abstract": "Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications.",
      "pdf_url": "http://arxiv.org/pdf/2504.11186v1",
      "published": "2025-04-15T13:42:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11186v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations",
      "authors": [
        "Liangbo Ning",
        "Wenqi Fan",
        "Qing Li"
      ],
      "abstract": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys)\nhas dramatically advanced personalized recommendations and drawn extensive\nattention. Despite the impressive progress, the safety of LLM-based RecSys\nagainst backdoor attacks remains largely under-explored. In this paper, we\nraise a new problem: Can a backdoor with a specific trigger be injected into\nLLM-based Recsys, leading to the manipulation of the recommendation responses\nwhen the backdoor trigger is appended to an item's title? To investigate the\nvulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new\nattack framework termed Backdoor Injection Poisoning for RecSys (BadRec).\nBadRec perturbs the items' titles with triggers and employs several fake users\nto interact with these items, effectively poisoning the training set and\ninjecting backdoors into LLM-based RecSys. Comprehensive experiments reveal\nthat poisoning just 1% of the training data with adversarial examples is\nsufficient to successfully implant backdoors, enabling manipulation of\nrecommendations. To further mitigate such a security threat, we propose a\nuniversal defense strategy called Poison Scanner (P-Scanner). Specifically, we\nintroduce an LLM-based poison scanner to detect the poisoned items by\nleveraging the powerful language understanding and rich knowledge of LLMs. A\ntrigger augmentation agent is employed to generate diverse synthetic triggers\nto guide the poison scanner in learning domain-specific knowledge of the\npoisoned item detection task. Extensive experiments on three real-world\ndatasets validate the effectiveness of the proposed P-Scanner.",
      "pdf_url": "http://arxiv.org/pdf/2504.11182v1",
      "published": "2025-04-15T13:37:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11182v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
      "authors": [
        "Johannes Jakubik",
        "Felix Yang",
        "Benedikt Blumenstiel",
        "Erik Scheurer",
        "Rocco Sedona",
        "Stefano Maurogiovanni",
        "Jente Bosmans",
        "Nikolaos Dionelis",
        "Valerio Marsocci",
        "Niklas Kopp",
        "Rahul Ramachandran",
        "Paolo Fraccaro",
        "Thomas Brunschwiler",
        "Gabriele Cavallaro",
        "Juan Bernabe-Moreno",
        "Nicolas Long√©p√©"
      ],
      "abstract": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license.",
      "pdf_url": "http://arxiv.org/pdf/2504.11171v1",
      "published": "2025-04-15T13:17:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11171v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos",
      "authors": [
        "Laura De Grazia",
        "Pol Pastells",
        "Mauro V√°zquez Chas",
        "Desmond Elliott",
        "Danae S√°nchez Villegas",
        "Mireia Farr√∫s",
        "Mariona Taul√©"
      ],
      "abstract": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context.",
      "pdf_url": "http://arxiv.org/pdf/2504.11169v1",
      "published": "2025-04-15T13:16:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11169v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
      "authors": [
        "William Hackett",
        "Lewis Birch",
        "Stefan Trawicki",
        "Neeraj Suri",
        "Peter Garraghan"
      ],
      "abstract": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.",
      "pdf_url": "http://arxiv.org/pdf/2504.11168v1",
      "published": "2025-04-15T13:16:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11168v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ]
    },
    {
      "title": "DMAGaze: Gaze Estimation Based on Feature Disentanglement and Multi-Scale Attention",
      "authors": [
        "Haohan Chen",
        "Hongjia Liu",
        "Shiyong Lan",
        "Wenwu Wang",
        "Yixin Qiao",
        "Yao Li",
        "Guonan Deng"
      ],
      "abstract": "Gaze estimation, which predicts gaze direction, commonly faces the challenge\nof interference from complex gaze-irrelevant information in face images. In\nthis work, we propose DMAGaze, a novel gaze estimation framework that exploits\ninformation from facial images in three aspects: gaze-relevant global features\n(disentangled from facial image), local eye features (extracted from cropped\neye patch), and head pose estimation features, to improve overall performance.\nFirstly, we design a new continuous mask-based Disentangler to accurately\ndisentangle gaze-relevant and gaze-irrelevant information in facial images by\nachieving the dual-branch disentanglement goal through separately\nreconstructing the eye and non-eye regions. Furthermore, we introduce a new\ncascaded attention module named Multi-Scale Global Local Attention Module\n(MS-GLAM). Through a customized cascaded attention structure, it effectively\nfocuses on global and local information at multiple scales, further enhancing\nthe information from the Disentangler. Finally, the global gaze-relevant\nfeatures disentangled by the upper face branch, combined with head pose and\nlocal eye features, are passed through the detection head for high-precision\ngaze estimation. Our proposed DMAGaze has been extensively validated on two\nmainstream public datasets, achieving state-of-the-art performance.",
      "pdf_url": "http://arxiv.org/pdf/2504.11160v1",
      "published": "2025-04-15T13:08:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11160v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "C-SHAP for time series: An approach to high-level temporal explanations",
      "authors": [
        "Annemarie Jutte",
        "Faizan Ahmed",
        "Jeroen Linssen",
        "Maurice van Keulen"
      ],
      "abstract": "Time series are ubiquitous in domains such as energy forecasting, healthcare,\nand industry. Using AI systems, some tasks within these domains can be\nefficiently handled. Explainable AI (XAI) aims to increase the reliability of\nAI solutions by explaining model reasoning. For time series, many XAI methods\nprovide point- or sequence-based attribution maps. These methods explain model\nreasoning in terms of low-level patterns. However, they do not capture\nhigh-level patterns that may also influence model reasoning. We propose a\nconcept-based method to provide explanations in terms of these high-level\npatterns. In this paper, we present C-SHAP for time series, an approach which\ndetermines the contribution of concepts to a model outcome. We provide a\ngeneral definition of C-SHAP and present an example implementation using time\nseries decomposition. Additionally, we demonstrate the effectiveness of the\nmethodology through a use case from the energy domain.",
      "pdf_url": "http://arxiv.org/pdf/2504.11159v1",
      "published": "2025-04-15T13:06:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11159v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Divergence of Empirical Neural Tangent Kernel in Classification Problems",
      "authors": [
        "Zixiong Yu",
        "Songtao Tian",
        "Guhan Chen"
      ],
      "abstract": "This paper demonstrates that in classification problems, fully connected\nneural networks (FCNs) and residual neural networks (ResNets) cannot be\napproximated by kernel logistic regression based on the Neural Tangent Kernel\n(NTK) under overtraining (i.e., when training time approaches infinity).\nSpecifically, when using the cross-entropy loss, regardless of how large the\nnetwork width is (as long as it is finite), the empirical NTK diverges from the\nNTK on the training samples as training time increases. To establish this\nresult, we first demonstrate the strictly positive definiteness of the NTKs for\nmulti-layer FCNs and ResNets. Then, we prove that during training, % with the\ncross-entropy loss, the neural network parameters diverge if the smallest\neigenvalue of the empirical NTK matrix (Gram matrix) with respect to training\nsamples is bounded below by a positive constant. This behavior contrasts\nsharply with the lazy training regime commonly observed in regression problems.\nConsequently, using a proof by contradiction, we show that the empirical NTK\ndoes not uniformly converge to the NTK across all times on the training samples\nas the network width increases. We validate our theoretical results through\nexperiments on both synthetic data and the MNIST classification task. This\nfinding implies that NTK theory is not applicable in this context, with\nsignificant theoretical implications for understanding neural networks in\nclassification problems.",
      "pdf_url": "http://arxiv.org/pdf/2504.11130v1",
      "published": "2025-04-15T12:30:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11130v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Fine-Tuning Large Language Models on Quantum Optimization Problems for Circuit Generation",
      "authors": [
        "Linus Jern",
        "Valter Uotila",
        "Cong Yu",
        "Bo Zhao"
      ],
      "abstract": "Large language models (LLM) have achieved remarkable outcomes in addressing\ncomplex problems, including math, coding, and analyzing large amounts of\nscientific reports. Yet few works have explored the potential of LLM in quantum\ncomputing. The most challenging problem is how to leverage LLMs to\nautomatically generate quantum circuits at a large scale. In this paper, we\naddress such a challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. In particular, we investigate the mechanisms to\ngenerate training data sets and construct the end-to-end pipeline to fine-tune\npre-trained LLMs that produce parameterized quantum circuits for optimization\nproblems. We have prepared 14,000 quantum circuits covering a substantial part\nof the quantum optimization landscape: 12 optimization problem instances and\ntheir optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can\nconstruct syntactically correct parametrized quantum circuits in the most\nrecent OpenQASM 3.0. We have evaluated the quality of the parameters by\ncomparing them to the optimized expectation values and distributions. Our\nevaluation shows that the fine-tuned LLM outperforms state-of-the-art models\nand that the parameters are better than random. The LLM-generated parametrized\ncircuits and initial parameters can be used as a starting point for further\noptimization, \\emph{e.g.,} templates in quantum machine learning and the\nbenchmark for compilers and hardware.",
      "pdf_url": "http://arxiv.org/pdf/2504.11109v1",
      "published": "2025-04-15T11:56:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11109v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification",
      "authors": [
        "Maximilian G. Schuh",
        "Joshua Hesse",
        "Stephan A. Sieber"
      ],
      "abstract": "Antibiotic resistance presents a growing global health crisis, demanding new\ntherapeutic strategies that target novel bacterial mechanisms. Recent advances\nin protein structure prediction and machine learning-driven molecule generation\noffer a promising opportunity to accelerate drug discovery. However, practical\nguidance on selecting and integrating these models into real-world pipelines\nremains limited. In this study, we develop an end-to-end, artificial\nintelligence-guided antibiotic discovery pipeline that spans target\nidentification to compound realization. We leverage structure-based clustering\nacross predicted proteomes of multiple pathogens to identify conserved,\nessential, and non-human-homologous targets. We then systematically evaluate\nsix leading 3D-structure-aware generative models$\\unicode{x2014}$spanning\ndiffusion, autoregressive, graph neural network, and language model\narchitectures$\\unicode{x2014}$on their usability, chemical validity, and\nbiological relevance. Rigorous post-processing filters and commercial analogue\nsearches reduce over 100 000 generated compounds to a focused, synthesizable\nset. Our results highlight DeepBlock and TamGen as top performers across\ndiverse criteria, while also revealing critical trade-offs between model\ncomplexity, usability, and output quality. This work provides a comparative\nbenchmark and blueprint for deploying artificial intelligence in early-stage\nantibiotic development.",
      "pdf_url": "http://arxiv.org/pdf/2504.11091v1",
      "published": "2025-04-15T11:36:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.11091v1",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ]
    }
  ]
}