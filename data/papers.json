{
  "last_updated": "2025-11-19T00:52:24.502606",
  "papers": [
    {
      "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
      "authors": [
        "Zhongang Cai",
        "Ruisi Wang",
        "Chenyang Gu",
        "Fanyi Pu",
        "Junxiang Xu",
        "Yubo Wang",
        "Wanqi Yin",
        "Zhitao Yang",
        "Chen Wei",
        "Qingping Sun",
        "Tongxi Zhou",
        "Jiaqi Li",
        "Hui En Pang",
        "Oscar Qian",
        "Yukun Wei",
        "Zhiqian Lin",
        "Xuanke Shi",
        "Kewang Deng",
        "Xiaoyang Han",
        "Zukai Chen",
        "Xiangyu Fan",
        "Hanming Deng",
        "Lewei Lu",
        "Liang Pan",
        "Bo Li",
        "Ziwei Liu",
        "Quan Wang",
        "Dahua Lin",
        "Lei Yang"
      ],
      "abstract": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
      "pdf_url": "https://arxiv.org/pdf/2511.13719v1",
      "published": "2025-11-17T18:59:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13719v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.RO"
      ]
    },
    {
      "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
      "authors": [
        "Junwei Yu",
        "Trevor Darrell",
        "XuDong Wang"
      ],
      "abstract": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.",
      "pdf_url": "https://arxiv.org/pdf/2511.13714v1",
      "published": "2025-11-17T18:58:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13714v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness",
      "authors": [
        "Kiana Vu",
        "İsmet Selçuk Özer",
        "Phung Lai",
        "Zheng Wu",
        "Thilanka Munasinghe",
        "Jennifer Wei"
      ],
      "abstract": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.",
      "pdf_url": "https://arxiv.org/pdf/2511.13712v1",
      "published": "2025-11-17T18:57:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13712v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands",
      "authors": [
        "Jianglong Ye",
        "Lai Wei",
        "Guangqi Jiang",
        "Changwei Jing",
        "Xueyan Zou",
        "Xiaolong Wang"
      ],
      "abstract": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision",
      "pdf_url": "https://arxiv.org/pdf/2511.13710v1",
      "published": "2025-11-17T18:56:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13710v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
      "authors": [
        "Lavender Y. Jiang",
        "Angelica Chen",
        "Xu Han",
        "Xujin Chris Liu",
        "Radhika Dua",
        "Kevin Eaton",
        "Frederick Wolff",
        "Robert Steele",
        "Jeff Zhang",
        "Anton Alyakin",
        "Qingkai Pan",
        "Yanbing Chen",
        "Karl L. Sangwon",
        "Daniel A. Alber",
        "Jaden Stryker",
        "Jin Vivian Lee",
        "Yindalon Aphinyanaphongs",
        "Kyunghyun Cho",
        "Eric Karl Oermann"
      ],
      "abstract": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
      "pdf_url": "https://arxiv.org/pdf/2511.13703v1",
      "published": "2025-11-17T18:52:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13703v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification",
      "authors": [
        "Luyao Niu",
        "Nuoxian Huang"
      ],
      "abstract": "Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.",
      "pdf_url": "https://arxiv.org/pdf/2511.13702v1",
      "published": "2025-11-17T18:52:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13702v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers",
      "authors": [
        "Disha Varshney",
        "Samarth Garg",
        "Sarthak Tyagi",
        "Deeksha Varshney",
        "Nayan Deep",
        "Asif Ekbal"
      ],
      "abstract": "In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.",
      "pdf_url": "https://arxiv.org/pdf/2511.13685v1",
      "published": "2025-11-17T18:39:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13685v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process",
      "authors": [
        "Agnieszka Bieńkowska",
        "Jacek Małecki",
        "Alexander Mathiesen-Ohman",
        "Katarzyna Tworek"
      ],
      "abstract": "This article develops the concept of Person-AI bidirectional fit, defined as the continuously evolving, context-sensitive alignment-primarily cognitive, but also emotional and behavioral-between a human decision-maker and an artificial intelligence system. Grounded in contingency theory and quality theory, the study examines the role of P-AI fit in managerial decision-making through a proof-of-concept case study involving a real hiring process for a Senior AI Lead. Three decision pathways are compared: (1) independent evaluations by a CEO, CTO, and CSO; (2) an evaluation produced by an augmented human-AI symbiotic intelligence system (H3LIX-LAIZA); and (3) an assessment generated by a general-purpose large language model. The results reveal substantial role-based divergence in human judgments, high alignment between H3LIX-LAIZA and the CEOs implicit decision model-including ethical disqualification of a high-risk candidate and a critical false-positive recommendation from the LLMr. The findings demonstrate that higher P-AI fit, exemplified by the CEO H3LIX-LAIZA relationship, functions as a mechanism linking augmented symbiotic intelligence to accurate, trustworthy, and context-sensitive decisions. The study provides an initial verification of the P-AI fit construct and a proof-of-concept for H3LIX-LAIZA as an augmented human-AI symbiotic intelligence system.",
      "pdf_url": "https://arxiv.org/pdf/2511.13670v1",
      "published": "2025-11-17T18:22:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13670v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Weight-sparse transformers have interpretable circuits",
      "authors": [
        "Leo Gao",
        "Achyuta Rajaram",
        "Jacob Coxon",
        "Soham V. Govande",
        "Bowen Baker",
        "Dan Mossing"
      ],
      "abstract": "Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.",
      "pdf_url": "https://arxiv.org/pdf/2511.13653v1",
      "published": "2025-11-17T18:02:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13653v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
      "authors": [
        "Chunqiu Steven Xia",
        "Zhe Wang",
        "Yan Yang",
        "Yuxiang Wei",
        "Lingming Zhang"
      ],
      "abstract": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
      "pdf_url": "https://arxiv.org/pdf/2511.13646v1",
      "published": "2025-11-17T17:58:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13646v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures",
      "authors": [
        "Haohui Wang",
        "Jingyuan Qi",
        "Jianpeng Chen",
        "Jun Wu",
        "Lifu Huang",
        "Lecheng Zheng",
        "Kevin Choi",
        "Balaji Veeramani",
        "Edward Bowen",
        "Alison Hu",
        "Tyler Cody",
        "Dawei Zhou"
      ],
      "abstract": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.",
      "pdf_url": "https://arxiv.org/pdf/2511.13640v1",
      "published": "2025-11-17T17:53:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13640v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Mimicry: Preference Coherence in LLMs",
      "authors": [
        "Luhan Mikaelson",
        "Derek Shiller",
        "Hayley Clatterbuck"
      ],
      "abstract": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.",
      "pdf_url": "https://arxiv.org/pdf/2511.13630v1",
      "published": "2025-11-17T17:41:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13630v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product",
      "authors": [
        "Kaiwen Xue",
        "Chenglong Li",
        "Zhonghong Ou",
        "Guoxin Zhang",
        "Kaoyan Lu",
        "Shuai Lyu",
        "Yifan Zhu",
        "Ping Zong Junpeng Ding",
        "Xinyu Liu",
        "Qunlin Chen",
        "Weiwei Qin",
        "Yiran Shen",
        "Jiayi Cen"
      ],
      "abstract": "Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.",
      "pdf_url": "https://arxiv.org/pdf/2511.13626v1",
      "published": "2025-11-17T17:34:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13626v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization",
      "authors": [
        "Kaichi Irie",
        "Shuhei Watanabe",
        "Masaki Onishi"
      ],
      "abstract": "Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.",
      "pdf_url": "https://arxiv.org/pdf/2511.13625v1",
      "published": "2025-11-17T17:32:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13625v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Alpha Divergence Losses for Biometric Verification",
      "authors": [
        "Dimitrios Koutsianos",
        "Ladislav Mosner",
        "Yannis Panagakis",
        "Themos Stafylakis"
      ],
      "abstract": "Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.",
      "pdf_url": "https://arxiv.org/pdf/2511.13621v1",
      "published": "2025-11-17T17:27:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13621v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
      "authors": [
        "Jiacheng Chen",
        "Qianjia Cheng",
        "Fangchen Yu",
        "Haiyuan Wan",
        "Yuchen Zhang",
        "Shenghe Zheng",
        "Junchi Yao",
        "Qingyang Zhang",
        "Haonan He",
        "Yun Luo",
        "Yufeng Zhao",
        "Futing Wang",
        "Li Sheng",
        "Chengxing Xie",
        "Yuxin Zuo",
        "Yizhuo Li",
        "Wenxauan Zeng",
        "Yulun Wu",
        "Rui Huang",
        "Dongzhan Zhou",
        "Kai Chen",
        "Yu Qiao",
        "Lei Bai",
        "Yu Cheng",
        "Ning Ding",
        "Bowen Zhou",
        "Peng Ye",
        "Ganqu Cui"
      ],
      "abstract": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
      "pdf_url": "https://arxiv.org/pdf/2511.13612v1",
      "published": "2025-11-17T17:18:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13612v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Robust Client-Server Watermarking for Split Federated Learning",
      "authors": [
        "Jiaxiong Tang",
        "Zhengchunmin Dai",
        "Liantao Wu",
        "Peng Sun",
        "Honglong Chen",
        "Zhenfu Cao"
      ],
      "abstract": "Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\\%$ watermark detection rate ($p-value \\lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.",
      "pdf_url": "https://arxiv.org/pdf/2511.13598v1",
      "published": "2025-11-17T16:58:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13598v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Physics-Informed Neural Networks for Nonlinear Output Regulation",
      "authors": [
        "Sebastiano Mengozzi",
        "Giovanni B. Esposito",
        "Michelangelo Bin",
        "Andrea Acquaviva",
        "Andrea Bartolini",
        "Lorenzo Marconi"
      ],
      "abstract": "This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold π(w) and a feedforward input c(w) that render such manifold invariant. The pair (π(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates π(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.",
      "pdf_url": "https://arxiv.org/pdf/2511.13595v1",
      "published": "2025-11-17T16:55:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13595v1",
      "categories": [
        "eess.SY",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation",
      "authors": [
        "Hao Wang",
        "Yuanfeng Song",
        "Xiaoming Yin",
        "Xing Chen"
      ],
      "abstract": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2511.13590v1",
      "published": "2025-11-17T16:52:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13590v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Data-driven Acceleration of MPC with Guarantees",
      "authors": [
        "Agustin Castellano",
        "Shijie Pan",
        "Enrique Mallada"
      ],
      "abstract": "Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.",
      "pdf_url": "https://arxiv.org/pdf/2511.13588v1",
      "published": "2025-11-17T16:51:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13588v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "math.DS"
      ]
    },
    {
      "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
      "authors": [
        "Haotian Dong",
        "Ye Li",
        "Rongwei Lu",
        "Chen Tang",
        "Shu-Tao Xia",
        "Zhi Wang"
      ],
      "abstract": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
      "pdf_url": "https://arxiv.org/pdf/2511.13587v1",
      "published": "2025-11-17T16:50:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13587v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification",
      "authors": [
        "Linhan Zhou",
        "Shuang Li",
        "Neng Dong",
        "Yonghang Tai",
        "Yafei Zhang",
        "Huafeng Li"
      ],
      "abstract": "Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.",
      "pdf_url": "https://arxiv.org/pdf/2511.13575v1",
      "published": "2025-11-17T16:39:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13575v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction",
      "authors": [
        "Jingyi Zhao",
        "Daqian Shi",
        "Zhengda Wang",
        "Xiongfeng Tang",
        "Yanguo Qin"
      ],
      "abstract": "Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.",
      "pdf_url": "https://arxiv.org/pdf/2511.13565v1",
      "published": "2025-11-17T16:33:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13565v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models",
      "authors": [
        "Siyang Cheng",
        "Gaotian Liu",
        "Rui Mei",
        "Yilin Wang",
        "Kejia Zhang",
        "Kaishuo Wei",
        "Yuqi Yu",
        "Weiping Wen",
        "Xiaojie Wu",
        "Junhua Liu"
      ],
      "abstract": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \\textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.",
      "pdf_url": "https://arxiv.org/pdf/2511.13548v1",
      "published": "2025-11-17T16:19:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13548v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks",
      "authors": [
        "Md. Iqbal Hossain",
        "Afia Sajeeda",
        "Neeresh Kumar Perla",
        "Ming Shao"
      ],
      "abstract": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.",
      "pdf_url": "https://arxiv.org/pdf/2511.13545v1",
      "published": "2025-11-17T16:16:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13545v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop",
      "authors": [
        "Amirreza Mehrabi",
        "Jason Wade Morphew",
        "Breejha Quezada",
        "N. Sanjay Rebello"
      ],
      "abstract": "Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.",
      "pdf_url": "https://arxiv.org/pdf/2511.13542v1",
      "published": "2025-11-17T16:15:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13542v1",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CY",
        "stat.AP"
      ]
    },
    {
      "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety",
      "authors": [
        "Vesna Poprcova",
        "Iulia Lefter",
        "Matthias Wieser",
        "Martijn Warnier",
        "Frances Brazier"
      ],
      "abstract": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.",
      "pdf_url": "https://arxiv.org/pdf/2511.13530v1",
      "published": "2025-11-17T16:03:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13530v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets",
      "authors": [
        "Máté Gedeon",
        "Piroska Zsófia Barta",
        "Péter Mihajlik",
        "Tekla Etelka Gráczi",
        "Anna Kohári",
        "Katalin Mády"
      ],
      "abstract": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\\% on spontaneous and 4.8\\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\\% and 18.26\\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.",
      "pdf_url": "https://arxiv.org/pdf/2511.13529v1",
      "published": "2025-11-17T16:02:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13529v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models",
      "authors": [
        "Zhengda Wang",
        "Daqian Shi",
        "Jingyi Zhao",
        "Xiaolei Diao",
        "Xiongfeng Tang",
        "Yanguo Qin"
      ],
      "abstract": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.",
      "pdf_url": "https://arxiv.org/pdf/2511.13526v1",
      "published": "2025-11-17T16:00:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13526v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions",
      "authors": [
        "Zichong Wang",
        "Zhipeng Yin",
        "Roland H. C. Yap",
        "Wenbin Zhang"
      ],
      "abstract": "Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.",
      "pdf_url": "https://arxiv.org/pdf/2511.13525v1",
      "published": "2025-11-17T15:59:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13525v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
      "authors": [
        "Yuhang Peng",
        "Yizhou Pan",
        "Xinning He",
        "Jihaoyu Yang",
        "Xinyu Yin",
        "Han Wang",
        "Xiaoji Zheng",
        "Chao Gao",
        "Jiangtao Gong"
      ],
      "abstract": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
      "pdf_url": "https://arxiv.org/pdf/2511.13524v1",
      "published": "2025-11-17T15:58:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13524v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Naga: Vedic Encoding for Deep State Space Models",
      "authors": [
        "Melanie Schaller",
        "Nick Janssen",
        "Bodo Rosenhahn"
      ],
      "abstract": "This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.",
      "pdf_url": "https://arxiv.org/pdf/2511.13510v1",
      "published": "2025-11-17T15:43:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13510v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ]
    },
    {
      "title": "A Lexical Analysis of online Reviews on Human-AI Interactions",
      "authors": [
        "Parisa Arbab",
        "Xiaowen Fang"
      ],
      "abstract": "This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from G2.com, Producthunt.com, and Trustpilot.com, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.",
      "pdf_url": "https://arxiv.org/pdf/2511.13480v1",
      "published": "2025-11-17T15:17:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13480v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling",
      "authors": [
        "Adam Hazimeh",
        "Ke Wang",
        "Mark Collier",
        "Gilles Baechler",
        "Efi Kokiopoulou",
        "Pascal Frossard"
      ],
      "abstract": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.",
      "pdf_url": "https://arxiv.org/pdf/2511.13478v1",
      "published": "2025-11-17T15:16:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13478v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation",
      "authors": [
        "Zhipeng Ma",
        "Ali Rida Bahja",
        "Andreas Burgdorf",
        "André Pomp",
        "Tobias Meisen",
        "Bo Nørregaard Jørgensen",
        "Zheng Grace Ma"
      ],
      "abstract": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.",
      "pdf_url": "https://arxiv.org/pdf/2511.13476v1",
      "published": "2025-11-17T15:14:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13476v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Quick Red Fox gets the best Data Driven Classroom Interviews: A manual for an interview app and its associated methodology",
      "authors": [
        "Jaclyn Ocumpaugh",
        "Luc Paquette",
        "Ryan S. Baker",
        "Amanda Barany",
        "Jeff Ginger",
        "Nathan Casano",
        "Andres F. Zambrano",
        "Xiner Liu",
        "Zhanlan Wei",
        "Yiqui Zhou",
        "Qianhui Liu",
        "Stephen Hutt",
        "Alexandra M. A. Andres",
        "Nidhi Nasiar",
        "Camille Giordano",
        "Martin van Velsen",
        "Micheal Mogessi"
      ],
      "abstract": "Data Driven Classroom Interviews (DDCIs) are an interviewing technique that is facilitated by recent technological developments in the learning analytics community. DDCIs are short, targeted interviews that allow researchers to contextualize students' interactions with a digital learning environment (e.g., intelligent tutoring systems or educational games) while minimizing the amount of time that the researcher interrupts that learning experience, and focusing researcher time on the events they most want to focus on DDCIs are facilitated by a research tool called the Quick Red Fox (QRF)--an open-source server-client Android app that optimizes researcher time by directing interviewers to users that have just displayed an interesting behavior (previously defined by the research team). QRF integrates with existing student modeling technologies (e.g., behavior-sensing, affect-sensing, detection of self-regulated learning) to alert researchers to key moments in a learner's experience. This manual documents the tech while providing training on the processes involved in developing triggers and interview techniques; it also suggests methods of analyses.",
      "pdf_url": "https://arxiv.org/pdf/2511.13466v1",
      "published": "2025-11-17T15:08:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13466v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "Multi-task GINN-LP for Multi-target Symbolic Regression",
      "authors": [
        "Hussein Rajabu",
        "Lijun Qian",
        "Xishuang Dong"
      ],
      "abstract": "In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.",
      "pdf_url": "https://arxiv.org/pdf/2511.13463v1",
      "published": "2025-11-17T15:07:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13463v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop",
      "authors": [
        "Agnese Chiatti",
        "Lara Piccolo",
        "Sara Bernardini",
        "Matteo Matteucci",
        "Viola Schiaffonati"
      ],
      "abstract": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.",
      "pdf_url": "https://arxiv.org/pdf/2511.13458v1",
      "published": "2025-11-17T15:04:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13458v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure",
      "authors": [
        "Bin Liu",
        "Qinghao Zhao",
        "Yuxi Zhou",
        "Zhejun Sun",
        "Kaijie Lei",
        "Deyun Zhang",
        "Shijia Geng",
        "Shenda Hong"
      ],
      "abstract": "Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.",
      "pdf_url": "https://arxiv.org/pdf/2511.13457v1",
      "published": "2025-11-17T15:03:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13457v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes",
      "authors": [
        "Zhipeng Ma",
        "Bo Nørregaard Jørgensen",
        "Zheng Grace Ma"
      ],
      "abstract": "Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.13444v1",
      "published": "2025-11-17T14:50:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13444v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline",
      "authors": [
        "Rui Zuo",
        "Qinyue Tong",
        "Zhe-Ming Lu",
        "Ziqian Lu"
      ],
      "abstract": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.",
      "pdf_url": "https://arxiv.org/pdf/2511.13442v1",
      "published": "2025-11-17T14:49:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13442v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Multi-Table Retrieval Through Iterative Search",
      "authors": [
        "Allaa Boutaleb",
        "Bernd Amann",
        "Rafael Angarita",
        "Hubert Naacke"
      ],
      "abstract": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
      "pdf_url": "https://arxiv.org/pdf/2511.13418v1",
      "published": "2025-11-17T14:31:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13418v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DB",
        "cs.LG"
      ]
    },
    {
      "title": "PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation",
      "authors": [
        "Hanwen Hu",
        "Zimo Wen",
        "Shiyou Qian",
        "Jian Co"
      ],
      "abstract": "Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.",
      "pdf_url": "https://arxiv.org/pdf/2511.13414v1",
      "published": "2025-11-17T14:28:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13414v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence",
      "authors": [
        "Przemyslaw Chojecki"
      ],
      "abstract": "We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing \"baby AGI\" becomes Superintelligence intuition.",
      "pdf_url": "https://arxiv.org/pdf/2511.13411v1",
      "published": "2025-11-17T14:24:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13411v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing",
      "authors": [
        "Yuchen Bao",
        "Yiting Wang",
        "Wenjian Huang",
        "Haowei Wang",
        "Shen Chen",
        "Taiping Yao",
        "Shouhong Ding",
        "Jianguo Zhang"
      ],
      "abstract": "Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS",
      "pdf_url": "https://arxiv.org/pdf/2511.13399v1",
      "published": "2025-11-17T14:15:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13399v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)",
      "authors": [
        "Nikos Theodoridis",
        "Tim Brophy",
        "Reenu Mohandas",
        "Ganesh Sistu",
        "Fiachra Collins",
        "Anthony Scanlan",
        "Ciaran Eising"
      ],
      "abstract": "The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.",
      "pdf_url": "https://arxiv.org/pdf/2511.13397v1",
      "published": "2025-11-17T14:12:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13397v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Finding Kissing Numbers with Game-theoretic Reinforcement Learning",
      "authors": [
        "Chengdong Ma",
        "Théo Tao Zhaowei",
        "Pengyu Li",
        "Minghao Liu",
        "Haojun Chen",
        "Zihao Mao",
        "Yuan Cheng",
        "Yuan Qi",
        "Yaodong Yang"
      ],
      "abstract": "Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.",
      "pdf_url": "https://arxiv.org/pdf/2511.13391v1",
      "published": "2025-11-17T14:02:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13391v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model",
      "authors": [
        "Fei Kong"
      ],
      "abstract": "Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.",
      "pdf_url": "https://arxiv.org/pdf/2511.13387v1",
      "published": "2025-11-17T13:58:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13387v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models",
      "authors": [
        "Carlo Teo Pedretti",
        "Davide Picca",
        "Dario Rodighiero"
      ],
      "abstract": "Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce's semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.",
      "pdf_url": "https://arxiv.org/pdf/2511.13378v1",
      "published": "2025-11-17T13:52:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13378v1",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs",
      "authors": [
        "Prakrit Timilsina",
        "Anuj Nepal",
        "Rajan Kadel",
        "Robin Doss"
      ],
      "abstract": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.13373v1",
      "published": "2025-11-17T13:47:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.13373v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}