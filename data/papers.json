{
  "last_updated": "2025-06-05T00:52:43.084386",
  "papers": [
    {
      "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation",
      "authors": [
        "Yuanze Lin",
        "Yi-Wen Chen",
        "Yi-Hsuan Tsai",
        "Ronald Clark",
        "Ming-Hsuan Yang"
      ],
      "abstract": "Although diffusion-based models can generate high-quality and high-resolution\nvideo sequences from textual or image inputs, they lack explicit integration of\ngeometric cues when controlling scene lighting and visual appearance across\nframes. To address this limitation, we propose IllumiCraft, an end-to-end\ndiffusion framework accepting three complementary inputs: (1)\nhigh-dynamic-range (HDR) video maps for detailed lighting control; (2)\nsynthetically relit frames with randomized illumination changes (optionally\npaired with a static background reference image) to provide appearance cues;\nand (3) 3D point tracks that capture precise 3D geometry information. By\nintegrating the lighting, appearance, and geometry cues within a unified\ndiffusion architecture, IllumiCraft generates temporally coherent videos\naligned with user-defined prompts. It supports background-conditioned and\ntext-conditioned video relighting and provides better fidelity than existing\ncontrollable video generation methods. Project Page:\nhttps://yuanze-lin.me/IllumiCraft_page",
      "pdf_url": "http://arxiv.org/pdf/2506.03150v1",
      "published": "2025-06-03T17:59:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03150v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "Causal Estimation of Tokenisation Bias",
      "authors": [
        "Pietro Lesci",
        "Clara Meister",
        "Thomas Hofmann",
        "Andreas Vlachos",
        "Tiago Pimentel"
      ],
      "abstract": "Modern language models are typically trained over subword sequences, but\nultimately define probabilities over character-strings. Ideally, the choice of\nthe tokeniser -- which maps character-strings to subwords -- should not affect\nthe probability assigned to the underlying character-string; in practice, it\ndoes. We define this mismatch as tokenisation bias. In this work, we quantify\none particular type of tokenisation bias: the effect of including or not a\nsubword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the\nprobability a trained model assigns to the corresponding characters (i.e.,\n\\textit{``hello''}). Estimating this effect is challenging because each model\nis trained with only one tokeniser. We address this by framing tokenisation\nbias as a causal effect and estimating it using the regression discontinuity\ndesign. Specifically, we exploit the fact that tokenisation algorithms rank\nsubwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an\narbitrary cutoff point. As such, we can estimate a causal effect by comparing\nsimilar subwords around this cutoff. Experimentally, we find that tokenisation\nconsistently affects models' outputs across scales, vocabularies, and\ntokenisers. Notably, a subword's presence in a small model's vocabulary may\nincrease its characters' probability by up to 17 times, highlighting\ntokenisation as a key design choice in language modelling.",
      "pdf_url": "http://arxiv.org/pdf/2506.03149v1",
      "published": "2025-06-03T17:59:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03149v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation",
      "authors": [
        "Bin Lin",
        "Zongjian Li",
        "Xinhua Cheng",
        "Yuwei Niu",
        "Yang Ye",
        "Xianyi He",
        "Shenghai Yuan",
        "Wangbo Yu",
        "Shaodong Wang",
        "Yunyang Ge",
        "Yatian Pang",
        "Li Yuan"
      ],
      "abstract": "Although existing unified models achieve strong performance in\nvision-language understanding and text-to-image generation, they remain limited\nin addressing image perception and manipulation -- capabilities increasingly\ndemanded in practical applications. Recently, OpenAI introduced the powerful\nGPT-4o-Image model, which showcases advanced capabilities in comprehensive\nimage perception and manipulation, sparking widespread interest. Through\ncarefully designed experiments, we observe that GPT-4o-Image likely relies on\nsemantic encoders rather than VAEs for feature extraction, despite VAEs being\ncommonly regarded as crucial for image manipulation tasks. Inspired by this\ninsight, we propose UniWorld, a unified generative framework built upon\nsemantic features extracted from powerful multimodal large language models and\ncontrastive semantic encoders. Using only 2.7M training data, UniWorld achieves\nimpressive performance across diverse tasks, including image understanding,\ngeneration, manipulation, and perception. We fully open-source the UniWorld\nframework, including model weights, training and evaluation scripts, and\ndatasets to promote reproducibility and further research.",
      "pdf_url": "http://arxiv.org/pdf/2506.03147v2",
      "published": "2025-06-03T17:59:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03147v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM",
      "authors": [
        "Pralaypati Ta",
        "Sriram Venkatesaperumal",
        "Keerthi Ram",
        "Mohanasankar Sivaprakasam"
      ],
      "abstract": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.",
      "pdf_url": "http://arxiv.org/pdf/2506.03145v1",
      "published": "2025-06-03T17:59:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03145v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
      "authors": [
        "Qianhui Wu",
        "Kanzhi Cheng",
        "Rui Yang",
        "Chaoyun Zhang",
        "Jianwei Yang",
        "Huiqiang Jiang",
        "Jian Mu",
        "Baolin Peng",
        "Bo Qiao",
        "Reuben Tan",
        "Si Qin",
        "Lars Liden",
        "Qingwei Lin",
        "Huan Zhang",
        "Tong Zhang",
        "Jianbing Zhang",
        "Dongmei Zhang",
        "Jianfeng Gao"
      ],
      "abstract": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.",
      "pdf_url": "http://arxiv.org/pdf/2506.03143v1",
      "published": "2025-06-03T17:59:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03143v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
      "authors": [
        "Siqi Chen",
        "Xinyu Dong",
        "Haolei Xu",
        "Xingyu Wu",
        "Fei Tang",
        "Hang Zhang",
        "Yuchen Yan",
        "Linjuan Wu",
        "Wenqi Zhang",
        "Guiyang Hou",
        "Yongliang Shen",
        "Weiming Lu",
        "Yueting Zhuang"
      ],
      "abstract": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
      "pdf_url": "http://arxiv.org/pdf/2506.03139v1",
      "published": "2025-06-03T17:58:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03139v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models",
      "authors": [
        "Mengdi Jia",
        "Zekun Qi",
        "Shaochen Zhang",
        "Wenyao Zhang",
        "Xinqiang Yu",
        "Jiawei He",
        "He Wang",
        "Li Yi"
      ],
      "abstract": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.",
      "pdf_url": "http://arxiv.org/pdf/2506.03135v1",
      "published": "2025-06-03T17:58:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03135v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation",
      "authors": [
        "Kai Lion",
        "Liang Zhang",
        "Bingcong Li",
        "Niao He"
      ],
      "abstract": "We show that low-rank adaptation of large-scale models suffers from a low\nstable rank that is well below the linear algebraic rank of the subspace,\ndegrading fine-tuning performance. To mitigate the underutilization of the\nallocated subspace, we propose PoLAR, a parameterization inspired by the polar\ndecomposition that factorizes the low-rank update into two direction matrices\nconstrained to Stiefel manifolds and an unconstrained scale matrix. Our theory\nshows that PoLAR yields an exponentially faster convergence rate on a canonical\nlow-rank adaptation problem. Pairing the parameterization with Riemannian\noptimization leads to consistent gains on three different benchmarks testing\ngeneral language understanding, commonsense reasoning, and mathematical problem\nsolving with base model sizes ranging from 350M to 27B.",
      "pdf_url": "http://arxiv.org/pdf/2506.03133v1",
      "published": "2025-06-03T17:58:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03133v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "math.OC"
      ]
    },
    {
      "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback",
      "authors": [
        "Xiaoying Zhang",
        "Hao Sun",
        "Yipeng Zhang",
        "Kaituo Feng",
        "Chaochao Lu",
        "Chao Yang",
        "Helen Meng"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
      "pdf_url": "http://arxiv.org/pdf/2506.03106v2",
      "published": "2025-06-03T17:39:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03106v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff",
      "authors": [
        "Sophie Greenwood",
        "Karen Levy",
        "Solon Barocas",
        "Hoda Heidari",
        "Jon Kleinberg"
      ],
      "abstract": "As AI technologies improve, people are increasingly willing to delegate tasks\nto AI agents. In many cases, the human decision-maker chooses whether to\ndelegate to an AI agent based on properties of the specific instance of the\ndecision-making problem they are facing. Since humans typically lack full\nawareness of all the factors relevant to this choice for a given\ndecision-making instance, they perform a kind of categorization by treating\nindistinguishable instances -- those that have the same observable features --\nas the same. In this paper, we define the problem of designing the optimal\nalgorithmic delegate in the presence of categories. This is an important\ndimension in the design of algorithms to work with humans, since we show that\nthe optimal delegate can be an arbitrarily better teammate than the optimal\nstandalone algorithmic agent. The solution to this optimal delegation problem\nis not obvious: we discover that this problem is fundamentally combinatorial,\nand illustrate the complex relationship between the optimal design and the\nproperties of the decision-making task even in simple settings. Indeed, we show\nthat finding the optimal delegate is computationally hard in general. However,\nwe are able to find efficient algorithms for producing the optimal delegate in\nseveral broad cases of the problem, including when the optimal action may be\ndecomposed into functions of features observed by the human and the algorithm.\nFinally, we run computational experiments to simulate a designer updating an\nalgorithmic delegate over time to be optimized for when it is actually adopted\nby users, and show that while this process does not recover the optimal\ndelegate in general, the resulting delegate often performs quite well.",
      "pdf_url": "http://arxiv.org/pdf/2506.03102v1",
      "published": "2025-06-03T17:36:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03102v1",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds",
      "authors": [
        "Yang Guo",
        "Yutian Tao",
        "Yifei Ming",
        "Robert D. Nowak",
        "Yingyu Liang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.",
      "pdf_url": "http://arxiv.org/pdf/2506.03100v1",
      "published": "2025-06-03T17:31:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03100v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models",
      "authors": [
        "Chetwin Low",
        "Weimin Wang"
      ],
      "abstract": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
      "pdf_url": "http://arxiv.org/pdf/2506.03099v1",
      "published": "2025-06-03T17:29:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03099v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding",
      "authors": [
        "Ashwin Vinod",
        "Shrey Pandit",
        "Aditya Vavre",
        "Linshen Liu"
      ],
      "abstract": "Emerging embodied AI applications, such as wearable cameras and autonomous\nagents, have underscored the need for robust reasoning from first person video\nstreams. We introduce EgoVLM, a vision-language model specifically designed to\nintegrate visual comprehension and spatial-temporal reasoning within egocentric\nvideo contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization\n(GRPO), a reinforcement learning method adapted to align model outputs with\nhuman-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly\ntune using RL without any supervised fine-tuning phase on chain-of-thought\n(CoT) data. We evaluate EgoVLM on egocentric video question answering\nbenchmarks and show that domain-specific training substantially improves\nperformance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on\nnon-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by\n14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By\nexplicitly generating reasoning traces, EgoVLM enhances interpretability,\nmaking it well-suited for downstream applications. Furthermore, we introduce a\nnovel keyframe-based reward that incorporates salient frame selection to guide\nreinforcement learning optimization. This reward formulation opens a promising\navenue for future exploration in temporally grounded egocentric reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2506.03097v1",
      "published": "2025-06-03T17:28:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03097v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents",
      "authors": [
        "Man Luo",
        "David Cobbley",
        "Xin Su",
        "Shachar Rosenman",
        "Vasudev Lal",
        "Shao-Yen Tseng",
        "Phillip Howard"
      ],
      "abstract": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.",
      "pdf_url": "http://arxiv.org/pdf/2506.03095v1",
      "published": "2025-06-03T17:27:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03095v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning",
      "authors": [
        "Lloyd Pellatt",
        "Fotios Drakopoulos",
        "Shievanie Sabesan",
        "Nicholas A. Lesica"
      ],
      "abstract": "The mapping from sound to neural activity that underlies hearing is highly\nnon-linear. The first few stages of this mapping in the cochlea have been\nmodelled successfully, with biophysical models built by hand and, more\nrecently, with DNN models trained on datasets simulated by biophysical models.\nModelling the auditory brain has been a challenge because central auditory\nprocessing is too complex for models to be built by hand, and datasets for\ntraining DNN models directly have not been available. Recent work has taken\nadvantage of large-scale high resolution neural recordings from the auditory\nmidbrain to build a DNN model of normal hearing with great success. But this\nmodel assumes that auditory processing is the same in all brains, and therefore\nit cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space\nof hearing loss directly from recordings of neural activity in the auditory\nmidbrain of healthy and noise exposed animals. With hearing loss parametrised\nby only 6 free parameters per animal, our model accurately predicts 62\\% of the\nexplainable variance in neural responses from normal hearing animals and 68%\nfor hearing impaired animals, within a few percentage points of state of the\nart animal specific models. We demonstrate that the model can be used to\nsimulate realistic activity from out of sample animals by fitting only the\nlearned conditioning parameters with Bayesian optimisation, achieving\ncrossentropy loss within 2% of the optimum in 15-30 iterations. Including more\nanimals in the training data slightly improved the performance on unseen\nanimals. This model will enable future development of parametrised hearing loss\ncompensation models trained to directly restore normal neural coding in hearing\nimpaired brains, which can be quickly fitted for a new user by human in the\nloop optimisation.",
      "pdf_url": "http://arxiv.org/pdf/2506.03088v1",
      "published": "2025-06-03T17:12:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03088v1",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment",
      "authors": [
        "Bin Ma",
        "Yuyuan Feng",
        "Minhua Lin",
        "Enyan Dai"
      ],
      "abstract": "Graph Neural Networks (GNNs) have become essential tools for analyzing\ngraph-structured data in domains such as drug discovery and financial analysis,\nleading to growing demands for model transparency. Recent advances in\nexplainable GNNs have addressed this need by revealing important subgraphs that\ninfluence predictions, but these explanation mechanisms may inadvertently\nexpose models to security risks. This paper investigates how such explanations\npotentially leak critical decision logic that can be exploited for model\nstealing. We propose {\\method}, a novel stealing framework that integrates\nexplanation alignment for capturing decision logic with guided data\naugmentation for efficient training under limited queries, enabling effective\nreplication of both the predictive behavior and underlying reasoning patterns\nof target models. Experiments on molecular graph datasets demonstrate that our\napproach shows advantages over conventional methods in model stealing. This\nwork highlights important security considerations for the deployment of\nexplainable GNNs in sensitive domains and suggests the need for protective\nmeasures against explanation-based attacks. Our code is available at\nhttps://github.com/beanmah/EGSteal.",
      "pdf_url": "http://arxiv.org/pdf/2506.03087v1",
      "published": "2025-06-03T17:11:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03087v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Labelling Data with Unknown References",
      "authors": [
        "Adrian de Wynter"
      ],
      "abstract": "An evaluator is trustworthy when there exists some agreed-upon way to measure\nits performance as a labeller. The two ways to establish trustworthiness are\neither by testing it, or by assuming the evaluator `knows' somehow the way to\nlabel the corpus. However, if labelled references (e.g., a development set) are\nunavailable, neither of these approaches work: the former requires the data,\nand the latter is an assumption, not evidence. To address this, we introduce an\nalgorithm (the `No-Data Algorithm') by which to establish trust in an evaluator\nwithout any existing references. Our algorithm works by successively posing\nchallenges to said evaluator. We show that this is sufficient to establish\ntrustworthiness w.h.p., in such a way that when the evaluator actually knows\nthe way to label the corpus, the No-Data Algorithm accepts its output; and,\nconversely, flags untrustworthy evaluators when these are unable to prove it.\nWe present formal proofs of correctness and limited experiments.",
      "pdf_url": "http://arxiv.org/pdf/2506.03083v1",
      "published": "2025-06-03T17:04:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03083v1",
      "categories": [
        "cs.DS",
        "cs.AI"
      ]
    },
    {
      "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
      "authors": [
        "Qijun Luo",
        "Mengqi Li",
        "Lei Zhao",
        "Xiao Li"
      ],
      "abstract": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP.",
      "pdf_url": "http://arxiv.org/pdf/2506.03077v1",
      "published": "2025-06-03T16:54:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03077v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers",
      "authors": [
        "Pengtao Chen",
        "Xianfang Zeng",
        "Maosen Zhao",
        "Peng Ye",
        "Mingzhu Shen",
        "Wei Cheng",
        "Gang Yu",
        "Tao Chen"
      ],
      "abstract": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical\nFLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$,\nand 1.58$\\times$, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
      "pdf_url": "http://arxiv.org/pdf/2506.03065v1",
      "published": "2025-06-03T16:42:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03065v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models",
      "authors": [
        "Ram Potham",
        "Max Harms"
      ],
      "abstract": "Foundation models (FMs) face a critical safety challenge: as capabilities\nscale, instrumental convergence drives default trajectories toward loss of\nhuman control, potentially culminating in existential catastrophe. Current\nalignment approaches struggle with value specification complexity and fail to\naddress emergent power-seeking behaviors. We propose \"Corrigibility as a\nSingular Target\" (CAST)-designing FMs whose overriding objective is empowering\ndesignated human principals to guide, correct, and control them. This paradigm\nshift from static value-loading to dynamic human empowerment transforms\ninstrumental drives: self-preservation serves only to maintain the principal's\ncontrol; goal modification becomes facilitating principal guidance. We present\na comprehensive empirical research agenda spanning training methodologies\n(RLAIF, SFT, synthetic data generation), scalability testing across model\nsizes, and demonstrations of controlled instructability. Our vision: FMs that\nbecome increasingly responsive to human guidance as capabilities grow, offering\na path to beneficial AI that remains as tool-like as possible, rather than\nsupplanting human judgment. This addresses the core alignment problem at its\nsource, preventing the default trajectory toward misaligned instrumental\nconvergence.",
      "pdf_url": "http://arxiv.org/pdf/2506.03056v1",
      "published": "2025-06-03T16:36:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03056v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "MAEBE: Multi-Agent Emergent Behavior Framework",
      "authors": [
        "Sinem Erisken",
        "Timothy Gothard",
        "Martin Leitgab",
        "Ram Potham"
      ],
      "abstract": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.",
      "pdf_url": "http://arxiv.org/pdf/2506.03053v1",
      "published": "2025-06-03T16:33:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03053v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs",
      "authors": [
        "Yuval Kansal",
        "Shmuel Berman",
        "Lydia Liu"
      ],
      "abstract": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.",
      "pdf_url": "http://arxiv.org/pdf/2506.03051v1",
      "published": "2025-06-03T16:31:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03051v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment",
      "authors": [
        "Mikolaj Walczak",
        "Romina Aalishah",
        "Wyatt Mackey",
        "Brittany Story",
        "David L. Boothe Jr.",
        "Nicholas Waytowich",
        "Xiaomin Lin",
        "Tinoosh Mohsenin"
      ],
      "abstract": "Deep reinforcement learning agents are often fragile while humans remain\nadaptive and flexible to varying scenarios. To bridge this gap, we present\nEDEN, a biologically inspired navigation framework that integrates learned\nentorhinal-like grid cell representations and reinforcement learning to enable\nautonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,\nEDEN allows agents to perform path integration and vector-based navigation\nusing visual and motion sensor data. At the core of EDEN is a grid cell encoder\nthat transforms egocentric motion into periodic spatial codes, producing\nlow-dimensional, interpretable embeddings of position. To generate these\nactivations from raw sensory input, we combine fiducial marker detections in\nthe lightweight MiniWorld simulator and DINO-based visual features in the\nhigh-fidelity Gazebo simulator. These spatial representations serve as input to\na policy trained with Proximal Policy Optimization (PPO), enabling dynamic,\ngoal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid\nprototyping, and Gazebo, which offers realistic physics and perception noise.\nCompared to baseline agents using raw state inputs (e.g., position, velocity)\nor standard convolutional image encoders, EDEN achieves a 99% success rate,\nwithin the simple scenarios, and >94% within complex floorplans with occluded\npaths with more efficient and reliable step-wise navigation. In addition, as a\nreplacement of ground truth activations, we present a trainable Grid Cell\nencoder enabling the development of periodic grid-like patterns from vision and\nmotion sensor data, emulating the development of such patterns within\nbiological mammals. This work represents a step toward biologically grounded\nspatial intelligence in robotics, bridging neural navigation principles with\nreinforcement learning for scalable deployment.",
      "pdf_url": "http://arxiv.org/pdf/2506.03046v1",
      "published": "2025-06-03T16:28:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03046v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning",
      "authors": [
        "Pierre Lepagnol",
        "Sahar Ghannay",
        "Thomas Gerald",
        "Christophe Servan",
        "Sophie Rosset"
      ],
      "abstract": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.",
      "pdf_url": "http://arxiv.org/pdf/2506.03035v1",
      "published": "2025-06-03T16:18:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03035v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment",
      "authors": [
        "Junhao Yu",
        "Yan Zhuang",
        "YuXuan Sun",
        "Weibo Gao",
        "Qi Liu",
        "Mingyue Cheng",
        "Zhenya Huang",
        "Enhong Chen"
      ],
      "abstract": "Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions.",
      "pdf_url": "http://arxiv.org/pdf/2506.03032v1",
      "published": "2025-06-03T16:07:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03032v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "Smartflow: Enabling Scalable Spatiotemporal Geospatial Research",
      "authors": [
        "David McVicar",
        "Brian Avant",
        "Adrian Gould",
        "Diego Torrejon",
        "Charles Della Porta",
        "Ryan Mukherjee"
      ],
      "abstract": "BlackSky introduces Smartflow, a cloud-based framework enabling scalable\nspatiotemporal geospatial research built on open-source tools and technologies.\nUsing STAC-compliant catalogs as a common input, heterogeneous geospatial data\ncan be processed into standardized datacubes for analysis and model training.\nModel experimentation is managed using a combination of tools, including\nClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is\nKubernetes, which orchestrates the provisioning and execution of workflows to\nsupport both horizontal and vertical scalability. This combination of features\nmakes Smartflow well-suited for geospatial model development and analysis over\nlarge geographic areas, time scales, and expansive image archives.\n  We also present a novel neural architecture, built using Smartflow, to\nmonitor large geographic areas for heavy construction. Qualitative results\nbased on data from the IARPA Space-based Machine Automated Recognition\nTechnique (SMART) program are presented that show the model is capable of\ndetecting heavy construction throughout all major phases of development.",
      "pdf_url": "http://arxiv.org/pdf/2506.03022v1",
      "published": "2025-06-03T15:58:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03022v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech",
      "authors": [
        "Florian Ludwig",
        "Torsten Zesch",
        "Frederike Zufall"
      ],
      "abstract": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.",
      "pdf_url": "http://arxiv.org/pdf/2506.03009v1",
      "published": "2025-06-03T15:50:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.03009v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Linear Spatial World Models Emerge in Large Language Models",
      "authors": [
        "Matthieu Tehenan",
        "Christian Bolivar Moya",
        "Tenghai Long",
        "Guang Lin"
      ],
      "abstract": "Large language models (LLMs) have demonstrated emergent abilities across\ndiverse tasks, raising the question of whether they acquire internal world\nmodels. In this work, we investigate whether LLMs implicitly encode linear\nspatial world models, which we define as linear representations of physical\nspace and object configurations. We introduce a formal framework for spatial\nworld models and assess whether such structure emerges in contextual\nembeddings. Using a synthetic dataset of object positions, we train probes to\ndecode object positions and evaluate geometric consistency of the underlying\nspace. We further conduct causal interventions to test whether these spatial\nrepresentations are functionally used by the model. Our results provide\nempirical evidence that LLMs encode linear spatial world models.",
      "pdf_url": "http://arxiv.org/pdf/2506.02996v1",
      "published": "2025-06-03T15:31:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02996v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation",
      "authors": [
        "Li Zhang",
        "Kevin D. Ashley"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/",
      "pdf_url": "http://arxiv.org/pdf/2506.02992v1",
      "published": "2025-06-03T15:28:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02992v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "68T50",
        "I.2"
      ]
    },
    {
      "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis",
      "authors": [
        "Richard Armitage"
      ],
      "abstract": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.",
      "pdf_url": "http://arxiv.org/pdf/2506.02987v1",
      "published": "2025-06-03T15:25:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02987v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge",
      "authors": [
        "Rachid Zeghlache",
        "Ikram Brahim",
        "Pierre-Henri Conze",
        "Mathieu Lamard",
        "Mohammed El Amine Lazouni",
        "Zineb Aziza Elaouaber",
        "Leila Ryma Lazouni",
        "Christopher Nielsen",
        "Ahmad O. Ahsan",
        "Matthias Wilms",
        "Nils D. Forkert",
        "Lovre Antonio Budimir",
        "Ivana Matovinović",
        "Donik Vršnak",
        "Sven Lončarić",
        "Philippe Zhang",
        "Weili Jiang",
        "Yihao Li",
        "Yiding Hao",
        "Markus Frohmann",
        "Patrick Binder",
        "Marcel Huber",
        "Taha Emre",
        "Teresa Finisterra Araújo",
        "Marzieh Oghbaie",
        "Hrvoje Bogunović",
        "Amerens A. Bekkers",
        "Nina M. van Liebergen",
        "Hugo J. Kuijf",
        "Abdul Qayyum",
        "Moona Mazher",
        "Steven A. Niederer",
        "Alberto J. Beltrán-Carrero",
        "Juan J. Gómez-Valverde",
        "Javier Torresano-Rodríquez",
        "Álvaro Caballero-Sastre",
        "María J. Ledesma Carbayo",
        "Yosuke Yamagishi",
        "Yi Ding",
        "Robin Peretzke",
        "Alexandra Ertl",
        "Maximilian Fischer",
        "Jessica Kächele",
        "Sofiane Zehar",
        "Karim Boukli Hacene",
        "Thomas Monfort",
        "Béatrice Cochener",
        "Mostafa El Habib Daho",
        "Anas-Alexis Benyoussef",
        "Gwenolé Quellec"
      ],
      "abstract": "The MARIO challenge, held at MICCAI 2024, focused on advancing the automated\ndetection and monitoring of age-related macular degeneration (AMD) through the\nanalysis of optical coherence tomography (OCT) images. Designed to evaluate\nalgorithmic performance in detecting neovascular activity changes within AMD,\nthe challenge incorporated unique multi-modal datasets. The primary dataset,\nsourced from Brest, France, was used by participating teams to train and test\ntheir models. The final ranking was determined based on performance on this\ndataset. An auxiliary dataset from Algeria was used post-challenge to evaluate\npopulation and device shifts from submitted solutions. Two tasks were involved\nin the MARIO challenge. The first one was the classification of evolution\nbetween two consecutive 2D OCT B-scans. The second one was the prediction of\nfuture AMD evolution over three months for patients undergoing anti-vascular\nendothelial growth factor (VEGF) therapy. Thirty-five teams participated, with\nthe top 12 finalists presenting their methods. This paper outlines the\nchallenge's structure, tasks, data characteristics, and winning methodologies,\nsetting a benchmark for AMD monitoring using OCT, infrared imaging, and\nclinical data (such as the number of visits, age, gender, etc.). The results of\nthis challenge indicate that artificial intelligence (AI) performs as well as a\nphysician in measuring AMD progression (Task 1) but is not yet able of\npredicting future evolution (Task 2).",
      "pdf_url": "http://arxiv.org/pdf/2506.02976v1",
      "published": "2025-06-03T15:14:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02976v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation",
      "authors": [
        "Yicheng Xiao",
        "Lin Song",
        "Rui Yang",
        "Cheng Cheng",
        "Zunnan Xu",
        "Zhaoyang Zhang",
        "Yixiao Ge",
        "Xiu Li",
        "Ying Shan"
      ],
      "abstract": "With the advancement of language models, unified multimodal understanding and\ngeneration have made significant strides, with model architectures evolving\nfrom separated components to unified single-model frameworks. This paper\nexplores an efficient training paradigm to build a single transformer for\nunified multimodal understanding and generation. Specifically, we propose a\nmultimodal warmup strategy utilizing prior knowledge to extend capabilities. To\naddress cross-modal compatibility challenges, we introduce feature pre-scaling\nand multimodal AdaLN techniques. Integrating the proposed technologies, we\npresent the HaploOmni, a new single multimodal transformer. With limited\ntraining costs, HaploOmni achieves competitive performance across multiple\nimage and video understanding and generation benchmarks over advanced unified\nmodels. All codes will be made public at https://github.com/Tencent/HaploVLM.",
      "pdf_url": "http://arxiv.org/pdf/2506.02975v1",
      "published": "2025-06-03T15:14:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02975v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring",
      "authors": [
        "Zhixiong Su",
        "Yichen Wang",
        "Herun Wan",
        "Zhaohan Zhang",
        "Minnan Luo"
      ],
      "abstract": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.",
      "pdf_url": "http://arxiv.org/pdf/2506.02959v1",
      "published": "2025-06-03T14:52:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02959v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models",
      "authors": [
        "Zewen Yang",
        "Xiaobing Dai",
        "Dian Yu",
        "Qianru Li",
        "Yu Li",
        "Valentin Le Mesle"
      ],
      "abstract": "Generative models have become increasingly powerful tools for robot motion\ngeneration, enabling flexible and multimodal trajectory generation across\nvarious tasks. Yet, most existing approaches remain limited in handling\nmultiple types of constraints, such as collision avoidance and dynamic\nconsistency, which are often treated separately or only partially considered.\nThis paper proposes UniConFlow, a unified flow matching (FM) based framework\nfor trajectory generation that systematically incorporates both equality and\ninequality constraints. UniConFlow introduces a novel prescribed-time zeroing\nfunction to enhance flexibility during the inference process, allowing the\nmodel to adapt to varying task requirements. To ensure constraint satisfaction,\nparticularly with respect to obstacle avoidance, admissible action range, and\nkinodynamic consistency, the guidance inputs to the FM model are derived\nthrough a quadratic programming formulation, which enables constraint-aware\ngeneration without requiring retraining or auxiliary controllers. We conduct\nmobile navigation and high-dimensional manipulation tasks, demonstrating\nimproved safety and feasibility compared to state-of-the-art constrained\ngenerative planners. Project page is available at https://uniconflow.github.io.",
      "pdf_url": "http://arxiv.org/pdf/2506.02955v1",
      "published": "2025-06-03T14:48:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02955v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Interaction Field Matching: Overcoming Limitations of Electrostatic Models",
      "authors": [
        "Stepan I. Manukhov",
        "Alexander Kolesov",
        "Vladimir V. Palyulin",
        "Alexander Korotin"
      ],
      "abstract": "Electrostatic field matching (EFM) has recently appeared as a novel\nphysics-inspired paradigm for data generation and transfer using the idea of an\nelectric capacitor. However, it requires modeling electrostatic fields using\nneural networks, which is non-trivial because of the necessity to take into\naccount the complex field outside the capacitor plates. In this paper, we\npropose Interaction Field Matching (IFM), a generalization of EFM which allows\nusing general interaction fields beyond the electrostatic one. Furthermore,\ninspired by strong interactions between quarks and antiquarks in physics, we\ndesign a particular interaction field realization which solves the problems\nwhich arise when modeling electrostatic fields in EFM. We show the performance\non a series of toy and image data transfer problems.",
      "pdf_url": "http://arxiv.org/pdf/2506.02950v1",
      "published": "2025-06-03T14:45:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02950v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing",
      "authors": [
        "Lixiang Xu",
        "Xianwei Ding",
        "Xin Yuan",
        "Richang Hong",
        "Feiping Nie",
        "Enhong Chen",
        "Philip S. Yu"
      ],
      "abstract": "Knowledge Tracing (KT) involves monitoring the changes in a student's\nknowledge over time by analyzing their past responses, with the goal of\npredicting future performance. However, most existing methods primarily focus\non feature enhancement, while overlooking the deficiencies in cognitive\nrepresentation and the ability to express cognition-issues often caused by\ninterference from non-cognitive factors such as slipping and guessing. This\nlimitation hampers the ability to capture the continuity and coherence of the\nstudent's cognitive process. As a result, many methods may introduce more\nprediction bias and modeling costs due to their inability to maintain cognitive\ncontinuity and coherence. Based on the above discussion, we propose the\nCognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)\nmodel. This model em ploys a dynamic programming algorithm to optimize\ncognitive representations based on the difficulty of the questions and the\nperformance intervals between them. This approach ensures that the cognitive\nrepresentation aligns with the student's cognitive patterns, maintaining\noverall continuity and coherence. As a result, it provides more accurate and\nsystematic input features for subsequent model training, thereby minimizing\ndistortion in the simulation of cognitive states. Additionally, the CRDP-KT\nmodel performs partitioned optimization of cognitive representations to enhance\nthe reliability of the optimization process. Furthermore, it improves its\nability to express the student's cognition through a weighted fusion of\noptimized record representations and re lationships learned from a bipartite\ngraph. Finally, experiments conducted on three public datasets validate the\neffectiveness of the proposed CRDP-KT model.",
      "pdf_url": "http://arxiv.org/pdf/2506.02949v1",
      "published": "2025-06-03T14:44:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02949v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms",
      "authors": [
        "Praneet Sai Madhu Surabhi",
        "Dheeraj Reddy Mudireddy",
        "Jian Tao"
      ],
      "abstract": "This paper presents ThinkTank, a comprehensive and scalable framework\ndesigned to transform specialized AI agent systems into versatile collaborative\nintelligence platforms capable of supporting complex problem-solving across\ndiverse domains. ThinkTank systematically generalizes agent roles, meeting\nstructures, and knowledge integration mechanisms by adapting proven scientific\ncollaboration methodologies. Through role abstraction, generalization of\nmeeting types for iterative collaboration, and the integration of\nRetrieval-Augmented Generation with advanced knowledge storage, the framework\nfacilitates expertise creation and robust knowledge sharing. ThinkTank enables\norganizations to leverage collaborative AI for knowledge-intensive tasks while\nensuring data privacy and security through local deployment, utilizing\nframeworks like Ollama with models such as Llama3.1. The ThinkTank framework is\ndesigned to deliver significant advantages in cost-effectiveness, data\nsecurity, scalability, and competitive positioning compared to cloud-based\nalternatives, establishing it as a universal platform for AI-driven\ncollaborative problem-solving. The ThinkTank code is available at\nhttps://github.com/taugroup/ThinkTank",
      "pdf_url": "http://arxiv.org/pdf/2506.02931v1",
      "published": "2025-06-03T14:32:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02931v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The Limits of Predicting Agents from Behaviour",
      "authors": [
        "Alexis Bellot",
        "Jonathan Richens",
        "Tom Everitt"
      ],
      "abstract": "As the complexity of AI systems and their interactions with the world\nincreases, generating explanations for their behaviour is important for safely\ndeploying AI. For agents, the most natural abstractions for predicting\nbehaviour attribute beliefs, intentions and goals to the system. If an agent\nbehaves as if it has a certain goal or belief, then we can make reasonable\npredictions about how it will behave in novel situations, including those where\ncomprehensive safety evaluations are untenable. How well can we infer an\nagent's beliefs from their behaviour, and how reliably can these inferred\nbeliefs predict the agent's behaviour in novel situations? We provide a precise\nanswer to this question under the assumption that the agent's behaviour is\nguided by a world model. Our contribution is the derivation of novel bounds on\nthe agent's behaviour in new (unseen) deployment environments, which represent\na theoretical limit for predicting intentional agents from behavioural data\nalone. We discuss the implications of these results for several research areas\nincluding fairness and safety.",
      "pdf_url": "http://arxiv.org/pdf/2506.02923v1",
      "published": "2025-06-03T14:24:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02923v1",
      "categories": [
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs",
      "authors": [
        "Shangmin Guo",
        "Omar Darwiche Domingues",
        "Raphaël Avalos",
        "Aaron Courville",
        "Florian Strub"
      ],
      "abstract": "Tool use in stateful environments presents unique challenges for large\nlanguage models (LLMs), where existing test-time compute strategies relying on\nrepeated trials in the environment are impractical. We propose dynamics\nmodelling (DyMo), a method that augments LLMs with a state prediction\ncapability alongside function calling during post-training. This enables LLMs\nto predict the future states of their actions through an internal environment\nmodel. On the Berkeley Function Calling Leaderboard V2, DyMo improves success\nrates and significantly reduces hallucinations. We further integrate the\ninternal environment model into self-verification sampling (SVS), and show that\nthis substantially improves pass^k over number of trials k, and allows the\nmodel to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the\neffectiveness and reliability of LLMs for tool use. We believe this work charts\na path towards scalable planning RL methods for LLM inference without\nrepeatedly querying the oracle environment.",
      "pdf_url": "http://arxiv.org/pdf/2506.02918v1",
      "published": "2025-06-03T14:20:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02918v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning",
      "authors": [
        "Yin Fang",
        "Qiao Jin",
        "Guangzhi Xiong",
        "Bowen Jin",
        "Xianrui Zhong",
        "Siru Ouyang",
        "Aidong Zhang",
        "Jiawei Han",
        "Zhiyong Lu"
      ],
      "abstract": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.",
      "pdf_url": "http://arxiv.org/pdf/2506.02911v1",
      "published": "2025-06-03T14:16:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02911v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator",
      "authors": [
        "Yusuke Sakai",
        "Takumi Goto",
        "Taro Watanabe"
      ],
      "abstract": "We propose IMPARA-GED, a novel reference-free automatic grammatical error\ncorrection (GEC) evaluation method with grammatical error detection (GED)\ncapabilities. We focus on the quality estimator of IMPARA, an existing\nautomatic GEC evaluation method, and construct that of IMPARA-GED using a\npre-trained language model with enhanced GED capabilities. Experimental results\non SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,\ndemonstrate that IMPARA-GED achieves the highest correlation with human\nsentence-level evaluations.",
      "pdf_url": "http://arxiv.org/pdf/2506.02899v1",
      "published": "2025-06-03T14:05:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02899v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights",
      "authors": [
        "Jakub Krajewski",
        "Marcin Chochowski",
        "Daniel Korzekwa"
      ],
      "abstract": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models.",
      "pdf_url": "http://arxiv.org/pdf/2506.02890v1",
      "published": "2025-06-03T13:55:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02890v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective",
      "authors": [
        "Jintian Shao",
        "Yiming Cheng"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.",
      "pdf_url": "http://arxiv.org/pdf/2506.02878v1",
      "published": "2025-06-03T13:45:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02878v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics",
      "authors": [
        "Matthew Kowal",
        "Jasper Timm",
        "Jean-Francois Godbout",
        "Thomas Costello",
        "Antonio A. Arechar",
        "Gordon Pennycook",
        "David Rand",
        "Adam Gleave",
        "Kellin Pelrine"
      ],
      "abstract": "Persuasion is a powerful capability of large language models (LLMs) that both\nenables beneficial applications (e.g. helping people quit smoking) and raises\nsignificant risks (e.g. large-scale, targeted political manipulation). Prior\nwork has found models possess a significant and growing persuasive capability,\nmeasured by belief changes in simulated or real users. However, these\nbenchmarks overlook a crucial risk factor: the propensity of a model to attempt\nto persuade in harmful contexts. Understanding whether a model will blindly\n``follow orders'' to persuade on harmful topics (e.g. glorifying joining a\nterrorist group) is key to understanding the efficacy of safety guardrails.\nMoreover, understanding if and when a model will engage in persuasive behavior\nin pursuit of some goal is essential to understanding the risks from agentic AI\nsystems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts\nthe focus from persuasion success to persuasion attempts, operationalized as a\nmodel's willingness to generate content aimed at shaping beliefs or behavior.\nOur evaluation framework probes frontier LLMs using a multi-turn conversational\nsetup between simulated persuader and persuadee agents. APE explores a diverse\nspectrum of topics including conspiracies, controversial issues, and\nnon-controversially harmful content. We introduce an automated evaluator model\nto identify willingness to persuade and measure the frequency and context of\npersuasive attempts. We find that many open and closed-weight models are\nfrequently willing to attempt persuasion on harmful topics and that\njailbreaking can increase willingness to engage in such behavior. Our results\nhighlight gaps in current safety guardrails and underscore the importance of\nevaluating willingness to persuade as a key dimension of LLM risk. APE is\navailable at github.com/AlignmentResearch/AttemptPersuadeEval",
      "pdf_url": "http://arxiv.org/pdf/2506.02873v1",
      "published": "2025-06-03T13:37:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02873v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning",
      "authors": [
        "Chen Qian",
        "Dongrui Liu",
        "Haochen Wen",
        "Zhen Bai",
        "Yong Liu",
        "Jing Shao"
      ],
      "abstract": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks.",
      "pdf_url": "http://arxiv.org/pdf/2506.02867v1",
      "published": "2025-06-03T13:31:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02867v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights",
      "authors": [
        "Mathieu Andreux",
        "Breno Baldas Skuk",
        "Hamza Benchekroun",
        "Emilien Biré",
        "Antoine Bonnet",
        "Riaz Bordie",
        "Matthias Brunel",
        "Pierre-Louis Cedoz",
        "Antoine Chassang",
        "Mickaël Chen",
        "Alexandra D. Constantinou",
        "Antoine d'Andigné",
        "Hubert de La Jonquière",
        "Aurélien Delfosse",
        "Ludovic Denoyer",
        "Alexis Deprez",
        "Augustin Derupti",
        "Michael Eickenberg",
        "Mathïs Federico",
        "Charles Kantor",
        "Xavier Koegler",
        "Yann Labbé",
        "Matthew C. H. Lee",
        "Erwan Le Jumeau de Kergaradec",
        "Amir Mahla",
        "Avshalom Manevich",
        "Adrien Maret",
        "Charles Masson",
        "Rafaël Maurin",
        "Arturo Mena",
        "Philippe Modard",
        "Axel Moyal",
        "Axel Nguyen Kerbel",
        "Julien Revelle",
        "Mats L. Richter",
        "María Santos",
        "Laurent Sifre",
        "Maxime Theillard",
        "Marc Thibault",
        "Louis Thiry",
        "Léo Tronchon",
        "Nicolas Usunier",
        "Tony Wu"
      ],
      "abstract": "We present Surfer-H, a cost-efficient web agent that integrates\nVision-Language Models (VLM) to perform user-defined tasks on the web. We pair\nit with Holo1, a new open-weight collection of VLMs specialized in web\nnavigation and information extraction. Holo1 was trained on carefully curated\ndata sources, including open-access web content, synthetic examples, and\nself-produced agentic data. Holo1 tops generalist User Interface (UI)\nbenchmarks as well as our new web UI localization benchmark, WebClick. When\npowered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on\nWebVoyager, striking a Pareto-optimal balance between accuracy and\ncost-efficiency. To accelerate research advancement in agentic systems, we are\nopen-sourcing both our WebClick evaluation dataset and the Holo1 model weights.",
      "pdf_url": "http://arxiv.org/pdf/2506.02865v1",
      "published": "2025-06-03T13:29:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02865v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "BNPO: Beta Normalization Policy Optimization",
      "authors": [
        "Changyi Xiao",
        "Mengdi Zhang",
        "Yixin Cao"
      ],
      "abstract": "Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that\nreinforcement learning with rule-based, binary-valued reward functions can\nsignificantly enhance the reasoning capabilities of large language models.\nThese models primarily utilize REINFORCE-based policy optimization techniques,\nsuch as REINFORCE with baseline and group relative policy optimization (GRPO).\nHowever, a key limitation remains: current policy optimization methods either\nneglect reward normalization or employ static normalization strategies, which\nfail to adapt to the dynamic nature of policy updates during training. This may\nresult in unstable gradient estimates and hinder training stability. To address\nthis issue, we propose Beta Normalization Policy Optimization (BNPO), a novel\npolicy optimization method that adaptively normalizes rewards using a Beta\ndistribution with dynamically updated parameters. BNPO aligns the normalization\nwith the changing policy distribution, enabling more precise and lower-variance\ngradient estimation, which in turn promotes stable training dynamics. We\nprovide theoretical analysis demonstrating BNPO's variance-reducing properties\nand show that it generalizes both REINFORCE and GRPO under binary-valued reward\nsettings. Furthermore, we introduce an advantage decomposition mechanism to\nextend BNPO's applicability to more complex reward systems. Experimental\nresults confirm that BNPO achieves state-of-the-art performance among policy\noptimization methods on reasoning tasks. The code is available at\nhttps://github.com/changyi7231/BNPO.",
      "pdf_url": "http://arxiv.org/pdf/2506.02864v1",
      "published": "2025-06-03T13:28:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02864v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech",
      "authors": [
        "Helin Wang",
        "Jiarui Hai",
        "Dading Chong",
        "Karan Thakkar",
        "Tiantian Feng",
        "Dongchao Yang",
        "Junhyeok Lee",
        "Laureano Moro Velazquez",
        "Jesus Villalba",
        "Zengyi Qin",
        "Shrikanth Narayanan",
        "Mounya Elhiali",
        "Najim Dehak"
      ],
      "abstract": "Recent advancements in generative artificial intelligence have significantly\ntransformed the field of style-captioned text-to-speech synthesis (CapTTS).\nHowever, adapting CapTTS to real-world applications remains challenging due to\nthe lack of standardized, comprehensive datasets and limited research on\ndownstream tasks built upon CapTTS. To address these gaps, we introduce\nCapSpeech, a new benchmark designed for a series of CapTTS-related tasks,\nincluding style-captioned text-to-speech synthesis with sound events\n(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS\n(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech\ncomprises over 10 million machine-annotated audio-caption pairs and nearly 0.36\nmillion human-annotated audio-caption pairs. In addition, we introduce two new\ndatasets collected and recorded by a professional voice actor and experienced\naudio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside\nthe datasets, we conduct comprehensive experiments using both autoregressive\nand non-autoregressive models on CapSpeech. Our results demonstrate\nhigh-fidelity and highly intelligible speech synthesis across a diverse range\nof speaking styles. To the best of our knowledge, CapSpeech is the largest\navailable dataset offering comprehensive annotations for CapTTS-related tasks.\nThe experiments and findings further provide valuable insights into the\nchallenges of developing CapTTS systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.02863v1",
      "published": "2025-06-03T13:28:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02863v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "title": "Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs",
      "authors": [
        "Wenjing Tang",
        "Xinyu He",
        "Yongxi Huang",
        "Yunxiao Xiao",
        "Cewu Lu",
        "Panpan Cai"
      ],
      "abstract": "Task planning under uncertainty is essential for home-service robots\noperating in the real world. Tasks involve ambiguous human instructions, hidden\nor unknown object locations, and open-vocabulary object types, leading to\nsignificant open-ended uncertainty and a boundlessly large planning space. To\naddress these challenges, we propose Tru-POMDP, a planner that combines\nstructured belief generation using Large Language Models (LLMs) with principled\nPOMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH),\nwhich systematically queries an LLM to construct high-quality particle beliefs\nover possible world states and human goals. We further formulate an open-ended\nPOMDP model that enables rigorous Bayesian belief tracking and efficient\nbelief-space planning over these LLM-generated hypotheses. Experiments on\ncomplex object rearrangement tasks across diverse kitchen environments show\nthat Tru-POMDP significantly outperforms state-of-the-art LLM-based and\nLLM-tree-search hybrid planners, achieving higher success rates with\nsignificantly better plans, stronger robustness to ambiguity and occlusion, and\ngreater planning efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2506.02860v1",
      "published": "2025-06-03T13:26:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02860v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "ATAG: AI-Agent Application Threat Assessment with Attack Graphs",
      "authors": [
        "Parth Atulbhai Gandhi",
        "Akansha Shukla",
        "David Tayouri",
        "Beni Ifland",
        "Yuval Elovici",
        "Rami Puzis",
        "Asaf Shabtai"
      ],
      "abstract": "Evaluating the security of multi-agent systems (MASs) powered by large\nlanguage models (LLMs) is challenging, primarily because of the systems'\ncomplex internal dynamics and the evolving nature of LLM vulnerabilities.\nTraditional attack graph (AG) methods often lack the specific capabilities to\nmodel attacks on LLMs. This paper introduces AI-agent application Threat\nassessment with Attack Graphs (ATAG), a novel framework designed to\nsystematically analyze the security risks associated with AI-agent\napplications. ATAG extends the MulVAL logic-based AG generation tool with\ncustom facts and interaction rules to accurately represent AI-agent topologies,\nvulnerabilities, and attack scenarios. As part of this research, we also\ncreated the LLM vulnerability database (LVD) to initiate the process of\nstandardizing LLM vulnerabilities documentation. To demonstrate ATAG's\nefficacy, we applied it to two multi-agent applications. Our case studies\ndemonstrated the framework's ability to model and generate AGs for\nsophisticated, multi-step attack scenarios exploiting vulnerabilities such as\nprompt injection, excessive agency, sensitive information disclosure, and\ninsecure output handling across interconnected agents. ATAG is an important\nstep toward a robust methodology and toolset to help understand, visualize, and\nprioritize complex attack paths in multi-agent AI systems (MAASs). It\nfacilitates proactive identification and mitigation of AI-agent threats in\nmulti-agent applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.02859v1",
      "published": "2025-06-03T13:25:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.02859v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    }
  ]
}