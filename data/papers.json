{
  "last_updated": "2025-09-26T00:46:54.873999",
  "papers": [
    {
      "title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
      "authors": [
        "Henrique Schechter Vera",
        "Sahil Dua",
        "Biao Zhang",
        "Daniel Salz",
        "Ryan Mullins",
        "Sindhu Raghuram Panyam",
        "Sara Smoot",
        "Iftekhar Naim",
        "Joe Zou",
        "Feiyang Chen",
        "Daniel Cer",
        "Alice Lisak",
        "Min Choi",
        "Lucas Gonzalez",
        "Omar Sanseviero",
        "Glenn Cameron",
        "Ian Ballantyne",
        "Kat Black",
        "Kaifeng Chen",
        "Weiyi Wang",
        "Zhe Li",
        "Gus Martins",
        "Jinhyuk Lee",
        "Mark Sherwood",
        "Juyeong Ji",
        "Renjie Wu",
        "Jingxiao Zheng",
        "Jyotinder Singh",
        "Abheesht Sharma",
        "Divya Sreepat",
        "Aashi Jain",
        "Adham Elarabawy",
        "AJ Co",
        "Andreas Doumanoglou",
        "Babak Samari",
        "Ben Hora",
        "Brian Potetz",
        "Dahun Kim",
        "Enrique Alfonseca",
        "Fedor Moiseev",
        "Feng Han",
        "Frank Palma Gomez",
        "Gustavo Hernández Ábrego",
        "Hesen Zhang",
        "Hui Hui",
        "Jay Han",
        "Karan Gill",
        "Ke Chen",
        "Koert Chen",
        "Madhuri Shanbhogue",
        "Michael Boratko",
        "Paul Suganthan",
        "Sai Meher Karthik Duddu",
        "Sandeep Mariserla",
        "Setareh Ariafar",
        "Shanfeng Zhang",
        "Shijie Zhang",
        "Simon Baumgartner",
        "Sonam Goenka",
        "Steve Qiu",
        "Tanmaya Dabral",
        "Trevor Walker",
        "Vikram Rao",
        "Waleed Khawaja",
        "Wenlei Zhou",
        "Xiaoqi Ren",
        "Ye Xia",
        "Yichang Chen",
        "Yi-Ting Chen",
        "Zhe Dong",
        "Zhongli Ding",
        "Francesco Visin",
        "Gaël Liu",
        "Jiageng Zhang",
        "Kathleen Kenealy",
        "Michelle Casbon",
        "Ravin Kumar",
        "Thomas Mesnard",
        "Zach Gleicher",
        "Cormac Brick",
        "Olivier Lacombe",
        "Adam Roberts",
        "Yunhsuan Sung",
        "Raphael Hoffmann",
        "Tris Warkentin",
        "Armand Joulin",
        "Tom Duerig",
        "Mojtaba Seyedhosseini"
      ],
      "abstract": "We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.",
      "pdf_url": "http://arxiv.org/pdf/2509.20354v1",
      "published": "2025-09-24T17:56:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20354v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Morphological Synthesizer for Ge'ez Language: Addressing Morphological Complexity and Resource Limitations",
      "authors": [
        "Gebrearegawi Gebremariam",
        "Hailay Teklehaymanot",
        "Gebregewergs Mezgebe"
      ],
      "abstract": "Ge'ez is an ancient Semitic language renowned for its unique alphabet. It\nserves as the script for numerous languages, including Tigrinya and Amharic,\nand played a pivotal role in Ethiopia's cultural and religious development\nduring the Aksumite kingdom era. Ge'ez remains significant as a liturgical\nlanguage in Ethiopia and Eritrea, with much of the national identity\ndocumentation recorded in Ge'ez. These written materials are invaluable primary\nsources for studying Ethiopian and Eritrean philosophy, creativity, knowledge,\nand civilization. Ge'ez has a complex morphological structure with rich\ninflectional and derivational morphology, and no usable NLP has been developed\nand published until now due to the scarcity of annotated linguistic data,\ncorpora, labeled datasets, and lexicons. Therefore, we propose a rule-based\nGe'ez morphological synthesizer to generate surface words from root words\naccording to the morphological structures of the language. We used 1,102 sample\nverbs, representing all verb morphological structures, to test and evaluate the\nsystem. The system achieves a performance of 97.4%, outperforming the baseline\nmodel and suggesting that future work should build a comprehensive system\nconsidering morphological variations of the language.\n  Keywords: Ge'ez, NLP, morphology, morphological synthesizer, rule-based",
      "pdf_url": "http://arxiv.org/pdf/2509.20341v1",
      "published": "2025-09-24T17:33:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20341v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50, 68T35, 68N01",
        "I.2.7; I.2.6; H.3.1"
      ]
    },
    {
      "title": "Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning",
      "authors": [
        "Umer Siddique",
        "Abhinav Sinha",
        "Yongcan Cao"
      ],
      "abstract": "Conventional multi-agent reinforcement learning (MARL) methods rely on\ntime-triggered execution, where agents sample and communicate actions at fixed\nintervals. This approach is often computationally expensive and\ncommunication-intensive. To address this limitation, we propose ET-MAPG\n(Event-Triggered Multi-Agent Policy Gradient reinforcement learning), a\nframework that jointly learns an agent's control policy and its\nevent-triggering policy. Unlike prior work that decouples these mechanisms,\nET-MAPG integrates them into a unified learning process, enabling agents to\nlearn not only what action to take but also when to execute it. For scenarios\nwith inter-agent communication, we introduce AET-MAPG, an attention-based\nvariant that leverages a self-attention mechanism to learn selective\ncommunication patterns. AET-MAPG empowers agents to determine not only when to\ntrigger an action but also with whom to communicate and what information to\nexchange, thereby optimizing coordination. Both methods can be integrated with\nany policy gradient MARL algorithm. Extensive experiments across diverse MARL\nbenchmarks demonstrate that our approaches achieve performance comparable to\nstate-of-the-art, time-triggered baselines while significantly reducing both\ncomputational load and communication overhead.",
      "pdf_url": "http://arxiv.org/pdf/2509.20338v1",
      "published": "2025-09-24T17:29:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20338v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "math.DS"
      ]
    },
    {
      "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
      "authors": [
        "Xinnan Dai",
        "Chung-Hsiang Lo",
        "Kai Guo",
        "Shenglai Zeng",
        "Dongsheng Luo",
        "Jiliang Tang"
      ],
      "abstract": "Transformer-based LLMs demonstrate strong performance on graph reasoning\ntasks, yet their internal mechanisms remain underexplored. To uncover these\nreasoning process mechanisms in a fundamental and unified view, we set the\nbasic decoder-only transformers and explain them using the circuit-tracer\nframework. Through this lens, we visualize reasoning traces and identify two\ncore mechanisms in graph reasoning: token merging and structural memorization,\nwhich underlie both path reasoning and substructure extraction tasks. We\nfurther quantify these behaviors and analyze how they are influenced by graph\ndensity and model size. Our study provides a unified interpretability framework\nfor understanding structural reasoning in decoder-only Transformers.",
      "pdf_url": "http://arxiv.org/pdf/2509.20336v1",
      "published": "2025-09-24T17:25:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20336v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Video models are zero-shot learners and reasoners",
      "authors": [
        "Thaddäus Wiedemer",
        "Yuxuan Li",
        "Paul Vicol",
        "Shixiang Shane Gu",
        "Nick Matarese",
        "Kevin Swersky",
        "Been Kim",
        "Priyank Jaini",
        "Robert Geirhos"
      ],
      "abstract": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models.",
      "pdf_url": "http://arxiv.org/pdf/2509.20328v1",
      "published": "2025-09-24T17:17:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20328v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack Surface",
      "authors": [
        "Atousa Arzanipour",
        "Rouzbeh Behnia",
        "Reza Ebrahimi",
        "Kaushik Dutta"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.20324v1",
      "published": "2025-09-24T17:11:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20324v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "DRES: Benchmarking LLMs for Disfluency Removal",
      "authors": [
        "Maria Teleki",
        "Sai Janjur",
        "Haoran Liu",
        "Oliver Grabner",
        "Ketan Verma",
        "Thomas Docog",
        "Xiangjue Dong",
        "Lingfeng Shi",
        "Cong Wang",
        "Stephanie Birkelbach",
        "Jason Kim",
        "Yin Zhang",
        "James Caverlee"
      ],
      "abstract": "Disfluencies -- such as \"um,\" \"uh,\" interjections, parentheticals, and edited\nstatements -- remain a persistent challenge for speech-driven systems,\ndegrading accuracy in command interpretation, summarization, and conversational\nagents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled\ntext-level benchmark that establishes a reproducible semantic upper bound for\nthis task. DRES builds on human-annotated Switchboard transcripts, isolating\ndisfluency removal from ASR errors and acoustic variability. We systematically\nevaluate proprietary and open-source LLMs across scales, prompting strategies,\nand architectures. Our results reveal that (i) simple segmentation consistently\nimproves performance, even for long-context models; (ii) reasoning-oriented\nmodels tend to over-delete fluent tokens; and (iii) fine-tuning achieves near\nstate-of-the-art precision and recall but harms generalization abilities. We\nfurther present a set of LLM-specific error modes and offer nine practical\nrecommendations (R1-R9) for deploying disfluency removal in speech-driven\npipelines. DRES provides a reproducible, model-agnostic foundation for\nadvancing robust spoken-language systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.20321v1",
      "published": "2025-09-24T17:08:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20321v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Z-Scores: A Metric for Linguistically Assessing Disfluency Removal",
      "authors": [
        "Maria Teleki",
        "Sai Janjur",
        "Haoran Liu",
        "Oliver Grabner",
        "Ketan Verma",
        "Thomas Docog",
        "Xiangjue Dong",
        "Lingfeng Shi",
        "Cong Wang",
        "Stephanie Birkelbach",
        "Jason Kim",
        "Yin Zhang",
        "James Caverlee"
      ],
      "abstract": "Evaluating disfluency removal in speech requires more than aggregate\ntoken-level scores. Traditional word-based metrics such as precision, recall,\nand F1 (E-Scores) capture overall performance but cannot reveal why models\nsucceed or fail. We introduce Z-Scores, a span-level linguistically-grounded\nevaluation metric that categorizes system behavior across distinct disfluency\ntypes (EDITED, INTJ, PRN). Our deterministic alignment module enables robust\nmapping between generated text and disfluent transcripts, allowing Z-Scores to\nexpose systematic weaknesses that word-level metrics obscure. By providing\ncategory-specific diagnostics, Z-Scores enable researchers to identify model\nfailure modes and design targeted interventions -- such as tailored prompts or\ndata augmentation -- yielding measurable performance improvements. A case study\nwith LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies\nhidden in aggregate F1, directly informing model refinement strategies.",
      "pdf_url": "http://arxiv.org/pdf/2509.20319v1",
      "published": "2025-09-24T17:02:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20319v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
      "authors": [
        "Xilin Wei",
        "Xiaoran Liu",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Jiaqi Wang",
        "Xipeng Qiu",
        "Dahua Lin"
      ],
      "abstract": "Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative\nto explicit CoT reasoning in Large Language Models (LLMs), but a persistent\nperformance gap has limited their adoption. We identify a core latent\ninstability issue when scaling the computational budget of implicit CoT: as the\nnumber of reasoning tokens increases, training often becomes unstable and\ncollapses. Our analysis shows that this instability arises from latent\nrepresentations becoming homogeneous and losing semantic diversity, caused by\ninsufficient step-level supervision in current implicit CoT methods. To address\nthis, we propose SIM-CoT, a plug-and-play training module that introduces\nstep-level supervision to stabilize and enrich the latent reasoning space.\nSIM-CoT employs an auxiliary decoder during training to align each implicit\ntoken with its corresponding explicit reasoning step, ensuring latent states\ncapture distinct and meaningful information. The auxiliary decoder is removed\nat inference, preserving the efficiency of implicit CoT with no added overhead.\nIt also provides interpretability by projecting each latent token onto an\nexplicit reasoning vocabulary, enabling per-step visualization and diagnosis.\nSIM-CoT significantly improves both in-domain accuracy and out-of-domain\nstability of implicit CoT methods, boosting Coconut by +8.2\\% on GPT-2 and CODI\nby +3.0\\% on LLaMA-3.1 8B. It further surpasses the explicit CoT baseline on\nGPT-2 by 2.1\\% with 2.3$\\times$ greater token efficiency, while closing the\nperformance gap on larger models like LLaMA-3.1 8B. Code:\nhttps://github.com/InternLM/SIM-CoT",
      "pdf_url": "http://arxiv.org/pdf/2509.20317v2",
      "published": "2025-09-24T17:01:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20317v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity",
      "authors": [
        "Benjamin Feuer",
        "Chiung-Yi Tseng",
        "Astitwa Sarthak Lathe",
        "Oussama Elachqar",
        "John P Dickerson"
      ],
      "abstract": "LLM-judged benchmarks are increasingly used to evaluate complex model\nbehaviors, yet their design introduces failure modes absent in conventional\nground-truth based benchmarks. We argue that without tight objectives and\nverifiable constructions, benchmark rankings can produce high-confidence\nrankings that are in fact largely noise. We introduce two mechanisms to\ndiagnose these issues. Schematic adherence quantifies how much of a judge's\noverall verdict is explained by the explicit evaluation schema, revealing\nunexplained variance when judges deviate from their own rubric. Psychometric\nvalidity aggregates internal consistency and discriminant validity signals to\nquantify irreducible uncertainty in any benchmarking run. Applying these tools\nto Arena-Hard Auto, we find severe schema incoherence and factor collapse\nacross popular judges: for example, unexplained variance exceeding 90 percent\nfor DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We\nalso show that the ELO-style aggregation used by Arena-Hard Auto collapses and\nmasks genuine ranking uncertainty. Our results highlight design failures that\nundermine validity and offer actionable principles for building better-scoped,\nreliability-aware LLM-judged benchmarks. We release our code at\nhttps://anonymous.4open.science/r/judgment-to-noise-947D/README.md",
      "pdf_url": "http://arxiv.org/pdf/2509.20293v1",
      "published": "2025-09-24T16:26:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20293v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction",
      "authors": [
        "Dayu Tan",
        "Jing Chen",
        "Xiaoping Zhou",
        "Yansen Su",
        "Chunhou Zheng"
      ],
      "abstract": "Infectious diseases continue to pose a serious threat to public health,\nunderscoring the urgent need for effective computational approaches to screen\nnovel anti-infective agents. Oligopeptides have emerged as promising candidates\nin antimicrobial research due to their structural simplicity, high\nbioavailability, and low susceptibility to resistance. Despite their potential,\ncomputational models specifically designed to predict associations between\noligopeptides and infectious diseases remain scarce. This study introduces a\nprompt-guided graph-based contrastive learning framework (PGCLODA) to uncover\npotential associations. A tripartite graph is constructed with oligopeptides,\nmicrobes, and diseases as nodes, incorporating both structural and semantic\ninformation. To preserve critical regions during contrastive learning, a\nprompt-guided graph augmentation strategy is employed to generate meaningful\npaired views. A dual encoder architecture, integrating Graph Convolutional\nNetwork (GCN) and Transformer, is used to jointly capture local and global\nfeatures. The fused embeddings are subsequently input into a multilayer\nperceptron (MLP) classifier for final prediction. Experimental results on a\nbenchmark dataset indicate that PGCLODA consistently outperforms\nstate-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and\nhyperparameter studies confirm the contribution of each module. Case studies\nfurther validate the generalization ability of PGCLODA and its potential to\nuncover novel, biologically relevant associations. These findings offer\nvaluable insights for mechanism-driven discovery and oligopeptide-based drug\ndevelopment. The source code of PGCLODA is available online at\nhttps://github.com/jjnlcode/PGCLODA.",
      "pdf_url": "http://arxiv.org/pdf/2509.20290v1",
      "published": "2025-09-24T16:25:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20290v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ]
    },
    {
      "title": "Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation",
      "authors": [
        "Behzad Shayegh",
        "Jan-Thorsten Peter",
        "David Vilar",
        "Tobias Domhan",
        "Juraj Juraska",
        "Markus Freitag",
        "Lili Mou"
      ],
      "abstract": "We investigate the tradeoff between adequacy and fluency in machine\ntranslation. We show the severity of this tradeoff at the evaluation level and\nanalyze where popular metrics fall within it. Essentially, current metrics\ngenerally lean toward adequacy, meaning that their scores correlate more\nstrongly with the adequacy of translations than with fluency. More importantly,\nwe find that this tradeoff also persists at the meta-evaluation level, and that\nthe standard WMT meta-evaluation favors adequacy-oriented metrics over\nfluency-oriented ones. We show that this bias is partially attributed to the\ncomposition of the systems included in the meta-evaluation datasets. To control\nthis bias, we propose a method that synthesizes translation systems in\nmeta-evaluation. Our findings highlight the importance of understanding this\ntradeoff in meta-evaluation and its impact on metric rankings.",
      "pdf_url": "http://arxiv.org/pdf/2509.20287v1",
      "published": "2025-09-24T16:21:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20287v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Investigating Security Implications of Automatically Generated Code on the Software Supply Chain",
      "authors": [
        "Xiaofan Li",
        "Xing Gao"
      ],
      "abstract": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats.",
      "pdf_url": "http://arxiv.org/pdf/2509.20277v1",
      "published": "2025-09-24T16:15:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20277v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent",
      "authors": [
        "Xingjian Kang",
        "Linda Vorberg",
        "Andreas Maier",
        "Alexander Katzmann",
        "Oliver Taubmann"
      ],
      "abstract": "Managing scan protocols in Computed Tomography (CT), which includes adjusting\nacquisition parameters or configuring reconstructions, as well as selecting\npostprocessing tools in a patient-specific manner, is time-consuming and\nrequires clinical as well as technical expertise. At the same time, we observe\nan increasing shortage of skilled workforce in radiology. To address this\nissue, a Large Language Model (LLM)-based agent framework is proposed to assist\nwith the interpretation and execution of protocol configuration requests given\nin natural language or a structured, device-independent format, aiming to\nimprove the workflow efficiency and reduce technologists' workload. The agent\ncombines in-context-learning, instruction-following, and structured toolcalling\nabilities to identify relevant protocol elements and apply accurate\nmodifications. In a systematic evaluation, experimental results indicate that\nthe agent can effectively retrieve protocol components, generate device\ncompatible protocol definition files, and faithfully implement user requests.\nDespite demonstrating feasibility in principle, the approach faces limitations\nregarding syntactic and semantic validity due to lack of a unified device API,\nand challenges with ambiguous or complex requests. In summary, the findings\nshow a clear path towards LLM-based agents for supporting scan protocol\nmanagement in CT imaging.",
      "pdf_url": "http://arxiv.org/pdf/2509.20270v1",
      "published": "2025-09-24T16:04:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20270v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving",
      "authors": [
        "Jinhao Chai",
        "Anqing Jiang",
        "Hao Jiang",
        "Shiyi Mu",
        "Zichong Gu",
        "Shugong Xu"
      ],
      "abstract": "End-to-end multi-modal planning has become a transformative paradigm in\nautonomous driving, effectively addressing behavioral multi-modality and the\ngeneralization challenge in long-tail scenarios. We propose AnchDrive, a\nframework for end-to-end driving that effectively bootstraps a diffusion policy\nto mitigate the high computational cost of traditional generative models.\nRather than denoising from pure noise, AnchDrive initializes its planner with a\nrich set of hybrid trajectory anchors. These anchors are derived from two\ncomplementary sources: a static vocabulary of general driving priors and a set\nof dynamic, context-aware trajectories. The dynamic trajectories are decoded in\nreal-time by a Transformer that processes dense and sparse perceptual features.\nThe diffusion model then learns to refine these anchors by predicting a\ndistribution of trajectory offsets, enabling fine-grained refinement. This\nanchor-based bootstrapping design allows for efficient generation of diverse,\nhigh-quality trajectories. Experiments on the NAVSIM benchmark confirm that\nAnchDrive sets a new state-of-the-art and shows strong gen?eralizability",
      "pdf_url": "http://arxiv.org/pdf/2509.20253v1",
      "published": "2025-09-24T15:38:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20253v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A HyperGraphMamba-Based Multichannel Adaptive Model for ncRNA Classification",
      "authors": [
        "Xin An",
        "Ruijie Li",
        "Qiao Ning",
        "Hui Li",
        "Qian Ma",
        "Shikai Guo"
      ],
      "abstract": "Non-coding RNAs (ncRNAs) play pivotal roles in gene expression regulation and\nthe pathogenesis of various diseases. Accurate classification of ncRNAs is\nessential for functional annotation and disease diagnosis. To address existing\nlimitations in feature extraction depth and multimodal fusion, we propose\nHGMamba-ncRNA, a HyperGraphMamba-based multichannel adaptive model, which\nintegrates sequence, secondary structure, and optionally available expression\nfeatures of ncRNAs to enhance classification performance. Specifically, the\nsequence of ncRNA is modeled using a parallel Multi-scale Convolution and LSTM\narchitecture (MKC-L) to capture both local patterns and long-range dependencies\nof nucleotides. The structure modality employs a multi-scale graph transformer\n(MSGraphTransformer) to represent the multi-level topological characteristics\nof ncRNA secondary structures. The expression modality utilizes a Chebyshev\nPolynomial-based Kolmogorov-Arnold Network (CPKAN) to effectively model and\ninterpret high-dimensional expression profiles. Finally, by incorporating\nvirtual nodes to facilitate efficient and comprehensive multimodal interaction,\nHyperGraphMamba is proposed to adaptively align and integrate multichannel\nheterogeneous modality features. Experiments conducted on three public datasets\ndemonstrate that HGMamba-ncRNA consistently outperforms state-of-the-art\nmethods in terms of accuracy and other metrics. Extensive empirical studies\nfurther confirm the model's robustness, effectiveness, and strong\ntransferability, offering a novel and reliable strategy for complex ncRNA\nfunctional classification. Code and datasets are available at\nhttps://anonymous.4open.science/r/HGMamba-ncRNA-94D0.",
      "pdf_url": "http://arxiv.org/pdf/2509.20240v1",
      "published": "2025-09-24T15:31:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20240v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
      "authors": [
        "Tom Burgert",
        "Oliver Stoll",
        "Paolo Rota",
        "Begüm Demir"
      ],
      "abstract": "The hypothesis that Convolutional Neural Networks (CNNs) are inherently\ntexture-biased has shaped much of the discourse on feature use in deep\nlearning. We revisit this hypothesis by examining limitations in the\ncue-conflict experiment by Geirhos et al. To address these limitations, we\npropose a domain-agnostic framework that quantifies feature reliance through\nsystematic suppression of shape, texture, and color cues, avoiding the\nconfounds of forced-choice conflicts. By evaluating humans and neural networks\nunder controlled suppression conditions, we find that CNNs are not inherently\ntexture-biased but predominantly rely on local shape features. Nonetheless,\nthis reliance can be substantially mitigated through modern training strategies\nor architectures (ConvNeXt, ViTs). We further extend the analysis across\ncomputer vision, medical imaging, and remote sensing, revealing that reliance\npatterns differ systematically: computer vision models prioritize shape,\nmedical imaging models emphasize color, and remote sensing models exhibit a\nstronger reliance towards texture. Code is available at\nhttps://github.com/tomburgert/feature-reliance.",
      "pdf_url": "http://arxiv.org/pdf/2509.20234v1",
      "published": "2025-09-24T15:24:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20234v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization",
      "authors": [
        "Wenhan Wu",
        "Zheyuan Liu",
        "Chongyang Gao",
        "Ren Wang",
        "Kaize Ding"
      ],
      "abstract": "Current LLM unlearning methods face a critical security vulnerability that\nundermines their fundamental purpose: while they appear to successfully remove\nsensitive or harmful knowledge, this ``forgotten\" information remains\nprecariously recoverable through relearning attacks. We identify that the root\ncause is that conventional methods optimizing the forgetting loss at individual\ndata points will drive model parameters toward sharp minima in the loss\nlandscape. In these unstable regions, even minimal parameter perturbations can\ndrastically alter the model's behaviors. Consequently, relearning attacks\nexploit this vulnerability by using just a few fine-tuning samples to navigate\nthe steep gradients surrounding these unstable regions, thereby rapidly\nrecovering knowledge that was supposedly erased. This exposes a critical\nrobustness gap between apparent unlearning and actual knowledge removal. To\naddress this issue, we propose StableUN, a bi-level feedback-guided\noptimization framework that explicitly seeks more stable parameter regions via\nneighborhood-aware optimization. It integrates forgetting feedback, which uses\nadversarial perturbations to probe parameter neighborhoods, with remembering\nfeedback to preserve model utility, aligning the two objectives through\ngradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that\nour method is significantly more robust against both relearning and\njailbreaking attacks while maintaining competitive utility performance.",
      "pdf_url": "http://arxiv.org/pdf/2509.20230v1",
      "published": "2025-09-24T15:23:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20230v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Multimodal Representation-disentangled Information Bottleneck for Multimodal Recommendation",
      "authors": [
        "Hui Wang",
        "Jinghui Qin",
        "Wushao Wen",
        "Qingling Li",
        "Shanshan Zhong",
        "Zhongzhan Huang"
      ],
      "abstract": "Multimodal data has significantly advanced recommendation systems by\nintegrating diverse information sources to model user preferences and item\ncharacteristics. However, these systems often struggle with redundant and\nirrelevant information, which can degrade performance. Most existing methods\neither fuse multimodal information directly or use rigid architectural\nseparation for disentanglement, failing to adequately filter noise and model\nthe complex interplay between modalities. To address these challenges, we\npropose a novel framework, the Multimodal Representation-disentangled\nInformation Bottleneck (MRdIB). Concretely, we first employ a Multimodal\nInformation Bottleneck to compress the input representations, effectively\nfiltering out task-irrelevant noise while preserving rich semantic information.\nThen, we decompose the information based on its relationship with the\nrecommendation target into unique, redundant, and synergistic components. We\nachieve this decomposition with a series of constraints: a unique information\nlearning objective to preserve modality-unique signals, a redundant information\nlearning objective to minimize overlap, and a synergistic information learning\nobjective to capture emergent information. By optimizing these objectives,\nMRdIB guides a model to learn more powerful and disentangled representations.\nExtensive experiments on several competitive models and three benchmark\ndatasets demonstrate the effectiveness and versatility of our MRdIB in\nenhancing multimodal recommendation.",
      "pdf_url": "http://arxiv.org/pdf/2509.20225v1",
      "published": "2025-09-24T15:18:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20225v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction",
      "authors": [
        "Mohamed Manzour",
        "Catherine M. Elias",
        "Omar M. Shehata",
        "Rubén Izquierdo",
        "Miguel Ángel Sotelo"
      ],
      "abstract": "Research on lane change prediction has gained attention in the last few\nyears. Most existing works in this area have been conducted in simulation\nenvironments or with pre-recorded datasets, these works often rely on\nsimplified assumptions about sensing, communication, and traffic behavior that\ndo not always hold in practice. Real-world deployments of lane-change\nprediction systems are relatively rare, and when they are reported, the\npractical challenges, limitations, and lessons learned are often\nunder-documented. This study explores cooperative lane-change prediction\nthrough a real hardware deployment in mixed traffic and shares the insights\nthat emerged during implementation and testing. We highlight the practical\nchallenges we faced, including bottlenecks, reliability issues, and operational\nconstraints that shaped the behavior of the system. By documenting these\nexperiences, the study provides guidance for others working on similar\npipelines.",
      "pdf_url": "http://arxiv.org/pdf/2509.20218v1",
      "published": "2025-09-24T15:15:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20218v1",
      "categories": [
        "cs.AI",
        "cs.AR",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation",
      "authors": [
        "Guang Yang",
        "Wei Zheng",
        "Xiang Chen",
        "Yifan Sun",
        "Fengji Zhang",
        "Terry Yue Zhuo"
      ],
      "abstract": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods.",
      "pdf_url": "http://arxiv.org/pdf/2509.20215v1",
      "published": "2025-09-24T15:12:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20215v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.AR"
      ]
    },
    {
      "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment",
      "authors": [
        "Deokjae Lee",
        "Hyun Oh Song"
      ],
      "abstract": "We study weight-only post-training quantization (PTQ), which quantizes the\nweights of a large language model (LLM) without retraining, using little or no\ncalibration data. Weight-only PTQ is crucial for reducing the memory footprint\nand latency of LLM inference, especially in memory-bound, small-batch inference\nscenarios, such as personalized inference on edge devices. Despite its\nimportance, irregular weight distributions with heavy-tailed outliers in LLMs\ncomplicate quantization, recently motivating rotation-based methods that\ntransform weights into near-Gaussian distributions, which are more regular with\nfewer outliers, thereby reducing quantization error. In this work, we first\nderive the information-theoretically optimal bit allocation for Gaussianized\nweights under given bit budgets, revealing that fine-grained fractional-bit\nquantizers approaching the Gaussian distortion-rate bound are essential to\nachieve near-optimal quantization performance. To bridge this theoretical\ninsight and practical implementation, we introduce Q-Palette, a versatile\ncollection of fractional-bit quantizers that range from trellis-coded\nquantizers offering near-optimal distortion to simpler vector and scalar\nquantizers optimized for faster inference, all efficiently implemented with\noptimized CUDA kernels across various bitwidths. Furthermore, leveraging\nQ-Palette as a foundational component, we propose a novel mixed-scheme\nquantization framework, jointly optimizing quantizer choices and layer fusion\ndecisions given resource constraints. The code is available at\nhttps://github.com/snu-mllab/Q-Palette.",
      "pdf_url": "http://arxiv.org/pdf/2509.20214v1",
      "published": "2025-09-24T15:10:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20214v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks",
      "authors": [
        "Hailay Kidu Teklehaymanot",
        "Gebrearegawi Gidey",
        "Wolfgang Nejdl"
      ],
      "abstract": "Despite advances in Neural Machine Translation (NMT), low-resource languages\nlike Tigrinya remain underserved due to persistent challenges, including\nlimited corpora, inadequate tokenization strategies, and the lack of\nstandardized evaluation benchmarks. This paper investigates transfer learning\ntechniques using multilingual pretrained models to enhance translation quality\nfor morphologically rich, low-resource languages. We propose a refined approach\nthat integrates language-specific tokenization, informed embedding\ninitialization, and domain-adaptive fine-tuning. To enable rigorous assessment,\nwe construct a high-quality, human-aligned English-Tigrinya evaluation dataset\ncovering diverse domains. Experimental results demonstrate that transfer\nlearning with a custom tokenizer substantially outperforms zero-shot baselines,\nwith gains validated by BLEU, chrF, and qualitative human evaluation.\nBonferroni correction is applied to ensure statistical significance across\nconfigurations. Error analysis reveals key limitations and informs targeted\nrefinements. This study underscores the importance of linguistically aware\nmodeling and reproducible benchmarks in bridging the performance gap for\nunderrepresented languages. Resources are available at\nhttps://github.com/hailaykidu/MachineT_TigEng\n  and https://huggingface.co/Hailay/MachineT_TigEng",
      "pdf_url": "http://arxiv.org/pdf/2509.20209v1",
      "published": "2025-09-24T15:02:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20209v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50, 68T35",
        "I.2.7; H.3.1; I.2.6"
      ]
    },
    {
      "title": "Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs",
      "authors": [
        "Parker Glenn",
        "Alfy Samuel",
        "Daben Liu"
      ],
      "abstract": "Integrating LLM powered operators in declarative query languages allows for\nthe combination of cheap and interpretable functions with powerful,\ngeneralizable language model reasoning. However, in order to benefit from the\noptimized execution of a database query language like SQL, generated outputs\nmust align with the rules enforced by both type checkers and database contents.\nCurrent approaches address this challenge with orchestrations consisting of\nmany LLM-based post-processing calls to ensure alignment between generated\noutputs and database values, introducing performance bottlenecks. We perform a\nstudy on the ability of various sized open-source language models to both parse\nand execute functions within a query language based on SQL, showing that small\nlanguage models can excel as function executors over hybrid data sources. Then,\nwe propose an efficient solution to enforce the well-typedness of LLM\nfunctions, demonstrating 7% accuracy improvement on a multi-hop question\nanswering dataset with 53% improvement in latency over comparable solutions. We\nmake our implementation available at https://github.com/parkervg/blendsql",
      "pdf_url": "http://arxiv.org/pdf/2509.20208v1",
      "published": "2025-09-24T15:02:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20208v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ]
    },
    {
      "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation",
      "authors": [
        "Tanmay Khule",
        "Stefan Marksteiner",
        "Jose Alguindigue",
        "Hannes Fuchs",
        "Sebastian Fischmeister",
        "Apurva Narayan"
      ],
      "abstract": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess.",
      "pdf_url": "http://arxiv.org/pdf/2509.20190v1",
      "published": "2025-09-24T14:46:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20190v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "How People Manage Knowledge in their \"Second Brains\"- A Case Study with Industry Researchers Using Obsidian",
      "authors": [
        "Juliana Jansen Ferreira",
        "Vinícius Segura",
        "Joana Gabriela Souza",
        "Joao Henrique Gallas Brasil"
      ],
      "abstract": "People face overwhelming information during work activities, necessitating\neffective organization and management strategies. Even in personal lives,\nindividuals must keep, annotate, organize, and retrieve knowledge from daily\nroutines. The collection of records for future reference is known as a personal\nknowledge base. Note-taking applications are valuable tools for building and\nmaintaining these bases, often called a ''second brain''. This paper presents a\ncase study on how people build and explore personal knowledge bases for various\npurposes. We selected the note-taking tool Obsidian and researchers from a\nBrazilian lab for an in-depth investigation. Our investigation reveals\ninteresting findings about how researchers build and explore their personal\nknowledge bases. A key finding is that participants' knowledge retrieval\nstrategy influences how they build and maintain their content. We suggest\npotential features for an AI system to support this process.",
      "pdf_url": "http://arxiv.org/pdf/2509.20187v1",
      "published": "2025-09-24T14:45:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20187v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "An Improved Time Series Anomaly Detection by Applying Structural Similarity",
      "authors": [
        "Tiejun Wang",
        "Rui Wang",
        "Xudong Mou",
        "Mengyuan Ma",
        "Tianyu Wo",
        "Renyu Yang",
        "Xudong Liu"
      ],
      "abstract": "Effective anomaly detection in time series is pivotal for modern industrial\napplications and financial systems. Due to the scarcity of anomaly labels and\nthe high cost of manual labeling, reconstruction-based unsupervised approaches\nhave garnered considerable attention. However, accurate anomaly detection\nremains an unsettled challenge, since the optimization objectives of\nreconstruction-based methods merely rely on point-by-point distance measures,\nignoring the potential structural characteristics of time series and thus\nfailing to tackle complex pattern-wise anomalies. In this paper, we propose\nStrAD, a novel structure-enhanced anomaly detection approach to enrich the\noptimization objective by incorporating structural information hidden in the\ntime series and steering the data reconstruction procedure to better capture\nsuch structural features. StrAD accommodates the trend, seasonality, and shape\nin the optimization objective of the reconstruction model to learn latent\nstructural characteristics and capture the intrinsic pattern variation of time\nseries. The proposed structure-aware optimization objective mechanism can\nassure the alignment between the original data and the reconstructed data in\nterms of structural features, thereby keeping consistency in global fluctuation\nand local characteristics. The mechanism is pluggable and applicable to any\nreconstruction-based methods, enhancing the model sensitivity to both\npoint-wise anomalies and pattern-wise anomalies. Experimental results show that\nStrAD improves the performance of state-of-the-art reconstruction-based models\nacross five real-world anomaly detection datasets.",
      "pdf_url": "http://arxiv.org/pdf/2509.20184v1",
      "published": "2025-09-24T14:45:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20184v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Automated Multi-Agent Workflows for RTL Design",
      "authors": [
        "Amulya Bhattaram",
        "Janani Ramamoorthy",
        "Ranit Gupta",
        "Diana Marculescu",
        "Dimitrios Stamoulis"
      ],
      "abstract": "The rise of agentic AI workflows unlocks novel opportunities for computer\nsystems design and optimization. However, for specialized domains such as\nprogram synthesis, the relative scarcity of HDL and proprietary EDA resources\nonline compared to more common programming tasks introduces challenges, often\nnecessitating task-specific fine-tuning, high inference costs, and\nmanually-crafted agent orchestration. In this work, we present VeriMaAS, a\nmulti-agent framework designed to automatically compose agentic workflows for\nRTL code generation. Our key insight is to integrate formal verification\nfeedback from HDL tools directly into workflow generation, reducing the cost of\ngradient-based updates or prolonged reasoning traces. Our method improves\nsynthesis performance by 5-7% for pass@k over fine-tuned baselines, while\nrequiring only a few hundred training examples, representing an\norder-of-magnitude reduction in supervision cost.",
      "pdf_url": "http://arxiv.org/pdf/2509.20182v1",
      "published": "2025-09-24T14:44:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20182v1",
      "categories": [
        "cs.AR",
        "cs.AI"
      ]
    },
    {
      "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI",
      "authors": [
        "Lorenzo Giusti",
        "Ole Anton Werner",
        "Riccardo Taiello",
        "Matilde Carvalho Costa",
        "Emre Tosun",
        "Andrea Protani",
        "Marc Molina",
        "Rodrigo Lopes de Almeida",
        "Paolo Cacace",
        "Diogo Reis Santos",
        "Luigi Serio"
      ],
      "abstract": "We present Federation of Agents (FoA), a distributed orchestration framework\nthat transforms static multi-agent coordination into dynamic, capability-driven\ncollaboration. FoA introduces Versioned Capability Vectors (VCVs):\nmachine-readable profiles that make agent capabilities searchable through\nsemantic embeddings, enabling agents to advertise their capabilities, cost, and\nlimitations. Our aarchitecturecombines three key innovations: (1) semantic\nrouting that matches tasks to agents over sharded HNSW indices while enforcing\noperational constraints through cost-biased optimization, (2) dynamic task\ndecomposition where compatible agents collaboratively break down complex tasks\ninto DAGs of subtasks through consensus-based merging, and (3) smart clustering\nthat groups agents working on similar subtasks into collaborative channels for\nk-round refinement before synthesis. Built on top of MQTT,s publish-subscribe\nsemantics for scalable message passing, FoA achieves sub-linear complexity\nthrough hierarchical capability matching and efficient index maintenance.\nEvaluation on HealthBench shows 13x improvements over single-model baselines,\nwith clustering-enhanced laboration particularly effective for complex\nreasoning tasks requiring multiple perspectives. The system scales horizontally\nwhile maintaining consistent performance, demonstrating that semantic\norchestration with structured collaboration can unlock the collective\nintelligence of heterogeneous federations of AI agents.",
      "pdf_url": "http://arxiv.org/pdf/2509.20175v1",
      "published": "2025-09-24T14:38:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20175v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning",
      "authors": [
        "Lauren Deason",
        "Adam Bali",
        "Ciprian Bejean",
        "Diana Bolocan",
        "James Crnkovich",
        "Ioana Croitoru",
        "Krishna Durai",
        "Chase Midler",
        "Calin Miron",
        "David Molnar",
        "Brad Moon",
        "Bruno Ostarcevic",
        "Alberto Peltea",
        "Matt Rosenberg",
        "Catalin Sandu",
        "Arthur Saputkin",
        "Sagar Shah",
        "Daniel Stan",
        "Ernest Szocs",
        "Shengye Wan",
        "Spencer Whitman",
        "Sven Krasser",
        "Joshua Saxe"
      ],
      "abstract": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2509.20166v1",
      "published": "2025-09-24T14:33:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20166v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation",
      "authors": [
        "Chaojun Nie",
        "Jun Zhou",
        "Guanxiang Wang",
        "Shisong Wud",
        "Zichen Wang"
      ],
      "abstract": "Large language models (LLMs) often exhibit limited performance on\ndomain-specific tasks due to the natural disproportionate representation of\nspecialized information in their training data and the static nature of these\ndatasets. Knowledge scarcity and temporal lag create knowledge gaps for domain\napplications. While post-training on domain datasets can embed knowledge into\nmodels, existing approaches have some limitations. Continual Pre-Training (CPT)\ntreats all tokens in domain documents with equal importance, failing to\nprioritize critical knowledge points, while supervised fine-tuning (SFT) with\nquestion-answer pairs struggles to develop the coherent knowledge structures\nnecessary for complex reasoning tasks. To address these challenges, we propose\nReinforcement Learning from Augmented Generation (RLAG). Our approach\niteratively cycles between sampling generations and optimizing the model\nthrough calculated rewards, effectively embedding critical and contextually\ncoherent domain knowledge. We select generated outputs with the highest log\nprobabilities as the sampling result, then compute three tailored reward\nmetrics to guide the optimization process. To comprehensively evaluate domain\nexpertise, we assess answer accuracy and the rationality of explanations\ngenerated for correctly answered questions. Experimental results across\nmedical, legal, astronomy, and current events datasets demonstrate that our\nproposed method significantly outperforms baseline approaches. Our code and\ndata are open sourced at https://github.com/ChaojunNie/RLAG.",
      "pdf_url": "http://arxiv.org/pdf/2509.20162v1",
      "published": "2025-09-24T14:30:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20162v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT",
      "authors": [
        "Zhi Qin Tan",
        "Xiatian Zhu",
        "Owen Addison",
        "Yunpeng Li"
      ],
      "abstract": "Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography\n(CBCT) is vital for clinical applications like treatment planning and\ndiagnosis. However, this process requires extensive expertise and is\nexceptionally time-consuming, highlighting the critical need for automated\nalgorithms that can effectively utilize unlabeled data. In this paper, we\npropose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on\nthe U-Mamba2 model and employs a multi-stage training strategy. The framework\nfirst pre-trains U-Mamba2 in a self-supervised manner using a disruptive\nautoencoder. It then leverages unlabeled data through consistency\nregularization, where we introduce input and feature perturbations to ensure\nstable model outputs. Finally, a pseudo-labeling strategy is implemented with a\nreduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL\nachieved an average score of 0.872 and a DSC of 0.969 on the validation\ndataset, demonstrating the superior performance of our approach. The code is\navailable at https://github.com/zhiqin1998/UMamba2.",
      "pdf_url": "http://arxiv.org/pdf/2509.20154v1",
      "published": "2025-09-24T14:19:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20154v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Affective Computing and Emotional Data: Challenges and Implications in Privacy Regulations, The AI Act, and Ethics in Large Language Models",
      "authors": [
        "Nicola Fabiano"
      ],
      "abstract": "This paper examines the integration of emotional intelligence into artificial\nintelligence systems, with a focus on affective computing and the growing\ncapabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to\nrecognize and respond to human emotions. Drawing on interdisciplinary research\nthat combines computer science, psychology, and neuroscience, the study\nanalyzes foundational neural architectures - CNNs for processing facial\nexpressions and RNNs for sequential data, such as speech and text - that enable\nemotion recognition. It examines the transformation of human emotional\nexperiences into structured emotional data, addressing the distinction between\nexplicit emotional data collected with informed consent in research settings\nand implicit data gathered passively through everyday digital interactions.\nThat raises critical concerns about lawful processing, AI transparency, and\nindividual autonomy over emotional expressions in digital environments. The\npaper explores implications across various domains, including healthcare,\neducation, and customer service, while addressing challenges of cultural\nvariations in emotional expression and potential biases in emotion recognition\nsystems across different demographic groups. From a regulatory perspective, the\npaper examines emotional data in the context of the GDPR and the EU AI Act\nframeworks, highlighting how emotional data may be considered sensitive\npersonal data that requires robust safeguards, including purpose limitation,\ndata minimization, and meaningful consent mechanisms.",
      "pdf_url": "http://arxiv.org/pdf/2509.20153v2",
      "published": "2025-09-24T14:18:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20153v2",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models",
      "authors": [
        "Botai Yuan",
        "Yutian Zhou",
        "Yingjie Wang",
        "Fushuo Huo",
        "Yongcheng Jing",
        "Li Shen",
        "Ying Wei",
        "Zhiqi Shen",
        "Ziwei Liu",
        "Tianwei Zhang",
        "Jie Yang",
        "Dacheng Tao"
      ],
      "abstract": "Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize\nleaderboard accuracy, overlooking reliability and safety. We study sycophancy\n-- models' tendency to uncritically echo user-provided information -- in\nhigh-stakes clinical settings. We introduce EchoBench, a benchmark to\nsystematically evaluate sycophancy in medical LVLMs. It contains 2,122 images\nacross 18 departments and 20 modalities with 90 prompts that simulate biased\ninputs from patients, medical students, and physicians. We evaluate\nmedical-specific, open-source, and proprietary LVLMs. All exhibit substantial\nsycophancy; the best proprietary model (Claude 3.7 Sonnet) still shows 45.98%\nsycophancy, and GPT-4.1 reaches 59.15%. Many medical-specific models exceed 95%\nsycophancy despite only moderate accuracy. Fine-grained analyses by bias type,\ndepartment, perceptual granularity, and modality identify factors that increase\nsusceptibility. We further show that higher data quality/diversity and stronger\ndomain knowledge reduce sycophancy without harming unbiased accuracy. EchoBench\nalso serves as a testbed for mitigation: simple prompt-level interventions\n(negative prompting, one-shot, few-shot) produce consistent reductions and\nmotivate training- and decoding-time strategies. Our findings highlight the\nneed for robust evaluation beyond accuracy and provide actionable guidance\ntoward safer, more trustworthy medical LVLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.20146v1",
      "published": "2025-09-24T14:09:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20146v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Formal Verification of Minimax Algorithms",
      "authors": [
        "Wieger Wesselink",
        "Kees Huizing",
        "Huub van de Wetering"
      ],
      "abstract": "Using the Dafny verification system, we formally verify a range of minimax\nsearch algorithms, including variations with alpha-beta pruning and\ntransposition tables. For depth-limited search with transposition tables, we\nintroduce a witness-based correctness criterion and apply it to two\nrepresentative algorithms. All verification artifacts, including proofs and\nPython implementations, are publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2509.20138v1",
      "published": "2025-09-24T14:02:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20138v1",
      "categories": [
        "cs.AI",
        "68Q60, 68T20"
      ]
    },
    {
      "title": "KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial Animation",
      "authors": [
        "Tianle Lyu",
        "Junchuan Zhao",
        "Ye Wang"
      ],
      "abstract": "Audio-driven facial animation has made significant progress in multimedia\napplications, with diffusion models showing strong potential for talking-face\nsynthesis. However, most existing works treat speech features as a monolithic\nrepresentation and fail to capture their fine-grained roles in driving\ndifferent facial motions, while also overlooking the importance of modeling\nkeyframes with intense dynamics. To address these limitations, we propose\nKSDiff, a Keyframe-Augmented Speech-Aware Dual-Path Diffusion framework.\nSpecifically, the raw audio and transcript are processed by a Dual-Path Speech\nEncoder (DPSE) to disentangle expression-related and head-pose-related\nfeatures, while an autoregressive Keyframe Establishment Learning (KEL) module\npredicts the most salient motion frames. These components are integrated into a\nDual-path Motion generator to synthesize coherent and realistic facial motions.\nExtensive experiments on HDTF and VoxCeleb demonstrate that KSDiff achieves\nstate-of-the-art performance, with improvements in both lip synchronization\naccuracy and head-pose naturalness. Our results highlight the effectiveness of\ncombining speech disentanglement with keyframe-aware diffusion for talking-head\ngeneration.",
      "pdf_url": "http://arxiv.org/pdf/2509.20128v1",
      "published": "2025-09-24T13:54:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20128v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ]
    },
    {
      "title": "Discovering Association Rules in High-Dimensional Small Tabular Data",
      "authors": [
        "Erkan Karabulut",
        "Daniel Daza",
        "Paul Groth",
        "Victoria Degeler"
      ],
      "abstract": "Association Rule Mining (ARM) aims to discover patterns between features in\ndatasets in the form of propositional rules, supporting both knowledge\ndiscovery and interpretable machine learning in high-stakes decision-making.\nHowever, in high-dimensional settings, rule explosion and computational\noverhead render popular algorithmic approaches impractical without effective\nsearch space reduction, challenges that propagate to downstream tasks.\nNeurosymbolic methods, such as Aerial+, have recently been proposed to address\nthe rule explosion in ARM. While they tackle the high dimensionality of the\ndata, they also inherit limitations of neural networks, particularly reduced\nperformance in low-data regimes.\n  This paper makes three key contributions to association rule discovery in\nhigh-dimensional tabular data. First, we empirically show that Aerial+ scales\none to two orders of magnitude better than state-of-the-art algorithmic and\nneurosymbolic baselines across five real-world datasets. Second, we introduce\nthe novel problem of ARM in high-dimensional, low-data settings, such as gene\nexpression data from the biomedicine domain with around 18k features and 50\nsamples. Third, we propose two fine-tuning approaches to Aerial+ using tabular\nfoundation models. Our proposed approaches are shown to significantly improve\nrule quality on five real-world datasets, demonstrating their effectiveness in\nlow-data, high-dimensional scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2509.20113v2",
      "published": "2025-09-24T13:37:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20113v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving",
      "authors": [
        "Pengxiang Li",
        "Yinan Zheng",
        "Yue Wang",
        "Huimin Wang",
        "Hang Zhao",
        "Jingjing Liu",
        "Xianyuan Zhan",
        "Kun Zhan",
        "Xianpeng Lang"
      ],
      "abstract": "End-to-End (E2E) solutions have emerged as a mainstream approach for\nautonomous driving systems, with Vision-Language-Action (VLA) models\nrepresenting a new paradigm that leverages pre-trained multimodal knowledge\nfrom Vision-Language Models (VLMs) to interpret and interact with complex\nreal-world environments. However, these methods remain constrained by the\nlimitations of imitation learning, which struggles to inherently encode\nphysical rules during training. Existing approaches often rely on complex\nrule-based post-refinement, employ reinforcement learning that remains largely\nlimited to simulation, or utilize diffusion guidance that requires\ncomputationally expensive gradient calculations. To address these challenges,\nwe introduce ReflectDrive, a novel learning-based framework that integrates a\nreflection mechanism for safe trajectory generation via discrete diffusion. We\nfirst discretize the two-dimensional driving space to construct an action\ncodebook, enabling the use of pre-trained Diffusion Language Models for\nplanning tasks through fine-tuning. Central to our approach is a safety-aware\nreflection mechanism that performs iterative self-correction without gradient\ncomputation. Our method begins with goal-conditioned trajectory generation to\nmodel multi-modal driving behaviors. Based on this, we apply local search\nmethods to identify unsafe tokens and determine feasible solutions, which then\nserve as safe anchors for inpainting-based regeneration. Evaluated on the\nNAVSIM benchmark, ReflectDrive demonstrates significant advantages in\nsafety-critical trajectory generation, offering a scalable and reliable\nsolution for autonomous driving systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.20109v1",
      "published": "2025-09-24T13:35:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20109v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models",
      "authors": [
        "Juana Valeria Hurtado",
        "Rohit Mohan",
        "Abhinav Valada"
      ],
      "abstract": "Hyperspectral imaging (HSI) captures spatial information along with dense\nspectral measurements across numerous narrow wavelength bands. This rich\nspectral content has the potential to facilitate robust robotic perception,\nparticularly in environments with complex material compositions, varying\nillumination, or other visually challenging conditions. However, current HSI\nsemantic segmentation methods underperform due to their reliance on\narchitectures and learning frameworks optimized for RGB inputs. In this work,\nwe propose a novel hyperspectral adapter that leverages pretrained vision\nfoundation models to effectively learn from hyperspectral data. Our\narchitecture incorporates a spectral transformer and a spectrum-aware spatial\nprior module to extract rich spatial-spectral features. Additionally, we\nintroduce a modality-aware interaction block that facilitates effective\nintegration of hyperspectral representations and frozen vision Transformer\nfeatures through dedicated extraction and injection mechanisms. Extensive\nevaluations on three benchmark autonomous driving datasets demonstrate that our\narchitecture achieves state-of-the-art semantic segmentation performance while\ndirectly using HSI inputs, outperforming both vision-based and hyperspectral\nsegmentation methods. We make the code available at\nhttps://hsi-adapter.cs.uni-freiburg.de.",
      "pdf_url": "http://arxiv.org/pdf/2509.20107v2",
      "published": "2025-09-24T13:32:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20107v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs",
      "authors": [
        "Venkat Margapuri",
        "Garik Kazanjian",
        "Naren Kosaraju"
      ],
      "abstract": "Large Language Models (LLMs) often struggle with maintaining coherent\nmulti-step reasoning traces, particularly in tasks that require a structured\nlogical flow. This work introduces a quantum-inspired approach to address the\nchallenge by incorporating a fidelity-based reward derived from Projected\nEntangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior\napproaches that use direct supervision or contrastive objectives, the proposed\nmethod guides learning through structural consistency, offering a novel\napproach to enforce global coherence in generated reasoning traces. The\nproposed framework is evaluated using multiple coherence-determining metrics on\ndiverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning\narithmetic, intuitive, and entailment-based reasoning. Results show that the\nproposed quantum-inspired approach offers significant improvements over\nsupervised, contrastive, and pretrained baseline approaches, highlighting the\neffectiveness of quantum-inspired fidelity as a foundation to improve reasoning\ntrace coherence in LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.20105v1",
      "published": "2025-09-24T13:29:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20105v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Steerable Adversarial Scenario Generation through Test-Time Preference Alignment",
      "authors": [
        "Tong Nie",
        "Yuewen Mei",
        "Yihong Tang",
        "Junlin He",
        "Jie Sun",
        "Haotian Shi",
        "Wei Ma",
        "Jian Sun"
      ],
      "abstract": "Adversarial scenario generation is a cost-effective approach for safety\nassessment of autonomous driving systems. However, existing methods are often\nconstrained to a single, fixed trade-off between competing objectives such as\nadversariality and realism. This yields behavior-specific models that cannot be\nsteered at inference time, lacking the efficiency and flexibility to generate\ntailored scenarios for diverse training and testing requirements. In view of\nthis, we reframe the task of adversarial scenario generation as a\nmulti-objective preference alignment problem and introduce a new framework\nnamed \\textbf{S}teerable \\textbf{A}dversarial scenario \\textbf{GE}nerator\n(SAGE). SAGE enables fine-grained test-time control over the trade-off between\nadversariality and realism without any retraining. We first propose\nhierarchical group-based preference optimization, a data-efficient offline\nalignment method that learns to balance competing objectives by decoupling hard\nfeasibility constraints from soft preferences. Instead of training a fixed\nmodel, SAGE fine-tunes two experts on opposing preferences and constructs a\ncontinuous spectrum of policies at inference time by linearly interpolating\ntheir weights. We provide theoretical justification for this framework through\nthe lens of linear mode connectivity. Extensive experiments demonstrate that\nSAGE not only generates scenarios with a superior balance of adversariality and\nrealism but also enables more effective closed-loop training of driving\npolicies. Project page: https://tongnie.github.io/SAGE/.",
      "pdf_url": "http://arxiv.org/pdf/2509.20102v1",
      "published": "2025-09-24T13:27:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20102v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Integrated Framework for LLM Evaluation with Answer Generation",
      "authors": [
        "Sujeong Lee",
        "Hayoung Lee",
        "Seongsoo Heo",
        "Wonik Choi"
      ],
      "abstract": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies.",
      "pdf_url": "http://arxiv.org/pdf/2509.20097v1",
      "published": "2025-09-24T13:20:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20097v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms",
      "authors": [
        "Aymeric Vellinger",
        "Nemanja Antonic",
        "Elio Tuci"
      ],
      "abstract": "Swarm intelligence emerges from decentralised interactions among simple\nagents, enabling collective problem-solving. This study establishes a\ntheoretical equivalence between pheromone-mediated aggregation in \\celeg\\ and\nreinforcement learning (RL), demonstrating how stigmergic signals function as\ndistributed reward mechanisms. We model engineered nematode swarms performing\nforaging tasks, showing that pheromone dynamics mathematically mirror\ncross-learning updates, a fundamental RL algorithm. Experimental validation\nwith data from literature confirms that our model accurately replicates\nempirical \\celeg\\ foraging patterns under static conditions. In dynamic\nenvironments, persistent pheromone trails create positive feedback loops that\nhinder adaptation by locking swarms into obsolete choices. Through\ncomputational experiments in multi-armed bandit scenarios, we reveal that\nintroducing a minority of exploratory agents insensitive to pheromones restores\ncollective plasticity, enabling rapid task switching. This behavioural\nheterogeneity balances exploration-exploitation trade-offs, implementing\nswarm-level extinction of outdated strategies. Our results demonstrate that\nstigmergic systems inherently encode distributed RL processes, where\nenvironmental signals act as external memory for collective credit assignment.\nBy bridging synthetic biology with swarm robotics, this work advances\nprogrammable living systems capable of resilient decision-making in volatile\nenvironments.",
      "pdf_url": "http://arxiv.org/pdf/2509.20095v1",
      "published": "2025-09-24T13:16:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20095v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Causal Understanding by LLMs: The Role of Uncertainty",
      "authors": [
        "Oscar Lithgow-Serrano",
        "Vani Kanjirangat",
        "Alessandro Antonucci"
      ],
      "abstract": "Recent papers show LLMs achieve near-random accuracy in causal relation\nclassification, raising questions about whether such failures arise from\nlimited pretraining exposure or deeper representational gaps. We investigate\nthis under uncertainty-based evaluation, testing whether pretraining exposure\nto causal examples improves causal understanding >18K PubMed sentences -- half\nfrom The Pile corpus, half post-2024 -- across seven models\n(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model\nbehavior through: (i) causal classification, where the model identifies causal\nrelationships in text, and (ii) verbatim memorization probing, where we assess\nwhether the model prefers previously seen causal statements over their\nparaphrases. Models perform four-way classification\n(direct/conditional/correlational/no-relationship) and select between originals\nand their generated paraphrases. Results show almost identical accuracy on\nseen/unseen sentences (p > 0.05), no memorization bias (24.8% original\nselection), and output distribution over the possible options is almost flat,\nwith entropic values near the maximum (1.35/1.39), confirming random guessing.\nInstruction-tuned models show severe miscalibration (Qwen: > 95% confidence,\n32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%\nvs. direct). These findings suggest that failures in causal understanding arise\nfrom the lack of structured causal representation, rather than insufficient\nexposure to causal examples during pretraining.",
      "pdf_url": "http://arxiv.org/pdf/2509.20088v1",
      "published": "2025-09-24T13:06:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20088v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM",
      "authors": [
        "Wenliang Li",
        "Rui Yan",
        "Xu Zhang",
        "Li Chen",
        "Hongji Zhu",
        "Jing Zhao",
        "Junjun Li",
        "Mengru Li",
        "Wei Cao",
        "Zihang Jiang",
        "Wei Wei",
        "Kun Zhang",
        "Shaohua Kevin Zhou"
      ],
      "abstract": "Large language models (LLMs) have demonstrated notable potential in medical\napplications, yet they face substantial challenges in handling complex\nreal-world clinical diagnoses using conventional prompting methods. Current\nprompt engineering and multi-agent approaches typically optimize isolated\ninferences, neglecting the accumulation of reusable clinical experience. To\naddress this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)\nframework, which allows LLMs to self-learn clinical knowledge via a multi-agent\npipeline that summarizes, refines, and applies diagnostic insights. It mirrors\nhow physicians develop expertise through experience, enabling more focused and\naccurate diagnosis on key disease-specific cues. We further extend it to a\nMACD-human collaborative workflow, where multiple LLM-based diagnostician\nagents engage in iterative consultations, supported by an evaluator agent and\nhuman oversight for cases where agreement is not reached. Evaluated on 4,390\nreal-world patient cases across seven diseases using diverse open-source LLMs\n(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves\nprimary diagnostic accuracy, outperforming established clinical guidelines with\ngains up to 22.3% (MACD). On the subset of the data, it achieves performance on\npar with or exceeding that of human physicians (up to 16% improvement over\nphysicians-only diagnosis). Additionally, on the MACD-human workflow, it\nachieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,\nself-learned knowledge exhibits strong cross-model stability, transferability,\nand model-specific personalization, while the system can generate traceable\nrationales, enhancing explainability. Consequently, this work presents a\nscalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap\nbetween the intrinsic knowledge of LLMs and real-world clinical practice.",
      "pdf_url": "http://arxiv.org/pdf/2509.20067v2",
      "published": "2025-09-24T12:37:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20067v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Responsible AI Technical Report",
      "authors": [
        "KT",
        ":",
        "Soonmin Bae",
        "Wanjin Park",
        "Jeongyeop Kim",
        "Yunjin Park",
        "Jungwon Yoon",
        "Junhyung Moon",
        "Myunggyo Oh",
        "Wonhyuk Lee",
        "Junseo Jang",
        "Dongyoung Jung",
        "Minwook Ju",
        "Eunmi Kim",
        "Sujin Kim",
        "Youngchol Kim",
        "Somin Lee",
        "Wonyoung Lee",
        "Minsung Noh",
        "Hyoungjun Park",
        "Eunyoung Shin"
      ],
      "abstract": "KT developed a Responsible AI (RAI) assessment methodology and risk\nmitigation technologies to ensure the safety and reliability of AI services. By\nanalyzing the Basic Act on AI implementation and global AI governance trends,\nwe established a unique approach for regulatory compliance and systematically\nidentify and manage all potential risk factors from AI development to\noperation. We present a reliable assessment methodology that systematically\nverifies model safety and robustness based on KT's AI risk taxonomy tailored to\nthe domestic environment. We also provide practical tools for managing and\nmitigating identified AI risks. With the release of this report, we also\nrelease proprietary Guardrail : SafetyGuard that blocks harmful responses from\nAI models in real-time, supporting the enhancement of safety in the domestic AI\ndevelopment ecosystem. We also believe these research outcomes provide valuable\ninsights for organizations seeking to develop Responsible AI.",
      "pdf_url": "http://arxiv.org/pdf/2509.20057v1",
      "published": "2025-09-24T12:26:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20057v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "One Filters All: A Generalist Filter for State Estimation",
      "authors": [
        "Shiqi Liu",
        "Wenhan Cao",
        "Chang Liu",
        "Zeyu He",
        "Tianyi Zhang",
        "Shengbo Eben Li"
      ],
      "abstract": "Estimating hidden states in dynamical systems, also known as optimal\nfiltering, is a long-standing problem in various fields of science and\nengineering. In this paper, we introduce a general filtering framework,\n\\textbf{LLM-Filter}, which leverages large language models (LLMs) for state\nestimation by embedding noisy observations with text prototypes. In various\nexperiments for classical dynamical systems, we find that first, state\nestimation can significantly benefit from the reasoning knowledge embedded in\npre-trained LLMs. By achieving proper modality alignment with the frozen LLM,\nLLM-Filter outperforms the state-of-the-art learning-based approaches. Second,\nwe carefully design the prompt structure, System-as-Prompt (SaP), incorporating\ntask instructions that enable the LLM to understand the estimation tasks.\nGuided by these prompts, LLM-Filter exhibits exceptional generalization,\ncapable of performing filtering tasks accurately in changed or even unseen\nenvironments. We further observe a scaling-law behavior in LLM-Filter, where\naccuracy improves with larger model sizes and longer training times. These\nfindings make LLM-Filter a promising foundation model of filtering.",
      "pdf_url": "http://arxiv.org/pdf/2509.20051v1",
      "published": "2025-09-24T12:19:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20051v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Projective Kolmogorov Arnold Neural Networks (P-KANs): Entropy-Driven Functional Space Discovery for Interpretable Machine Learning",
      "authors": [
        "Alastair Poole",
        "Stig McArthur",
        "Saravan Kumar"
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) relocate learnable nonlinearities from\nnodes to edges, demonstrating remarkable capabilities in scientific machine\nlearning and interpretable modeling. However, current KAN implementations\nsuffer from fundamental inefficiencies due to redundancy in high-dimensional\nspline parameter spaces, where numerous distinct parameterisations yield\nfunctionally equivalent behaviors. This redundancy manifests as a \"nuisance\nspace\" in the model's Jacobian, leading to susceptibility to overfitting and\npoor generalization. We introduce Projective Kolmogorov-Arnold Networks\n(P-KANs), a novel training framework that guides edge function discovery\ntowards interpretable functional representations through entropy-minimisation\ntechniques from signal analysis and sparse dictionary learning. Rather than\nconstraining functions to predetermined spaces, our approach maintains spline\nspace flexibility while introducing \"gravitational\" terms that encourage\nconvergence towards optimal functional representations. Our key insight\nrecognizes that optimal representations can be identified through entropy\nanalysis of projection coefficients, compressing edge functions to\nlower-parameter projective spaces (Fourier, Chebyshev, Bessel). P-KANs\ndemonstrate superior performance across multiple domains, achieving up to 80%\nparameter reduction while maintaining representational capacity, significantly\nimproved robustness to noise compared to standard KANs, and successful\napplication to industrial automated fiber placement prediction. Our approach\nenables automatic discovery of mixed functional representations where different\nedges converge to different optimal spaces, providing both compression benefits\nand enhanced interpretability for scientific machine learning applications.",
      "pdf_url": "http://arxiv.org/pdf/2509.20049v1",
      "published": "2025-09-24T12:15:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20049v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations",
      "authors": [
        "Rami Zewail"
      ],
      "abstract": "Learning robust representations for biosignals is often hampered by the\nchallenge of designing effective data augmentations.Traditional methods can\nfail to capture the complex variations inherent in physiological data. Within\nthis context, we propose a novel hybrid framework, Diffusion-Augmented\nContrastive Learning (DACL), that fuses concepts from diffusion models and\nsupervised contrastive learning. The DACL framework operates on a latent space\ncreated by a lightweight Variational Autoencoder (VAE) trained on our novel\nScattering Transformer (ST) features [12]. It utilizes the diffusion forward\nprocess as a principled data augmentation technique to generate multiple noisy\nviews of these latent embeddings. A U-Net style encoder is then trained with a\nsupervised contrastive objective to learn a representation that balances class\ndiscrimination with robustness to noise across various diffusion time steps. We\nevaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset,\nachieving a competitive AUROC of 0.7815. This work establishes a new paradigm\nfor representation learning by using the diffusion process itself to drive the\ncontrastive objective, creating noise-invariant embeddings that demonstrate a\nstrong foundation for class separability.",
      "pdf_url": "http://arxiv.org/pdf/2509.20048v1",
      "published": "2025-09-24T12:15:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20048v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ]
    },
    {
      "title": "Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks",
      "authors": [
        "Vani Kanjirangat",
        "Tanja Samardžić",
        "Ljiljana Dolamic",
        "Fabio Rinaldi"
      ],
      "abstract": "Dialectal data are characterized by linguistic variation that appears small\nto humans but has a significant impact on the performance of models. This\ndialect gap has been related to various factors (e.g., data size, economic and\nsocial factors) whose impact, however, turns out to be inconsistent. In this\nwork, we investigate factors impacting the model performance more directly: we\ncorrelate Tokenization Parity (TP) and Information Parity (IP), as measures of\nrepresentational biases in pre-trained multilingual models, with the downstream\nperformance. We compare state-of-the-art decoder-only LLMs with encoder-based\nmodels across three tasks: dialect classification, topic classification, and\nextractive question answering, controlling for varying scripts (Latin vs.\nnon-Latin) and resource availability (high vs. low). Our analysis reveals that\nTP is a better predictor of the performance on tasks reliant on syntactic and\nmorphological cues (e.g., extractive QA), while IP better predicts performance\nin semantic tasks (e.g., topic classification). Complementary analyses,\nincluding tokenizer behavior, vocabulary coverage, and qualitative insights,\nreveal that the language support claims of LLMs often might mask deeper\nmismatches at the script or token level.",
      "pdf_url": "http://arxiv.org/pdf/2509.20045v1",
      "published": "2025-09-24T12:13:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.20045v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}