{
  "last_updated": "2025-07-11T00:55:20.498991",
  "papers": [
    {
      "title": "An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator",
      "authors": [
        "Yulin An",
        "Enrique del Castillo"
      ],
      "abstract": "The spectrum of the Laplace-Beltrami (LB) operator is central in geometric\ndeep learning tasks, capturing intrinsic properties of the shape of the object\nunder consideration. The best established method for its estimation, from a\ntriangulated mesh of the object, is based on the Finite Element Method (FEM),\nand computes the top k LB eigenvalues with a complexity of O(Nk), where N is\nthe number of points. This can render the FEM method inefficient when\nrepeatedly applied to databases of CAD mechanical parts, or in quality control\napplications where part metrology is acquired as large meshes and decisions\nabout the quality of each part are needed quickly and frequently. As a solution\nto this problem, we present a geometric deep learning framework to predict the\nLB spectrum efficiently given the CAD mesh of a part, achieving significant\ncomputational savings without sacrificing accuracy, demonstrating that the LB\nspectrum is learnable. The proposed Graph Neural Network architecture uses a\nrich set of part mesh features - including Gaussian curvature, mean curvature,\nand principal curvatures. In addition to our trained network, we make\navailable, for repeatability, a large curated dataset of real-world mechanical\nCAD models derived from the publicly available ABC dataset used for training\nand testing. Experimental results show that our method reduces computation time\nof the LB spectrum by approximately 5 times over linear FEM while delivering\ncompetitive accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2507.07073v1",
      "published": "2025-07-09T17:31:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.07073v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering",
      "authors": [
        "Shahana Yasmin Chowdhury",
        "Bithi Banik",
        "Md Tamjidul Hoque",
        "Shreya Banerjee"
      ],
      "abstract": "Nowadays, speech emotion recognition (SER) plays a vital role in the field of\nhuman-computer interaction (HCI) and the evolution of artificial intelligence\n(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:\nneutral, happy, sad, angry, fear, disgust, and surprise, which are trained on\nfive datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).\nThe model achieves high accuracy on individual datasets, including 97.83% on\nRAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS\nand EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,\noutperforming previously reported results. To our knowledge, no existing study\nhas evaluated a single SER model across all five benchmark datasets (i.e.,\nR+T+S+C+E) simultaneously. In our work, we introduce this comprehensive\ncombination and achieve a remarkable overall accuracy of 93.76%. These results\nconfirm the robustness and generalizability of our DCRF-BiLSTM framework across\ndiverse datasets.",
      "pdf_url": "http://arxiv.org/pdf/2507.07046v1",
      "published": "2025-07-09T17:07:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.07046v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "Surrogate Model for Heat Transfer Prediction in Impinging Jet Arrays using Dynamic Inlet/Outlet and Flow Rate Control",
      "authors": [
        "Mikael Vaillant",
        "Victor Oliveira Ferreira",
        "Wiebke Mainville",
        "Jean-Michel Lamarre",
        "Vincent Raymond",
        "Moncef Chioua",
        "Bruno Blais"
      ],
      "abstract": "This study presents a surrogate model designed to predict the Nusselt number\ndistribution in an enclosed impinging jet arrays, where each jet function\nindependently and where jets can be transformed from inlets to outlets, leading\nto a vast number of possible flow arrangements. While computational fluid\ndynamics (CFD) simulations can model heat transfer with high fidelity, their\ncost prohibits real-time application such as model-based temperature control.\nTo address this, we generate a CNN-based surrogate model that can predict the\nNusselt distribution in real time. We train it with data from implicit large\neddy computational fluid dynamics simulations (Re < 2,000). We train two\ndistinct models, one for a five by one array of jets (83 simulations) and one\nfor a three by three array of jets (100 simulations). We introduce a method to\nextrapolate predictions to higher Reynolds numbers (Re < 10,000) using a\ncorrelation-based scaling. The surrogate models achieve high accuracy, with a\nnormalized mean average error below 2% on validation data for the five by one\nsurrogate model and 0.6% for the three by three surrogate model. Experimental\nvalidation confirms the model's predictive capabilities. This work provides a\nfoundation for model-based control strategies in advanced thermal management\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2507.07034v1",
      "published": "2025-07-09T17:03:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.07034v1",
      "categories": [
        "physics.flu-dyn",
        "cs.AI"
      ]
    },
    {
      "title": "Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices",
      "authors": [
        "Parshva Dhilankumar Patel"
      ],
      "abstract": "This paper presents the design and development of an OCR-powered pipeline for\nefficient table extraction from invoices. The system leverages Tesseract OCR\nfor text recognition and custom post-processing logic to detect, align, and\nextract structured tabular data from scanned invoice documents. Our approach\nincludes dynamic preprocessing, table boundary detection, and row-column\nmapping, optimized for noisy and non-standard invoice formats. The resulting\npipeline significantly improves data extraction accuracy and consistency,\nsupporting real-world use cases such as automated financial workflows and\ndigital archiving.",
      "pdf_url": "http://arxiv.org/pdf/2507.07029v1",
      "published": "2025-07-09T16:59:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.07029v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.10; I.4.9; H.3.1"
      ]
    },
    {
      "title": "FlexOlmo: Open Language Models for Flexible Data Use",
      "authors": [
        "Weijia Shi",
        "Akshita Bhagia",
        "Kevin Farhat",
        "Niklas Muennighoff",
        "Pete Walsh",
        "Jacob Morrison",
        "Dustin Schwenk",
        "Shayne Longpre",
        "Jake Poznanski",
        "Allyson Ettinger",
        "Daogao Liu",
        "Margaret Li",
        "Dirk Groeneveld",
        "Mike Lewis",
        "Wen-tau Yih",
        "Luca Soldaini",
        "Kyle Lo",
        "Noah A. Smith",
        "Luke Zettlemoyer",
        "Pang Wei Koh",
        "Hannaneh Hajishirzi",
        "Ali Farhadi",
        "Sewon Min"
      ],
      "abstract": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.",
      "pdf_url": "http://arxiv.org/pdf/2507.07024v1",
      "published": "2025-07-09T16:54:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.07024v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "First Return, Entropy-Eliciting Explore",
      "authors": [
        "Tianyu Zheng",
        "Tianshun Xing",
        "Qingshui Gu",
        "Taoran Liang",
        "Xingwei Qu",
        "Xin Zhou",
        "Yizhi Li",
        "Zhoufutu Wen",
        "Chenghua Lin",
        "Wenhao Huang",
        "Qian Liu",
        "Ge Zhang",
        "Zejun Ma"
      ],
      "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
      "pdf_url": "http://arxiv.org/pdf/2507.07017v1",
      "published": "2025-07-09T16:45:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.07017v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing",
      "authors": [
        "Eunbyeol Cho",
        "Jiyoun Kim",
        "Minjae Lee",
        "Sungjin Park",
        "Edward Choi"
      ],
      "abstract": "Electronic Health Records (EHR) are time-series relational databases that\nrecord patient interactions and medical events over time, serving as a critical\nresource for healthcare research and applications. However, privacy concerns\nand regulatory restrictions limit the sharing and utilization of such sensitive\ndata, necessitating the generation of synthetic EHR datasets. Unlike previous\nEHR synthesis methods, which typically generate medical records consisting of\nexpert-chosen features (e.g. a few vital signs or structured codes only), we\nintroduce RawMed, the first framework to synthesize multi-table, time-series\nEHR data that closely resembles raw EHRs. Using text-based representation and\ncompression techniques, RawMed captures complex structures and temporal\ndynamics with minimal preprocessing. We also propose a new evaluation framework\nfor multi-table time-series synthetic EHRs, assessing distributional\nsimilarity, inter-table relationships, temporal dynamics, and privacy.\nValidated on two open-source EHR datasets, RawMed outperforms baseline models\nin fidelity and utility. The code is available at\nhttps://github.com/eunbyeol-cho/RawMed.",
      "pdf_url": "http://arxiv.org/pdf/2507.06996v1",
      "published": "2025-07-09T16:22:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06996v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients",
      "authors": [
        "Qilong Xing",
        "Zikai Song",
        "Bingxin Gong",
        "Lian Yang",
        "Junqing Yu",
        "Wei Yang"
      ],
      "abstract": "Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing\nimmunotherapy is essential for personalized treatment planning, enabling\ninformed patient decisions, and improving both treatment outcomes and quality\nof life. However, the lack of large, relevant datasets and effective\nmulti-modal feature fusion strategies pose significant challenges in this\ndomain. To address these challenges, we present a large-scale dataset and\nintroduce a novel framework for multi-modal feature fusion aimed at enhancing\nthe accuracy of survival prediction. The dataset comprises 3D CT images and\ncorresponding clinical records from NSCLC patients treated with immune\ncheckpoint inhibitors (ICI), along with progression-free survival (PFS) and\noverall survival (OS) data. We further propose a cross-modality masked learning\napproach for medical feature fusion, consisting of two distinct branches, each\ntailored to its respective modality: a Slice-Depth Transformer for extracting\n3D features from CT images and a graph-based Transformer for learning node\nfeatures and relationships among clinical variables in tabular data. The fusion\nprocess is guided by a masked modality learning strategy, wherein the model\nutilizes the intact modality to reconstruct missing components. This mechanism\nimproves the integration of modality-specific features, fostering more\neffective inter-modality relationships and feature interactions. Our approach\ndemonstrates superior performance in multi-modal integration for NSCLC survival\nprediction, surpassing existing methods and setting a new benchmark for\nprognostic models in this context.",
      "pdf_url": "http://arxiv.org/pdf/2507.06994v1",
      "published": "2025-07-09T16:19:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06994v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation",
      "authors": [
        "Jieren Deng",
        "Aleksandar Cvetkovic",
        "Pak Kiu Chung",
        "Dragomir Yankov",
        "Chiqun Zhang"
      ],
      "abstract": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.",
      "pdf_url": "http://arxiv.org/pdf/2507.06993v1",
      "published": "2025-07-09T16:18:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06993v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation",
      "authors": [
        "Qilong Xing",
        "Zikai Song",
        "Youjia Zhang",
        "Na Feng",
        "Junqing Yu",
        "Wei Yang"
      ],
      "abstract": "Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation.",
      "pdf_url": "http://arxiv.org/pdf/2507.06992v1",
      "published": "2025-07-09T16:15:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06992v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy",
      "authors": [
        "Bogdan Kulynych",
        "Juan Felipe Gomez",
        "Georgios Kaissis",
        "Jamie Hayes",
        "Borja Balle",
        "Flavio du Pin Calmon",
        "Jean Louis Raisaro"
      ],
      "abstract": "Differentially private (DP) mechanisms are difficult to interpret and\ncalibrate because existing methods for mapping standard privacy parameters to\nconcrete privacy risks -- re-identification, attribute inference, and data\nreconstruction -- are both overly pessimistic and inconsistent. In this work,\nwe use the hypothesis-testing interpretation of DP ($f$-DP), and determine that\nbounds on attack success can take the same unified form across\nre-identification, attribute inference, and data reconstruction risks. Our\nunified bounds are (1) consistent across a multitude of attack settings, and\n(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary\n(including worst-case) levels of baseline risk. Empirically, our results are\ntighter than prior methods using $\\varepsilon$-DP, R\\'enyi DP, and concentrated\nDP. As a result, calibrating noise using our bounds can reduce the required\nnoise by 20% at the same risk level, which yields, e.g., more than 15pp\naccuracy increase in a text classification task. Overall, this unifying\nperspective provides a principled framework for interpreting and calibrating\nthe degree of protection in DP against specific levels of re-identification,\nattribute inference, or data reconstruction risk.",
      "pdf_url": "http://arxiv.org/pdf/2507.06969v1",
      "published": "2025-07-09T15:59:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06969v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CY",
        "stat.ML"
      ]
    },
    {
      "title": "Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report",
      "authors": [
        "Li Du",
        "Hanyu Zhao",
        "Yiming Ju",
        "Tengfei Pan"
      ],
      "abstract": "Instruction tuning has become a foundation for unlocking the capabilities of\nlarge-scale pretrained models and improving their performance on complex tasks.\nThus, the construction of high-quality instruction datasets is crucial for\nenhancing model performance and generalizability. Although current instruction\ndatasets have reached tens of millions of samples, models finetuned on them may\nstill struggle with complex instruction following and tasks in rare domains.\nThis is primarily due to limited expansion in both ``coverage'' (coverage of\ntask types and knowledge areas) and ``depth'' (instruction complexity) of the\ninstruction set. To address this issue, we propose a systematic instruction\ndata construction framework, which integrates a hierarchical labeling system,\nan informative seed selection algorithm, an evolutionary data synthesis\nprocess, and a model deficiency diagnosis with targeted data generation. These\ncomponents form an iterative closed-loop to continuously enhance the coverage\nand depth of instruction data. Based on this framework, we construct\nInfinityInstruct-Subject, a high-quality dataset containing ~1.5 million\ninstructions. Experiments on multiple foundation models and benchmark tasks\ndemonstrate its effectiveness in improving instruction-following capabilities.\nFurther analyses suggest that InfinityInstruct-Subject shows enlarged coverage\nand depth compared to comparable synthesized instruction datasets. Our work\nlays a theoretical and practical foundation for the efficient, continuous\nevolution of instruction datasets, moving from data quantity expansion to\nqualitative improvement.",
      "pdf_url": "http://arxiv.org/pdf/2507.06968v1",
      "published": "2025-07-09T15:59:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06968v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Noisy PDE Training Requires Bigger PINNs",
      "authors": [
        "Sebastien Andre-Sloan",
        "Anirbit Mukherjee",
        "Matthew Colbrook"
      ],
      "abstract": "Physics-Informed Neural Networks (PINNs) are increasingly used to approximate\nsolutions of partial differential equations (PDEs), especially in high\ndimensions. In real-world applications, data samples are noisy, so it is\nimportant to know when a predictor can still achieve low empirical risk.\nHowever, little is known about the conditions under which a PINN can do so\neffectively. We prove a lower bound on the size of neural networks required for\nthe supervised PINN empirical risk to fall below the variance of noisy\nsupervision labels. Specifically, if a predictor achieves an empirical risk\n$O(\\eta)$ below $\\sigma^2$ (variance of supervision data), then necessarily\n$d_N\\log d_N\\gtrsim N_s \\eta^2$, where $N_s$ is the number of samples and $d_N$\nis the number of trainable parameters of the PINN. A similar constraint applies\nto the fully unsupervised PINN setting when boundary labels are sampled\nnoisily. Consequently, increasing the number of noisy supervision labels alone\ndoes not provide a ``free lunch'' in reducing empirical risk. We also show\nempirically that PINNs can indeed achieve empirical risks below $\\sigma^2$\nunder such conditions. As a case study, we investigate PINNs applied to the\nHamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for\nquantitatively understanding the parameter requirements for training PINNs in\nthe presence of noise.",
      "pdf_url": "http://arxiv.org/pdf/2507.06967v1",
      "published": "2025-07-09T15:58:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06967v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale",
      "authors": [
        "Xiao Liang",
        "Jiawei Hu",
        "Di Wang",
        "Zhi Ma",
        "Lin Zhao",
        "Ronghan Li",
        "Bo Wan",
        "Quan Wang"
      ],
      "abstract": "Vision-language models (VLMs) are prone to hallucinations that critically\ncompromise reliability in medical applications. While preference optimization\ncan mitigate these hallucinations through clinical feedback, its implementation\nfaces challenges such as clinically irrelevant training samples, imbalanced\ndata distributions, and prohibitive expert annotation costs. To address these\nchallenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy\nthat combines confidence-similarity joint mining with counterfactual rationale.\nOur approach begins by synthesizing a unified, fine-grained multi-task chest\nX-ray visual instruction dataset across different question types for supervised\nfine-tuning (SFT). We then identify hard examples through token-level\nconfidence analysis of SFT failures and use similarity-based retrieval to\nexpand hard examples for balancing preference sample distributions, while\nsynthetic counterfactual rationales provide fine-grained clinical preferences,\neliminating the need for additional expert input. Experiments show that CheXPO\nachieves 8.93% relative performance gain using only 5% of SFT samples, reaching\nstate-of-the-art performance across diverse clinical tasks and providing a\nscalable, interpretable solution for real-world radiology applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.06959v1",
      "published": "2025-07-09T15:40:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06959v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models",
      "authors": [
        "Keyon Vafa",
        "Peter G. Chang",
        "Ashesh Rambachan",
        "Sendhil Mullainathan"
      ],
      "abstract": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.",
      "pdf_url": "http://arxiv.org/pdf/2507.06952v2",
      "published": "2025-07-09T15:36:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06952v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G",
      "authors": [
        "Michele Polese",
        "Niloofar Mohamadi",
        "Salvatore D'Oro",
        "Tommaso Melodia"
      ],
      "abstract": "The proliferation of data-intensive Artificial Intelligence (AI) applications\nat the network edge demands a fundamental shift in RAN design, from merely\nconsuming AI for network optimization, to actively enabling distributed AI\nworkloads. This paradigm shift presents a significant opportunity for network\noperators to monetize AI at the edge while leveraging existing infrastructure\ninvestments. To realize this vision, this article presents a novel converged\nO-RAN and AI-RAN architecture that unifies orchestration and management of both\ntelecommunications and AI workloads on shared infrastructure. The proposed\narchitecture extends the Open RAN principles of modularity, disaggregation, and\ncloud-nativeness to support heterogeneous AI deployments. We introduce two key\narchitectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN\nService Management and Orchestration (SMO) to enable integrated resource and\nallocation across RAN and AI workloads; and (ii) AI-RAN sites that provide\ndistributed edge AI platforms with real-time processing capabilities. The\nproposed system supports flexible deployment options, allowing AI workloads to\nbe orchestrated with specific timing requirements (real-time or batch\nprocessing) and geographic targeting. The proposed architecture addresses the\norchestration requirements for managing heterogeneous workloads at different\ntime scales while maintaining open, standardized interfaces and multi-vendor\ninteroperability.",
      "pdf_url": "http://arxiv.org/pdf/2507.06911v1",
      "published": "2025-07-09T14:49:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06911v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "eess.SP"
      ]
    },
    {
      "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction",
      "authors": [
        "Xiao Wang",
        "Jiahuan Pei",
        "Diancheng Shui",
        "Zhiguang Han",
        "Xin Sun",
        "Dawei Zhu",
        "Xiaoyu Shen"
      ],
      "abstract": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.",
      "pdf_url": "http://arxiv.org/pdf/2507.06909v1",
      "published": "2025-07-09T14:47:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06909v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection",
      "authors": [
        "Ziyan Liu",
        "Chunxiao Fan",
        "Haoran Lou",
        "Yuexin Wu",
        "Kaiwei Deng"
      ],
      "abstract": "The rapid expansion of memes on social media has highlighted the urgent need\nfor effective approaches to detect harmful content. However, traditional\ndata-driven approaches struggle to detect new memes due to their evolving\nnature and the lack of up-to-date annotated data. To address this issue, we\npropose MIND, a multi-agent framework for zero-shot harmful meme detection that\ndoes not rely on annotated data. MIND implements three key strategies: 1) We\nretrieve similar memes from an unannotated reference set to provide contextual\ninformation. 2) We propose a bi-directional insight derivation mechanism to\nextract a comprehensive understanding of similar memes. 3) We then employ a\nmulti-agent debate mechanism to ensure robust decision-making through reasoned\narbitration. Extensive experiments on three meme datasets demonstrate that our\nproposed framework not only outperforms existing zero-shot approaches but also\nshows strong generalization across different model architectures and parameter\nscales, providing a scalable solution for harmful meme detection. The code is\navailable at https://github.com/destroy-lonely/MIND.",
      "pdf_url": "http://arxiv.org/pdf/2507.06908v1",
      "published": "2025-07-09T14:46:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06908v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation",
      "authors": [
        "Ziang Ye",
        "Yang Zhang",
        "Wentao Shi",
        "Xiaoyu You",
        "Fuli Feng",
        "Tat-Seng Chua"
      ],
      "abstract": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents.",
      "pdf_url": "http://arxiv.org/pdf/2507.06899v1",
      "published": "2025-07-09T14:36:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06899v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN",
      "authors": [
        "Luca Mariotti",
        "Veronica Guidetti",
        "Federica Mandreoli"
      ],
      "abstract": "The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.06895v1",
      "published": "2025-07-09T14:33:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06895v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights",
      "authors": [
        "Alexandra Abbas",
        "Celia Waggoner",
        "Justin Olive"
      ],
      "abstract": "AI evaluations have become critical tools for assessing large language model\ncapabilities and safety. This paper presents practical insights from eight\nmonths of maintaining $inspect\\_evals$, an open-source repository of 70+\ncommunity-contributed AI evaluations. We identify key challenges in\nimplementing and maintaining AI evaluations and develop solutions including:\n(1) a structured cohort management framework for scaling community\ncontributions, (2) statistical methodologies for optimal resampling and\ncross-model comparison with uncertainty quantification, and (3) systematic\nquality control processes for reproducibility. Our analysis reveals that AI\nevaluation requires specialized infrastructure, statistical rigor, and\ncommunity coordination beyond traditional software development practices.",
      "pdf_url": "http://arxiv.org/pdf/2507.06893v1",
      "published": "2025-07-09T14:30:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06893v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model",
      "authors": [
        "Jing Liang",
        "Hongyao Tang",
        "Yi Ma",
        "Jinyi Liu",
        "Yan Zheng",
        "Shuyue Hu",
        "Lei Bai",
        "Jianye Hao"
      ],
      "abstract": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.",
      "pdf_url": "http://arxiv.org/pdf/2507.06892v2",
      "published": "2025-07-09T14:29:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06892v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis in Smart Microgrids Using Dual Fractional-Order Feature Analysis",
      "authors": [
        "Yifan Wang"
      ],
      "abstract": "Cyber-attacks jeopardize the safe operation of smart microgrids. At the same\ntime, existing diagnostic methods either depend on expensive multi-point\ninstrumentation or stringent modelling assumptions that are untenable under\nsingle-sensor constraints. This paper proposes a Fractional-Order\nMemory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency\nfault localisation and cyber-attack detection using only one VPQ\n(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual\nfractional-order feature library by jointly applying Caputo and\nGr\\\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and\nslow drifts in the VPQ signal. A two-stage hierarchical classifier then\npinpoints the affected inverter and isolates the faulty IGBT switch,\neffectively alleviating class imbalance. Robustness is further strengthened\nthrough Progressive Memory-Replay Adversarial Training (PMR-AT), whose\nattack-aware loss is dynamically re-weighted via Online Hard Example Mining\n(OHEM) to prioritise the most challenging samples. Experiments on a\nfour-inverter microgrid testbed comprising 1 normal and 24 fault classes under\nfour attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0\n% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining\n96.7 % under attack-free conditions. These results establish FO-MADS as a\ncost-effective and readily deployable solution that markedly enhances the\ncyber-physical resilience of smart microgrids.",
      "pdf_url": "http://arxiv.org/pdf/2507.06890v1",
      "published": "2025-07-09T14:27:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06890v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ]
    },
    {
      "title": "Winning and losing with Artificial Intelligence: What public discourse about ChatGPT tells us about how societies make sense of technological change",
      "authors": [
        "Adrian Rauchfleisch",
        "Joshua Philip Suarez",
        "Nikka Marie Sales",
        "Andreas Jungherr"
      ],
      "abstract": "Public product launches in Artificial Intelligence can serve as focusing\nevents for collective attention, surfacing how societies react to technological\nchange. Social media provide a window into the sensemaking around these events,\nsurfacing hopes and fears and showing who chooses to engage in the discourse\nand when. We demonstrate that public sensemaking about AI is shaped by economic\ninterests and cultural values of those involved. We analyze 3.8 million tweets\nposted by 1.6 million users across 117 countries in response to the public\nlaunch of ChatGPT in 2022. Our analysis shows how economic self-interest,\nproxied by occupational skill types in writing, programming, and mathematics,\nand national cultural orientations, as measured by Hofstede's individualism,\nuncertainty avoidance, and power distance dimensions, shape who speaks, when\nthey speak, and their stance towards ChatGPT. Roles requiring more technical\nskills, such as programming and mathematics, tend to engage earlier and express\nmore positive stances, whereas writing-centric occupations join later with\ngreater skepticism. At the cultural level, individualism predicts both earlier\nengagement and a more negative stance, and uncertainty avoidance reduces the\nprevalence of positive stances but does not delay when users first engage with\nChatGPT. Aggregate sentiment trends mask the dynamics observed in our study.\nThe shift toward a more critical stance towards ChatGPT over time stems\nprimarily from the entry of more skeptical voices rather than a change of heart\namong early adopters. Our findings underscore the importance of both the\noccupational background and cultural context in understanding public reactions\nto AI.",
      "pdf_url": "http://arxiv.org/pdf/2507.06876v1",
      "published": "2025-07-09T14:15:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06876v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "I.2; J.4; K.4.0"
      ]
    },
    {
      "title": "IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization",
      "authors": [
        "Subrat Kishore Dutta",
        "Xiao Zhang"
      ],
      "abstract": "Despite modifying only a small localized input region, adversarial patches\ncan drastically change the prediction of computer vision models. However, prior\nmethods either cannot perform satisfactorily under targeted attack scenarios or\nfail to produce contextually coherent adversarial patches, causing them to be\neasily noticeable by human examiners and insufficiently stealthy against\nautomatic patch defenses. In this paper, we introduce IAP, a novel attack\nframework that generates highly invisible adversarial patches based on\nperceptibility-aware localization and perturbation optimization schemes.\nSpecifically, IAP first searches for a proper location to place the patch by\nleveraging classwise localization and sensitivity maps, balancing the\nsusceptibility of patch location to both victim model prediction and human\nvisual system, then employs a perceptibility-regularized adversarial loss and a\ngradient update rule that prioritizes color constancy for optimizing invisible\nperturbations. Comprehensive experiments across various image benchmarks and\nmodel architectures demonstrate that IAP consistently achieves competitive\nattack success rates in targeted settings with significantly improved patch\ninvisibility compared to existing baselines. In addition to being highly\nimperceptible to humans, IAP is shown to be stealthy enough to render several\nstate-of-the-art patch defenses ineffective.",
      "pdf_url": "http://arxiv.org/pdf/2507.06856v1",
      "published": "2025-07-09T13:58:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06856v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models",
      "authors": [
        "Liang Wang",
        "Yu Rong",
        "Tingyang Xu",
        "Zhenyi Zhong",
        "Zhiyuan Liu",
        "Pengju Wang",
        "Deli Zhao",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
      ],
      "abstract": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.",
      "pdf_url": "http://arxiv.org/pdf/2507.06853v1",
      "published": "2025-07-09T13:57:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06853v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "physics.chem-ph",
        "q-bio.MN"
      ]
    },
    {
      "title": "SCC-recursiveness in infinite argumentation (extended version)",
      "authors": [
        "Uri Andrews",
        "Luca San Mauro"
      ],
      "abstract": "Argumentation frameworks (AFs) are a foundational tool in artificial\nintelligence for modeling structured reasoning and conflict. SCC-recursiveness\nis a well-known design principle in which the evaluation of arguments is\ndecomposed according to the strongly connected components (SCCs) of the attack\ngraph, proceeding recursively from \"higher\" to \"lower\" components. While\nSCC-recursive semantics such as \\cft and \\stgt have proven effective for finite\nAFs, Baumann and Spanring showed the failure of SCC-recursive semantics to\ngeneralize reliably to infinite AFs due to issues with well-foundedness.\n  We propose two approaches to extending SCC-recursiveness to the infinite\nsetting. We systematically evaluate these semantics using Baroni and Giacomin's\nestablished criteria, showing in particular that directionality fails in\ngeneral. We then examine these semantics' behavior in finitary frameworks,\nwhere we find some of our semantics satisfy directionality. These results\nadvance the theory of infinite argumentation and lay the groundwork for\nreasoning systems capable of handling unbounded or evolving domains.",
      "pdf_url": "http://arxiv.org/pdf/2507.06852v1",
      "published": "2025-07-09T13:57:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06852v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover",
      "authors": [
        "Matteo Lupinacci",
        "Francesco Aurelio Pironti",
        "Francesco Blefari",
        "Francesco Romeo",
        "Luigi Arena",
        "Angelo Furfaro"
      ],
      "abstract": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors.",
      "pdf_url": "http://arxiv.org/pdf/2507.06850v2",
      "published": "2025-07-09T13:54:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06850v2",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion",
      "authors": [
        "Yizhuo Wu",
        "Ang Li",
        "Chang Gao"
      ],
      "abstract": "Neural network (NN)-based Digital Predistortion (DPD) stands out in improving\nsignal quality in wideband radio frequency (RF) power amplifiers (PAs)\nemploying complex modulation. However, NN DPDs usually rely on a large number\nof parameters for effective linearization and can significantly contribute to\nthe energy consumption of the digital back-end in RF systems. This paper\npresents OpenDPDv2, a unified framework for PA modeling, DPD learning, and\nmodel optimization to reduce power consumption while maintaining high\nlinearization performance. The optimization techniques feature a novel DPD\nalgorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The\ntop-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an\nAdjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude\n(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal\nsparsity of input signals and hidden neurons, the inference energy of our model\ncan be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM\nwith 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth\n256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,\ndatasets, and documentation are publicly accessible at:\nhttps://github.com/lab-emi/OpenDPD.",
      "pdf_url": "http://arxiv.org/pdf/2507.06849v1",
      "published": "2025-07-09T13:54:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06849v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation",
      "authors": [
        "Tao Feng",
        "Xianbing Zhao",
        "Zhenhua Chen",
        "Tien Tsin Wong",
        "Hamid Rezatofighi",
        "Gholamreza Haffari",
        "Lizhen Qu"
      ],
      "abstract": "Recent advances in diffusion-based and autoregressive video generation models\nhave achieved remarkable visual realism. However, these models typically lack\naccurate physical alignment, failing to replicate real-world dynamics in object\nmotion. This limitation arises primarily from their reliance on learned\nstatistical correlations rather than capturing mechanisms adhering to physical\nlaws. To address this issue, we introduce a novel framework that integrates\nsymbolic regression (SR) and trajectory-guided image-to-video (I2V) models for\nphysics-grounded video forecasting. Our approach extracts motion trajectories\nfrom input videos, uses a retrieval-based pre-training mechanism to enhance\nsymbolic regression, and discovers equations of motion to forecast physically\naccurate future trajectories. These trajectories then guide video generation\nwithout requiring fine-tuning of existing models. Evaluated on scenarios in\nClassical Mechanics, including spring-mass, pendulums, and projectile motions,\nour method successfully recovers ground-truth analytical equations and improves\nthe physical alignment of generated videos over baseline methods.",
      "pdf_url": "http://arxiv.org/pdf/2507.06830v1",
      "published": "2025-07-09T13:28:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06830v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean Data",
      "authors": [
        "Xuesong Li",
        "Nassir Navab",
        "Zhongliang Jiang"
      ],
      "abstract": "Image denoising is a fundamental task in computer vision, particularly in\nmedical ultrasound (US) imaging, where speckle noise significantly degrades\nimage quality. Although recent advancements in deep neural networks have led to\nsubstantial improvements in denoising for natural images, these methods cannot\nbe directly applied to US speckle noise, as it is not purely random. Instead,\nUS speckle arises from complex wave interference within the body\nmicrostructure, making it tissue-dependent. This dependency means that\nobtaining two independent noisy observations of the same scene, as required by\npioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also\ncannot handle US speckle noise due to its high spatial dependency. To address\nthis challenge, we introduce Speckle2Self, a novel self-supervised algorithm\nfor speckle reduction using only single noisy observations. The key insight is\nthat applying a multi-scale perturbation (MSP) operation introduces\ntissue-dependent variations in the speckle pattern across different scales,\nwhile preserving the shared anatomical structure. This enables effective\nspeckle suppression by modeling the clean image as a low-rank signal and\nisolating the sparse noise component. To demonstrate its effectiveness,\nSpeckle2Self is comprehensively compared with conventional filter-based\ndenoising algorithms and SOTA learning-based methods, using both realistic\nsimulated US images and human carotid US images. Additionally, data from\nmultiple US machines are employed to evaluate model generalization and\nadaptability to images from unseen domains. \\textit{Code and datasets will be\nreleased upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2507.06828v1",
      "published": "2025-07-09T13:28:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06828v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning",
      "authors": [
        "Matej Straka",
        "Martin Schmid"
      ],
      "abstract": "We introduce a real-time strategy game environment based on Generals.io, a\ngame with thousands of weekly active players. Our environment is fully\ncompatible with Gymnasium and PettingZoo and is capable of running thousands of\nframes per second on commodity hardware. We also present a reference agent,\ntrained with supervised pre-training and self-play, which reached the top\n0.003% of the 1v1 human leaderboard after only 36 hours on a single H100 GPU.\nTo accelerate learning, we incorporate potential-based reward shaping and\nmemory features. Our contributions of a modular RTS benchmark and a competitive\nbaseline agent provide an accessible yet challenging platform for advancing\nmulti-agent reinforcement learning research. The documented code, together with\nexamples and tutorials, is available at\nhttps://github.com/strakam/generals-bots.",
      "pdf_url": "http://arxiv.org/pdf/2507.06825v2",
      "published": "2025-07-09T13:15:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06825v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning",
      "authors": [
        "Chuhang Zheng",
        "Chunwei Tian",
        "Jie Wen",
        "Daoqiang Zhang",
        "Qi Zhu"
      ],
      "abstract": "Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning.",
      "pdf_url": "http://arxiv.org/pdf/2507.06821v2",
      "published": "2025-07-09T13:08:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06821v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Comprehensive Evaluation of Prototype Neural Networks",
      "authors": [
        "Philipp Schlinge",
        "Steffen Meinert",
        "Martin Atzmueller"
      ],
      "abstract": "Prototype models are an important method for explainable artificial\nintelligence (XAI) and interpretable machine learning. In this paper, we\nperform an in-depth analysis of a set of prominent prototype models including\nProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive\nset of metrics. In addition to applying standard metrics from literature, we\npropose several new metrics to further complement the analysis of model\ninterpretability. In our experimentation, we apply the set of prototype models\non a diverse set of datasets including fine-grained classification, Non-IID\nsettings and multi-label classification to further contrast the performance.\nFurthermore, we also provide our code as an open-source library, which\nfacilitates simple application of the metrics itself, as well as extensibility\n- providing the option for easily adding new metrics and models.\nhttps://github.com/uos-sis/quanproto",
      "pdf_url": "http://arxiv.org/pdf/2507.06819v1",
      "published": "2025-07-09T13:08:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06819v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Intrinsic Training Signals for Federated Learning Aggregation",
      "authors": [
        "Cosimo Fiorini",
        "Matteo Mosconi",
        "Pietro Buzzega",
        "Riccardo Salami",
        "Simone Calderara"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. While existing approaches\nfor aggregating client-specific classification heads and adapted backbone\nparameters require architectural modifications or loss function changes, our\nmethod uniquely leverages intrinsic training signals already available during\nstandard optimization. We present LIVAR (Layer Importance and VARiance-based\nmerging), which introduces: i) a variance-weighted classifier aggregation\nscheme using naturally emergent feature statistics, and ii) an\nexplainability-driven LoRA merging technique based on SHAP analysis of existing\nupdate parameter patterns. Without any architectural overhead, LIVAR achieves\nstate-of-the-art performance on multiple benchmarks while maintaining seamless\nintegration with existing FL methods. This work demonstrates that effective\nmodel merging can be achieved solely through existing training signals,\nestablishing a new paradigm for efficient federated model aggregation. The code\nwill be made publicly available upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2507.06813v1",
      "published": "2025-07-09T13:03:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06813v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Democratizing High-Fidelity Co-Speech Gesture Video Generation",
      "authors": [
        "Xu Yang",
        "Shaoli Huang",
        "Shenbo Xie",
        "Xuelin Chen",
        "Yifei Liu",
        "Changxing Ding"
      ],
      "abstract": "Co-speech gesture video generation aims to synthesize realistic,\naudio-aligned videos of speakers, complete with synchronized facial expressions\nand body gestures. This task presents challenges due to the significant\none-to-many mapping between audio and visual content, further complicated by\nthe scarcity of large-scale public datasets and high computational demands. We\npropose a lightweight framework that utilizes 2D full-body skeletons as an\nefficient auxiliary condition to bridge audio signals with visual outputs. Our\napproach introduces a diffusion model conditioned on fine-grained audio\nsegments and a skeleton extracted from the speaker's reference image,\npredicting skeletal motions through skeleton-audio feature fusion to ensure\nstrict audio coordination and body shape consistency. The generated skeletons\nare then fed into an off-the-shelf human video generation model with the\nspeaker's reference image to synthesize high-fidelity videos. To democratize\nresearch, we present CSG-405-the first public dataset with 405 hours of\nhigh-resolution videos across 71 speech types, annotated with 2D skeletons and\ndiverse speaker demographics. Experiments show that our method exceeds\nstate-of-the-art approaches in visual quality and synchronization while\ngeneralizing across speakers and contexts.",
      "pdf_url": "http://arxiv.org/pdf/2507.06812v1",
      "published": "2025-07-09T13:02:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06812v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams",
      "authors": [
        "Matthew Anderson Hendricks",
        "Alice Cicirello"
      ],
      "abstract": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.",
      "pdf_url": "http://arxiv.org/pdf/2507.06803v1",
      "published": "2025-07-09T12:44:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06803v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ]
    },
    {
      "title": "Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)",
      "authors": [
        "Uri Andrews",
        "Luca San Mauro"
      ],
      "abstract": "Dialectical systems are a mathematical formalism for modeling an agent\nupdating a knowledge base seeking consistency. Introduced in the 1970s by\nRoberto Magari, they were originally conceived to capture how a working\nmathematician or a research community refines beliefs in the pursuit of truth.\nDialectical systems also serve as natural models for the belief change of an\nautomated agent, offering a unifying, computable framework for dynamic belief\nmanagement.\n  The literature distinguishes three main models of dialectical systems:\n(d-)dialectical systems based on revising beliefs when they are seen to be\ninconsistent, p-dialectical systems based on revising beliefs based on finding\na counterexample, and q-dialectical systems which can do both. We answer an\nopen problem in the literature by proving that q-dialectical systems are\nstrictly more powerful than p-dialectical systems, which are themselves known\nto be strictly stronger than (d-)dialectical systems. This result highlights\nthe complementary roles of counterexample and contradiction in automated belief\nrevision, and thus also in the reasoning processes of mathematicians and\nresearch communities.",
      "pdf_url": "http://arxiv.org/pdf/2507.06798v1",
      "published": "2025-07-09T12:35:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06798v1",
      "categories": [
        "cs.AI",
        "math.LO"
      ]
    },
    {
      "title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining",
      "authors": [
        "Seonwu Kim",
        "Yohan Na",
        "Kihun Kim",
        "Hanhee Cho",
        "Geun Lim",
        "Mintae Kim",
        "Seongik Park",
        "Ki Hyun Kim",
        "Youngsub Han",
        "Byoung-Ki Jeon"
      ],
      "abstract": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.",
      "pdf_url": "http://arxiv.org/pdf/2507.06795v2",
      "published": "2025-07-09T12:30:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06795v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Temporal Information Retrieval via Time-Specifier Model Merging",
      "authors": [
        "SeungYoon Han",
        "Taeho Hwang",
        "Sukmin Cho",
        "Soyeong Jeong",
        "Hoyun Song",
        "Huije Lee",
        "Jong C. Park"
      ],
      "abstract": "The rapid expansion of digital information and knowledge across structured\nand unstructured sources has heightened the importance of Information Retrieval\n(IR). While dense retrieval methods have substantially improved semantic\nmatching for general queries, they consistently underperform on queries with\nexplicit temporal constraints--often those containing numerical expressions and\ntime specifiers such as ``in 2015.'' Existing approaches to Temporal\nInformation Retrieval (TIR) improve temporal reasoning but often suffer from\ncatastrophic forgetting, leading to reduced performance on non-temporal\nqueries. To address this, we propose Time-Specifier Model Merging (TSM), a\nnovel method that enhances temporal retrieval while preserving accuracy on\nnon-temporal queries. TSM trains specialized retrievers for individual time\nspecifiers and merges them in to a unified model, enabling precise handling of\ntemporal constraints without compromising non-temporal retrieval. Extensive\nexperiments on both temporal and non-temporal datasets demonstrate that TSM\nsignificantly improves performance on temporally constrained queries while\nmaintaining strong results on non-temporal queries, consistently outperforming\nother baseline methods. Our code is available at\nhttps://github.com/seungyoonee/TSM .",
      "pdf_url": "http://arxiv.org/pdf/2507.06782v1",
      "published": "2025-07-09T12:16:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06782v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views",
      "authors": [
        "Saif Ur Rehman Khan",
        "Muhammad Nabeel Asim",
        "Sebastian Vollmer",
        "Andreas Dengel"
      ],
      "abstract": "The framework is designed to improve performance in the analysis of combined\nas well as single anatomical perspectives for MRI disease diagnosis. It\nspecifically addresses the performance degradation observed in state-of-the-art\n(SOTA) models, particularly when processing axial, coronal, and sagittal\nanatomical planes. The paper introduces the FOLC-Net framework, which\nincorporates a novel federated-optimized lightweight architecture with\napproximately 1.217 million parameters and a storage requirement of only 0.9\nMB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for\nefficient model structure generation, global model cloning for scalable\ntraining, and ConvNeXt for enhanced client adaptability. The model was\nevaluated on combined multi-view data as well as individual views, such as\naxial, coronal, and sagittal, to assess its robustness in various medical\nimaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different\ndata to evaluate its ability to generalize beyond the training dataset. The\nresults show that FOLC-Net outperforms existing models, particularly in the\nchallenging sagittal view. For instance, FOLC-Net achieved an accuracy of\n92.44% on the sagittal view, significantly higher than the 88.37% accuracy of\nstudy method (DL + Residual Learning) and 88.95% of DL models. Additionally,\nFOLC-Net demonstrated improved accuracy across all individual views, providing\na more reliable and robust solution for medical image analysis in decentralized\nenvironments. FOLC-Net addresses the limitations of existing SOTA models by\nproviding a framework that ensures better adaptability to individual views\nwhile maintaining strong performance in multi-view settings. The incorporation\nof MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs\nbetter in real-world medical applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.06763v1",
      "published": "2025-07-09T11:40:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06763v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution",
      "authors": [
        "Ye Kyaw Thu",
        "Thura Aung",
        "Thazin Myint Oo",
        "Thepchai Supnithi"
      ],
      "abstract": "This paper presents the first application of Kolmogorov-Arnold Convolution\nfor Text (KAConvText) in sentence classification, addressing three tasks:\nimbalanced binary hate speech detection, balanced multiclass news\nclassification, and imbalanced multiclass ethnic language identification. We\ninvestigate various embedding configurations, comparing random to fastText\nembeddings in both static and fine-tuned settings, with embedding dimensions of\n100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs\nand CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we\ninvestigated KAConvText with different classification heads - MLP and KAN,\nwhere using KAN head supports enhanced interpretability. Results show that\nKAConvText-MLP with fine-tuned fastText embeddings achieves the best\nperformance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,\n92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%\naccuracy (F1-score = 0.9982) for language identification.",
      "pdf_url": "http://arxiv.org/pdf/2507.06753v1",
      "published": "2025-07-09T11:25:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06753v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; I.2.6"
      ]
    },
    {
      "title": "DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement",
      "authors": [
        "Xinyu Xie",
        "Weifeng Cao",
        "Jun Shi",
        "Yangyang Hu",
        "Hui Liang",
        "Wanyong Liang",
        "Xiaoliang Qian"
      ],
      "abstract": "Spatio-temporal video prediction plays a pivotal role in critical domains,\nranging from weather forecasting to industrial automation. However, in\nhigh-precision industrial scenarios such as semiconductor manufacturing, the\nabsence of specialized benchmark datasets severely hampers research on modeling\nand predicting complex processes. To address this challenge, we make a twofold\ncontribution.First, we construct and release the Chip Dicing Lane Dataset\n(CHDL), the first public temporal image dataset dedicated to the semiconductor\nwafer dicing process. Captured via an industrial-grade vision system, CHDL\nprovides a much-needed and challenging benchmark for high-fidelity process\nmodeling, defect detection, and digital twin development.Second, we propose\nDIFFUMA, an innovative dual-path prediction architecture specifically designed\nfor such fine-grained dynamics. The model captures global long-range temporal\ncontext through a parallel Mamba module, while simultaneously leveraging a\ndiffusion module, guided by temporal features, to restore and enhance\nfine-grained spatial details, effectively combating feature degradation.\nExperiments demonstrate that on our CHDL benchmark, DIFFUMA significantly\noutperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and\nimproving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.\nThis superior performance also generalizes to natural phenomena datasets. Our\nwork not only delivers a new state-of-the-art (SOTA) model but, more\nimportantly, provides the community with an invaluable data resource to drive\nfuture research in industrial AI.",
      "pdf_url": "http://arxiv.org/pdf/2507.06738v1",
      "published": "2025-07-09T10:51:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06738v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool",
      "authors": [
        "Milena Pustet",
        "Elisabeth Steffen",
        "Helena Mihaljević",
        "Grischa Stanjek",
        "Yannis Illies"
      ],
      "abstract": "The role of civil society organizations (CSOs) in monitoring harmful online\ncontent is increasingly crucial, especially as platform providers reduce their\ninvestment in content moderation. AI tools can assist in detecting and\nmonitoring harmful content at scale. However, few open-source tools offer\nseamless integration of AI models and social media monitoring infrastructures.\nGiven their thematic expertise and contextual understanding of harmful content,\nCSOs should be active partners in co-developing technological tools, providing\nfeedback, helping to improve models, and ensuring alignment with stakeholder\nneeds and values, rather than as passive 'consumers'. However, collaborations\nbetween the open source community, academia, and civil society remain rare, and\nresearch on harmful content seldom translates into practical tools usable by\ncivil society actors. This work in progress explores how CSOs can be\nmeaningfully involved in an AI-assisted open-source monitoring tool of\nanti-democratic movements on Telegram, which we are currently developing in\ncollaboration with CSO stakeholders.",
      "pdf_url": "http://arxiv.org/pdf/2507.06734v1",
      "published": "2025-07-09T10:46:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06734v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.",
      "pdf_url": "http://arxiv.org/pdf/2507.06715v1",
      "published": "2025-07-09T10:13:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06715v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Photometric Stereo using Gaussian Splatting and inverse rendering",
      "authors": [
        "Matéo Ducastel",
        "David Tschumperlé",
        "Yvain Quéau"
      ],
      "abstract": "Recent state-of-the-art algorithms in photometric stereo rely on neural\nnetworks and operate either through prior learning or inverse rendering\noptimization. Here, we revisit the problem of calibrated photometric stereo by\nleveraging recent advances in 3D inverse rendering using the Gaussian Splatting\nformalism. This allows us to parameterize the 3D scene to be reconstructed and\noptimize it in a more interpretable manner. Our approach incorporates a\nsimplified model for light representation and demonstrates the potential of the\nGaussian Splatting rendering engine for the photometric stereo problem.",
      "pdf_url": "http://arxiv.org/pdf/2507.06684v1",
      "published": "2025-07-09T09:22:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06684v1",
      "categories": [
        "eess.IV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring State-Space-Model based Language Model in Music Generation",
      "authors": [
        "Wei-Jaw Lee",
        "Fang-Chih Hsieh",
        "Xuanjun Chen",
        "Fang-Duo Tsai",
        "Yi-Hsuan Yang"
      ],
      "abstract": "The recent surge in State Space Models (SSMs), particularly the emergence of\nMamba, has established them as strong alternatives or complementary modules to\nTransformers across diverse domains. In this work, we aim to explore the\npotential of Mamba-based architectures for text-to-music generation. We adopt\ndiscrete tokens of Residual Vector Quantization (RVQ) as the modeling\nrepresentation and empirically find that a single-layer codebook can capture\nsemantic information in music. Motivated by this observation, we focus on\nmodeling a single-codebook representation and adapt SiMBA, originally designed\nas a Mamba-based encoder, to function as a decoder for sequence modeling. We\ncompare its performance against a standard Transformer-based decoder. Our\nresults suggest that, under limited-resource settings, SiMBA achieves much\nfaster convergence and generates outputs closer to the ground truth. This\ndemonstrates the promise of SSMs for efficient and expressive text-to-music\ngeneration. We put audio examples on Github.",
      "pdf_url": "http://arxiv.org/pdf/2507.06674v1",
      "published": "2025-07-09T09:05:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06674v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models",
      "authors": [
        "Gennadii Iakovlev"
      ],
      "abstract": "This project introduces a new measure of elite polarization via actor and\nsubject detection using artificial intelligence. I identify when politicians\nmention one another in parliamentary speeches, note who is speaking and who is\nbeing addressed, and assess the emotional temperature behind these evaluations.\nThis maps how elites evaluate their various out-parties, allowing us to create\nan index of mutual out-party hostility, that is, elite polarization. While I\nanalyzed polarization data over the past four decades for the UK, and two\ndecades for Hungary and Italy, my approach lays the groundwork for a\ntwenty-year, EU-wide time-series dataset on elite polarization. I obtain the\nresults that can be aggregated by party and quarter. The resulting index\ndemonstrates a good face validity: it reacts to events such as electoral\ncampaigns, country- and party-level crises, and to parties losing and assuming\npower.",
      "pdf_url": "http://arxiv.org/pdf/2507.06658v1",
      "published": "2025-07-09T08:44:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06658v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval",
      "authors": [
        "Naoya Sogi",
        "Takashi Shibata",
        "Makoto Terao",
        "Masanori Suganuma",
        "Takayuki Okatani"
      ],
      "abstract": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval\nfor enhancing the efficiency of a practical application. Conventional methods\nfocus solely on increasing the diversity metric of image appearances. However,\nthe diversity metric and its desired value vary depending on the application,\nwhich limits the applications of RD. This paper proposes a novel task called\nCDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims\nto refine the diversities of multiple attributes, according to the\napplication's context. To address this task, we propose Multi-Source DPPs, a\nsimple yet strong baseline that extends the Determinantal Point Process (DPP)\nto multi-sources. We model MS-DPP as a single DPP model with a unified\nsimilarity matrix based on a manifold representation. We also introduce Tangent\nNormalization to reflect contexts. Extensive experiments demonstrate the\neffectiveness of the proposed method. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/msdpp.",
      "pdf_url": "http://arxiv.org/pdf/2507.06654v1",
      "published": "2025-07-09T08:38:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06654v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Deep Disentangled Representation Network for Treatment Effect Estimation",
      "authors": [
        "Hui Meng",
        "Keping Yang",
        "Xuyu Peng",
        "Bo Zheng"
      ],
      "abstract": "Estimating individual-level treatment effect from observational data is a\nfundamental problem in causal inference and has attracted increasing attention\nin the fields of education, healthcare, and public policy.In this work, we\nconcentrate on the study of disentangled representation methods that have shown\npromising outcomes by decomposing observed covariates into instrumental,\nconfounding, and adjustment factors. However, most of the previous work has\nprimarily revolved around generative models or hard decomposition methods for\ncovariates, which often struggle to guarantee the attainment of precisely\ndisentangled factors. In order to effectively model different causal\nrelationships, we propose a novel treatment effect estimation algorithm that\nincorporates a mixture of experts with multi-head attention and a linear\northogonal regularizer to softly decompose the pre-treatment variables, and\nsimultaneously eliminates selection bias via importance sampling re-weighting\ntechniques. We conduct extensive experiments on both public semi-synthetic and\nreal-world production datasets. The experimental results clearly demonstrate\nthat our algorithm outperforms the state-of-the-art methods focused on\nindividual treatment effects.",
      "pdf_url": "http://arxiv.org/pdf/2507.06650v1",
      "published": "2025-07-09T08:29:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.06650v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}