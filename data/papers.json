{
  "last_updated": "2026-02-04T01:05:48.435875",
  "papers": [
    {
      "title": "Reward-free Alignment for Conflicting Objectives",
      "authors": [
        "Peter Chen",
        "Xiaopeng Li",
        "Xi Chen",
        "Tianyi Lin"
      ],
      "abstract": "Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.",
      "pdf_url": "https://arxiv.org/pdf/2602.02495v1",
      "published": "2026-02-02T18:59:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02495v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "authors": [
        "Zehong Ma",
        "Ruihan Xu",
        "Shiliang Zhang"
      ],
      "abstract": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
      "pdf_url": "https://arxiv.org/pdf/2602.02493v1",
      "published": "2026-02-02T18:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02493v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "authors": [
        "Jialiang Zhu",
        "Gongrui Zhang",
        "Xiaolong Ma",
        "Lin Xu",
        "Miaosen Zhang",
        "Ruiqi Yang",
        "Song Wang",
        "Kai Qiu",
        "Zhirong Wu",
        "Qi Dai",
        "Ruichun Ma",
        "Bei Liu",
        "Yifan Yang",
        "Chong Luo",
        "Zhengyuan Yang",
        "Linjie Li",
        "Lijuan Wang",
        "Weizhu Chen",
        "Xin Geng",
        "Baining Guo"
      ],
      "abstract": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
      "pdf_url": "https://arxiv.org/pdf/2602.02486v1",
      "published": "2026-02-02T18:58:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02486v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Flow Policy Gradients for Robot Control",
      "authors": [
        "Brent Yi",
        "Hongsuk Choi",
        "Himanshu Gaurav Singh",
        "Xiaoyu Huang",
        "Takara E. Truong",
        "Carmelo Sferrazza",
        "Yi Ma",
        "Rocky Duan",
        "Pieter Abbeel",
        "Guanya Shi",
        "Karen Liu",
        "Angjoo Kanazawa"
      ],
      "abstract": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.",
      "pdf_url": "https://arxiv.org/pdf/2602.02481v1",
      "published": "2026-02-02T18:56:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02481v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories",
      "authors": [
        "Shraddha Barke",
        "Arnav Goyal",
        "Alind Khare",
        "Avaljot Singh",
        "Suman Nath",
        "Chetan Bansal"
      ],
      "abstract": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.",
      "pdf_url": "https://arxiv.org/pdf/2602.02475v1",
      "published": "2026-02-02T18:54:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02475v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
      "authors": [
        "Haozhen Zhang",
        "Quanyu Long",
        "Jianzhu Bao",
        "Tao Feng",
        "Weizhi Zhang",
        "Haodong Yue",
        "Wenya Wang"
      ],
      "abstract": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.",
      "pdf_url": "https://arxiv.org/pdf/2602.02474v1",
      "published": "2026-02-02T18:53:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02474v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multi-head automated segmentation by incorporating detection head into the contextual layer neural network",
      "authors": [
        "Edwin Kys",
        "Febian Febian"
      ],
      "abstract": "Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \\pm 0.036$ versus $0.732 \\pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.",
      "pdf_url": "https://arxiv.org/pdf/2602.02471v1",
      "published": "2026-02-02T18:51:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02471v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.med-ph"
      ]
    },
    {
      "title": "Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge",
      "authors": [
        "Xutao Ma",
        "Yixiao Huang",
        "Hanlin Zhu",
        "Somayeh Sojoudi"
      ],
      "abstract": "Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the \"reversal curse\" -- when trained on forward knowledge data of the form \"$A \\rightarrow B$\" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge \"$B \\leftarrow A$\" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form \"$A \\to A$\" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.",
      "pdf_url": "https://arxiv.org/pdf/2602.02470v1",
      "published": "2026-02-02T18:50:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02470v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts",
      "authors": [
        "Aiden Yiliu Li",
        "Xinyue Hao",
        "Shilong Liu",
        "Mengdi Wang"
      ],
      "abstract": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.",
      "pdf_url": "https://arxiv.org/pdf/2602.02468v1",
      "published": "2026-02-02T18:50:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02468v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
      "authors": [
        "Jana Zeller",
        "Thadd√§us Wiedemer",
        "Fanfei Li",
        "Thomas Klein",
        "Prasanna Mayilvahanan",
        "Matthias Bethge",
        "Felix Wichmann",
        "Ryan Cotterell",
        "Wieland Brendel"
      ],
      "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
      "pdf_url": "https://arxiv.org/pdf/2602.02465v1",
      "published": "2026-02-02T18:49:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02465v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models",
      "authors": [
        "Gabriele Maraia",
        "Marco Valentino",
        "Fabio Massimo Zanzotto",
        "Leonardo Ranaldi"
      ],
      "abstract": "Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.",
      "pdf_url": "https://arxiv.org/pdf/2602.02462v1",
      "published": "2026-02-02T18:48:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02462v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction",
      "authors": [
        "Han Bao",
        "Zheyuan Zhang",
        "Pengcheng Jing",
        "Zhengqing Yuan",
        "Kaiwen Shi",
        "Yanfang Ye"
      ],
      "abstract": "As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \\textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \\textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \\textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \\MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.",
      "pdf_url": "https://arxiv.org/pdf/2602.02455v1",
      "published": "2026-02-02T18:46:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02455v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ]
    },
    {
      "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
      "authors": [
        "Ansh Kumar Sharma",
        "Yixiang Sun",
        "Ninghao Lu",
        "Yunzhe Zhang",
        "Jiarao Liu",
        "Sherry Yang"
      ],
      "abstract": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.",
      "pdf_url": "https://arxiv.org/pdf/2602.02454v1",
      "published": "2026-02-02T18:44:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02454v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
      "authors": [
        "Andong Chen",
        "Wenxin Zhu",
        "Qiuyu Ding",
        "Yuchen Song",
        "Muyun Yang",
        "Tiejun Zhao"
      ],
      "abstract": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2602.02453v1",
      "published": "2026-02-02T18:43:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02453v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization",
      "authors": [
        "Patrick Cooper",
        "Alvaro Velasquez"
      ],
      "abstract": "Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.",
      "pdf_url": "https://arxiv.org/pdf/2602.02451v1",
      "published": "2026-02-02T18:43:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02451v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
      "authors": [
        "Dianyi Wang",
        "Chaofan Ma",
        "Feng Han",
        "Size Wu",
        "Wei Song",
        "Yibin Wang",
        "Zhixiong Zhang",
        "Tianhang Wang",
        "Siyuan Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "abstract": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.",
      "pdf_url": "https://arxiv.org/pdf/2602.02437v1",
      "published": "2026-02-02T18:34:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02437v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Poly-attention: a general scheme for higher-order self-attention",
      "authors": [
        "Sayak Chakrabarti",
        "Toniann Pitassi",
        "Josh Alman"
      ],
      "abstract": "The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.\n  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.\n  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.",
      "pdf_url": "https://arxiv.org/pdf/2602.02422v1",
      "published": "2026-02-02T18:24:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02422v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
      "authors": [
        "Qingni Wang",
        "Yue Fan",
        "Xin Eric Wang"
      ],
      "abstract": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.",
      "pdf_url": "https://arxiv.org/pdf/2602.02419v1",
      "published": "2026-02-02T18:22:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02419v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Structure Enables Effective Self-Localization of Errors in LLMs",
      "authors": [
        "Ankur Samanta",
        "Akshayaa Magesh",
        "Ayush Jain",
        "Kavosh Asadi",
        "Youliang Yu",
        "Daniel Jiang",
        "Boris Vidolov",
        "Kaveh Hassani",
        "Paul Sajda",
        "Jalaj Bhandari",
        "Yonathan Efroni"
      ],
      "abstract": "Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.",
      "pdf_url": "https://arxiv.org/pdf/2602.02416v1",
      "published": "2026-02-02T18:15:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02416v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ReasonEdit: Editing Vision-Language Models using Human Reasoning",
      "authors": [
        "Jiaxing Qiu",
        "Kaihua Hou",
        "Roxana Daneshjou",
        "Ahmed Alaa",
        "Thomas Hartvigsen"
      ],
      "abstract": "Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.",
      "pdf_url": "https://arxiv.org/pdf/2602.02408v1",
      "published": "2026-02-02T18:06:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02408v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
      "authors": [
        "Ethan Mendes",
        "Jungsoo Park",
        "Alan Ritter"
      ],
      "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.",
      "pdf_url": "https://arxiv.org/pdf/2602.02405v1",
      "published": "2026-02-02T18:03:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02405v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
      "authors": [
        "Mu Huang",
        "Hui Wang",
        "Kerui Ren",
        "Linning Xu",
        "Yunsong Zhou",
        "Mulin Yu",
        "Bo Dai",
        "Jiangmiao Pang"
      ],
      "abstract": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.",
      "pdf_url": "https://arxiv.org/pdf/2602.02402v1",
      "published": "2026-02-02T17:59:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02402v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "physics.app-ph"
      ]
    },
    {
      "title": "David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning",
      "authors": [
        "Samuel Nellessen",
        "Tal Kachman"
      ],
      "abstract": "The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary \"tags along\" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.",
      "pdf_url": "https://arxiv.org/pdf/2602.02395v1",
      "published": "2026-02-02T17:56:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02395v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.MA"
      ]
    },
    {
      "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory",
      "authors": [
        "Ruiqi Wu",
        "Xuanhua He",
        "Meng Cheng",
        "Tianyu Yang",
        "Yong Zhang",
        "Zhuoliang Kang",
        "Xunliang Cai",
        "Xiaoming Wei",
        "Chunle Guo",
        "Chongyi Li",
        "Ming-Ming Cheng"
      ],
      "abstract": "We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.",
      "pdf_url": "https://arxiv.org/pdf/2602.02393v1",
      "published": "2026-02-02T17:52:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02393v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing",
      "authors": [
        "Mika Okamoto",
        "Ansel Kaplan Erol",
        "Glenn Matlin"
      ],
      "abstract": "How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2602.02386v1",
      "published": "2026-02-02T17:49:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02386v1",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making",
      "authors": [
        "Raunak Jain",
        "Mudita Khurana",
        "John Stephens",
        "Srinivas Dharmasanam",
        "Shankar Venkataraman"
      ],
      "abstract": "As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.",
      "pdf_url": "https://arxiv.org/pdf/2602.02378v1",
      "published": "2026-02-02T17:42:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02378v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback",
      "authors": [
        "Yaolun Zhang",
        "Yiran Wu",
        "Yijiong Yu",
        "Qingyun Wu",
        "Huazheng Wang"
      ],
      "abstract": "Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \\emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \\textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \\textsc{Live-Evo} decouples \\emph{what happened} from \\emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \\textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \\textit{Prophet Arena} benchmark over a 10-week horizon, \\textsc{Live-Evo} improves Brier score by 20.8\\% and increases market returns by 12.9\\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.",
      "pdf_url": "https://arxiv.org/pdf/2602.02369v1",
      "published": "2026-02-02T17:34:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02369v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "ReasonCACHE: Teaching LLMs To Reason Without Weight Updates",
      "authors": [
        "Sharut Gupta",
        "Phillip Isola",
        "Stefanie Jegelka",
        "David Lopez-Paz",
        "Kartik Ahuja",
        "Mark Ibrahim",
        "Mohammad Pezeshki"
      ],
      "abstract": "Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2602.02366v1",
      "published": "2026-02-02T17:24:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02366v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
      "authors": [
        "Mouxiang Chen",
        "Lei Zhang",
        "Yunlong Feng",
        "Xuwu Wang",
        "Wenting Zhao",
        "Ruisheng Cao",
        "Jiaxi Yang",
        "Jiawei Chen",
        "Mingze Li",
        "Zeyao Ma",
        "Hao Ge",
        "Zongmeng Zhang",
        "Zeyu Cui",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Jianling Sun",
        "Junyang Lin",
        "Binyuan Hui"
      ],
      "abstract": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
      "pdf_url": "https://arxiv.org/pdf/2602.02361v1",
      "published": "2026-02-02T17:20:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02361v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Implicit neural representation of textures",
      "authors": [
        "Albert Kwok",
        "Zheyuan Hu",
        "Dounia Hammou"
      ],
      "abstract": "Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.",
      "pdf_url": "https://arxiv.org/pdf/2602.02354v1",
      "published": "2026-02-02T17:17:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02354v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ]
    },
    {
      "title": "Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data",
      "authors": [
        "Veronica Sanz"
      ],
      "abstract": "Symmetries play a central role in physics, organizing dynamics, constraining interactions, and determining the effective number of physical degrees of freedom. In parallel, modern artificial intelligence methods have demonstrated a remarkable ability to extract low-dimensional structure from high-dimensional data through representation learning. This review examines the interplay between these two perspectives, focusing on the extent to which symmetry-induced constraints can be identified, encoded, or diagnosed using machine learning techniques.\n  Rather than emphasizing architectures that enforce known symmetries by construction, we concentrate on data-driven approaches and latent representation learning, with particular attention to variational autoencoders. We discuss how symmetries and conservation laws reduce the intrinsic dimensionality of physical datasets, and how this reduction may manifest itself through self-organization of latent spaces in generative models trained to balance reconstruction and compression. We review recent results, including case studies from simple geometric systems and particle physics processes, and analyze the theoretical and practical limitations of inferring symmetry structure without explicit inductive bias.",
      "pdf_url": "https://arxiv.org/pdf/2602.02351v1",
      "published": "2026-02-02T17:15:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02351v1",
      "categories": [
        "hep-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "authors": [
        "Xingyuan Hua",
        "Sheng Yue",
        "Xinyi Li",
        "Yizhe Zhao",
        "Jinrui Zhang",
        "Ju Ren"
      ],
      "abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.",
      "pdf_url": "https://arxiv.org/pdf/2602.02350v1",
      "published": "2026-02-02T17:15:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02350v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
      "authors": [
        "Ziwen Xu",
        "Chenyan Wu",
        "Hengyu Sun",
        "Haiwen Hong",
        "Mengru Wang",
        "Yunzhi Yao",
        "Longtao Huang",
        "Hui Xue",
        "Shumin Deng",
        "Zhixuan Chu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.",
      "pdf_url": "https://arxiv.org/pdf/2602.02343v1",
      "published": "2026-02-02T17:04:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02343v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs",
      "authors": [
        "Yu Liang",
        "Zhongjin Zhang",
        "Yuxuan Zhu",
        "Kerui Zhang",
        "Zhiluohan Guo",
        "Wenhang Zhou",
        "Zonqi Yang",
        "Kangle Wu",
        "Yabo Ni",
        "Anxiang Zeng",
        "Cong Fu",
        "Jianxin Wang",
        "Jiazhi Xia"
      ],
      "abstract": "Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.",
      "pdf_url": "https://arxiv.org/pdf/2602.02338v1",
      "published": "2026-02-02T17:00:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02338v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents",
      "authors": [
        "Weiming Sheng",
        "Jinlang Wang",
        "Manuel Barros",
        "Aldrin Montana",
        "Jacopo Tagliabue",
        "Luca Bigon"
      ],
      "abstract": "Lakehouses are the default cloud platform for analytics and AI, but they become unsafe when untrusted actors concurrently operate on production data: upstream-downstream mismatches surface only at runtime, and multi-table pipelines can leak partial effects. Inspired by software engineering, we design Bauplan, a code-first lakehouse that aims to make (most) illegal states unrepresentable using familiar abstractions. Bauplan acts along three axes: typed table contracts to make pipeline boundaries checkable, Git-like data versioning for review and reproducibility, and transactional runs that guarantee pipeline-level atomicity. We report early results from a lightweight formal transaction model and discuss future work motivated by counterexamples.",
      "pdf_url": "https://arxiv.org/pdf/2602.02335v1",
      "published": "2026-02-02T16:58:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02335v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.DB"
      ]
    },
    {
      "title": "VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations",
      "authors": [
        "Fatemeh Zargarbashi",
        "Dhruv Agrawal",
        "Jakob Buhmann",
        "Martin Guay",
        "Stelian Coros",
        "Robert W. Sumner"
      ],
      "abstract": "Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.",
      "pdf_url": "https://arxiv.org/pdf/2602.02334v1",
      "published": "2026-02-02T16:58:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02334v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "authors": [
        "Shaoting Zhu",
        "Baijun Ye",
        "Jiaxuan Wang",
        "Jiakang Chen",
        "Ziwen Zhuang",
        "Linzhan Mou",
        "Runhan Huang",
        "Hang Zhao"
      ],
      "abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.",
      "pdf_url": "https://arxiv.org/pdf/2602.02331v1",
      "published": "2026-02-02T16:55:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02331v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method",
      "authors": [
        "Feiyang Cai",
        "Guijuan He",
        "Yi Hu",
        "Jingjing Wang",
        "Joshua Luo",
        "Tianyu Zhu",
        "Srikanth Pilla",
        "Gang Li",
        "Ling Liu",
        "Feng Luo"
      ],
      "abstract": "Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.",
      "pdf_url": "https://arxiv.org/pdf/2602.02320v1",
      "published": "2026-02-02T16:49:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02320v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.BM"
      ]
    },
    {
      "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient",
      "authors": [
        "Changming Li",
        "Kaixing Zhang",
        "Haoyun Xu",
        "Yingdong Shi",
        "Zheng Zhang",
        "Kaitao Song",
        "Kan Ren"
      ],
      "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.",
      "pdf_url": "https://arxiv.org/pdf/2602.02313v1",
      "published": "2026-02-02T16:43:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02313v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "FragmentFlow: Scalable Transition State Generation for Large Molecules",
      "authors": [
        "Ron Shprints",
        "Peter Holderrieth",
        "Juno Nam",
        "Rafael G√≥mez-Bombarelli",
        "Tommi Jaakkola"
      ],
      "abstract": "Transition states (TSs) are central to understanding and quantitatively predicting chemical reactivity and reaction mechanisms. Although traditional TS generation methods are computationally expensive, recent generative modeling approaches have enabled chemically meaningful TS prediction for relatively small molecules. However, these methods fail to generalize to practically relevant reaction substrates because of distribution shifts induced by increasing molecular sizes. Furthermore, TS geometries for larger molecules are not available at scale, making it infeasible to train generative models from scratch on such molecules. To address these challenges, we introduce FragmentFlow: a divide-and-conquer approach that trains a generative model to predict TS geometries for the reactive core atoms, which define the reaction mechanism. The full TS structure is then reconstructed by re-attaching substituent fragments to the predicted core. By operating on reactive cores, whose size and composition remain relatively invariant across molecular contexts, FragmentFlow mitigates distribution shifts in generative modeling. Evaluated on a new curated dataset of reactions involving reactants with up to 33 heavy atoms, FragmentFlow correctly identifies 90% of TSs while requiring 30% fewer saddle-point optimization steps than classical initialization schemes. These results point toward scalable TS generation for high-throughput reactivity studies.",
      "pdf_url": "https://arxiv.org/pdf/2602.02310v1",
      "published": "2026-02-02T16:42:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02310v1",
      "categories": [
        "physics.chem-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Spark: Modular Spiking Neural Networks",
      "authors": [
        "Mario Franco",
        "Carlos Gershenson"
      ],
      "abstract": "Nowadays, neural networks act as a synonym for artificial intelligence. Present neural network models, although remarkably powerful, are inefficient both in terms of data and energy. Several alternative forms of neural networks have been proposed to address some of these problems. Specifically, spiking neural networks are suitable for efficient hardware implementations. However, effective learning algorithms for spiking networks remain elusive, although it is suspected that effective plasticity mechanisms could alleviate the problem of data efficiency. Here, we present a new framework for spiking neural networks - Spark - built upon the idea of modular design, from simple components to entire models. The aim of this framework is to provide an efficient and streamlined pipeline for spiking neural networks. We showcase this framework by solving the sparse-reward cartpole problem with simple plasticity mechanisms. We hope that a framework compatible with traditional ML pipelines may accelerate research in the area, specifically for continuous and unbatched learning, akin to the one animals exhibit.",
      "pdf_url": "https://arxiv.org/pdf/2602.02306v1",
      "published": "2026-02-02T16:36:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02306v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach",
      "authors": [
        "Martino Ciaperoni",
        "Marzio Di Vece",
        "Luca Pappalardo",
        "Fosca Giannotti",
        "Francesco Giannini"
      ],
      "abstract": "Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Œî$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Œî$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Œî$-XAI experiment.",
      "pdf_url": "https://arxiv.org/pdf/2602.02304v1",
      "published": "2026-02-02T16:36:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02304v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Advancing General-Purpose Reasoning Models with Modular Gradient Surgery",
      "authors": [
        "Min Cai",
        "Yu Liang",
        "Longzheng Wang",
        "Yan Wang",
        "Yueyang Zhang",
        "Long Xia",
        "Zhiyuan Sun",
        "Xi Ye",
        "Daiting Shi"
      ],
      "abstract": "Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\\%) and 4.5 (11.1\\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.",
      "pdf_url": "https://arxiv.org/pdf/2602.02301v1",
      "published": "2026-02-02T16:34:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02301v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Decoupling Generalizability and Membership Privacy Risks in Neural Networks",
      "authors": [
        "Xingli Fang",
        "Jung-Eun Kim"
      ],
      "abstract": "A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.",
      "pdf_url": "https://arxiv.org/pdf/2602.02296v1",
      "published": "2026-02-02T16:32:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02296v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?",
      "authors": [
        "Alex Argese",
        "Pasquale Lisena",
        "Rapha√´l Troncy"
      ],
      "abstract": "Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.",
      "pdf_url": "https://arxiv.org/pdf/2602.02290v1",
      "published": "2026-02-02T16:29:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02290v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "An Optimization Method for Autoregressive Time Series Forecasting",
      "authors": [
        "Zheng Li",
        "Jerry Cheng",
        "Huanying Gu"
      ],
      "abstract": "Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt",
      "pdf_url": "https://arxiv.org/pdf/2602.02288v1",
      "published": "2026-02-02T16:28:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02288v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild",
      "authors": [
        "Arnab Das",
        "Yassine El Kheir",
        "Enes Erdem Erdogan",
        "Feidi Kallel",
        "Tim Polzehl",
        "Sebastian Moeller"
      ],
      "abstract": "This paper presents the DFKI-Speech system developed for the WildSpoof Challenge under the Spoofing aware Automatic Speaker Verification (SASV) track. We propose a robust SASV framework in which a spoofing detector and a speaker verification (SV) network operate in tandem. The spoofing detector employs a self-supervised speech embedding extractor as the frontend, combined with a state-of-the-art graph neural network backend. In addition, a top-3 layer based mixture-of-experts (MoE) is used to fuse high-level and low-level features for effective spoofed utterance detection. For speaker verification, we adapt a low-complexity convolutional neural network that fuses 2D and 1D features at multiple scales, trained with the SphereFace loss. Additionally, contrastive circle loss is applied to adaptively weight positive and negative pairs within each training batch, enabling the network to better distinguish between hard and easy sample pairs. Finally, fixed imposter cohort based AS Norm score normalization and model ensembling are used to further enhance the discriminative capability of the speaker verification system.",
      "pdf_url": "https://arxiv.org/pdf/2602.02286v1",
      "published": "2026-02-02T16:27:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02286v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Backpropagation as Physical Relaxation: Exact Gradients in Finite Time",
      "authors": [
        "Antonino Emanuele Scurria"
      ],
      "abstract": "Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.",
      "pdf_url": "https://arxiv.org/pdf/2602.02281v1",
      "published": "2026-02-02T16:21:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02281v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "physics.class-ph",
        "physics.comp-ph"
      ]
    },
    {
      "title": "RACA: Representation-Aware Coverage Criteria for LLM Safety Testing",
      "authors": [
        "Zeming Wei",
        "Zhixin Zhang",
        "Chengcan Wu",
        "Yihao Zhang",
        "Xiaokun Luan",
        "Meng Sun"
      ],
      "abstract": "Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.",
      "pdf_url": "https://arxiv.org/pdf/2602.02280v1",
      "published": "2026-02-02T16:20:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02280v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems",
      "authors": [
        "Jon ≈†kerlj",
        "Seongjin Bien",
        "Abdeldjallil Naceri",
        "Sami Haddadin"
      ],
      "abstract": "We present $multipanda\\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.",
      "pdf_url": "https://arxiv.org/pdf/2602.02269v1",
      "published": "2026-02-02T16:11:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.02269v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SE",
        "eess.SY"
      ]
    }
  ]
}