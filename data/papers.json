{
  "last_updated": "2025-08-08T00:58:20.013880",
  "papers": [
    {
      "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience",
      "authors": [
        "Zeyi Sun",
        "Ziyu Liu",
        "Yuhang Zang",
        "Yuhang Cao",
        "Xiaoyi Dong",
        "Tong Wu",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
      "pdf_url": "http://arxiv.org/pdf/2508.04700v1",
      "published": "2025-08-06T17:58:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04700v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.MA",
        "cs.MM"
      ]
    },
    {
      "title": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis",
      "authors": [
        "Anushka Yadav",
        "Isha Nalawade",
        "Srujana Pillarichety",
        "Yashwanth Babu",
        "Reshmi Ghosh",
        "Samyadeep Basu",
        "Wenlong Zhao",
        "Ali Nasaeh",
        "Sriram Balasubramanian",
        "Soundararajan Srinivasan"
      ],
      "abstract": "The emergence of reasoning models and their integration into practical AI\nchat bots has led to breakthroughs in solving advanced math, deep search, and\nextractive question answering problems that requires a complex and multi-step\nthought process. Yet, a complete understanding of why these models hallucinate\nmore than general purpose language models is missing. In this investigative\nstudy, we systematicallyexplore reasoning failures of contemporary language\nmodels on multi-hop question answering tasks. We introduce a novel, nuanced\nerror categorization framework that examines failures across three critical\ndimensions: the diversity and uniqueness of source documents involved (\"hops\"),\ncompleteness in capturing relevant information (\"coverage\"), and cognitive\ninefficiency (\"overthinking\"). Through rigorous hu-man annotation, supported by\ncomplementary automated metrics, our exploration uncovers intricate error\npatterns often hidden by accuracy-centric evaluations. This investigative\napproach provides deeper insights into the cognitive limitations of current\nmodels and offers actionable guidance toward enhancing reasoning fidelity,\ntransparency, and robustness in future language modeling efforts.",
      "pdf_url": "http://arxiv.org/pdf/2508.04699v1",
      "published": "2025-08-06T17:58:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04699v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario",
      "authors": [
        "Yuanchen Bai",
        "Zijian Ding",
        "Shaoyue Wen",
        "Xiang Chang",
        "Angelique Taylor"
      ],
      "abstract": "Multi-agent robotic systems (MARS) build upon multi-agent systems by\nintegrating physical and task-related constraints, increasing the complexity of\naction execution and agent coordination. However, despite the availability of\nadvanced multi-agent frameworks, their real-world deployment on robots remains\nlimited, hindering the advancement of MARS research in practice. To bridge this\ngap, we conducted two studies to investigate performance trade-offs of\nhierarchical multi-agent frameworks in a simulated real-world multi-robot\nhealthcare scenario. In Study 1, using CrewAI, we iteratively refine the\nsystem's knowledge base, to systematically identify and categorize coordination\nfailures (e.g., tool access violations, lack of timely handling of failure\nreports) not resolvable by providing contextual knowledge alone. In Study 2,\nusing AutoGen, we evaluate a redesigned bidirectional communication structure\nand further measure the trade-offs between reasoning and non-reasoning models\noperating within the same robotic team setting. Drawing from our empirical\nfindings, we emphasize the tension between autonomy and stability and the\nimportance of edge-case testing to improve system reliability and safety for\nfuture real-world deployment. Supplementary materials, including codes, task\nagent setup, trace outputs, and annotated examples of coordination failures and\nreasoning behaviors, are available at:\nhttps://byc-sophie.github.io/mas-to-mars/.",
      "pdf_url": "http://arxiv.org/pdf/2508.04691v1",
      "published": "2025-08-06T17:54:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04691v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering",
      "authors": [
        "Karthik Menon",
        "Batool Arhamna Haider",
        "Muhammad Arham",
        "Kanwal Mehreen",
        "Ram Mohan Rao Kadiyala",
        "Hamza Farooq"
      ],
      "abstract": "This study introduces Query Attribute Modeling (QAM), a hybrid framework that\nenhances search precision and relevance by decomposing open text queries into\nstructured metadata tags and semantic elements. QAM addresses traditional\nsearch limitations by automatically extracting metadata filters from free-form\ntext queries, reducing noise and enabling focused retrieval of relevant items.\n  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique\nitems with 40,000+ reviews and detailed product attributes) demonstrated QAM's\nsuperior performance, achieving a mean average precision at 5 (mAP@5) of\n52.99\\%. This represents significant improvement over conventional methods,\nincluding BM25 keyword search, encoder-based semantic similarity search,\ncross-encoder re-ranking, and hybrid search combining BM25 and semantic results\nvia Reciprocal Rank Fusion (RRF). The results establish QAM as a robust\nsolution for Enterprise Search applications, particularly in e-commerce\nsystems.",
      "pdf_url": "http://arxiv.org/pdf/2508.04683v1",
      "published": "2025-08-06T17:47:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04683v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay",
      "authors": [
        "Yunan Zhang",
        "Shuoran Jiang",
        "Mengchen Zhao",
        "Yuefeng Li",
        "Yang Fan",
        "Xiangping Wu",
        "Qingcai Chen"
      ],
      "abstract": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.",
      "pdf_url": "http://arxiv.org/pdf/2508.04676v1",
      "published": "2025-08-06T17:42:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04676v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "How are CS students using resources and AI tools for coding tasks?",
      "authors": [
        "Natalia Echeverry",
        "Arun Lekshmi Narayanan"
      ],
      "abstract": "A survey of 26 CS students reveals that AI coding assistants are mainly used\nfor writing code (second to online searches) while AI chatbots are the top\nresource for debugging. Participants with different coding experience prefer\nonline help over direct human help from peers and instructors.",
      "pdf_url": "http://arxiv.org/pdf/2508.04667v1",
      "published": "2025-08-06T17:35:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04667v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management",
      "authors": [
        "Mo Li",
        "L. H. Xu",
        "Qitai Tan",
        "Ting Cao",
        "Yunxin Liu"
      ],
      "abstract": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.",
      "pdf_url": "http://arxiv.org/pdf/2508.04664v1",
      "published": "2025-08-06T17:32:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04664v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models",
      "authors": [
        "Young D. Kwon",
        "Rui Li",
        "Sijia Li",
        "Da Li",
        "Sourav Bhattacharya",
        "Stylianos I. Venieris"
      ],
      "abstract": "State-of-the-art text-to-image diffusion models (DMs) achieve remarkable\nquality, yet their massive parameter scale (8-11B) poses significant challenges\nfor inferences on resource-constrained devices. In this paper, we present\nHierarchicalPrune, a novel compression framework grounded in a key observation:\nDM blocks exhibit distinct functional hierarchies, where early blocks establish\nsemantic structures while later blocks handle texture refinements.\nHierarchicalPrune synergistically combines three techniques: (1) Hierarchical\nPosition Pruning, which identifies and removes less essential later blocks\nbased on position hierarchy; (2) Positional Weight Preservation, which\nsystematically protects early model portions that are essential for semantic\nstructural integrity; and (3) Sensitivity-Guided Distillation, which adjusts\nknowledge-transfer intensity based on our discovery of block-wise sensitivity\nvariations. As a result, our framework brings billion-scale diffusion models\ninto a range more suitable for on-device inference, while preserving the\nquality of the output images. Specifically, when combined with INT4 weight\nquantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction\n(e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on\nserver and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score\nand 7% in HPSv2 score compared to the original model. Last but not least, our\ncomprehensive user study with 85 participants demonstrates that\nHierarchicalPrune maintains perceptual quality comparable to the original model\nwhile significantly outperforming prior works.",
      "pdf_url": "http://arxiv.org/pdf/2508.04663v1",
      "published": "2025-08-06T17:30:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04663v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper",
      "authors": [
        "Akhil Saketh Reddy Sabbella",
        "Ch. Lakshmi Prachothan",
        "Eswar Kumar Panta"
      ],
      "abstract": "In the poultry industry, detecting chicken illnesses is essential to avoid\nfinancial losses. Conventional techniques depend on manual observation, which\nis laborious and prone to mistakes. Using YOLO v8 a deep learning model for\nreal-time object recognition. This study suggests an AI based approach, by\ndeveloping a system that analyzes high resolution chicken photos, YOLO v8\ndetects signs of illness, such as abnormalities in behavior and appearance. A\nsizable, annotated dataset has been used to train the algorithm, which provides\naccurate real-time identification of infected chicken and prompt warnings to\nfarm operators for prompt action. By facilitating early infection\nidentification, eliminating the need for human inspection, and enhancing\nbiosecurity in large-scale farms, this AI technology improves chicken health\nmanagement. The real-time features of YOLO v8 provide a scalable and effective\nmethod for improving farm management techniques.",
      "pdf_url": "http://arxiv.org/pdf/2508.04658v1",
      "published": "2025-08-06T17:27:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04658v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "X-SAM: From Segment Anything to Any Segmentation",
      "authors": [
        "Hao Wang",
        "Limeng Qiao",
        "Zequn Jie",
        "Zhijian Huang",
        "Chengjian Feng",
        "Qingfang Zheng",
        "Lin Ma",
        "Xiangyuan Lan",
        "Xiaodan Liang"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM.",
      "pdf_url": "http://arxiv.org/pdf/2508.04655v1",
      "published": "2025-08-06T17:19:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04655v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
      "authors": [
        "Shuo Liu",
        "Zeyu Liang",
        "Xueguang Lyu",
        "Christopher Amato"
      ],
      "abstract": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges.",
      "pdf_url": "http://arxiv.org/pdf/2508.04652v1",
      "published": "2025-08-06T17:18:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04652v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation",
      "authors": [
        "Yu Song",
        "Zhigang Hua",
        "Harry Shomer",
        "Yan Xie",
        "Jingzhe Liu",
        "Bo Long",
        "Hui Liu"
      ],
      "abstract": "Link Prediction (LP) is a critical task in graph machine learning. While\nGraph Neural Networks (GNNs) have significantly advanced LP performance\nrecently, existing methods face key challenges including limited supervision\nfrom sparse connectivity, sensitivity to initialization, and poor\ngeneralization under distribution shifts. We explore pretraining as a solution\nto address these challenges. Unlike node classification, LP is inherently a\npairwise task, which requires the integration of both node- and edge-level\ninformation. In this work, we present the first systematic study on the\ntransferability of these distinct modules and propose a late fusion strategy to\neffectively combine their outputs for improved performance. To handle the\ndiversity of pretraining data and avoid negative transfer, we introduce a\nMixture-of-Experts (MoE) framework that captures distinct patterns in separate\nexperts, facilitating seamless application of the pretrained model on diverse\ndownstream datasets. For fast adaptation, we develop a parameter-efficient\ntuning strategy that allows the pretrained model to adapt to unseen datasets\nwith minimal computational overhead. Experiments on 16 datasets across two\ndomains demonstrate the effectiveness of our approach, achieving\nstate-of-the-art performance on low-resource link prediction while obtaining\ncompetitive results compared to end-to-end trained methods, with over 10,000x\nlower computational overhead.",
      "pdf_url": "http://arxiv.org/pdf/2508.04645v1",
      "published": "2025-08-06T17:10:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04645v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis",
      "authors": [
        "Feifan Song",
        "Bofei Gao",
        "Yifan Song",
        "Yi Liu",
        "Weimin Xiong",
        "Yuyang Song",
        "Tianyu Liu",
        "Guoyin Wang",
        "Houfeng Wang"
      ],
      "abstract": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead.",
      "pdf_url": "http://arxiv.org/pdf/2508.04626v1",
      "published": "2025-08-06T16:51:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04626v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs",
      "authors": [
        "Dengzhao Fang",
        "Jingtong Gao",
        "Chengcheng Zhu",
        "Yu Li",
        "Xiangyu Zhao",
        "Yi Chang"
      ],
      "abstract": "Recommender systems are indispensable for helping users navigate the immense\nitem catalogs of modern online platforms. Recently, generative recommendation\nhas emerged as a promising paradigm, unifying the conventional\nretrieve-and-rank pipeline into an end-to-end model capable of dynamic\ngeneration. However, existing generative methods are fundamentally constrained\nby their unsupervised tokenization, which generates semantic IDs suffering from\ntwo critical flaws: (1) they are semantically flat and uninterpretable, lacking\na coherent hierarchy, and (2) they are prone to representation entanglement\n(i.e., ``ID collisions''), which harms recommendation accuracy and diversity.\nTo overcome these limitations, we propose HiD-VAE, a novel framework that\nlearns hierarchically disentangled item representations through two core\ninnovations. First, HiD-VAE pioneers a hierarchically-supervised quantization\nprocess that aligns discrete codes with multi-level item tags, yielding more\nuniform and disentangled IDs. Crucially, the trained codebooks can predict\nhierarchical tags, providing a traceable and interpretable semantic path for\neach recommendation. Second, to combat representation entanglement, HiD-VAE\nincorporates a novel uniqueness loss that directly penalizes latent space\noverlap. This mechanism not only resolves the critical ID collision problem but\nalso promotes recommendation diversity by ensuring a more comprehensive\nutilization of the item representation space. These high-quality, disentangled\nIDs provide a powerful foundation for downstream generative models. Extensive\nexperiments on three public benchmarks validate HiD-VAE's superior performance\nagainst state-of-the-art methods. The code is available at\nhttps://anonymous.4open.science/r/HiD-VAE-84B2.",
      "pdf_url": "http://arxiv.org/pdf/2508.04618v1",
      "published": "2025-08-06T16:45:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04618v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning",
      "authors": [
        "Md Zesun Ahmed Mia",
        "Malyaban Bal",
        "Sen Lu",
        "George M. Nishibuchi",
        "Suhas Chelian",
        "Srini Vasan",
        "Abhronil Sengupta"
      ],
      "abstract": "Inspired by the brain's hierarchical processing and energy efficiency, this\npaper presents a Spiking Neural Network (SNN) architecture for lifelong Network\nIntrusion Detection System (NIDS). The proposed system first employs an\nefficient static SNN to identify potential intrusions, which then activates an\nadaptive dynamic SNN responsible for classifying the specific attack type.\nMimicking biological adaptation, the dynamic classifier utilizes Grow When\nRequired (GWR)-inspired structural plasticity and a novel Adaptive\nSpike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible\nmechanisms enable the network to learn new threats incrementally while\npreserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual\nlearning setting, the architecture demonstrates robust adaptation, reduced\ncatastrophic forgetting, and achieves $85.3$\\% overall accuracy. Furthermore,\nsimulations using the Intel Lava framework confirm high operational sparsity,\nhighlighting the potential for low-power deployment on neuromorphic hardware.",
      "pdf_url": "http://arxiv.org/pdf/2508.04610v2",
      "published": "2025-08-06T16:29:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04610v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ]
    },
    {
      "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search",
      "authors": [
        "Zhejun Zhao",
        "Yuehu Dong",
        "Alley Liu",
        "Lixue Zheng",
        "Pingsheng Liu",
        "Dongdong Shen",
        "Long Xia",
        "Jiashu Zhao",
        "Dawei Yin"
      ],
      "abstract": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system.",
      "pdf_url": "http://arxiv.org/pdf/2508.04604v1",
      "published": "2025-08-06T16:24:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04604v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "GraphProp: Training the Graph Foundation Models using Graph Properties",
      "authors": [
        "Ziheng Sun",
        "Qi Feng",
        "Lehao Lin",
        "Chris Ding",
        "Jicong Fan"
      ],
      "abstract": "This work focuses on training graph foundation models (GFMs) that have strong\ngeneralization ability in graph-level tasks such as graph classification.\nEffective GFM training requires capturing information consistent across\ndifferent domains. We discover that graph structures provide more consistent\ncross-domain information compared to node features and graph labels. However,\ntraditional GFMs primarily focus on transferring node features from various\ndomains into a unified representation space but often lack structural\ncross-domain generalization. To address this, we introduce GraphProp, which\nemphasizes structural generalization. The training process of GraphProp\nconsists of two main phases. First, we train a structural GFM by predicting\ngraph invariants. Since graph invariants are properties of graphs that depend\nonly on the abstract structure, not on particular labellings or drawings of the\ngraph, this structural GFM has a strong ability to capture the abstract\nstructural information and provide discriminative graph representations\ncomparable across diverse domains. In the second phase, we use the\nrepresentations given by the structural GFM as positional encodings to train a\ncomprehensive GFM. This phase utilizes domain-specific node attributes and\ngraph labels to further improve cross-domain node feature generalization. Our\nexperiments demonstrate that GraphProp significantly outperforms the\ncompetitors in supervised learning and few-shot learning, especially in\nhandling graphs without node attributes.",
      "pdf_url": "http://arxiv.org/pdf/2508.04594v1",
      "published": "2025-08-06T16:12:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04594v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI",
      "authors": [
        "Nicola Casali",
        "Alessandro Brusaferri",
        "Giuseppe Baselli",
        "Stefano Fumagalli",
        "Edoardo Micotti",
        "Gianluigi Forloni",
        "Riaz Hussein",
        "Giovanna Rizzo",
        "Alfonso Mastropietro"
      ],
      "abstract": "Accurate estimation of intravoxel incoherent motion (IVIM) parameters from\ndiffusion-weighted MRI remains challenging due to the ill-posed nature of the\ninverse problem and high sensitivity to noise, particularly in the perfusion\ncompartment. In this work, we propose a probabilistic deep learning framework\nbased on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling\nestimation of total predictive uncertainty and decomposition into aleatoric\n(AU) and epistemic (EU) components. The method was benchmarked against non\nprobabilistic neural networks, a Bayesian fitting approach and a probabilistic\nnetwork with single Gaussian parametrization. Supervised training was performed\non synthetic data, and evaluation was conducted on both simulated and an in\nvivo dataset. The reliability of the quantified uncertainties was assessed\nusing calibration curves, output distribution sharpness, and the Continuous\nRanked Probability Score (CRPS). MDNs produced more calibrated and sharper\npredictive distributions for the diffusion coefficient D and fraction f\nparameters, although slight overconfidence was observed in pseudo-diffusion\ncoefficient D*. The Robust Coefficient of Variation (RCV) indicated smoother in\nvivo estimates for D* with MDNs compared to Gaussian model. Despite the\ntraining data covering the expected physiological range, elevated EU in vivo\nsuggests a mismatch with real acquisition conditions, highlighting the\nimportance of incorporating EU, which was allowed by DE. Overall, we present a\ncomprehensive framework for IVIM fitting with uncertainty quantification, which\nenables the identification and interpretation of unreliable estimates. The\nproposed approach can also be adopted for fitting other physical models through\nappropriate architectural and simulation adjustments.",
      "pdf_url": "http://arxiv.org/pdf/2508.04588v2",
      "published": "2025-08-06T16:08:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04588v2",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference",
      "authors": [
        "Nuo Chen",
        "Moming Duan",
        "Andre Huikai Lin",
        "Qian Wang",
        "Jiaying Wu",
        "Bingsheng He"
      ],
      "abstract": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research.",
      "pdf_url": "http://arxiv.org/pdf/2508.04586v1",
      "published": "2025-08-06T16:08:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04586v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning",
      "authors": [
        "Magauiya Zhussip",
        "Dmitriy Shopkhoev",
        "Ammar Ali",
        "Stamatios Lefkimmiatis"
      ],
      "abstract": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
      "pdf_url": "http://arxiv.org/pdf/2508.04581v1",
      "published": "2025-08-06T16:06:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04581v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges",
      "authors": [
        "Yue Zhou",
        "Yi Chang",
        "Yuan Wu"
      ],
      "abstract": "Reasoning is a critical capability of multimodal large language models\n(MLLMs) for solving complex multimodal tasks, and judging the correctness of\nreasoning steps is crucial for improving this capability. Recently, MLLM-based\nprocess judges (MPJs) have been widely used to assess the correctness of\nreasoning steps in multimodal tasks. Therefore, evaluating MPJs is important\nfor identifying their limitations and guiding future improvements. However,\nexisting benchmarks for MPJs mainly focus on tasks such as step correctness\nclassification and reasoning process search, while overlooking a key aspect:\nwhether the confidence scores produced by MPJs at the step level are reliable.\nTo address this gap, we propose ConfProBench, the first comprehensive benchmark\ndesigned to systematically evaluate the reliability of step-level confidence\nscores generated by MPJs. Our benchmark constructs three types of adversarially\nperturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and\nImage Perturbation, to test the robustness of MPJ confidence under\nperturbations. In addition, we introduce three novel evaluation metrics:\nConfidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and\nConfidence Calibration Score (CCS), which evaluate robustness, sensitivity, and\ncalibration, respectively. We evaluate 14 state-of-the-art MLLMs, including\nboth proprietary and open-source models. Experiments reveal limitations in\ncurrent MPJs' confidence performance and offer competitive baselines to support\nfuture research.",
      "pdf_url": "http://arxiv.org/pdf/2508.04576v1",
      "published": "2025-08-06T16:00:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04576v1",
      "categories": [
        "cs.AI",
        "I.2.6; I.2.7; D.2.8"
      ]
    },
    {
      "title": "Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration",
      "authors": [
        "Nuo Chen",
        "Yicheng Tong",
        "Jiaying Wu",
        "Minh Duc Duong",
        "Qian Wang",
        "Qingyun Zou",
        "Bryan Hooi",
        "Bingsheng He"
      ],
      "abstract": "While AI agents show potential in scientific ideation, most existing\nframeworks rely on single-agent refinement, limiting creativity due to bounded\nknowledge and perspective. Inspired by real-world research dynamics, this paper\ninvestigates whether structured multi-agent discussions can surpass solitary\nideation. We propose a cooperative multi-agent framework for generating\nresearch proposals and systematically compare configurations including group\nsize, leaderled versus leaderless structures, and team compositions varying in\ninterdisciplinarity and seniority. To assess idea quality, we employ a\ncomprehensive protocol with agent-based scoring and human review across\ndimensions such as novelty, strategic vision, and integration depth. Our\nresults show that multi-agent discussions substantially outperform solitary\nbaselines. A designated leader acts as a catalyst, transforming discussion into\nmore integrated and visionary proposals. Notably, we find that cognitive\ndiversity is a primary driver of quality, yet expertise is a non-negotiable\nprerequisite, as teams lacking a foundation of senior knowledge fail to surpass\neven a single competent agent. These findings offer actionable insights for\ndesigning collaborative AI ideation systems and shed light on how team\nstructure influences creative outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2508.04575v1",
      "published": "2025-08-06T15:59:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04575v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization",
      "authors": [
        "Jinxing Zhou",
        "Ziheng Zhou",
        "Yanghao Zhou",
        "Yuxin Mao",
        "Zhangling Duan",
        "Dan Guo"
      ],
      "abstract": "The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally\nlocalize events in untrimmed videos that occur simultaneously in both the audio\nand visual modalities. This paper explores DAVEL under a new and more\nchallenging weakly-supervised setting (W-DAVEL task), where only video-level\nevent labels are provided and the temporal boundaries of each event are\nunknown. We address W-DAVEL by exploiting \\textit{cross-modal salient anchors},\nwhich are defined as reliable timestamps that are well predicted under weak\nsupervision and exhibit highly consistent event semantics across audio and\nvisual modalities. Specifically, we propose a \\textit{Mutual Event Agreement\nEvaluation} module, which generates an agreement score by measuring the\ndiscrepancy between the predicted audio and visual event classes. Then, the\nagreement score is utilized in a \\textit{Cross-modal Salient Anchor\nIdentification} module, which identifies the audio and visual anchor features\nthrough global-video and local temporal window identification mechanisms. The\nanchor features after multimodal integration are fed into an\n\\textit{Anchor-based Temporal Propagation} module to enhance event semantic\nencoding in the original temporal audio and visual features, facilitating\nbetter temporal localization under weak supervision. We establish benchmarks\nfor W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance.",
      "pdf_url": "http://arxiv.org/pdf/2508.04566v1",
      "published": "2025-08-06T15:49:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04566v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset",
      "authors": [
        "Mei Jiang",
        "Houping Yue",
        "Bingdong Li",
        "Hao Hao",
        "Ying Qian",
        "Bo Jiang",
        "Aimin Zhou"
      ],
      "abstract": "Fostering students' abilities for knowledge integration and transfer in\ncomplex problem-solving scenarios is a core objective of modern education, and\ninterdisciplinary STEM is a key pathway to achieve this, yet it requires expert\nguidance that is difficult to scale. While LLMs offer potential in this regard,\ntheir true capability for guided instruction remains unclear due to the lack of\nan effective evaluation benchmark. To address this, we introduce SID, the first\nbenchmark designed to systematically evaluate the higher-order guidance\ncapabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our\ncontributions include a large-scale dataset of 10,000 dialogue turns across 48\ncomplex STEM projects, a novel annotation schema for capturing deep pedagogical\nfeatures, and a new suite of evaluation metrics (e.g., X-SRG). Baseline\nexperiments confirm that even state-of-the-art LLMs struggle to execute\neffective guided dialogues that lead students to achieve knowledge integration\nand transfer. This highlights the critical value of our benchmark in driving\nthe development of more pedagogically-aware LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2508.04563v1",
      "published": "2025-08-06T15:49:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04563v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning",
      "authors": [
        "Quang-Trung Truong",
        "Yuk-Kwan Wong",
        "Vo Hoang Kim Tuyen Dang",
        "Rinaldi Gotama",
        "Duc Thanh Nguyen",
        "Sai-Kit Yeung"
      ],
      "abstract": "Marine videos present significant challenges for video understanding due to\nthe dynamics of marine objects and the surrounding environment, camera motion,\nand the complexity of underwater scenes. Existing video captioning datasets,\ntypically focused on generic or human-centric domains, often fail to generalize\nto the complexities of the marine environment and gain insights about marine\nlife. To address these limitations, we propose a two-stage marine\nobject-oriented video captioning pipeline. We introduce a comprehensive video\nunderstanding benchmark that leverages the triplets of video, text, and\nsegmentation masks to facilitate visual grounding and captioning, leading to\nimproved marine video understanding and analysis, and marine video generation.\nAdditionally, we highlight the effectiveness of video splitting in order to\ndetect salient object transitions in scene changes, which significantly enrich\nthe semantics of captioning content. Our dataset and code have been released at\nhttps://msc.hkustvgd.com.",
      "pdf_url": "http://arxiv.org/pdf/2508.04549v1",
      "published": "2025-08-06T15:34:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04549v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning",
      "authors": [
        "Zhuang Chen",
        "Guanqun Bi",
        "Wen Zhang",
        "Jiawei Hu",
        "Aoyun Wang",
        "Xiyao Xiao",
        "Kun Feng",
        "Minlie Huang"
      ],
      "abstract": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare.",
      "pdf_url": "http://arxiv.org/pdf/2508.04531v1",
      "published": "2025-08-06T15:13:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04531v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection",
      "authors": [
        "Tianxiao Li",
        "Zhenglin Huang",
        "Haiquan Wen",
        "Yiwei He",
        "Shuchang Lyu",
        "Baoyuan Wu",
        "Guangliang Cheng"
      ],
      "abstract": "The rapid advancement of AI-generation models has enabled the creation of\nhyperrealistic imagery, posing ethical risks through widespread misinformation.\nCurrent deepfake detection methods, categorized as face specific detectors or\ngeneral AI-generated detectors, lack transparency by framing detection as a\nclassification task without explaining decisions. While several LLM-based\napproaches offer explainability, they suffer from coarse-grained analyses and\ndependency on labor-intensive annotations. This paper introduces RAIDX\n(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel\ndeepfake detection framework integrating Retrieval-Augmented Generation (RAG)\nand Group Relative Policy Optimization (GRPO) to enhance detection accuracy and\ndecision explainability. Specifically, RAIDX leverages RAG to incorporate\nexternal knowledge for improved detection accuracy and employs GRPO to\nautonomously generate fine-grained textual explanations and saliency maps,\neliminating the need for extensive manual annotations. Experiments on multiple\nbenchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and\nproviding interpretable rationales in both textual descriptions and saliency\nmaps, achieving state-of-the-art detection performance while advancing\ntransparency in deepfake identification. RAIDX represents the first unified\nframework to synergize RAG and GRPO, addressing critical gaps in accuracy and\nexplainability. Our code and models will be publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2508.04524v1",
      "published": "2025-08-06T15:08:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04524v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Argumentative Debates for Transparent Bias Detection [Technical Report]",
      "authors": [
        "Hamed Ayoobi",
        "Nico Potyka",
        "Anna Rapberger",
        "Francesca Toni"
      ],
      "abstract": "As the use of AI systems in society grows, addressing potential biases that\nemerge from data or are learned by models is essential to prevent systematic\ndisadvantages against specific groups. Several notions of (un)fairness have\nbeen proposed in the literature, alongside corresponding algorithmic methods\nfor detecting and mitigating unfairness, but, with very few exceptions, these\ntend to ignore transparency. Instead, interpretability and explainability are\ncore requirements for algorithmic fairness, even more so than for other\nalgorithmic solutions, given the human-oriented nature of fairness. In this\npaper, we contribute a novel interpretable, explainable method for bias\ndetection relying on debates about the presence of bias against individuals,\nbased on the values of protected features for the individuals and others in\ntheir neighbourhoods. Our method builds upon techniques from formal and\ncomputational argumentation, whereby debates result from arguing about biases\nwithin and across neighbourhoods. We provide formal, quantitative, and\nqualitative evaluations of our method, highlighting its strengths in\nperformance against baselines, as well as its interpretability and\nexplainability.",
      "pdf_url": "http://arxiv.org/pdf/2508.04511v1",
      "published": "2025-08-06T14:56:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04511v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers",
      "authors": [
        "Federico Zucchi",
        "Thomas Lampert"
      ],
      "abstract": "Multivariate time-series classification is pivotal in domains ranging from\nwearable sensing to biomedical monitoring. Despite recent advances,\nTransformer- and CNN-based models often remain computationally heavy, offer\nlimited frequency diversity, and require extensive parameter budgets. We\npropose PRISM (Per-channel Resolution-Informed Symmetric Module), a\nconvolutional-based feature extractor that applies symmetric\nfinite-impulse-response (FIR) filters at multiple temporal scales,\nindependently per channel. This multi-resolution, per-channel design yields\nhighly frequency-selective embeddings without any inter-channel convolutions,\ngreatly reducing model size and complexity. Across human-activity, sleep-stage\nand biomedical benchmarks, PRISM, paired with lightweight classification heads,\nmatches or outperforms leading CNN and Transformer baselines, while using\nroughly an order of magnitude fewer parameters and FLOPs. By uniting classical\nsignal processing insights with modern deep learning, PRISM offers an accurate,\nresource-efficient solution for multivariate time-series classification.",
      "pdf_url": "http://arxiv.org/pdf/2508.04503v1",
      "published": "2025-08-06T14:50:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04503v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Learning Robust Intervention Representations with Delta Embeddings",
      "authors": [
        "Panagiotis Alimisis",
        "Christos Diou"
      ],
      "abstract": "Causal representation learning has attracted significant research interest\nduring the past few years, as a means for improving model generalization and\nrobustness. Causal representations of interventional image pairs, have the\nproperty that only variables corresponding to scene elements affected by the\nintervention / action are changed between the start state and the end state.\nWhile most work in this area has focused on identifying and representing the\nvariables of the scene under a causal model, fewer efforts have focused on\nrepresentations of the interventions themselves. In this work, we show that an\neffective strategy for improving out of distribution (OOD) robustness is to\nfocus on the representation of interventions in the latent space. Specifically,\nwe propose that an intervention can be represented by a Causal Delta Embedding\nthat is invariant to the visual scene and sparse in terms of the causal\nvariables it affects. Leveraging this insight, we propose a framework that is\ncapable of learning causal representations from image pairs, without any\nadditional supervision. Experiments in the Causal Triplet challenge demonstrate\nthat Causal Delta Embeddings are highly effective in OOD settings,\nsignificantly exceeding baseline performance in both synthetic and real-world\nbenchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2508.04492v1",
      "published": "2025-08-06T14:39:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04492v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation",
      "authors": [
        "Erin Lanus",
        "Daniel Wolodkin",
        "Laura J. Freeman"
      ],
      "abstract": "A common use of machine learning (ML) models is predicting the class of a\nsample. Object detection is an extension of classification that includes\nlocalization of the object via a bounding box within the sample.\nClassification, and by extension object detection, is typically evaluated by\ncounting a prediction as incorrect if the predicted label does not match the\nground truth label. This pass/fail scoring treats all misclassifications as\nequivalent. In many cases, class labels can be organized into a class taxonomy\nwith a hierarchical structure to either reflect relationships among the data or\noperator valuation of misclassifications. When such a hierarchical structure\nexists, hierarchical scoring metrics can return the model performance of a\ngiven prediction related to the distance between the prediction and the ground\ntruth label. Such metrics can be viewed as giving partial credit to predictions\ninstead of pass/fail, enabling a finer-grained understanding of the impact of\nmisclassifications. This work develops hierarchical scoring metrics varying in\ncomplexity that utilize scoring trees to encode relationships between class\nlabels and produce metrics that reflect distance in the scoring tree. The\nscoring metrics are demonstrated on an abstract use case with scoring trees\nthat represent three weighting strategies and evaluated by the kind of errors\ndiscouraged. Results demonstrate that these metrics capture errors with finer\ngranularity and the scoring trees enable tuning. This work demonstrates an\napproach to evaluating ML performance that ranks models not only by how many\nerrors are made but by the kind or impact of errors. Python implementations of\nthe scoring metrics will be available in an open-source repository at time of\npublication.",
      "pdf_url": "http://arxiv.org/pdf/2508.04489v1",
      "published": "2025-08-06T14:37:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04489v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Benchmarking Quantum and Classical Sequential Models for Urban Telecommunication Forecasting",
      "authors": [
        "Chi-Sheng Chen",
        "Samuel Yen-Chi Chen",
        "Yun-Cheng Tsai"
      ],
      "abstract": "In this study, we evaluate the performance of classical and quantum-inspired\nsequential models in forecasting univariate time series of incoming SMS\nactivity (SMS-in) using the Milan Telecommunication Activity Dataset. Due to\ndata completeness limitations, we focus exclusively on the SMS-in signal for\neach spatial grid cell. We compare five models, LSTM (baseline), Quantum LSTM\n(QLSTM), Quantum Adaptive Self-Attention (QASA), Quantum Receptance Weighted\nKey-Value (QRWKV), and Quantum Fast Weight Programmers (QFWP), under varying\ninput sequence lengths (4, 8, 12, 16, 32 and 64). All models are trained to\npredict the next 10-minute SMS-in value based solely on historical values\nwithin a given sequence window. Our findings indicate that different models\nexhibit varying sensitivities to sequence length, suggesting that quantum\nenhancements are not universally advantageous. Rather, the effectiveness of\nquantum modules is highly dependent on the specific task and architectural\ndesign, reflecting inherent trade-offs among model size, parameterization\nstrategies, and temporal modeling capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2508.04488v1",
      "published": "2025-08-06T14:37:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04488v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use",
      "authors": [
        "Xueyu Hu",
        "Tao Xiong",
        "Biao Yi",
        "Zishu Wei",
        "Ruixuan Xiao",
        "Yurun Chen",
        "Jiasheng Ye",
        "Meiling Tao",
        "Xiangxin Zhou",
        "Ziyu Zhao",
        "Yuhuai Li",
        "Shengze Xu",
        "Shenzhi Wang",
        "Xinchen Xu",
        "Shuofei Qiao",
        "Zhaokai Wang",
        "Kun Kuang",
        "Tieyong Zeng",
        "Liang Wang",
        "Jiwei Li",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou",
        "Guoyin Wang",
        "Keting Yin",
        "Zhou Zhao",
        "Hongxia Yang",
        "Fan Wu",
        "Shengyu Zhang",
        "Fei Wu"
      ],
      "abstract": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.",
      "pdf_url": "http://arxiv.org/pdf/2508.04482v1",
      "published": "2025-08-06T14:33:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04482v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Metric Learning in an RKHS",
      "authors": [
        "Gokcan Tatli",
        "Yi Chen",
        "Blake Mason",
        "Robert Nowak",
        "Ramya Korlakai Vinayak"
      ],
      "abstract": "Metric learning from a set of triplet comparisons in the form of \"Do you\nthink item h is more similar to item i or item j?\", indicating similarity and\ndifferences between items, plays a key role in various applications including\nimage retrieval, recommendation systems, and cognitive psychology. The goal is\nto learn a metric in the RKHS that reflects the comparisons. Nonlinear metric\nlearning using kernel methods and neural networks have shown great empirical\npromise. While previous works have addressed certain aspects of this problem,\nthere is little or no theoretical understanding of such methods. The exception\nis the special (linear) case in which the RKHS is the standard Euclidean space\n$\\mathbb{R}^d$; there is a comprehensive theory for metric learning in\n$\\mathbb{R}^d$. This paper develops a general RKHS framework for metric\nlearning and provides novel generalization guarantees and sample complexity\nbounds. We validate our findings through a set of simulations and experiments\non real datasets. Our code is publicly available at\nhttps://github.com/RamyaLab/metric-learning-RKHS.",
      "pdf_url": "http://arxiv.org/pdf/2508.04476v1",
      "published": "2025-08-06T14:29:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04476v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model",
      "authors": [
        "Hongxu Chen",
        "Zhen Wang",
        "Taoran Mei",
        "Lin Li",
        "Bowei Zhu",
        "Runshi Li",
        "Long Chen"
      ],
      "abstract": "Concept Erasure, which aims to prevent pretrained text-to-image models from\ngenerating content associated with semantic-harmful concepts (i.e., target\nconcepts), is getting increased attention. State-of-the-art methods formulate\nthis task as an optimization problem: they align all target concepts with\nsemantic-harmless anchor concepts, and apply closed-form solutions to update\nthe model accordingly. While these closed-form methods are efficient, we argue\nthat existing methods have two overlooked limitations: 1) They often result in\nincomplete erasure due to \"non-zero alignment residual\", especially when text\nprompts are relatively complex. 2) They may suffer from generation quality\ndegradation as they always concentrate parameter updates in a few deep layers.\nTo address these issues, we propose a novel closed-form method ErasePro: it is\ndesigned for more complete concept erasure and better preserving overall\ngenerative quality. Specifically, ErasePro first introduces a strict\nzero-residual constraint into the optimization objective, ensuring perfect\nalignment between target and anchor concept features and enabling more complete\nerasure. Secondly, it employs a progressive, layer-wise update strategy that\ngradually transfers target concept features to those of the anchor concept from\nshallow to deep layers. As the depth increases, the required parameter changes\ndiminish, thereby reducing deviations in sensitive deep layers and preserving\ngenerative quality. Empirical results across different concept erasure tasks\n(including instance, art style, and nudity erasure) have demonstrated the\neffectiveness of our ErasePro.",
      "pdf_url": "http://arxiv.org/pdf/2508.04472v1",
      "published": "2025-08-06T14:19:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04472v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Small transformer architectures for task switching",
      "authors": [
        "Claudius Gros"
      ],
      "abstract": "The rapid progress seen in terms of large-scale generative AI is largely\nbased on the attention mechanism. It is conversely non-trivial to conceive\nsmall-scale applications for which attention-based architectures outperform\ntraditional approaches, such as multi-layer perceptrons or recurrent networks.\nWe examine this problem in the context of 'task switching'. In this framework\nmodels work on ongoing token sequences with the current task being determined\nby stochastically interspersed control tokens. We show that standard\ntransformers cannot solve a basic task switching reference model based on\nfinite domain arithmetics which contains subtasks dedicated to increment /\naddition / reverse copy / context (IARC). We show that transformers, long\nshort-term memory recurrent networks (LSTM), and plain multi-layer perceptrons\n(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our\ncomparative study by including an extension of the standard transformer\narchitecture to its non-translational invariant counterpart, the cisformer, and\nan alternative attention mechanism, extensive attention. A combination of the\nlatter is found to be the only model able to achieve considerable performance\nlevels, of around 95%. Our results indicate that the workings of attention can\nbe understood better, and even improved, when comparing qualitatively different\nformulations in task-switching settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.04461v1",
      "published": "2025-08-06T14:01:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04461v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "From \"Aha Moments\" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control",
      "authors": [
        "Rui Ha",
        "Chaozhuo Li",
        "Rui Pu",
        "Sen Su"
      ],
      "abstract": "Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex\nreasoning by spontaneously exhibiting cognitive behaviors such as step-by-step\nreasoning, reflection, and backtracking, commonly referred to as \"Aha Moments\".\nHowever, such emergent behaviors remain unregulated and uncontrolled, often\nresulting in overthinking, where the model continues generating redundant\nreasoning content even after reaching reliable conclusions. This leads to\nexcessive computational costs and increased latency, limiting the practical\ndeployment of LRMs. The root cause lies in the absence of intrinsic regulatory\nmechanisms, as current models are unable to monitor and adaptively manage their\nreasoning process to determine when to continue, backtrack, or terminate. To\naddress this issue, we propose the Meta-cognitive Reasoning Framework (MERA),\nwhich explicitly decouples the thinking process into distinct reasoning and\ncontrol components, thereby enabling the independent optimization of control\nstrategies. Specifically, MERA incorporates a takeover-based data construction\nmechanism that identifies critical decision points during reasoning and\ndelegates the creation of control signals to auxiliary LLMs, thereby enabling\nthe construction of high-quality reasoning-control data. Additionally, a\nstructured reasoning-control separation is implemented via supervised\nfine-tuning, enabling the model to generate explicit traces and acquire initial\nmeta-cognitive control capabilities. Finally, MERA employs Control-Segment\nPolicy Optimization (CSPO), which combines segment-wise Group Relative Policy\nOptimization (GRPO) with a control-masking mechanism to optimize control\nbehavior learning while minimizing interference from irrelevant content.\nExperiments on various reasoning benchmarks demonstrate that models trained\nwith MERA enhance both reasoning efficiency and accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.04460v1",
      "published": "2025-08-06T13:59:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04460v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Automatic LLM Red Teaming",
      "authors": [
        "Roman Belaire",
        "Arunesh Sinha",
        "Pradeep Varakantham"
      ],
      "abstract": "Red teaming is critical for identifying vulnerabilities and building trust in\ncurrent LLMs. However, current automated methods for Large Language Models\n(LLMs) rely on brittle prompt templates or single-turn attacks, failing to\ncapture the complex, interactive nature of real-world adversarial dialogues. We\npropose a novel paradigm: training an AI to strategically `break' another AI.\nBy formalizing red teaming as a Markov Decision Process (MDP) and employing a\nhierarchical Reinforcement Learning (RL) framework, we effectively address the\ninherent sparse reward and long-horizon challenges. Our generative agent learns\ncoherent, multi-turn attack strategies through a fine-grained, token-level harm\nreward, enabling it to uncover subtle vulnerabilities missed by existing\nbaselines. This approach sets a new state-of-the-art, fundamentally reframing\nLLM red teaming as a dynamic, trajectory-based process (rather than a one-step\ntest) essential for robust AI deployment.",
      "pdf_url": "http://arxiv.org/pdf/2508.04451v1",
      "published": "2025-08-06T13:52:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04451v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling",
      "authors": [
        "Biao Hu",
        "Guoyin Wang"
      ],
      "abstract": "We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a\nnovel generative model that integrates the cloud model into the Wasserstein\nAuto-Encoder (WAE) framework. By leveraging the characteristic functions of the\ncloud model to regularize the latent space, our approach enables more accurate\nmodeling of complex data distributions. Unlike conventional methods that rely\non a standard Gaussian prior and traditional divergence measures, our method\nemploys a cloud model prior, providing a more flexible and realistic\nrepresentation of the latent space, thus mitigating the homogenization observed\nin reconstructed samples. We derive the characteristic function of the cloud\nmodel and propose a corresponding regularizer within the WAE framework.\nExtensive quantitative and qualitative evaluations on MNIST, FashionMNIST,\nCIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in\nterms of reconstruction quality, latent space structuring, and sample\ndiversity. This work not only establishes a novel integration of cloud model\ntheory with MMD-based regularization but also offers a promising new\nperspective for enhancing autoencoder-based generative models.",
      "pdf_url": "http://arxiv.org/pdf/2508.04447v1",
      "published": "2025-08-06T13:44:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04447v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI",
      "authors": [
        "Rohaizah Abdul Wahid",
        "Muhamad Said Nizamuddin Nadim",
        "Suliana Sulaiman",
        "Syahmi Akmal Shaharudin",
        "Muhammad Danial Jupikil",
        "Iqqwan Jasman Su Azlan Su"
      ],
      "abstract": "This paper addresses the critical need for scalable and high-quality\neducational assessment tools within the Malaysian education system. It\nhighlights the potential of Generative AI (GenAI) while acknowledging the\nsignificant challenges of ensuring factual accuracy and curriculum alignment,\nespecially for low-resource languages like Bahasa Melayu. This research\nintroduces and compares four incremental pipelines for generating Form 1\nMathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's\nGPT-4o. The methods range from non-grounded prompting (structured and basic) to\nRetrieval-Augmented Generation (RAG) approaches (one using the LangChain\nframework, one implemented manually). The system is grounded in official\ncurriculum documents, including teacher-prepared notes and the yearly teaching\nplan (RPT). A dual-pronged automated evaluation framework is employed to assess\nthe generated questions. Curriculum alignment is measured using Semantic\nTextual Similarity (STS) against the RPT, while contextual validity is verified\nthrough a novel RAG-based Question-Answering (RAG-QA) method. The results\ndemonstrate that RAG-based pipelines significantly outperform non-grounded\nprompting methods, producing questions with higher curriculum alignment and\nfactual validity. The study further analyzes the trade-offs between the ease of\nimplementation of framework-based RAG and the fine-grained control offered by a\nmanual pipeline. This work presents a validated methodology for generating\ncurriculum-specific educational content in a low-resource language, introduces\na symbiotic RAG-QA evaluation technique, and provides actionable insights for\nthe development and deployment of practical EdTech solutions in Malaysia and\nsimilar regions.",
      "pdf_url": "http://arxiv.org/pdf/2508.04442v1",
      "published": "2025-08-06T13:30:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04442v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion",
      "authors": [
        "Yutong Wu",
        "Di Huang",
        "Ruosi Wan",
        "Yue Peng",
        "Shijie Shang",
        "Chenrui Cao",
        "Lei Qi",
        "Rui Zhang",
        "Zidong Du",
        "Jie Yan",
        "Xing Hu"
      ],
      "abstract": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.",
      "pdf_url": "http://arxiv.org/pdf/2508.04440v1",
      "published": "2025-08-06T13:28:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04440v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices",
      "authors": [
        "Si Chen",
        "Izzy Molnar",
        "Ting Hua",
        "Peiyu Li",
        "Le Huy Khiem",
        "G. Alex Ambrose",
        "Jim Lang",
        "Ronald Metoyer",
        "Nitesh V. Chawla"
      ],
      "abstract": "High-quality, multi-turn instructional dialogues between novices and experts\nare essential for developing AI systems that support teaching, learning, and\ndecision-making. These dialogues often involve scaffolding -- the process by\nwhich an expert supports a novice's thinking through questions, feedback, and\nstep-by-step guidance. However, such data are scarce due to privacy concerns in\nrecording and the vulnerability inherent in help-seeking. We present\nSimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding\ndialogues. Using teaching development coaching as an example domain,\nSimInstruct simulates novice instructors via LLMs, varying their teaching\nchallenges and LLM's persona traits, while human experts provide multi-turn\nfeedback, reasoning, and instructional support. This design enables the\ncreation of realistic, pedagogically rich dialogues without requiring real\nnovice participants. Our results reveal that persona traits, such as\nextroversion and introversion, meaningfully influence how experts engage.\nCompared to real mentoring recordings, SimInstruct dialogues demonstrate\ncomparable pedagogical relevance and cognitive depth. Experts also reported the\nprocess as engaging and reflective, improving both data quality and their own\nprofessional insight. We further fine-tuned a LLaMA model to be an expert model\nusing the augmented dataset, which outperformed GPT-4o in instructional\nquality. Our analysis highlights GPT-4o's limitations in weak reflective\nquestioning, overuse of generic praise, a condescending tone, and a tendency to\noverwhelm novices with excessive suggestions.",
      "pdf_url": "http://arxiv.org/pdf/2508.04428v1",
      "published": "2025-08-06T13:16:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04428v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models",
      "authors": [
        "Md Raisul Kibria",
        "Sébastien Lafond",
        "Janan Arslan"
      ],
      "abstract": "Multimodal learning has witnessed remarkable advancements in recent years,\nparticularly with the integration of attention-based models, leading to\nsignificant performance gains across a variety of tasks. Parallel to this\nprogress, the demand for explainable artificial intelligence (XAI) has spurred\na growing body of research aimed at interpreting the complex decision-making\nprocesses of these models. This systematic literature review analyzes research\npublished between January 2020 and early 2024 that focuses on the\nexplainability of multimodal models. Framed within the broader goals of XAI, we\nexamine the literature across multiple dimensions, including model\narchitecture, modalities involved, explanation algorithms and evaluation\nmethodologies. Our analysis reveals that the majority of studies are\nconcentrated on vision-language and language-only models, with attention-based\ntechniques being the most commonly employed for explanation. However, these\nmethods often fall short in capturing the full spectrum of interactions between\nmodalities, a challenge further compounded by the architectural heterogeneity\nacross domains. Importantly, we find that evaluation methods for XAI in\nmultimodal settings are largely non-systematic, lacking consistency,\nrobustness, and consideration for modality-specific cognitive and contextual\nfactors. Based on these findings, we provide a comprehensive set of\nrecommendations aimed at promoting rigorous, transparent, and standardized\nevaluation and reporting practices in multimodal XAI research. Our goal is to\nsupport future research in more interpretable, accountable, and responsible\nmulitmodal AI systems, with explainability at their core.",
      "pdf_url": "http://arxiv.org/pdf/2508.04427v1",
      "published": "2025-08-06T13:14:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04427v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation",
      "authors": [
        "Jinxing Zhou",
        "Yanghao Zhou",
        "Mingfei Han",
        "Tong Wang",
        "Xiaojun Chang",
        "Hisham Cholakkal",
        "Rao Muhammad Anwer"
      ],
      "abstract": "Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects\nin audible videos based on given reference expressions. Prior works typically\nrely on learning latent embeddings via multimodal fusion to prompt a tunable\nSAM/SAM2 decoder for segmentation, which requires strong pixel-level\nsupervision and lacks interpretability. From a novel perspective of explicit\nreference understanding, we propose TGS-Agent, which decomposes the task into a\nThink-Ground-Segment process, mimicking the human reasoning procedure by first\nidentifying the referred object through multimodal analysis, followed by\ncoarse-grained grounding and precise segmentation. To this end, we first\npropose Ref-Thinker, a multimodal language model capable of reasoning over\ntextual, visual, and auditory cues. We construct an instruction-tuning dataset\nwith explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The\nobject description inferred by Ref-Thinker is used as an explicit prompt for\nGrounding-DINO and SAM2, which perform grounding and segmentation without\nrelying on pixel-level supervision. Additionally, we introduce\nR\\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and\nreasoning-intensive references for better evaluating model generalization. Our\napproach achieves state-of-the-art results on both standard Ref-AVSBench and\nproposed R\\textsuperscript{2}-AVSBench. Code will be available at\nhttps://github.com/jasongief/TGS-Agent.",
      "pdf_url": "http://arxiv.org/pdf/2508.04418v1",
      "published": "2025-08-06T13:05:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04418v1",
      "categories": [
        "cs.MM",
        "cs.CV",
        "cs.MA",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents",
      "authors": [
        "Thassilo M. Schiepanski",
        "Nicholas Piël"
      ],
      "abstract": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2508.04412v1",
      "published": "2025-08-06T12:56:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04412v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ]
    },
    {
      "title": "Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models",
      "authors": [
        "Yinan Yu",
        "Alex Gonzalez-Caceres",
        "Samuel Scheidegger",
        "Sanjay Somanath",
        "Alexander Hollberg"
      ],
      "abstract": "Renovating existing buildings is essential for climate impact. Early-phase\nrenovation planning requires simulations based on thermal 3D models at Level of\nDetail (LoD) 3, which include features like windows. However, scalable and\naccurate identification of such features remains a challenge. This paper\npresents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that\ngenerates LoD3 thermal models by extracting geometries from images using both\ncomputer vision and deep learning. Unlike existing methods relying on\nsegmentation and projection, SI3FP directly models geometric primitives in the\northographic image plane, providing a unified interface while reducing\nperspective distortions. SI3FP supports both sparse (e.g., Google Street View)\nand dense (e.g., hand-held camera) data sources. Tested on typical Swedish\nresidential buildings, SI3FP achieved approximately 5% error in window-to-wall\nratio estimates, demonstrating sufficient accuracy for early-stage renovation\nanalysis. The pipeline facilitates large-scale energy renovation planning and\nhas broader applications in urban development and planning.",
      "pdf_url": "http://arxiv.org/pdf/2508.04406v1",
      "published": "2025-08-06T12:48:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04406v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Why are LLMs' abilities emergent?",
      "authors": [
        "Vladimír Havlík"
      ],
      "abstract": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents.",
      "pdf_url": "http://arxiv.org/pdf/2508.04401v1",
      "published": "2025-08-06T12:43:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04401v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky",
      "authors": [
        "Xu Zhang",
        "Mei Chen"
      ],
      "abstract": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP.",
      "pdf_url": "http://arxiv.org/pdf/2508.04399v1",
      "published": "2025-08-06T12:41:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04399v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning",
      "authors": [
        "Weitai Kang",
        "Bin Lei",
        "Gaowen Liu",
        "Caiwen Ding",
        "Yan Yan"
      ],
      "abstract": "Graphical user interface visual grounding (GUI-VG), a core capability for GUI\nagents, has primarily relied on supervised fine-tuning (SFT) of multimodal\nlarge language models (MLLMs), which demands extensive data curation and\nsignificant training costs. However, as MLLMs continue to advance and even\ncover GUI domains during pretraining, the necessity of exhaustive SFT\npost-training becomes increasingly questionable. Meanwhile, recent successes of\nrule-based reinforcement fine-tuning (RFT) suggest a more efficient\nalternative. Despite this promise, the optimal manner of applying RFT for\nGUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a\nreinforcement learning-based GUI-VG method built on a systematic empirical\nstudy and a novel stabilization technique. We find that naive application of\nRFT underperforms the SFT baseline, motivating a deeper exploration. First, we\ndecompose RFT into its core components and analyze the optimal formulation of\neach. Second, we propose a novel Adversarial KL Factor that dynamically\nstabilizes training to mitigate reward over-optimization. Third, we further\nexplore the training configurations of RFT to enhance effectiveness. Extensive\nexperiments show that GuirlVG, with only 5.2K training samples, outperforms SFT\nmethods trained on over 10M samples, achieving a 7.7% improvement on\nScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on\nScreenSpotV2.",
      "pdf_url": "http://arxiv.org/pdf/2508.04389v1",
      "published": "2025-08-06T12:35:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04389v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Artificial Consciousness as Interface Representation",
      "authors": [
        "Robert Prentner"
      ],
      "abstract": "Whether artificial intelligence (AI) systems can possess consciousness is a\ncontentious question because of the inherent challenges of defining and\noperationalizing subjective experience. This paper proposes a framework to\nreframe the question of artificial consciousness into empirically tractable\ntests. We introduce three evaluative criteria - S (subjective-linguistic), L\n(latent-emergent), and P (phenomenological-structural) - collectively termed\nSLP-tests, which assess whether an AI system instantiates interface\nrepresentations that facilitate consciousness-like properties. Drawing on\ncategory theory, we model interface representations as mappings between\nrelational substrates (RS) and observable behaviors, akin to specific types of\nabstraction layers. The SLP-tests collectively operationalize subjective\nexperience not as an intrinsic property of physical systems but as a functional\ninterface to a relational entity.",
      "pdf_url": "http://arxiv.org/pdf/2508.04383v1",
      "published": "2025-08-06T12:25:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.04383v1",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ]
    }
  ]
}