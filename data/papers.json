{
  "last_updated": "2025-09-10T00:46:51.032952",
  "papers": [
    {
      "title": "H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers",
      "authors": [
        "Wenhao Li",
        "Mengyuan Liu",
        "Hong Liu",
        "Pichao Wang",
        "Shijian Lu",
        "Nicu Sebe"
      ],
      "abstract": "Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a hierarchical plug-and-play pruning-and-recovering\nframework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient\ntransformer-based 3D human pose estimation from videos. H$_{2}$OT begins with\nprogressively pruning pose tokens of redundant frames and ends with recovering\nfull-length sequences, resulting in a few pose tokens in the intermediate\ntransformer blocks and thus improving the model efficiency. It works with two\nkey modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module\n(TRM). TPM dynamically selects a few representative tokens to eliminate the\nredundancy of video frames, while TRM restores the detailed spatio-temporal\ninformation based on the selected tokens, thereby expanding the network output\nto the original full-length temporal resolution for fast inference. Our method\nis general-purpose: it can be easily incorporated into common VPT models on\nboth seq2seq and seq2frame pipelines while effectively accommodating different\ntoken pruning and recovery strategies. In addition, our H$_{2}$OT reveals that\nmaintaining the full pose sequence is unnecessary, and a few pose tokens of\nrepresentative frames can achieve both high efficiency and estimation accuracy.\nExtensive experiments on multiple benchmark datasets demonstrate both the\neffectiveness and efficiency of the proposed method. Code and models are\navailable at https://github.com/NationalGAILab/HoT.",
      "pdf_url": "http://arxiv.org/pdf/2509.06956v1",
      "published": "2025-09-08T17:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06956v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments",
      "authors": [
        "Jiahui Yang",
        "Jason Jingzhou Liu",
        "Yulong Li",
        "Youssef Khaky",
        "Kenneth Shaw",
        "Deepak Pathak"
      ],
      "abstract": "Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com",
      "pdf_url": "http://arxiv.org/pdf/2509.06953v1",
      "published": "2025-09-08T17:59:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06953v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Interleaving Reasoning for Better Text-to-Image Generation",
      "authors": [
        "Wenxuan Huang",
        "Shuang Chen",
        "Zheyong Xie",
        "Shaosheng Cao",
        "Shixiang Tang",
        "Yufan Shen",
        "Qingyu Yin",
        "Wenbo Hu",
        "Xiaoman Wang",
        "Yuntian Tang",
        "Junbo Qiao",
        "Yue Guo",
        "Yao Hu",
        "Zhenfei Yin",
        "Philip Torr",
        "Yu Cheng",
        "Wanli Ouyang",
        "Shaohui Lin"
      ],
      "abstract": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
      "pdf_url": "http://arxiv.org/pdf/2509.06945v2",
      "published": "2025-09-08T17:56:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06945v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference",
      "authors": [
        "Xiangwei Shen",
        "Zhimin Li",
        "Zhantao Yang",
        "Shiyi Zhang",
        "Yingfang Zhang",
        "Donghao Li",
        "Chunyu Wang",
        "Qinglin Lu",
        "Yansong Tang"
      ],
      "abstract": "Recent studies have demonstrated the effectiveness of directly aligning\ndiffusion models with human preferences using differentiable reward. However,\nthey exhibit two primary challenges: (1) they rely on multistep denoising with\ngradient computation for reward scoring, which is computationally expensive,\nthus restricting optimization to only a few diffusion steps; (2) they often\nneed continuous offline adaptation of reward models in order to achieve desired\naesthetic quality, such as photorealism or precise lighting effects. To address\nthe limitation of multistep denoising, we propose Direct-Align, a method that\npredefines a noise prior to effectively recover original images from any time\nsteps via interpolation, leveraging the equation that diffusion states are\ninterpolations between noise and target images, which effectively avoids\nover-optimization in late timesteps. Furthermore, we introduce Semantic\nRelative Preference Optimization (SRPO), in which rewards are formulated as\ntext-conditioned signals. This approach enables online adjustment of rewards in\nresponse to positive and negative prompt augmentation, thereby reducing the\nreliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model\nwith optimized denoising and online reward adjustment, we improve its\nhuman-evaluated realism and aesthetic quality by over 3x.",
      "pdf_url": "http://arxiv.org/pdf/2509.06942v1",
      "published": "2025-09-08T17:54:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06942v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers",
      "authors": [
        "Praneet Suresh",
        "Jack Stanley",
        "Sonia Joseph",
        "Luca Scimeca",
        "Danilo Bzdok"
      ],
      "abstract": "As generative AI systems become competent and democratized in science,\nbusiness, and government, deeper insight into their failure modes now poses an\nacute need. The occasional volatility in their behavior, such as the propensity\nof transformer models to hallucinate, impedes trust and adoption of emerging AI\nsolutions in high-stakes areas. In the present work, we establish how and when\nhallucinations arise in pre-trained transformer models through concept\nrepresentations captured by sparse autoencoders, under scenarios with\nexperimentally controlled uncertainty in the input space. Our systematic\nexperiments reveal that the number of semantic concepts used by the transformer\nmodel grows as the input information becomes increasingly unstructured. In the\nface of growing uncertainty in the input space, the transformer model becomes\nprone to activate coherent yet input-insensitive semantic features, leading to\nhallucinated output. At its extreme, for pure-noise inputs, we identify a wide\nvariety of robustly triggered and meaningful concepts in the intermediate\nactivations of pre-trained transformer models, whose functional integrity we\nconfirm through targeted steering. We also show that hallucinations in the\noutput of a transformer model can be reliably predicted from the concept\npatterns embedded in transformer layer activations. This collection of insights\non transformer internal processing mechanics has immediate consequences for\naligning AI models with human values, AI safety, opening the attack surface for\npotential adversarial attacks, and providing a basis for automatic\nquantification of a model's hallucination risk.",
      "pdf_url": "http://arxiv.org/pdf/2509.06938v1",
      "published": "2025-09-08T17:50:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06938v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities",
      "authors": [
        "Safayat Bin Hakim",
        "Muhammad Adil",
        "Alvaro Velasquez",
        "Shouhuai Xu",
        "Houbing Herbert Song"
      ],
      "abstract": "Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit\nfundamental limitations: inadequate conceptual grounding leading to\nnon-robustness against novel attacks; limited instructibility impeding\nanalyst-guided adaptation; and misalignment with cybersecurity objectives.\nNeuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize\ncybersecurity AI. However, there is no systematic understanding of this\nemerging approach. These hybrid systems address critical cybersecurity\nchallenges by combining neural pattern recognition with symbolic reasoning,\nenabling enhanced threat understanding while introducing concerning autonomous\noffensive capabilities that reshape threat landscapes. In this survey, we\nsystematically characterize this field by analyzing 127 publications spanning\n2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)\nframework to evaluate these systems, focusing on both cyber defense and cyber\noffense across network security, malware analysis, and cyber operations. Our\nanalysis shows advantages of multi-agent NeSy architectures and identifies\ncritical implementation challenges including standardization gaps,\ncomputational complexity, and human-AI collaboration requirements that\nconstrain deployment. We show that causal reasoning integration is the most\ntransformative advancement, enabling proactive defense beyond correlation-based\napproaches. Our findings highlight dual-use implications where autonomous\nsystems demonstrate substantial capabilities in zero-day exploitation while\nachieving significant cost reductions, altering threat dynamics. We provide\ninsights and future research directions, emphasizing the urgent need for\ncommunity-driven standardization frameworks and responsible development\npractices that ensure advancement serves defensive cybersecurity objectives\nwhile maintaining societal alignment.",
      "pdf_url": "http://arxiv.org/pdf/2509.06921v1",
      "published": "2025-09-08T17:33:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06921v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection",
      "authors": [
        "Haywood Gelman",
        "John D. Hastings",
        "David Kenley"
      ],
      "abstract": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection.",
      "pdf_url": "http://arxiv.org/pdf/2509.06920v1",
      "published": "2025-09-08T17:32:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06920v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "C.2.0; I.2.7; K.4.1; H.3.3"
      ]
    },
    {
      "title": "Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition",
      "authors": [
        "Tarhib Al Azad",
        "Shahana Ibrahim"
      ],
      "abstract": "Robust out-of-distribution (OOD) detection is an indispensable component of\nmodern artificial intelligence (AI) systems, especially in safety-critical\napplications where models must identify inputs from unfamiliar classes not seen\nduring training. While OOD detection has been extensively studied in the\nmachine learning literature--with both post hoc and training-based\napproaches--its effectiveness under noisy training labels remains\nunderexplored. Recent studies suggest that label noise can significantly\ndegrade OOD performance, yet principled solutions to this issue are lacking. In\nthis work, we demonstrate that directly combining existing label noise-robust\nmethods with OOD detection strategies is insufficient to address this critical\nchallenge. To overcome this, we propose a robust OOD detection framework that\nintegrates loss correction techniques from the noisy label learning literature\nwith low-rank and sparse decomposition methods from signal processing.\nExtensive experiments on both synthetic and real-world datasets demonstrate\nthat our method significantly outperforms the state-of-the-art OOD detection\ntechniques, particularly under severe noisy label settings.",
      "pdf_url": "http://arxiv.org/pdf/2509.06918v1",
      "published": "2025-09-08T17:28:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06918v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents",
      "authors": [
        "Jiacheng Miao",
        "Joe R. Davis",
        "Jonathan K. Pritchard",
        "James Zou"
      ],
      "abstract": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.",
      "pdf_url": "http://arxiv.org/pdf/2509.06917v1",
      "published": "2025-09-08T17:28:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06917v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers",
      "authors": [
        "Morteza Kiani Haftlang",
        "Mohammadhossein Malmir",
        "Foroutan Parand",
        "Umberto Michelucci",
        "Safouane El Ghazouali"
      ],
      "abstract": "Medical image segmentation is a critical task in clinical workflows,\nparticularly for the detection and delineation of pathological regions. While\nconvolutional architectures like U-Net have become standard for such tasks,\ntheir limited receptive field restricts global context modeling. Recent efforts\nintegrating transformers have addressed this, but often result in deep,\ncomputationally expensive models unsuitable for real-time use. In this work, we\npresent a novel end-to-end lightweight architecture designed specifically for\nreal-time binary medical image segmentation. Our model combines a Swin\nTransformer-like encoder with a U-Net-like decoder, connected via skip pathways\nto preserve spatial detail while capturing contextual information. Unlike\nexisting designs such as Swin Transformer or U-Net, our architecture is\nsignificantly shallower and competitively efficient. To improve the encoder's\nability to learn meaningful features without relying on large amounts of\nlabeled data, we first train it using Barlow Twins, a self-supervised learning\nmethod that helps the model focus on important patterns by reducing unnecessary\nrepetition in the learned features. After this pretraining, we fine-tune the\nentire model for our specific task. Experiments on benchmark binary\nsegmentation tasks demonstrate that our model achieves competitive accuracy\nwith substantially reduced parameter count and faster inference, positioning it\nas a practical alternative for deployment in real-time and resource-limited\nclinical environments. The code for our method is available at Github\nrepository: https://github.com/mkianih/Barlow-Swin.",
      "pdf_url": "http://arxiv.org/pdf/2509.06885v1",
      "published": "2025-09-08T17:05:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06885v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction",
      "authors": [
        "Joe Wilder",
        "Nikhil Kadapala",
        "Benji Xu",
        "Mohammed Alsaadi",
        "Aiden Parsons",
        "Mitchell Rogers",
        "Palash Agarwal",
        "Adam Hassick",
        "Laura Dietz"
      ],
      "abstract": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower.",
      "pdf_url": "http://arxiv.org/pdf/2509.06883v1",
      "published": "2025-09-08T17:02:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06883v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification",
      "authors": [
        "Sukumar Kishanthan",
        "Asela Hevapathige"
      ],
      "abstract": "Class imbalance in machine learning poses a significant challenge, as skewed\ndatasets often hinder performance on minority classes. Traditional oversampling\ntechniques, which are commonly used to alleviate class imbalance, have several\ndrawbacks: they treat features independently, lack similarity-based controls,\nlimit sample diversity, and fail to manage synthetic variety effectively. To\novercome these issues, we introduce AxelSMOTE, an innovative agent-based\napproach that views data instances as autonomous agents engaging in complex\ninteractions. Based on Axelrod's cultural dissemination model, AxelSMOTE\nimplements four key innovations: (1) trait-based feature grouping to preserve\ncorrelations; (2) a similarity-based probabilistic exchange mechanism for\nmeaningful interactions; (3) Beta distribution blending for realistic\ninterpolation; and (4) controlled diversity injection to avoid overfitting.\nExperiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms\nstate-of-the-art sampling methods while maintaining computational efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2509.06875v1",
      "published": "2025-09-08T16:47:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06875v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL",
      "authors": [
        "Bhavya Agrawalla",
        "Michal Nauman",
        "Khush Agarwal",
        "Aviral Kumar"
      ],
      "abstract": "A hallmark of modern large-scale machine learning techniques is the use of\ntraining objectives that provide dense supervision to intermediate\ncomputations, such as teacher forcing the next token in language models or\ndenoising step-by-step in diffusion models. This enables models to learn\ncomplex functions in a generalizable manner. Motivated by this observation, we\ninvestigate the benefits of iterative computation for temporal difference (TD)\nmethods in reinforcement learning (RL). Typically they represent value\nfunctions in a monolithic fashion, without iterative compute. We introduce floq\n(flow-matching Q-functions), an approach that parameterizes the Q-function\nusing a velocity field and trains it using techniques from flow-matching,\ntypically used in generative modeling. This velocity field underneath the flow\nis trained using a TD-learning objective, which bootstraps from values produced\nby a target velocity field, computed by running multiple steps of numerical\nintegration. Crucially, floq allows for more fine-grained control and scaling\nof the Q-function capacity than monolithic architectures, by appropriately\nsetting the number of integration steps. Across a suite of challenging offline\nRL benchmarks and online fine-tuning tasks, floq improves performance by nearly\n1.8x. floq scales capacity far better than standard TD-learning architectures,\nhighlighting the potential of iterative computation for value learning.",
      "pdf_url": "http://arxiv.org/pdf/2509.06863v1",
      "published": "2025-09-08T16:31:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06863v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet",
      "authors": [
        "James Xu Zhao",
        "Bryan Hooi",
        "See-Kiong Ng"
      ],
      "abstract": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge",
      "pdf_url": "http://arxiv.org/pdf/2509.06861v1",
      "published": "2025-09-08T16:28:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06861v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Disentangling Interaction and Bias Effects in Opinion Dynamics of Large Language Models",
      "authors": [
        "Vincent C. Brockers",
        "David A. Ehrlich",
        "Viola Priesemann"
      ],
      "abstract": "Large Language Models are increasingly used to simulate human opinion\ndynamics, yet the effect of genuine interaction is often obscured by systematic\nbiases. We present a Bayesian framework to disentangle and quantify three such\nbiases: (i) a topic bias toward prior opinions in the training data; (ii) an\nagreement bias favoring agreement irrespective of the question; and (iii) an\nanchoring bias toward the initiating agent's stance. Applying this framework to\nmulti-step dialogues reveals that opinion trajectories tend to quickly converge\nto a shared attractor, with the influence of the interaction fading over time,\nand the impact of biases differing between LLMs. In addition, we fine-tune an\nLLM on different sets of strongly opinionated statements (incl. misinformation)\nand demonstrate that the opinion attractor shifts correspondingly. Exposing\nstark differences between LLMs and providing quantitative tools to compare them\nto human subjects in the future, our approach highlights both chances and\npitfalls in using LLMs as proxies for human behavior.",
      "pdf_url": "http://arxiv.org/pdf/2509.06858v1",
      "published": "2025-09-08T16:26:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06858v1",
      "categories": [
        "physics.soc-ph",
        "cs.AI",
        "nlin.AO"
      ]
    },
    {
      "title": "Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice",
      "authors": [
        "Hajar Moradmand",
        "Lei Ren"
      ],
      "abstract": "Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van\nDer Heijde Score (TSS) is crucial, but manual scoring is often time-consuming\nand subjective. This study introduces an Automated Radiographic Sharp Scoring\n(ARTSS) framework that leverages deep learning to analyze full-hand X-ray\nimages, aiming to reduce inter- and intra-observer variability. The research\nuniquely accommodates patients with joint disappearance and variable-length\nimage sequences. We developed ARTSS using data from 970 patients, structured\ninto four stages: I) Image pre-processing and re-orientation using ResNet50,\nII) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and\nIV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201,\nEfficientNetB0, and Vision Transformer (ViT). We evaluated model performance\nwith Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute\nerror (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS\nfrom two radiologists was used as the ground truth. Model training employed\n3-fold cross-validation, with each fold consisting of 452 training and 227\nvalidation samples, and external testing included 291 unseen subjects. Our\njoint identification model achieved 99% accuracy. The best-performing model,\nViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results\ndemonstrate the potential of deep learning to automate RA scoring, which can\nsignificantly enhance clinical practice. Our approach addresses the challenge\nof joint disappearance and variable joint numbers, offers timesaving benefits,\nreduces inter- and intra-reader variability, improves radiologist accuracy, and\naids rheumatologists in making more informed decisions.",
      "pdf_url": "http://arxiv.org/pdf/2509.06854v1",
      "published": "2025-09-08T16:21:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06854v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Reinforcement learning meets bioprocess control through behaviour cloning: Real-world deployment in an industrial photobioreactor",
      "authors": [
        "Juan D. Gil",
        "Ehecatl Antonio Del Rio Chanona",
        "José L. Guzmán",
        "Manuel Berenguel"
      ],
      "abstract": "The inherent complexity of living cells as production units creates major\nchallenges for maintaining stable and optimal bioprocess conditions, especially\nin open Photobioreactors (PBRs) exposed to fluctuating environments. To address\nthis, we propose a Reinforcement Learning (RL) control approach, combined with\nBehavior Cloning (BC), for pH regulation in open PBR systems. This represents,\nto the best of our knowledge, the first application of an RL-based control\nstrategy to such a nonlinear and disturbance-prone bioprocess. Our method\nbegins with an offline training stage in which the RL agent learns from\ntrajectories generated by a nominal Proportional-Integral-Derivative (PID)\ncontroller, without direct interaction with the real system. This is followed\nby a daily online fine-tuning phase, enabling adaptation to evolving process\ndynamics and stronger rejection of fast, transient disturbances. This hybrid\noffline-online strategy allows deployment of an adaptive control policy capable\nof handling the inherent nonlinearities and external perturbations in open\nPBRs. Simulation studies highlight the advantages of our method: the Integral\nof Absolute Error (IAE) was reduced by 8% compared to PID control and by 5%\nrelative to standard off-policy RL. Moreover, control effort decreased\nsubstantially-by 54% compared to PID and 7% compared to standard RL-an\nimportant factor for minimizing operational costs. Finally, an 8-day\nexperimental validation under varying environmental conditions confirmed the\nrobustness and reliability of the proposed approach. Overall, this work\ndemonstrates the potential of RL-based methods for bioprocess control and paves\nthe way for their broader application to other nonlinear, disturbance-prone\nsystems.",
      "pdf_url": "http://arxiv.org/pdf/2509.06853v1",
      "published": "2025-09-08T16:21:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06853v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ]
    },
    {
      "title": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens",
      "authors": [
        "Eugene Kwek",
        "Wenpeng Yin"
      ],
      "abstract": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency.",
      "pdf_url": "http://arxiv.org/pdf/2509.06836v1",
      "published": "2025-09-08T16:07:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06836v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems",
      "authors": [
        "Chenyang Zhu",
        "Spencer Hong",
        "Jingyu Wu",
        "Kushal Chawla",
        "Charlotte Tang",
        "Youbing Yin",
        "Nathan Wolfe",
        "Erin Babinsky",
        "Daben Liu"
      ],
      "abstract": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review.",
      "pdf_url": "http://arxiv.org/pdf/2509.06822v1",
      "published": "2025-09-08T15:57:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06822v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem",
      "authors": [
        "Valentin Quesnel",
        "Damien Sileo"
      ],
      "abstract": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1",
      "pdf_url": "http://arxiv.org/pdf/2509.06809v1",
      "published": "2025-09-08T15:43:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06809v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML",
      "authors": [
        "Haoyu Dong",
        "Pengkun Zhang",
        "Mingzhe Lu",
        "Yanzhen Shen",
        "Guolin Ke"
      ],
      "abstract": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
      "pdf_url": "http://arxiv.org/pdf/2509.06806v1",
      "published": "2025-09-08T15:38:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06806v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting",
      "authors": [
        "Shashidhar Reddy Javaji",
        "Bhavul Gauri",
        "Zining Zhu"
      ],
      "abstract": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies.",
      "pdf_url": "http://arxiv.org/pdf/2509.06770v1",
      "published": "2025-09-08T14:54:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06770v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization",
      "authors": [
        "Thanh Thi Nguyen",
        "Campbell Wilson",
        "Janis Dalins"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) or multimodal large language models\nrepresent a significant advancement in artificial intelligence, enabling\nsystems to understand and generate content across both visual and textual\nmodalities. While large-scale pretraining has driven substantial progress,\nfine-tuning these models for aligning with human values or engaging in specific\ntasks or behaviors remains a critical challenge. Deep Reinforcement Learning\n(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for\nthis aligning process. While DRL enables models to optimize actions using\nreward signals instead of relying solely on supervised preference data, DPO\ndirectly aligns the policy with preferences, eliminating the need for an\nexplicit reward model. This overview explores paradigms for fine-tuning LVLMs,\nhighlighting how DRL and DPO techniques can be used to align models with human\npreferences and values, improve task performance, and enable adaptive\nmultimodal interaction. We categorize key approaches, examine sources of\npreference data, reward signals, and discuss open challenges such as\nscalability, sample efficiency, continual learning, generalization, and safety.\nThe goal is to provide a clear understanding of how DRL and DPO contribute to\nthe evolution of robust and human-aligned LVLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.06759v1",
      "published": "2025-09-08T14:47:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06759v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Long-Range Graph Wavelet Networks",
      "authors": [
        "Filippo Guerranti",
        "Fabrizio Forte",
        "Simon Geisler",
        "Stephan Günnemann"
      ],
      "abstract": "Modeling long-range interactions, the propagation of information across\ndistant parts of a graph, is a central challenge in graph machine learning.\nGraph wavelets, inspired by multi-resolution signal processing, provide a\nprincipled way to capture both local and global structures. However, existing\nwavelet-based graph neural networks rely on finite-order polynomial\napproximations, which limit their receptive fields and hinder long-range\npropagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which\ndecompose wavelet filters into complementary local and global components. Local\naggregation is handled with efficient low-order polynomials, while long-range\ninteractions are captured through a flexible spectral domain parameterization.\nThis hybrid design unifies short- and long-distance information flow within a\nprincipled wavelet framework. Experiments show that LR-GWN achieves\nstate-of-the-art performance among wavelet-based methods on long-range\nbenchmarks, while remaining competitive on short-range datasets.",
      "pdf_url": "http://arxiv.org/pdf/2509.06743v1",
      "published": "2025-09-08T14:35:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06743v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction",
      "authors": [
        "Jie Yang",
        "Jiajun Chen",
        "Zhangyue Yin",
        "Shuo Chen",
        "Yuxin Wang",
        "Yiran Guo",
        "Yuan Li",
        "Yining Zheng",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "abstract": "Intelligent vehicle cockpits present unique challenges for API Agents,\nrequiring coordination across tightly-coupled subsystems that exceed typical\ntask environments' complexity. Traditional Function Calling (FC) approaches\noperate statelessly, requiring multiple exploratory calls to build\nenvironmental awareness before execution, leading to inefficiency and limited\nerror recovery. We introduce VehicleWorld, the first comprehensive environment\nfor the automotive domain, featuring 30 modules, 250 APIs, and 680 properties\nwith fully executable implementations that provide real-time state information\nduring agent execution. This environment enables precise evaluation of vehicle\nagent behaviors across diverse, challenging scenarios. Through systematic\nanalysis, we discovered that direct state prediction outperforms function\ncalling for environmental control. Building on this insight, we propose\nState-based Function Call (SFC), a novel approach that maintains explicit\nsystem state awareness and implements direct state transitions to achieve\ntarget conditions. Experimental results demonstrate that SFC significantly\noutperforms traditional FC approaches, achieving superior execution accuracy\nand reduced latency. We have made all implementation code publicly available on\nGithub https://github.com/OpenMOSS/VehicleWorld.",
      "pdf_url": "http://arxiv.org/pdf/2509.06736v1",
      "published": "2025-09-08T14:28:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06736v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ]
    },
    {
      "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
      "authors": [
        "Wenjun Li",
        "Zhi Chen",
        "Jingru Lin",
        "Hannan Cao",
        "Wei Han",
        "Sheng Liang",
        "Zhi Zhang",
        "Kuicai Dong",
        "Dexun Li",
        "Chen Zhang",
        "Yong Liu"
      ],
      "abstract": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.",
      "pdf_url": "http://arxiv.org/pdf/2509.06733v1",
      "published": "2025-09-08T14:27:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06733v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture",
      "authors": [
        "Mustafa Yurdakul",
        "Şakir Taşdemir"
      ],
      "abstract": "Brain tumors are serious health problems that require early diagnosis due to\ntheir high mortality rates. Diagnosing tumors by examining Magnetic Resonance\nImaging (MRI) images is a process that requires expertise and is prone to\nerror. Therefore, the need for automated diagnosis systems is increasing day by\nday. In this context, a robust and explainable Deep Learning (DL) model for the\nclassification of brain tumors is proposed. In this study, a publicly available\nFigshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI\nimages of three tumor types was used. First, the classification performance of\nnine well-known CNN architectures was evaluated to determine the most effective\nbackbone. Among these, EfficientNetV2 demonstrated the best performance and was\nselected as the backbone for further development. Subsequently, an\nattention-based MLP-Mixer architecture was integrated into EfficientNetV2 to\nenhance its classification capability. The performance of the final model was\ncomprehensively compared with basic CNNs and the methods in the literature.\nAdditionally, Grad-CAM visualization was used to interpret and validate the\ndecision-making process of the proposed model. The proposed model's performance\nwas evaluated using the five-fold cross-validation method. The proposed model\ndemonstrated superior performance with 99.50% accuracy, 99.47% precision,\n99.52% recall and 99.49% F1 score. The results obtained show that the model\noutperforms the studies in the literature. Moreover, Grad-CAM visualizations\ndemonstrate that the model effectively focuses on relevant regions of MRI\nimages, thus improving interpretability and clinical reliability. A robust deep\nlearning model for clinical decision support systems has been obtained by\ncombining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy\nand interpretability in brain tumor classification.",
      "pdf_url": "http://arxiv.org/pdf/2509.06713v1",
      "published": "2025-09-08T14:08:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06713v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks",
      "authors": [
        "Su Hyeong Lee",
        "Risi Kondor",
        "Richard Ngo"
      ],
      "abstract": "We develop a theory of intelligent agency grounded in probabilistic modeling\nfor neural models. Agents are represented as outcome distributions with\nepistemic utility given by log score, and compositions are defined through\nweighted logarithmic pooling that strictly improves every member's welfare. We\nprove that strict unanimity is impossible under linear pooling or in binary\noutcome spaces, but possible with three or more outcomes. Our framework admits\nrecursive structure via cloning invariance, continuity, and openness, while\ntilt-based analysis rules out trivial duplication. Finally, we formalize an\nagentic alignment phenomenon in LLMs using our theory: eliciting a benevolent\npersona (\"Luigi'\") induces an antagonistic counterpart (\"Waluigi\"), while a\nmanifest-then-suppress Waluigi strategy yields strictly larger first-order\nmisalignment reduction than pure Luigi reinforcement alone. These results\nclarify how developing a principled mathematical framework for how subagents\ncan coalesce into coherent higher-level entities provides novel implications\nfor alignment in agentic AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.06701v1",
      "published": "2025-09-08T13:55:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06701v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation",
      "authors": [
        "Victor Toscano-Duran",
        "Rocio Gonzalez-Diaz",
        "Miguel A. Gutiérrez-Naranjo"
      ],
      "abstract": "While it is well-established that artificial neural networks are universal\napproximators for continuous functions on compact domains, many modern\napproaches rely on deep or overparameterized architectures that incur high\ncomputational costs. In this paper, a new type of small shallow neural network,\ncalled the Barycentric Neural Network (BNN), is proposed, which leverages a\nfixed set of base points and their barycentric coordinates to define both its\nstructure and its parameters. We demonstrate that our BNN enables the exact\nrepresentation of continuous piecewise linear functions (CPLFs), ensuring\nstrict continuity across segments. Since any continuous function over a compact\ndomain can be approximated arbitrarily well by CPLFs, the BNN naturally emerges\nas a flexible and interpretable tool for function approximation. Beyond the use\nof this representation, the main contribution of the paper is the introduction\nof a new variant of persistent entropy, a topological feature that is stable\nand scale invariant, called the length-weighted persistent entropy (LWPE),\nwhich is weighted by the lifetime of topological features. Our framework, which\ncombines the BNN with a loss function based on our LWPE, aims to provide\nflexible and geometrically interpretable approximations of nonlinear continuous\nfunctions in resource-constrained settings, such as those with limited base\npoints for BNN design and few training epochs. Instead of optimizing internal\nweights, our approach directly optimizes the base points that define the BNN.\nExperimental results show that our approach achieves superior and faster\napproximation performance compared to classical loss functions such as MSE,\nRMSE, MAE, and log-cosh.",
      "pdf_url": "http://arxiv.org/pdf/2509.06694v2",
      "published": "2025-09-08T13:47:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06694v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring",
      "authors": [
        "Usman Haider",
        "Lukasz Szemet",
        "Daniel Kelly",
        "Vasileios Sergis",
        "Andrew C. Daly",
        "Karl Mason"
      ],
      "abstract": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.06690v1",
      "published": "2025-09-08T13:44:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06690v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.AR",
        "N/A",
        "I.2.9; I.2.10; I.4.6"
      ]
    },
    {
      "title": "TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations",
      "authors": [
        "Xiaolu Fu",
        "Ziyuan Bao",
        "Eiman Kanjo"
      ],
      "abstract": "Vehicular ad hoc networks (VANETs) are a crucial component of intelligent\ntransportation systems; however, routing remains challenging due to dynamic\ntopologies, incomplete observations, and the limited resources of edge devices.\nExisting reinforcement learning (RL) approaches often assume fixed graph\nstructures and require retraining when network conditions change, making them\nunsuitable for deployment on constrained hardware. We present TrajAware, an\nRL-based framework designed for edge AI deployment in VANETs. TrajAware\nintegrates three components: (i) action space pruning, which reduces redundant\nneighbour options while preserving two-hop reachability, alleviating the curse\nof dimensionality; (ii) graph cross-attention, which maps pruned neighbours to\nthe global graph context, producing features that generalise across diverse\nnetwork sizes; and (iii) trajectory-aware prediction, which uses historical\nroutes and junction information to estimate real-time positions under partial\nobservations. We evaluate TrajAware in the open-source SUMO simulator using\nreal-world city maps with a leave-one-city-out setup. Results show that\nTrajAware achieves near-shortest paths and high delivery ratios while\nmaintaining efficiency suitable for constrained edge devices, outperforming\nstate-of-the-art baselines in both full and partial observation scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2509.06665v1",
      "published": "2025-09-08T13:24:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06665v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AnalysisGNN: Unified Music Analysis with Graph Neural Networks",
      "authors": [
        "Emmanouil Karystinaios",
        "Johannes Hentschel",
        "Markus Neuwirth",
        "Gerhard Widmer"
      ],
      "abstract": "Recent years have seen a boom in computational approaches to music analysis,\nyet each one is typically tailored to a specific analytical domain. In this\nwork, we introduce AnalysisGNN, a novel graph neural network framework that\nleverages a data-shuffling strategy with a custom weighted multi-task loss and\nlogit fusion between task-specific classifiers to integrate heterogeneously\nannotated symbolic datasets for comprehensive score analysis. We further\nintegrate a Non-Chord-Tone prediction module, which identifies and excludes\npassing and non-functional notes from all tasks, thereby improving the\nconsistency of label signals. Experimental evaluations demonstrate that\nAnalysisGNN achieves performance comparable to traditional static-dataset\napproaches, while showing increased resilience to domain shifts and annotation\ninconsistencies across multiple heterogeneous corpora.",
      "pdf_url": "http://arxiv.org/pdf/2509.06654v1",
      "published": "2025-09-08T13:11:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06654v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning",
      "authors": [
        "Zhou-Peng Shou",
        "Zhi-Qiang You",
        "Fang Wang",
        "Hai-Bo Liu"
      ],
      "abstract": "Targeting the issues of \"shortcuts\" and insufficient contextual understanding\nin complex cross-modal reasoning of multimodal large models, this paper\nproposes a zero-shot multimodal reasoning component guided by human-like\ncognitive strategies centered on an \"intent sketch\". The component comprises a\nplug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and\nStrategy Selector-that explicitly constructs a \"understand-plan-select\"\ncognitive process. By generating and filtering \"intent sketch\" strategies to\nguide the final reasoning, it requires no parameter fine-tuning and achieves\ncross-model transfer solely through in-context engineering.\nInformation-theoretic analysis shows that this process can reduce conditional\nentropy and improve information utilization efficiency, thereby suppressing\nunintended shortcut reasoning. Experiments on IntentBench, WorldSense, and\nDaily-Omni validate the method's generality and robust gains; compared with\ntheir respective baselines, the complete \"three-module\" scheme yields\nconsistent improvements across different reasoning engines and pipeline\ncombinations, with gains up to approximately 9.51 percentage points,\ndemonstrating the practical value and portability of the \"intent sketch\"\nreasoning component in zero-shot scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2509.06641v1",
      "published": "2025-09-08T12:57:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06641v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The First Voice Timbre Attribute Detection Challenge",
      "authors": [
        "Liping Chen",
        "Jinghao He",
        "Zhengyan Sheng",
        "Kong Aik Lee",
        "Zhen-Hua Ling"
      ],
      "abstract": "The first voice timbre attribute detection challenge is featured in a special\nsession at NCMMSC 2025. It focuses on the explainability of voice timbre and\ncompares the intensity of two speech utterances in a specified timbre\ndescriptor dimension. The evaluation was conducted on the VCTK-RVA dataset.\nParticipants developed their systems and submitted their outputs to the\norganizer, who evaluated the performance and sent feedback to them. Six teams\nsubmitted their outputs, with five providing descriptions of their\nmethodologies.",
      "pdf_url": "http://arxiv.org/pdf/2509.06635v1",
      "published": "2025-09-08T12:54:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06635v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework",
      "authors": [
        "Aswini Kumar Patra"
      ],
      "abstract": "Plants in their natural habitats endure an array of interacting stresses,\nboth biotic and abiotic, that rarely occur in isolation. Nutrient\nstress-particularly nitrogen deficiency-becomes even more critical when\ncompounded with drought and weed competition, making it increasingly difficult\nto distinguish and address its effects. Early detection of nitrogen stress is\ntherefore crucial for protecting plant health and implementing effective\nmanagement strategies. This study proposes a novel deep learning framework to\naccurately classify nitrogen stress severity in a combined stress environment.\nOur model uses a unique blend of four imaging modalities-RGB, multispectral,\nand two infrared wavelengths-to capture a wide range of physiological plant\nresponses from canopy images. These images, provided as time-series data,\ndocument plant health across three levels of nitrogen availability (low,\nmedium, and high) under varying water stress and weed pressures. The core of\nour approach is a spatio-temporal deep learning pipeline that merges a\nConvolutional Neural Network (CNN) for extracting spatial features from images\nwith a Long Short-Term Memory (LSTM) network to capture temporal dependencies.\nWe also devised and evaluated a spatial-only CNN pipeline for comparison. Our\nCNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively\nsurpassing the spatial-only model's 80.45% and other previously reported\nmachine learning method's 76%. These results bring actionable insights based on\nthe power of our CNN-LSTM approach in effectively capturing the subtle and\ncomplex interactions between nitrogen deficiency, water stress, and weed\npressure. This robust platform offers a promising tool for the timely and\nproactive identification of nitrogen stress severity, enabling better crop\nmanagement and improved plant health.",
      "pdf_url": "http://arxiv.org/pdf/2509.06625v1",
      "published": "2025-09-08T12:41:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06625v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "BEAM: Brainwave Empathy Assessment Model for Early Childhood",
      "authors": [
        "Chen Xie",
        "Gaofeng Wu",
        "Kaidong Wang",
        "Zihao Zhu",
        "Xiaoshu Luo",
        "Yan Liang",
        "Feiyu Quan",
        "Ruoxi Wu",
        "Xianghui Huang",
        "Han Zhang"
      ],
      "abstract": "Empathy in young children is crucial for their social and emotional\ndevelopment, yet predicting it remains challenging. Traditional methods often\nonly rely on self-reports or observer-based labeling, which are susceptible to\nbias and fail to objectively capture the process of empathy formation. EEG\noffers an objective alternative; however, current approaches primarily extract\nstatic patterns, neglecting temporal dynamics. To overcome these limitations,\nwe propose a novel deep learning framework, the Brainwave Empathy Assessment\nModel (BEAM), to predict empathy levels in children aged 4-6 years. BEAM\nleverages multi-view EEG signals to capture both cognitive and emotional\ndimensions of empathy. The framework comprises three key components: 1) a\nLaBraM-based encoder for effective spatio-temporal feature extraction, 2) a\nfeature fusion module to integrate complementary information from multi-view\nsignals, and 3) a contrastive learning module to enhance class separation.\nValidated on the CBCP dataset, BEAM outperforms state-of-the-art methods across\nmultiple metrics, demonstrating its potential for objective empathy assessment\nand providing a preliminary insight into early interventions in children's\nprosocial development.",
      "pdf_url": "http://arxiv.org/pdf/2509.06620v1",
      "published": "2025-09-08T12:39:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06620v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards",
      "authors": [
        "Noel Codella",
        "Sam Preston",
        "Hao Qiu",
        "Leonardo Schettini",
        "Wen-wai Yim",
        "Mert Öz",
        "Shrey Jain",
        "Matthew P. Lungren",
        "Thomas Osborne"
      ],
      "abstract": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs.",
      "pdf_url": "http://arxiv.org/pdf/2509.06602v1",
      "published": "2025-09-08T12:15:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06602v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos",
      "authors": [
        "Davide Berghi",
        "Philip J. B. Jackson"
      ],
      "abstract": "In this study, we address the multimodal task of stereo sound event\nlocalization and detection with source distance estimation (3D SELD) in regular\nvideo content. 3D SELD is a complex task that combines temporal event\nclassification with spatial localization, requiring reasoning across spatial,\ntemporal, and semantic dimensions. The last is arguably the most challenging to\nmodel. Traditional SELD approaches typically rely on multichannel input,\nlimiting their capacity to benefit from large-scale pre-training due to data\nconstraints. To overcome this, we enhance a standard SELD architecture with\nsemantic information by integrating pre-trained, contrastive language-aligned\nmodels: CLAP for audio and OWL-ViT for visual inputs. These embeddings are\nincorporated into a modified Conformer module tailored for multimodal fusion,\nwhich we refer to as the Cross-Modal Conformer. We perform an ablation study on\nthe development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the\nindividual contributions of the language-aligned models and benchmark against\nthe DCASE Task 3 baseline systems. Additionally, we detail the curation process\nof large synthetic audio and audio-visual datasets used for model pre-training.\nThese datasets were further expanded through left-right channel swapping\naugmentation. Our approach, combining extensive pre-training, model ensembling,\nand visual post-processing, achieved second rank in the DCASE 2025 Challenge\nTask 3 (Track B), underscoring the effectiveness of our method. Future work\nwill explore the modality-specific contributions and architectural refinements.",
      "pdf_url": "http://arxiv.org/pdf/2509.06598v1",
      "published": "2025-09-08T12:07:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06598v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "eess.IV",
        "eess.SP"
      ]
    },
    {
      "title": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models",
      "authors": [
        "Xin Tong",
        "Zhi Lin",
        "Jingya Wang",
        "Bo Jin"
      ],
      "abstract": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings.",
      "pdf_url": "http://arxiv.org/pdf/2509.06596v1",
      "published": "2025-09-08T12:06:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06596v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Integrated Detection and Tracking Based on Radar Range-Doppler Feature",
      "authors": [
        "Chenyu Zhang",
        "Yuanhang Wu",
        "Xiaoxi Ma",
        "Wei Yi"
      ],
      "abstract": "Detection and tracking are the basic tasks of radar systems. Current joint\ndetection tracking methods, which focus on dynamically adjusting detection\nthresholds from tracking results, still present challenges in fully utilizing\nthe potential of radar signals. These are mainly reflected in the limited\ncapacity of the constant false-alarm rate model to accurately represent\ninformation, the insufficient depiction of complex scenes, and the limited\ninformation acquired by the tracker. We introduce the Integrated Detection and\nTracking based on radar feature (InDT) method, which comprises a network\narchitecture for radar signal detection and a tracker that leverages detection\nassistance. The InDT detector extracts feature information from each\nRange-Doppler (RD) matrix and then returns the target position through the\nfeature enhancement module and the detection head. The InDT tracker adaptively\nupdates the measurement noise covariance of the Kalman filter based on\ndetection confidence. The similarity of target RD features is measured by\ncosine distance, which enhances the data association process by combining\nlocation and feature information. Finally, the efficacy of the proposed method\nwas validated through testing on both simulated data and publicly available\ndatasets.",
      "pdf_url": "http://arxiv.org/pdf/2509.06569v1",
      "published": "2025-09-08T11:32:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06569v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs",
      "authors": [
        "Jack Wilkie",
        "Hanan Hindy",
        "Christos Tachtatzis",
        "Robert Atkinson"
      ],
      "abstract": "Network intrusion detection remains a critical challenge in cybersecurity.\nWhile supervised machine learning models achieve state-of-the-art performance,\ntheir reliance on large labelled datasets makes them impractical for many\nreal-world applications. Anomaly detection methods, which train exclusively on\nbenign traffic to identify malicious activity, suffer from high false positive\nrates, limiting their usability. Recently, self-supervised learning techniques\nhave demonstrated improved performance with lower false positive rates by\nlearning discriminative latent representations of benign traffic. In\nparticular, contrastive self-supervised models achieve this by minimizing the\ndistance between similar (positive) views of benign traffic while maximizing it\nbetween dissimilar (negative) views. Existing approaches generate positive\nviews through data augmentation and treat other samples as negative. In\ncontrast, this work introduces Contrastive Learning using Augmented Negative\npairs (CLAN), a novel paradigm for network intrusion detection where augmented\nsamples are treated as negative views - representing potentially malicious\ndistributions - while other benign samples serve as positive views. This\napproach enhances both classification accuracy and inference efficiency after\npretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset\ndemonstrates that the proposed method surpasses existing self-supervised and\nanomaly detection techniques in a binary classification task. Furthermore, when\nfine-tuned on a limited labelled dataset, the proposed approach achieves\nsuperior multi-class classification performance compared to existing\nself-supervised models.",
      "pdf_url": "http://arxiv.org/pdf/2509.06550v1",
      "published": "2025-09-08T11:04:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06550v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.NI",
        "I.2.6; K.6.5"
      ]
    },
    {
      "title": "Signal-Based Malware Classification Using 1D CNNs",
      "authors": [
        "Jack Wilkie",
        "Hanan Hindy",
        "Ivan Andonovic",
        "Christos Tachtatzis",
        "Robert Atkinson"
      ],
      "abstract": "Malware classification is a contemporary and ongoing challenge in\ncyber-security: modern obfuscation techniques are able to evade traditional\nstatic analysis, while dynamic analysis is too resource intensive to be\ndeployed at a large scale. One prominent line of research addresses these\nlimitations by converting malware binaries into 2D images by heuristically\nreshaping them into a 2D grid before resizing using Lanczos resampling. These\nimages can then be classified based on their textural information using\ncomputer vision approaches. While this approach can detect obfuscated malware\nmore effectively than static analysis, the process of converting files into 2D\nimages results in significant information loss due to both quantisation noise,\ncaused by rounding to integer pixel values, and the introduction of 2D\ndependencies which do not exist in the original data. This loss of signal\nlimits the classification performance of the downstream model. This work\naddresses these weaknesses by instead resizing the files into 1D signals which\navoids the need for heuristic reshaping, and additionally these signals do not\nsuffer from quantisation noise due to being stored in a floating-point format.\nIt is shown that existing 2D CNN architectures can be readily adapted to\nclassify these 1D signals for improved performance. Furthermore, a bespoke 1D\nconvolutional neural network, based on the ResNet architecture and\nsqueeze-and-excitation layers, was developed to classify these signals and\nevaluated on the MalNet dataset. It was found to achieve state-of-the-art\nperformance on binary, type, and family level classification with F1 scores of\n0.874, 0.503, and 0.507, respectively, paving the way for future models to\noperate on the proposed signal modality.",
      "pdf_url": "http://arxiv.org/pdf/2509.06548v2",
      "published": "2025-09-08T11:03:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06548v2",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "I.2.6; K.6.5"
      ]
    },
    {
      "title": "Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model",
      "authors": [
        "Duc Huy Le",
        "Rolf Stadler"
      ],
      "abstract": "CAGE-2 is an accepted benchmark for learning and evaluating defender\nstrategies against cyberattacks. It reflects a scenario where a defender agent\nprotects an IT infrastructure against various attacks. Many defender methods\nfor CAGE-2 have been proposed in the literature. In this paper, we construct a\nformal model for CAGE-2 using the framework of Partially Observable Markov\nDecision Process (POMDP). Based on this model, we define an optimal defender\nstrategy for CAGE-2 and introduce a method to efficiently learn this strategy.\nOur method, called BF-PPO, is based on PPO, and it uses particle filter to\nmitigate the computational complexity due to the large state space of the\nCAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and\ncompare its performance with that of CARDIFF, the highest ranked method on the\nCAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the\nlearned defender strategy and the required training time.",
      "pdf_url": "http://arxiv.org/pdf/2509.06539v1",
      "published": "2025-09-08T10:51:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06539v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "On the Reproducibility of \"FairCLIP: Harnessing Fairness in Vision-Language Learning''",
      "authors": [
        "Hua Chang Bakker",
        "Stan Fris",
        "Angela Madelon Bernardy",
        "Stan Deutekom"
      ],
      "abstract": "We investigated the reproducibility of FairCLIP, proposed by Luo et al.\n(2024), for improving the group fairness of CLIP (Radford et al., 2021) by\nminimizing image-text similarity score disparities across sensitive groups\nusing the Sinkhorn distance. The experimental setup of Luo et al. (2024) was\nreproduced to primarily investigate the research findings for FairCLIP. The\nmodel description by Luo et al. (2024) was found to differ from the original\nimplementation. Therefore, a new implementation, A-FairCLIP, is introduced to\nexamine specific design choices. Furthermore, FairCLIP+ is proposed to extend\nthe FairCLIP objective to include multiple attributes. Additionally, the impact\nof the distance minimization on FairCLIP's fairness and performance was\nexplored. In alignment with the original authors, CLIP was found to be biased\ntowards certain demographics when applied to zero-shot glaucoma classification\nusing medical scans and clinical notes from the Harvard-FairVLMed dataset.\nHowever, the experimental results on two datasets do not support their claim\nthat FairCLIP improves the performance and fairness of CLIP. Although the\nregularization objective reduces Sinkhorn distances, both the official\nimplementation and the aligned implementation, A-FairCLIP, were not found to\nimprove performance nor fairness in zero-shot glaucoma classification.",
      "pdf_url": "http://arxiv.org/pdf/2509.06535v1",
      "published": "2025-09-08T10:41:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06535v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion",
      "authors": [
        "Mengxue Yang",
        "Chun Yang",
        "Jiaqi Zhu",
        "Jiafan Li",
        "Jingqi Zhang",
        "Yuyang Li",
        "Ying Li"
      ],
      "abstract": "Link prediction in knowledge graphs requires integrating structural\ninformation and semantic context to infer missing entities. While large\nlanguage models offer strong generative reasoning capabilities, their limited\nexploitation of structural signals often results in structural sparsity and\nsemantic ambiguity, especially under incomplete or zero-shot settings. To\naddress these challenges, we propose SLiNT (Structure-aware Language model with\nInjection and coNtrastive Training), a modular framework that injects\nknowledge-graph-derived structural context into a frozen LLM backbone with\nlightweight LoRA-based adaptation for robust link prediction. Specifically,\nStructure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to\nenrich sparse entities and mitigate missing context; Dynamic Hard Contrastive\nLearning (DHCL) introduces fine-grained supervision by interpolating hard\npositives and negatives to resolve entity-level ambiguity; and\nGradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware\nintervention while preserving the core LLM parameters. Experiments on WN18RR\nand FB15k-237 show that SLiNT achieves superior or competitive performance\ncompared with both embedding-based and generation-based baselines,\ndemonstrating the effectiveness of structure-aware representation learning for\nscalable knowledge graph completion.",
      "pdf_url": "http://arxiv.org/pdf/2509.06531v1",
      "published": "2025-09-08T10:36:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06531v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training",
      "authors": [
        "Andrei Baroian",
        "Kasper Notebomer"
      ],
      "abstract": "Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential.",
      "pdf_url": "http://arxiv.org/pdf/2509.06518v1",
      "published": "2025-09-08T10:24:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06518v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients",
      "authors": [
        "Zongheng Guo",
        "Tao Chen",
        "Manuela Ferrario"
      ],
      "abstract": "Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in\nintesive care unit (ICU) and operating room (OR). However, the high incidence\nof poor, incomplete, and inconsistent signal quality, can lead to false alarms\nor diagnostic inaccuracies. The methods explored so far suffer from limited\ngeneralizability, reliance on extensive labeled data, and poor cross-task\ntransferability. To overcome these challenges, we introduce QualityFM, a novel\nmultimodal foundation model for these physiological signals, designed to\nacquire a general-purpose understanding of signal quality. Our model is\npre-trained on an large-scale dataset comprising over 21 million 30-second\nwaveforms and 179,757 hours of data. Our approach involves a dual-track\narchitecture that processes paired physiological signals of differing quality,\nleveraging a self-distillation strategy where an encoder for high-quality\nsignals is used to guide the training of an encoder for low-quality signals. To\nefficiently handle long sequential signals and capture essential local\nquasi-periodic patterns, we integrate a windowed sparse attention mechanism\nwithin our Transformer-based model. Furthermore, a composite loss function,\nwhich combines direct distillation loss on encoder outputs with indirect\nreconstruction loss based on power and phase spectra, ensures the preservation\nof frequency-domain characteristics of the signals. We pre-train three models\nwith varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy\nand practical value through transfer learning on three distinct clinical tasks:\nfalse alarm of ventricular tachycardia detection, the identification of atrial\nfibrillation and the estimation of arterial blood pressure (ABP) from PPG and\nECG signals.",
      "pdf_url": "http://arxiv.org/pdf/2509.06516v1",
      "published": "2025-09-08T10:20:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06516v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "J.3"
      ]
    },
    {
      "title": "An AI system to help scientists write expert-level empirical software",
      "authors": [
        "Eser Aygün",
        "Anastasiya Belyaeva",
        "Gheorghe Comanici",
        "Marc Coram",
        "Hao Cui",
        "Jake Garrison",
        "Renee Johnston Anton Kast",
        "Cory Y. McLean",
        "Peter Norgaard",
        "Zahra Shamsi",
        "David Smalling",
        "James Thompson",
        "Subhashini Venugopalan",
        "Brian P. Williams",
        "Chujun He",
        "Sarah Martinson",
        "Martyna Plomecka",
        "Lai Wei",
        "Yuchen Zhou",
        "Qian-Ze Zhu",
        "Matthew Abraham",
        "Erica Brand",
        "Anna Bulanova",
        "Jeffrey A. Cardille",
        "Chris Co",
        "Scott Ellsworth",
        "Grace Joseph",
        "Malcolm Kane",
        "Ryan Krueger",
        "Johan Kartiwa",
        "Dan Liebling",
        "Jan-Matthis Lueckmann",
        "Paul Raccuglia",
        "Xuefei",
        "Wang",
        "Katherine Chou",
        "James Manyika",
        "Yossi Matias",
        "John C. Platt",
        "Lizzie Dorfman",
        "Shibl Mourad",
        "Michael P. Brenner"
      ],
      "abstract": "The cycle of scientific discovery is frequently bottlenecked by the slow,\nmanual creation of software to support computational experiments. To address\nthis, we present an AI system that creates expert-level scientific software\nwhose goal is to maximize a quality metric. The system uses a Large Language\nModel (LLM) and Tree Search (TS) to systematically improve the quality metric\nand intelligently navigate the large space of possible solutions. The system\nachieves expert-level results when it explores and integrates complex research\nideas from external sources. The effectiveness of tree search is demonstrated\nacross a wide range of benchmarks. In bioinformatics, it discovered 40 novel\nmethods for single-cell data analysis that outperformed the top human-developed\nmethods on a public leaderboard. In epidemiology, it generated 14 models that\noutperformed the CDC ensemble and all other individual models for forecasting\nCOVID-19 hospitalizations. Our method also produced state-of-the-art software\nfor geospatial analysis, neural activity prediction in zebrafish, time series\nforecasting and numerical solution of integrals. By devising and implementing\nnovel solutions to diverse tasks, the system represents a significant step\ntowards accelerating scientific progress.",
      "pdf_url": "http://arxiv.org/pdf/2509.06503v1",
      "published": "2025-09-08T10:08:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06503v1",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ]
    },
    {
      "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers",
      "authors": [
        "Ran Xin",
        "Zeyu Zheng",
        "Yanchen Nie",
        "Kun Yuan",
        "Xia Xiao"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
      "pdf_url": "http://arxiv.org/pdf/2509.06493v1",
      "published": "2025-09-08T09:54:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06493v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization",
      "authors": [
        "Niki Kotecha",
        "Ehecatl Antonio del Rio Chanona"
      ],
      "abstract": "In supply chain management, decision-making often involves balancing multiple\nconflicting objectives, such as cost reduction, service level improvement, and\nenvironmental sustainability. Traditional multi-objective optimization methods,\nsuch as linear programming and evolutionary algorithms, struggle to adapt in\nreal-time to the dynamic nature of supply chains. In this paper, we propose an\napproach that combines Reinforcement Learning (RL) and Multi-Objective\nEvolutionary Algorithms (MOEAs) to address these challenges for dynamic\nmulti-objective optimization under uncertainty. Our method leverages MOEAs to\nsearch the parameter space of policy neural networks, generating a Pareto front\nof policies. This provides decision-makers with a diverse population of\npolicies that can be dynamically switched based on the current system\nobjectives, ensuring flexibility and adaptability in real-time decision-making.\nWe also introduce Conditional Value-at-Risk (CVaR) to incorporate\nrisk-sensitive decision-making, enhancing resilience in uncertain environments.\nWe demonstrate the effectiveness of our approach through case studies,\nshowcasing its ability to respond to supply chain dynamics and outperforming\nstate-of-the-art methods in an inventory management case study. The proposed\nstrategy not only improves decision-making efficiency but also offers a more\nrobust framework for managing uncertainty and optimizing performance in supply\nchains.",
      "pdf_url": "http://arxiv.org/pdf/2509.06490v1",
      "published": "2025-09-08T09:51:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.06490v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}