{
  "last_updated": "2025-10-21T00:51:02.028357",
  "papers": [
    {
      "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM",
      "authors": [
        "Hanrong Ye",
        "Chao-Han Huck Yang",
        "Arushi Goel",
        "Wei Huang",
        "Ligeng Zhu",
        "Yuanhang Su",
        "Sean Lin",
        "An-Chieh Cheng",
        "Zhen Wan",
        "Jinchuan Tian",
        "Yuming Lou",
        "Dong Yang",
        "Zhijian Liu",
        "Yukang Chen",
        "Ambrish Dantrey",
        "Ehsan Jahangiri",
        "Sreyan Ghosh",
        "Daguang Xu",
        "Ehsan Hosseini-Asl",
        "Danial Mohseni Taheri",
        "Vidya Murali",
        "Sifei Liu",
        "Jason Lu",
        "Oluwatobi Olabiyi",
        "Frank Wang",
        "Rafael Valle",
        "Bryan Catanzaro",
        "Andrew Tao",
        "Song Han",
        "Jan Kautz",
        "Hongxu Yin",
        "Pavlo Molchanov"
      ],
      "abstract": "Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.",
      "pdf_url": "http://arxiv.org/pdf/2510.15870v1",
      "published": "2025-10-17T17:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15870v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction",
      "authors": [
        "Simon Yu",
        "Gang Li",
        "Weiyan Shi",
        "Peng Qi"
      ],
      "abstract": "Large language models (LLMs) are moving beyond static uses and are now\npowering agents that learn continually during their interaction with external\nenvironments. For example, agents can learn reusable skills while navigating\nweb pages or toggling new tools. However, existing methods for skill learning\noften create skills that are over-specialized to a single website and fail to\ngeneralize. We introduce PolySkill, a new framework that enables agents to\nlearn generalizable and compositional skills. The core idea, inspired by\npolymorphism in software engineering, is to decouple a skill's abstract goal\n(what it accomplishes) and its concrete implementation (how it is executed).\nExperiments show that our method (1) improves skill reuse by 1.7x on seen\nwebsites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on\nunseen websites, while reducing steps by over 20%. (3) In self-exploration\nsettings without specified tasks, our framework improves the quality of\nproposed tasks and enables agents to learn generalizable skills that work\nacross different sites. By enabling the agent to identify and refine its own\ngoals, the PolySkill enhances the agent's ability to learn a better curriculum,\nleading to the acquisition of more generalizable skills compared to baseline\nmethods. This work provides a practical path toward building agents capable of\ncontinual learning in adaptive environments. Our findings show that separating\na skill's goal from its execution is a crucial step toward developing\nautonomous agents that can learn and generalize across the open web\ncontinuously.",
      "pdf_url": "http://arxiv.org/pdf/2510.15863v1",
      "published": "2025-10-17T17:56:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15863v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold",
      "authors": [
        "Yi Wan",
        "Jiuqi Wang",
        "Liam Li",
        "Jinsong Liu",
        "Ruihao Zhu",
        "Zheqing Zhu"
      ],
      "abstract": "Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under MIT license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS.",
      "pdf_url": "http://arxiv.org/pdf/2510.15862v1",
      "published": "2025-10-17T17:53:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15862v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training",
      "authors": [
        "Pengkai Wang",
        "Qi Zuo",
        "Pengwei Liu",
        "Zhijie Sang",
        "Congkai Xie",
        "Hongxia Yang"
      ],
      "abstract": "Large Language Models (LLMs) have shown substantial advances through\nreinforcement learning (RL), particularly in domains where rewards can be\nprogrammatically verified, such as mathematics and code. In these areas, models\nbenefit from a well-defined operational base guided by explicit rule-based\nobjectives. However, this progress reveals a significant limitation: in\nopen-ended domains where rewards are ambiguous, subjective, or\ncontext-dependent, such as creative writing, scientific reasoning, and notably\nmedical consultation, robust reward functions are lacking, making these areas\nchallenging for current RL strategies. To bridge this gap, we introduce ORBIT,\nan open-ended rubric-based incremental training framework specifically designed\nfor high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue\ngeneration with the dynamic creation of rubrics, employing these rubrics to\ndirect an incremental RL process. In particular, this approach does not depend\non external medical knowledge or manual rules, instead utilizing rubric-guided\nfeedback to shape learning. When implemented on the Qwen3-4B-Instruct model,\nour method can greatly enhance its performance on the HealthBench-Hard\nbenchmark from 7.0 to 27.2 using only 2k samples, thus achieving\nstate-of-the-art results for models of this scale. Our analysis confirms that\nrubric-driven RL fos-ters consistent performance gains across diverse\nconsultation scenarios, going beyond simple numerical improvements. These\nfindings underscore rubric-based feedback as a scalable strategy for advancing\nLLMs in intricate, open-ended tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.15859v1",
      "published": "2025-10-17T17:51:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15859v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Self-Certifying Primal-Dual Optimization Proxies for Large-Scale Batch Economic Dispatch",
      "authors": [
        "Michael Klamkin",
        "Mathieu Tanneau",
        "Pascal Van Hentenryck"
      ],
      "abstract": "Recent research has shown that optimization proxies can be trained to high\nfidelity, achieving average optimality gaps under 1% for large-scale problems.\nHowever, worst-case analyses show that there exist in-distribution queries that\nresult in orders of magnitude higher optimality gap, making it difficult to\ntrust the predictions in practice. This paper aims at striking a balance\nbetween classical solvers and optimization proxies in order to enable\ntrustworthy deployments with interpretable speed-optimality tradeoffs based on\na user-defined optimality threshold. To this end, the paper proposes a hybrid\nsolver that leverages duality theory to efficiently bound the optimality gap of\npredictions, falling back to a classical solver for queries where optimality\ncannot be certified. To improve the achieved speedup of the hybrid solver, the\npaper proposes an alternative training procedure that combines the primal and\ndual proxy training. Experiments on large-scale transmission systems show that\nthe hybrid solver is highly scalable. The proposed hybrid solver achieves\nspeedups of over 1000x compared to a parallelized simplex-based solver while\nguaranteeing a maximum optimality gap of 2%.",
      "pdf_url": "http://arxiv.org/pdf/2510.15850v1",
      "published": "2025-10-17T17:45:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15850v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ]
    },
    {
      "title": "Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer Framework",
      "authors": [
        "Shayan Rokhva",
        "Mousa Alizadeh",
        "Maryam Abdollahi Shamami"
      ],
      "abstract": "Accurately detecting sentiment polarity and intensity in product reviews and\nsocial media posts remains challenging due to informal and domain-specific\nlanguage. To address this, we propose a novel hybrid lexicon-fuzzy-transformer\nframework that combines rule-based heuristics, contextual deep learning, and\nfuzzy logic to generate continuous sentiment scores reflecting both polarity\nand strength. The pipeline begins with VADER-based initial sentiment\nestimations, which are refined through a two-stage adjustment process. This\ninvolves leveraging confidence scores from DistilBERT, a lightweight\ntransformer and applying fuzzy logic principles to mitigate excessive\nneutrality bias and enhance granularity. A custom fuzzy inference system then\nmaps the refined scores onto a 0 to 1 continuum, producing expert)like\njudgments. The framework is rigorously evaluated on four domain-specific\ndatasets. food delivery, e-commerce, tourism, and fashion. Results show\nimproved alignment with user ratings, better identification of sentiment\nextremes, and reduced misclassifications. Both quantitative metrics\n(distributional alignment, confusion matrices) and qualitative insights (case\nstudies, runtime analysis) affirm the models robustness and efficiency. This\nwork demonstrates the value of integrating symbolic reasoning with neural\nmodels for interpretable, finegrained sentiment analysis in linguistically\ndynamic domains.",
      "pdf_url": "http://arxiv.org/pdf/2510.15843v1",
      "published": "2025-10-17T17:36:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15843v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients",
      "authors": [
        "Dominik Kallusky",
        "Vinay Rao",
        "Vishal Nandavanam",
        "Hao-Jun Michael Shi"
      ],
      "abstract": "The rapid development of large language models (LLMs) has driven the demand\nfor more efficient optimization techniques. Among these, the Lookahead family\nof optimizers employs a two-loop framework, maintaining fast and slow sets of\nmodel weights. Multiple inner optimizer steps on the fast weights produce a\ntrajectory - the pseudo-gradient - that is used to update the slow weights.\nDiLoCo, a notable example originally designed for distributed training, applies\nNesterov momentum to the averaged pseudo-gradient from multiple workers,\nclaiming to even outperform AdamW in a non-distributed setup. In this paper, we\nempirically show that DiLoCo's surprising effectiveness stems primarily from\napplying Nesterov momentum to the pseudo-gradient, which improves training in a\nnon-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov\nOuter Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains\nof 1.5 - 2.5$\\times$ in a non-distributed setting up to a scale of 1e23\ntraining FLOPs, with improvements that increase with model size. Because of its\nminimal compute and memory overhead and compatibility with model sharding, SNOO\nis a practical enhancement for a variety of inner optimizers, including AdamW\nand Muon.",
      "pdf_url": "http://arxiv.org/pdf/2510.15830v1",
      "published": "2025-10-17T17:11:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15830v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "GENESIS: A Generative Model of Episodic-Semantic Interaction",
      "authors": [
        "Marco D'Alessandro",
        "Leo D'Amato",
        "Mikel Elkano",
        "Mikel Uriz",
        "Giovanni Pezzulo"
      ],
      "abstract": "A central challenge in cognitive neuroscience is to explain how semantic and\nepisodic memory, two major forms of declarative memory, typically associated\nwith cortical and hippocampal processing, interact to support learning, recall,\nand imagination. Despite significant advances, we still lack a unified\ncomputational framework that jointly accounts for core empirical phenomena\nacross both semantic and episodic processing domains. Here, we introduce the\nGenerative Episodic-Semantic Integration System (GENESIS), a computational\nmodel that formalizes memory as the interaction between two limited-capacity\ngenerative systems: a Cortical-VAE, supporting semantic learning and\ngeneralization, and a Hippocampal-VAE, supporting episodic encoding and\nretrieval within a retrieval-augmented generation (RAG) architecture. GENESIS\nreproduces hallmark behavioral findings, including generalization in semantic\nmemory, recognition, serial recall effects and gist-based distortions in\nepisodic memory, and constructive episodic simulation, while capturing their\ndynamic interactions. The model elucidates how capacity constraints shape the\nfidelity and memorability of experiences, how semantic processing introduces\nsystematic distortions in episodic recall, and how episodic replay can\nrecombine previous experiences. Together, these results provide a principled\naccount of memory as an active, constructive, and resource-bounded process.\nGENESIS thus advances a unified theoretical framework that bridges semantic and\nepisodic memory, offering new insights into the generative foundations of human\ncognition.",
      "pdf_url": "http://arxiv.org/pdf/2510.15828v1",
      "published": "2025-10-17T17:11:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15828v1",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ]
    },
    {
      "title": "Chronos-2: From Univariate to Universal Forecasting",
      "authors": [
        "Abdul Fatir Ansari",
        "Oleksandr Shchur",
        "Jaris Küken",
        "Andreas Auer",
        "Boran Han",
        "Pedro Mercado",
        "Syama Sundar Rangapuram",
        "Huibin Shen",
        "Lorenzo Stella",
        "Xiyuan Zhang",
        "Mononito Goswami",
        "Shubham Kapoor",
        "Danielle C. Maddix",
        "Pablo Guerron",
        "Tony Hu",
        "Junming Yin",
        "Nick Erickson",
        "Prateek Mutalik Desai",
        "Hao Wang",
        "Huzefa Rangwala",
        "George Karypis",
        "Yuyang Wang",
        "Michael Bohlke-Schneider"
      ],
      "abstract": "Pretrained time series models have enabled inference-only forecasting systems\nthat produce accurate predictions without task-specific training. However,\nexisting approaches largely focus on univariate forecasting, limiting their\napplicability in real-world scenarios where multivariate data and covariates\nplay a crucial role. We present Chronos-2, a pretrained model capable of\nhandling univariate, multivariate, and covariate-informed forecasting tasks in\na zero-shot manner. Chronos-2 employs a group attention mechanism that\nfacilitates in-context learning (ICL) through efficient information sharing\nacross multiple time series within a group, which may represent sets of related\nseries, variates of a multivariate series, or targets and covariates in a\nforecasting task. These general capabilities are achieved through training on\nsynthetic datasets that impose diverse multivariate structures on univariate\nseries. Chronos-2 delivers state-of-the-art performance across three\ncomprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On\nfev-bench, which emphasizes multivariate and covariate-informed forecasting,\nChronos-2's universal ICL capabilities lead to substantial improvements over\nexisting models. On tasks involving covariates, it consistently outperforms\nbaselines by a wide margin. Case studies in the energy and retail domains\nfurther highlight its practical advantages. The in-context learning\ncapabilities of Chronos-2 establish it as a general-purpose forecasting model\nthat can be used \"as is\" in real-world forecasting pipelines.",
      "pdf_url": "http://arxiv.org/pdf/2510.15821v1",
      "published": "2025-10-17T17:00:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15821v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "AB-UPT for Automotive and Aerospace Applications",
      "authors": [
        "Benedikt Alkin",
        "Richard Kurle",
        "Louis Serrano",
        "Dennis Just",
        "Johannes Brandstetter"
      ],
      "abstract": "The recently proposed Anchored-Branched Universal Physics Transformers\n(AB-UPT) shows strong capabilities to replicate automotive computational fluid\ndynamics simulations requiring orders of magnitudes less compute than\ntraditional numerical solvers. In this technical report, we add two new\ndatasets to the body of empirically evaluated use-cases of AB-UPT, combining\nhigh-quality data generation with state-of-the-art neural surrogates. Both\ndatasets were generated with the Luminary Cloud platform containing automotives\n(SHIFT-SUV) and aircrafts (SHIFT-Wing). We start by detailing the data\ngeneration. Next, we show favorable performances of AB-UPT against previous\nstate-of-the-art transformer-based baselines on both datasets, followed by\nextensive qualitative and quantitative evaluations of our best AB-UPT model.\nAB-UPT shows strong performances across the board. Notably, it obtains near\nperfect prediction of integrated aerodynamic forces within seconds from a\nsimple isotopically tesselate geometry representation and is trainable within a\nday on a single GPU, paving the way for industry-scale applications.",
      "pdf_url": "http://arxiv.org/pdf/2510.15808v1",
      "published": "2025-10-17T16:40:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15808v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation in Long COVID",
      "authors": [
        "Philip DiGiacomo",
        "Haoyang Wang",
        "Jinrui Fang",
        "Yan Leng",
        "W Michael Brode",
        "Ying Ding"
      ],
      "abstract": "As AI chatbots gain adoption in clinical medicine, developing effective\nframeworks for complex, emerging diseases presents significant challenges. We\ndeveloped and evaluated six Retrieval-Augmented Generation (RAG) corpus\nconfigurations for Long COVID (LC) clinical question answering, ranging from\nexpert-curated sources to large-scale literature databases. Our evaluation\nemployed an LLM-as-a-judge framework across faithfulness, relevance, and\ncomprehensiveness metrics using LongCOVID-CQ, a novel dataset of\nexpert-generated clinical questions. Our RAG corpus configuration combining\nclinical guidelines with high-quality systematic reviews consistently\noutperformed both narrow single-guideline approaches and large-scale literature\ndatabases. Our findings suggest that for emerging diseases, retrieval grounded\nin curated secondary reviews provides an optimal balance between narrow\nconsensus documents and unfiltered primary literature, supporting clinical\ndecision-making while avoiding information overload and oversimplified\nguidance. We propose Guide-RAG, a chatbot system and accompanying evaluation\nframework that integrates both curated expert knowledge and comprehensive\nliterature databases to effectively answer LC clinical questions.",
      "pdf_url": "http://arxiv.org/pdf/2510.15782v1",
      "published": "2025-10-17T16:05:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15782v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Controlling the image generation process with parametric activation functions",
      "authors": [
        "Ilia Pavlov"
      ],
      "abstract": "As image generative models continue to increase not only in their fidelity\nbut also in their ubiquity the development of tools that leverage direct\ninteraction with their internal mechanisms in an interpretable way has received\nlittle attention In this work we introduce a system that allows users to\ndevelop a better understanding of the model through interaction and\nexperimentation By giving users the ability to replace activation functions of\na generative network with parametric ones and a way to set the parameters of\nthese functions we introduce an alternative approach to control the networks\noutput We demonstrate the use of our method on StyleGAN2 and BigGAN networks\ntrained on FFHQ and ImageNet respectively.",
      "pdf_url": "http://arxiv.org/pdf/2510.15778v1",
      "published": "2025-10-17T16:02:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15778v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL",
      "authors": [
        "Richard M. Bailey"
      ],
      "abstract": "So-called `wicked problems', those involving complex multi-dimensional\nsettings, non-verifiable outcomes, heterogeneous impacts and a lack of single\nobjectively correct answers, have plagued humans throughout history. Modern\nexamples include decisions over justice frameworks, solving environmental\npollution, planning for pandemic resilience and food security. The use of\nstate-of-the-art artificial intelligence systems (notably Large Language\nModel-based agents) collaborating with humans on solving such problems is being\nactively explored. While the abilities of LLMs can be improved by, for example,\nfine-tuning, hand-crafted system prompts and scaffolding with external tools,\nLLMs lack endogenous mechanisms to develop expertise through experience in such\nsettings. This work address this gap with Dialectica, a framework where agents\nengage in structured dialogue on defined topics, augmented by memory,\nself-reflection, and policy-constrained context editing. Formally, discussion\nis viewed as an implicit meta-reinforcement learning process. The\n`dialogue-trained' agents are evaluated post-hoc using judged pairwise\ncomparisons of elicited responses. Across two model architectures (locally run\nQwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based\ncontext editing during discussion produces agents which dominate their baseline\ncounterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and\nAlphaRank mass. The predicted signatures of learning are observed qualitatively\nin statement and reflection logs, where reflections identify weaknesses and\nreliably shape subsequent statements. Agreement between quantitative and\nqualitative evidence supports dialogue-driven context evolution as a practical\npath to targeted expertise amplification in open non-verifiable domains.",
      "pdf_url": "http://arxiv.org/pdf/2510.15772v1",
      "published": "2025-10-17T15:59:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15772v1",
      "categories": [
        "cs.AI",
        "I.2.0"
      ]
    },
    {
      "title": "Preliminary Quantitative Study on Explainability and Trust in AI Systems",
      "authors": [
        "Allen Daniel Sunny"
      ],
      "abstract": "Large-scale AI models such as GPT-4 have accelerated the deployment of\nartificial intelligence across critical domains including law, healthcare, and\nfinance, raising urgent questions about trust and transparency. This study\ninvestigates the relationship between explainability and user trust in AI\nsystems through a quantitative experimental design. Using an interactive,\nweb-based loan approval simulation, we compare how different types of\nexplanations, ranging from basic feature importance to interactive\ncounterfactuals influence perceived trust. Results suggest that interactivity\nenhances both user engagement and confidence, and that the clarity and\nrelevance of explanations are key determinants of trust. These findings\ncontribute empirical evidence to the growing field of human-centered\nexplainable AI, highlighting measurable effects of explainability design on\nuser perception",
      "pdf_url": "http://arxiv.org/pdf/2510.15769v1",
      "published": "2025-10-17T15:59:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15769v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Semantic segmentation with coarse annotations",
      "authors": [
        "Jort de Jong",
        "Mike Holenderski"
      ],
      "abstract": "Semantic segmentation is the task of classifying each pixel in an image.\nTraining a segmentation model achieves best results using annotated images,\nwhere each pixel is annotated with the corresponding class. When obtaining fine\nannotations is difficult or expensive, it may be possible to acquire coarse\nannotations, e.g. by roughly annotating pixels in an images leaving some pixels\naround the boundaries between classes unlabeled. Segmentation with coarse\nannotations is difficult, in particular when the objective is to optimize the\nalignment of boundaries between classes. This paper proposes a regularization\nmethod for models with an encoder-decoder architecture with superpixel based\nupsampling. It encourages the segmented pixels in the decoded image to be\nSLIC-superpixels, which are based on pixel color and position, independent of\nthe segmentation annotation. The method is applied to FCN-16 fully\nconvolutional network architecture and evaluated on the SUIM, Cityscapes, and\nPanNuke data sets. It is shown that the boundary recall improves significantly\ncompared to state-of-the-art models when trained on coarse annotations.",
      "pdf_url": "http://arxiv.org/pdf/2510.15756v1",
      "published": "2025-10-17T15:41:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15756v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation",
      "authors": [
        "Yitong Sun",
        "Yao Huang",
        "Ruochen Zhang",
        "Huanran Chen",
        "Shouwei Ruan",
        "Ranjie Duan",
        "Xingxing Wei"
      ],
      "abstract": "Despite the impressive generative capabilities of text-to-image (T2I)\ndiffusion models, they remain vulnerable to generating inappropriate content,\nespecially when confronted with implicit sexual prompts. Unlike explicit\nharmful prompts, these subtle cues, often disguised as seemingly benign terms,\ncan unexpectedly trigger sexual content due to underlying model biases, raising\nsignificant ethical concerns. However, existing detection methods are primarily\ndesigned to identify explicit sexual content and therefore struggle to detect\nthese implicit cues. Fine-tuning approaches, while effective to some extent,\nrisk degrading the model's generative quality, creating an undesirable\ntrade-off. To address this, we propose NDM, the first noise-driven detection\nand mitigation framework, which could detect and mitigate implicit malicious\nintention in T2I generation while preserving the model's original generative\ncapabilities. Specifically, we introduce two key innovations: first, we\nleverage the separability of early-stage predicted noise to develop a\nnoise-based detection method that could identify malicious content with high\naccuracy and efficiency; second, we propose a noise-enhanced adaptive negative\nguidance mechanism that could optimize the initial noise by suppressing the\nprominent region's attention, thereby enhancing the effectiveness of adaptive\nnegative guidance for sexual mitigation. Experimentally, we validate NDM on\nboth natural and adversarial datasets, demonstrating its superior performance\nover existing SOTA methods, including SLD, UCE, and RECE, etc. Code and\nresources are available at https://github.com/lorraine021/NDM.",
      "pdf_url": "http://arxiv.org/pdf/2510.15752v1",
      "published": "2025-10-17T15:37:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15752v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment",
      "authors": [
        "Minlin Zeng",
        "Zhipeng Zhou",
        "Yang Qiu",
        "Zhiqi Shen"
      ],
      "abstract": "Parkinson's disease assessment has garnered growing interest in recent years,\nparticularly with the advent of sensor data and machine learning techniques.\nAmong these, multimodal approaches have demonstrated strong performance by\neffectively integrating complementary information from various data sources.\nHowever, two major limitations hinder their practical application: (1) the need\nto synchronize all modalities during training, and (2) the dependence on all\nmodalities during inference. To address these issues, we propose the first\nParkinson's assessment system that formulates multimodal learning as a\nmulti-objective optimization (MOO) problem. This not only allows for more\nflexible modality requirements during both training and inference, but also\nhandles modality collapse issue during multimodal information fusion. In\naddition, to mitigate the imbalance within individual modalities, we introduce\na margin-based class rebalancing strategy to enhance category learning. We\nconduct extensive experiments on three public datasets under both synchronous\nand asynchronous settings. The results show that our framework-Towards Relaxed\nInPuts (TRIP)-achieves state-of-the-art performance, outperforming the best\nbaselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous\nsetting, and by 4.86 and 2.30 percentage points in the synchronous setting,\nhighlighting its effectiveness and adaptability.",
      "pdf_url": "http://arxiv.org/pdf/2510.15748v1",
      "published": "2025-10-17T15:35:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15748v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation",
      "authors": [
        "Gao Yang",
        "Yuhang Liu",
        "Siyu Miao",
        "Xinyue Liang",
        "Zhengyang Liu",
        "Heyan Huang"
      ],
      "abstract": "Ideal or real - that is the question.In this work, we explore whether\nprinciples from game theory can be effectively applied to the evaluation of\nlarge language models (LLMs). This inquiry is motivated by the growing\ninadequacy of conventional evaluation practices, which often rely on\nfixed-format tasks with reference answers and struggle to capture the nuanced,\nsubjective, and open-ended nature of modern LLM behavior. To address these\nchallenges, we propose a novel alternative: automatic mutual evaluation, where\nLLMs assess each other's output through self-play and peer review. These peer\nassessments are then systematically compared with human voting behavior to\nevaluate their alignment with human judgment. Our framework incorporates\ngame-theoretic voting algorithms to aggregate peer reviews, enabling a\nprincipled investigation into whether model-generated rankings reflect human\npreferences. Empirical results reveal both convergences and divergences between\ntheoretical predictions and human evaluations, offering valuable insights into\nthe promises and limitations of mutual evaluation. To the best of our\nknowledge, this is the first work to jointly integrate mutual evaluation,\ngame-theoretic aggregation, and human-grounded validation for evaluating the\ncapabilities of LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.15746v1",
      "published": "2025-10-17T15:34:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15746v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "AURA: An Agent Autonomy Risk Assessment Framework",
      "authors": [
        "Lorenzo Satta Chiris",
        "Ayush Mishra"
      ],
      "abstract": "As autonomous agentic AI systems see increasing adoption across\norganisations, persistent challenges in alignment, governance, and risk\nmanagement threaten to impede deployment at scale. We present AURA (Agent\naUtonomy Risk Assessment), a unified framework designed to detect, quantify,\nand mitigate risks arising from agentic AI. Building on recent research and\npractical deployments, AURA introduces a gamma-based risk scoring methodology\nthat balances risk assessment accuracy with computational efficiency and\npractical considerations. AURA provides an interactive process to score,\nevaluate and mitigate the risks of running one or multiple AI Agents,\nsynchronously or asynchronously (autonomously). The framework is engineered for\nHuman-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H)\ncommunication mechanisms, allowing for seamless integration with agentic\nsystems for autonomous self-assessment, rendering it interoperable with\nestablished protocols (MCP and A2A) and tools. AURA supports a responsible and\ntransparent adoption of agentic AI and provides robust risk detection and\nmitigation while balancing computational resources, positioning it as a\ncritical enabler for large-scale, governable agentic AI in enterprise\nenvironments.",
      "pdf_url": "http://arxiv.org/pdf/2510.15739v1",
      "published": "2025-10-17T15:30:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15739v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Attention Sinks in Diffusion Language Models",
      "authors": [
        "Maximo Eduardo Rulli",
        "Simone Petruzzi",
        "Edoardo Michielon",
        "Fabrizio Silvestri",
        "Simone Scardapane",
        "Alessio Devoto"
      ],
      "abstract": "Masked Diffusion Language Models (DLMs) have recently emerged as a promising\nalternative to traditional Autoregressive Models (ARMs). DLMs employ\ntransformer encoders with bidirectional attention, enabling parallel token\ngeneration while maintaining competitive performance. Although their efficiency\nand effectiveness have been extensively studied, the internal mechanisms that\ngovern DLMs remain largely unexplored. In this work, we conduct an empirical\nanalysis of DLM attention patterns, focusing on the attention sinking\nphenomenon, an effect previously observed in various transformer-based\narchitectures. Our findings reveal that DLMs also exhibit attention sinks, but\nwith distinct characteristics. First, unlike in ARMs, the sink positions in\nDLMs tend to shift throughout the generation process, displaying a dynamic\nbehaviour. Second, while ARMs are highly sensitive to the removal of attention\nsinks, DLMs remain robust: masking sinks leads to only a minor degradation in\nperformance. These results provide new insights into the inner workings of\ndiffusion-based language models and highlight fundamental differences in how\nthey allocate and utilize attention compared to autoregressive models.",
      "pdf_url": "http://arxiv.org/pdf/2510.15731v1",
      "published": "2025-10-17T15:23:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15731v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "RLAF: Reinforcement Learning from Automaton Feedback",
      "authors": [
        "Mahyar Alinejad",
        "Alvaro Velasquez",
        "Yue Wang",
        "George Atia"
      ],
      "abstract": "Reinforcement Learning (RL) in environments with complex, history-dependent\nreward structures poses significant challenges for traditional methods. In this\nwork, we introduce a novel approach that leverages automaton-based feedback to\nguide the learning process, replacing explicit reward functions with\npreferences derived from a deterministic finite automaton (DFA). Unlike\nconventional approaches that use automata for direct reward specification, our\nmethod employs the structure of the DFA to generate preferences over\ntrajectories that are used to learn a reward function, eliminating the need for\nmanual reward engineering. Our framework introduces a static approach that uses\nthe learned reward function directly for policy optimization and a dynamic\napproach that involves continuous refining of the reward function and policy\nthrough iterative updates until convergence.\n  Our experiments in both discrete and continuous environments demonstrate that\nour approach enables the RL agent to learn effective policies for tasks with\ntemporal dependencies, outperforming traditional reward engineering and\nautomaton-based baselines such as reward machines and LTL-guided methods. Our\nresults highlight the advantages of automaton-based preferences in handling\nnon-Markovian rewards, offering a scalable, efficient, and human-independent\nalternative to traditional reward modeling. We also provide a convergence\nguarantee showing that under standard assumptions our automaton-guided\npreference-based framework learns a policy that is near-optimal with respect to\nthe true non-Markovian objective.",
      "pdf_url": "http://arxiv.org/pdf/2510.15728v1",
      "published": "2025-10-17T15:17:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15728v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Invoice Information Extraction: Methods and Performance Evaluation",
      "authors": [
        "Sai Yashwant",
        "Anurag Dubey",
        "Praneeth Paikray",
        "Gantala Thulsiram"
      ],
      "abstract": "This paper presents methods for extracting structured information from\ninvoice documents and proposes a set of evaluation metrics (EM) to assess the\naccuracy of the extracted data against annotated ground truth. The approach\ninvolves pre-processing scanned or digital invoices, applying Docling and\nLlamaCloud Services to identify and extract key fields such as invoice number,\ndate, total amount, and vendor details. To ensure the reliability of the\nextraction process, we establish a robust evaluation framework comprising\nfield-level precision, consistency check failures, and exact match accuracy.\nThe proposed metrics provide a standardized way to compare different extraction\nmethods and highlight strengths and weaknesses in field-specific performance.",
      "pdf_url": "http://arxiv.org/pdf/2510.15727v1",
      "published": "2025-10-17T15:16:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15727v1",
      "categories": [
        "cs.AI",
        "cs.DB"
      ]
    },
    {
      "title": "DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification",
      "authors": [
        "Tingyu Lin",
        "Armin Dadras",
        "Florian Kleber",
        "Robert Sablatnig"
      ],
      "abstract": "Camera movement classification (CMC) models trained on contemporary,\nhigh-quality footage often degrade when applied to archival film, where noise,\nmissing frames, and low contrast obscure motion cues. We bridge this gap by\nassembling a unified benchmark that consolidates two modern corpora into four\ncanonical classes and restructures the HISTORIAN collection into five balanced\ncategories. Building on this benchmark, we introduce DGME-T, a lightweight\nextension to the Video Swin Transformer that injects directional grid motion\nencoding, derived from optical flow, via a learnable and normalised late-fusion\nlayer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and\nits macro F1 from 82.08% to 87.81% on modern clips, while still improving the\ndemanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%\nto 82.63% macro F1. A cross-domain study further shows that an intermediate\nfine-tuning stage on modern data increases historical performance by more than\nfive percentage points. These results demonstrate that structured motion priors\nand transformer representations are complementary and that even a small,\ncarefully calibrated motion head can substantially enhance robustness in\ndegraded film analysis. Related resources are available at\nhttps://github.com/linty5/DGME-T.",
      "pdf_url": "http://arxiv.org/pdf/2510.15725v1",
      "published": "2025-10-17T15:14:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15725v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "ProSh: Probabilistic Shielding for Model-free Reinforcement Learning",
      "authors": [
        "Edwin Hamel-De le Court",
        "Gaspard Ohlmann",
        "Francesco Belardinelli"
      ],
      "abstract": "Safety is a major concern in reinforcement learning (RL): we aim at\ndeveloping RL systems that not only perform optimally, but are also safe to\ndeploy by providing formal guarantees about their safety. To this end, we\nintroduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free\nalgorithm for safe reinforcement learning under cost constraints. ProSh\naugments the Constrained MDP state space with a risk budget and enforces safety\nby applying a shield to the agent's policy distribution using a learned cost\ncritic. The shield ensures that all sampled actions remain safe in expectation.\nWe also show that optimality is preserved when the environment is\ndeterministic. Since ProSh is model-free, safety during training depends on the\nknowledge we have acquired about the environment. We provide a tight\nupper-bound on the cost in expectation, depending only on the backup-critic\naccuracy, that is always satisfied during training. Under mild, practically\nachievable assumptions, ProSh guarantees safety even at training time, as shown\nin the experiments.",
      "pdf_url": "http://arxiv.org/pdf/2510.15720v1",
      "published": "2025-10-17T15:08:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15720v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Direct Preference Optimization with Unobserved Preference Heterogeneity: The Necessity of Ternary Preferences",
      "authors": [
        "Keertana Chidambaram",
        "Karthik Vinary Seetharaman",
        "Vasilis Syrgkanis"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become central to\naligning large language models with human values, typically by first learning a\nreward model from preference data which is then used to update the model with\nreinforcement learning. Recent alternatives such as Direct Preference\nOptimization (DPO) simplify this pipeline by directly optimizing on\npreferences. However, both approaches often assume uniform annotator\npreferences and rely on binary comparisons, overlooking two key limitations:\nthe diversity of human evaluators and the limitations of pairwise feedback. In\nthis work, we address both these issues. First, we connect preference learning\nin RLHF with the econometrics literature and show that binary comparisons are\ninsufficient for identifying latent user preferences from finite user data and\ninfinite users, while (even incomplete) rankings over three or more responses\nensure identifiability. Second, we introduce methods to incorporate\nheterogeneous preferences into alignment algorithms. We develop an\nExpectation-Maximization adaptation of DPO that discovers latent annotator\ntypes and trains a mixture of LLMs accordingly. Then we propose an aggregation\nalgorithm using a min-max regret fairness criterion to produce a single\ngenerative policy with equitable performance guarantees. Together, these\ncontributions establish a theoretical and algorithmic framework for fairness\nand personalization for diverse users in generative model alignment.",
      "pdf_url": "http://arxiv.org/pdf/2510.15716v1",
      "published": "2025-10-17T15:00:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15716v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Beyond-Diagonal RIS Under Non-Idealities: Learning-Based Architecture Discovery and Optimization",
      "authors": [
        "Binggui Zhou",
        "Bruno Clerckx"
      ],
      "abstract": "Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has recently been\nintroduced to enable advanced control over electromagnetic waves to further\nincrease the benefits of traditional RIS in enhancing signal quality and\nimproving spectral and energy efficiency for next-generation wireless networks.\nA significant issue in designing and deploying BD-RIS is the tradeoff between\nits performance and circuit complexity. Despite some efforts in exploring\noptimal architectures with the lowest circuit complexities for ideal BD-RIS,\narchitecture discovery for non-ideal BD-RIS remains uninvestigated. Therefore,\nhow non-idealities and circuit complexity jointly affect the performance of\nBD-RIS remains unclear, making it difficult to achieve the performance -\ncircuit complexity tradeoff in the presence of non-idealities. Essentially,\narchitecture discovery for non-ideal BD-RIS faces challenges from both the\ncomputational complexity of global architecture search and the difficulty in\nachieving global optima. To tackle these challenges, we propose a\nlearning-based two-tier architecture discovery framework (LTTADF) consisting of\nan architecture generator and a performance optimizer to jointly discover\noptimal architectures of non-ideal BD-RIS given specific circuit complexities,\nwhich can effectively explore over a large architecture space while avoiding\ngetting trapped in poor local optima and thus achieving near-optimal solutions\nfor the performance optimization. Numerical results provide valuable insights\nfor deploying non-ideal BD-RIS considering the performance - circuit complexity\ntradeoff.",
      "pdf_url": "http://arxiv.org/pdf/2510.15701v1",
      "published": "2025-10-17T14:46:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15701v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "eess.SP",
        "math.IT"
      ]
    },
    {
      "title": "ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations",
      "authors": [
        "Alex Gu",
        "Bartosz Piotrowski",
        "Fabian Gloeckle",
        "Kaiyu Yang",
        "Aram H. Markosyan"
      ],
      "abstract": "Neural theorem proving has advanced rapidly in the past year, reaching IMO\ngold-medalist capabilities and producing formal proofs that span thousands of\nlines. Although such proofs are mechanically verified by formal systems like\nLean, their excessive length renders them difficult for humans to comprehend\nand limits their usefulness for mathematical insight. Proof simplification is\ntherefore a critical bottleneck. Yet, training data for this task is scarce,\nand existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --\nstruggle with the extremely long proofs generated by RL-trained provers. We\nintroduce ProofOptimizer, the first language model trained to simplify Lean\nproofs without requiring additional human supervision. ProofOptimizer is\ntrained via expert iteration and reinforcement learning, using Lean to verify\nsimplifications and provide training signal. At inference time, it operates\nwithin an iterative proof-shortening workflow, progressively reducing proof\nlength. Experiments show that ProofOptimizer substantially compresses proofs\ngenerated by state-of-the-art RL-trained provers on standard benchmarks,\nreducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on\nSeed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check\nfaster in Lean and further improve downstream prover performance when reused as\ntraining data for supervised finetuning.",
      "pdf_url": "http://arxiv.org/pdf/2510.15700v1",
      "published": "2025-10-17T14:45:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15700v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ]
    },
    {
      "title": "Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction",
      "authors": [
        "Tian Guo",
        "Emmanuel Hauptmann"
      ],
      "abstract": "In quantitative investing, return prediction supports various tasks,\nincluding stock selection, portfolio optimization, and risk management.\nQuantitative factors, such as valuation, quality, and growth, capture various\ncharacteristics of stocks. Unstructured financial data, like news and\ntranscripts, has attracted growing attention, driven by recent advances in\nlarge language models (LLMs). This paper examines effective methods for\nleveraging multimodal factors and newsflow in return prediction and stock\nselection. First, we introduce a fusion learning framework to learn a unified\nrepresentation from factors and newsflow representations generated by an LLM.\nWithin this framework, we compare three representative methods: representation\ncombination, representation summation, and attentive representations. Next,\nbuilding on empirical observations from fusion learning, we explore the mixture\nmodel that adaptively combines predictions made by single modalities and their\nfusion. To mitigate the training instability observed in the mixture model, we\nintroduce a decoupled training approach with theoretical insights. Finally, our\nexperiments on real investment universes yield several insights into effective\nmultimodal modeling of factors and news for stock return prediction.",
      "pdf_url": "http://arxiv.org/pdf/2510.15691v1",
      "published": "2025-10-17T14:35:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15691v1",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "KS-Net: Multi-layer network model for determining the rotor type from motor parameters in interior PMSMs",
      "authors": [
        "Kivanc Dogan",
        "Ahmet Orhan"
      ],
      "abstract": "The demand for high efficiency and precise control in electric drive systems\nhas led to the widespread adoption of Interior Permanent Magnet Synchronous\nMotors (IPMSMs). The performance of these motors is significantly influenced by\nrotor geometry. Traditionally, rotor shape analysis has been conducted using\nthe finite element method (FEM), which involves high computational costs. This\nstudy aims to classify the rotor shape (2D type, V type, Nabla type) of IPMSMs\nusing electromagnetic parameters through machine learning-based methods and to\ndemonstrate the applicability of this approach as an alternative to classical\nmethods. In this context, a custom deep learning model, KS-Net, developed by\nthe user, was comparatively evaluated against Cubic SVM, Quadratic SVM, Fine\nKNN, Cosine KNN, and Fine Tree algorithms. The balanced dataset, consisting of\n9,000 samples, was tested using 10-fold cross-validation, and performance\nmetrics such as accuracy, precision, recall, and F1-score were employed. The\nresults indicate that the Cubic SVM and Quadratic SVM algorithms classified all\nsamples flawlessly, achieving 100% accuracy, while the KS-Net model achieved\n99.98% accuracy with only two misclassifications, demonstrating competitiveness\nwith classical methods. This study shows that the rotor shape of IPMSMs can be\npredicted with high accuracy using data-driven approaches, offering a fast and\ncost-effective alternative to FEM-based analyses. The findings provide a solid\nfoundation for accelerating motor design processes, developing automated rotor\nidentification systems, and enabling data-driven fault diagnosis in engineering\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2510.15688v1",
      "published": "2025-10-17T14:32:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15688v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2"
      ]
    },
    {
      "title": "Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI",
      "authors": [
        "Gerard Comas-Quiles",
        "Carles Garcia-Cabrera",
        "Julia Dietlmeier",
        "Noel E. O'Connor",
        "Ferran Marques"
      ],
      "abstract": "Unsupervised anomaly detection (UAD) presents a complementary alternative to\nsupervised learning for brain tumor segmentation in magnetic resonance imaging\n(MRI), particularly when annotated datasets are limited, costly, or\ninconsistent. In this work, we propose a novel Multimodal Vision Transformer\nAutoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and\nlocalize tumors via reconstruction-based error maps. This unsupervised paradigm\nenables segmentation without reliance on manual labels, addressing a key\nscalability bottleneck in neuroimaging workflows. Our method is evaluated in\nthe BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors\nsuch as gliomas, meningiomas, and pediatric brain tumors. To enhance\nperformance, we introduce a multimodal early-late fusion strategy that\nleverages complementary information across multiple MRI sequences, and a\npost-processing pipeline that integrates the Segment Anything Model (SAM) to\nrefine predicted tumor contours. Despite the known challenges of UAD,\nparticularly in detecting small or non-enhancing lesions, our method achieves\nclinically meaningful tumor localization, with lesion-wise Dice Similarity\nCoefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing\nTumor) on the test set, and an anomaly Detection Rate of 89.4% on the\nvalidation set. These findings highlight the potential of transformer-based\nunsupervised models to serve as scalable, label-efficient tools for\nneuro-oncological imaging.",
      "pdf_url": "http://arxiv.org/pdf/2510.15684v1",
      "published": "2025-10-17T14:26:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15684v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Mixture of Experts Approaches in Dense Retrieval Tasks",
      "authors": [
        "Effrosyni Sokli",
        "Pranav Kasela",
        "Georgios Peikos",
        "Gabriella Pasi"
      ],
      "abstract": "Dense Retrieval Models (DRMs) are a prominent development in Information\nRetrieval (IR). A key challenge with these neural Transformer-based models is\nthat they often struggle to generalize beyond the specific tasks and domains\nthey were trained on. To address this challenge, prior research in IR\nincorporated the Mixture-of-Experts (MoE) framework within each Transformer\nlayer of a DRM, which, though effective, substantially increased the number of\nadditional parameters. In this paper, we propose a more efficient design, which\nintroduces a single MoE block (SB-MoE) after the final Transformer layer. To\nassess the retrieval effectiveness of SB-MoE, we perform an empirical\nevaluation across three IR tasks. Our experiments involve two evaluation\nsetups, aiming to assess both in-domain effectiveness and the model's zero-shot\ngeneralizability. In the first setup, we fine-tune SB-MoE with four different\nunderlying DRMs on seven IR benchmarks and evaluate them on their respective\ntest sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform\nzero-shot evaluation on thirteen BEIR datasets. Additionally, we perform\nfurther experiments to analyze the model's dependency on its hyperparameters\n(i.e., the number of employed and activated experts) and investigate how this\nvariation affects SB-MoE's performance. The obtained results show that SB-MoE\nis particularly effective for DRMs with lightweight base models, such as\nTinyBERT and BERT-Small, consistently exceeding standard model fine-tuning\nacross benchmarks. For DRMs with more parameters, such as BERT-Base and\nContriever, our model requires a larger number of training samples to achieve\nimproved retrieval performance. Our code is available online at:\nhttps://github.com/FaySokli/SB-MoE.",
      "pdf_url": "http://arxiv.org/pdf/2510.15683v1",
      "published": "2025-10-17T14:23:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15683v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "I.2.4; I.2.7"
      ]
    },
    {
      "title": "ProofBridge: Auto-Formalization of Natural Language Proofs in Lean via Joint Embeddings",
      "authors": [
        "Prithwish Jana",
        "Kaan Kale",
        "Ahmet Ege Tanriverdi",
        "Cruise Song",
        "Sriram Vishwanath",
        "Vijay Ganesh"
      ],
      "abstract": "Translating human-written mathematical theorems and proofs from natural\nlanguage (NL) into formal languages (FLs) like Lean 4 has long been a\nsignificant challenge for AI. Most state-of-the-art methods address this\nseparately, first translating theorems and then generating proofs, creating a\nfundamental disconnect vis-a-vis true proof auto-formalization. This two-step\nprocess and its limitations were evident even in AlphaProof's silver-medal\nperformance at the 2024 IMO, where problem statements needed manual translation\nbefore automated proof synthesis.\n  We present ProofBridge, a unified framework for automatically translating\nentire NL theorems and proofs into Lean 4. At its core is a joint embedding\nmodel that aligns NL and FL (NL-FL) theorem-proof pairs in a shared semantic\nspace, enabling cross-modal retrieval of semantically relevant FL examples to\nguide translation. Our training ensures that NL-FL theorems (and their proofs)\nare mapped close together in this space if and only if the NL-FL pairs are\nsemantically equivalent. ProofBridge integrates retrieval-augmented fine-tuning\nwith iterative proof repair, leveraging Lean's type checker and semantic\nequivalence feedback to ensure both syntactic correctness and semantic\nfidelity. Experiments show substantial improvements in proof auto-formalization\nover strong baselines (including GPT-5, Gemini-2.5, Kimina-Prover,\nDeepSeek-Prover), with our retrieval-augmented approach yielding significant\ngains in semantic correctness (SC, via proving bi-directional equivalence) and\ntype correctness (TC, via type-checking theorem+proof) across pass@k metrics on\nminiF2F-Test-PF, a dataset we curated. In particular, ProofBridge improves\ncross-modal retrieval quality by up to 3.28x Recall@1 over all-MiniLM-L6-v2,\nand achieves +31.14% SC and +1.64% TC (pass@32) compared to the baseline\nKimina-Prover-RL-1.7B.",
      "pdf_url": "http://arxiv.org/pdf/2510.15681v1",
      "published": "2025-10-17T14:20:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15681v1",
      "categories": [
        "cs.LO",
        "cs.AI"
      ]
    },
    {
      "title": "CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning",
      "authors": [
        "Yung-Chen Tang",
        "Pin-Yu Chen",
        "Andrea Cavallaro"
      ],
      "abstract": "Allocating more computation during inference time (test-time scaling)\nimproves language model performance, especially for reasoning tasks. However,\npopular methods like Best-of-$N$ sampling often show diminishing returns as $N$\nincreases. To address this inefficiency, we introduce a general test-time\ncalibration framework that adaptively modifies the model toward high-reward\nreasoning paths, with theoretical guarantees of improving the lower bound of\nexpected reward under finite sampling, all without large language model (LLM)\nretraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),\na two-phase method that first explores the solution space and then learns a\ncalibration of the logits via an input-specific temperature $T$ and additive\nshift vector $\\delta$, guiding generation toward more reliable reasoning.\nExperiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,\nwith up to $4\\times$ fewer rollouts to reach the same accuracy, while often\nachieving higher accuracy under fixed budgets. We also analyze the\ncomplementary roles of $T$ and $\\delta$ in balancing output diversity and\ncorrectness, and demonstrate that the framework also generalizes to step-level\nsampling strategies such as beam search. For more information, please refer to\nour project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.",
      "pdf_url": "http://arxiv.org/pdf/2510.15674v1",
      "published": "2025-10-17T14:04:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15674v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Valeo Near-Field: a novel dataset for pedestrian intent detection",
      "authors": [
        "Antonyo Musabini",
        "Rachid Benmokhtar",
        "Jagdish Bhanushali",
        "Victor Galizzi",
        "Bertrand Luvison",
        "Xavier Perrotton"
      ],
      "abstract": "This paper presents a novel dataset aimed at detecting pedestrians'\nintentions as they approach an ego-vehicle. The dataset comprises synchronized\nmulti-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic\nsensor readings, and motion capture-based 3D body poses, collected across\ndiverse real-world scenarios. Key contributions include detailed annotations of\n3D body joint positions synchronized with fisheye camera images, as well as\naccurate 3D pedestrian positions extracted from lidar data, facilitating robust\nbenchmarking for perception algorithms. We release a portion of the dataset\nalong with a comprehensive benchmark suite, featuring evaluation metrics for\naccuracy, efficiency, and scalability on embedded systems. By addressing\nreal-world challenges such as sensor occlusions, dynamic environments, and\nhardware constraints, this dataset offers a unique resource for developing and\nevaluating state-of-the-art algorithms in pedestrian detection, 3D pose\nestimation and 4D trajectory and intention prediction. Additionally, we provide\nbaseline performance metrics using custom neural network architectures and\nsuggest future research directions to encourage the adoption and enhancement of\nthe dataset. This work aims to serve as a foundation for researchers seeking to\nadvance the capabilities of intelligent vehicles in near-field scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2510.15673v1",
      "published": "2025-10-17T14:02:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15673v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Enhance Large Language Models as Recommendation Systems with Collaborative Filtering",
      "authors": [
        "Zhisheng Yang",
        "Xiaofei Xu",
        "Ke Deng",
        "Li Li"
      ],
      "abstract": "As powerful tools in Natural Language Processing (NLP), Large Language Models\n(LLMs) have been leveraged for crafting recommendations to achieve precise\nalignment with user preferences and elevate the quality of the recommendations.\nThe existing approaches implement both non-tuning and tuning strategies.\nCompared to following the tuning strategy, the approaches following the\nnon-tuning strategy avoid the relatively costly, time-consuming, and\nexpertise-requiring process of further training pre-trained LLMs on\ntask-specific datasets, but they suffer the issue of not having the\ntask-specific business or local enterprise knowledge. To the best of our\nknowledge, none of the existing approaches following the non-tuning strategy\nexplicitly integrates collaborative filtering, one of the most successful\nrecommendation techniques. This study aims to fill the gap by proposing\ncritique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose,\nwe train a separate machine-learning model called Critic that implements\ncollaborative filtering for recommendations by learning from the interactions\nbetween many users and items. The Critic provides critiques to LLMs to\nsignificantly refine the recommendations. Extensive experiments have verified\nthe effectiveness of Critic-LLM-RS on real datasets.",
      "pdf_url": "http://arxiv.org/pdf/2510.15647v1",
      "published": "2025-10-17T13:35:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15647v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation",
      "authors": [
        "Ed Li",
        "Junyu Ren",
        "Xintian Pan",
        "Cat Yan",
        "Chuanhao Li",
        "Dirk Bergemann",
        "Zhuoran Yang"
      ],
      "abstract": "The automation of scientific discovery represents a critical milestone in\nArtificial Intelligence (AI) research. However, existing agentic systems for\nscience suffer from two fundamental limitations: rigid, pre-programmed\nworkflows that cannot adapt to intermediate findings, and inadequate context\nmanagement that hinders long-horizon research. We present\n\\texttt{freephdlabor}, an open-source multiagent framework featuring\n\\textit{fully dynamic workflows} determined by real-time agent reasoning and a\n\\coloremph{\\textit{modular architecture}} enabling seamless customization --\nusers can modify, add, or remove agents to address domain-specific\nrequirements. The framework provides comprehensive infrastructure including\n\\textit{automatic context compaction}, \\textit{workspace-based communication}\nto prevent information degradation, \\textit{memory persistence} across\nsessions, and \\textit{non-blocking human intervention} mechanisms. These\nfeatures collectively transform automated research from isolated, single-run\nattempts into \\textit{continual research programs} that build systematically on\nprior explorations and incorporate human feedback. By providing both the\narchitectural principles and practical implementation for building customizable\nco-scientist systems, this work aims to facilitate broader adoption of\nautomated research across scientific domains, enabling practitioners to deploy\ninteractive multiagent systems that autonomously conduct end-to-end research --\nfrom ideation through experimentation to publication-ready manuscripts.",
      "pdf_url": "http://arxiv.org/pdf/2510.15624v1",
      "published": "2025-10-17T13:13:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15624v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "CQD-SHAP: Explainable Complex Query Answering via Shapley Values",
      "authors": [
        "Parsa Abbasi",
        "Stefan Heindorf"
      ],
      "abstract": "Complex query answering (CQA) goes beyond the well-studied link prediction\ntask by addressing more sophisticated queries that require multi-hop reasoning\nover incomplete knowledge graphs (KGs). Research on neural and neurosymbolic\nCQA methods is still an emerging field. Almost all of these methods can be\nregarded as black-box models, which may raise concerns about user trust.\nAlthough neurosymbolic approaches like CQD are slightly more interpretable,\nallowing intermediate results to be tracked, the importance of different parts\nof the query remains unexplained. In this paper, we propose CQD-SHAP, a novel\nframework that computes the contribution of each query part to the ranking of a\nspecific answer. This contribution explains the value of leveraging a neural\npredictor that can infer new knowledge from an incomplete KG, rather than a\nsymbolic approach relying solely on existing facts in the KG. CQD-SHAP is\nformulated based on Shapley values from cooperative game theory and satisfies\nall the fundamental Shapley axioms. Automated evaluation of these explanations\nin terms of necessary and sufficient explanations, and comparisons with various\nbaselines, shows the effectiveness of this approach for most query types.",
      "pdf_url": "http://arxiv.org/pdf/2510.15623v1",
      "published": "2025-10-17T13:09:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15623v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism",
      "authors": [
        "Haoran Sun",
        "Yankai Jiang",
        "Zhenyu Tang",
        "Yaning Pan",
        "Shuang Gu",
        "Zekai Lin",
        "Lilong Wang",
        "Wenjie Lou",
        "Lei Liu",
        "Lei Bai",
        "Xiaosong Wang"
      ],
      "abstract": "The foundation of reproducible science lies in protocols that are precise,\nlogically ordered, and executable. The autonomous generation of these protocols\nthrough natural language queries could greatly improve the efficiency of the\nreproduction process. However, current leading large language models (LLMs)\noften generate incomplete or inconsistent protocols, limiting their utility. To\naddress this limitation, we first introduce SciRecipe, a large-scale dataset of\nover 12K structured protocols spanning 27 biological subfields and encompassing\nboth comprehension and problem-solving tasks. To further improve protocol\ngeneration, we propose the \"Sketch-and-Fill\" paradigm, which separates\nanalysis, structuring, and expression to ensure each step is explicit and\nverifiable. Complementing this, the structured component-based reward mechanism\nevaluates step granularity, action order, and semantic fidelity, aligning model\noptimization with experimental reliability. Building on these components, we\ndevelop Thoth, trained through a staged Knowledge-to-Action process that\nprogresses from knowledge acquisition to operational reasoning and ultimately\nto robust, executable protocol generation. Across multiple benchmarks, Thoth\nconsistently surpasses both proprietary and open-source LLMs, achieving\nsignificant improvements in step alignment, logical sequencing, and semantic\naccuracy. Our approach paves the way for reliable scientific assistants that\nbridge knowledge with experimental execution. All data, code, and models will\nbe released publicly.",
      "pdf_url": "http://arxiv.org/pdf/2510.15600v1",
      "published": "2025-10-17T12:47:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15600v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Context-aware deep learning using individualized prior information reduces false positives in disease risk prediction and longitudinal health assessment",
      "authors": [
        "Lavanya Umapathy",
        "Patricia M Johnson",
        "Tarun Dutt",
        "Angela Tong",
        "Madhur Nayan",
        "Hersh Chandarana",
        "Daniel K Sodickson"
      ],
      "abstract": "Temporal context in medicine is valuable in assessing key changes in patient\nhealth over time. We developed a machine learning framework to integrate\ndiverse context from prior visits to improve health monitoring, especially when\nprior visits are limited and their frequency is variable. Our model first\nestimates initial risk of disease using medical data from the most recent\npatient visit, then refines this assessment using information digested from\npreviously collected imaging and/or clinical biomarkers. We applied our\nframework to prostate cancer (PCa) risk prediction using data from a large\npopulation (28,342 patients, 39,013 magnetic resonance imaging scans, 68,931\nblood tests) collected over nearly a decade. For predictions of the risk of\nclinically significant PCa at the time of the visit, integrating prior context\ndirectly converted false positives to true negatives, increasing overall\nspecificity while preserving high sensitivity. False positive rates were\nreduced progressively from 51% to 33% when integrating information from up to\nthree prior imaging examinations, as compared to using data from a single\nvisit, and were further reduced to 24% when also including additional context\nfrom prior clinical data. For predicting the risk of PCa within five years of\nthe visit, incorporating prior context reduced false positive rates still\nfurther (64% to 9%). Our findings show that information collected over time\nprovides relevant context to enhance the specificity of medical risk\nprediction. For a wide range of progressive conditions, sufficient reduction of\nfalse positive rates using context could offer a pathway to expand longitudinal\nhealth monitoring programs to large populations with comparatively low baseline\nrisk of disease, leading to earlier detection and improved health outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2510.15591v1",
      "published": "2025-10-17T12:38:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15591v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy",
      "authors": [
        "Mohammad Soltaninezhad",
        "Yashar Rouzbahani",
        "Jhonatan Contreras",
        "Rohan Chippalkatti",
        "Daniel Kwaku Abankwa",
        "Christian Eggeling",
        "Thomas Bocklitz"
      ],
      "abstract": "Lightweight deep learning models offer substantial reductions in\ncomputational cost and environmental impact, making them crucial for scientific\napplications. We present a lightweight CycleGAN for modality transfer in\nfluorescence microscopy (confocal to super-resolution STED/deconvolved STED),\naddressing the common challenge of unpaired datasets. By replacing the\ntraditional channel-doubling strategy in the U-Net-based generator with a fixed\nchannel approach, we drastically reduce trainable parameters from 41.8 million\nto approximately nine thousand, achieving superior performance with faster\ntraining and lower memory usage. We also introduce the GAN as a diagnostic tool\nfor experimental and labeling quality. When trained on high-quality images, the\nGAN learns the characteristics of optimal imaging; deviations between its\ngenerated outputs and new experimental images can reveal issues such as\nphotobleaching, artifacts, or inaccurate labeling. This establishes the model\nas a practical tool for validating experimental accuracy and image fidelity in\nmicroscopy workflows.",
      "pdf_url": "http://arxiv.org/pdf/2510.15579v1",
      "published": "2025-10-17T12:20:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15579v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Spark Effect: On Engineering Creative Diversity in Multi-Agent AI Systems",
      "authors": [
        "Alexander Doudkin",
        "Anton Voelker",
        "Friedrich von Borries"
      ],
      "abstract": "Creative services teams increasingly rely on large language models (LLMs) to\naccelerate ideation, yet production systems often converge on homogeneous\noutputs that fail to meet brand or artistic expectations. Art of X developed\npersona-conditioned LLM agents -- internally branded as \"Sparks\" and\ninstantiated through a library of role-inspired system prompts -- to\nintentionally diversify agent behaviour within a multi-agent workflow. This\nwhite paper documents the problem framing, experimental design, and\nquantitative evidence behind the Spark agent programme. Using an LLM-as-a-judge\nprotocol calibrated against human gold standards, we observe a mean diversity\ngain of +4.1 points (on a 1-10 scale) when persona-conditioned Spark agents\nreplace a uniform system prompt, narrowing the gap to human experts to 1.0\npoint. We also surface evaluator bias and procedural considerations for future\ndeployments.",
      "pdf_url": "http://arxiv.org/pdf/2510.15568v1",
      "published": "2025-10-17T11:56:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15568v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2.11; I.2.7; J.5"
      ]
    },
    {
      "title": "SpikeVox: Towards Energy-Efficient Speech Therapy Framework with Spike-driven Generative Language Models",
      "authors": [
        "Rachmad Vidya Wicaksana Putra",
        "Aadithyan Rajesh Nair",
        "Muhammad Shafique"
      ],
      "abstract": "Speech disorders can significantly affect the patients capability to\ncommunicate, learn, and socialize. However, existing speech therapy solutions\n(e.g., therapist or tools) are still limited and costly, hence such solutions\nremain inadequate for serving millions of patients worldwide. To address this,\nstate-of-the-art methods employ neural network (NN) algorithms to help\naccurately detecting speech disorders. However, these methods do not provide\ntherapy recommendation as feedback, hence providing partial solution for\npatients. Moreover, these methods incur high energy consumption due to their\ncomplex and resource-intensive NN processing, hence hindering their deployments\non low-power/energy platforms (e.g., smartphones). Toward this, we propose\nSpikeVox, a novel framework for enabling energy-efficient speech therapy\nsolutions through spike-driven generative language model. Specifically,\nSpikeVox employs a speech recognition module to perform highly accurate\nspeech-to-text conversion; leverages a spike-driven generative language model\nto efficiently perform pattern analysis for speech disorder detection and\ngenerates suitable exercises for therapy; provides guidance on correct\npronunciation as feedback; as well as utilizes the REST API to enable seamless\ninteraction for users. Experimental results demonstrate that SpikeVox achieves\n88% confidence level on average in speech disorder recognition, while providing\na complete feedback for therapy exercises. Therefore, SpikeVox provides a\ncomprehensive framework for energy-efficient speech therapy solutions, and\npotentially addresses the significant global speech therapy access gap.",
      "pdf_url": "http://arxiv.org/pdf/2510.15566v1",
      "published": "2025-10-17T11:54:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15566v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament",
      "authors": [
        "Jiayuan Bai",
        "Xuan-guang Pan",
        "Chongyang Tao",
        "Shuai Ma"
      ],
      "abstract": "Text-to-SQL is a pivotal task that bridges natural language understanding and\nstructured data access, yet it remains fundamentally challenging due to\nsemantic ambiguity and complex compositional reasoning. While large language\nmodels (LLMs) have greatly advanced SQL generation though prompting, supervised\nfinetuning and reinforced tuning, the shift toward test-time scaling exposes a\nnew bottleneck: selecting the correct query from a diverse candidate pool.\nExisting selection approaches, such as self-consistency or best-of-$N$\ndecoding, provide only shallow signals, making them prone to inconsistent\nscoring, fragile reasoning chains, and a failure to capture fine-grained\nsemantic distinctions between closely related SQL candidates. To this end, we\nintroduce JudgeSQL, a principled framework that redefines SQL candidate\nselection through structured reasoning and weighted consensus tournament\nmechanism. JudgeSQL develops a reasoning-based SQL judge model that distills\nreasoning traces with reinforcement learning guided by verifiable rewards,\nenabling accurate and interpretable judgments. Building on this, a weighted\nconsensus tournament integrates explicit reasoning preferences with implicit\ngenerator confidence, yielding selections that are both more reliable and more\nefficient. Extensive experiments on the BIRD benchmark demonstrate that\nJudgeSQL exhibits superior SQL judgment capabilities and good cross-scale\ngeneralization and robustness to generator capacity.",
      "pdf_url": "http://arxiv.org/pdf/2510.15560v1",
      "published": "2025-10-17T11:46:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15560v1",
      "categories": [
        "cs.AI",
        "cs.DB"
      ]
    },
    {
      "title": "KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models",
      "authors": [
        "Dongjun Kim",
        "Chanhee Park",
        "Chanjun Park",
        "Heuiseok Lim"
      ],
      "abstract": "The instruction-following capabilities of large language models (LLMs) are\npivotal for numerous applications, from conversational agents to complex\nreasoning systems. However, current evaluations predominantly focus on English\nmodels, neglecting the linguistic and cultural nuances of other languages.\nSpecifically, Korean, with its distinct syntax, rich morphological features,\nhonorific system, and dual numbering systems, lacks a dedicated benchmark for\nassessing open-ended instruction-following capabilities. To address this gap,\nwe introduce the Korean Instruction-following Task Evaluation (KITE), a\ncomprehensive benchmark designed to evaluate both general and Korean-specific\ninstructions. Unlike existing Korean benchmarks that focus mainly on factual\nknowledge or multiple-choice testing, KITE directly targets diverse, open-ended\ninstruction-following tasks. Our evaluation pipeline combines automated metrics\nwith human assessments, revealing performance disparities across models and\nproviding deeper insights into their strengths and weaknesses. By publicly\nreleasing the KITE dataset and code, we aim to foster further research on\nculturally and linguistically inclusive LLM development and inspire similar\nendeavors for other underrepresented languages.",
      "pdf_url": "http://arxiv.org/pdf/2510.15558v1",
      "published": "2025-10-17T11:45:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15558v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents",
      "authors": [
        "Tingyu Lin",
        "Marco Peer",
        "Florian Kleber",
        "Robert Sablatnig"
      ],
      "abstract": "This paper presents ClapperText, a benchmark dataset for handwritten and\nprinted text recognition in visually degraded and low-resource settings. The\ndataset is derived from 127 World War II-era archival video segments containing\nclapperboards that record structured production metadata such as date,\nlocation, and camera-operator identity. ClapperText includes 9,813 annotated\nframes and 94,573 word-level text instances, 67% of which are handwritten and\n1,566 are partially occluded. Each instance includes transcription, semantic\ncategory, text type, and occlusion status, with annotations available as\nrotated bounding boxes represented as 4-point polygons to support spatially\nprecise OCR applications. Recognizing clapperboard text poses significant\nchallenges, including motion blur, handwriting variation, exposure\nfluctuations, and cluttered backgrounds, mirroring broader challenges in\nhistorical document analysis where structured content appears in degraded,\nnon-standard forms. We provide both full-frame annotations and cropped word\nimages to support downstream tasks. Using a consistent per-video evaluation\nprotocol, we benchmark six representative recognition and seven detection\nmodels under zero-shot and fine-tuned conditions. Despite the small training\nset (18 videos), fine-tuning leads to substantial performance gains,\nhighlighting ClapperText's suitability for few-shot learning scenarios. The\ndataset offers a realistic and culturally grounded resource for advancing\nrobust OCR and document understanding in low-resource archival contexts. The\ndataset and evaluation code are available at\nhttps://github.com/linty5/ClapperText.",
      "pdf_url": "http://arxiv.org/pdf/2510.15557v1",
      "published": "2025-10-17T11:44:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15557v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "Think Parallax: Solving Multi-Hop Problems via Multi-View Knowledge-Graph-Based Retrieval-Augmented Generation",
      "authors": [
        "Jinliang Liu"
      ],
      "abstract": "Large language models (LLMs) excel at language understanding but often\nhallucinate and struggle with multi-hop reasoning. Knowledge-graph-based\nretrieval-augmented generation (KG-RAG) offers grounding, yet most methods rely\non flat embeddings and noisy path exploration. We propose ParallaxRAG, a\nframework that symmetrically decouples queries and graph triples into\nmulti-view spaces, enabling a robust retrieval architecture that explicitly\nenforces head diversity while constraining weakly related paths. Central to our\napproach is the observation that different attention heads specialize in\nsemantic relations at distinct reasoning stages, contributing to different hops\nof the reasoning chain. This specialization allows ParallaxRAG to construct\ncleaner subgraphs and guide LLMs through grounded, step-wise reasoning.\nExperiments on WebQSP and CWQ, under our unified, reproducible setup (BGE-M3 +\nLlama3.1-8B), demonstrate competitive retrieval and QA performance, alongside\nreduced hallucination and good generalization. Our results highlight multi-view\nhead specialization as a principled direction for knowledge-grounded multi-hop\nreasoning. Our implementation will be released as soon as the paper is\naccepted.",
      "pdf_url": "http://arxiv.org/pdf/2510.15552v1",
      "published": "2025-10-17T11:34:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15552v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking Cross-lingual Gaps from a Statistical Viewpoint",
      "authors": [
        "Vihari Piratla",
        "Purvam Jain",
        "Darshan Singh",
        "Partha Talukdar",
        "Trevor Cohn"
      ],
      "abstract": "Any piece of knowledge is usually expressed in one or a handful of natural\nlanguages on the web or in any large corpus. Large Language Models (LLMs) act\nas a bridge by acquiring knowledge from a source language and making it\naccessible when queried from target languages. Prior research has pointed to a\ncross-lingual gap, viz., a drop in accuracy when the knowledge is queried in a\ntarget language compared to when the query is in the source language. Existing\nresearch has rationalized divergence in latent representations in source and\ntarget languages as the source of cross-lingual gap. In this work, we take an\nalternative view and hypothesize that the variance of responses in the target\nlanguage is the main cause of this gap. For the first time, we formalize the\ncross-lingual gap in terms of bias-variance decomposition. We present extensive\nexperimental evidence which support proposed formulation and hypothesis. We\nthen reinforce our hypothesis through multiple inference-time interventions\nthat control the variance and reduce the cross-lingual gap. We demonstrate a\nsimple prompt instruction to reduce the response variance, which improved\ntarget accuracy by 20-25% across different models.",
      "pdf_url": "http://arxiv.org/pdf/2510.15551v1",
      "published": "2025-10-17T11:34:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15551v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors",
      "authors": [
        "Usman Ali",
        "Ali Zia",
        "Waqas Ali",
        "Umer Ramzan",
        "Abdul Rehman",
        "Muhammad Tayyab Chaudhry",
        "Wei Xiang"
      ],
      "abstract": "Reliable induction motor (IM) fault diagnosis is vital for industrial safety\nand operational continuity, mitigating costly unplanned downtime. Conventional\napproaches often struggle to capture complex multimodal signal relationships,\nare constrained to unimodal data or single fault types, and exhibit performance\ndegradation under noisy or cross-domain conditions. This paper proposes the\nMultimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unified\nframework for robust fault diagnosis. To the best of our knowledge, MM-HCAN is\nthe first to integrate contrastive learning within a hypergraph topology\nspecifically designed for multimodal sensor fusion, enabling the joint\nmodelling of intra- and inter-modal dependencies and enhancing generalisation\nbeyond Euclidean embedding spaces. The model facilitates simultaneous diagnosis\nof bearing, stator, and rotor faults, addressing the engineering need for\nconsolidated di- agnostic capabilities. Evaluated on three real-world\nbenchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domain\ngeneralisation and resilience to noise, demonstrating its suitability for\nreal-world deployment. An ablation study validates the contribution of each\ncomponent. MM-HCAN provides a scalable and robust solution for comprehensive\nmulti-fault diagnosis, supporting predictive maintenance and extended asset\nlongevity in industrial environments.",
      "pdf_url": "http://arxiv.org/pdf/2510.15547v1",
      "published": "2025-10-17T11:28:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15547v1",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.SY",
        "eess.SP",
        "eess.SY"
      ]
    },
    {
      "title": "TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs",
      "authors": [
        "Sibo Xiao",
        "Jinyuan Fu",
        "Zhongle Xie",
        "Lidan Shou"
      ],
      "abstract": "Accelerating the inference of large language models (LLMs) has been a\ncritical challenge in generative AI. Speculative decoding (SD) substantially\nimproves LLM inference efficiency. However, its utility is limited by a\nfundamental constraint: the draft and target models must share the same\nvocabulary, thus limiting the herd of available draft models and often\nnecessitating the training of a new model from scratch. Inspired by Dynamic\nTime Warping (DTW), a classic algorithm for aligning time series, we propose\nthe algorithm TokenTiming for universal speculative decoding. It operates by\nre-encoding the draft token sequence to get a new target token sequence, and\nthen uses DTW to build a mapping to transfer the probability distributions for\nspeculative sampling. Benefiting from this, our method accommodates mismatched\nvocabularies and works with any off-the-shelf models without retraining and\nmodification. We conduct comprehensive experiments on various tasks,\ndemonstrating 1.57x speedup. This work enables a universal approach for draft\nmodel selection, making SD a more versatile and practical tool for LLM\nacceleration.",
      "pdf_url": "http://arxiv.org/pdf/2510.15545v1",
      "published": "2025-10-17T11:25:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15545v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval",
      "authors": [
        "Qiyu Wu",
        "Shuyang Cui",
        "Satoshi Hayakawa",
        "Wei-Yao Wang",
        "Hiromi Wakaki",
        "Yuki Mitsufuji"
      ],
      "abstract": "Multimodal retrieval, which seeks to retrieve relevant content across\nmodalities such as text or image, supports applications from AI search to\ncontents production. Despite the success of separate-encoder approaches like\nCLIP align modality-specific embeddings with contrastive learning, recent\nmultimodal large language models (MLLMs) enable a unified encoder that directly\nprocesses composed inputs. While flexible and advanced, we identify that\nunified encoders trained with conventional contrastive learning are prone to\nlearn modality shortcut, leading to poor robustness under distribution shifts.\nWe propose a modality composition awareness framework to mitigate this issue.\nConcretely, a preference loss enforces multimodal embeddings to outperform\ntheir unimodal counterparts, while a composition regularization objective\naligns multimodal embeddings with prototypes composed from its unimodal parts.\nThese objectives explicitly model structural relationships between the composed\nrepresentation and its unimodal counterparts. Experiments on various benchmarks\nshow gains in out-of-distribution retrieval, highlighting modality composition\nawareness as a effective principle for robust composed multimodal retrieval\nwhen utilizing MLLMs as the unified encoder.",
      "pdf_url": "http://arxiv.org/pdf/2510.15543v1",
      "published": "2025-10-17T11:20:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.15543v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.MM"
      ]
    }
  ]
}