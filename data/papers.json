{
  "last_updated": "2025-07-09T00:55:13.302094",
  "papers": [
    {
      "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions",
      "authors": [
        "Yuanzhe Hu",
        "Yu Wang",
        "Julian McAuley"
      ],
      "abstract": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on\nevaluating reasoning, planning, and execution capabilities, while another\ncritical component-memory, encompassing how agents memorize, update, and\nretrieve long-term information-is under-evaluated due to the lack of\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\npaper, we identify four core competencies essential for memory agents: accurate\nretrieval, test-time learning, long-range understanding, and conflict\nresolution. Existing datasets either rely on limited context lengths or are\ntailored for static, long-context settings like book-based QA, which do not\nreflect the interactive, multi-turn nature of memory agents that incrementally\naccumulate information. Furthermore, no existing benchmarks cover all four\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\nspecifically designed for memory agents. Our benchmark combines reformulated\nexisting datasets with newly constructed ones, covering the above four memory\ncompetencies, providing a systematic and challenging testbed for assessing\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\nagents with external memory modules and tool integration. Empirical results\nreveal that current methods fall short of mastering all four competencies,\nunderscoring the need for further research into comprehensive memory mechanisms\nfor LLM agents.",
      "pdf_url": "http://arxiv.org/pdf/2507.05257v1",
      "published": "2025-07-07T17:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05257v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving",
      "authors": [
        "Fabian Konstantinidis",
        "Ariel Dallari Guerreiro",
        "Raphael Trumpp",
        "Moritz Sackmann",
        "Ulrich Hofmann",
        "Marco Caccamo",
        "Christoph Stiller"
      ],
      "abstract": "Accurate motion prediction of surrounding traffic participants is crucial for\nthe safe and efficient operation of automated vehicles in dynamic environments.\nMarginal prediction models commonly forecast each agent's future trajectories\nindependently, often leading to sub-optimal planning decisions for an automated\nvehicle. In contrast, joint prediction models explicitly account for the\ninteractions between agents, yielding socially and physically consistent\npredictions on a scene level. However, existing approaches differ not only in\ntheir problem formulation but also in the model architectures and\nimplementation details used, making it difficult to compare them. In this work,\nwe systematically investigate different approaches to joint motion prediction,\nincluding post-processing of the marginal predictions, explicitly training the\nmodel for joint predictions, and framing the problem as a generative task. We\nevaluate each approach in terms of prediction accuracy, multi-modality, and\ninference efficiency, offering a comprehensive analysis of the strengths and\nlimitations of each approach. Several prediction examples are available at\nhttps://frommarginaltojointpred.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2507.05254v1",
      "published": "2025-07-07T17:58:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05254v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ]
    },
    {
      "title": "Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving",
      "authors": [
        "Elahe Delavari",
        "Feeza Khan Khanzada",
        "Jaerock Kwon"
      ],
      "abstract": "Reinforcement Learning (RL) offers a promising framework for autonomous\ndriving by enabling agents to learn control policies through interaction with\nenvironments. However, large and high-dimensional action spaces often used to\nsupport fine-grained control can impede training efficiency and increase\nexploration costs. In this study, we introduce and evaluate two novel\nstructured action space modification strategies for RL in autonomous driving:\ndynamic masking and relative action space reduction. These approaches are\nsystematically compared against fixed reduction schemes and full action space\nbaselines to assess their impact on policy learning and performance. Our\nframework leverages a multimodal Proximal Policy Optimization agent that\nprocesses both semantic image sequences and scalar vehicle states. The proposed\ndynamic and relative strategies incorporate real-time action masking based on\ncontext and state transitions, preserving action consistency while eliminating\ninvalid or suboptimal choices. Through comprehensive experiments across diverse\ndriving routes, we show that action space reduction significantly improves\ntraining stability and policy performance. The dynamic and relative schemes, in\nparticular, achieve a favorable balance between learning speed, control\nprecision, and generalization. These findings highlight the importance of\ncontext-aware action space design for scalable and reliable RL in autonomous\ndriving tasks.",
      "pdf_url": "http://arxiv.org/pdf/2507.05251v1",
      "published": "2025-07-07T17:58:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05251v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors",
      "authors": [
        "Scott Emmons",
        "Erik Jenner",
        "David K. Elson",
        "Rif A. Saurous",
        "Senthooran Rajamanoharan",
        "Heng Chen",
        "Irhum Shafkat",
        "Rohin Shah"
      ],
      "abstract": "While chain-of-thought (CoT) monitoring is an appealing AI safety defense,\nrecent work on \"unfaithfulness\" has cast doubt on its reliability. These\nfindings highlight an important failure mode, particularly when CoT acts as a\npost-hoc rationalization in applications like auditing for bias. However, for\nthe distinct problem of runtime monitoring to prevent severe harm, we argue the\nkey property is not faithfulness but monitorability. To this end, we introduce\na conceptual framework distinguishing CoT-as-rationalization from\nCoT-as-computation. We expect that certain classes of severe harm will require\ncomplex, multi-step reasoning that necessitates CoT-as-computation. Replicating\nthe experimental setups of prior work, we increase the difficulty of the bad\nbehavior to enforce this necessity condition; this forces the model to expose\nits reasoning, making it monitorable. We then present methodology guidelines to\nstress-test CoT monitoring against deliberate evasion. Applying these\nguidelines, we find that models can learn to obscure their intentions, but only\nwhen given significant help, such as detailed human-written strategies or\niterative optimization against the monitor. We conclude that, while not\ninfallible, CoT monitoring offers a substantial layer of defense that requires\nactive protection and continued stress-testing.",
      "pdf_url": "http://arxiv.org/pdf/2507.05246v1",
      "published": "2025-07-07T17:54:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05246v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration",
      "authors": [
        "Benjamin Li",
        "Shuyang Shi",
        "Lucia Romero",
        "Huao Li",
        "Yaqi Xie",
        "Woojun Kim",
        "Stefanos Nikolaidis",
        "Michael Lewis",
        "Katia Sycara",
        "Simon Stepputtis"
      ],
      "abstract": "In collaborative tasks, being able to adapt to your teammates is a necessary\nrequirement for success. When teammates are heterogeneous, such as in\nhuman-agent teams, agents need to be able to observe, recognize, and adapt to\ntheir human partners in real time. This becomes particularly challenging in\ntasks with time pressure and complex strategic spaces where the dynamics can\nchange rapidly. In this work, we introduce TALENTS, a strategy-conditioned\ncooperator framework that learns to represent, categorize, and adapt to a range\nof partner strategies, enabling ad-hoc teamwork. Our approach utilizes a\nvariational autoencoder to learn a latent strategy space from trajectory data.\nThis latent space represents the underlying strategies that agents employ.\nSubsequently, the system identifies different types of strategy by clustering\nthe data. Finally, a cooperator agent is trained to generate partners for each\ntype of strategy, conditioned on these clusters. In order to adapt to\npreviously unseen partners, we leverage a fixed-share regret minimization\nalgorithm that infers and adjusts the estimated partner strategy dynamically.\nWe assess our approach in a customized version of the Overcooked environment,\nposing a challenging cooperative cooking task that demands strong coordination\nacross a wide range of possible strategies. Using an online user study, we show\nthat our agent outperforms current baselines when working with unfamiliar human\npartners.",
      "pdf_url": "http://arxiv.org/pdf/2507.05244v1",
      "published": "2025-07-07T17:53:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05244v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
      "authors": [
        "Jingyi Chai",
        "Shuo Tang",
        "Rui Ye",
        "Yuwen Du",
        "Xinyu Zhu",
        "Mengcheng Zhou",
        "Yanfeng Wang",
        "Weinan E",
        "Yuzhi Zhang",
        "Linfeng Zhang",
        "Siheng Chen"
      ],
      "abstract": "The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training.",
      "pdf_url": "http://arxiv.org/pdf/2507.05241v2",
      "published": "2025-07-07T17:50:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05241v2",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "CTA: Cross-Task Alignment for Better Test Time Training",
      "authors": [
        "Samuel Barbeau",
        "Pedram Fekri",
        "David Osowiechi",
        "Ali Bahri",
        "Moslem Yazdanpanah",
        "Masih Aminbeidokhti",
        "Christian Desrosiers"
      ],
      "abstract": "Deep learning models have demonstrated exceptional performance across a wide\nrange of computer vision tasks. However, their performance often degrades\nsignificantly when faced with distribution shifts, such as domain or dataset\nchanges. Test-Time Training (TTT) has emerged as an effective method to enhance\nmodel robustness by incorporating an auxiliary unsupervised task during\ntraining and leveraging it for model updates at test time. In this work, we\nintroduce CTA (Cross-Task Alignment), a novel approach for improving TTT.\nUnlike existing TTT methods, CTA does not require a specialized model\narchitecture and instead takes inspiration from the success of multi-modal\ncontrastive learning to align a supervised encoder with a self-supervised one.\nThis process enforces alignment between the learned representations of both\nmodels, thereby mitigating the risk of gradient interference, preserving the\nintrinsic robustness of self-supervised learning and enabling more semantically\nmeaningful updates at test-time. Experimental results demonstrate substantial\nimprovements in robustness and generalization over the state-of-the-art on\nseveral benchmark datasets.",
      "pdf_url": "http://arxiv.org/pdf/2507.05221v2",
      "published": "2025-07-07T17:33:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05221v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
      "authors": [
        "Zongyan Han",
        "Mohamed El Amine Boudjoghra",
        "Jiahua Dong",
        "Jinhong Wang",
        "Rao Muhammad Anwer"
      ],
      "abstract": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.",
      "pdf_url": "http://arxiv.org/pdf/2507.05211v1",
      "published": "2025-07-07T17:22:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05211v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MedGemma Technical Report",
      "authors": [
        "Andrew Sellergren",
        "Sahar Kazemzadeh",
        "Tiam Jaroensri",
        "Atilla Kiraly",
        "Madeleine Traverse",
        "Timo Kohlberger",
        "Shawn Xu",
        "Fayaz Jamil",
        "Cían Hughes",
        "Charles Lau",
        "Justin Chen",
        "Fereshteh Mahvar",
        "Liron Yatziv",
        "Tiffany Chen",
        "Bram Sterling",
        "Stefanie Anna Baby",
        "Susanna Maria Baby",
        "Jeremy Lai",
        "Samuel Schmidgall",
        "Lu Yang",
        "Kejia Chen",
        "Per Bjornsson",
        "Shashir Reddy",
        "Ryan Brush",
        "Kenneth Philbrick",
        "Howard Hu",
        "Howard Yang",
        "Richa Tiwari",
        "Sunny Jansen",
        "Preeti Singh",
        "Yun Liu",
        "Shekoofeh Azizi",
        "Aishwarya Kamath",
        "Johan Ferret",
        "Shreya Pathak",
        "Nino Vieillard",
        "Ramona Merhej",
        "Sarah Perrin",
        "Tatiana Matejovicova",
        "Alexandre Ramé",
        "Morgane Riviere",
        "Louis Rouillard",
        "Thomas Mesnard",
        "Geoffrey Cideron",
        "Jean-bastien Grill",
        "Sabela Ramos",
        "Edouard Yvinec",
        "Michelle Casbon",
        "Elena Buchatskaya",
        "Jean-Baptiste Alayrac",
        "Dmitry",
        "Lepikhin",
        "Vlad Feinberg",
        "Sebastian Borgeaud",
        "Alek Andreev",
        "Cassidy Hardin",
        "Robert Dadashi",
        "Léonard Hussenot",
        "Armand Joulin",
        "Olivier Bachem",
        "Yossi Matias",
        "Katherine Chou",
        "Avinatan Hassidim",
        "Kavi Goel",
        "Clement Farabet",
        "Joelle Barral",
        "Tris Warkentin",
        "Jonathon Shlens",
        "David Fleet",
        "Victor Cotruta",
        "Omar Sanseviero",
        "Gus Martins",
        "Phoebe Kirk",
        "Anand Rao",
        "Shravya Shetty",
        "David F. Steiner",
        "Can Kirmizibayrak",
        "Rory Pilgrim",
        "Daniel Golden",
        "Lin Yang"
      ],
      "abstract": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.",
      "pdf_url": "http://arxiv.org/pdf/2507.05201v1",
      "published": "2025-07-07T17:01:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05201v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling",
      "authors": [
        "Boyuan Wang",
        "Xinpan Meng",
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Angen Ye",
        "Yang Wang",
        "Zhiqin Yang",
        "Chaojun Ni",
        "Guan Huang",
        "Xingang Wang"
      ],
      "abstract": "The rapid advancement of Embodied AI has led to an increasing demand for\nlarge-scale, high-quality real-world data. However, collecting such embodied\ndata remains costly and inefficient. As a result, simulation environments have\nbecome a crucial surrogate for training robot policies. Yet, the significant\nReal2Sim2Real gap remains a critical bottleneck, particularly in terms of\nphysical dynamics and visual appearance. To address this challenge, we propose\nEmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both\nthe physics and appearance perspectives. Specifically, we propose PhysAligner,\na differentiable physics module designed to reduce the Real2Sim physical gap.\nIt jointly optimizes robot-specific parameters such as control gains and\nfriction coefficients to better align simulated dynamics with real-world\nobservations. In addition, we introduce VisAligner, which incorporates a\nconditional video diffusion model to bridge the Sim2Real appearance gap by\ntranslating low-fidelity simulated renderings into photorealistic videos\nconditioned on simulation states, enabling high-fidelity visual transfer.\nExtensive experiments validate the effectiveness of EmbodieDreamer. The\nproposed PhysAligner reduces physical parameter estimation error by 3.74%\ncompared to simulated annealing methods while improving optimization speed by\n89.91\\%. Moreover, training robot policies in the generated photorealistic\nenvironment leads to a 29.17% improvement in the average task success rate\nacross real-world tasks after reinforcement learning. Code, model and data will\nbe publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2507.05198v1",
      "published": "2025-07-07T16:58:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05198v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Train-before-Test Harmonizes Language Model Rankings",
      "authors": [
        "Guanhua Zhang",
        "Ricardo Dominguez-Olmedo",
        "Moritz Hardt"
      ],
      "abstract": "Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. Recent work attributed ranking\ndisagreement to the phenomenon of training on the test task: As released,\ndifferent models exhibit a different level of preparation for any given test\ntask. A candidate solution to the problem is train-before-test: Give each model\nthe same benchmark-specific finetuning before evaluation. Our primary\ncontribution is a broad empirical evaluation of train-before-test across 24\nbenchmarks and 61 models. We show that train-before-test significantly improves\nranking agreement consistently across all benchmarks. Whereas rankings have\nlittle external validity to start with, they enjoy a significant degree of\nexternal validity when applying train-before-test: Model rankings transfer\ngracefully from one benchmark to the other. Even within the same model family,\ntrain-before-test reduces strong ranking disagreement to near-perfect\nagreement. In addition, train-before-test reduces the model-score matrix to\nessentially rank one, revealing new insights into the latent factors of\nbenchmark performance. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking.",
      "pdf_url": "http://arxiv.org/pdf/2507.05195v1",
      "published": "2025-07-07T16:54:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05195v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Infrastructuring Contestability: A Framework for Community-Defined AI Value Pluralism",
      "authors": [
        "Andreas Mayer"
      ],
      "abstract": "The proliferation of AI-driven systems presents a fundamental challenge to\nHuman-Computer Interaction (HCI) and Computer-Supported Cooperative Work\n(CSCW), often diminishing user agency and failing to account for value\npluralism. Current approaches to value alignment, which rely on centralized,\ntop-down definitions, lack the mechanisms for meaningful contestability. This\nleaves users and communities unable to challenge or shape the values embedded\nin the systems that govern their digital lives, creating a crisis of legitimacy\nand trust. This paper introduces Community-Defined AI Value Pluralism (CDAVP),\na socio-technical framework that addresses this gap. It reframes the design\nproblem from achieving a single aligned state to infrastructuring a dynamic\necosystem for value deliberation and application. At its core, CDAVP enables\ndiverse, self-organizing communities to define and maintain explicit value\nprofiles - rich, machine-readable representations that can encompass not only\npreferences but also community-specific rights and duties. These profiles are\nthen contextually activated by the end-user, who retains ultimate control\n(agency) over which values guide the AI's behavior. AI applications, in turn,\nare designed to transparently interpret these profiles and moderate conflicts,\nadhering to a set of non-negotiable, democratically-legitimated meta-rules. The\ndesigner's role shifts from crafting static interfaces to becoming an architect\nof participatory ecosystems. We argue that infrastructuring for pluralism is a\nnecessary pathway toward achieving robust algorithmic accountability and\ngenuinely contestable, human-centric AI.",
      "pdf_url": "http://arxiv.org/pdf/2507.05187v1",
      "published": "2025-07-07T16:45:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05187v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale",
      "authors": [
        "Jonathan Hyun",
        "Nicholas R Waytowich",
        "Boyuan Chen"
      ],
      "abstract": "Despite rapid progress in large language model (LLM)-based multi-agent\nsystems, current benchmarks fall short in evaluating their scalability,\nrobustness, and coordination capabilities in complex, dynamic, real-world\ntasks. Existing environments typically focus on small-scale, fully observable,\nor low-complexity domains, limiting their utility for developing and assessing\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\nan open-source benchmark designed to close this gap. Built atop the human-AI\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\nobservability, stochastic dynamics, and long-horizon planning objectives. The\nenvironment supports both low-level control and high-level natural language\ninteractions through modular Perception and Execution modules. We implement and\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\nuncovering significant performance gaps that highlight the unsolved challenges\nin large-scale coordination, communication, spatial reasoning, and long-horizon\nplanning under uncertainty. By providing more realistic complexity, scalable\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\ncritical foundation for advancing research in scalable multi-agent Agentic\nintelligence. All code, environments, data, and baselines will be released to\nsupport future research in this emerging domain.",
      "pdf_url": "http://arxiv.org/pdf/2507.05178v1",
      "published": "2025-07-07T16:33:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05178v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech Language Model",
      "authors": [
        "Chen Wang",
        "Tianyu Peng",
        "Wen Yang",
        "Yinan Bai",
        "Guangfu Wang",
        "Jun Lin",
        "Lanpeng Jia",
        "Lingxiang Wu",
        "Jinqiao Wang",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "abstract": "Empathetic interaction is a cornerstone of human-machine communication, due\nto the need for understanding speech enriched with paralinguistic cues and\ngenerating emotional and expressive responses. However, the most powerful\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\nthe architecture, data and development opaque to researchers. Given the\ncritical need for transparent research into the LSLMs and empathetic behavior,\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\ndesigned to enable empathetic speech interactions. Based on our empathetic\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\ndecoding architecture to achieve low-latency speech generation. To facilitate\nend-to-end training, OpenS2S incorporates an automated data construction\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\nlow cost. By leveraging large language models to generate empathetic content\nand controllable text-to-speech systems to introduce speaker and emotional\nvariation, we construct a scalable training corpus with rich paralinguistic\ndiversity and minimal human supervision. We release the fully open-source\nOpenS2S model, including the dataset, model weights, pre-training and\nfine-tuning codes, to empower the broader research community and accelerate\ninnovation in empathetic speech systems. The project webpage can be accessed at\nhttps://casia-lm.github.io/OpenS2S",
      "pdf_url": "http://arxiv.org/pdf/2507.05177v2",
      "published": "2025-07-07T16:31:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05177v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Critiques of World Models",
      "authors": [
        "Eric Xing",
        "Mingkai Deng",
        "Jinyu Hou",
        "Zhiting Hu"
      ],
      "abstract": "World Model, the supposed algorithmic surrogate of the real-world environment\nwhich biological agents experience with and act upon, has been an emerging\ntopic in recent years because of the rising needs to develop virtual agents\nwith artificial (general) intelligence. There has been much debate on what a\nworld model really is, how to build it, how to use it, and how to evaluate it.\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\nand drawing inspiration from the concept of \"hypothetical thinking\" in\npsychology literature, we offer critiques of several schools of thoughts on\nworld modeling, and argue the primary goal of a world model to be simulating\nall actionable possibilities of the real world for purposeful reasoning and\nacting. Building on the critiques, we propose a new architecture for a\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\ncontinuous/discrete representations, and a generative and self-supervision\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\nAGI system enabled by such a model.",
      "pdf_url": "http://arxiv.org/pdf/2507.05169v1",
      "published": "2025-07-07T16:23:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05169v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains",
      "authors": [
        "Nicholas Chivaran",
        "Jianbing Ni"
      ],
      "abstract": "The recent proliferation of photorealistic AI-generated images (AIGI) has\nraised urgent concerns about their potential misuse, particularly on social\nmedia platforms. Current state-of-the-art AIGI detection methods typically rely\non large, deep neural architectures, creating significant computational\nbarriers to real-time, large-scale deployment on platforms like social media.\nTo challenge this reliance on computationally intensive models, we introduce\nLAID, the first framework -- to our knowledge -- that benchmarks and evaluates\nthe detection performance and efficiency of off-the-shelf lightweight neural\nnetworks. In this framework, we comprehensively train and evaluate selected\nmodels on a representative subset of the GenImage dataset across spatial,\nspectral, and fusion image domains. Our results demonstrate that lightweight\nmodels can achieve competitive accuracy, even under adversarial conditions,\nwhile incurring substantially lower memory and computation costs compared to\ncurrent state-of-the-art methods. This study offers valuable insight into the\ntrade-off between efficiency and performance in AIGI detection and lays a\nfoundation for the development of practical, scalable, and trustworthy\ndetection systems. The source code of LAID can be found at:\nhttps://github.com/nchivar/LAID.",
      "pdf_url": "http://arxiv.org/pdf/2507.05162v1",
      "published": "2025-07-07T16:18:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05162v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models",
      "authors": [
        "Chinnappa Guggilla",
        "Budhaditya Roy",
        "Trupti Ramdas Chavan",
        "Abdul Rahman",
        "Edward Bowen"
      ],
      "abstract": "Large Language Models (LLMs) possess an extraordinary capability to produce\ntext that is not only coherent and contextually relevant but also strikingly\nsimilar to human writing. They adapt to various styles and genres, producing\ncontent that is both grammatically correct and semantically meaningful.\nRecently, LLMs have been misused to create highly realistic phishing emails,\nspread fake news, generate code to automate cyber crime, and write fraudulent\nscientific articles. Additionally, in many real-world applications, the\ngenerated content including style and topic and the generator model are not\nknown beforehand. The increasing prevalence and sophistication of artificial\nintelligence (AI)-generated texts have made their detection progressively more\nchallenging. Various attempts have been made to distinguish machine-generated\ntext from human-authored content using linguistic, statistical, machine\nlearning, and ensemble-based approaches. This work focuses on two primary\nobjectives Task-A, which involves distinguishing human-written text from\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\nmodel responsible for the generation. Both of these tasks are based on fine\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.",
      "pdf_url": "http://arxiv.org/pdf/2507.05157v1",
      "published": "2025-07-07T16:13:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05157v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Effects of Unplanned Incoming Flights on Airport Relief Processes after a Major Natural Disaster",
      "authors": [
        "Luka Van de Sype",
        "Matthieu Vert",
        "Alexei Sharpanskykh",
        "Seyed Sahand Mohammadi Ziabari"
      ],
      "abstract": "The severity of natural disasters is increasing every year, impacting many\npeople's lives. During the response phase of disasters, airports are important\nhubs where relief aid arrives and people need to be evacuated. However, the\nairport often forms a bottleneck in these relief operations due to the sudden\nneed for increased capacity. Limited research has been done on the operational\nside of airport disaster management. Experts identify the main problems as,\nfirst, the asymmetry of information between the airport and incoming flights,\nand second, the lack of resources. The goal of this research is to understand\nthe effects of incomplete knowledge of incoming flights with different resource\nallocation strategies on the performance of cargo handling operations at an\nairport after a natural disaster. An agent-based model is created, implementing\nrealistic offloading strategies with different degrees of information\nuncertainty. Model calibration and verification are performed with experts in\nthe field. The model performance is measured by the average turnaround time,\nwhich is divided into offloading time, boarding time, and cumulative waiting\ntimes. The results show that the effects of one unplanned aircraft are\nnegligible. However, all waiting times increase with more arriving unplanned\naircraft.",
      "pdf_url": "http://arxiv.org/pdf/2507.05150v1",
      "published": "2025-07-07T16:00:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05150v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "OGF: An Online Gradient Flow Method for Optimizing the Statistical Steady-State Time Averages of Unsteady Turbulent Flows",
      "authors": [
        "Tom Hickling",
        "Jonathan F. MacArt",
        "Justin Sirignano",
        "Den Waidmann"
      ],
      "abstract": "Turbulent flows are chaotic and unsteady, but their statistical distribution\nconverges to a statistical steady state. Engineering quantities of interest\ntypically take the form of time-average statistics such as $ \\frac{1}{t}\n\\int_0^t f ( u(x,\\tau; \\theta) ) d\\tau \\overset{t \\rightarrow\n\\infty}{\\rightarrow} F(x; \\theta)$, where $u(x,t; \\theta)$ are solutions of the\nNavier--Stokes equations with parameters $\\theta$. Optimizing over $F(x;\n\\theta)$ has many engineering applications including geometric optimization,\nflow control, and closure modeling. However, this remains an open challenge, as\nexisting computational approaches are incapable of scaling to physically\nrepresentative numbers of grid points. The fundamental obstacle is the\nchaoticity of turbulent flows: gradients calculated with the adjoint method\ndiverge exponentially as $t \\rightarrow \\infty$.\n  We develop a new online gradient-flow (OGF) method that is scalable to large\ndegree-of-freedom systems and enables optimizing for the steady-state\nstatistics of chaotic, unsteady, turbulence-resolving simulations. The method\nforward-propagates an online estimate for the gradient of $F(x; \\theta)$ while\nsimultaneously performing online updates of the parameters $\\theta$. A key\nfeature is the fully online nature of the algorithm to facilitate faster\noptimization progress and its combination with a finite-difference estimator to\navoid the divergence of gradients due to chaoticity. The proposed OGF method is\ndemonstrated for optimizations over three chaotic ordinary and partial\ndifferential equations: the Lorenz-63 equation, the Kuramoto--Sivashinsky\nequation, and Navier--Stokes solutions of compressible, forced, homogeneous\nisotropic turbulence. In each case, the OGF method successfully reduces the\nloss based on $F(x; \\theta)$ by several orders of magnitude and accurately\nrecovers the optimal parameters.",
      "pdf_url": "http://arxiv.org/pdf/2507.05149v1",
      "published": "2025-07-07T16:00:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05149v1",
      "categories": [
        "physics.flu-dyn",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "GIST: Cross-Domain Click-Through Rate Prediction via Guided Content-Behavior Distillation",
      "authors": [
        "Wei Xu",
        "Haoran Li",
        "Baoyuan Ou",
        "Lai Xu",
        "Yingjie Qin",
        "Ruilong Su",
        "Ruiwen Xu"
      ],
      "abstract": "Cross-domain Click-Through Rate prediction aims to tackle the data sparsity\nand the cold start problems in online advertising systems by transferring\nknowledge from source domains to a target domain. Most existing methods rely on\noverlapping users to facilitate this transfer, often focusing on joint training\nor pre-training with fine-tuning approach to connect the source and target\ndomains. However, in real-world industrial settings, joint training struggles\nto learn optimal representations with different distributions, and pre-training\nwith fine-tuning is not well-suited for continuously integrating new data. To\naddress these issues, we propose GIST, a cross-domain lifelong sequence model\nthat decouples the training processes of the source and target domains. Unlike\nprevious methods that search lifelong sequences in the source domains using\nonly content or behavior signals or their simple combinations, we innovatively\nintroduce a Content-Behavior Joint Training Module (CBJT), which aligns\ncontent-behavior distributions and combines them with guided information to\nfacilitate a more stable representation. Furthermore, we develop an Asymmetric\nSimilarity Integration strategy (ASI) to augment knowledge transfer through\nsimilarity computation. Extensive experiments demonstrate the effectiveness of\nGIST, surpassing SOTA methods on offline evaluations and an online A/B test.\nDeployed on the Xiaohongshu (RedNote) platform, GIST effectively enhances\nonline ads system performance at scale, serving hundreds of millions of daily\nactive users.",
      "pdf_url": "http://arxiv.org/pdf/2507.05142v1",
      "published": "2025-07-07T15:51:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05142v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization",
      "authors": [
        "Jaewook Lee",
        "Alexander Scarlatos",
        "Andrew Lan"
      ],
      "abstract": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.",
      "pdf_url": "http://arxiv.org/pdf/2507.05137v1",
      "published": "2025-07-07T15:49:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05137v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques",
      "authors": [
        "Walid Mohamed Aly",
        "Taysir Hassan A. Soliman",
        "Amr Mohamed AbdelAziz"
      ],
      "abstract": "Large Language Models (LLMs) continue to advance natural language processing\nwith their ability to generate human-like text across a range of tasks. Despite\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\nperformance in text summarization across various domains and datasets has not\nbeen comprehensively evaluated. At the same time, the ability to summarize text\neffectively without relying on extensive training data has become a crucial\nbottleneck. To address these issues, we present a systematic evaluation of six\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\nand ArXiv (scientific). By leveraging prompt engineering techniques including\nzero-shot and in-context learning, our study evaluates the performance using\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\ntimes is conducted to better understand the trade-off between summarization\nquality and computational efficiency. For Long documents, introduce a\nsentence-based chunking strategy that enables LLMs with shorter context windows\nto summarize extended inputs in multiple stages. The findings reveal that while\nLLMs perform competitively on news and dialog tasks, their performance on long\nscientific documents improves significantly when aided by chunking strategies.\nIn addition, notable performance variations were observed based on model\nparameters, dataset properties, and prompt design. These results offer\nactionable insights into how different LLMs behave across task types,\ncontributing to ongoing research in efficient, instruction-based NLP systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.05123v1",
      "published": "2025-07-07T15:34:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05123v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LVM4CSI: Enabling Direct Application of Pre-Trained Large Vision Models for Wireless Channel Tasks",
      "authors": [
        "Jiajia Guo",
        "Peiwen Jiang",
        "Chao-Kai Wen",
        "Shi Jin",
        "Jun Zhang"
      ],
      "abstract": "Accurate channel state information (CSI) is critical to the performance of\nwireless communication systems, especially with the increasing scale and\ncomplexity introduced by 5G and future 6G technologies. While artificial\nintelligence (AI) offers a promising approach to CSI acquisition and\nutilization, existing methods largely depend on task-specific neural networks\n(NNs) that require expert-driven design and large training datasets, limiting\ntheir generalizability and practicality. To address these challenges, we\npropose LVM4CSI, a general and efficient framework that leverages the\nstructural similarity between CSI and computer vision (CV) data to directly\napply large vision models (LVMs) pre-trained on extensive CV datasets to\nwireless tasks without any fine-tuning, in contrast to large language\nmodel-based methods that generally necessitate fine-tuning. LVM4CSI maps CSI\ntasks to analogous CV tasks, transforms complex-valued CSI into visual formats\ncompatible with LVMs, and integrates lightweight trainable layers to adapt\nextracted features to specific communication objectives. We validate LVM4CSI\nthrough three representative case studies, including channel estimation, human\nactivity recognition, and user localization. Results demonstrate that LVM4CSI\nachieves comparable or superior performance to task-specific NNs, including an\nimprovement exceeding 9.61 dB in channel estimation and approximately 40%\nreduction in localization error. Furthermore, it significantly reduces the\nnumber of trainable parameters and eliminates the need for task-specific NN\ndesign.",
      "pdf_url": "http://arxiv.org/pdf/2507.05121v1",
      "published": "2025-07-07T15:33:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05121v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "math.IT"
      ]
    },
    {
      "title": "VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots",
      "authors": [
        "Danil S. Grigorev",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "In the field of robotics, researchers face a critical challenge in ensuring\nreliable and efficient task planning. Verifying high-level task plans before\nexecution significantly reduces errors and enhance the overall performance of\nthese systems. In this paper, we propose an architecture for automatically\nverifying high-level task plans before their execution in simulator or\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\nconsists of two key steps: first, the conversion of natural language\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\nanalysis of action sequences. The module uses the reasoning capabilities of the\nLLM to evaluate logical coherence and identify potential gaps in the plan.\nRigorous testing on datasets of varying complexity demonstrates the broad\napplicability of the module to household tasks. We contribute to improving the\nreliability and efficiency of task planning and addresses the critical need for\nrobust pre-execution verification in autonomous systems. The code is available\nat https://verifyllm.github.io.",
      "pdf_url": "http://arxiv.org/pdf/2507.05118v1",
      "published": "2025-07-07T15:31:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05118v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift",
      "authors": [
        "Shixuan Liu",
        "Yue He",
        "Yunfei Wang",
        "Hao Zou",
        "Haoxiang Cheng",
        "Wenjing Yang",
        "Peng Cui",
        "Zhong Liu"
      ],
      "abstract": "Knowledge graph (KG) reasoning remains a critical research area focused on\ninferring missing knowledge by analyzing relationships among observed facts.\nDespite its success, a key limitation of existing KG reasoning methods is their\ndependence on the I.I.D assumption. This assumption can easily be violated due\nto unknown sample selection bias during training or agnostic distribution\nshifts during testing, significantly compromising model performance and\nreliability. To facilitate the deployment of KG reasoning in wild environments,\nthis study investigates learning logical rules from KGs affected by unknown\nselection bias. Additionally, we address test sets with agnostic distribution\nshifts, formally defining this challenge as out-of-distribution (OOD) KG\nreasoning-a previously underexplored problem. To solve the issue, we propose\nthe Stable Rule Learning (StableRule) framework, an end-to-end methodology that\nintegrates feature decorrelation with rule learning network, to enhance OOD\ngeneralization performance. By leveraging feature decorrelation, the StableRule\nframework mitigates the adverse effects of covariate shifts arising in OOD\nscenarios, thereby improving the robustness of the rule learning component in\neffectively deriving logical rules. Extensive experiments on seven benchmark\nKGs demonstrate the framework's superior effectiveness and stability across\ndiverse heterogeneous environments, underscoring its practical significance for\nreal-world applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.05110v2",
      "published": "2025-07-07T15:27:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05110v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration",
      "authors": [
        "Yuyi Zhang",
        "Peirong Zhang",
        "Zhenhua Yang",
        "Pengyu Yan",
        "Yongxin Shi",
        "Pengwei Liu",
        "Fengjun Guo",
        "Lianwen Jin"
      ],
      "abstract": "Historical documents represent an invaluable cultural heritage, yet have\nundergone significant degradation over time through tears, water erosion, and\noxidation. Existing Historical Document Restoration (HDR) methods primarily\nfocus on single modality or limited-size restoration, failing to meet practical\nneeds. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel\nautomated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and\n6,543 synthetic images with character-level and line-level locations, as well\nas character annotations in different damage grades. AutoHDR mimics historians'\nrestoration workflows through a three-stage approach: OCR-assisted damage\nlocalization, vision-language context text prediction, and patch autoregressive\nappearance restoration. The modular architecture of AutoHDR enables seamless\nhuman-machine collaboration, allowing for flexible intervention and\noptimization at each restoration stage. Experiments demonstrate AutoHDR's\nremarkable performance in HDR. When processing severely damaged documents, our\nmethod improves OCR accuracy from 46.83\\% to 84.05\\%, with further enhancement\nto 94.25\\% through human-machine collaboration. We believe this work represents\na significant advancement in automated historical document restoration and\ncontributes substantially to cultural heritage preservation. The model and\ndataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.",
      "pdf_url": "http://arxiv.org/pdf/2507.05108v1",
      "published": "2025-07-07T15:26:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05108v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs",
      "authors": [
        "Xinzhe Zheng",
        "Hao Du",
        "Fanding Xu",
        "Jinzhe Li",
        "Zhiyuan Liu",
        "Wenkang Wang",
        "Tao Chen",
        "Wanli Ouyang",
        "Stan Z. Li",
        "Yan Lu",
        "Nanqing Dong",
        "Yang Zhang"
      ],
      "abstract": "Deep learning-based computational methods have achieved promising results in\npredicting protein-protein interactions (PPIs). However, existing benchmarks\npredominantly focus on isolated pairwise evaluations, overlooking a model's\ncapability to reconstruct biologically meaningful PPI networks, which is\ncrucial for biology research. To address this gap, we introduce PRING, the\nfirst comprehensive benchmark that evaluates protein-protein interaction\nprediction from a graph-level perspective. PRING curates a high-quality,\nmulti-species PPI network dataset comprising 21,484 proteins and 186,818\ninteractions, with well-designed strategies to address both data redundancy and\nleakage. Building on this golden-standard dataset, we establish two\ncomplementary evaluation paradigms: (1) topology-oriented tasks, which assess\nintra and cross-species PPI network construction, and (2) function-oriented\ntasks, including protein complex pathway prediction, GO module analysis, and\nessential protein justification. These evaluations not only reflect the model's\ncapability to understand the network topology but also facilitate protein\nfunction annotation, biological module detection, and even disease mechanism\nanalysis. Extensive experiments on four representative model categories,\nconsisting of sequence similarity-based, naive sequence-based, protein language\nmodel-based, and structure-based approaches, demonstrate that current PPI\nmodels have potential limitations in recovering both structural and functional\nproperties of PPI networks, highlighting the gap in supporting real-world\nbiological applications. We believe PRING provides a reliable platform to guide\nthe development of more effective PPI prediction models for the community. The\ndataset and source code of PRING are available at\nhttps://github.com/SophieSarceau/PRING.",
      "pdf_url": "http://arxiv.org/pdf/2507.05101v1",
      "published": "2025-07-07T15:21:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05101v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.MN"
      ]
    },
    {
      "title": "Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance",
      "authors": [
        "Tobias Demmler",
        "Jakob Häringer",
        "Andreas Tamke",
        "Thao Dang",
        "Alexander Hegai",
        "Lars Mikelsons"
      ],
      "abstract": "Accurate trajectory prediction is critical for safe autonomous navigation,\nyet the impact of dataset design on model performance remains understudied.\nThis work systematically examines how feature selection, cross-dataset\ntransfer, and geographic diversity influence trajectory prediction accuracy in\nmulti-agent settings. We evaluate a state-of-the-art model using our novel L4\nMotion Forecasting dataset based on our own data recordings in Germany and the\nUS. This includes enhanced map and agent features. We compare our dataset to\nthe US-centric Argoverse 2 benchmark. First, we find that incorporating\nsupplementary map and agent features unique to our dataset, yields no\nmeasurable improvement over baseline features, demonstrating that modern\narchitectures do not need extensive feature sets for optimal performance. The\nlimited features of public datasets are sufficient to capture convoluted\ninteractions without added complexity. Second, we perform cross-dataset\nexperiments to evaluate how effective domain knowledge can be transferred\nbetween datasets. Third, we group our dataset by country and check the\nknowledge transfer between different driving cultures.",
      "pdf_url": "http://arxiv.org/pdf/2507.05098v1",
      "published": "2025-07-07T15:18:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05098v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders",
      "authors": [
        "Alberto Castagnaro",
        "Umberto Salviati",
        "Mauro Conti",
        "Luca Pajola",
        "Simeone Pizzi"
      ],
      "abstract": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations.",
      "pdf_url": "http://arxiv.org/pdf/2507.05093v1",
      "published": "2025-07-07T15:13:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05093v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "How Rules Represent Causal Knowledge: Causal Modeling with Abductive Logic Programs",
      "authors": [
        "Kilian Rückschloß",
        "Felix Weitkämper"
      ],
      "abstract": "Pearl observes that causal knowledge enables predicting the effects of\ninterventions, such as actions, whereas descriptive knowledge only permits\ndrawing conclusions from observation. This paper extends Pearl's approach to\ncausality and interventions to the setting of stratified abductive logic\nprograms. It shows how stable models of such programs can be given a causal\ninterpretation by building on philosophical foundations and recent work by\nBochman and Eelink et al. In particular, it provides a translation of abductive\nlogic programs into causal systems, thereby clarifying the informal causal\nreading of logic program rules and supporting principled reasoning about\nexternal actions. The main result establishes that the stable model semantics\nfor stratified programs conforms to key philosophical principles of causation,\nsuch as causal sufficiency, natural necessity, and irrelevance of unobserved\neffects. This justifies the use of stratified abductive logic programs as a\nframework for causal modeling and for predicting the effects of interventions",
      "pdf_url": "http://arxiv.org/pdf/2507.05088v1",
      "published": "2025-07-07T15:12:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05088v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Sequential Attention-based Sampling for Histopathological Analysis",
      "authors": [
        "Tarun G",
        "Naman Malpani",
        "Gugan Thoppe",
        "Sridharan Devarajan"
      ],
      "abstract": "Deep neural networks are increasingly applied for automated histopathology.\nYet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering\nit computationally infeasible to analyze them entirely at high resolution.\nDiagnostic labels are largely available only at the slide-level, because expert\nannotation of images at a finer (patch) level is both laborious and expensive.\nMoreover, regions with diagnostic information typically occupy only a small\nfraction of the WSI, making it inefficient to examine the entire slide at full\nresolution. Here, we propose SASHA -- {\\it S}equential {\\it A}ttention-based\n{\\it S}ampling for {\\it H}istopathological {\\it A}nalysis -- a deep\nreinforcement learning approach for efficient analysis of histopathological\nimages. First, SASHA learns informative features with a lightweight\nhierarchical, attention-based multiple instance learning (MIL) model. Second,\nSASHA samples intelligently and zooms selectively into a small fraction\n(10-20\\%) of high-resolution patches, to achieve reliable diagnosis. We show\nthat SASHA matches state-of-the-art methods that analyze the WSI fully at\nhigh-resolution, albeit at a fraction of their computational and memory costs.\nIn addition, it significantly outperforms competing, sparse sampling methods.\nWe propose SASHA as an intelligent sampling model for medical imaging\nchallenges that involve automated diagnosis with exceptionally large images\ncontaining sparsely informative features.",
      "pdf_url": "http://arxiv.org/pdf/2507.05077v1",
      "published": "2025-07-07T15:03:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05077v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "ICAS: Detecting Training Data from Autoregressive Image Generative Models",
      "authors": [
        "Hongyao Yu",
        "Yixiang Qiu",
        "Yiheng Yang",
        "Hao Fang",
        "Tianqu Zhuang",
        "Jiaxin Hong",
        "Bin Chen",
        "Hao Wu",
        "Shu-Tao Xia"
      ],
      "abstract": "Autoregressive image generation has witnessed rapid advancements, with\nprominent models such as scale-wise visual auto-regression pushing the\nboundaries of visual synthesis. However, these developments also raise\nsignificant concerns regarding data privacy and copyright. In response,\ntraining data detection has emerged as a critical task for identifying\nunauthorized data usage in model training. To better understand the\nvulnerability of autoregressive image generative models to such detection, we\nconduct the first study applying membership inference to this domain. Our\napproach comprises two key components: implicit classification and an adaptive\nscore aggregation strategy. First, we compute the implicit token-wise\nclassification score within the query image. Then we propose an adaptive score\naggregation strategy to acquire a final score, which places greater emphasis on\nthe tokens with lower scores. A higher final score indicates that the sample is\nmore likely to be involved in the training set. To validate the effectiveness\nof our method, we adapt existing detection algorithms originally designed for\nLLMs to visual autoregressive models. Extensive experiments demonstrate the\nsuperiority of our method in both class-conditional and text-to-image\nscenarios. Moreover, our approach exhibits strong robustness and generalization\nunder various data transformations. Furthermore, sufficient experiments suggest\ntwo novel key findings: (1) A linear scaling law on membership inference,\nexposing the vulnerability of large foundation models. (2) Training data from\nscale-wise visual autoregressive models is easier to detect than other\nautoregressive paradigms.Our code is available at\nhttps://github.com/Chrisqcwx/ImageAR-MIA.",
      "pdf_url": "http://arxiv.org/pdf/2507.05068v1",
      "published": "2025-07-07T14:50:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05068v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Replacing thinking with tool usage enables reasoning in small language models",
      "authors": [
        "Corrado Rainone",
        "Tim Bakker",
        "Roland Memisevic"
      ],
      "abstract": "Recent advances have established a new machine learning paradigm based on\nscaling up compute at inference time as well as at training time. In that line\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\nused for training Large Language Models to expend extra compute during\ninference in the form of \"thoughts\" expressed in natural language. In this\npaper, we propose to instead format these tokens as a multi-turn interaction\ntrace with a stateful tool. At each turn, the new state of the tool is appended\nto the context of the model, whose job is to generate the tokens necessary to\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\nrepairing malfunctioning Python code, and show that this constrained setup\nallows for faster sampling of experience and a denser reward signal, allowing\neven models of size up to 3B parameters to learn how to proficiently expend\nadditional compute on the task.",
      "pdf_url": "http://arxiv.org/pdf/2507.05065v1",
      "published": "2025-07-07T14:49:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05065v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling",
      "authors": [
        "Xin Dong",
        "Shichao Dong",
        "Jin Wang",
        "Jing Huang",
        "Li Zhou",
        "Zenghui Sun",
        "Lihua Jing",
        "Jingsong Lan",
        "Xiaoyong Zhu",
        "Bo Zheng"
      ],
      "abstract": "Hallucinations in large vision-language models (LVLMs) pose significant\nchallenges for real-world applications, as LVLMs may generate responses that\nappear plausible yet remain inconsistent with the associated visual content.\nThis issue rarely occurs in human cognition. We argue that this discrepancy\narises from humans' ability to effectively leverage multimodal interaction\ninformation in data samples. Specifically, humans typically first gather\nmultimodal information, analyze the interactions across modalities for\nunderstanding, and then express their understanding through language. Motivated\nby this observation, we conduct extensive experiments on popular LVLMs and\nobtained insights that surprisingly reveal human-like, though less pronounced,\ncognitive behavior of LVLMs on multimodal samples. Building on these findings,\nwe further propose \\textbf{INTER}: \\textbf{Inter}action Guidance Sampling, a\nnovel training-free algorithm that mitigate hallucinations without requiring\nadditional data. Specifically, INTER explicitly guides LVLMs to effectively\nreapply their understanding of multimodal interaction information when\ngenerating responses, thereby reducing potential hallucinations. On six\nbenchmarks including VQA and image captioning tasks, INTER achieves an average\nimprovement of up to 3.4\\% on five LVLMs compared to the state-of-the-art\ndecoding strategy. The code will be released when the paper is accepted.",
      "pdf_url": "http://arxiv.org/pdf/2507.05056v1",
      "published": "2025-07-07T14:38:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05056v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good",
      "authors": [
        "Celeste Campos-Castillo",
        "Xuan Kang",
        "Linnea I. Laestadius"
      ],
      "abstract": "Recently, research into chatbots (also known as conversational agents, AI\nagents, voice assistants), which are computer applications using artificial\nintelligence to mimic human-like conversation, has grown sharply. Despite this\ngrowth, sociology lags other disciplines (including computer science, medicine,\npsychology, and communication) in publishing about chatbots. We suggest\nsociology can advance understanding of human-chatbot interaction and offer four\nsociological theories to enhance extant work in this field. The first two\ntheories (resource substitution theory, power-dependence theory) add new\ninsights to existing models of the drivers of chatbot use, which overlook\nsociological concerns about how social structure (e.g., systemic\ndiscrimination, the uneven distribution of resources within networks) inclines\nindividuals to use chatbots, including problematic levels of emotional\ndependency on chatbots. The second two theories (affect control theory,\nfundamental cause of disease theory) help inform the development of\nchatbot-driven interventions that minimize safety risks and enhance equity by\nleveraging sociological insights into how chatbot outputs could attend to\ncultural contexts (e.g., affective norms) to promote wellbeing and enhance\ncommunities (e.g., opportunities for civic participation). We discuss the value\nof applying sociological theories for advancing theorizing about human-chatbot\ninteraction and developing chatbots for social good.",
      "pdf_url": "http://arxiv.org/pdf/2507.05030v1",
      "published": "2025-07-07T14:12:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05030v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "J.4"
      ]
    },
    {
      "title": "Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision",
      "authors": [
        "Soham Walimbe",
        "Britty Baby",
        "Vinkle Srivastav",
        "Nicolas Padoy"
      ],
      "abstract": "Surgical AI often involves multiple tasks within a single procedure, like\nphase recognition or assessing the Critical View of Safety in laparoscopic\ncholecystectomy. Traditional models, built for one task at a time, lack\nflexibility, requiring a separate model for each. To address this, we introduce\nMML-SurgAdapt, a unified multi-task framework with Vision-Language Models\n(VLMs), specifically CLIP, to handle diverse surgical tasks through natural\nlanguage supervision. A key challenge in multi-task learning is the presence of\npartial annotations when integrating different tasks. To overcome this, we\nemploy Single Positive Multi-Label (SPML) learning, which traditionally reduces\nannotation burden by training models with only one positive label per instance.\nOur framework extends this approach to integrate data from multiple surgical\ntasks within a single procedure, enabling effective learning despite incomplete\nor noisy annotations. We demonstrate the effectiveness of our model on a\ncombined dataset consisting of Cholec80, Endoscapes2023, and CholecT50,\nutilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt\nperforms comparably to task-specific benchmarks, with the added advantage of\nhandling noisy annotations. It also outperforms the existing SPML frameworks\nfor the task. By reducing the required labels by 23%, our approach proposes a\nmore scalable and efficient labeling process, significantly easing the\nannotation burden on clinicians. To our knowledge, this is the first\napplication of SPML to integrate data from multiple surgical tasks, presenting\na novel and generalizable solution for multi-task learning in surgical computer\nvision. Implementation is available at:\nhttps://github.com/CAMMA-public/MML-SurgAdapt",
      "pdf_url": "http://arxiv.org/pdf/2507.05020v1",
      "published": "2025-07-07T14:03:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05020v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Meta-Learning Transformers to Improve In-Context Generalization",
      "authors": [
        "Lorenzo Braccaioli",
        "Anna Vettoruzzo",
        "Prabhant Singh",
        "Joaquin Vanschoren",
        "Mohamed-Rafik Bouguelia",
        "Nicola Conci"
      ],
      "abstract": "In-context learning enables transformer models to generalize to new tasks\nbased solely on input prompts, without any need for weight updates. However,\nexisting training paradigms typically rely on large, unstructured datasets that\nare costly to store, difficult to evaluate for quality and balance, and pose\nprivacy and ethical concerns due to the inclusion of sensitive information.\nMotivated by these limitations and risks, we propose an alternative training\nstrategy where we leverage a collection of multiple, small-scale, and\ndomain-specific datasets. We empirically demonstrate that the increased quality\nand diversity of such data improve the generalization abilities of in-context\nlearners beyond their training domain, while achieving comparable performance\nwith models trained on a single large-scale dataset. We investigate this\nparadigm by leveraging meta-learning to train an in-context learner on the\nMeta-Album collection under several settings. Firstly, we show the performance\nin a controlled environment, where the test domain is completely excluded from\nthe training knowledge. Secondly, we explore the robustness of these models to\nforgetting in a continual scenario where the information is accessible for a\nlimited time. Finally, we explore the more challenging unsupervised scenario.\nOur findings demonstrate that transformers still generalize for in-context\nprediction when trained on a curated dataset collection while offering\nadvantages in modularity and replaceability.",
      "pdf_url": "http://arxiv.org/pdf/2507.05019v1",
      "published": "2025-07-07T14:02:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05019v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning",
      "authors": [
        "Maxence Boels",
        "Harry Robertshaw",
        "Alejandro Granados",
        "Prokar Dasgupta",
        "Sebastien Ourselin"
      ],
      "abstract": "Surgical action planning requires predicting future instrument-verb-target\ntriplets for real-time assistance. While teleoperated robotic surgery provides\nnatural expert demonstrations for imitation learning (IL), reinforcement\nlearning (RL) could potentially discover superior strategies through\nexploration. We present the first comprehensive comparison of IL versus RL for\nsurgical action planning on CholecT50. Our Dual-task Autoregressive Imitation\nLearning (DARIL) baseline achieves 34.6% action triplet recognition mAP and\n33.6% next frame prediction mAP with smooth planning degradation to 29.2% at\n10-second horizons. We evaluated three RL variants: world model-based RL,\ndirect video RL, and inverse RL enhancement. Surprisingly, all RL approaches\nunderperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while\ndirect video RL achieved only 15.9%. Our analysis reveals that distribution\nmatching on expert-annotated test sets systematically favors IL over\npotentially valid RL policies that differ from training demonstrations. This\nchallenges assumptions about RL superiority in sequential decision making and\nprovides crucial insights for surgical AI development.",
      "pdf_url": "http://arxiv.org/pdf/2507.05011v1",
      "published": "2025-07-07T13:49:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05011v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Multi-modal Representations for Fine-grained Multi-label Critical View of Safety Recognition",
      "authors": [
        "Britty Baby",
        "Vinkle Srivastav",
        "Pooja P. Jain",
        "Kun Yuan",
        "Pietro Mascagni",
        "Nicolas Padoy"
      ],
      "abstract": "The Critical View of Safety (CVS) is crucial for safe laparoscopic\ncholecystectomy, yet assessing CVS criteria remains a complex and challenging\ntask, even for experts. Traditional models for CVS recognition depend on\nvision-only models learning with costly, labor-intensive spatial annotations.\nThis study investigates how text can be harnessed as a powerful tool for both\ntraining and inference in multi-modal surgical foundation models to automate\nCVS recognition. Unlike many existing multi-modal models, which are primarily\nadapted for multi-class classification, CVS recognition requires a multi-label\nframework. Zero-shot evaluation of existing multi-modal surgical models shows a\nsignificant performance gap for this task. To address this, we propose\nCVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,\nbinary classification across multiple labels by aligning image embeddings with\ntextual descriptions of each CVS criterion using positive and negative prompts.\nBy adapting PeskaVLP, a state-of-the-art surgical foundation model, on the\nEndoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the\nResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that\nCVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,\nboosts CVS recognition over image-only methods. We also propose text-specific\ninference methods, that helps in analysing the image-text alignment. While\nfurther work is needed to match state-of-the-art spatial annotation-based\nmethods, this approach highlights the potential of adapting generalist models\nto specialized surgical tasks. Code:\nhttps://github.com/CAMMA-public/CVS-AdaptNet",
      "pdf_url": "http://arxiv.org/pdf/2507.05007v1",
      "published": "2025-07-07T13:44:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.05007v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Supported Abstract Argumentation for Case-Based Reasoning",
      "authors": [
        "Adam Gould",
        "Gabriel de Olim Gaul",
        "Francesca Toni"
      ],
      "abstract": "We introduce Supported Abstract Argumentation for Case-Based Reasoning\n(sAA-CBR), a binary classification model in which past cases engage in debates\nby arguing in favour of their labelling and attacking or supporting those with\nopposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of\nits precursor AA-CBR, which can contain extraneous cases (or spikes) that are\nnot included in the debates. We prove that sAA-CBR contains no spikes, without\ntrading off key model properties",
      "pdf_url": "http://arxiv.org/pdf/2507.04994v1",
      "published": "2025-07-07T13:32:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04994v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Classification of autoimmune diseases from Peripheral blood TCR repertoires by multimodal multi-instance learning",
      "authors": [
        "Ruihao Zhang",
        "Fei Ye",
        "Dandan Meng",
        "Yixuan Huang",
        "Maochen",
        "Xiao Liu"
      ],
      "abstract": "T cell receptor (TCR) repertoires encode critical immunological signatures\nfor autoimmune diseases, yet their clinical application remains limited by\nsequence sparsity and low witness rates. We developed EAMil, a multi-instance\ndeep learning framework that leverages TCR sequencing data to diagnose systemic\nlupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional\naccuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding\nand enhanced gate attention mechanisms, our model achieved state-of-the-art\nperformance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully\nidentified disease-associated genes with over 90% concordance with established\ndifferential analyses and effectively distinguished disease-specific TCR genes.\nThe model demonstrated robustness in classifying multiple disease categories,\nutilizing the SLEDAI score to stratify SLE patients by disease severity as well\nas to diagnose the site of damage in SLE patients, and effectively controlling\nfor confounding factors such as age and gender. This interpretable framework\nfor immune receptor analysis provides new insights for autoimmune disease\ndetection and classification with broad potential clinical applications across\nimmune-mediated conditions.",
      "pdf_url": "http://arxiv.org/pdf/2507.04981v2",
      "published": "2025-07-07T13:24:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04981v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN"
      ]
    },
    {
      "title": "LAPS-Diff: A Diffusion-Based Framework for Singing Voice Synthesis With Language Aware Prosody-Style Guided Learning",
      "authors": [
        "Sandipan Dhar",
        "Mayank Gupta",
        "Preeti Rao"
      ],
      "abstract": "The field of Singing Voice Synthesis (SVS) has seen significant advancements\nin recent years due to the rapid progress of diffusion-based approaches.\nHowever, capturing vocal style, genre-specific pitch inflections, and\nlanguage-dependent characteristics remains challenging, particularly in\nlow-resource scenarios. To address this, we propose LAPS-Diff, a diffusion\nmodel integrated with language-aware embeddings and a vocal-style guided\nlearning mechanism, specifically designed for Bollywood Hindi singing style. We\ncurate a Hindi SVS dataset and leverage pre-trained language models to extract\nword and phone-level embeddings for an enriched lyrics representation.\nAdditionally, we incorporated a style encoder and a pitch extraction model to\ncompute style and pitch losses, capturing features essential to the naturalness\nand expressiveness of the synthesized singing, particularly in terms of vocal\nstyle and pitch variations. Furthermore, we utilize MERT and IndicWav2Vec\nmodels to extract musical and contextual embeddings, serving as conditional\npriors to refine the acoustic feature generation process further. Based on\nobjective and subjective evaluations, we demonstrate that LAPS-Diff\nsignificantly improves the quality of the generated samples compared to the\nconsidered state-of-the-art (SOTA) model for our constrained dataset that is\ntypical of the low resource scenario.",
      "pdf_url": "http://arxiv.org/pdf/2507.04966v1",
      "published": "2025-07-07T13:09:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04966v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Hear-Your-Click: Interactive Video-to-Audio Generation via Object-aware Contrastive Audio-Visual Fine-tuning",
      "authors": [
        "Yingshan Liang",
        "Keyu Fan",
        "Zhicheng Du",
        "Yiran Wang",
        "Qingyang Shi",
        "Xinyu Zhang",
        "Jiasheng Lu",
        "Peiwu Qin"
      ],
      "abstract": "Video-to-audio (V2A) generation shows great potential in fields such as film\nproduction. Despite significant advances, current V2A methods, which rely on\nglobal video information, struggle with complex scenes and often fail to\ngenerate audio tailored to specific objects or regions in the videos. To\naddress these limitations, we introduce Hear-Your-Click, an interactive V2A\nframework that enables users to generate sounds for specific objects in the\nvideos by simply clicking on the frame. To achieve this, we propose\nObject-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided\nVisual Encoder (MVE) to obtain object-level visual features aligned with\ncorresponding audio segments. Furthermore, we tailor two data augmentation\nstrategies: Random Video Stitching (RVS) and Mask-guided Loudness Modulation\n(MLM), aimed at enhancing the model's sensitivity to the segmented objects. To\neffectively measure the audio-visual correspondence, we design a new evaluation\nmetric, the CAV score, for evaluation. Extensive experiments demonstrate that\nour framework offers more precise control and improved generation performance\nacross various metrics. Project Page:\nhttps://github.com/SynapGrid/Hear-Your-Click",
      "pdf_url": "http://arxiv.org/pdf/2507.04959v1",
      "published": "2025-07-07T13:01:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04959v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation",
      "authors": [
        "Fathinah Izzati",
        "Xinyue Li",
        "Gus Xia"
      ],
      "abstract": "We propose Expotion (Facial Expression and Motion Control for Multimodal\nMusic Generation), a generative model leveraging multimodal visual controls -\nspecifically, human facial expressions and upper-body motion - as well as text\nprompts to produce expressive and temporally accurate music. We adopt\nparameter-efficient fine-tuning (PEFT) on the pretrained text-to-music\ngeneration model, enabling fine-grained adaptation to the multimodal controls\nusing a small dataset. To ensure precise synchronization between video and\nmusic, we introduce a temporal smoothing strategy to align multiple modalities.\nExperiments demonstrate that integrating visual features alongside textual\ndescriptions enhances the overall quality of generated music in terms of\nmusicality, creativity, beat-tempo consistency, temporal alignment with the\nvideo, and text adherence, surpassing both proposed baselines and existing\nstate-of-the-art video-to-music generation models. Additionally, we introduce a\nnovel dataset consisting of 7 hours of synchronized video recordings capturing\nexpressive facial and upper-body gestures aligned with corresponding music,\nproviding significant potential for future research in multimodal and\ninteractive music generation.",
      "pdf_url": "http://arxiv.org/pdf/2507.04955v1",
      "published": "2025-07-07T12:56:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04955v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ]
    },
    {
      "title": "DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer",
      "authors": [
        "Yecheng Wu",
        "Junyu Chen",
        "Zhuoyang Zhang",
        "Enze Xie",
        "Jincheng Yu",
        "Junsong Chen",
        "Jinyi Hu",
        "Yao Lu",
        "Song Han",
        "Han Cai"
      ],
      "abstract": "We introduce DC-AR, a novel masked autoregressive (AR) text-to-image\ngeneration framework that delivers superior image generation quality with\nexceptional computational efficiency. Due to the tokenizers' limitations, prior\nmasked AR models have lagged behind diffusion models in terms of quality or\nefficiency. We overcome this limitation by introducing DC-HT - a deep\ncompression hybrid tokenizer for AR models that achieves a 32x spatial\ncompression ratio while maintaining high reconstruction fidelity and\ncross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT\nand create a new hybrid masked autoregressive image generation framework that\nfirst produces the structural elements through discrete tokens and then applies\nrefinements via residual tokens. DC-AR achieves state-of-the-art results with a\ngFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while\noffering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to\nprior leading diffusion and autoregressive models.",
      "pdf_url": "http://arxiv.org/pdf/2507.04947v1",
      "published": "2025-07-07T12:45:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04947v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Object-centric Denoising Diffusion Models for Physical Reasoning",
      "authors": [
        "Moritz Lange",
        "Raphael C. Engelhardt",
        "Wolfgang Konen",
        "Andrew Melnik",
        "Laurenz Wiskott"
      ],
      "abstract": "Reasoning about the trajectories of multiple, interacting objects is integral\nto physical reasoning tasks in machine learning. This involves conditions\nimposed on the objects at different time steps, for instance initial states or\ndesired goal states. Existing approaches in physical reasoning generally rely\non autoregressive modeling, which can only be conditioned on initial states,\nbut not on later states. In fields such as planning for reinforcement learning,\nsimilar challenges are being addressed with denoising diffusion models. In this\nwork, we propose an object-centric denoising diffusion model architecture for\nphysical reasoning that is translation equivariant over time, permutation\nequivariant over objects, and can be conditioned on arbitrary time steps for\narbitrary objects. We demonstrate how this model can solve tasks with multiple\nconditions and examine its performance when changing object numbers and\ntrajectory lengths during inference.",
      "pdf_url": "http://arxiv.org/pdf/2507.04920v1",
      "published": "2025-07-07T12:06:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04920v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Leadership Detection via Time-Lagged Correlation-Based Network Inference",
      "authors": [
        "Thayanne França da Silva",
        "José Everardo Bessa Maia"
      ],
      "abstract": "Understanding leadership dynamics in collective behavior is a key challenge\nin animal ecology, swarm robotics, and intelligent transportation. Traditional\ninformation-theoretic approaches, including Transfer Entropy (TE) and\nTime-Lagged Mutual Information (TLMI), have been widely used to infer\nleader-follower relationships but face critical limitations in noisy or\nshort-duration datasets due to their reliance on robust probability\nestimations. This study proposes a method based on dynamic network inference\nusing time-lagged correlations across multiple kinematic variables: velocity,\nacceleration, and direction. Our approach constructs directed influence graphs\nover time, enabling the identification of leadership patterns without the need\nfor large volumes of data or parameter-sensitive discretization. We validate\nour method through two multi-agent simulations in NetLogo: a modified Vicsek\nmodel with informed leaders and a predator-prey model featuring coordinated and\nindependent wolf groups. Experimental results demonstrate that the\nnetwork-based method outperforms TE and TLMI in scenarios with limited\nspatiotemporal observations, ranking true leaders at the top of influence\nmetrics more consistently than TE and TLMI.",
      "pdf_url": "http://arxiv.org/pdf/2507.04917v1",
      "published": "2025-07-07T12:04:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04917v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "nlin.AO"
      ]
    },
    {
      "title": "HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding",
      "authors": [
        "Yuxuan Cai",
        "Jiangning Zhang",
        "Zhenye Gan",
        "Qingdong He",
        "Xiaobin Hu",
        "Junwei Zhu",
        "Yabiao Wang",
        "Chengjie Wang",
        "Zhucun Xue",
        "Xinwei He",
        "Xiang Bai"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances in visual understanding tasks involving both images and videos.\nHowever, their capacity to comprehend human-centric video data remains\nunderexplored, primarily due to the absence of comprehensive and high-quality\nevaluation benchmarks. Existing human-centric benchmarks predominantly\nemphasize video generation quality and action recognition, while overlooking\nessential perceptual and cognitive abilities required in human-centered\nscenarios. Furthermore, they are often limited by single-question paradigms and\noverly simplistic evaluation metrics. To address above limitations, we propose\na modern HV-MMBench, a rigorously curated benchmark designed to provide a more\nholistic evaluation of MLLMs in human-centric video understanding. Compared to\nexisting human-centric video benchmarks, our work offers the following key\nfeatures: (1) Diverse evaluation dimensions: HV-MMBench encompasses 15 tasks,\nranging from basic attribute perception (e.g., age estimation, emotion\nrecognition) to advanced cognitive reasoning (e.g., social relationship\nprediction, intention prediction), enabling comprehensive assessment of model\ncapabilities; (2) Varied data types: The benchmark includes multiple-choice,\nfill-in-blank, true/false, and open-ended question formats, combined with\ndiverse evaluation metrics, to more accurately and robustly reflect model\nperformance; (3) Multi-domain video coverage: The benchmark spans 50 distinct\nvisual scenarios, enabling comprehensive evaluation across fine-grained scene\nvariations; (4) Temporal coverage: The benchmark covers videos from short-term\n(10 seconds) to long-term (up to 30min) durations, supporting systematic\nanalysis of models temporal reasoning abilities across diverse contextual\nlengths.",
      "pdf_url": "http://arxiv.org/pdf/2507.04909v1",
      "published": "2025-07-07T11:52:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04909v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning",
      "authors": [
        "Thinh Dao",
        "Dung Thuy Nguyen",
        "Khoa D Doan",
        "Kok-Seng Wong"
      ],
      "abstract": "Federated Learning (FL) systems are vulnerable to backdoor attacks, where\nadversaries train their local models on poisoned data and submit poisoned model\nupdates to compromise the global model. Despite numerous proposed attacks and\ndefenses, divergent experimental settings, implementation errors, and\nunrealistic assumptions hinder fair comparisons and valid conclusions about\ntheir effectiveness in real-world scenarios. To address this, we introduce\nBackFed - a comprehensive benchmark suite designed to standardize, streamline,\nand reliably evaluate backdoor attacks and defenses in FL, with a focus on\npractical constraints. Our benchmark offers key advantages through its\nmulti-processing implementation that significantly accelerates experimentation\nand the modular design that enables seamless integration of new methods via\nwell-defined APIs. With a standardized evaluation pipeline, we envision BackFed\nas a plug-and-play environment for researchers to comprehensively and reliably\nevaluate new attacks and defenses. Using BackFed, we conduct large-scale\nstudies of representative backdoor attacks and defenses across both Computer\nVision and Natural Language Processing tasks with diverse model architectures\nand experimental settings. Our experiments critically assess the performance of\nproposed attacks and defenses, revealing unknown limitations and modes of\nfailures under practical conditions. These empirical insights provide valuable\nguidance for the development of new methods and for enhancing the security of\nFL systems. Our framework is openly available at\nhttps://github.com/thinh-dao/BackFed.",
      "pdf_url": "http://arxiv.org/pdf/2507.04903v1",
      "published": "2025-07-07T11:40:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04903v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction",
      "authors": [
        "Kaleem Ullah Qasim",
        "Jiashu Zhang"
      ],
      "abstract": "Accident severity prediction plays a critical role in transportation safety\nsystems but is a persistently difficult task due to incomplete data, strong\nfeature dependencies, and severe class imbalance in which rare but\nhigh-severity cases are underrepresented and hard to detect. Existing methods\noften rely on monolithic models or black box prompting, which struggle to scale\nin noisy, real-world settings and offer limited interpretability. To address\nthese challenges, we propose MARBLE a multiagent rule based LLM engine that\ndecomposes the severity prediction task across a team of specialized reasoning\nagents, including an interchangeable ML-backed agent. Each agent focuses on a\nsemantic subset of features (e.g., spatial, environmental, temporal), enabling\nscoped reasoning and modular prompting without the risk of prompt saturation.\nPredictions are coordinated through either rule-based or LLM-guided consensus\nmechanisms that account for class rarity and confidence dynamics. The system\nretains structured traces of agent-level reasoning and coordination outcomes,\nsupporting in-depth interpretability and post-hoc performance diagnostics.\nAcross both UK and US datasets, MARBLE consistently outperforms traditional\nmachine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning\nmethods including Chain-of-Thought (CoT), Least-to-Most (L2M), and\nTree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below\n48%. This performance redefines the practical ceiling for accident severity\nclassification under real world noise and extreme class imbalance. Our results\nposition MARBLE as a generalizable and interpretable framework for reasoning\nunder uncertainty in safety-critical applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.04893v1",
      "published": "2025-07-07T11:27:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.04893v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ]
    }
  ]
}