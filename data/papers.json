{
  "last_updated": "2025-06-09T00:56:07.532385",
  "papers": [
    {
      "title": "Refer to Anything with Vision-Language Prompts",
      "authors": [
        "Shengcao Cao",
        "Zijun Wei",
        "Jason Kuen",
        "Kangning Liu",
        "Lingzhi Zhang",
        "Jiuxiang Gu",
        "HyunJoon Jung",
        "Liang-Yan Gui",
        "Yu-Xiong Wang"
      ],
      "abstract": "Recent image segmentation models have advanced to segment images into\nhigh-quality masks for visual entities, and yet they cannot provide\ncomprehensive semantic understanding for complex queries based on both language\nand vision. This limitation reduces their effectiveness in applications that\nrequire user-friendly interactions driven by vision-language prompts. To bridge\nthis gap, we introduce a novel task of omnimodal referring expression\nsegmentation (ORES). In this task, a model produces a group of masks based on\narbitrary prompts specified by text only or text plus reference visual\nentities. To address this new challenge, we propose a novel framework to \"Refer\nto Any Segmentation Mask Group\" (RAS), which augments segmentation models with\ncomplex multimodal interactions and comprehension via a mask-centric large\nmultimodal model. For training and benchmarking ORES models, we create datasets\nMaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by\ntext and reference entities. Through extensive evaluation, we demonstrate\nsuperior performance of RAS on our new ORES task, as well as classic referring\nexpression segmentation (RES) and generalized referring expression segmentation\n(GRES) tasks. Project page: https://Ref2Any.github.io.",
      "pdf_url": "http://arxiv.org/pdf/2506.05342v1",
      "published": "2025-06-05T17:59:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05342v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning",
      "authors": [
        "Xingjian Ran",
        "Yixuan Li",
        "Linning Xu",
        "Mulin Yu",
        "Bo Dai"
      ],
      "abstract": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility.",
      "pdf_url": "http://arxiv.org/pdf/2506.05341v1",
      "published": "2025-06-05T17:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05341v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Diffusion Transformer Designs via Grafting",
      "authors": [
        "Keshigeyan Chandrasegaran",
        "Michael Poli",
        "Daniel Y. Fu",
        "Dongjun Kim",
        "Lea M. Hadzic",
        "Manling Li",
        "Agrim Gupta",
        "Stefano Massaroli",
        "Azalia Mirhoseini",
        "Juan Carlos Niebles",
        "Stefano Ermon",
        "Li Fei-Fei"
      ],
      "abstract": "Designing model architectures requires decisions such as selecting operators\n(e.g., attention, convolution) and configurations (e.g., depth, width).\nHowever, evaluating the impact of these decisions on model quality requires\ncostly pretraining, limiting architectural investigation. Inspired by how new\nsoftware is built on existing code, we ask: can new architecture designs be\nstudied using pretrained models? To this end, we present grafting, a simple\napproach for editing pretrained diffusion transformers (DiTs) to materialize\nnew architectures under small compute budgets. Informed by our analysis of\nactivation behavior and attention locality, we construct a testbed based on the\nDiT-XL/2 design to study the impact of grafting on model quality. Using this\ntestbed, we develop a family of hybrid designs via grafting: replacing softmax\nattention with gated convolution, local attention, and linear attention, and\nreplacing MLPs with variable expansion ratio and convolutional variants.\nNotably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for\nDiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model\n(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by\nconverting every pair of sequential transformer blocks into parallel blocks via\ngrafting. This reduces model depth by 2x and yields better quality (FID: 2.77)\nthan other models of comparable depth. Together, we show that new diffusion\nmodel designs can be explored by grafting pretrained DiTs, with edits ranging\nfrom operator replacement to architecture restructuring. Code and grafted\nmodels: https://grafting.stanford.edu",
      "pdf_url": "http://arxiv.org/pdf/2506.05340v1",
      "published": "2025-06-05T17:59:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05340v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay",
      "authors": [
        "Yifan Sun",
        "Jingyan Shen",
        "Yibin Wang",
        "Tianyu Chen",
        "Zhendong Wang",
        "Mingyuan Zhou",
        "Huan Zhang"
      ],
      "abstract": "Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm.",
      "pdf_url": "http://arxiv.org/pdf/2506.05316v1",
      "published": "2025-06-05T17:55:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05316v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models",
      "authors": [
        "Taha Entesari",
        "Arman Hatami",
        "Rinat Khaziev",
        "Anil Ramakrishna",
        "Mahyar Fazlyab"
      ],
      "abstract": "Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility.",
      "pdf_url": "http://arxiv.org/pdf/2506.05314v1",
      "published": "2025-06-05T17:55:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05314v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games",
      "authors": [
        "Niv Eckhaus",
        "Uri Berger",
        "Gabriel Stanovsky"
      ],
      "abstract": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated.",
      "pdf_url": "http://arxiv.org/pdf/2506.05309v1",
      "published": "2025-06-05T17:53:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05309v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback",
      "authors": [
        "Deepak Pandita",
        "Tharindu Cyril Weerasooriya",
        "Ankit Parag Shah",
        "Christopher M. Homan",
        "Wei Wei"
      ],
      "abstract": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.",
      "pdf_url": "http://arxiv.org/pdf/2506.05305v1",
      "published": "2025-06-05T17:52:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05305v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Control Tax: The Price of Keeping AI in Check",
      "authors": [
        "Mikhail Terekhov",
        "Zhen Ning David Liu",
        "Caglar Gulcehre",
        "Samuel Albanie"
      ],
      "abstract": "The rapid integration of agentic AI into high-stakes real-world applications\nrequires robust oversight mechanisms. The emerging field of AI Control (AIC)\naims to provide such an oversight mechanism, but practical adoption depends\nheavily on implementation overhead. To study this problem better, we introduce\nthe notion of Control tax -- the operational and financial cost of integrating\ncontrol measures into AI pipelines. Our work makes three key contributions to\nthe field of AIC: (1) we introduce a theoretical framework that quantifies the\nControl Tax and maps classifier performance to safety assurances; (2) we\nconduct comprehensive evaluations of state-of-the-art language models in\nadversarial settings, where attacker models insert subtle backdoors into code\nwhile monitoring models attempt to detect these vulnerabilities; and (3) we\nprovide empirical financial cost estimates for control protocols and develop\noptimized monitoring strategies that balance safety and cost-effectiveness\nwhile accounting for practical constraints like auditing budgets. Our framework\nenables practitioners to make informed decisions by systematically connecting\nsafety guarantees with their costs, advancing AIC through principled economic\nfeasibility assessment across different deployment contexts.",
      "pdf_url": "http://arxiv.org/pdf/2506.05296v1",
      "published": "2025-06-05T17:48:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05296v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Sample Complexity and Representation Ability of Test-time Scaling Paradigms",
      "authors": [
        "Baihe Huang",
        "Shanda Li",
        "Tianhao Wu",
        "Yiming Yang",
        "Ameet Talwalkar",
        "Kannan Ramchandran",
        "Michael I. Jordan",
        "Jiantao Jiao"
      ],
      "abstract": "Test-time scaling paradigms have significantly advanced the capabilities of\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\ntheoretical understanding of the sample efficiency of various test-time\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\nremains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an\nexpressiveness result for the self-correction approach with verifier feedback:\nit enables Transformers to simulate online learning over a pool of experts at\ntest time. Therefore, a single Transformer architecture can provably solve\nmultiple tasks without prior knowledge of the specific task associated with a\nuser query, extending the representation theory of Transformers from\nsingle-task to multi-task settings. Finally, we empirically validate our\ntheoretical results, demonstrating the practical effectiveness of\nself-correction methods.",
      "pdf_url": "http://arxiv.org/pdf/2506.05295v1",
      "published": "2025-06-05T17:48:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05295v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Rectified Point Flow: Generic Point Cloud Pose Estimation",
      "authors": [
        "Tao Sun",
        "Liyuan Zhu",
        "Shengyu Huang",
        "Shuran Song",
        "Iro Armeni"
      ],
      "abstract": "We introduce Rectified Point Flow, a unified parameterization that formulates\npairwise point cloud registration and multi-part shape assembly as a single\nconditional generative problem. Given unposed point clouds, our method learns a\ncontinuous point-wise velocity field that transports noisy points toward their\ntarget positions, from which part poses are recovered. In contrast to prior\nwork that regresses part-wise poses with ad-hoc symmetry handling, our method\nintrinsically learns assembly symmetries without symmetry labels. Together with\na self-supervised encoder focused on overlapping points, our method achieves a\nnew state-of-the-art performance on six benchmarks spanning pairwise\nregistration and shape assembly. Notably, our unified formulation enables\neffective joint training on diverse datasets, facilitating the learning of\nshared geometric priors and consequently boosting accuracy. Project page:\nhttps://rectified-pointflow.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2506.05282v1",
      "published": "2025-06-05T17:36:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05282v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Fast-DataShapley: Neural Modeling for Training Data Valuation",
      "authors": [
        "Haifeng Sun",
        "Yu Xiong",
        "Runze Wu",
        "Xinyu Cai",
        "Changjie Fan",
        "Lan Zhang",
        "Xiang-Yang Li"
      ],
      "abstract": "The value and copyright of training data are crucial in the artificial\nintelligence industry. Service platforms should protect data providers'\nlegitimate rights and fairly reward them for their contributions. Shapley\nvalue, a potent tool for evaluating contributions, outperforms other methods in\ntheory, but its computational overhead escalates exponentially with the number\nof data providers. Recent works based on Shapley values attempt to mitigate\ncomputation complexity by approximation algorithms. However, they need to\nretrain for each test sample, leading to intolerable costs. We propose\nFast-DataShapley, a one-pass training method that leverages the weighted least\nsquares characterization of the Shapley value to train a reusable explainer\nmodel with real-time reasoning speed. Given new test samples, no retraining is\nrequired to calculate the Shapley values of the training data. Additionally, we\npropose three methods with theoretical guarantees to reduce training overhead\nfrom two aspects: the approximate calculation of the utility function and the\ngroup calculation of the training data. We analyze time complexity to show the\nefficiency of our methods. The experimental evaluations on various image\ndatasets demonstrate superior performance and efficiency compared to baselines.\nSpecifically, the performance is improved to more than 2.5 times, and the\nexplainer's training speed can be increased by two orders of magnitude.",
      "pdf_url": "http://arxiv.org/pdf/2506.05281v1",
      "published": "2025-06-05T17:35:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05281v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning",
      "authors": [
        "Nan Huo",
        "Jinyang Li",
        "Bowen Qin",
        "Ge Qu",
        "Xiaolong Li",
        "Xiaodong Li",
        "Chenhao Ma",
        "Reynold Cheng"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.05278v1",
      "published": "2025-06-05T17:33:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05278v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams",
      "authors": [
        "Mohammed Almutairi"
      ],
      "abstract": "Effective teamwork is essential across diverse domains. During the team\nformation stage, a key challenge is forming teams that effectively balance user\npreferences with task objectives to enhance overall team satisfaction. In the\nteam performing stage, maintaining cohesion and engagement is critical for\nsustaining high team performance. However, existing computational tools and\nalgorithms for team optimization often rely on static data inputs, narrow\nalgorithmic objectives, or solutions tailored for specific contexts, failing to\naccount for the dynamic interplay of team members personalities, evolving\ngoals, and changing individual preferences. Therefore, teams may encounter\nmember dissatisfaction, as purely algorithmic assignments can reduce members\ncommitment to team goals or experience suboptimal engagement due to the absence\nof timely, personalized guidance to help members adjust their behaviors and\ninteractions as team dynamics evolve. Ultimately, these challenges can lead to\nreduced overall team performance. My Ph.D. dissertation aims to develop\nAI-augmented team optimization frameworks and practical systems that enhance\nteam satisfaction, engagement, and performance. First, I propose a team\nformation framework that leverages a multi-armed bandit algorithm to\niteratively refine team composition based on user preferences, ensuring\nalignment between individual needs and collective team goals to enhance team\nsatisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an\nAI-powered system that utilizes large language models (LLMs) to deliver\nimmediate, personalized feedback to both teams and individual members,\nenhancing cohesion and engagement. Finally, I present PuppeteerLLM, an\nLLM-based simulation framework that simulates multi-agent teams to model\ncomplex team dynamics within realistic environments, incorporating task-driven\ncollaboration and long-term coordination.",
      "pdf_url": "http://arxiv.org/pdf/2506.05265v1",
      "published": "2025-06-05T17:24:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05265v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning",
      "authors": [
        "Violet Xiang",
        "Chase Blagden",
        "Rafael Rafailov",
        "Nathan Lile",
        "Sang Truong",
        "Chelsea Finn",
        "Nick Haber"
      ],
      "abstract": "Large reasoning models (LRMs) achieve higher performance on challenging\nreasoning tasks by generating more tokens at inference time, but this verbosity\noften wastes computation on easy problems. Existing solutions, including\nsupervised finetuning on shorter traces, user-controlled budgets, or RL with\nuniform penalties, either require data curation, manual configuration, or treat\nall problems alike regardless of difficulty. We introduce Adaptive Length\nPenalty (ALP), a reinforcement learning objective tailoring generation length\nto per-prompt solve rate. During training, ALP monitors each prompt's online\nsolve rate through multiple rollouts and adds a differentiable penalty whose\nmagnitude scales inversely with that rate, so confident (easy) prompts incur a\nhigh cost for extra tokens while hard prompts remain unhindered. Posttraining\nDeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly\ndropping performance. Relative to fixed-budget and uniform penalty baselines,\nALP redistributes its reduced budget more intelligently by cutting compute on\neasy prompts and reallocating saved tokens to difficult ones, delivering higher\naccuracy on the hardest problems with higher cost.",
      "pdf_url": "http://arxiv.org/pdf/2506.05256v2",
      "published": "2025-06-05T17:17:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05256v2",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MesaNet: Sequence Modeling by Locally Optimal Test-Time Training",
      "authors": [
        "Johannes von Oswald",
        "Nino Scherrer",
        "Seijin Kobayashi",
        "Luca Versari",
        "Songlin Yang",
        "Maximilian Schlegel",
        "Kaitlin Maile",
        "Yanick Schimpf",
        "Oliver Sieberling",
        "Alexander Meulemans",
        "Rif A. Saurous",
        "Guillaume Lajoie",
        "Charlotte Frenkel",
        "Razvan Pascanu",
        "Blaise Agüera y Arcas",
        "João Sacramento"
      ],
      "abstract": "Sequence modeling is currently dominated by causal transformer architectures\nthat use softmax self-attention. Although widely adopted, transformers require\nscaling memory and compute linearly during inference. A recent stream of work\nlinearized the softmax operation, resulting in powerful recurrent neural\nnetwork (RNN) models with constant memory and compute costs such as DeltaNet,\nMamba or xLSTM. These models can be unified by noting that their recurrent\nlayer dynamics can all be derived from an in-context regression objective,\napproximately optimized through an online learning rule. Here, we join this\nline of work and introduce a numerically stable, chunkwise parallelizable\nversion of the recently proposed Mesa layer (von Oswald et al., 2024), and\nstudy it in language modeling at the billion-parameter scale. This layer again\nstems from an in-context loss, but which is now minimized to optimality at\nevery time point using a fast conjugate gradient solver. Through an extensive\nsuite of experiments, we show that optimal test-time training enables reaching\nlower language modeling perplexity and higher downstream benchmark performance\nthan previous RNNs, especially on tasks requiring long context understanding.\nThis performance gain comes at the cost of additional flops spent during\ninference time. Our results are therefore intriguingly related to recent trends\nof increasing test-time compute to improve performance -- here by spending\ncompute to solve sequential optimization problems within the neural network\nitself.",
      "pdf_url": "http://arxiv.org/pdf/2506.05233v1",
      "published": "2025-06-05T16:50:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05233v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning",
      "authors": [
        "Jingyu Hu",
        "Hongbo Bo",
        "Jun Hong",
        "Xiaowei Liu",
        "Weiru Liu"
      ],
      "abstract": "Graph Neural Networks (GNNs) often suffer from degree bias in node\nclassification tasks, where prediction performance varies across nodes with\ndifferent degrees. Several approaches, which adopt Graph Contrastive Learning\n(GCL), have been proposed to mitigate this bias. However, the limited number of\npositive pairs and the equal weighting of all positives and negatives in GCL\nstill lead to low-degree nodes acquiring insufficient and noisy information.\nThis paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to\nmitigate degree bias. It adds more positive pairs by leveraging node labels and\nadaptively weights positive and negative pairs based on their learning\nhardness. In addition, we develop an experimental framework named SHARP to\nextend HAR to a broader range of scenarios. Both our theoretical analysis and\nexperiments validate the effectiveness of SHARP. The experimental results\nacross four datasets show that SHARP achieves better performance against\nbaselines at both global and degree levels.",
      "pdf_url": "http://arxiv.org/pdf/2506.05214v1",
      "published": "2025-06-05T16:28:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05214v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "LLM-First Search: Self-Guided Exploration of the Solution Space",
      "authors": [
        "Nathan Herr",
        "Tim Rocktäschel",
        "Roberta Raileanu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable improvements in\nreasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed\nexploration hyperparameters limits their adaptability across tasks of varying\ndifficulty, rendering them impractical or expensive in certain settings. In\nthis paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM\nSelf-Guided Search} method that removes the need for pre-defined search\nstrategies by empowering the LLM to autonomously control the search process via\nself-guided exploration. Rather than relying on external heuristics or\nhardcoded policies, the LLM evaluates whether to pursue the current search path\nor explore alternative branches based on its internal scoring mechanisms. This\nenables more flexible and context-sensitive reasoning without requiring manual\ntuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku\nagainst three classic widely-used search algorithms, Tree-of-Thoughts' Breadth\nFirst Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which\nhave been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with\nincreased compute budget. Our code is publicly available at\n\\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.",
      "pdf_url": "http://arxiv.org/pdf/2506.05213v1",
      "published": "2025-06-05T16:27:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05213v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Intentionally Unintentional: GenAI Exceptionalism and the First Amendment",
      "authors": [
        "David Atkinson",
        "Jena D. Hwang",
        "Jacob Morrison"
      ],
      "abstract": "This paper challenges the assumption that courts should grant First Amendment\nprotections to outputs from large generative AI models, such as GPT-4 and\nGemini. We argue that because these models lack intentionality, their outputs\ndo not constitute speech as understood in the context of established legal\nprecedent, so there can be no speech to protect. Furthermore, if the model\noutputs are not speech, users cannot claim a First Amendment speech right to\nreceive the outputs. We also argue that extending First Amendment rights to AI\nmodels would not serve the fundamental purposes of free speech, such as\npromoting a marketplace of ideas, facilitating self-governance, or fostering\nself-expression. In fact, granting First Amendment protections to AI models\nwould be detrimental to society because it would hinder the government's\nability to regulate these powerful technologies effectively, potentially\nleading to the unchecked spread of misinformation and other harms.",
      "pdf_url": "http://arxiv.org/pdf/2506.05211v1",
      "published": "2025-06-05T16:26:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05211v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Counterfactual reasoning: an analysis of in-context emergence",
      "authors": [
        "Moritz Miller",
        "Bernhard Schölkopf",
        "Siyuan Guo"
      ],
      "abstract": "Large-scale neural language models (LMs) exhibit remarkable performance in\nin-context learning: the ability to learn and reason the input context on the\nfly without parameter update. This work studies in-context counterfactual\nreasoning in language models, that is, to predict the consequences of changes\nunder hypothetical scenarios. We focus on studying a well-defined synthetic\nsetup: a linear regression task that requires noise abduction, where accurate\nprediction is based on inferring and copying the contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning in this controlled setup and provide insights that counterfactual\nreasoning for a broad class of functions can be reduced to a transformation on\nin-context observations; we find self-attention, model depth, and data\ndiversity in pre-training drive performance in Transformers. More\ninterestingly, our findings extend beyond regression tasks and show that\nTransformers can perform noise abduction on sequential data, providing\npreliminary evidence on the potential for counterfactual story generation. Our\ncode is available under\nhttps://github.com/moXmiller/counterfactual-reasoning.git .",
      "pdf_url": "http://arxiv.org/pdf/2506.05188v1",
      "published": "2025-06-05T16:02:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05188v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "TreeRPO: Tree Relative Policy Optimization",
      "authors": [
        "Zhicheng Yang",
        "Zhijiang Guo",
        "Yinya Huang",
        "Xiaodan Liang",
        "Yiwei Wang",
        "Jing Tang"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,\na key limitation of existing approaches is that rewards defined at the full\ntrajectory level provide insufficient guidance for optimizing the intermediate\nsteps of a reasoning process. To address this, we introduce \\textbf{\\name}, a\nnovel method that estimates the mathematical expectations of rewards at various\nreasoning steps using tree sampling. Unlike prior methods that rely on a\nseparate step reward model, \\name directly estimates these rewards through this\nsampling process. Building on the group-relative reward training mechanism of\nGRPO, \\name innovatively computes rewards based on step-level groups generated\nduring tree sampling. This advancement allows \\name to produce fine-grained and\ndense reward signals, significantly enhancing the learning process and overall\nperformance of LLMs. Experimental results demonstrate that our \\name algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name\nsignificantly outperforms GRPO by 2.9\\% in performance while simultaneously\nreducing the average response length by 18.1\\%, showcasing its effectiveness\nand efficiency. Our code will be available at\n\\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.",
      "pdf_url": "http://arxiv.org/pdf/2506.05183v1",
      "published": "2025-06-05T15:56:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05183v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
      "authors": [
        "Yeonseok Jeong",
        "Jinsu Kim",
        "Dohyeon Lee",
        "Seung-won Hwang"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
      "pdf_url": "http://arxiv.org/pdf/2506.05167v2",
      "published": "2025-06-05T15:43:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05167v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective",
      "authors": [
        "Bhavik Chandna",
        "Zubair Bashir",
        "Procheta Sen"
      ],
      "abstract": "Large Language Models (LLMs) are known to exhibit social, demographic, and\ngender biases, often as a consequence of the data on which they are trained. In\nthis work, we adopt a mechanistic interpretability approach to analyze how such\nbiases are structurally represented within models such as GPT-2 and Llama2.\nFocusing on demographic and gender biases, we explore different metrics to\nidentify the internal edges responsible for biased behavior. We then assess the\nstability, localization, and generalizability of these components across\ndataset and linguistic variations. Through systematic ablations, we demonstrate\nthat bias-related computations are highly localized, often concentrated in a\nsmall subset of layers. Moreover, the identified components change across\nfine-tuning settings, including those unrelated to bias. Finally, we show that\nremoving these components not only reduces biased outputs but also affects\nother NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.05166v2",
      "published": "2025-06-05T15:43:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05166v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Knowledgeable-r1: Policy Optimization for Knowledge Exploration in Retrieval-Augmented Generation",
      "authors": [
        "Chenyu Lin",
        "Yilin Wen",
        "Du Su",
        "Fei Sun",
        "Muhan Chen",
        "Chenfu Bao",
        "Zhonghou Lv"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is a mainstream method for improving\nperformance on knowledge-intensive tasks. However,current RAG systems often\nplace too much emphasis on retrieved contexts. This can lead to reliance on\ninaccurate sources and overlook the model's inherent knowledge, especially when\ndealing with misleading or excessive information. To resolve this imbalance, we\npropose Knowledgeable-r1 that using joint sampling and define multi policy\ndistributions in knowledge capability exploration to stimulate large language\nmodels'self-integrated utilization of parametric and contextual knowledge.\nExperiments show that Knowledgeable-r1 significantly enhances robustness and\nreasoning accuracy in both parameters and contextual conflict tasks and general\nRAG tasks, especially outperforming baselines by 17.07% in counterfactual\nscenarios and demonstrating consistent gains across RAG tasks. Our code are\navailable at https://github.com/lcy80366872/ knowledgeable-r1.",
      "pdf_url": "http://arxiv.org/pdf/2506.05154v1",
      "published": "2025-06-05T15:34:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05154v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models",
      "authors": [
        "Chih-Kai Yang",
        "Neo Ho",
        "Yi-Jyun Lee",
        "Hung-yi Lee"
      ],
      "abstract": "Understanding the internal mechanisms of large audio-language models (LALMs)\nis crucial for interpreting their behavior and improving performance. This work\npresents the first in-depth analysis of how LALMs internally perceive and\nrecognize auditory attributes. By applying vocabulary projection on three\nstate-of-the-art LALMs, we track how attribute information evolves across\nlayers and token positions. We find that attribute information generally\ndecreases with layer depth when recognition fails, and that resolving\nattributes at earlier layers correlates with better accuracy. Moreover, LALMs\nheavily rely on querying auditory inputs for predicting attributes instead of\naggregating necessary information in hidden states at attribute-mentioning\npositions. Based on our findings, we demonstrate a method to enhance LALMs. Our\nresults offer insights into auditory attribute processing, paving the way for\nfuture improvements.",
      "pdf_url": "http://arxiv.org/pdf/2506.05140v1",
      "published": "2025-06-05T15:22:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05140v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning",
      "authors": [
        "Tanmay Parekh",
        "Kartik Mehta",
        "Ninareh Mehrabi",
        "Kai-Wei Chang",
        "Nanyun Peng"
      ],
      "abstract": "Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework.",
      "pdf_url": "http://arxiv.org/pdf/2506.05128v1",
      "published": "2025-06-05T15:16:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05128v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Truly Self-Improving Agents Require Intrinsic Metacognitive Learning",
      "authors": [
        "Tennison Liu",
        "Mihaela van der Schaar"
      ],
      "abstract": "Self-improving agents aim to continuously acquire new capabilities with\nminimal supervision. However, current approaches face two key limitations:\ntheir self-improvement processes are often rigid, fail to generalize across\ntasks domains, and struggle to scale with increasing agent capabilities. We\nargue that effective self-improvement requires intrinsic metacognitive\nlearning, defined as an agent's intrinsic ability to actively evaluate, reflect\non, and adapt its own learning processes. Drawing inspiration from human\nmetacognition, we introduce a formal framework comprising three components:\nmetacognitive knowledge (self-assessment of capabilities, tasks, and learning\nstrategies), metacognitive planning (deciding what and how to learn), and\nmetacognitive evaluation (reflecting on learning experiences to improve future\nlearning). Analyzing existing self-improving agents, we find they rely\npredominantly on extrinsic metacognitive mechanisms, which are fixed,\nhuman-designed loops that limit scalability and adaptability. Examining each\ncomponent, we contend that many ingredients for intrinsic metacognition are\nalready present. Finally, we explore how to optimally distribute metacognitive\nresponsibilities between humans and agents, and robustly evaluate and improve\nintrinsic metacognitive learning, key challenges that must be addressed to\nenable truly sustained, generalized, and aligned self-improvement.",
      "pdf_url": "http://arxiv.org/pdf/2506.05109v1",
      "published": "2025-06-05T14:53:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05109v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Survey on the Evaluation of Generative Models in Music",
      "authors": [
        "Alexander Lerch",
        "Claire Arthur",
        "Nick Bryan-Kinns",
        "Corey Ford",
        "Qianyi Sun",
        "Ashvala Vinay"
      ],
      "abstract": "Research on generative systems in music has seen considerable attention and\ngrowth in recent years. A variety of attempts have been made to systematically\nevaluate such systems. We provide an interdisciplinary review of the common\nevaluation targets, methodologies, and metrics for the evaluation of both\nsystem output and model usability, covering subjective and objective\napproaches, qualitative and quantitative approaches, as well as empirical and\ncomputational methods. We discuss the advantages and challenges of such\napproaches from a musicological, an engineering, and an HCI perspective.",
      "pdf_url": "http://arxiv.org/pdf/2506.05104v1",
      "published": "2025-06-05T14:46:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05104v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation",
      "authors": [
        "Keyu Zhao",
        "Fengli Xu",
        "Yong Li"
      ],
      "abstract": "Driven by advances in Large Language Models (LLMs), integrating them into\nrecommendation tasks has gained interest due to their strong semantic\nunderstanding and prompt flexibility. Prior work encoded user-item interactions\nor metadata into prompts for recommendations. In parallel, LLM reasoning,\nboosted by test-time scaling and reinforcement learning, has excelled in fields\nlike mathematics and code, where reasoning traces and correctness signals are\nclear, enabling high performance and interpretability. However, directly\napplying these reasoning methods to recommendation is ineffective because user\nfeedback is implicit and lacks reasoning supervision. To address this, we\npropose $\\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that\nsamples interaction chains from the user-item graph and converts them into\nstructured interaction-of-thoughts via a progressive masked prompting strategy,\nwith each thought representing stepwise reasoning grounded in interaction\ncontext. This allows LLMs to simulate step-by-step decision-making based on\nimplicit patterns. We design a two-stage training pipeline: supervised\nfine-tuning teaches basic reasoning from high-quality traces, and reinforcement\nlearning refines reasoning via reward signals, alleviating sparse explicit\nsupervision. Experiments on three real-world datasets show R2Rec outperforms\nclassical and LLM-based baselines with an average $\\textbf{10.48%}$ improvement\nin HitRatio@1 and $\\textbf{131.81%}$ gain over the original LLM. Furthermore,\nthe explicit reasoning chains enhance interpretability by revealing the\ndecision process. Our code is available at:\nhttps://anonymous.4open.science/r/R2Rec-7C5D.",
      "pdf_url": "http://arxiv.org/pdf/2506.05069v1",
      "published": "2025-06-05T14:16:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05069v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Does It Make Sense to Speak of Introspection in Large Language Models?",
      "authors": [
        "Iulia M. Comsa",
        "Murray Shanahan"
      ],
      "abstract": "Large language models (LLMs) exhibit compelling linguistic behaviour, and\nsometimes offer self-reports, that is to say statements about their own nature,\ninner workings, or behaviour. In humans, such reports are often attributed to a\nfaculty of introspection and are typically linked to consciousness. This raises\nthe question of how to interpret self-reports produced by LLMs, given their\nincreasing linguistic fluency and cognitive capabilities. To what extent (if\nany) can the concept of introspection be meaningfully applied to LLMs? Here, we\npresent and critique two examples of apparent introspective self-report from\nLLMs. In the first example, an LLM attempts to describe the process behind its\nown \"creative\" writing, and we argue this is not a valid example of\nintrospection. In the second example, an LLM correctly infers the value of its\nown temperature parameter, and we argue that this can be legitimately\nconsidered a minimal example of introspection, albeit one that is (presumably)\nnot accompanied by conscious experience.",
      "pdf_url": "http://arxiv.org/pdf/2506.05068v2",
      "published": "2025-06-05T14:13:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05068v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages",
      "authors": [
        "Moshe Ofer",
        "Orel Zamler",
        "Amos Azaria"
      ],
      "abstract": "Large Language Models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages due to limited training data. This paper presents\nTALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource\nLanguages), which integrates an LLM with two bilingual translation models. TALL\ntransforms low-resource inputs into high-resource representations, leveraging\nthe LLM's capabilities while preserving linguistic features through dimension\nalignment layers and custom transformers. Our experiments on Hebrew demonstrate\nsignificant improvements over several baselines, including direct use, naive\ntranslation, and fine-tuning approaches. The architecture employs a\nparameter-efficient strategy, freezing pre-trained components while training\nonly lightweight adapter modules, balancing computational efficiency with\nperformance gains.",
      "pdf_url": "http://arxiv.org/pdf/2506.05057v1",
      "published": "2025-06-05T14:02:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05057v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation",
      "authors": [
        "Hyeongwon Jang",
        "Changhun Kim",
        "Eunho Yang"
      ],
      "abstract": "Recent explainable artificial intelligence (XAI) methods for time series\nprimarily estimate point-wise attribution magnitudes, while overlooking the\ndirectional impact on predictions, leading to suboptimal identification of\nsignificant points. Our analysis shows that conventional Integrated Gradients\n(IG) effectively capture critical points with both positive and negative\nimpacts on predictions. However, current evaluation metrics fail to assess this\ncapability, as they inadvertently cancel out opposing feature contributions. To\naddress this limitation, we propose novel evaluation metrics-Cumulative\nPrediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to\nsystematically assess whether attribution methods accurately identify\nsignificant positive and negative points in time series XAI. Under these\nmetrics, conventional IG outperforms recent counterparts. However, directly\napplying IG to time series data may lead to suboptimal outcomes, as generated\npaths ignore temporal relationships and introduce out-of-distribution samples.\nTo overcome these challenges, we introduce TIMING, which enhances IG by\nincorporating temporal awareness while maintaining its theoretical properties.\nExtensive experiments on synthetic and real-world time series benchmarks\ndemonstrate that TIMING outperforms existing time series XAI baselines. Our\ncode is available at https://github.com/drumpt/TIMING.",
      "pdf_url": "http://arxiv.org/pdf/2506.05035v1",
      "published": "2025-06-05T13:40:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05035v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Identifying and Understanding Cross-Class Features in Adversarial Training",
      "authors": [
        "Zeming Wei",
        "Yiwen Guo",
        "Yisen Wang"
      ],
      "abstract": "Adversarial training (AT) has been considered one of the most effective\nmethods for making deep neural networks robust against adversarial attacks,\nwhile the training mechanisms and dynamics of AT remain open research problems.\nIn this paper, we present a novel perspective on studying AT through the lens\nof class-wise feature attribution. Specifically, we identify the impact of a\nkey family of features on AT that are shared by multiple classes, which we call\ncross-class features. These features are typically useful for robust\nclassification, which we offer theoretical evidence to illustrate through a\nsynthetic data model. Through systematic studies across multiple model\narchitectures and settings, we find that during the initial stage of AT, the\nmodel tends to learn more cross-class features until the best robustness\ncheckpoint. As AT further squeezes the training robust loss and causes robust\noverfitting, the model tends to make decisions based on more class-specific\nfeatures. Based on these discoveries, we further provide a unified view of two\nexisting properties of AT, including the advantage of soft-label training and\nrobust overfitting. Overall, these insights refine the current understanding of\nAT mechanisms and provide new perspectives on studying them. Our code is\navailable at https://github.com/PKU-ML/Cross-Class-Features-AT.",
      "pdf_url": "http://arxiv.org/pdf/2506.05032v1",
      "published": "2025-06-05T13:40:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05032v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV",
        "math.OC"
      ]
    },
    {
      "title": "Artificial Intelligence Should Genuinely Support Clinical Reasoning and Decision Making To Bridge the Translational Gap",
      "authors": [
        "Kacper Sokol",
        "James Fackler",
        "Julia E Vogt"
      ],
      "abstract": "Artificial intelligence promises to revolutionise medicine, yet its impact\nremains limited because of the pervasive translational gap. We posit that the\nprevailing technology-centric approaches underpin this challenge, rendering\nsuch systems fundamentally incompatible with clinical practice, specifically\ndiagnostic reasoning and decision making. Instead, we propose a novel\nsociotechnical conceptualisation of data-driven support tools designed to\ncomplement doctors' cognitive and epistemic activities. Crucially, it\nprioritises real-world impact over superhuman performance on inconsequential\nbenchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2506.05030v1",
      "published": "2025-06-05T13:39:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05030v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System",
      "authors": [
        "Haokun Liu",
        "Zhaoqi Ma",
        "Yunong Li",
        "Junichiro Sugihara",
        "Yicheng Chen",
        "Jinjie Li",
        "Moju Zhao"
      ],
      "abstract": "Heterogeneous multi-robot systems show great potential in complex tasks\nrequiring coordinated hybrid cooperation. However, traditional approaches\nrelying on static models often struggle with task diversity and dynamic\nenvironments. This highlights the need for generalizable intelligence that can\nbridge high-level reasoning with low-level execution across heterogeneous\nagents. To address this, we propose a hierarchical framework integrating a\nprompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision\nLanguage Model (VLM). The LLM performs task decomposition and global semantic\nmap construction, while the VLM extracts task-specified semantic labels and 2D\nspatial information from aerial images to support local planning. Within this\nframework, the aerial robot follows a globally optimized semantic path and\ncontinuously provides bird-view images, guiding the ground robot's local\nsemantic navigation and manipulation, including target-absent scenarios where\nimplicit alignment is maintained. Experiments on a real-world letter-cubes\narrangement task demonstrate the framework's adaptability and robustness in\ndynamic environments. To the best of our knowledge, this is the first\ndemonstration of an aerial-ground heterogeneous system integrating VLM-based\nperception with LLM-driven task reasoning and motion planning.",
      "pdf_url": "http://arxiv.org/pdf/2506.05020v1",
      "published": "2025-06-05T13:27:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05020v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Reasonable Concept Bottleneck Models",
      "authors": [
        "Nektarios Kalampalikis",
        "Kavya Gupta",
        "Georgi Vitanov",
        "Isabel Valera"
      ],
      "abstract": "In this paper, we propose $\\textbf{C}$oncept $\\textbf{REA}$soning\n$\\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs)\nthat: (i) explicitly encodes concept-concept (${\\texttt{C-C}}$) and\nconcept-task (${\\texttt{C$\\rightarrow$Y}}$) relationships to enforce a desired\nmodel reasoning; and (ii) use a regularized side-channel to achieve competitive\ntask performance, while keeping high concept importance. Specifically, CREAM\narchitecturally embeds (bi)directed concept-concept, and concept to task\nrelationships specified by a human expert, while severing undesired information\nflows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates\na black-box side-channel that is regularized to encourage task predictions to\nbe grounded in the relevant concepts, thereby utilizing the side-channel only\nwhen necessary to enhance performance. Our experiments show that: (i) CREAM\nmainly relies on concepts while achieving task performance on par with\nblack-box models; and (ii) the embedded ${\\texttt{C-C}}$ and\n${\\texttt{C$\\rightarrow$Y}}$ relationships ease model interventions and\nmitigate concept leakage.",
      "pdf_url": "http://arxiv.org/pdf/2506.05014v1",
      "published": "2025-06-05T13:22:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.05014v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning",
      "authors": [
        "Mehdi Azarafza",
        "Mojtaba Nayyeri",
        "Faezeh Pasandideh",
        "Steffen Staab",
        "Achim Rettberg"
      ],
      "abstract": "Autonomous UAV operation necessitates reliable mathematical reasoning for\ntasks such as trajectory planning and power management. While traditional\nflight control relies on hardcoded equations, recent Large Language Models\n(LLMs) offer potential for more flexible problem-solving but struggle with\nreliably selecting and applying correct mathematical formulations and executing\nprecise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented\ngeneration framework designed to improve the mathematical reasoning of several\nLLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in\nUAV-specific contexts by providing access to relevant domain literature. To\nconduct an initial assessment, we introduce the UAV-Math-Bench, a small problem\nset comprising 20 UAV-centric mathematical problems across four difficulty\nlevels. Our experiments demonstrate that incorporating retrieval substantially\nincreases exact answer accuracy (achieving up to 75% with o1), reduces\ninstances of incorrect formulation selection (from 25% without RAG to 5% with\nRAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders\nof magnitude for the best-performing models. This pilot study indicates that\nRAG can enable general-purpose LLMs to function as more reliable tools for\nengineering analysis, although direct real-time flight control requires further\ninvestigation and validation on a larger scale. All benchmark data, question\nand answer are publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2506.04998v1",
      "published": "2025-06-05T13:09:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04998v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair",
      "authors": [
        "Zanis Ali Khan",
        "Aayush Garg",
        "Qiang Tang"
      ],
      "abstract": "Software vulnerabilities pose significant security threats, requiring\neffective mitigation. While Automated Program Repair (APR) has advanced in\nfixing general bugs, vulnerability patching, a security-critical aspect of APR\nremains underexplored. This study investigates pre-trained language models,\nCodeBERT and CodeT5, for automated vulnerability patching across six datasets\nand four languages. We evaluate their accuracy and generalization to unknown\nvulnerabilities. Results show that while both models face challenges with\nfragmented or sparse context, CodeBERT performs comparatively better in such\nscenarios, whereas CodeT5 excels in capturing complex vulnerability patterns.\nCodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned\nmodels on both in-distribution (trained) and out-of-distribution (unseen)\ndatasets. While fine-tuning improves in-distribution performance, models\nstruggle to generalize to unseen data, highlighting challenges in robust\nvulnerability detection. This study benchmarks model performance, identifies\nlimitations in generalization, and provides actionable insights to advance\nautomated vulnerability patching for real-world security applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.04987v1",
      "published": "2025-06-05T13:00:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04987v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation",
      "authors": [
        "Adrian Marius Dumitran",
        "Theodor-Pierre Moroianu",
        "Vasile Paul Alexe"
      ],
      "abstract": "This paper presents a comprehensive evaluation of the performance of\nstate-of-the-art Large Language Models (LLMs) on challenging university-level\nalgorithms exams. By testing multiple models on both a Romanian exam and its\nhigh-quality English translation, we analyze LLMs' problem-solving\ncapabilities, consistency, and multilingual performance. Our empirical study\nreveals that the most recent models not only achieve scores comparable to\ntop-performing students but also demonstrate robust reasoning skills on\ncomplex, multi-step algorithmic challenges, even though difficulties remain\nwith graph-based tasks. Building on these findings, we explore the potential of\nLLMs to support educational environments through the generation of high-quality\neditorial content, offering instructors a powerful tool to enhance student\nfeedback. The insights and best practices discussed herein pave the way for\nfurther integration of generative AI in advanced algorithm education.",
      "pdf_url": "http://arxiv.org/pdf/2506.04965v1",
      "published": "2025-06-05T12:41:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04965v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations",
      "authors": [
        "Igor Meleshin",
        "Anna Chistyakova",
        "Anastasia Antsiferova",
        "Dmitriy Vatolin"
      ],
      "abstract": "Image Quality Assessment (IQA) models are increasingly relied upon to\nevaluate image quality in real-world systems -- from compression and\nenhancement to generation and streaming. Yet their adoption brings a\nfundamental risk: these models are inherently unstable. Adversarial\nmanipulations can easily fool them, inflating scores and undermining trust.\nTraditionally, such vulnerabilities are addressed through data-driven defenses\n-- adversarial retraining, regularization, or input purification. But what if\nthis is the wrong lens? What if robustness in perceptual models is not\nsomething to learn but something to design? In this work, we propose a\nprovocative idea: robustness as an architectural prior. Rather than training\nmodels to resist perturbations, we reshape their internal structure to suppress\nsensitivity from the ground up. We achieve this by enforcing orthogonal\ninformation flow, constraining the network to norm-preserving operations -- and\nfurther stabilizing the system through pruning and fine-tuning. The result is a\nrobust IQA architecture that withstands adversarial attacks without requiring\nadversarial training or significant changes to the original model. This\napproach suggests a shift in perspective: from optimizing robustness through\ndata to engineering it through design.",
      "pdf_url": "http://arxiv.org/pdf/2506.04951v1",
      "published": "2025-06-05T12:24:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04951v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CzechLynx: A Dataset for Individual Identification and Pose Estimation of the Eurasian Lynx",
      "authors": [
        "Lukas Picek",
        "Elisa Belotti",
        "Michal Bojda",
        "Ludek Bufka",
        "Vojtech Cermak",
        "Martin Dula",
        "Rostislav Dvorak",
        "Luboslav Hrdy",
        "Miroslav Jirik",
        "Vaclav Kocourek",
        "Josefa Krausova",
        "Jirı Labuda",
        "Jakub Straka",
        "Ludek Toman",
        "Vlado Trulık",
        "Martin Vana",
        "Miroslav Kutal"
      ],
      "abstract": "We introduce CzechLynx, the first large-scale, open-access dataset for\nindividual identification, 2D pose estimation, and instance segmentation of the\nEurasian lynx (Lynx lynx). CzechLynx includes more than 30k camera trap images\nannotated with segmentation masks, identity labels, and 20-point skeletons and\ncovers 219 unique individuals across 15 years of systematic monitoring in two\ngeographically distinct regions: Southwest Bohemia and the Western Carpathians.\nTo increase the data variability, we create a complementary synthetic set with\nmore than 100k photorealistic images generated via a Unity-based pipeline and\ndiffusion-driven text-to-texture modeling, covering diverse environments,\nposes, and coat-pattern variations. To allow testing generalization across\nspatial and temporal domains, we define three tailored evaluation\nprotocols/splits: (i) geo-aware, (ii) time-aware open-set, and (iii) time-aware\nclosed-set. This dataset is targeted to be instrumental in benchmarking\nstate-of-the-art models and the development of novel methods for not just\nindividual animal re-identification.",
      "pdf_url": "http://arxiv.org/pdf/2506.04931v1",
      "published": "2025-06-05T12:05:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04931v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback",
      "authors": [
        "Junior Cedric Tonga",
        "KV Aditya Srivatsa",
        "Kaushal Kumar Maurya",
        "Fajri Koto",
        "Ekaterina Kochmar"
      ],
      "abstract": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive.",
      "pdf_url": "http://arxiv.org/pdf/2506.04920v1",
      "published": "2025-06-05T11:53:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04920v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Energentic Intelligence: From Self-Sustaining Systems to Enduring Artificial Life",
      "authors": [
        "Atahan Karagoz"
      ],
      "abstract": "This paper introduces Energentic Intelligence, a class of autonomous systems\ndefined not by task performance, but by their capacity to sustain themselves\nthrough internal energy regulation. Departing from conventional reward-driven\nparadigms, these agents treat survival-maintaining functional operation under\nfluctuating energetic and thermal conditions-as the central objective. We\nformalize this principle through an energy-based utility function and a\nviability-constrained survival horizon, and propose a modular architecture that\nintegrates energy harvesting, thermal regulation, and adaptive computation into\na closed-loop control system. A simulated environment demonstrates the\nemergence of stable, resource-aware behavior without external supervision.\nTogether, these contributions provide a theoretical and architectural\nfoundation for deploying autonomous agents in resource-volatile settings where\npersistence must be self-regulated and infrastructure cannot be assumed.",
      "pdf_url": "http://arxiv.org/pdf/2506.04916v1",
      "published": "2025-06-05T11:52:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04916v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Differentiable Logic Cellular Automata: From Game of Life to Pattern Generation",
      "authors": [
        "Pietro Miotti",
        "Eyvind Niklasson",
        "Ettore Randazzo",
        "Alexander Mordvintsev"
      ],
      "abstract": "This paper introduces Differentiable Logic Cellular Automata (DiffLogic CA),\na novel combination of Neural Cellular Automata (NCA) and Differentiable Logic\nGates Networks (DLGNs). The fundamental computation units of the model are\ndifferentiable logic gates, combined into a circuit. During training, the model\nis fully end-to-end differentiable allowing gradient-based training, and at\ninference time it operates in a fully discrete state space. This enables\nlearning local update rules for cellular automata while preserving their\ninherent discrete nature. We demonstrate the versatility of our approach\nthrough a series of milestones: (1) fully learning the rules of Conway's Game\nof Life, (2) generating checkerboard patterns that exhibit resilience to noise\nand damage, (3) growing a lizard shape, and (4) multi-color pattern generation.\nOur model successfully learns recurrent circuits capable of generating desired\ntarget patterns. For simpler patterns, we observe success with both synchronous\nand asynchronous updates, demonstrating significant generalization capabilities\nand robustness to perturbations. We make the case that this combination of\nDLGNs and NCA represents a step toward programmable matter and robust computing\nsystems that combine binary logic, neural network adaptability, and localized\nprocessing. This work, to the best of our knowledge, is the first successful\napplication of differentiable logic gate networks in recurrent architectures.",
      "pdf_url": "http://arxiv.org/pdf/2506.04912v1",
      "published": "2025-06-05T11:45:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04912v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models",
      "authors": [
        "Kai Wang",
        "Yihao Zhang",
        "Meng Sun"
      ],
      "abstract": "The honesty of large language models (LLMs) is a critical alignment\nchallenge, especially as advanced systems with chain-of-thought (CoT) reasoning\nmay strategically deceive humans. Unlike traditional honesty issues on LLMs,\nwhich could be possibly explained as some kind of hallucination, those models'\nexplicit thought paths enable us to study strategic deception--goal-driven,\nintentional misinformation where reasoning contradicts outputs. Using\nrepresentation engineering, we systematically induce, detect, and control such\ndeception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear\nArtificial Tomography (LAT) for 89% detection accuracy. Through activation\nsteering, we achieve a 40% success rate in eliciting context-appropriate\ndeception without explicit prompts, unveiling the specific honesty-related\nissue of reasoning models and providing tools for trustworthy AI alignment.",
      "pdf_url": "http://arxiv.org/pdf/2506.04909v1",
      "published": "2025-06-05T11:44:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04909v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots",
      "authors": [
        "Alex Pan",
        "Mary-Anne Williams"
      ],
      "abstract": "Large Language Models (LLMs), whilst great at extracting facts from text,\nstruggle with nested narrative reasoning. Existing long context and multi-hop\nQA benchmarks inadequately test this, lacking realistic distractors or failing\nto decouple context length from reasoning complexity, masking a fundamental LLM\nlimitation. We introduce Verbose ListOps, a novel benchmark that\nprogrammatically transposes ListOps computations into lengthy, coherent\nstories. This uniquely forces internal computation and state management of\nnested reasoning problems by withholding intermediate results, and offers\nfine-grained controls for both narrative size \\emph{and} reasoning difficulty.\nWhilst benchmarks like LongReason (2025) advance approaches for synthetically\nexpanding the context size of multi-hop QA problems, Verbose ListOps pinpoints\na specific LLM vulnerability: difficulty in state management for nested\nsub-reasoning amongst semantically-relevant, distracting narrative. Our\nexperiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse\nin performance on Verbose ListOps at modest (~10k token) narrative lengths,\ndespite effortlessly solving raw ListOps equations. Addressing this failure is\nparamount for real-world text interpretation which requires identifying key\nreasoning points, tracking conceptual intermediate results, and filtering\nirrelevant information. Verbose ListOps, and its extensible generation\nframework thus enables targeted reasoning enhancements beyond mere\ncontext-window expansion; a critical step to automating the world's knowledge\nwork.",
      "pdf_url": "http://arxiv.org/pdf/2506.04907v1",
      "published": "2025-06-05T11:41:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04907v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "LLMs for sensory-motor control: Combining in-context and iterative learning",
      "authors": [
        "Jônata Tyska Carvalho",
        "Stefano Nolfi"
      ],
      "abstract": "We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. Initially, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. In most cases, it\nsuccessfully identifies optimal or high-performing solutions by integrating\nsymbolic knowledge derived through reasoning with sub-symbolic sensory-motor\ndata gathered as the agent interacts with its environment.",
      "pdf_url": "http://arxiv.org/pdf/2506.04867v1",
      "published": "2025-06-05T10:38:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04867v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Towards Network Data Analytics in 5G Systems and Beyond",
      "authors": [
        "Marcos Lima Romero",
        "Ricardo Suyama"
      ],
      "abstract": "Data has become a critical asset in the digital economy, yet it remains\nunderutilized by Mobile Network Operators (MNOs), unlike Over-the-Top (OTT)\nplayers that lead global market valuations. To move beyond the commoditization\nof connectivity and deliver greater value to customers, data analytics emerges\nas a strategic enabler. Using data efficiently is essential for unlocking new\nservice opportunities, optimizing operational efficiency, and mitigating\noperational and business risks. Since Release 15, the 3rd Generation\nPartnership Project (3GPP) has introduced the Network Data Analytics Function\n(NWDAF) to provide powerful insights and predictions using data collected\nacross mobile networks, supporting both user-centric and network-oriented use\ncases. However, academic research has largely focused on a limited set of\nmethods and use cases, driven by the availability of datasets, restricting\nbroader exploration. This study analyzes trends and gaps in more than 70\narticles and proposes two novel use cases to promote the adoption of NWDAF and\nexplore its potential for monetization.",
      "pdf_url": "http://arxiv.org/pdf/2506.04860v1",
      "published": "2025-06-05T10:26:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04860v1",
      "categories": [
        "cs.NI",
        "cs.AI"
      ]
    },
    {
      "title": "Sparse Autoencoders, Again?",
      "authors": [
        "Yin Lu",
        "Xuening Zhu",
        "Tong He",
        "David Wipf"
      ],
      "abstract": "Is there really much more to say about sparse autoencoders (SAEs)?\nAutoencoders in general, and SAEs in particular, represent deep architectures\nthat are capable of modeling low-dimensional latent structure in data. Such\nstructure could reflect, among other things, correlation patterns in large\nlanguage model activations, or complex natural image manifolds. And yet despite\nthe wide-ranging applicability, there have been relatively few changes to SAEs\nbeyond the original recipe from decades ago, namely, standard deep\nencoder/decoder layers trained with a classical/deterministic sparse\nregularizer applied within the latent space. One possible exception is the\nvariational autoencoder (VAE), which adopts a stochastic encoder module capable\nof producing sparse representations when applied to manifold data. In this work\nwe formalize underappreciated weaknesses with both canonical SAEs, as well as\nanalogous VAEs applied to similar tasks, and propose a hybrid alternative model\nthat circumvents these prior limitations. In terms of theoretical support, we\nprove that global minima of our proposed model recover certain forms of\nstructured data spread across a union of manifolds. Meanwhile, empirical\nevaluations on synthetic and real-world datasets substantiate the efficacy of\nour approach in accurately estimating underlying manifold dimensions and\nproducing sparser latent representations without compromising reconstruction\nerror. In general, we are able to exceed the performance of equivalent-capacity\nSAEs and VAEs, as well as recent diffusion models where applicable, within\ndomains such as images and language model activation patterns.",
      "pdf_url": "http://arxiv.org/pdf/2506.04859v2",
      "published": "2025-06-05T10:26:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04859v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights",
      "authors": [
        "Giorgio Biancini",
        "Alessio Ferrato",
        "Carla Limongelli"
      ],
      "abstract": "Integrating Artificial Intelligence (AI) in educational settings has brought\nnew learning approaches, transforming the practices of both students and\neducators. Among the various technologies driving this transformation, Large\nLanguage Models (LLMs) have emerged as powerful tools for creating educational\nmaterials and question answering, but there are still space for new\napplications. Educators commonly use Multiple-Choice Questions (MCQs) to assess\nstudent knowledge, but manually generating these questions is\nresource-intensive and requires significant time and cognitive effort. In our\nopinion, LLMs offer a promising solution to these challenges. This paper\npresents a novel comparative analysis of three widely known LLMs - Llama 2,\nMistral, and GPT-3.5 - to explore their potential for creating informative and\nchallenging MCQs. In our approach, we do not rely on the knowledge of the LLM,\nbut we inject the knowledge into the prompt to contrast the hallucinations,\ngiving the educators control over the test's source text, too. Our experiment\ninvolving 21 educators shows that GPT-3.5 generates the most effective MCQs\nacross several known metrics. Additionally, it shows that there is still some\nreluctance to adopt AI in the educational field. This study sheds light on the\npotential of LLMs to generate MCQs and improve the educational experience,\nproviding valuable insights for the future.",
      "pdf_url": "http://arxiv.org/pdf/2506.04851v1",
      "published": "2025-06-05T10:21:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04851v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles",
      "authors": [
        "Julien Soulé",
        "Jean-Paul Jamont",
        "Michel Occello",
        "Paul Théron",
        "Louis-Marie Traonouez"
      ],
      "abstract": "As cyber-attacks show to be more and more complex and coordinated,\ncyber-defenders strategy through multi-agent approaches could be key to tackle\nagainst cyber-attacks as close as entry points in a networked system. This\npaper presents a Markovian modeling and implementation through a simulator of\nfighting cyber-attacker agents and cyber-defender agents deployed on host\nnetwork nodes. It aims to provide an experimental framework to implement\nrealistically based coordinated cyber-attack scenarios while assessing\ncyber-defenders dynamic organizations. We abstracted network nodes by sets of\nproperties including agents' ones. Actions applied by agents model how the\nnetwork reacts depending in a given state and what properties are to change.\nCollective choice of the actions brings the whole environment closer or farther\nfrom respective cyber-attackers and cyber-defenders goals. Using the simulator,\nwe implemented a realistically inspired scenario with several behavior\nimplementation approaches for cyber-defenders and cyber-attackers.",
      "pdf_url": "http://arxiv.org/pdf/2506.04849v1",
      "published": "2025-06-05T10:17:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.04849v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}