{
  "last_updated": "2025-07-26T00:55:24.838296",
  "papers": [
    {
      "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation",
      "authors": [
        "Ye-Chan Kim",
        "SeungJu Cha",
        "Si-Woo Kim",
        "Taewhan Kim",
        "Dong-Jin Kim"
      ],
      "abstract": "Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time.",
      "pdf_url": "http://arxiv.org/pdf/2507.18632v1",
      "published": "2025-07-24T17:59:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18632v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation",
      "authors": [
        "Shuqing Li",
        "Anson Y. Lam",
        "Yun Peng",
        "Wenxuan Wang",
        "Michael R. Lyu"
      ],
      "abstract": "Graphical user interface (UI) software has undergone a fundamental\ntransformation from traditional two-dimensional (2D) desktop/web/mobile\ninterfaces to spatial three-dimensional (3D) environments. While existing work\nhas made remarkable success in automated 2D software generation, such as\nHTML/CSS and mobile app interface code synthesis, the generation of 3D software\nstill remains under-explored. Current methods for 3D software generation\nusually generate the 3D environments as a whole and cannot modify or control\nspecific elements in the software. Furthermore, these methods struggle to\nhandle the complex spatial and semantic constraints inherent in the real world.\nTo address the challenges, we present Scenethesis, a novel\nrequirement-sensitive 3D software synthesis approach that maintains formal\ntraceability between user specifications and generated 3D software. Scenethesis\nis built upon ScenethesisLang, a domain-specific language that serves as a\ngranular constraint-aware intermediate representation (IR) to bridge natural\nlanguage requirements and executable 3D software. It serves both as a\ncomprehensive scene description language enabling fine-grained modification of\n3D software elements and as a formal constraint-expressive specification\nlanguage capable of expressing complex spatial constraints. By decomposing 3D\nsoftware synthesis into stages operating on ScenethesisLang, Scenethesis\nenables independent verification, targeted modification, and systematic\nconstraint satisfaction. Our evaluation demonstrates that Scenethesis\naccurately captures over 80% of user requirements and satisfies more than 90%\nof hard constraints while handling over 100 constraints simultaneously.\nFurthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual\nevaluation scores compared to the state-of-the-art method.",
      "pdf_url": "http://arxiv.org/pdf/2507.18625v1",
      "published": "2025-07-24T17:58:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18625v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "cs.SE"
      ]
    },
    {
      "title": "Moving Out: Physically-grounded Human-AI Collaboration",
      "authors": [
        "Xuhui Kang",
        "Sung-Wook Lee",
        "Haolin Liu",
        "Yuyan Wang",
        "Yen-Ling Kuo"
      ],
      "abstract": "The ability to adapt to physical actions and constraints in an environment is\ncrucial for embodied agents (e.g., robots) to effectively collaborate with\nhumans. Such physically grounded human-AI collaboration must account for the\nincreased complexity of the continuous state-action space and constrained\ndynamics caused by physical constraints. In this paper, we introduce\n\\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a\nwide range of collaboration modes affected by physical attributes and\nconstraints, such as moving heavy items together and maintaining consistent\nactions to move a big item around a corner. Using Moving Out, we designed two\ntasks and collected human-human interaction data to evaluate models' abilities\nto adapt to diverse human behaviors and unseen physical attributes. To address\nthe challenges in physical environments, we propose a novel method, BASS\n(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of\nagents and their understanding of the outcome of actions. Our experiments show\nthat BASS outperforms state-of-the-art models in AI-AI and human-AI\ncollaboration. The project page is available at\n\\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\\_ai/}.",
      "pdf_url": "http://arxiv.org/pdf/2507.18623v1",
      "published": "2025-07-24T17:57:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18623v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning",
      "authors": [
        "Si-Woo Kim",
        "MinJu Jeon",
        "Ye-Chan Kim",
        "Soeun Lee",
        "Taewhan Kim",
        "Dong-Jin Kim"
      ],
      "abstract": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets\ngenerated by text-to-image (T2I) models to mitigate the need for costly manual\nannotation. However, these T2I models often produce images that exhibit\nsemantic misalignments with their corresponding input captions (e.g., missing\nobjects, incorrect attributes), resulting in noisy synthetic image-caption\npairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these\nmethods are ill-suited for the distinct challenges of synthetic data, where\ncaptions are typically well-formed, but images may be inaccurate\nrepresentations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC.\nInstead of conventional filtering or regeneration, SynC focuses on reassigning\ncaptions to the most semantically aligned images already present within the\nsynthetic image pool. Our approach employs a one-to-many mapping strategy by\ninitially retrieving multiple relevant candidate images for each caption. We\nthen apply a cycle-consistency-inspired alignment scorer that selects the best\nimage by verifying its ability to retrieve the original caption via\nimage-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models\non standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art\nresults in several scenarios. SynC offers an effective strategy for curating\nrefined synthetic data to enhance ZIC.",
      "pdf_url": "http://arxiv.org/pdf/2507.18616v1",
      "published": "2025-07-24T17:53:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18616v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Approximate SMT Counting Beyond Discrete Domains",
      "authors": [
        "Arijit Shaw",
        "Kuldeep S. Meel"
      ],
      "abstract": "Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,\nsolving complex formulas across discrete and continuous domains. Recent\nprogress in propositional model counting motivates extending SMT capabilities\ntoward model counting, especially for hybrid SMT formulas. Existing approaches,\nlike bit-blasting, are limited to discrete variables, highlighting the\nchallenge of counting solutions projected onto the discrete domain in hybrid\nformulas.\n  We introduce pact, an SMT model counter for hybrid formulas that uses\nhashing-based approximate model counting to estimate solutions with theoretical\nguarantees. pact makes a logarithmic number of SMT solver calls relative to the\nprojection variables, leveraging optimized hash functions. pact achieves\nsignificant performance improvements over baselines on a large suite of\nbenchmarks. In particular, out of 14,202 instances, pact successfully finished\non 603 instances, while Baseline could only finish on 13 instances.",
      "pdf_url": "http://arxiv.org/pdf/2507.18612v1",
      "published": "2025-07-24T17:48:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18612v1",
      "categories": [
        "cs.LO",
        "cs.AI"
      ]
    },
    {
      "title": "DRWKV: Focusing on Object Edges for Low-Light Image Enhancement",
      "authors": [
        "Xuecheng Bai",
        "Yuxiang Wang",
        "Boyu Hu",
        "Qinyuan Jie",
        "Chuanzhi Xu",
        "Hongru Xiao",
        "Kechen Li",
        "Vera Chung"
      ],
      "abstract": "Low-light image enhancement remains a challenging task, particularly in\npreserving object edge continuity and fine structural details under extreme\nillumination degradation. In this paper, we propose a novel model, DRWKV\n(Detailed Receptance Weighted Key Value), which integrates our proposed Global\nEdge Retinex (GER) theory, enabling effective decoupling of illumination and\nedge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV\nAttention, a spiral-scanning mechanism that captures spatial edge continuity\nand models irregular structures more effectively. Thirdly, we design the\nBilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align\nluminance and chrominance features, improving visual naturalness and mitigating\nartifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV\nachieves leading performance in PSNR, SSIM, and NIQE while maintaining low\ncomputational complexity. Furthermore, DRWKV enhances downstream performance in\nlow-light multi-object tracking tasks, validating its generalization\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2507.18594v1",
      "published": "2025-07-24T17:24:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18594v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff",
      "authors": [
        "Jérôme Emery",
        "Ali Hasanzadeh Karkan",
        "Jean-François Frigon",
        "François Leduc-Primeau"
      ],
      "abstract": "Deep learning (DL) has emerged as a solution for precoding in massive\nmultiple-input multiple-output (mMIMO) systems due to its capacity to learn the\ncharacteristics of the propagation environment. However, training such a model\nrequires high-quality, local datasets at the deployment site, which are often\ndifficult to collect. We propose a transformer-based foundation model for mMIMO\nprecoding that seeks to minimize the energy consumption of the transmitter\nwhile dynamically adapting to per-user rate requirements. At equal energy\nconsumption, zero-shot deployment of the proposed foundation model\nsignificantly outperforms zero forcing, and approaches weighted minimum mean\nsquared error performance with 8x less complexity. To address model adaptation\nin data-scarce settings, we introduce a data augmentation method that finds\ntraining samples similar to the target distribution by computing the cosine\nsimilarity between the outputs of the pre-trained feature extractor. Our work\nenables the implementation of DL-based solutions in practice by addressing\nchallenges of data availability and training complexity. Moreover, the ability\nto dynamically configure per-user rate requirements can be leveraged by higher\nlevel resource allocation and scheduling algorithms for greater control over\nenergy efficiency, spectral efficiency and fairness.",
      "pdf_url": "http://arxiv.org/pdf/2507.18587v1",
      "published": "2025-07-24T17:10:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18587v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs",
      "authors": [
        "Xiaopeng Ke",
        "Hexuan Deng",
        "Xuebo Liu",
        "Jun Rao",
        "Zhenxi Song",
        "Jun Yu",
        "Min Zhang"
      ],
      "abstract": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt.",
      "pdf_url": "http://arxiv.org/pdf/2507.18584v1",
      "published": "2025-07-24T17:03:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18584v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data",
      "authors": [
        "Zhengyun Zhao",
        "Huaiyuan Ying",
        "Yue Zhong",
        "Sheng Yu"
      ],
      "abstract": "Electronic Health Records (EHRs) are pivotal in clinical practices, yet their\nretrieval remains a challenge mainly due to semantic gap issues. Recent\nadvancements in dense retrieval offer promising solutions but existing models,\nboth general-domain and biomedical-domain, fall short due to insufficient\nmedical knowledge or mismatched training corpora. This paper introduces\n\\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for\nEHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV\ndischarge summaries to address the need for extensive medical knowledge and\nlarge-scale training data. The first stage involves medical entity extraction\nand knowledge injection from a biomedical knowledge graph, while the second\nstage employs large language models to generate diverse training data. We train\ntwo variants of \\texttt{DR.EHR}, with 110M and 7B parameters, respectively.\nEvaluated on the CliniQ benchmark, our models significantly outperforms all\nexisting dense retrievers, achieving state-of-the-art results. Detailed\nanalyses confirm our models' superiority across various match and query types,\nparticularly in challenging semantic matches like implication and abbreviation.\nAblation studies validate the effectiveness of each pipeline component, and\nsupplementary experiments on EHR QA datasets demonstrate the models'\ngeneralizability on natural language questions, including complex ones with\nmultiple entities. This work significantly advances EHR retrieval, offering a\nrobust solution for clinical applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.18583v1",
      "published": "2025-07-24T17:02:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18583v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law",
      "authors": [
        "Shanghai AI Lab",
        ":",
        "Yicheng Bao",
        "Guanxu Chen",
        "Mingkang Chen",
        "Yunhao Chen",
        "Chiyu Chen",
        "Lingjie Chen",
        "Sirui Chen",
        "Xinquan Chen",
        "Jie Cheng",
        "Yu Cheng",
        "Dengke Deng",
        "Yizhuo Ding",
        "Dan Ding",
        "Xiaoshan Ding",
        "Yi Ding",
        "Zhichen Dong",
        "Lingxiao Du",
        "Yuyu Fan",
        "Xinshun Feng",
        "Yanwei Fu",
        "Yuxuan Gao",
        "Ruijun Ge",
        "Tianle Gu",
        "Lujun Gui",
        "Jiaxuan Guo",
        "Qianxi He",
        "Yuenan Hou",
        "Xuhao Hu",
        "Hong Huang",
        "Kaichen Huang",
        "Shiyang Huang",
        "Yuxian Jiang",
        "Shanzhe Lei",
        "Jie Li",
        "Lijun Li",
        "Hao Li",
        "Juncheng Li",
        "Xiangtian Li",
        "Yafu Li",
        "Lingyu Li",
        "Xueyan Li",
        "Haotian Liang",
        "Dongrui Liu",
        "Qihua Liu",
        "Zhixuan Liu",
        "Bangwei Liu",
        "Huacan Liu",
        "Yuexiao Liu",
        "Zongkai Liu",
        "Chaochao Lu",
        "Yudong Lu",
        "Xiaoya Lu",
        "Zhenghao Lu",
        "Qitan Lv",
        "Caoyuan Ma",
        "Jiachen Ma",
        "Xiaoya Ma",
        "Zhongtian Ma",
        "Lingyu Meng",
        "Ziqi Miao",
        "Yazhe Niu",
        "Yuezhang Peng",
        "Yuan Pu",
        "Han Qi",
        "Chen Qian",
        "Xingge Qiao",
        "Jingjing Qu",
        "Jiashu Qu",
        "Wanying Qu",
        "Wenwen Qu",
        "Xiaoye Qu",
        "Qihan Ren",
        "Qingnan Ren",
        "Qingyu Ren",
        "Jing Shao",
        "Wenqi Shao",
        "Shuai Shao",
        "Dongxing Shi",
        "Xin Song",
        "Xinhao Song",
        "Yan Teng",
        "Xuan Tong",
        "Yingchun Wang",
        "Xuhong Wang",
        "Shujie Wang",
        "Xin Wang",
        "Yige Wang",
        "Yixu Wang",
        "Yuanfu Wang",
        "Futing Wang",
        "Ruofan Wang",
        "Wenjie Wang",
        "Yajie Wang",
        "Muhao Wei",
        "Xiaoyu Wen",
        "Fenghua Weng",
        "Yuqi Wu",
        "Yingtong Xiong",
        "Xingcheng Xu",
        "Chao Yang",
        "Yue Yang",
        "Yang Yao",
        "Yulei Ye",
        "Zhenyun Yin",
        "Yi Yu",
        "Bo Zhang",
        "Qiaosheng Zhang",
        "Jinxuan Zhang",
        "Yexin Zhang",
        "Yinqiang Zheng",
        "Hefeng Zhou",
        "Zhanhui Zhou",
        "Pengyu Zhu",
        "Qingzi Zhu",
        "Yubo Zhu",
        "Bowen Zhou"
      ],
      "abstract": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI.",
      "pdf_url": "http://arxiv.org/pdf/2507.18576v1",
      "published": "2025-07-24T16:49:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18576v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster Design",
      "authors": [
        "Donghoon Shin",
        "Daniel Lee",
        "Gary Hsieh",
        "Gromit Yeuk-Yin Chan"
      ],
      "abstract": "Poster designing can benefit from synchronous feedback from target audiences.\nHowever, gathering audiences with diverse perspectives and reconciling them on\ndesign edits can be challenging. Recent generative AI models present\nopportunities to simulate human-like interactions, but it is unclear how they\nmay be used for feedback processes in design. We introduce PosterMate, a poster\ndesign assistant that facilitates collaboration by creating audience-driven\npersona agents constructed from marketing documents. PosterMate gathers\nfeedback from each persona agent regarding poster components, and stimulates\ndiscussion with the help of a moderator to reach a conclusion. These\nagreed-upon edits can then be directly integrated into the poster design.\nThrough our user study (N=12), we identified the potential of PosterMate to\ncapture overlooked viewpoints, while serving as an effective prototyping tool.\nAdditionally, our controlled online evaluation (N=100) revealed that the\nfeedback from an individual persona agent is appropriate given its persona\nidentity, and the discussion effectively synthesizes the different persona\nagents' perspectives.",
      "pdf_url": "http://arxiv.org/pdf/2507.18572v1",
      "published": "2025-07-24T16:46:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18572v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "H.5.2; I.2.7"
      ]
    },
    {
      "title": "Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications",
      "authors": [
        "Ruben Gamboa",
        "Panagiotis Manolios"
      ],
      "abstract": "The ACL2 Workshop series is the major technical forum for users of the ACL2\ntheorem proving system to present research related to the ACL2 theorem prover\nand its applications. ACL2 is an industrial-strength automated reasoning\nsystem, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM\nSoftware System Award was awarded to Boyer, Kaufmann, and Moore for their work\non ACL2 and the other theorem provers in the Boyer-Moore family.",
      "pdf_url": "http://arxiv.org/pdf/2507.18567v1",
      "published": "2025-07-24T16:42:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18567v1",
      "categories": [
        "cs.LO",
        "cs.AI"
      ]
    },
    {
      "title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation",
      "authors": [
        "Jiafeng Xiong",
        "Yuting Zhao"
      ],
      "abstract": "Multimodal Machine Translation (MMT) has demonstrated the significant help of\nvisual information in machine translation. However, existing MMT methods face\nchallenges in leveraging the modality gap by enforcing rigid visual-linguistic\nalignment whilst being confined to inference within their trained multimodal\ndomains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage\nGraph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph\nAttention Network adapter to learn multimodal knowledge in a unified fused\nspace and inductively generalize it to broader image-free translation domains.\nExperimental results on the Multi30K dataset of English-to-French and\nEnglish-to-German tasks demonstrate that our GIIFT surpasses existing\napproaches and achieves the state-of-the-art, even without images during\ninference. Results on the WMT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards\ninductive image-free inference.",
      "pdf_url": "http://arxiv.org/pdf/2507.18562v1",
      "published": "2025-07-24T16:36:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18562v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Internal Data: Constructing Complete Datasets for Fairness Testing",
      "authors": [
        "Varsha Ramineni",
        "Hossein A. Rahmani",
        "Emine Yilmaz",
        "David Barber"
      ],
      "abstract": "As AI becomes prevalent in high-risk domains and decision-making, it is\nessential to test for potential harms and biases. This urgency is reflected by\nthe global emergence of AI regulations that emphasise fairness and adequate\ntesting, with some mandating independent bias audits. However, procuring the\nnecessary data for fairness testing remains a significant challenge.\nParticularly in industry settings, legal and privacy concerns restrict the\ncollection of demographic data required to assess group disparities, and\nauditors face practical and cultural challenges in gaining access to data.\nFurther, internal historical datasets are often insufficiently representative\nto identify real-world biases. This work focuses on evaluating classifier\nfairness when complete datasets including demographics are inaccessible. We\npropose leveraging separate overlapping datasets to construct complete\nsynthetic data that includes demographic information and accurately reflects\nthe underlying relationships between protected attributes and model features.\nWe validate the fidelity of the synthetic data by comparing it to real data,\nand empirically demonstrate that fairness metrics derived from testing on such\nsynthetic data are consistent with those obtained from real data. This work,\ntherefore, offers a path to overcome real-world data scarcity for fairness\ntesting, enabling independent, model-agnostic evaluation of fairness, and\nserving as a viable substitute where real data is limited.",
      "pdf_url": "http://arxiv.org/pdf/2507.18561v1",
      "published": "2025-07-24T16:35:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18561v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven Sentiment Integration for Financial Portfolio Optimization",
      "authors": [
        "Benjamin Coriat",
        "Eric Benhamou"
      ],
      "abstract": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.",
      "pdf_url": "http://arxiv.org/pdf/2507.18560v1",
      "published": "2025-07-24T16:35:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18560v1",
      "categories": [
        "q-fin.PM",
        "cs.AI"
      ]
    },
    {
      "title": "VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding",
      "authors": [
        "Baoyao Yang",
        "Wanyun Li",
        "Dixin Chen",
        "Junxiang Chen",
        "Wenbin Yao",
        "Haifeng Lin"
      ],
      "abstract": "This paper introduces VideoMind, a video-centric omni-modal dataset designed\nfor deep video content cognition and enhanced multi-modal feature\nrepresentation. The dataset comprises 103K video samples (3K reserved for\ntesting), each paired with audio and systematically detailed textual\ndescriptions. Specifically, every video and its audio is described across three\nhierarchical layers (factual, abstract, and intent), progressing from surface\nto depth. It contains over 22 million words, averaging ~225 words per sample.\nVideoMind's key distinction from existing datasets is its provision of intent\nexpressions, which require contextual integration across the entire video and\nare not directly observable. These deep-cognitive expressions are generated\nusing a Chain-of-Thought (COT) approach, prompting the mLLM through\nstep-by-step reasoning. Each description includes annotations for subject,\nplace, time, event, action, and intent, supporting downstream recognition\ntasks. Crucially, we establish a gold-standard benchmark with 3,000 manually\nvalidated samples for evaluating deep-cognitive video understanding. We design\nhybrid-cognitive retrieval experiments, scored by multi-level retrieval\nmetrics, to appropriately assess deep video comprehension. Evaluation results\nfor models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a\npowerful benchmark for fine-grained cross-modal alignment and advances fields\nrequiring in-depth video understanding, such as emotion and intent recognition.\nThe data is publicly available on GitHub, HuggingFace, and OpenDataLab,\nhttps://github.com/cdx-cindy/VideoMind.",
      "pdf_url": "http://arxiv.org/pdf/2507.18552v1",
      "published": "2025-07-24T16:19:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18552v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T45, 68T50, 68U35,",
        "I.4.8; I.2.7; I.2.10; H.5.1"
      ]
    },
    {
      "title": "On the Performance of Concept Probing: The Influence of the Data (Extended Version)",
      "authors": [
        "Manuel de Sousa Ribeiro",
        "Afonso Leote",
        "João Leite"
      ],
      "abstract": "Concept probing has recently garnered increasing interest as a way to help\ninterpret artificial neural networks, dealing both with their typically large\nsize and their subsymbolic nature, which ultimately renders them unfeasible for\ndirect human interpretation. Concept probing works by training additional\nclassifiers to map the internal representations of a model into human-defined\nconcepts of interest, thus allowing humans to peek inside artificial neural\nnetworks. Research on concept probing has mainly focused on the model being\nprobed or the probing model itself, paying limited attention to the data\nrequired to train such probing models. In this paper, we address this gap.\nFocusing on concept probing in the context of image classification tasks, we\ninvestigate the effect of the data used to train probing models on their\nperformance. We also make available concept labels for two widely used\ndatasets.",
      "pdf_url": "http://arxiv.org/pdf/2507.18550v1",
      "published": "2025-07-24T16:18:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18550v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.NE"
      ]
    },
    {
      "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface",
      "authors": [
        "Urchade Zaratiana",
        "Gil Pasternak",
        "Oliver Boyd",
        "George Hurn-Maloney",
        "Ash Lewis"
      ],
      "abstract": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.",
      "pdf_url": "http://arxiv.org/pdf/2507.18546v1",
      "published": "2025-07-24T16:11:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18546v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation",
      "authors": [
        "Magnus Bengtsson",
        "Kenneth Östberg"
      ],
      "abstract": "We introduce C2G-KD, a data-free knowledge distillation framework where a\nclass-conditional generator is trained to produce synthetic samples guided by a\nfrozen teacher model and geometric constraints derived from PCA. The generator\nnever observes real training data but instead learns to activate the teacher's\noutput through a combination of semantic and structural losses. By constraining\ngenerated samples to lie within class-specific PCA subspaces estimated from as\nfew as two real examples per class, we preserve topological consistency and\ndiversity. Experiments on MNIST show that even minimal class structure is\nsufficient to bootstrap useful synthetic training pipelines.",
      "pdf_url": "http://arxiv.org/pdf/2507.18533v1",
      "published": "2025-07-24T16:00:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18533v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning",
      "authors": [
        "Zhongtian Sun",
        "Anoushka Harit",
        "Alexandra Cristea",
        "Christl A. Donnelly",
        "Pietro Liò"
      ],
      "abstract": "Graph Neural Networks (GNNs) have demonstrated significant success in\nlearning from graph-structured data but often struggle on heterophilous graphs,\nwhere connected nodes differ in features or class labels. This limitation\narises from indiscriminate neighbor aggregation and insufficient incorporation\nof higher-order structural patterns. To address these challenges, we propose\nGLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel\nframework that integrates logic-guided reasoning, dynamic graph refinement, and\nadaptive clustering to enhance graph representation learning. GLANCE combines a\nlogic layer for interpretable and structured embeddings, multi-head\nattention-based edge pruning for denoising graph structures, and clustering\nmechanisms for capturing global patterns. Experimental results in benchmark\ndatasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE\nachieves competitive performance, offering robust and interpretable solutions\nfor heterophilous graph scenarios. The proposed framework is lightweight,\nadaptable, and uniquely suited to the challenges of heterophilous graphs.",
      "pdf_url": "http://arxiv.org/pdf/2507.18521v1",
      "published": "2025-07-24T15:45:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18521v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Explaining How Visual, Textual and Multimodal Encoders Share Concepts",
      "authors": [
        "Clément Cornet",
        "Romaric Besançon",
        "Hervé Le Borgne"
      ],
      "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful technique for\nextracting human-interpretable features from neural networks activations.\nPrevious works compared different models based on SAE-derived features but\nthose comparisons have been restricted to models within the same modality. We\npropose a novel indicator allowing quantitative comparison of models across SAE\nfeatures, and use it to conduct a comparative study of visual, textual and\nmultimodal encoders. We also propose to quantify the Comparative Sharedness of\nindividual features between different classes of models. With these two new\ntools, we conduct several studies on 21 encoders of the three types, with two\nsignificantly different sizes, and considering generalist and domain specific\ndatasets. The results allow to revisit previous studies at the light of\nencoders trained in a multimodal context and to quantify to which extent all\nthese models share some representations or features. They also suggest that\nvisual features that are specific to VLMs among vision encoders are shared with\ntext encoders, highlighting the impact of text pretraining. The code is\navailable at https://github.com/CEA-LIST/SAEshareConcepts",
      "pdf_url": "http://arxiv.org/pdf/2507.18512v1",
      "published": "2025-07-24T15:33:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18512v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments",
      "authors": [
        "Xiao Yang",
        "Lingxuan Wu",
        "Lizhong Wang",
        "Chengyang Ying",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "Adversarial attacks in 3D environments have emerged as a critical threat to\nthe reliability of visual perception systems, particularly in safety-sensitive\napplications such as identity verification and autonomous driving. These\nattacks employ adversarial patches and 3D objects to manipulate deep neural\nnetwork (DNN) predictions by exploiting vulnerabilities within complex scenes.\nExisting defense mechanisms, such as adversarial training and purification,\nprimarily employ passive strategies to enhance robustness. However, these\napproaches often rely on pre-defined assumptions about adversarial tactics,\nlimiting their adaptability in dynamic 3D settings. To address these\nchallenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a\nproactive defense framework that leverages adaptive exploration and interaction\nwith the environment to improve perception robustness in 3D adversarial\ncontexts. By implementing a multi-step objective that balances immediate\nprediction accuracy with predictive entropy minimization, Rein-EAD optimizes\ndefense strategies over a multi-step horizon. Additionally, Rein-EAD involves\nan uncertainty-oriented reward-shaping mechanism that facilitates efficient\npolicy updates, thereby reducing computational overhead and supporting\nreal-world applicability without the need for differentiable environments.\nComprehensive experiments validate the effectiveness of Rein-EAD, demonstrating\na substantial reduction in attack success rates while preserving standard\naccuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization\nto unseen and adaptive attacks, making it suitable for real-world complex\ntasks, including 3D object classification, face recognition and autonomous\ndriving.",
      "pdf_url": "http://arxiv.org/pdf/2507.18484v1",
      "published": "2025-07-24T14:56:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18484v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Automated Code Review Using Large Language Models with Symbolic Reasoning",
      "authors": [
        "Busra Icoz",
        "Goksel Biricik"
      ],
      "abstract": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.",
      "pdf_url": "http://arxiv.org/pdf/2507.18476v1",
      "published": "2025-07-24T14:50:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18476v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols",
      "authors": [
        "Luo Cheng",
        "Hanwei Zhang",
        "Lijun Zhang",
        "Holger Hermanns"
      ],
      "abstract": "Adversarial robustness in LiDAR-based 3D object detection is a critical\nresearch area due to its widespread application in real-world scenarios. While\nmany digital attacks manipulate point clouds or meshes, they often lack\nphysical realizability, limiting their practical impact. Physical adversarial\nobject attacks remain underexplored and suffer from poor reproducibility due to\ninconsistent setups and hardware differences. To address this, we propose a\ndevice-agnostic, standardized framework that abstracts key elements of physical\nadversarial object attacks, supports diverse methods, and provides open-source\ncode with benchmarking protocols in simulation and real-world settings. Our\nframework enables fair comparison, accelerates research, and is validated by\nsuccessfully transferring simulated attacks to a physical LiDAR system. Beyond\nthe framework, we offer insights into factors influencing attack success and\nadvance understanding of adversarial robustness in real-world LiDAR perception.",
      "pdf_url": "http://arxiv.org/pdf/2507.18457v1",
      "published": "2025-07-24T14:37:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18457v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Generation of Synthetic Clinical Text: A Systematic Review",
      "authors": [
        "Basel Alshaikhdeeb",
        "Ahmed Abdelmonem Hemedan",
        "Soumyabrata Ghosh",
        "Irina Balaur",
        "Venkata Satagopam"
      ],
      "abstract": "Generating clinical synthetic text represents an effective solution for\ncommon clinical NLP issues like sparsity and privacy. This paper aims to\nconduct a systematic review on generating synthetic medical free-text by\nformulating quantitative analysis to three research questions concerning (i)\nthe purpose of generation, (ii) the techniques, and (iii) the evaluation\nmethods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,\nGoogle Scholar, and arXiv databases for publications associated with generating\nsynthetic medical unstructured free-text. We have identified 94 relevant\narticles out of 1,398 collected ones. A great deal of attention has been given\nto the generation of synthetic medical text from 2018 onwards, where the main\npurpose of such a generation is towards text augmentation, assistive writing,\ncorpus building, privacy-preserving, annotation, and usefulness. Transformer\narchitectures were the main predominant technique used to generate the text,\nespecially the GPTs. On the other hand, there were four main aspects of\nevaluation, including similarity, privacy, structure, and utility, where\nutility was the most frequent method used to assess the generated synthetic\nmedical text. Although the generated synthetic medical text demonstrated a\nmoderate possibility to act as real medical documents in different downstream\nNLP tasks, it has proven to be a great asset as augmented, complementary to the\nreal documents, towards improving the accuracy and overcoming\nsparsity/undersampling issues. Yet, privacy is still a major issue behind\ngenerating synthetic medical text, where more human assessments are needed to\ncheck for the existence of any sensitive information. Despite that, advances in\ngenerating synthetic medical text will considerably accelerate the adoption of\nworkflows and pipeline development, discarding the time-consuming legalities of\ndata transfer.",
      "pdf_url": "http://arxiv.org/pdf/2507.18451v1",
      "published": "2025-07-24T14:35:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18451v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language",
      "authors": [
        "Md Obyedullahil Mamun",
        "Md Adyelullahil Mamun",
        "Arif Ahmad",
        "Md. Imran Hossain Emu"
      ],
      "abstract": "Punctuation restoration enhances the readability of text and is critical for\npost-processing tasks in Automatic Speech Recognition (ASR), especially for\nlow-resource languages like Bangla. In this study, we explore the application\nof transformer-based models, specifically XLM-RoBERTa-large, to automatically\nrestore punctuation in unpunctuated Bangla text. We focus on predicting four\npunctuation marks: period, comma, question mark, and exclamation mark across\ndiverse text domains. To address the scarcity of annotated resources, we\nconstructed a large, varied training corpus and applied data augmentation\ntechniques. Our best-performing model, trained with an augmentation factor of\nalpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the\nReference set, and 90.2% on the ASR set.\n  Results show strong generalization to reference and ASR transcripts,\ndemonstrating the model's effectiveness in real-world, noisy scenarios. This\nwork establishes a strong baseline for Bangla punctuation restoration and\ncontributes publicly available datasets and code to support future research in\nlow-resource NLP.",
      "pdf_url": "http://arxiv.org/pdf/2507.18448v1",
      "published": "2025-07-24T14:33:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18448v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2; I.7"
      ]
    },
    {
      "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data",
      "authors": [
        "Rana Alshaikh",
        "Israa Alghanmi",
        "Shelan Jeawak"
      ],
      "abstract": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data.",
      "pdf_url": "http://arxiv.org/pdf/2507.18442v1",
      "published": "2025-07-24T14:26:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18442v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GPU Accelerated Compact-Table Propagation",
      "authors": [
        "Enrico Santi",
        "Fabio Tardivo",
        "Agostino Dovier",
        "Andrea Formisano"
      ],
      "abstract": "Constraint Programming developed within Logic Programming in the Eighties;\nnowadays all Prolog systems encompass modules capable of handling constraint\nprogramming on finite domains demanding their solution to a constraint solver.\nThis work focuses on a specific form of constraint, the so-called table\nconstraint, used to specify conditions on the values of variables as an\nenumeration of alternative options. Since every condition on a set of finite\ndomain variables can be ultimately expressed as a finite set of cases, Table\ncan, in principle, simulate any other constraint. These characteristics make\nTable one of the most studied constraints ever, leading to a series of\nincreasingly efficient propagation algorithms. Despite this, it is not uncommon\nto encounter real-world problems with hundreds or thousands of valid cases that\nare simply too many to be handled effectively with standard CPU-based\napproaches. In this paper, we deal with the Compact-Table (CT) algorithm, the\nstate-of-the-art propagation algorithms for Table. We describe how CT can be\nenhanced by exploiting the massive computational power offered by modern GPUs\nto handle large Table constraints. In particular, we report on the design and\nimplementation of GPU-accelerated CT, on its integration into an existing\nconstraint solver, and on an experimental validation performed on a significant\nset of instances.",
      "pdf_url": "http://arxiv.org/pdf/2507.18413v1",
      "published": "2025-07-24T13:53:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18413v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation",
      "authors": [
        "Kwong Ho Li",
        "Wathsala Karunarathne"
      ],
      "abstract": "This paper investigates the application of Reinforcement Learning (RL) to\noptimise call routing in call centres to minimise client waiting time and staff\nidle time. Two methods are compared: a model-based approach using Value\nIteration (VI) under known system dynamics, and a model-free approach using\nProximal Policy Optimisation (PPO) that learns from experience. For the\nmodel-based approach, a theoretical model is used, while a simulation model\ncombining Discrete Event Simulation (DES) with the OpenAI Gym environment is\ndeveloped for model-free learning. Both models frame the problem as a Markov\nDecision Process (MDP) within a Skills-Based Routing (SBR) framework, with\nPoisson client arrivals and exponentially distributed service and abandonment\ntimes. For policy evaluation, random, VI, and PPO policies are evaluated using\nthe simulation model. After 1,000 test episodes, PPO consistently achives the\nhighest rewards, along with the lowest client waiting time and staff idle time,\ndespite requiring longer training time.",
      "pdf_url": "http://arxiv.org/pdf/2507.18398v1",
      "published": "2025-07-24T13:31:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18398v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
      "authors": [
        "Asaf Yehudai",
        "Lilach Eden",
        "Yotam Perlitz",
        "Roy Bar-Haim",
        "Michal Shmueli-Scheuer"
      ],
      "abstract": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.",
      "pdf_url": "http://arxiv.org/pdf/2507.18392v1",
      "published": "2025-07-24T13:15:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18392v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Revisiting LLM Reasoning via Information Bottleneck",
      "authors": [
        "Shiye Lei",
        "Zhihao Cheng",
        "Kai Jia",
        "Dacheng Tao"
      ],
      "abstract": "Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance.",
      "pdf_url": "http://arxiv.org/pdf/2507.18391v1",
      "published": "2025-07-24T13:14:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18391v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios",
      "authors": [
        "Zhuang Qiang Bok",
        "Watson Wei Khong Chua"
      ],
      "abstract": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance.",
      "pdf_url": "http://arxiv.org/pdf/2507.18368v1",
      "published": "2025-07-24T12:47:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18368v1",
      "categories": [
        "cs.AI",
        "I.2.0; I.2.6; J.4"
      ]
    },
    {
      "title": "The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams",
      "authors": [
        "Peter Baumgartner",
        "Lachlan McGinness"
      ],
      "abstract": "We present our method for automatically marking Physics exams. The marking\nproblem consists in assessing typed student answers for correctness with\nrespect to a ground truth solution. This is a challenging problem that we seek\nto tackle using a combination of a computer algebra system, an SMT solver and a\nterm rewriting system. A Large Language Model is used to interpret and remove\nerrors from student responses and rewrite these in a machine readable format.\nOnce formalized and language-aligned, the next step then consists in applying\nautomated reasoning techniques for assessing student solution correctness. We\nconsider two methods of automated theorem proving: off-the-shelf SMT solving\nand term rewriting systems tailored for physics problems involving\ntrigonometric expressions. The development of the term rewrite system and\nestablishing termination and confluence properties was not trivial, and we\ndescribe it in some detail in the paper. We evaluate our system on a rich pool\nof over 1500 real-world student exam responses from the 2023 Australian Physics\nOlympiad.",
      "pdf_url": "http://arxiv.org/pdf/2507.18337v1",
      "published": "2025-07-24T12:08:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18337v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Improving Bird Classification with Primary Color Additives",
      "authors": [
        "Ezhini Rasendiran R",
        "Chandresh Kumar Maurya"
      ],
      "abstract": "We address the problem of classifying bird species using their song\nrecordings, a challenging task due to environmental noise, overlapping\nvocalizations, and missing labels. Existing models struggle with low-SNR or\nmulti-species recordings. We hypothesize that birds can be classified by\nvisualizing their pitch pattern, speed, and repetition, collectively called\nmotifs. Deep learning models applied to spectrogram images help, but similar\nmotifs across species cause confusion. To mitigate this, we embed frequency\ninformation into spectrograms using primary color additives. This enhances\nspecies distinction and improves classification accuracy. Our experiments show\nthat the proposed approach achieves statistically significant gains over models\nwithout colorization and surpasses the BirdCLEF 2024 winner, improving F1 by\n7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the\neffectiveness of incorporating frequency information via colorization.",
      "pdf_url": "http://arxiv.org/pdf/2507.18334v1",
      "published": "2025-07-24T12:05:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18334v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "A Concept for Efficient Scalability of Automated Driving Allowing for Technical, Legal, Cultural, and Ethical Differences",
      "authors": [
        "Lars Ullrich",
        "Michael Buchholz",
        "Jonathan Petit",
        "Klaus Dietmayer",
        "Knut Graichen"
      ],
      "abstract": "Efficient scalability of automated driving (AD) is key to reducing costs,\nenhancing safety, conserving resources, and maximizing impact. However,\nresearch focuses on specific vehicles and context, while broad deployment\nrequires scalability across various configurations and environments.\nDifferences in vehicle types, sensors, actuators, but also traffic regulations,\nlegal requirements, cultural dynamics, or even ethical paradigms demand high\nflexibility of data-driven developed capabilities. In this paper, we address\nthe challenge of scalable adaptation of generic capabilities to desired systems\nand environments. Our concept follows a two-stage fine-tuning process. In the\nfirst stage, fine-tuning to the specific environment takes place through a\ncountry-specific reward model that serves as an interface between technological\nadaptations and socio-political requirements. In the second stage,\nvehicle-specific transfer learning facilitates system adaptation and governs\nthe validation of design decisions. In sum, our concept offers a data-driven\nprocess that integrates both technological and socio-political aspects,\nenabling effective scalability across technical, legal, cultural, and ethical\ndifferences.",
      "pdf_url": "http://arxiv.org/pdf/2507.18326v1",
      "published": "2025-07-24T11:51:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18326v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation",
      "authors": [
        "Minje Park",
        "Jeonghwa Lim",
        "Taehyung Yu",
        "Sunghoon Joo"
      ],
      "abstract": "Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform\nfeatures, is critical for clinical diagnosis. Despite recent advances using\ndeep learning, progress has been limited by the scarcity of publicly available\nannotated datasets. Semi-supervised learning presents a promising solution by\nleveraging abundant unlabeled ECG data. In this study, we present the first\nsystematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG\ndelineation. We curated and unified multiple public datasets, including\npreviously underused sources, to support robust and diverse evaluation. We\nadopted five representative SemiSeg algorithms from computer vision,\nimplemented them on two different architectures: the convolutional network and\nthe transformer, and evaluated them in two different settings: in-domain and\ncross-domain. Additionally, we propose ECG-specific training configurations and\naugmentation strategies and introduce a standardized evaluation framework. Our\nresults show that the transformer outperforms the convolutional network in\nsemi-supervised ECG delineation. We anticipate that our benchmark will serve as\na foundation for advancing semi-supervised ECG delineation methods and will\nfacilitate further research in this domain.",
      "pdf_url": "http://arxiv.org/pdf/2507.18323v1",
      "published": "2025-07-24T11:49:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18323v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models",
      "authors": [
        "Delong Ran",
        "Xinlei He",
        "Tianshuo Cong",
        "Anyu Wang",
        "Qi Li",
        "Xiaoyun Wang"
      ],
      "abstract": "Language Models (LMs) typically adhere to a \"pre-training and fine-tuning\"\nparadigm, where a universal pre-trained model can be fine-tuned to cater to\nvarious specialized domains. Low-Rank Adaptation (LoRA) has gained the most\nwidespread use in LM fine-tuning due to its lightweight computational cost and\nremarkable performance. Because the proportion of parameters tuned by LoRA is\nrelatively small, there might be a misleading impression that the LoRA\nfine-tuning data is invulnerable to Membership Inference Attacks (MIAs).\nHowever, we identify that utilizing the pre-trained model can induce more\ninformation leakage, which is neglected by existing MIAs. Therefore, we\nintroduce LoRA-Leak, a holistic evaluation framework for MIAs against the\nfine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership\ninference attacks, including ten existing MIAs, and five improved MIAs that\nleverage the pre-trained model as a reference. In experiments, we apply\nLoRA-Leak to three advanced LMs across three popular natural language\nprocessing tasks, demonstrating that LoRA-based fine-tuned LMs are still\nvulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).\nWe also applied LoRA-Leak to different fine-tuning settings to understand the\nresulting privacy risks. We further explore four defenses and find that only\ndropout and excluding specific LM layers during fine-tuning effectively\nmitigate MIA risks while maintaining utility. We highlight that under the\n\"pre-training and fine-tuning\" paradigm, the existence of the pre-trained model\nmakes MIA a more severe risk for LoRA-based LMs. We hope that our findings can\nprovide guidance on data privacy protection for specialized LM providers.",
      "pdf_url": "http://arxiv.org/pdf/2507.18302v1",
      "published": "2025-07-24T11:18:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18302v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Foundations for Risk Assessment of AI in Protecting Fundamental Rights",
      "authors": [
        "Antonino Rotolo",
        "Beatrice Ferrigno",
        "Jose Miguel Angel Garcia Godinez",
        "Claudio Novelli",
        "Giovanni Sartor"
      ],
      "abstract": "This chapter introduces a conceptual framework for qualitative risk\nassessment of AI, particularly in the context of the EU AI Act. The framework\naddresses the complexities of legal compliance and fundamental rights\nprotection by itegrating definitional balancing and defeasible reasoning.\nDefinitional balancing employs proportionality analysis to resolve conflicts\nbetween competing rights, while defeasible reasoning accommodates the dynamic\nnature of legal decision-making. Our approach stresses the need for an analysis\nof AI deployment scenarios and for identifying potential legal violations and\nmulti-layered impacts on fundamental rights. On the basis of this analysis, we\nprovide philosophical foundations for a logical account of AI risk analysis. In\nparticular, we consider the basic building blocks for conceptually grasping the\ninteraction between AI deployment scenarios and fundamental rights,\nincorporating in defeasible reasoning definitional balancing and arguments\nabout the contextual promotion or demotion of rights. This layered approach\nallows for more operative models of assessment of both high-risk AI systems and\nGeneral Purpose AI (GPAI) systems, emphasizing the broader applicability of the\nlatter. Future work aims to develop a formal model and effective algorithms to\nenhance AI risk assessment, bridging theoretical insights with practical\napplications to support responsible AI governance.",
      "pdf_url": "http://arxiv.org/pdf/2507.18290v1",
      "published": "2025-07-24T10:52:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18290v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "TCM-Tongue: A Standardized Tongue Image Dataset with Pathological Annotations for AI-Assisted TCM Diagnosis",
      "authors": [
        "Xuebo Jin",
        "Longfei Gao",
        "Anshuo Tong",
        "Zhengyang Chen",
        "Jianlei Kong",
        "Ning Sun",
        "Huijun Ma",
        "Qiang Wang",
        "Yuting Bai",
        "Tingli Su"
      ],
      "abstract": "Traditional Chinese medicine (TCM) tongue diagnosis, while clinically\nvaluable, faces standardization challenges due to subjective interpretation and\ninconsistent imaging protocols, compounded by the lack of large-scale,\nannotated datasets for AI development. To address this gap, we present the\nfirst specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719\nhigh-quality images captured under standardized conditions and annotated with\n20 pathological symptom categories (averaging 2.54 clinically validated labels\nper image, all verified by licensed TCM practitioners). The dataset supports\nmultiple annotation formats (COCO, TXT, XML) for broad usability and has been\nbenchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and\nMobileNetV2) to demonstrate its utility for AI development. This resource\nprovides a critical foundation for advancing reliable computational tools in\nTCM, bridging the data shortage that has hindered progress in the field, and\nfacilitating the integration of AI into both research and clinical practice\nthrough standardized, high-quality diagnostic data.",
      "pdf_url": "http://arxiv.org/pdf/2507.18288v1",
      "published": "2025-07-24T10:49:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18288v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models",
      "authors": [
        "Suhang Wu",
        "Jialong Tang",
        "Chengyi Yang",
        "Pei Zhang",
        "Baosong Yang",
        "Junhui Li",
        "Junfeng Yao",
        "Min Zhang",
        "Jinsong Su"
      ],
      "abstract": "Direct speech translation (ST) has garnered increasing attention nowadays,\nyet the accurate translation of terminology within utterances remains a great\nchallenge. In this regard, current studies mainly concentrate on leveraging\nvarious translation knowledge into ST models. However, these methods often\nstruggle with interference from irrelevant noise and can not fully utilize the\ntranslation knowledge. To address these issues, in this paper, we propose a\nnovel Locate-and-Focus method for terminology translation. It first effectively\nlocates the speech clips containing terminologies within the utterance to\nconstruct translation knowledge, minimizing irrelevant information for the ST\nmodel. Subsequently, it associates the translation knowledge with the utterance\nand hypothesis from both audio and textual modalities, allowing the ST model to\nbetter focus on translation knowledge during translation. Experimental results\nacross various datasets demonstrate that our method effectively locates\nterminologies within utterances and enhances the success rate of terminology\ntranslation, while maintaining robust general translation performance.",
      "pdf_url": "http://arxiv.org/pdf/2507.18263v1",
      "published": "2025-07-24T10:07:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18263v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation",
      "authors": [
        "Chenyu Su",
        "Weiwei Shang",
        "Chen Qian",
        "Fei Zhang",
        "Shuang Cong"
      ],
      "abstract": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io.",
      "pdf_url": "http://arxiv.org/pdf/2507.18262v1",
      "published": "2025-07-24T10:07:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18262v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection",
      "authors": [
        "Junyao Li",
        "Yahao Lu",
        "Xingyuan Guo",
        "Xiaoyu Xian",
        "Tiantian Wang",
        "Yukai Shi"
      ],
      "abstract": "Infrared small target detection (ISTD) plays a vital role in numerous\npractical applications. In pursuit of determining the performance boundaries,\nresearchers employ large and expensive manual-labeling data for representation\nlearning. Nevertheless, this approach renders the state-of-the-art ISTD methods\nhighly fragile in real-world challenges. In this paper, we first study the\nvariation in detection performance across several mainstream methods under\nvarious scarcity -- namely, the absence of high-quality infrared data -- that\nchallenge the prevailing theories about practical ISTD. To address this\nconcern, we introduce the Gaussian Agnostic Representation Learning.\nSpecifically, we propose the Gaussian Group Squeezer, leveraging Gaussian\nsampling and compression for non-uniform quantization. By exploiting a diverse\narray of training samples, we enhance the resilience of ISTD models against\nvarious challenges. Then, we introduce two-stage diffusion models for\nreal-world reconstruction. By aligning quantized signals closely with\nreal-world distributions, we significantly elevate the quality and fidelity of\nthe synthetic samples. Comparative evaluations against state-of-the-art\ndetection methods in various scarcity scenarios demonstrate the efficacy of the\nproposed approach.",
      "pdf_url": "http://arxiv.org/pdf/2507.18260v1",
      "published": "2025-07-24T10:03:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18260v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning",
      "authors": [
        "Dongyang Guo",
        "Yasmeen Abdrabou",
        "Enkeleda Thaqi",
        "Enkelejda Kasneci"
      ],
      "abstract": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics.",
      "pdf_url": "http://arxiv.org/pdf/2507.18252v1",
      "published": "2025-07-24T09:49:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18252v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "DepthDark: Robust Monocular Depth Estimation for Low-Light Environments",
      "authors": [
        "Longjian Zeng",
        "Zunjie Zhu",
        "Rongfeng Lu",
        "Ming Lu",
        "Bolun Zheng",
        "Chenggang Yan",
        "Anke Xue"
      ],
      "abstract": "In recent years, foundation models for monocular depth estimation have\nreceived increasing attention. Current methods mainly address typical daylight\nconditions, but their effectiveness notably decreases in low-light\nenvironments. There is a lack of robust foundational models for monocular depth\nestimation specifically designed for low-light scenarios. This largely stems\nfrom the absence of large-scale, high-quality paired depth datasets for\nlow-light conditions and the effective parameter-efficient fine-tuning (PEFT)\nstrategy. To address these challenges, we propose DepthDark, a robust\nfoundation model for low-light monocular depth estimation. We first introduce a\nflare-simulation module and a noise-simulation module to accurately simulate\nthe imaging process under nighttime conditions, producing high-quality paired\ndepth datasets for low-light conditions. Additionally, we present an effective\nlow-light PEFT strategy that utilizes illumination guidance and multiscale\nfeature fusion to enhance the model's capability in low-light environments. Our\nmethod achieves state-of-the-art depth estimation performance on the\nchallenging nuScenes-Night and RobotCar-Night datasets, validating its\neffectiveness using limited training data and computing resources.",
      "pdf_url": "http://arxiv.org/pdf/2507.18243v1",
      "published": "2025-07-24T09:32:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18243v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "From Individual Learning to Market Equilibrium: Correcting Structural and Parametric Biases in RL Simulations of Economic Models",
      "authors": [
        "Zeqiang Zhang",
        "Ruxin Chen"
      ],
      "abstract": "The application of Reinforcement Learning (RL) to economic modeling reveals a\nfundamental conflict between the assumptions of equilibrium theory and the\nemergent behavior of learning agents. While canonical economic models assume\natomistic agents act as `takers' of aggregate market conditions, a naive\nsingle-agent RL simulation incentivizes the agent to become a `manipulator' of\nits environment. This paper first demonstrates this discrepancy within a\nsearch-and-matching model with concave production, showing that a standard RL\nagent learns a non-equilibrium, monopsonistic policy. Additionally, we identify\na parametric bias arising from the mismatch between economic discounting and\nRL's treatment of intertemporal costs. To address both issues, we propose a\ncalibrated Mean-Field Reinforcement Learning framework that embeds a\nrepresentative agent in a fixed macroeconomic field and adjusts the cost\nfunction to reflect economic opportunity costs. Our iterative algorithm\nconverges to a self-consistent fixed point where the agent's policy aligns with\nthe competitive equilibrium. This approach provides a tractable and\ntheoretically sound methodology for modeling learning agents in economic\nsystems within the broader domain of computational social science.",
      "pdf_url": "http://arxiv.org/pdf/2507.18229v1",
      "published": "2025-07-24T09:21:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18229v1",
      "categories": [
        "econ.GN",
        "cs.AI",
        "q-fin.EC"
      ]
    },
    {
      "title": "GenAI for Automotive Software Development: From Requirements to Wheels",
      "authors": [
        "Nenad Petrovic",
        "Fengjunjie Pan",
        "Vahid Zolfaghari",
        "Krzysztof Lebioda",
        "Andre Schamschurko",
        "Alois Knoll"
      ],
      "abstract": "This paper introduces a GenAI-empowered approach to automated development of\nautomotive software, with emphasis on autonomous and Advanced Driver Assistance\nSystems (ADAS) capabilities. The process starts with requirements as input,\nwhile the main generated outputs are test scenario code for simulation\nenvironment, together with implementation of desired ADAS capabilities\ntargeting hardware platform of the vehicle connected to testbench. Moreover, we\nintroduce additional steps for requirements consistency checking leveraging\nModel-Driven Engineering (MDE). In the proposed workflow, Large Language Models\n(LLMs) are used for model-based summarization of requirements (Ecore metamodel,\nXMI model instance and OCL constraint creation), test scenario generation,\nsimulation code (Python) and target platform code generation (C++).\nAdditionally, Retrieval Augmented Generation (RAG) is adopted to enhance test\nscenario generation from autonomous driving regulations-related documents. Our\napproach aims shorter compliance and re-engineering cycles, as well as reduced\ndevelopment and testing time when it comes to ADAS-related capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2507.18223v1",
      "published": "2025-07-24T09:17:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18223v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting",
      "authors": [
        "Zhongzheng Yuan",
        "Lianshuai Guo",
        "Xunkai Li",
        "Yinlin Zhu",
        "Wenyu Wang",
        "Meixia Qu"
      ],
      "abstract": "Federated Graph Learning (FGL) is a distributed learning paradigm that\nenables collaborative training over large-scale subgraphs located on multiple\nlocal systems. However, most existing FGL approaches rely on synchronous\ncommunication, which leads to inefficiencies and is often impractical in\nreal-world deployments. Meanwhile, current asynchronous federated learning\n(AFL) methods are primarily designed for conventional tasks such as image\nclassification and natural language processing, without accounting for the\nunique topological properties of graph data. Directly applying these methods to\ngraph learning can possibly result in semantic drift and representational\ninconsistency in the global model. To address these challenges, we propose\nFedSA-GCL, a semi-asynchronous federated framework that leverages both\ninter-client label distribution divergence and graph topological\ncharacteristics through a novel ClusterCast mechanism for efficient training.\nWe evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain\nand Metis split algorithms, and compare it against 9 baselines. Extensive\nexperiments demonstrate that our method achieves strong robustness and\noutstanding efficiency, outperforming the baselines by an average of 2.92% with\nthe Louvain and by 3.4% with the Metis.",
      "pdf_url": "http://arxiv.org/pdf/2507.18219v1",
      "published": "2025-07-24T09:15:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18219v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Information Security Based on LLM Approaches: A Review",
      "authors": [
        "Chang Gong",
        "Zhongwen Li",
        "Xiaoqi Li"
      ],
      "abstract": "Information security is facing increasingly severe challenges, and\ntraditional protection means are difficult to cope with complex and changing\nthreats. In recent years, as an emerging intelligent technology, large language\nmodels (LLMs) have shown a broad application prospect in the field of\ninformation security. In this paper, we focus on the key role of LLM in\ninformation security, systematically review its application progress in\nmalicious behavior prediction, network threat analysis, system vulnerability\ndetection, malicious code identification, and cryptographic algorithm\noptimization, and explore its potential in enhancing security protection\nperformance. Based on neural networks and Transformer architecture, this paper\nanalyzes the technical basis of large language models and their advantages in\nnatural language processing tasks. It is shown that the introduction of large\nlanguage modeling helps to improve the detection accuracy and reduce the false\nalarm rate of security systems. Finally, this paper summarizes the current\napplication results and points out that it still faces challenges in model\ntransparency, interpretability, and scene adaptability, among other issues. It\nis necessary to explore further the optimization of the model structure and the\nimprovement of the generalization ability to realize a more intelligent and\naccurate information security protection system.",
      "pdf_url": "http://arxiv.org/pdf/2507.18215v1",
      "published": "2025-07-24T09:09:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18215v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation",
      "authors": [
        "Arup Kumar Sahoo",
        "Itzik Klein"
      ],
      "abstract": "A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application.",
      "pdf_url": "http://arxiv.org/pdf/2507.18206v1",
      "published": "2025-07-24T09:02:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18206v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection",
      "authors": [
        "San Kim",
        "Jonghwi Kim",
        "Yejin Jeon",
        "Gary Geunbae Lee"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nproviding external knowledge for accurate and up-to-date responses. However,\nthis reliance on external sources exposes a security risk, attackers can inject\npoisoned documents into the knowledge base to steer the generation process\ntoward harmful or misleading outputs. In this paper, we propose Gradient-based\nMasked Token Probability (GMTP), a novel defense method to detect and filter\nout adversarially crafted documents. Specifically, GMTP identifies high-impact\ntokens by examining gradients of the retriever's similarity function. These key\ntokens are then masked, and their probabilities are checked via a Masked\nLanguage Model (MLM). Since injected tokens typically exhibit markedly low\nmasked-token probabilities, this enables GMTP to easily detect malicious\ndocuments and achieve high-precision filtering. Experiments demonstrate that\nGMTP is able to eliminate over 90% of poisoned content while retaining relevant\ndocuments, thus maintaining robust retrieval and generation performance across\ndiverse datasets and adversarial settings.",
      "pdf_url": "http://arxiv.org/pdf/2507.18202v1",
      "published": "2025-07-24T08:58:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18202v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}