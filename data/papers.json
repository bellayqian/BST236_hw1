{
  "last_updated": "2025-10-10T00:47:39.057183",
  "papers": [
    {
      "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
      "authors": [
        "Yunhao Fang",
        "Weihao Yu",
        "Shu Zhong",
        "Qinghao Ye",
        "Xuehan Xiong",
        "Lai Wei"
      ],
      "abstract": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
      "pdf_url": "http://arxiv.org/pdf/2510.07318v1",
      "published": "2025-10-08T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07318v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
      "authors": [
        "Ming Zhong",
        "Xiang Zhou",
        "Ting-Yun Chang",
        "Qingze Wang",
        "Nan Xu",
        "Xiance Si",
        "Dan Garrette",
        "Shyam Upadhyay",
        "Jeremiah Liu",
        "Jiawei Han",
        "Benoit Schillings",
        "Jiao Sun"
      ],
      "abstract": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
      "pdf_url": "http://arxiv.org/pdf/2510.07315v1",
      "published": "2025-10-08T17:59:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07315v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "title": "GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations",
      "authors": [
        "Fabian Paischer",
        "Gianluca Galletti",
        "William Hornsby",
        "Paul Setinek",
        "Lorenzo Zanisi",
        "Naomi Carey",
        "Stanislas Pamela",
        "Johannes Brandstetter"
      ],
      "abstract": "Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to viable fusion power is understanding\nplasma turbulence, which significantly impairs plasma confinement, and is vital\nfor next-generation reactor design. Plasma turbulence is governed by the\nnonlinear gyrokinetic equation, which evolves a 5D distribution function over\ntime. Due to its high computational cost, reduced-order models are often\nemployed in practice to approximate turbulent transport of energy. However,\nthey omit nonlinear effects unique to the full 5D dynamics. To tackle this, we\nintroduce GyroSwin, the first scalable 5D neural surrogate that can model 5D\nnonlinear gyrokinetic simulations, thereby capturing the physical phenomena\nneglected by reduced models, while providing accurate estimates of turbulent\nheat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D,\n(ii) introduces cross-attention and integration modules for latent\n3D$\\leftrightarrow$5D interactions between electrostatic potential fields and\nthe distribution function, and (iii) performs channelwise mode separation\ninspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely\nused reduced numerics on heat flux prediction, captures the turbulent energy\ncascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three\norders of magnitude while remaining physically verifiable. GyroSwin shows\npromising scaling laws, tested up to one billion parameters, paving the way for\nscalable neural surrogates for gyrokinetic simulations of plasma turbulence.",
      "pdf_url": "http://arxiv.org/pdf/2510.07314v1",
      "published": "2025-10-08T17:59:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07314v1",
      "categories": [
        "physics.plasm-ph",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning",
      "authors": [
        "Sumeet Ramesh Motwani",
        "Alesia Ivanova",
        "Ziyang Cai",
        "Philip Torr",
        "Riashat Islam",
        "Shital Shah",
        "Christian Schroeder de Witt",
        "Charles London"
      ],
      "abstract": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data.",
      "pdf_url": "http://arxiv.org/pdf/2510.07312v1",
      "published": "2025-10-08T17:58:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07312v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
      "authors": [
        "Rushi Qiang",
        "Yuchen Zhuang",
        "Anikait Singh",
        "Percy Liang",
        "Chao Zhang",
        "Sherry Yang",
        "Bo Dai"
      ],
      "abstract": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.",
      "pdf_url": "http://arxiv.org/pdf/2510.07307v1",
      "published": "2025-10-08T17:57:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07307v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Cocoon: A System Architecture for Differentially Private Training with Correlated Noises",
      "authors": [
        "Donghwan Kim",
        "Xin Gu",
        "Jinho Baek",
        "Timothy Lo",
        "Younghoon Min",
        "Kwangsik Shin",
        "Jongryool Kim",
        "Jongse Park",
        "Kiwan Maeng"
      ],
      "abstract": "Machine learning (ML) models memorize and leak training data, causing serious\nprivacy issues to data owners. Training algorithms with differential privacy\n(DP), such as DP-SGD, have been gaining attention as a solution. However,\nDP-SGD adds a noise at each training iteration, which degrades the accuracy of\nthe trained model. To improve accuracy, a new family of approaches adds\ncarefully designed correlated noises, so that noises cancel out each other\nacross iterations. We performed an extensive characterization study of these\nnew mechanisms, for the first time to the best of our knowledge, and show they\nincur non-negligible overheads when the model is large or uses large embedding\ntables. Motivated by the analysis, we propose Cocoon, a hardware-software\nco-designed framework for efficient training with correlated noises. Cocoon\naccelerates models with embedding tables through pre-computing and storing\ncorrelated noises in a coalesced format (Cocoon-Emb), and supports large models\nthrough a custom near-memory processing device (Cocoon-NMP). On a real system\nwith an FPGA-based NMP device prototype, Cocoon improves the performance by\n2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).",
      "pdf_url": "http://arxiv.org/pdf/2510.07304v1",
      "published": "2025-10-08T17:56:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07304v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "Agentic generative AI for media content discovery at the national football league",
      "authors": [
        "Henry Wang",
        "Md Sirajus Salekin",
        "Jake Lee",
        "Ross Claytor",
        "Shinan Zhang",
        "Michael Chi"
      ],
      "abstract": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.",
      "pdf_url": "http://arxiv.org/pdf/2510.07297v1",
      "published": "2025-10-08T17:51:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07297v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs",
      "authors": [
        "Peize He",
        "Zichen Wen",
        "Yubo Wang",
        "Yuxuan Wang",
        "Xiaoqian Liu",
        "Jiajie Huang",
        "Zehui Lei",
        "Zhuangcheng Gu",
        "Xiangqi Jin",
        "Jiabing Yang",
        "Kai Li",
        "Zhifei Liu",
        "Weijia Li",
        "Cunxiang Wang",
        "Conghui He",
        "Linfeng Zhang"
      ],
      "abstract": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.07293v1",
      "published": "2025-10-08T17:50:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07293v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ]
    },
    {
      "title": "Evolutionary Profiles for Protein Fitness Prediction",
      "authors": [
        "Jigang Fan",
        "Xiaoran Jiao",
        "Shengdong Lin",
        "Zhanming Liang",
        "Weian Mao",
        "Chenchen Jing",
        "Hao Chen",
        "Chunhua Shen"
      ],
      "abstract": "Predicting the fitness impact of mutations is central to protein engineering\nbut constrained by limited assays relative to the size of sequence space.\nProtein language models (pLMs) trained with masked language modeling (MLM)\nexhibit strong zero-shot fitness prediction; we provide a unifying view by\ninterpreting natural evolution as implicit reward maximization and MLM as\ninverse reinforcement learning (IRL), in which extant sequences act as expert\ndemonstrations and pLM log-odds serve as fitness estimates. Building on this\nperspective, we introduce EvoIF, a lightweight model that integrates two\ncomplementary sources of evolutionary signal: (i) within-family profiles from\nretrieved homologs and (ii) cross-family structural-evolutionary constraints\ndistilled from inverse folding logits. EvoIF fuses sequence-structure\nrepresentations with these profiles via a compact transition block, yielding\ncalibrated probabilities for log-odds scoring. On ProteinGym (217 mutational\nassays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve\nstate-of-the-art or competitive performance while using only 0.15% of the\ntraining data and fewer parameters than recent large models. Ablations confirm\nthat within-family and cross-family profiles are complementary, improving\nrobustness across function types, MSA depths, taxa, and mutation depths. The\ncodes will be made publicly available at https://github.com/aim-uofa/EvoIF.",
      "pdf_url": "http://arxiv.org/pdf/2510.07286v1",
      "published": "2025-10-08T17:46:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07286v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ]
    },
    {
      "title": "GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)",
      "authors": [
        "Tianxiang Xu",
        "Zhichao Wen",
        "Xinyu Zhao",
        "Qi Hu",
        "Yan Li",
        "Chang Liu"
      ],
      "abstract": "The escalating complexity of network threats and the inherent class imbalance\nin traffic data present formidable challenges for modern Intrusion Detection\nSystems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological\nstructures and Temporal Convolutional Networks (TCNs) are proficient in\ncapturing time-series dependencies, a framework that synergistically integrates\nboth while explicitly addressing data imbalance remains an open challenge. This\npaper introduces a novel deep learning framework, named Gated Temporal\nConvolutional Network and Graph (GTCN-G), engineered to overcome these\nlimitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting\nhierarchical temporal features from network flows with a Graph Convolutional\nNetwork (GCN) designed to learn from the underlying graph structure. The core\ninnovation lies in the integration of a residual learning mechanism,\nimplemented via a Graph Attention Network (GAT). This mechanism preserves\noriginal feature information through residual connections, which is critical\nfor mitigating the class imbalance problem and enhancing detection sensitivity\nfor rare malicious activities (minority classes). We conducted extensive\nexperiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to\nvalidate our approach. The empirical results demonstrate that the proposed\nGTCN-G model achieves state-of-the-art performance, significantly outperforming\nexisting baseline models in both binary and multi-class classification tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.07285v1",
      "published": "2025-10-08T17:45:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07285v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Online Rubrics Elicitation from Pairwise Comparisons",
      "authors": [
        "MohammadHossein Rezaei",
        "Robert Vacareanu",
        "Zihao Wang",
        "Clinton Wang",
        "Yunzhong He",
        "Afra Feyza Akyürek"
      ],
      "abstract": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2510.07284v1",
      "published": "2025-10-08T17:44:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07284v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences",
      "authors": [
        "Pulkit Rustagi",
        "Kyle Hollins Wray",
        "Sandhya Saisubramanian"
      ],
      "abstract": "Many real-world scenarios require multiple agents to coordinate in shared\nenvironments, while balancing trade-offs between multiple, potentially\ncompeting objectives. Current multi-objective multi-agent path finding\n(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto\nfrontiers. They do not explicitly optimize for user-defined preferences, even\nwhen the preferences are available, and scale poorly with the number of\nobjectives. We propose a lexicographic framework for modeling MO-MAPF, along\nwith an algorithm \\textit{Lexicographic Conflict-Based Search} (LCBS) that\ndirectly computes a single solution aligned with a lexicographic preference\nover objectives. LCBS integrates a priority-aware low-level $A^*$ search with\nconflict-based search, avoiding Pareto frontier construction and enabling\nefficient planning guided by preference over objectives. We provide insights\ninto optimality and scalability, and empirically demonstrate that LCBS computes\noptimal solutions while scaling to instances with up to ten objectives -- far\nbeyond the limits of existing MO-MAPF methods. Evaluations on standard and\nrandomized MAPF benchmarks show consistently higher success rates against\nstate-of-the-art baselines, especially with increasing number of objectives.",
      "pdf_url": "http://arxiv.org/pdf/2510.07276v1",
      "published": "2025-10-08T17:40:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07276v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "On the false election between regulation and innovation. Ideas for regulation through the responsible use of artificial intelligence in research and education.[Spanish version]",
      "authors": [
        "Pompeu Casanovas"
      ],
      "abstract": "This short essay is a reworking of the answers offered by the author at the\nDebate Session of the AIHUB (CSIC) and EduCaixa Summer School, organized by\nMarta Garcia-Matos and Lissette Lemus, and coordinated by Albert Sabater\n(OEIAC, UG), with the participation of Vanina Martinez-Posse (IIIA-CSIC),\nEulalia Soler (Eurecat) and Pompeu Casanovas (IIIA-CSIC) on July 4th 2025.\nAlbert Sabater posed three questions: (1) How can regulatory frameworks\npriori-tise the protection of fundamental rights (privacy, non-discrimination,\nautonomy, etc.) in the development of AI, without falling into the false\ndichotomy between regulation and innova-tion? (2) Given the risks of AI (bias,\nmass surveillance, manipulation), what examples of regu-lations or policies\nhave demonstrated that it is possible to foster responsible innovation, putting\nthe public interest before profitability, without giving in to competitive\npressure from actors such as China or the US? (3) In a scenario where the US\nprioritizes flexibility, what mecha-nisms could ensure that international\ncooperation in AI does not become a race to the bottom in rights, but rather a\nglobal standard of accountability? The article attempts to answer these three\nquestions and concludes with some reflections on the relevance of the answers\nfor education and research.",
      "pdf_url": "http://arxiv.org/pdf/2510.07268v1",
      "published": "2025-10-08T17:33:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07268v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "J.4; K.3; K.4; K.5"
      ]
    },
    {
      "title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation",
      "authors": [
        "Joseph Enguehard",
        "Morgane Van Ermengem",
        "Kate Atkinson",
        "Sujeong Cha",
        "Arijit Ghosh Chowdhury",
        "Prashanth Kallur Ramaswamy",
        "Jeremy Roghair",
        "Hannah R Marlowe",
        "Carina Suzana Negreanu",
        "Kitty Boxall",
        "Diana Mincu"
      ],
      "abstract": "Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering.",
      "pdf_url": "http://arxiv.org/pdf/2510.07243v1",
      "published": "2025-10-08T17:10:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07243v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships",
      "authors": [
        "Donggyu Lee",
        "Sungwon Park",
        "Yerin Hwang",
        "Hyunwoo Oh",
        "Hyoshin Kim",
        "Jungwon Kim",
        "Meeyoung Cha",
        "Sangyoon Park",
        "Jihee Kim"
      ],
      "abstract": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2510.07231v1",
      "published": "2025-10-08T17:00:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07231v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation",
      "authors": [
        "Arjun Krishnakumar",
        "Rhea Sanjay Sukthanker",
        "Hannan Javed Mahadik",
        "Gabriela Kadlecová",
        "Vladyslav Moroshan",
        "Timur Carstensen",
        "Frank Hutter",
        "Aaron Klein"
      ],
      "abstract": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale.",
      "pdf_url": "http://arxiv.org/pdf/2510.07227v1",
      "published": "2025-10-08T16:57:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07227v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation",
      "authors": [
        "Wen Ye",
        "Zhaocheng Liu",
        "Yuwei Gui",
        "Tingyu Yuan",
        "Yunyue Su",
        "Bowen Fang",
        "Chaoyang Zhao",
        "Qiang Liu",
        "Liang Wang"
      ],
      "abstract": "Text-to-image synthesis has made remarkable progress, yet accurately\ninterpreting complex and lengthy prompts remains challenging, often resulting\nin semantic inconsistencies and missing details. Existing solutions, such as\nfine-tuning, are model-specific and require training, while prior automatic\nprompt optimization (APO) approaches typically lack systematic error analysis\nand refinement strategies, resulting in limited reliability and effectiveness.\nMeanwhile, test-time scaling methods operate on fixed prompts and on noise or\nsample numbers, limiting their interpretability and adaptability. To solve\nthese, we introduce a flexible and efficient test-time prompt optimization\nstrategy that operates directly on the input text. We propose a plug-and-play\nmulti-agent system called GenPilot, integrating error analysis,\nclustering-based adaptive exploration, fine-grained verification, and a memory\nmodule for iterative optimization. Our approach is model-agnostic,\ninterpretable, and well-suited for handling long and complex prompts.\nSimultaneously, we summarize the common patterns of errors and the refinement\nstrategy, offering more experience and encouraging further exploration.\nExperiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%\ndemonstrate the strong capability of our methods in enhancing the text and\nimage consistency and structural coherence of generated images, revealing the\neffectiveness of our test-time prompt optimization strategy. The code is\navailable at https://github.com/27yw/GenPilot.",
      "pdf_url": "http://arxiv.org/pdf/2510.07217v1",
      "published": "2025-10-08T16:51:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07217v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models",
      "authors": [
        "Chengzhi Zhong",
        "Fei Cheng",
        "Qianying Liu",
        "Yugo Murawaki",
        "Chenhui Chu",
        "Sadao Kurohashi"
      ],
      "abstract": "Large language models exhibit strong multilingual capabilities despite\nlimited exposure to non-English data. Prior studies show that English-centric\nlarge language models map multilingual content into English-aligned\nrepresentations at intermediate layers and then project them back into\ntarget-language token spaces in the final layer. From this observation, we\nhypothesize that this cross-lingual transition is governed by a small and\nsparse set of dimensions, which occur at consistent indices across the\nintermediate to final layers. Building on this insight, we introduce a simple,\ntraining-free method to identify and manipulate these dimensions, requiring\nonly as few as 50 sentences of either parallel or monolingual data. Experiments\non a multilingual generation control task reveal the interpretability of these\ndimensions, demonstrating that the interventions in these dimensions can switch\nthe output language while preserving semantic content, and that it surpasses\nthe performance of prior neuron-based approaches at a substantially lower cost.",
      "pdf_url": "http://arxiv.org/pdf/2510.07213v1",
      "published": "2025-10-08T16:46:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07213v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving",
      "authors": [
        "Donald Pfaffmann",
        "Matthias Klusch",
        "Marcel Steinmetz"
      ],
      "abstract": "We present a novel hybrid learning-assisted planning method, named HyPlan,\nfor solving the collision-free navigation problem for self-driving cars in\npartially observable traffic environments. HyPlan combines methods for\nmulti-agent behavior prediction, deep reinforcement learning with proximal\npolicy optimization and approximated online POMDP planning with heuristic\nconfidence-based vertical pruning to reduce its execution time without\ncompromising safety of driving. Our experimental performance analysis on the\nCARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed\nthat HyPlan may navigate safer than selected relevant baselines and perform\nsignificantly faster than considered alternative online POMDP planners.",
      "pdf_url": "http://arxiv.org/pdf/2510.07210v1",
      "published": "2025-10-08T16:44:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07210v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Resolution scaling governs DINOv3 transfer performance in chest radiograph classification",
      "authors": [
        "Soroosh Tayebi Arasteh",
        "Mina Shaigan",
        "Christiane Kuhl",
        "Jakob Nikolas Kather",
        "Sven Nebelung",
        "Daniel Truhn"
      ],
      "abstract": "Self-supervised learning (SSL) has advanced visual representation learning,\nbut its value in chest radiography, a high-volume imaging modality with\nfine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL\nmodels through Gram-anchored self-distillation. Whether these design choices\nimprove transfer learning for chest radiography has not been systematically\ntested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across\nseven datasets (n>814,000). Two representative backbones were evaluated:\nViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and\n1024x1024 pixels. We additionally assessed frozen features from a 7B model. The\nprimary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2\nachieved comparable performance on adult datasets. Increasing resolution to\n512x512 yielded consistent improvements for DINOv3 over both DINOv2 and\nImageNet. In contrast, results in pediatric cohort showed no differences across\ninitializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models\nusing frozen DINOv3-7B features underperformed relative to fully finetuned\n86-89M-parameter backbones, highlighting the importance of domain adaptation.\nScaling to 1024x1024 did not further improve accuracy. Resolution-related gains\nwere most evident for boundary-dependent and small focal abnormalities. In\nchest radiography, higher input resolution is critical for leveraging the\nbenefits of modern self-supervised models. 512x512 pixels represent a practical\nupper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest\nperformance, while larger inputs offer minimal return on cost. Clinically,\nthese findings support use of finetuned, mid-sized backbones at 512x512 for\nchest radiograph interpretation, with the greatest gains expected in detecting\nsubtle or boundary-centered lesions relevant to emergency and critical care\nsettings.",
      "pdf_url": "http://arxiv.org/pdf/2510.07191v1",
      "published": "2025-10-08T16:25:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07191v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics",
      "authors": [
        "Yi Han",
        "Cheng Chi",
        "Enshen Zhou",
        "Shanyu Rong",
        "Jingkun An",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Lu Sheng",
        "Shanghang Zhang"
      ],
      "abstract": "Vision-Language Models (VLMs) have shown remarkable capabilities in spatial\nreasoning, yet they remain fundamentally limited to qualitative precision and\nlack the computational precision required for real-world robotics. Current\napproaches fail to leverage metric cues from depth sensors and camera\ncalibration, instead reducing geometric problems to pattern recognition tasks\nthat cannot deliver the centimeter-level accuracy essential for robotic\nmanipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel\nframework that transforms VLMs from perceptual estimators to geometric\ncomputers by enabling them to generate and execute precise geometric\ncomputations through external tools. Rather than attempting to internalize\ncomplex geometric operations within neural networks, TIGeR empowers models to\nrecognize geometric reasoning requirements, synthesize appropriate\ncomputational code, and invoke specialized libraries for exact calculations. To\nsupport this paradigm, we introduce TIGeR-300K, a comprehensive\ntool-invocation-oriented dataset covering point transformations, pose\nestimation, trajectory generation, and spatial compatibility verification,\ncomplete with tool invocation sequences and intermediate computations. Through\na two-stage training pipeline combining supervised fine-tuning (SFT) and\nreinforcement fine-tuning (RFT) with our proposed hierarchical reward design,\nTIGeR achieves SOTA performance on geometric reasoning benchmarks while\ndemonstrating centimeter-level precision in real-world robotic manipulation\ntasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.07181v1",
      "published": "2025-10-08T16:20:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07181v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents",
      "authors": [
        "Tianshi Zheng",
        "Kelvin Kiu-Wai Tam",
        "Newt Hue-Nam K. Nguyen",
        "Baixuan Xu",
        "Zhaowei Wang",
        "Jiayang Cheng",
        "Hong Ting Tsang",
        "Weiqi Wang",
        "Jiaxin Bai",
        "Tianqing Fang",
        "Yangqiu Song",
        "Ginny Y. Wong",
        "Simon See"
      ],
      "abstract": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.",
      "pdf_url": "http://arxiv.org/pdf/2510.07172v1",
      "published": "2025-10-08T16:12:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07172v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Integrating Domain Knowledge into Process Discovery Using Large Language Models",
      "authors": [
        "Ali Norouzifar",
        "Humam Kourani",
        "Marcus Dees",
        "Wil van der Aalst"
      ],
      "abstract": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework.",
      "pdf_url": "http://arxiv.org/pdf/2510.07161v1",
      "published": "2025-10-08T15:59:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07161v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL",
      "authors": [
        "Egor Cherepanov",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "Real-world robotic agents must act under partial observability and long\nhorizons, where key cues may appear long before they affect decision making.\nHowever, most modern approaches rely solely on instantaneous information,\nwithout incorporating insights from the past. Standard recurrent or transformer\nmodels struggle with retaining and leveraging long-term dependencies: context\nwindows truncate history, while naive memory extensions fail under scale and\nsparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a\ntransformer architecture with structured external memory. Each layer maintains\nmemory embeddings, interacts with them via bidirectional cross-attention, and\nupdates them through an Least Recently Used (LRU) memory module using\nreplacement or convex blending. ELMUR extends effective horizons up to 100,000\ntimes beyond the attention window and achieves a 100% success rate on a\nsynthetic T-Maze task with corridors up to one million steps. In POPGym, it\noutperforms baselines on more than half of the tasks. On MIKASA-Robo\nsparse-reward manipulation tasks with visual observations, it nearly doubles\nthe performance of strong baselines. These results demonstrate that structured,\nlayer-local external memory offers a simple and scalable approach to decision\nmaking under partial observability.",
      "pdf_url": "http://arxiv.org/pdf/2510.07151v1",
      "published": "2025-10-08T15:50:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07151v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "A Multi-Agent Framework for Stateful Inference-Time Search",
      "authors": [
        "Arshika Lalan",
        "Rajat Ghosh",
        "Aditya Kolsur",
        "Debojyoti Dutta"
      ],
      "abstract": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation.",
      "pdf_url": "http://arxiv.org/pdf/2510.07147v1",
      "published": "2025-10-08T15:48:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07147v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "cs.SE"
      ]
    },
    {
      "title": "Comparing human and language models sentence processing difficulties on complex structures",
      "authors": [
        "Samuel Joseph Amouyal",
        "Aya Meltzer-Asscher",
        "Jonathan Berant"
      ],
      "abstract": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.07141v1",
      "published": "2025-10-08T15:42:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07141v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking",
      "authors": [
        "Jiahang Liu",
        "Yunpeng Qi",
        "Jiazhao Zhang",
        "Minghan Li",
        "Shaoan Wang",
        "Kui Wu",
        "Hanjing Ye",
        "Hong Zhang",
        "Zhibo Chen",
        "Fangwei Zhong",
        "Zhizheng Zhang",
        "He Wang"
      ],
      "abstract": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins\npractical applications, such as companion robots, guidance robots and service\nassistants, where continuously following moving targets is essential. Recent\nadvances have enabled language-guided tracking in complex and unstructured\nscenes. However, existing approaches lack explicit spatial reasoning and\neffective temporal memory, causing failures under severe occlusions or in the\npresence of similar-looking distractors. To address these challenges, we\npresent TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances\nembodied visual tracking with two key modules, a spatial reasoning mechanism\nand a Target Identification Memory (TIM). The reasoning module introduces a\nChain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative\nposition and encodes it as a compact polar-coordinate token for action\nprediction. Guided by these spatial priors, the TIM employs a gated update\nstrategy to preserve long-horizon target memory, ensuring spatiotemporal\nconsistency and mitigating target loss during extended occlusions. Extensive\nexperiments show that TrackVLA++ achieves state-of-the-art performance on\npublic benchmarks across both egocentric and multi-camera settings. On the\nchallenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading\napproach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong\nzero-shot generalization, enabling robust real-world tracking in dynamic and\noccluded scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2510.07134v1",
      "published": "2025-10-08T15:29:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07134v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model",
      "authors": [
        "Tony Zhang",
        "Burak Kantarci",
        "Umair Siddique"
      ],
      "abstract": "Ensuring the safety of self-driving cars remains a major challenge due to the\ncomplexity and unpredictability of real-world driving environments. Traditional\ntesting methods face significant limitations, such as the oracle problem, which\nmakes it difficult to determine whether a system's behavior is correct, and the\ninability to cover the full range of scenarios an autonomous vehicle may\nencounter. In this paper, we introduce a digital twin-driven metamorphic\ntesting framework that addresses these challenges by creating a virtual replica\nof the self-driving system and its operating environment. By combining digital\ntwin technology with AI-based image generative models such as Stable Diffusion,\nour approach enables the systematic generation of realistic and diverse driving\nscenes. This includes variations in weather, road topology, and environmental\nfeatures, all while maintaining the core semantics of the original scenario.\nThe digital twin provides a synchronized simulation environment where changes\ncan be tested in a controlled and repeatable manner. Within this environment,\nwe define three metamorphic relations inspired by real-world traffic rules and\nvehicle behavior. We validate our framework in the Udacity self-driving\nsimulator and demonstrate that it significantly enhances test coverage and\neffectiveness. Our method achieves the highest true positive rate (0.719), F1\nscore (0.689), and precision (0.662) compared to baseline approaches. This\npaper highlights the value of integrating digital twins with AI-powered\nscenario generation to create a scalable, automated, and high-fidelity testing\nsolution for autonomous vehicle safety.",
      "pdf_url": "http://arxiv.org/pdf/2510.07133v1",
      "published": "2025-10-08T15:27:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07133v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Graph Conditioned Diffusion for Controllable Histopathology Image Generation",
      "authors": [
        "Sarah Cechnicka",
        "Matthew Baugh",
        "Weitong Zhang",
        "Mischa Dombrowski",
        "Zhe Li",
        "Johannes C. Paetzold",
        "Candice Roufosse",
        "Bernhard Kainz"
      ],
      "abstract": "Recent advances in Diffusion Probabilistic Models (DPMs) have set new\nstandards in high-quality image synthesis. Yet, controlled generation remains\nchallenging, particularly in sensitive areas such as medical imaging. Medical\nimages feature inherent structure such as consistent spatial arrangement, shape\nor texture, all of which are critical for diagnosis. However, existing DPMs\noperate in noisy latent spaces that lack semantic structure and strong priors,\nmaking it difficult to ensure meaningful control over generated content. To\naddress this, we propose graph-based object-level representations for\nGraph-Conditioned-Diffusion. Our approach generates graph nodes corresponding\nto each major structure in the image, encapsulating their individual features\nand relationships. These graph representations are processed by a transformer\nmodule and integrated into a diffusion model via the text-conditioning\nmechanism, enabling fine-grained control over generation. We evaluate this\napproach using a real-world histopathology use case, demonstrating that our\ngenerated data can reliably substitute for annotated patient data in downstream\nsegmentation tasks. The code is available here.",
      "pdf_url": "http://arxiv.org/pdf/2510.07129v1",
      "published": "2025-10-08T15:26:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07129v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Contingencies of Physical Embodiment Allow for Open-Endedness and Care",
      "authors": [
        "Leonardo Christov-Moore",
        "Arthur Juliani",
        "Alex Kiefer",
        "Nicco Reggente",
        "B. Scott Rousse",
        "Adam Safron",
        "Nicol'as Hinrichs",
        "Daniel Polani",
        "Antonio Damasio"
      ],
      "abstract": "Physical vulnerability and mortality are often seen as obstacles to be\navoided in the development of artificial agents, which struggle to adapt to\nopen-ended environments and provide aligned care. Meanwhile, biological\norganisms survive, thrive, and care for each other in an open-ended physical\nworld with relative ease and efficiency. Understanding the role of the\nconditions of life in this disparity can aid in developing more robust,\nadaptive, and caring artificial agents. Here we define two minimal conditions\nfor physical embodiment inspired by the existentialist phenomenology of Martin\nHeidegger: being-in-the-world (the agent is a part of the environment) and\nbeing-towards-death (unless counteracted, the agent drifts toward terminal\nstates due to the second law of thermodynamics). We propose that from these\nconditions we can obtain both a homeostatic drive - aimed at maintaining\nintegrity and avoiding death by expending energy to learn and act - and an\nintrinsic drive to continue to do so in as many ways as possible. Drawing\ninspiration from Friedrich Nietzsche's existentialist concept of will-to-power,\nwe examine how intrinsic drives to maximize control over future states, e.g.,\nempowerment, allow agents to increase the probability that they will be able to\nmeet their future homeostatic needs, thereby enhancing their capacity to\nmaintain physical integrity. We formalize these concepts within a reinforcement\nlearning framework, which enables us to examine how intrinsically driven\nembodied agents learning in open-ended multi-agent environments may cultivate\nthe capacities for open-endedness and care.ov",
      "pdf_url": "http://arxiv.org/pdf/2510.07117v1",
      "published": "2025-10-08T15:10:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07117v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning",
      "authors": [
        "Taylor Sorensen",
        "Yejin Choi"
      ],
      "abstract": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity,\nor legitimate disagreement between annotators. In this paper, we outline our\nsystem for modeling human variation. Our system leverages language models'\n(LLMs) in-context learning abilities, along with a two-step meta-learning\ntraining procedure for 1) post-training on many datasets requiring in-context\nlearning and 2) specializing the model via in-context meta-learning to the\nparticular data distribution of interest. We also evaluate the performance of\nour system submission to the Learning With Disagreements (LeWiDi) competition,\nwhere it was the overall winner on both tasks. Additionally, we perform an\nablation study to measure the importance of each system component. We find that\nincluding rater examples in-context is crucial for our system's performance,\ndataset-specific fine-tuning is helpful on the larger datasets, post-training\non other in-context datasets is helpful on one of the competition datasets, and\nthat performance improves with model scale.",
      "pdf_url": "http://arxiv.org/pdf/2510.07105v1",
      "published": "2025-10-08T14:59:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07105v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report",
      "authors": [
        "Riccardo Mereu",
        "Aidan Scannell",
        "Yuxin Hou",
        "Yi Zhao",
        "Aditya Jitta",
        "Antonio Dominguez",
        "Luigi Acerbi",
        "Amos Storkey",
        "Paul Chang"
      ],
      "abstract": "World models are a powerful paradigm in AI and robotics, enabling agents to\nreason about the future by predicting visual observations or compact latent\nstates. The 1X World Model Challenge introduces an open-source benchmark of\nreal-world humanoid interaction, with two complementary tracks: sampling,\nfocused on forecasting future image frames, and compression, focused on\npredicting future discrete latent codes. For the sampling track, we adapt the\nvideo generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned\nfuture frame prediction. We condition the video generation on robot states\nusing AdaLN-Zero, and further post-train the model using LoRA. For the\ncompression track, we train a Spatio-Temporal Transformer model from scratch.\nOur models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386\nin the compression task, securing 1st place in both challenges.",
      "pdf_url": "http://arxiv.org/pdf/2510.07092v1",
      "published": "2025-10-08T14:49:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07092v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas",
      "authors": [
        "Baixuan Xu",
        "Tianshi Zheng",
        "Zhaowei Wang",
        "Hong Ting Tsang",
        "Weiqi Wang",
        "Tianqing Fang",
        "Yangqiu Song"
      ],
      "abstract": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy.",
      "pdf_url": "http://arxiv.org/pdf/2510.07091v1",
      "published": "2025-10-08T14:47:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07091v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting",
      "authors": [
        "Tan Wang",
        "Yun Wei Dong",
        "Tao Zhang",
        "Qi Wang"
      ],
      "abstract": "Transformer-based methods have achieved impressive results in time series\nforecasting. However, existing Transformers still exhibit limitations in\nsequence modeling as they tend to overemphasize temporal dependencies. This\nincurs additional computational overhead without yielding corresponding\nperformance gains. We find that the performance of Transformers is highly\ndependent on the embedding method used to learn effective representations. To\naddress this issue, we extract multivariate features to augment the effective\ninformation captured in the embedding layer, yielding multidimensional\nembeddings that convey richer and more meaningful sequence representations.\nThese representations enable Transformer-based forecasters to better understand\nthe series. Specifically, we introduce Hybrid Temporal and Multivariate\nEmbeddings (HTME). The HTME extractor integrates a lightweight temporal feature\nextraction module with a carefully designed multivariate feature extraction\nmodule to provide complementary features, thereby achieving a balance between\nmodel complexity and performance. By combining HTME with the Transformer\narchitecture, we present HTMformer, leveraging the enhanced feature extraction\ncapability of the HTME extractor to build a lightweight forecaster. Experiments\nconducted on eight real-world datasets demonstrate that our approach\noutperforms existing baselines in both accuracy and efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2510.07084v1",
      "published": "2025-10-08T14:40:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07084v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
      "authors": [
        "Kento Kawaharazuka",
        "Jihoon Oh",
        "Jun Yamada",
        "Ingmar Posner",
        "Yuke Zhu"
      ],
      "abstract": "Amid growing efforts to leverage advances in large language models (LLMs) and\nvision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models\nhave recently gained significant attention. By unifying vision, language, and\naction data at scale, which have traditionally been studied separately, VLA\nmodels aim to learn policies that generalise across diverse tasks, objects,\nembodiments, and environments. This generalisation capability is expected to\nenable robots to solve novel downstream tasks with minimal or no additional\ntask-specific data, facilitating more flexible and scalable real-world\ndeployment. Unlike previous surveys that focus narrowly on action\nrepresentations or high-level model architectures, this work offers a\ncomprehensive, full-stack review, integrating both software and hardware\ncomponents of VLA systems. In particular, this paper provides a systematic\nreview of VLAs, covering their strategy and architectural transition,\narchitectures and building blocks, modality-specific processing techniques, and\nlearning paradigms. In addition, to support the deployment of VLAs in\nreal-world robotic applications, we also review commonly used robot platforms,\ndata collection strategies, publicly available datasets, data augmentation\nmethods, and evaluation benchmarks. Throughout this comprehensive survey, this\npaper aims to offer practical guidance for the robotics community in applying\nVLAs to real-world robotic systems. All references categorized by training\napproach, evaluation method, modality, and dataset are available in the table\non our project website: https://vla-survey.github.io .",
      "pdf_url": "http://arxiv.org/pdf/2510.07077v1",
      "published": "2025-10-08T14:38:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07077v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish",
      "authors": [
        "Fred Philippy",
        "Laura Bernardy",
        "Siwen Guo",
        "Jacques Klein",
        "Tegawendé F. Bissyandé"
      ],
      "abstract": "Instruction tuning has become a key technique for enhancing the performance\nof large language models, enabling them to better follow human prompts.\nHowever, low-resource languages such as Luxembourgish face severe limitations\ndue to the lack of high-quality instruction datasets. Traditional reliance on\nmachine translation often introduces semantic misalignment and cultural\ninaccuracies. In this work, we address these challenges by creating a\ncross-lingual instruction tuning dataset for Luxembourgish, without resorting\nto machine-generated translations into it. Instead, by leveraging aligned data\nfrom English, French, and German, we build a high-quality dataset that\npreserves linguistic and cultural nuances. We provide evidence that\ncross-lingual instruction tuning not only improves representational alignment\nacross languages but also the model's generative capabilities in Luxembourgish.\nThis highlights how cross-lingual data curation can avoid the common pitfalls\nof machine-translated data and directly benefit low-resource language\ndevelopment.",
      "pdf_url": "http://arxiv.org/pdf/2510.07074v1",
      "published": "2025-10-08T14:35:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07074v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems",
      "authors": [
        "André Hottung",
        "Federico Berto",
        "Chuanbo Hua",
        "Nayeli Gast Zepeda",
        "Daniel Wetzel",
        "Michael Römer",
        "Haoran Ye",
        "Davide Zago",
        "Michael Poli",
        "Stefano Massaroli",
        "Jinkyoo Park",
        "Kevin Tierney"
      ],
      "abstract": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery.",
      "pdf_url": "http://arxiv.org/pdf/2510.07073v1",
      "published": "2025-10-08T14:35:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07073v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Inductive Learning for Possibilistic Logic Programs Under Stable Models",
      "authors": [
        "Hongbo Hu",
        "Yisong Wang",
        "Yi Huang",
        "Kewen Wang"
      ],
      "abstract": "Possibilistic logic programs (poss-programs) under stable models are a major\nvariant of answer set programming (ASP). While its semantics (possibilistic\nstable models) and properties have been well investigated, the problem of\ninductive reasoning has not been investigated yet. This paper presents an\napproach to extracting poss-programs from a background program and examples\n(parts of intended possibilistic stable models). To this end, the notion of\ninduction tasks is first formally defined, its properties are investigated and\ntwo algorithms ilpsm and ilpsmmin for computing induction solutions are\npresented. An implementation of ilpsmmin is also provided and experimental\nresults show that when inputs are ordinary logic programs, the prototype\noutperforms a major inductive learning system for normal logic programs from\nstable models on the datasets that are randomly generated.",
      "pdf_url": "http://arxiv.org/pdf/2510.07069v1",
      "published": "2025-10-08T14:32:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07069v1",
      "categories": [
        "cs.AI",
        "I.2.4"
      ]
    },
    {
      "title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations",
      "authors": [
        "Manh Hung Nguyen",
        "Sebastian Tschiatschek",
        "Adish Singla"
      ],
      "abstract": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent.",
      "pdf_url": "http://arxiv.org/pdf/2510.07064v1",
      "published": "2025-10-08T14:28:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07064v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Introspection in Learned Semantic Scene Graph Localisation",
      "authors": [
        "Manshika Charvi Bissessur",
        "Efimia Panagiotaki",
        "Daniele De Martini"
      ],
      "abstract": "This work investigates how semantics influence localisation performance and\nrobustness in a learned self-supervised, contrastive semantic localisation\nframework. After training a localisation network on both original and perturbed\nmaps, we conduct a thorough post-hoc introspection analysis to probe whether\nthe model filters environmental noise and prioritises distinctive landmarks\nover routine clutter. We validate various interpretability methods and present\na comparative reliability analysis. Integrated gradients and Attention Weights\nconsistently emerge as the most reliable probes of learned behaviour. A\nsemantic class ablation further reveals an implicit weighting in which frequent\nobjects are often down-weighted. Overall, the results indicate that the model\nlearns noise-robust, semantically salient relations about place definition,\nthereby enabling explainable registration under challenging visual and\nstructural variations.",
      "pdf_url": "http://arxiv.org/pdf/2510.07053v1",
      "published": "2025-10-08T14:21:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07053v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO",
        "I.2.10; I.2.9; I.4.8; I.5.2; I.5.1"
      ]
    },
    {
      "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models",
      "authors": [
        "Yuntao Gui",
        "James Cheng"
      ],
      "abstract": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3",
      "pdf_url": "http://arxiv.org/pdf/2510.07048v1",
      "published": "2025-10-08T14:16:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07048v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ]
    },
    {
      "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning",
      "authors": [
        "Wenxun Wu",
        "Yuanyang Li",
        "Guhan Chen",
        "Linyue Wang",
        "Hongyang Chen"
      ],
      "abstract": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.07038v1",
      "published": "2025-10-08T14:04:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07038v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration",
      "authors": [
        "Tengwei Song",
        "Min Wu",
        "Yuan Fang"
      ],
      "abstract": "Molecular representation learning plays a crucial role in advancing\napplications such as drug discovery and material design. Existing work\nleverages 2D and 3D modalities of molecular information for pre-training,\naiming to capture comprehensive structural and geometric insights. However,\nthese methods require paired 2D and 3D molecular data to train the model\neffectively and prevent it from collapsing into a single modality, posing\nlimitations in scenarios where a certain modality is unavailable or\ncomputationally expensive to generate. To overcome this limitation, we propose\nFlexMol, a flexible molecule pre-training framework that learns unified\nmolecular representations while supporting single-modality input. Specifically,\ninspired by the unified structure in vision-language models, our approach\nemploys separate models for 2D and 3D molecular data, leverages parameter\nsharing to improve computational efficiency, and utilizes a decoder to generate\nfeatures for the missing modality. This enables a multistage continuous\nlearning process where both modalities contribute collaboratively during\ntraining, while ensuring robustness when only one modality is available during\ninference. Extensive experiments demonstrate that FlexMol achieves superior\nperformance across a wide range of molecular property prediction tasks, and we\nalso empirically demonstrate its effectiveness with incomplete data. Our code\nand data are available at https://github.com/tewiSong/FlexMol.",
      "pdf_url": "http://arxiv.org/pdf/2510.07035v1",
      "published": "2025-10-08T14:02:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07035v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge",
      "authors": [
        "Shrestha Ghosh",
        "Luca Giordano",
        "Yujia Hu",
        "Tuan-Phong Nguyen",
        "Simon Razniewski"
      ],
      "abstract": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI\ntasks. A significant contributor is their factual knowledge, which, to date,\nremains poorly understood, and is usually analyzed from biased samples. In this\npaper, we take a deep tour into the factual knowledge (or beliefs) of a\nfrontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited\nset of 100 million beliefs of one of the strongest currently available frontier\nLLMs, GPT-4.1. We find that the models' factual knowledge differs quite\nsignificantly from established knowledge bases, and that its accuracy is\nsignificantly lower than indicated by previous benchmarks. We also find that\ninconsistency, ambiguity and hallucinations are major issues, shedding light on\nfuture research opportunities concerning factual LLM knowledge.",
      "pdf_url": "http://arxiv.org/pdf/2510.07024v2",
      "published": "2025-10-08T13:48:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07024v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy",
      "authors": [
        "ZiHeng Huang",
        "Di Wu",
        "Jun Bai",
        "Jiale Zhang",
        "Sicong Cao",
        "Ji Zhang",
        "Yingjie Hu"
      ],
      "abstract": "Machine unlearning is critical for enforcing data deletion rights like the\n\"right to be forgotten.\" As a decentralized paradigm, Federated Learning (FL)\nalso requires unlearning, but realistic implementations face two major\nchallenges. First, fairness in Federated Unlearning (FU) is often overlooked.\nExact unlearning methods typically force all clients into costly retraining,\neven those uninvolved. Approximate approaches, using gradient ascent or\ndistillation, make coarse interventions that can unfairly degrade performance\nfor clients with only retained data. Second, most FU evaluations rely on\nsynthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity.\nThese unrealistic benchmarks obscure the true impact of unlearning and limit\nthe applicability of current methods. We first conduct a comprehensive\nbenchmark of existing FU methods under realistic data heterogeneity and\nfairness conditions. We then propose a novel, fairness-aware FU approach,\nFederated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address\nboth challenges. FedCCCU offers a practical and scalable solution for\nreal-world FU. Experimental results show that existing methods perform poorly\nin realistic settings, while our approach consistently outperforms them.",
      "pdf_url": "http://arxiv.org/pdf/2510.07022v1",
      "published": "2025-10-08T13:47:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07022v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Native Hybrid Attention for Efficient Sequence Modeling",
      "authors": [
        "Jusen Du",
        "Jiaxi Hu",
        "Tao Zhang",
        "Weigao Sun",
        "Yu Cheng"
      ],
      "abstract": "Transformers excel at sequence modeling but face quadratic complexity, while\nlinear attention offers improved efficiency but often compromises recall\naccuracy over long contexts. In this work, we introduce Native Hybrid Attention\n(NHA), a novel hybrid architecture of linear and full attention that integrates\nboth intra \\& inter-layer hybridization into a unified layer design. NHA\nmaintains long-term context in key-value slots updated by a linear RNN, and\naugments them with short-term tokens from a sliding window. A single\n\\texttt{softmax attention} operation is then applied over all keys and values,\nenabling per-token and per-head context-dependent weighting without requiring\nadditional fusion parameters. The inter-layer behavior is controlled through a\nsingle hyperparameter, the sliding window size, which allows smooth adjustment\nbetween purely linear and full attention while keeping all layers structurally\nuniform. Experimental results show that NHA surpasses Transformers and other\nhybrid baselines on recall-intensive and commonsense reasoning tasks.\nFurthermore, pretrained LLMs can be structurally hybridized with NHA, achieving\ncompetitive accuracy while delivering significant efficiency gains. Code is\navailable at https://github.com/JusenD/NHA.",
      "pdf_url": "http://arxiv.org/pdf/2510.07019v1",
      "published": "2025-10-08T13:44:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07019v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages",
      "authors": [
        "Neel Prabhanjan Rachamalla",
        "Aravind Konakalla",
        "Gautam Rajeev",
        "Ashish Kulkarni",
        "Chandra Khatri",
        "Shubham Agarwal"
      ],
      "abstract": "The effectiveness of Large Language Models (LLMs) depends heavily on the\navailability of high-quality post-training data, particularly\ninstruction-tuning and preference-based examples. Existing open-source\ndatasets, however, often lack multilingual coverage, cultural grounding, and\nsuffer from task diversity gaps that are especially pronounced for Indian\nlanguages. We introduce a human-in-the-loop pipeline that combines translations\nwith synthetic expansion to produce reliable and diverse Indic post-training\ndata. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and\nPragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56\nsub-categories, leveraging 57 diverse datasets. Our dataset protocol\nincorporates several often-overlooked dimensions and emphasize task diversity,\nmulti-turn dialogue, instruction fidelity, safety alignment, and preservation\nof cultural nuance, providing a foundation for more inclusive and effective\nmultilingual LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.07000v1",
      "published": "2025-10-08T13:23:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.07000v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "The Limits of Goal-Setting Theory in LLM-Driven Assessment",
      "authors": [
        "Mrityunjay Kumar"
      ],
      "abstract": "Many users interact with AI tools like ChatGPT using a mental model that\ntreats the system as human-like, which we call Model H. According to\ngoal-setting theory, increased specificity in goals should reduce performance\nvariance. If Model H holds, then prompting a chatbot with more detailed\ninstructions should lead to more consistent evaluation behavior.\n  This paper tests that assumption through a controlled experiment in which\nChatGPT evaluated 29 student submissions using four prompts with increasing\nspecificity. We measured consistency using intra-rater reliability (Cohen's\nKappa) across repeated runs.\n  Contrary to expectations, performance did not improve consistently with\nincreased prompt specificity, and performance variance remained largely\nunchanged. These findings challenge the assumption that LLMs behave like human\nevaluators and highlight the need for greater robustness and improved input\nintegration in future model development.",
      "pdf_url": "http://arxiv.org/pdf/2510.06997v1",
      "published": "2025-10-08T13:20:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06997v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "VelLMes: A high-interaction AI-based deception framework",
      "authors": [
        "Muris Sladić",
        "Veronica Valeros",
        "Carlos Catania",
        "Sebastian Garcia"
      ],
      "abstract": "There are very few SotA deception systems based on Large Language Models. The\nexisting ones are limited only to simulating one type of service, mainly SSH\nshells. These systems - but also the deception technologies not based on LLMs -\nlack an extensive evaluation that includes human attackers. Generative AI has\nrecently become a valuable asset for cybersecurity researchers and\npractitioners, and the field of cyber-deception is no exception. Researchers\nhave demonstrated how LLMs can be leveraged to create realistic-looking\nhoneytokens, fake users, and even simulated systems that can be used as\nhoneypots. This paper presents an AI-based deception framework called VelLMes,\nwhich can simulate multiple protocols and services such as SSH Linux shell,\nMySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus\nVelLMes offers a variety of choices for deception design based on the users'\nneeds. VelLMes is designed to be attacked by humans, so interactivity and\nrealism are key for its performance. We evaluate the generative capabilities\nand the deception capabilities. Generative capabilities were evaluated using\nunit tests for LLMs. The results of the unit tests show that, with careful\nprompting, LLMs can produce realistic-looking responses, with some LLMs having\na 100% passing rate. In the case of the SSH Linux shell, we evaluated deception\ncapabilities with 89 human attackers. The results showed that about 30% of the\nattackers thought that they were interacting with a real system when they were\nassigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH\nLinux shell honeypot on the Internet to capture real-life attacks. Analysis of\nthese attacks showed us that LLM honeypots simulating Linux shells can perform\nwell against unstructured and unexpected attacks on the Internet, responding\ncorrectly to most of the issued commands.",
      "pdf_url": "http://arxiv.org/pdf/2510.06975v1",
      "published": "2025-10-08T13:00:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06975v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Learning Global Representation from Queries for Vectorized HD Map Construction",
      "authors": [
        "Shoumeng Qiu",
        "Xinrun Li",
        "Yang Long",
        "Xiangyang Xue",
        "Varun Ojha",
        "Jian Pu"
      ],
      "abstract": "The online construction of vectorized high-definition (HD) maps is a\ncornerstone of modern autonomous driving systems. State-of-the-art approaches,\nparticularly those based on the DETR framework, formulate this as an instance\ndetection problem. However, their reliance on independent, learnable object\nqueries results in a predominantly local query perspective, neglecting the\ninherent global representation within HD maps. In this work, we propose\n\\textbf{MapGR} (\\textbf{G}lobal \\textbf{R}epresentation learning for HD\n\\textbf{Map} construction), an architecture designed to learn and utilize a\nglobal representations from queries. Our method introduces two synergistic\nmodules: a Global Representation Learning (GRL) module, which encourages the\ndistribution of all queries to better align with the global map through a\ncarefully designed holistic segmentation task, and a Global Representation\nGuidance (GRG) module, which endows each individual query with explicit,\nglobal-level contextual information to facilitate its optimization. Evaluations\non the nuScenes and Argoverse2 datasets validate the efficacy of our approach,\ndemonstrating substantial improvements in mean Average Precision (mAP) compared\nto leading baselines.",
      "pdf_url": "http://arxiv.org/pdf/2510.06969v1",
      "published": "2025-10-08T12:56:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06969v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}