{
  "last_updated": "2025-09-30T00:48:38.332517",
  "papers": [
    {
      "title": "VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing",
      "authors": [
        "Ke Wang",
        "Houxing Ren",
        "Zimu Lu",
        "Mingjie Zhan",
        "Hongsheng Li"
      ],
      "abstract": "The growing capabilities of large language models and multimodal systems have\nspurred interest in voice-first AI assistants, yet existing benchmarks are\ninadequate for evaluating the full range of these systems' capabilities. We\nintroduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI\nassistants across listening, speaking, and viewing. VoiceAssistant-Eval\ncomprises 10,497 curated examples spanning 13 task categories. These tasks\ninclude natural sounds, music, and spoken dialogue for listening; multi-turn\ndialogue, role-play imitation, and various scenarios for speaking; and highly\nheterogeneous images for viewing. To demonstrate its utility, we evaluate 21\nopen-source models and GPT-4o-Audio, measuring the quality of the response\ncontent and speech, as well as their consistency. The results reveal three key\nfindings: (1) proprietary models do not universally outperform open-source\nmodels; (2) most models excel at speaking tasks but lag in audio understanding;\nand (3) well-designed smaller models can rival much larger ones. Notably, the\nmid-sized Step-Audio-2-mini (7B) achieves more than double the listening\naccuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal\n(audio plus visual) input and role-play voice imitation tasks are difficult for\ncurrent models, and significant gaps persist in robustness and safety\nalignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous\nframework for evaluating and guiding the development of next-generation AI\nassistants. Code and data will be released at\nhttps://mathllm.github.io/VoiceAssistantEval/ .",
      "pdf_url": "http://arxiv.org/pdf/2509.22651v1",
      "published": "2025-09-26T17:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22651v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.SD"
      ]
    },
    {
      "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
      "authors": [
        "Chih Yao Hu",
        "Yang-Sen Lin",
        "Yuna Lee",
        "Chih-Hai Su",
        "Jie-Ying Lee",
        "Shr-Ruei Tsai",
        "Chin-Yang Lin",
        "Kuan-Wen Chen",
        "Tsung-Wei Ke",
        "Yu-Lun Liu"
      ],
      "abstract": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language\nnavigation (AVLN) framework built atop vision-language models (VLMs). SPF is\ncapable of navigating to any goal based on any type of free-form instructions\nin any kind of environment. In contrast to existing VLM-based approaches that\ntreat action prediction as a text generation task, our key insight is to\nconsider action prediction for AVLN as a 2D spatial grounding task. SPF\nharnesses VLMs to decompose vague language instructions into iterative\nannotation of 2D waypoints on the input image. Along with the predicted\ntraveling distance, SPF transforms predicted 2D waypoints into 3D displacement\nvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the\ntraveling distance to facilitate more efficient navigation. Notably, SPF\nperforms navigation in a closed-loop control manner, enabling UAVs to follow\ndynamic targets in dynamic environments. SPF sets a new state of the art in DRL\nsimulation benchmark, outperforming the previous best method by an absolute\nmargin of 63%. In extensive real-world evaluations, SPF outperforms strong\nbaselines by a large margin. We also conduct comprehensive ablation studies to\nhighlight the effectiveness of our design choice. Lastly, SPF shows remarkable\ngeneralization to different VLMs. Project page: https://spf-web.pages.dev",
      "pdf_url": "http://arxiv.org/pdf/2509.22653v1",
      "published": "2025-09-26T17:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22653v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Toward a Physics of Deep Learning and Brains",
      "authors": [
        "Arsham Ghavasieh",
        "Meritxell Vila-Minana",
        "Akanksha Khurd",
        "John Beggs",
        "Gerardo Ortiz",
        "Santo Fortunato"
      ],
      "abstract": "Deep neural networks and brains both learn and share superficial\nsimilarities: processing nodes are likened to neurons and adjustable weights\nare likened to modifiable synapses. But can a unified theoretical framework be\nfound to underlie them both? Here we show that the equations used to describe\nneuronal avalanches in living brains can also be applied to cascades of\nactivity in deep neural networks. These equations are derived from\nnon-equilibrium statistical physics and show that deep neural networks learn\nbest when poised between absorbing and active phases. Because these networks\nare strongly driven by inputs, however, they do not operate at a true critical\npoint but within a quasi-critical regime -- one that still approximately\nsatisfies crackling noise scaling relations. By training networks with\ndifferent initializations, we show that maximal susceptibility is a more\nreliable predictor of learning than proximity to the critical point itself.\nThis provides a blueprint for engineering improved network performance.\nFinally, using finite-size scaling we identify distinct universality classes,\nincluding Barkhausen noise and directed percolation. This theoretical framework\ndemonstrates that universal features are shared by both biological and\nartificial neural networks.",
      "pdf_url": "http://arxiv.org/pdf/2509.22649v1",
      "published": "2025-09-26T17:59:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22649v1",
      "categories": [
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "nlin.AO",
        "physics.bio-ph"
      ]
    },
    {
      "title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning",
      "authors": [
        "Long Xing",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Yuhang Cao",
        "Jianze Liang",
        "Qidong Huang",
        "Jiaqi Wang",
        "Feng Wu",
        "Dahua Lin"
      ],
      "abstract": "Image captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-training Large Vision-Language Models\n(LVLMs). Current state-of-the-art captioning models are typically trained with\nSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduce\nCaptioning Reinforcement Learning (CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answering Multiple-Choice Questions based solely on\nthat caption. As the first study to apply RLVR to the subjective image\ncaptioning task, we demonstrate that CapRL significantly enhances multiple\nsettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B\nresults in substantial gains across 12 benchmarks. Moreover, within the Prism\nFramework for caption quality evaluation, CapRL achieves performance comparable\nto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.\nCode is available here: https://github.com/InternLM/CapRL.",
      "pdf_url": "http://arxiv.org/pdf/2509.22647v1",
      "published": "2025-09-26T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22647v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs",
      "authors": [
        "Xingyu Fu",
        "Siyi Liu",
        "Yinuo Xu",
        "Pan Lu",
        "Guangqiuse Hu",
        "Tianbo Yang",
        "Taran Anantasagar",
        "Christopher Shen",
        "Yikai Mao",
        "Yuanzhe Liu",
        "Keyush Shah",
        "Chung Un Lee",
        "Yejin Choi",
        "James Zou",
        "Dan Roth",
        "Chris Callison-Burch"
      ],
      "abstract": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
      "pdf_url": "http://arxiv.org/pdf/2509.22646v1",
      "published": "2025-09-26T17:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22646v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning",
      "authors": [
        "Zimu Lu",
        "Houxing Ren",
        "Yunqiao Yang",
        "Ke Wang",
        "Zhuofan Zong",
        "Junting Pan",
        "Mingjie Zhan",
        "Hongsheng Li"
      ],
      "abstract": "Agent systems powered by large language models (LLMs) have demonstrated\nimpressive performance on repository-level code-generation tasks. However, for\ntasks such as website codebase generation, which depend heavily on visual\neffects and user-interaction feedback, current code agents rely only on simple\ncode execution for feedback and verification. This approach fails to capture\nthe actual quality of the generated code. In this paper, we propose\nWebGen-Agent, a novel website-generation agent that leverages comprehensive and\nmulti-level visual feedback to iteratively generate and refine the website\ncodebase. Detailed and expressive text descriptions and suggestions regarding\nthe screenshots and GUI-agent testing of the websites are generated by a visual\nlanguage model (VLM), together with scores that quantify their quality. The\nscreenshot and GUI-agent scores are further integrated with a backtracking and\nselect-best mechanism, enhancing the performance of the agent. Utilizing the\naccurate visual scores inherent in the WebGen-Agent workflow, we further\nintroduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve\nthe ability of LLMs to act as the reasoning engine of WebGen-Agent. By using\nthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we\nprovide a dense and reliable process supervision signal, which effectively\nimproves the model's website-generation ability. On the WebGen-Bench dataset,\nWebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%\nand its appearance score from 3.0 to 3.9, outperforming the previous\nstate-of-the-art agent system. Additionally, our Step-GRPO training approach\nincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and\nraises the appearance score from 3.4 to 3.7.",
      "pdf_url": "http://arxiv.org/pdf/2509.22644v1",
      "published": "2025-09-26T17:59:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22644v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Hierarchical Representation Matching for CLIP-based Class-Incremental Learning",
      "authors": [
        "Zhen-Hao Wen",
        "Yan Wang",
        "Ji Feng",
        "Han-Jia Ye",
        "De-Chuan Zhan",
        "Da-Wei Zhou"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to endow models with the ability to\ncontinuously adapt to evolving data streams. Recent advances in pre-trained\nvision-language models (e.g., CLIP) provide a powerful foundation for this\ntask. However, existing approaches often rely on simplistic templates, such as\n\"a photo of a [CLASS]\", which overlook the hierarchical nature of visual\nconcepts. For example, recognizing \"cat\" versus \"car\" depends on coarse-grained\ncues, while distinguishing \"cat\" from \"lion\" requires fine-grained details.\nSimilarly, the current feature mapping in CLIP relies solely on the\nrepresentation from the last layer, neglecting the hierarchical information\ncontained in earlier layers. In this work, we introduce HiErarchical\nRepresentation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages\nLLMs to recursively generate discriminative textual descriptors, thereby\naugmenting the semantic space with explicit hierarchical cues. These\ndescriptors are matched to different levels of the semantic hierarchy and\nadaptively routed based on task-specific requirements, enabling precise\ndiscrimination while alleviating catastrophic forgetting in incremental tasks.\nExtensive experiments on multiple benchmarks demonstrate that our method\nconsistently achieves state-of-the-art performance.",
      "pdf_url": "http://arxiv.org/pdf/2509.22645v1",
      "published": "2025-09-26T17:59:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22645v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity",
      "authors": [
        "Arkadiy Saakyan",
        "Najoung Kim",
        "Smaranda Muresan",
        "Tuhin Chakrabarty"
      ],
      "abstract": "N-gram novelty is widely used to evaluate language models' ability to\ngenerate text outside of their training data. More recently, it has also been\nadopted as a metric for measuring textual creativity. However, theoretical work\non creativity suggests that this approach may be inadequate, as it does not\naccount for creativity's dual nature: novelty (how original the text is) and\nappropriateness (how sensical and pragmatic it is). We investigate the\nrelationship between this notion of creativity and n-gram novelty through 7542\nexpert writer annotations (n=26) of novelty, pragmaticality, and sensicality\nvia close reading of human and AI-generated text. We find that while n-gram\nnovelty is positively associated with expert writer-judged creativity, ~91% of\ntop-quartile expressions by n-gram novelty are not judged as creative,\ncautioning against relying on n-gram novelty alone. Furthermore, unlike\nhuman-written text, higher n-gram novelty in open-source LLMs correlates with\nlower pragmaticality. In an exploratory study with frontier close-source\nmodels, we additionally confirm that they are less likely to produce creative\nexpressions than humans. Using our dataset, we test whether zero-shot,\nfew-shot, and finetuned models are able to identify creative expressions (a\npositive aspect of writing) and non-pragmatic ones (a negative aspect).\nOverall, frontier LLMs exhibit performance much higher than random but leave\nroom for improvement, especially struggling to identify non-pragmatic\nexpressions. We further find that LLM-as-a-Judge novelty scores from the\nbest-performing model were predictive of expert writer preferences.",
      "pdf_url": "http://arxiv.org/pdf/2509.22641v1",
      "published": "2025-09-26T17:59:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22641v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
      "authors": [
        "Renjie Luo",
        "Zichen Liu",
        "Xiangyan Liu",
        "Chao Du",
        "Min Lin",
        "Wenhu Chen",
        "Wei Lu",
        "Tianyu Pang"
      ],
      "abstract": "LLMs are often trained with RL from human or AI feedback, yet such methods\ntypically compress nuanced feedback into scalar rewards, discarding much of\ntheir richness and inducing scale imbalance. We propose treating verbal\nfeedback as a conditioning signal. Inspired by language priors in text-to-image\ngeneration, which enable novel outputs from unseen prompts, we introduce the\nfeedback-conditional policy (FCP). FCP learns directly from response-feedback\npairs, approximating the feedback-conditional posterior through maximum\nlikelihood training on offline data. We further develop an online bootstrapping\nstage where the policy generates under positive conditions and receives fresh\nfeedback to refine itself. This reframes feedback-driven learning as\nconditional generation rather than reward optimization, offering a more\nexpressive way for LLMs to directly learn from verbal feedback. Our code is\navailable at https://github.com/sail-sg/feedback-conditional-policy.",
      "pdf_url": "http://arxiv.org/pdf/2509.22638v1",
      "published": "2025-09-26T17:58:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22638v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Variational Reasoning for Language Models",
      "authors": [
        "Xiangxin Zhou",
        "Zichen Liu",
        "Haonan Wang",
        "Chao Du",
        "Min Lin",
        "Chongxuan Li",
        "Liang Wang",
        "Tianyu Pang"
      ],
      "abstract": "We introduce a variational reasoning framework for language models that\ntreats thinking traces as latent variables and optimizes them through\nvariational inference. Starting from the evidence lower bound (ELBO), we extend\nit to a multi-trace objective for tighter bounds and propose a forward-KL\nformulation that stabilizes the training of the variational posterior. We\nfurther show that rejection sampling finetuning and binary-reward RL, including\nGRPO, can be interpreted as local forward-KL objectives, where an implicit\nweighting by model accuracy naturally arises from the derivation and reveals a\npreviously unnoticed bias toward easier questions. We empirically validate our\nmethod on the Qwen 2.5 and Qwen 3 model families across a wide range of\nreasoning tasks. Overall, our work provides a principled probabilistic\nperspective that unifies variational inference with RL-style methods and yields\nstable objectives for improving the reasoning ability of language models. Our\ncode is available at https://github.com/sail-sg/variational-reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2509.22637v1",
      "published": "2025-09-26T17:58:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22637v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback",
      "authors": [
        "Gen Li",
        "Yuling Yan"
      ],
      "abstract": "Reinforcement learning with human feedback (RLHF), which learns a reward\nmodel from human preference data and then optimizes a policy to favor preferred\nresponses, has emerged as a central paradigm for aligning large language models\n(LLMs) with human preferences. In this paper, we investigate exploration\nprinciples for online RLHF, where one seeks to adaptively collect new\npreference data to refine both the reward model and the policy in a\ndata-efficient manner. By examining existing optimism-based exploration\nalgorithms, we identify a drawback in their sampling protocol: they tend to\ngather comparisons that fail to reduce the most informative uncertainties in\nreward differences, and we prove lower bounds showing that such methods can\nincur linear regret over exponentially long horizons. Motivated by this\ninsight, we propose a new exploration scheme that directs preference queries\ntoward reducing uncertainty in reward differences most relevant to policy\nimprovement. Under a multi-armed bandit model of RLHF, we establish regret\nbounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter\nthat balances reward maximization against mitigating distribution shift. To our\nknowledge, this is the first online RLHF algorithm with regret scaling\npolynomially in all model parameters.",
      "pdf_url": "http://arxiv.org/pdf/2509.22633v1",
      "published": "2025-09-26T17:57:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22633v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "StateX: Enhancing RNN Recall via Post-training State Expansion",
      "authors": [
        "Xingyu Shen",
        "Yingfa Chen",
        "Zhen Leng Thai",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "While Transformer-based models have demonstrated remarkable language modeling\nperformance, their high complexities result in high costs when processing long\ncontexts. In contrast, recurrent neural networks (RNNs) such as linear\nattention and state space models have gained popularity due to their constant\nper-token complexities. However, these recurrent models struggle with tasks\nthat require accurate recall of contextual information from long contexts,\nbecause all contextual information is compressed into a constant-size recurrent\nstate. Previous works have shown that recall ability is positively correlated\nwith the recurrent state size, yet directly training RNNs with larger recurrent\nstates results in high training costs. In this paper, we introduce StateX, a\ntraining pipeline for efficiently expanding the states of pre-trained RNNs\nthrough post-training. For two popular classes of RNNs, linear attention and\nstate space models, we design post-training architectural modifications to\nscale up the state size with no or negligible increase in model parameters.\nExperiments on models up to 1.3B parameters demonstrate that StateX efficiently\nenhances the recall and in-context learning ability of RNNs without incurring\nhigh post-training costs or compromising other capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2509.22630v1",
      "published": "2025-09-26T17:55:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22630v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Learning Admissible Heuristics for A*: Theory and Practice",
      "authors": [
        "Ehsan Futuhi",
        "Nathan R. Sturtevant"
      ],
      "abstract": "Heuristic functions are central to the performance of search algorithms such\nas A-star, where admissibility - the property of never overestimating the true\nshortest-path cost - guarantees solution optimality. Recent deep learning\napproaches often disregard admissibility and provide limited guarantees on\ngeneralization beyond the training data. This paper addresses both of these\nlimitations. First, we pose heuristic learning as a constrained optimization\nproblem and introduce Cross-Entropy Admissibility (CEA), a loss function that\nenforces admissibility during training. On the Rubik's Cube domain, this method\nyields near-admissible heuristics with significantly stronger guidance than\ncompressed pattern database (PDB) heuristics. Theoretically, we study the\nsample complexity of learning heuristics. By leveraging PDB abstractions and\nthe structural properties of graphs such as the Rubik's Cube, we tighten the\nbound on the number of training samples needed for A-star to generalize.\nReplacing a general hypothesis class with a ReLU neural network gives bounds\nthat depend primarily on the network's width and depth, rather than on graph\nsize. Using the same network, we also provide the first generalization\nguarantees for goal-dependent heuristics.",
      "pdf_url": "http://arxiv.org/pdf/2509.22626v1",
      "published": "2025-09-26T17:51:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22626v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Theoretical Analysis of Discrete Flow Matching Generative Models",
      "authors": [
        "Maojiang Su",
        "Mingcheng Lu",
        "Jerry Yao-Chieh Hu",
        "Shang Wu",
        "Zhao Song",
        "Alex Reneau",
        "Han Liu"
      ],
      "abstract": "We provide a theoretical analysis for end-to-end training Discrete Flow\nMatching (DFM) generative models. DFM is a promising discrete generative\nmodeling framework that learns the underlying generative dynamics by training a\nneural network to approximate the transformative velocity field. Our analysis\nestablishes a clear chain of guarantees by decomposing the final distribution\nestimation error. We first prove that the total variation distance between the\ngenerated and target distributions is controlled by the risk of the learned\nvelocity field. We then bound this risk by analyzing its two primary sources:\n(i) Approximation Error, where we quantify the capacity of the Transformer\narchitecture to represent the true velocity, and (ii) Estimation Error, where\nwe derive statistical convergence rates that bound the error from training on a\nfinite dataset. By composing these results, we provide the first formal proof\nthat the distribution generated by a trained DFM model provably converges to\nthe true data distribution as the training set size increases.",
      "pdf_url": "http://arxiv.org/pdf/2509.22623v1",
      "published": "2025-09-26T17:48:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22623v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning",
      "authors": [
        "Aayush Mishra",
        "Daniel Khashabi",
        "Anqi Liu"
      ],
      "abstract": "Supervised Fine-Tuning (SFT) is used to specialize model behavior by training\nweights to produce intended target responses for queries. In contrast,\nIn-Context Learning (ICL) adapts models during inference with instructions or\ndemonstrations in the prompt. ICL can offer better generalizability and more\ncalibrated responses compared to SFT in data scarce settings, at the cost of\nmore inference compute. In this work, we ask the question: Can ICL's internal\ncomputations be used to improve the qualities of SFT? We first show that ICL\nand SFT produce distinct activation patterns, indicating that the two methods\nachieve adaptation through different functional mechanisms. Motivated by this\nobservation and to use ICL's rich functionality, we introduce ICL Activation\nAlignment (IA2), a self-distillation technique which aims to replicate ICL's\nactivation patterns in SFT models and incentivizes ICL-like internal reasoning.\nPerforming IA2 as a priming step before SFT significantly improves the accuracy\nand calibration of model outputs, as shown by our extensive empirical results\non 12 popular benchmarks and 2 model families. This finding is not only\npractically useful, but also offers a conceptual window into the inner\nmechanics of model adaptation.",
      "pdf_url": "http://arxiv.org/pdf/2509.22621v1",
      "published": "2025-09-26T17:46:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22621v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting",
      "authors": [
        "Yasmine Omri",
        "Connor Ding",
        "Tsachy Weissman",
        "Thierry Tambe"
      ],
      "abstract": "Modern vision language pipelines are driven by RGB vision encoders trained on\nmassive image text corpora. While these pipelines have enabled impressive zero\nshot capabilities and strong transfer across tasks, they still inherit two\nstructural inefficiencies from the pixel domain: (i) transmitting dense RGB\nimages from edge devices to the cloud is energy intensive and costly, and (ii)\npatch based tokenization explodes sequence length, stressing attention budgets\nand context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative\nvisual substrate for alignment: a compact, spatially adaptive representation\nthat parameterizes images by a set of colored anisotropic Gaussians. We develop\na scalable 2DGS pipeline with structured initialization, luminance aware\npruning, and batched CUDA kernels, achieving over 90x faster fitting and about\n97% GPU utilization compared to prior implementations. We further adapt\ncontrastive language image pretraining (CLIP) to 2DGS by reusing a frozen\nRGB-based transformer backbone with a lightweight splat aware input stem and a\nperceiver resampler, training only about 7% of the total parameters. On large\nDataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K\nperformance while compressing inputs 3 to 20x relative to pixels. While\naccuracy currently trails RGB encoders, our results establish 2DGS as a viable\nmultimodal substrate, pinpoint architectural bottlenecks, and open a path\ntoward representations that are both semantically powerful and transmission\nefficient for edge cloud learning.",
      "pdf_url": "http://arxiv.org/pdf/2509.22615v1",
      "published": "2025-09-26T17:41:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22615v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
      "authors": [
        "Siwei Wang",
        "Yifei Shen",
        "Haoran Sun",
        "Shi Feng",
        "Shang-Hua Teng",
        "Li Dong",
        "Yaru Hao",
        "Wei Chen"
      ],
      "abstract": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
      "pdf_url": "http://arxiv.org/pdf/2509.22613v1",
      "published": "2025-09-26T17:39:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22613v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
      "authors": [
        "Junkang Wu",
        "Kexin Huang",
        "Jiancan Wu",
        "An Zhang",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM\nreasoning, but training often oscillates between {entropy collapse} and\n{entropy explosion}. We trace both hazards to the mean baseline used in\nvalue-free RL (e.g., GRPO and DAPO), which improperly penalizes\nnegative-advantage samples under reward outliers. We propose {Quantile\nAdvantage Estimation} (QAE), replacing the mean with a group-wise K-quantile\nbaseline. QAE induces a response-level, two-regime gate: on hard queries (p <=\n1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it\ntargets remaining failures. Under first-order softmax updates, we prove\n{two-sided entropy safety}, giving lower and upper bounds on one-step entropy\nchange that curb explosion and prevent collapse. Empirically, this minimal\nmodification stabilizes entropy, sparsifies credit assignment (with tuned K,\nroughly 80% of responses receive zero advantage), and yields sustained pass@1\ngains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results\nidentify {baseline design} -- rather than token-level heuristics -- as the\nprimary mechanism for scaling RLVR.",
      "pdf_url": "http://arxiv.org/pdf/2509.22611v1",
      "published": "2025-09-26T17:37:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22611v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning",
      "authors": [
        "Yulei Qin",
        "Xiaoyu Tan",
        "Zhengbao He",
        "Gang Li",
        "Haojia Lin",
        "Zongyi Li",
        "Zihan Xu",
        "Yuchen Shi",
        "Siqi Cai",
        "Renting Rui",
        "Shaofei Cai",
        "Yuzheng Cai",
        "Xuan Zhang",
        "Sheng Ye",
        "Ke Li",
        "Xing Sun"
      ],
      "abstract": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence.",
      "pdf_url": "http://arxiv.org/pdf/2509.22601v1",
      "published": "2025-09-26T17:20:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22601v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.MA"
      ]
    },
    {
      "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time",
      "authors": [
        "Yixuan Han",
        "Fan Ma",
        "Ruijie Quan",
        "Yi Yang"
      ],
      "abstract": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2509.22572v1",
      "published": "2025-09-26T16:49:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22572v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration",
      "authors": [
        "Qi Mao",
        "Tinghan Yang",
        "Jiahao Li",
        "Bin Li",
        "Libiao Jin",
        "Yan Lu"
      ],
      "abstract": "The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI\nagents is transforming human-AI collaboration into bidirectional, multimodal\ninteraction. However, existing codecs remain optimized for unimodal, one-way\ncommunication, resulting in repeated degradation under conventional\ncompress-transmit-reconstruct pipelines. To address this limitation, we propose\nUniMIC, a Unified token-based Multimodal Interactive Coding framework that\nbridges edge devices and cloud AI agents. Instead of transmitting raw pixels or\nplain text, UniMIC employs compact tokenized representations as the\ncommunication medium, enabling efficient low-bitrate transmission while\nmaintaining compatibility with LMMs. To further enhance compression,\nlightweight Transformer-based entropy models with scenario-specific\ndesigns-generic, masked, and text-conditioned-effectively minimize inter-token\nredundancy. Extensive experiments on text-to-image generation, text-guided\ninpainting, outpainting, and visual question answering show that UniMIC\nachieves substantial bitrate savings and remains robust even at ultra-low\nbitrates (<0.05bpp), without compromising downstream task performance. These\nresults establish UniMIC as a practical and forward-looking paradigm for\nnext-generation multimodal interactive communication.",
      "pdf_url": "http://arxiv.org/pdf/2509.22570v1",
      "published": "2025-09-26T16:46:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22570v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "From Parameters to Behavior: Unsupervised Compression of the Policy Space",
      "authors": [
        "Davide Tenedini",
        "Riccardo Zamboni",
        "Mirco Mutti",
        "Marcello Restelli"
      ],
      "abstract": "Despite its recent successes, Deep Reinforcement Learning (DRL) is\nnotoriously sample-inefficient. We argue that this inefficiency stems from the\nstandard practice of optimizing policies directly in the high-dimensional and\nhighly redundant parameter space $\\Theta$. This challenge is greatly compounded\nin multi-task settings. In this work, we develop a novel, unsupervised approach\nthat compresses the policy parameter space $\\Theta$ into a low-dimensional\nlatent space $\\mathcal{Z}$. We train a generative model\n$g:\\mathcal{Z}\\to\\Theta$ by optimizing a behavioral reconstruction loss, which\nensures that the latent space is organized by functional similarity rather than\nproximity in parameterization. We conjecture that the inherent dimensionality\nof this manifold is a function of the environment's complexity, rather than the\nsize of the policy network. We validate our approach in continuous control\ndomains, showing that the parameterization of standard policy networks can be\ncompressed up to five orders of magnitude while retaining most of its\nexpressivity. As a byproduct, we show that the learned manifold enables\ntask-specific adaptation via Policy Gradient operating in the latent space\n$\\mathcal{Z}$.",
      "pdf_url": "http://arxiv.org/pdf/2509.22566v1",
      "published": "2025-09-26T16:42:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22566v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation",
      "authors": [
        "Wenyuan Chen",
        "Fateme Nateghi Haredasht",
        "Kameron C. Black",
        "Francois Grolleau",
        "Emily Alsentzer",
        "Jonathan H. Chen",
        "Stephen P. Ma"
      ],
      "abstract": "Asynchronous patient-clinician messaging via EHR portals is a growing source\nof clinician workload, prompting interest in large language models (LLMs) to\nassist with draft responses. However, LLM outputs may contain clinical\ninaccuracies, omissions, or tone mismatches, making robust evaluation\nessential. Our contributions are threefold: (1) we introduce a clinically\ngrounded error ontology comprising 5 domains and 59 granular error codes,\ndeveloped through inductive coding and expert adjudication; (2) we develop a\nretrieval-augmented evaluation pipeline (RAEC) that leverages semantically\nsimilar historical message-response pairs to improve judgment quality; and (3)\nwe provide a two-stage prompting architecture using DSPy to enable scalable,\ninterpretable, and hierarchical error detection. Our approach assesses the\nquality of drafts both in isolation and with reference to similar past\nmessage-response pairs retrieved from institutional archives. Using a two-stage\nDSPy pipeline, we compared baseline and reference-enhanced evaluations on over\n1,500 patient messages. Retrieval context improved error identification in\ndomains such as clinical completeness and workflow appropriateness. Human\nvalidation on 100 messages demonstrated superior agreement (concordance = 50%\nvs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.\nbaseline, supporting the use of our RAEC pipeline as AI guardrails for patient\nmessaging.",
      "pdf_url": "http://arxiv.org/pdf/2509.22565v1",
      "published": "2025-09-26T16:42:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22565v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Activation Function Design Sustains Plasticity in Continual Learning",
      "authors": [
        "Lute Lillo",
        "Nick Cheney"
      ],
      "abstract": "In independent, identically distributed (i.i.d.) training regimes, activation\nfunctions have been benchmarked extensively, and their differences often shrink\nonce model size and optimization are tuned. In continual learning, however, the\npicture is different: beyond catastrophic forgetting, models can progressively\nlose the ability to adapt (referred to as loss of plasticity) and the role of\nthe non-linearity in this failure mode remains underexplored. We show that\nactivation choice is a primary, architecture-agnostic lever for mitigating\nplasticity loss. Building on a property-level analysis of negative-branch shape\nand saturation behavior, we introduce two drop-in nonlinearities (Smooth-Leaky\nand Randomized Smooth-Leaky) and evaluate them in two complementary settings:\n(i) supervised class-incremental benchmarks and (ii) reinforcement learning\nwith non-stationary MuJoCo environments designed to induce controlled\ndistribution and dynamics shifts. We also provide a simple stress protocol and\ndiagnostics that link the shape of the activation to the adaptation under\nchange. The takeaway is straightforward: thoughtful activation design offers a\nlightweight, domain-general way to sustain plasticity in continual learning\nwithout extra capacity or task-specific tuning.",
      "pdf_url": "http://arxiv.org/pdf/2509.22562v1",
      "published": "2025-09-26T16:41:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22562v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models",
      "authors": [
        "Chenyu Zhou",
        "Tianyi Xu",
        "Jianghao Lin",
        "Dongdong Ge"
      ],
      "abstract": "Large Language Models (LLMs) have shown promising capabilities for solving\nOperations Research (OR) problems. While reinforcement learning serves as a\npowerful paradigm for LLM training on OR problems, existing works generally\nface two key limitations. First, outcome reward suffers from the credit\nassignment problem, where correct final answers can reinforce flawed reasoning.\nSecond, conventional discriminative process supervision is myopic, failing to\nevaluate the interdependent steps of OR modeling holistically. To this end, we\nintroduce StepORLM, a novel self-evolving framework with generative process\nsupervision. At its core, StepORLM features a co-evolutionary loop where a\npolicy model and a generative process reward model (GenPRM) iteratively improve\non each other. This loop is driven by a dual-feedback mechanism: definitive,\noutcome-based verification from an external solver, and nuanced, holistic\nprocess evaluation from the GenPRM. The combined signal is used to align the\npolicy via Weighted Direct Preference Optimization (W-DPO) and simultaneously\nrefine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new\nstate-of-the-art across six benchmarks, significantly outperforming vastly\nlarger generalist models, agentic methods, and specialized baselines. Moreover,\nthe co-evolved GenPRM is able to act as a powerful and universally applicable\nprocess verifier, substantially boosting the inference scaling performance of\nboth our own model and other existing LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.22558v1",
      "published": "2025-09-26T16:39:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22558v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ConQuER: Modular Architectures for Control and Bias Mitigation in IQP Quantum Generative Models",
      "authors": [
        "Xiaocheng Zou",
        "Shijin Duan",
        "Charles Fleming",
        "Gaowen Liu",
        "Ramana Rao Kompella",
        "Shaolei Ren",
        "Xiaolin Xu"
      ],
      "abstract": "Quantum generative models based on instantaneous quantum polynomial (IQP)\ncircuits show great promise in learning complex distributions while maintaining\nclassical trainability. However, current implementations suffer from two key\nlimitations: lack of controllability over generated outputs and severe\ngeneration bias towards certain expected patterns. We present a Controllable\nQuantum Generative Framework, ConQuER, which addresses both challenges through\na modular circuit architecture. ConQuER embeds a lightweight controller circuit\nthat can be directly combined with pre-trained IQP circuits to precisely\ncontrol the output distribution without full retraining. Leveraging the\nadvantages of IQP, our scheme enables precise control over properties such as\nthe Hamming Weight distribution with minimal parameter and gate overhead. In\naddition, inspired by the controller design, we extend this modular approach\nthrough data-driven optimization to embed implicit control paths in the\nunderlying IQP architecture, significantly reducing generation bias on\nstructured datasets. ConQuER retains efficient classical training properties\nand high scalability. We experimentally validate ConQuER on multiple quantum\nstate datasets, demonstrating its superior control accuracy and balanced\ngeneration performance, only with very low overhead cost over original IQP\ncircuits. Our framework bridges the gap between the advantages of quantum\ncomputing and the practical needs of controllable generation modeling.",
      "pdf_url": "http://arxiv.org/pdf/2509.22551v1",
      "published": "2025-09-26T16:32:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22551v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Does AI Coaching Prepare us for Workplace Negotiations?",
      "authors": [
        "Veda Duddu",
        "Jash Rajesh Parekh",
        "Andy Mao",
        "Hanyi Min",
        "Ziang Xiao",
        "Vedant Das Swain",
        "Koustuv Saha"
      ],
      "abstract": "Workplace negotiations are undermined by psychological barriers, which can\neven derail well-prepared tactics. AI offers personalized and always --\navailable negotiation coaching, yet its effectiveness for negotiation\npreparedness remains unclear. We built Trucey, a prototype AI coach grounded in\nBrett's negotiation model. We conducted a between-subjects experiment (N=267),\ncomparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by\nin-depth interviews (N=15). While Trucey showed the strongest reductions in\nfear relative to both comparison conditions, the Handbook outperformed both AIs\nin usability and psychological empowerment. Interviews revealed that the\nHandbook's comprehensive, reviewable content was crucial for participants'\nconfidence and preparedness. In contrast, although participants valued AI's\nrehearsal capability, its guidance often felt verbose and fragmented --\ndelivered in bits and pieces that required additional effort -- leaving them\nuncertain or overwhelmed. These findings challenge assumptions of AI\nsuperiority and motivate hybrid designs that integrate structured,\ntheory-driven content with targeted rehearsal, clear boundaries, and adaptive\nscaffolds to address psychological barriers and support negotiation\npreparedness.",
      "pdf_url": "http://arxiv.org/pdf/2509.22545v1",
      "published": "2025-09-26T16:21:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22545v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "The Emergence of Altruism in Large-Language-Model Agents Society",
      "authors": [
        "Haoyang Li",
        "Xiao Jia",
        "Zhanzhan Zhao"
      ],
      "abstract": "Leveraging Large Language Models (LLMs) for social simulation is a frontier\nin computational social science. Understanding the social logics these agents\nembody is critical to this attempt. However, existing research has primarily\nfocused on cooperation in small-scale, task-oriented games, overlooking how\naltruism, which means sacrificing self-interest for collective benefit, emerges\nin large-scale agent societies. To address this gap, we introduce a\nSchelling-variant urban migration model that creates a social dilemma,\ncompelling over 200 LLM agents to navigate an explicit conflict between\negoistic (personal utility) and altruistic (system utility) goals. Our central\nfinding is a fundamental difference in the social tendencies of LLMs. We\nidentify two distinct archetypes: \"Adaptive Egoists\", which default to\nprioritizing self-interest but whose altruistic behaviors significantly\nincrease under the influence of a social norm-setting message board; and\n\"Altruistic Optimizers\", which exhibit an inherent altruistic logic,\nconsistently prioritizing collective benefit even at a direct cost to\nthemselves. Furthermore, to qualitatively analyze the cognitive underpinnings\nof these decisions, we introduce a method inspired by Grounded Theory to\nsystematically code agent reasoning. In summary, this research provides the\nfirst evidence of intrinsic heterogeneity in the egoistic and altruistic\ntendencies of different LLMs. We propose that for social simulation, model\nselection is not merely a matter of choosing reasoning capability, but of\nchoosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a\nmore suitable choice for simulating complex human societies, \"Altruistic\nOptimizers\" are better suited for modeling idealized pro-social actors or\nscenarios where collective welfare is the primary consideration.",
      "pdf_url": "http://arxiv.org/pdf/2509.22537v1",
      "published": "2025-09-26T16:17:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22537v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models",
      "authors": [
        "Wenjun Wang",
        "Shuo Cai",
        "Congkai Xie",
        "Mingfa Feng",
        "Yiming Zhang",
        "Zhen Li",
        "Kejing Yang",
        "Ming Li",
        "Jiannong Cao",
        "Yuan Xie",
        "Hongxia Yang"
      ],
      "abstract": "The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training.",
      "pdf_url": "http://arxiv.org/pdf/2509.22536v1",
      "published": "2025-09-26T16:16:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22536v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model",
      "authors": [
        "Bo Li",
        "Guanzhi Deng",
        "Ronghao Chen",
        "Junrong Yue",
        "Shuo Zhang",
        "Qinghua Zhao",
        "Linqi Song",
        "Lijie Wen"
      ],
      "abstract": "Understanding how Large Language Models (LLMs) perform complex reasoning and\ntheir failure mechanisms is a challenge in interpretability research. To\nprovide a measurable geometric analysis perspective, we define the concept of\nthe Reasoning Manifold, a latent low-dimensional geometric structure formed by\nthe internal representations corresponding to all correctly reasoned\ngenerations. This structure can be conceptualized as the embodiment of the\neffective thinking paths that the model has learned to successfully solve a\ngiven task. Based on this concept, we build REMA, a framework that explains the\norigins of failures by quantitatively comparing the spatial relationships of\ninternal model representations corresponding to both erroneous and correct\nreasoning samples. Specifically, REMA first quantifies the geometric deviation\nof each erroneous representation by calculating its k-nearest neighbors\ndistance to the approximated manifold formed by correct representations,\nthereby providing a unified failure signal. It then localizes the divergence\npoints where these deviations first become significant by tracking this\ndeviation metric across the model's layers and comparing it against a baseline\nof internal fluctuations from correct representations, thus identifying where\nthe reasoning chain begins to go off-track. Our extensive experiments on\ndiverse language and multimodal models and tasks demonstrate the\nlow-dimensional nature of the reasoning manifold and the high separability\nbetween erroneous and correct reasoning representations. The results also\nvalidate the effectiveness of the REMA framework in analyzing the origins of\nreasoning failures. This research connects abstract reasoning failures to\nmeasurable geometric deviations in representations, providing new avenues for\nin-depth understanding and diagnosis of the internal computational processes of\nblack-box models.",
      "pdf_url": "http://arxiv.org/pdf/2509.22518v1",
      "published": "2025-09-26T16:02:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22518v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent and Explainable Digital Assessments",
      "authors": [
        "Rakesh Thakur",
        "Shivaansh Kaushik",
        "Gauri Chopra",
        "Harsh Rohilla"
      ],
      "abstract": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment.",
      "pdf_url": "http://arxiv.org/pdf/2509.22516v1",
      "published": "2025-09-26T16:00:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22516v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory",
      "authors": [
        "Yunhao Yuan",
        "Jiaxun Zhang",
        "Talayeh Aledavood",
        "Renwen Zhang",
        "Koustuv Saha"
      ],
      "abstract": "AI-powered companion chatbots (AICCs) such as Replika are increasingly\npopular, offering empathetic interactions, yet their psychosocial impacts\nremain unclear. We examined how engaging with AICCs shaped wellbeing and how\nusers perceived these experiences. First, we conducted a large-scale\nquasi-experimental study of longitudinal Reddit data, applying stratified\npropensity score matching and Difference-in-Differences regression. Findings\nrevealed mixed effects -- greater affective and grief expression, readability,\nand interpersonal focus, alongside increases in language about loneliness and\nsuicidal ideation. Second, we complemented these results with 15\nsemi-structured interviews, which we thematically analyzed and contextualized\nusing Knapp's relationship development model. We identified trajectories of\ninitiation, escalation, and bonding, wherein AICCs provided emotional\nvalidation and social rehearsal but also carried risks of over-reliance and\nwithdrawal. Triangulating across methods, we offer design implications for AI\ncompanions that scaffold healthy boundaries, support mindful engagement,\nsupport disclosure without dependency, and surface relationship stages --\nmaximizing psychosocial benefits while mitigating risks.",
      "pdf_url": "http://arxiv.org/pdf/2509.22505v1",
      "published": "2025-09-26T15:47:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22505v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "stat.AP"
      ]
    },
    {
      "title": "Estimating the Empowerment of Language Model Agents",
      "authors": [
        "Jinyeop Song",
        "Jeff Gore",
        "Max Kleiman-Weiner"
      ],
      "abstract": "As language model (LM) agents become more capable and gain broader access to\nreal-world tools, there is a growing need for scalable evaluation frameworks of\nagentic capability. However, conventional benchmark-centric evaluations are\ncostly to design and require human designers to come up with valid tasks that\ntranslate into insights about general model capabilities. In this work, we\npropose information-theoretic evaluation based on empowerment, the mutual\ninformation between an agent's actions and future states, as an open-ended\nmethod for evaluating LM agents. We introduce EELMA (Estimating Empowerment of\nLanguage Model Agents), an algorithm for approximating effective empowerment\nfrom multi-turn text interactions. We validate EELMA on both language games and\nscaled-up realistic web-browsing scenarios. We find that empowerment strongly\ncorrelates with average task performance, characterize the impact of\nenvironmental complexity and agentic factors such as chain-of-thought, model\nscale, and memory length on estimated empowerment, and that high empowerment\nstates and actions are often pivotal moments for general capabilities.\nTogether, these results demonstrate empowerment as an appealing general-purpose\nmetric for evaluating and monitoring LM agents in complex, open-ended settings.",
      "pdf_url": "http://arxiv.org/pdf/2509.22504v1",
      "published": "2025-09-26T15:46:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22504v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios",
      "authors": [
        "Chenglin Yu",
        "Yang Yu",
        "Songmiao Wang",
        "Yucheng Wang",
        "Yifan Yang",
        "Jinjia Li",
        "Ming Li",
        "Hongxia Yang"
      ],
      "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin organizing and executing complex tasks, and many such agents are now widely\nused in various application scenarios. However, developing these agents\nrequires carefully designed workflows, carefully crafted prompts, and iterative\ntuning, which requires LLM techniques and domain-specific expertise. These\nhand-crafted limitations hinder the scalability and cost-effectiveness of LLM\nagents across a wide range of industries. To address these challenges, we\npropose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that\ncan be applied to \\textbf{infi}nite scenarios, which introduces several key\ninnovations: a generalized \"agent-as-a-tool\" mechanism that automatically\ndecomposes complex agents into hierarchical multi-agent systems; a dual-audit\nmechanism that ensures the quality and stability of task completion; an agent\nrouting function that enables efficient task-agent matching; and an agent\nself-evolution mechanism that autonomously restructures the agent DAG based on\nnew tasks, poor performance, or optimization opportunities. Furthermore,\nInfiAgent's atomic task design supports agent parallelism, significantly\nimproving execution efficiency. This framework evolves into a versatile\npyramid-like multi-agent system capable of solving a wide range of problems.\nEvaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\%\nhigher performance compared to ADAS (similar auto-generated agent framework),\nwhile a case study of the AI research assistant InfiHelper shows that it\ngenerates scientific papers that have received recognition from human reviewers\nat top-tier IEEE conferences.",
      "pdf_url": "http://arxiv.org/pdf/2509.22502v1",
      "published": "2025-09-26T15:44:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22502v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Ontological foundations for contrastive explanatory narration of robot plans",
      "authors": [
        "Alberto Olivares-Alarcos",
        "Sergi Foix",
        "Júlia Borràs",
        "Gerard Canal",
        "Guillem Alenyà"
      ],
      "abstract": "Mutual understanding of artificial agents' decisions is key to ensuring a\ntrustworthy and successful human-robot interaction. Hence, robots are expected\nto make reasonable decisions and communicate them to humans when needed. In\nthis article, the focus is on an approach to modeling and reasoning about the\ncomparison of two competing plans, so that robots can later explain the\ndivergent result. First, a novel ontological model is proposed to formalize and\nreason about the differences between competing plans, enabling the\nclassification of the most appropriate one (e.g., the shortest, the safest, the\nclosest to human preferences, etc.). This work also investigates the\nlimitations of a baseline algorithm for ontology-based explanatory narration.\nTo address these limitations, a novel algorithm is presented, leveraging\ndivergent knowledge between plans and facilitating the construction of\ncontrastive narratives. Through empirical evaluation, it is observed that the\nexplanations excel beyond the baseline method.",
      "pdf_url": "http://arxiv.org/pdf/2509.22493v1",
      "published": "2025-09-26T15:37:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22493v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.IR",
        "cs.LO"
      ]
    },
    {
      "title": "A Machine Learning Pipeline for Multiple Sclerosis Biomarker Discovery: Comparing explainable AI and Traditional Statistical Approaches",
      "authors": [
        "Samuele Punzo",
        "Silvia Giulia Galfrè",
        "Francesco Massafra",
        "Alessandro Maglione",
        "Corrado Priami",
        "Alina Sîrbu"
      ],
      "abstract": "We present a machine learning pipeline for biomarker discovery in Multiple\nSclerosis (MS), integrating eight publicly available microarray datasets from\nPeripheral Blood Mononuclear Cells (PBMC). After robust preprocessing we\ntrained an XGBoost classifier optimized via Bayesian search. SHapley Additive\nexPlanations (SHAP) were used to identify key features for model prediction,\nindicating thus possible biomarkers. These were compared with genes identified\nthrough classical Differential Expression Analysis (DEA). Our comparison\nrevealed both overlapping and unique biomarkers between SHAP and DEA,\nsuggesting complementary strengths. Enrichment analysis confirmed the\nbiological relevance of SHAP-selected genes, linking them to pathways such as\nsphingolipid signaling, Th1/Th2/Th17 cell differentiation, and Epstein-Barr\nvirus infection all known to be associated with MS. This study highlights the\nvalue of combining explainable AI (xAI) with traditional statistical methods to\ngain deeper insights into disease mechanism.",
      "pdf_url": "http://arxiv.org/pdf/2509.22484v1",
      "published": "2025-09-26T15:31:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22484v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "OFMU: Optimization-Driven Framework for Machine Unlearning",
      "authors": [
        "Sadia Asif",
        "Mohammad Mohammadi Amiri"
      ],
      "abstract": "Large language models deployed in sensitive applications increasingly require\nthe ability to unlearn specific knowledge, such as user requests, copyrighted\nmaterials, or outdated information, without retraining from scratch to ensure\nregulatory compliance, user privacy, and safety. This task, known as machine\nunlearning, aims to remove the influence of targeted data (forgetting) while\nmaintaining performance on the remaining data (retention). A common approach is\nto formulate this as a multi-objective problem and reduce it to a\nsingle-objective problem via scalarization, where forgetting and retention\nlosses are combined using a weighted sum. However, this often results in\nunstable training dynamics and degraded model utility due to conflicting\ngradient directions. To address these challenges, we propose OFMU, a\npenalty-based bi-level optimization framework that explicitly prioritizes\nforgetting while preserving retention through a hierarchical structure. Our\nmethod enforces forgetting via an inner maximization step that incorporates a\nsimilarity-aware penalty to decorrelate the gradients of the forget and\nretention objectives, and restores utility through an outer minimization step.\nTo ensure scalability, we develop a two-loop algorithm with provable\nconvergence guarantees under both convex and non-convex regimes. We further\nprovide a rigorous theoretical analysis of convergence rates and show that our\napproach achieves better trade-offs between forgetting efficacy and model\nutility compared to prior methods. Extensive experiments across vision and\nlanguage benchmarks demonstrate that OFMU consistently outperforms existing\nunlearning methods in both forgetting efficacy and retained utility.",
      "pdf_url": "http://arxiv.org/pdf/2509.22483v1",
      "published": "2025-09-26T15:31:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22483v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6; I.2.7"
      ]
    },
    {
      "title": "Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving",
      "authors": [
        "Hang Li",
        "Kaiqi Yang",
        "Yucheng Chu",
        "Hui Liu",
        "Jiliang Tang"
      ],
      "abstract": "Large language models (LLMs) have been widely used for problem-solving tasks.\nMost recent work improves their performance through supervised fine-tuning\n(SFT) with labeled data or reinforcement learning (RL) from task feedback. In\nthis paper, we study a new perspective: the divergence in solutions generated\nby LLMs for a single problem. We show that higher solution divergence is\npositively related to better problem-solving abilities across various models.\nBased on this finding, we propose solution divergence as a novel metric that\ncan support both SFT and RL strategies. We test this idea on three\nrepresentative problem domains and find that using solution divergence\nconsistently improves success rates. These results suggest that solution\ndivergence is a simple but effective tool for advancing LLM training and\nevaluation.",
      "pdf_url": "http://arxiv.org/pdf/2509.22480v1",
      "published": "2025-09-26T15:27:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22480v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning",
      "authors": [
        "Antreas Ioannou",
        "Andreas Shiamishis",
        "Nora Hollenstein",
        "Nezihe Merve Gürel"
      ],
      "abstract": "In an era dominated by Large Language Models (LLMs), understanding their\ncapabilities and limitations, especially in high-stakes fields like law, is\ncrucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,\nDeepSeek, and other emerging models are increasingly integrated into legal\nworkflows, their performance in multilingual, jurisdictionally diverse, and\nadversarial contexts remains insufficiently explored. This work evaluates LLaMA\nand Gemini on multilingual legal and non-legal benchmarks, and assesses their\nadversarial robustness in legal tasks through character and word-level\nperturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.\nWe moreover present an open-source, modular evaluation pipeline designed to\nsupport multilingual, task-diverse benchmarking of any combination of LLMs and\ndatasets, with a particular focus on legal tasks, including classification,\nsummarization, open questions, and general reasoning. Our findings confirm that\nlegal tasks pose significant challenges for LLMs with accuracies often below\n50% on legal reasoning benchmarks such as LEXam, compared to over 70% on\ngeneral-purpose tasks like XNLI. In addition, while English generally yields\nmore stable results, it does not always lead to higher accuracy. Prompt\nsensitivity and adversarial vulnerability is also shown to persist across\nlanguages. Finally, a correlation is found between the performance of a\nlanguage and its syntactic similarity to English. We also observe that LLaMA is\nweaker than Gemini, with the latter showing an average advantage of about 24\npercentage points across the same task. Despite improvements in newer LLMs,\nchallenges remain in deploying them reliably for critical, multilingual legal\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2509.22472v1",
      "published": "2025-09-26T15:19:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22472v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining",
      "authors": [
        "Boshra Ariguib",
        "Mathias Niepert",
        "Andrei Manolache"
      ],
      "abstract": "High-quality molecular representations are essential for property prediction\nand molecular design, yet large labeled datasets remain scarce. While\nself-supervised pretraining on molecular graphs has shown promise, many\nexisting approaches either depend on hand-crafted augmentations or complex\ngenerative objectives, and often rely solely on 2D topology, leaving valuable\n3D structural information underutilized. To address this gap, we introduce\nC-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework\nthat integrates 2D graphs with ensembles of 3D conformers. C-FREE learns\nmolecular representations by predicting subgraph embeddings from their\ncomplementary neighborhoods in the latent space, using fixed-radius ego-nets as\nmodeling units across different conformers. This design allows us to integrate\nboth geometric and topological information within a hybrid Graph Neural Network\n(GNN)-Transformer backbone, without negatives, positional encodings, or\nexpensive pre-processing. Pretraining on the GEOM dataset, which provides rich\n3D conformational diversity, C-FREE achieves state-of-the-art results on\nMoleculeNet, surpassing contrastive, generative, and other multimodal\nself-supervised methods. Fine-tuning across datasets with diverse sizes and\nmolecule types further demonstrates that pretraining transfers effectively to\nnew chemical domains, highlighting the importance of 3D-informed molecular\nrepresentations.",
      "pdf_url": "http://arxiv.org/pdf/2509.22468v1",
      "published": "2025-09-26T15:16:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22468v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark",
      "authors": [
        "Hui Li",
        "Changhao Jiang",
        "Hongyu Wang",
        "Ming Zhang",
        "Jiajun Sun",
        "Zhixiong Yang",
        "Yifei Cao",
        "Shihan Dou",
        "Xiaoran Fan",
        "Baoyu Fan",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "The ability to reason from audio, including speech, paralinguistic cues,\nenvironmental sounds, and music, is essential for AI agents to interact\neffectively in real-world scenarios. Existing benchmarks mainly focus on static\nor single-scene settings and do not fully capture scenarios where multiple\nspeakers, unfolding events, and heterogeneous audio sources interact. To\naddress these challenges, we introduce MDAR, a benchmark for evaluating models\non complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR\ncomprises 3,000 carefully curated question-answer pairs linked to diverse audio\nclips, covering five categories of complex reasoning and spanning three\nquestion types. We benchmark 26 state-of-the-art audio language models on MDAR\nand observe that they exhibit limitations in complex reasoning tasks. On\nsingle-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy,\nwhereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio\nsubstantially outperforms Qwen2.5-Omni on the more challenging multiple-choice\nand open-ended tasks. Across all three question types, no model achieves 80%\nperformance. These findings underscore the unique challenges posed by MDAR and\nits value as a benchmark for advancing audio reasoning research.Code and\nbenchmark can be found at https://github.com/luckyerr/MDAR.",
      "pdf_url": "http://arxiv.org/pdf/2509.22461v1",
      "published": "2025-09-26T15:12:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22461v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ]
    },
    {
      "title": "GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation",
      "authors": [
        "Shichao Weng",
        "Zhiqiang Wang",
        "Yuhua Zhou",
        "Rui Lu",
        "Ting Liu",
        "Zhiyang Teng",
        "Xiaozhang Liu",
        "Hanmeng Liu"
      ],
      "abstract": "Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large\nLanguage Models (MLLMs), requiring not only the joint interpretation of text\nand diagrams but also iterative visuospatial reasoning. While existing\napproaches process diagrams as static images, they lack the capacity for\ndynamic manipulation - a core aspect of human geometric reasoning involving\nauxiliary line construction and affine transformations. We present GeoSketch, a\nneural-symbolic framework that recasts geometric reasoning as an interactive\nperception-reasoning-action loop. GeoSketch integrates: (1) a Perception module\nthat abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning\nmodule that applies geometric theorems to decide the next deductive step, and\n(3) a Sketch Action module that executes operations such as drawing auxiliary\nlines or applying transformations, thereby updating the diagram in a closed\nloop. To train this agent, we develop a two-stage pipeline: supervised\nfine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement\nlearning with dense, symbolic rewards to enhance robustness and strategic\nexploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a\nhigh-quality set of 390 geometry problems requiring auxiliary construction or\naffine transformations. Experiments on strong MLLM baselines demonstrate that\nGeoSketch significantly improves stepwise reasoning accuracy and\nproblem-solving success over static perception methods. By unifying\nhierarchical decision-making, executable visual actions, and symbolic\nverification, GeoSketch advances multimodal reasoning from static\ninterpretation to dynamic, verifiable interaction, establishing a new\nfoundation for solving complex visuospatial problems.",
      "pdf_url": "http://arxiv.org/pdf/2509.22460v1",
      "published": "2025-09-26T15:12:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22460v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Physics-informed GNN for medium-high voltage AC power flow with edge-aware attention and line search correction operator",
      "authors": [
        "Changhun Kim",
        "Timon Conrad",
        "Redwanul Karim",
        "Julian Oelhaf",
        "David Riebesel",
        "Tomás Arias-Vergara",
        "Andreas Maier",
        "Johann Jäger",
        "Siming Bayer"
      ],
      "abstract": "Physics-informed graph neural networks (PIGNNs) have emerged as fast AC\npower-flow solvers that can replace classic Newton--Raphson (NR) solvers,\nespecially when thousands of scenarios must be evaluated. However, current\nPIGNNs still need accuracy improvements at parity speed; in particular, the\nphysics loss is inoperative at inference, which can deter operational adoption.\nWe address this with PIGNN-Attn-LS, combining an edge-aware attention mechanism\nthat explicitly encodes line physics via per-edge biases, capturing the grid's\nanisotropy, with a backtracking line-search-based globalized correction\noperator that restores an operative decrease criterion at inference. Training\nand testing use a realistic High-/Medium-Voltage scenario generator, with NR\nused only to construct reference states. On held-out HV cases consisting of\n4--32-bus grids, PIGNN-Attn-LS achieves a test RMSE of 0.00033 p.u. in voltage\nand 0.08$^\\circ$ in angle, outperforming the PIGNN-MLP baseline by 99.5\\% and\n87.1\\%, respectively. With streaming micro-batches, it delivers 2--5$\\times$\nfaster batched inference than NR on 4--1024-bus grids.",
      "pdf_url": "http://arxiv.org/pdf/2509.22458v1",
      "published": "2025-09-26T15:09:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22458v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Guiding Evolution of Artificial Life Using Vision-Language Models",
      "authors": [
        "Nikhil Baid",
        "Hannah Erlebach",
        "Paul Hellegouarch",
        "Frederico Wieser"
      ],
      "abstract": "Foundation models (FMs) have recently opened up new frontiers in the field of\nartificial life (ALife) by providing powerful tools to automate search through\nALife simulations. Previous work aligns ALife simulations with natural language\ntarget prompts using vision-language models (VLMs). We build on Automated\nSearch for Artificial Life (ASAL) by introducing ASAL++, a method for\nopen-ended-like search guided by multimodal FMs. We use a second FM to propose\nnew evolutionary targets based on a simulation's visual history. This induces\nan evolutionary trajectory with increasingly complex targets.\n  We explore two strategies: (1) evolving a simulation to match a single new\nprompt at each iteration (Evolved Supervised Targets: EST) and (2) evolving a\nsimulation to match the entire sequence of generated prompts (Evolved Temporal\nTargets: ETT). We test our method empirically in the Lenia substrate using\nGemma-3 to propose evolutionary targets, and show that EST promotes greater\nvisual novelty, while ETT fosters more coherent and interpretable evolutionary\nsequences.\n  Our results suggest that ASAL++ points towards new directions for FM-driven\nALife discovery with open-ended characteristics.",
      "pdf_url": "http://arxiv.org/pdf/2509.22447v1",
      "published": "2025-09-26T15:03:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22447v1",
      "categories": [
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers",
      "authors": [
        "Peter Shaw",
        "James Cohan",
        "Jacob Eisenstein",
        "Kristina Toutanova"
      ],
      "abstract": "The Minimum Description Length (MDL) principle offers a formal framework for\napplying Occam's razor in machine learning. However, its application to neural\nnetworks such as Transformers is challenging due to the lack of a principled,\nuniversal measure for model complexity. This paper introduces the theoretical\nnotion of asymptotically optimal description length objectives, grounded in the\ntheory of Kolmogorov complexity. We establish that a minimizer of such an\nobjective achieves optimal compression, for any dataset, up to an additive\nconstant, in the limit as model resource bounds increase. We prove that\nasymptotically optimal objectives exist for Transformers, building on a new\ndemonstration of their computational universality. We further show that such\nobjectives can be tractable and differentiable by constructing and analyzing a\nvariational objective based on an adaptive Gaussian mixture prior. Our\nempirical analysis shows that this variational objective selects for a\nlow-complexity solution with strong generalization on an algorithmic task, but\nstandard optimizers fail to find such solutions from a random initialization,\nhighlighting key optimization challenges. More broadly, by providing a\ntheoretical framework for identifying description length objectives with strong\nasymptotic guarantees, we outline a potential path towards training neural\nnetworks that achieve greater compression and generalization.",
      "pdf_url": "http://arxiv.org/pdf/2509.22445v1",
      "published": "2025-09-26T15:02:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22445v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Learning to Ball: Composing Policies for Long-Horizon Basketball Moves",
      "authors": [
        "Pei Xu",
        "Zhen Wu",
        "Ruocheng Wang",
        "Vishnu Sarukkai",
        "Kayvon Fatahalian",
        "Ioannis Karamouzas",
        "Victor Zordan",
        "C. Karen Liu"
      ],
      "abstract": "Learning a control policy for a multi-phase, long-horizon task, such as\nbasketball maneuvers, remains challenging for reinforcement learning approaches\ndue to the need for seamless policy composition and transitions between skills.\nA long-horizon task typically consists of distinct subtasks with well-defined\ngoals, separated by transitional subtasks with unclear goals but critical to\nthe success of the entire task. Existing methods like the mixture of experts\nand skill chaining struggle with tasks where individual policies do not share\nsignificant commonly explored states or lack well-defined initial and terminal\nstates between different phases. In this paper, we introduce a novel policy\nintegration framework to enable the composition of drastically different motor\nskills in multi-phase long-horizon tasks with ill-defined intermediate states.\nBased on that, we further introduce a high-level soft router to enable seamless\nand robust transitions between the subtasks. We evaluate our framework on a set\nof fundamental basketball skills and challenging transitions. Policies trained\nby our approach can effectively control the simulated character to interact\nwith the ball and accomplish the long-horizon task specified by real-time user\ncommands, without relying on ball trajectory references.",
      "pdf_url": "http://arxiv.org/pdf/2509.22442v1",
      "published": "2025-09-26T15:02:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22442v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding",
      "authors": [
        "Ziheng Chi",
        "Yifan Hou",
        "Chenxi Pang",
        "Shaobo Cui",
        "Mubashara Akhtar",
        "Mrinmaya Sachan"
      ],
      "abstract": "Diagrams convey symbolic information in a visual format rather than a linear\nstream of words, making them especially challenging for AI models to process.\nWhile recent evaluations suggest that vision-language models (VLMs) perform\nwell on diagram-related benchmarks, their reliance on knowledge, reasoning, or\nmodality shortcuts raises concerns about whether they genuinely understand and\nreason over diagrams. To address this gap, we introduce Chimera, a\ncomprehensive test suite comprising 7,500 high-quality diagrams sourced from\nWikipedia; each diagram is annotated with its symbolic content represented by\nsemantic triples along with multi-level questions designed to assess four\nfundamental aspects of diagram comprehension: entity recognition, relation\nunderstanding, knowledge grounding, and visual reasoning. We use Chimera to\nmeasure the presence of three types of shortcuts in visual question answering:\n(1) the visual-memorization shortcut, where VLMs rely on memorized visual\npatterns; (2) the knowledge-recall shortcut, where models leverage memorized\nfactual knowledge instead of interpreting the diagram; and (3) the Clever-Hans\nshortcut, where models exploit superficial language patterns or priors without\ntrue comprehension. We evaluate 15 open-source VLMs from 7 model families on\nChimera and find that their seemingly strong performance largely stems from\nshortcut behaviors: visual-memorization shortcuts have slight impact,\nknowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts\ncontribute significantly. These findings expose critical limitations in current\nVLMs and underscore the need for more robust evaluation protocols that\nbenchmark genuine comprehension of complex visual inputs (e.g., diagrams)\nrather than question-answering shortcuts.",
      "pdf_url": "http://arxiv.org/pdf/2509.22437v1",
      "published": "2025-09-26T14:55:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22437v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Global Convergence in Neural ODEs: Impact of Activation Functions",
      "authors": [
        "Tianxiang Gao",
        "Siyuan Sun",
        "Hailiang Liu",
        "Hongyang Gao"
      ],
      "abstract": "Neural Ordinary Differential Equations (ODEs) have been successful in various\napplications due to their continuous nature and parameter-sharing efficiency.\nHowever, these unique characteristics also introduce challenges in training,\nparticularly with respect to gradient computation accuracy and convergence\nanalysis. In this paper, we address these challenges by investigating the\nimpact of activation functions. We demonstrate that the properties of\nactivation functions, specifically smoothness and nonlinearity, are critical to\nthe training dynamics. Smooth activation functions guarantee globally unique\nsolutions for both forward and backward ODEs, while sufficient nonlinearity is\nessential for maintaining the spectral properties of the Neural Tangent Kernel\n(NTK) during training. Together, these properties enable us to establish the\nglobal convergence of Neural ODEs under gradient descent in overparameterized\nregimes. Our theoretical findings are validated by numerical experiments, which\nnot only support our analysis but also provide practical guidelines for scaling\nNeural ODEs, potentially leading to faster training and improved performance in\nreal-world applications.",
      "pdf_url": "http://arxiv.org/pdf/2509.22436v1",
      "published": "2025-09-26T14:54:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22436v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics",
      "authors": [
        "Margherita Martorana",
        "Francesca Urgese",
        "Ilaria Tiddi",
        "Stefan Schlobach"
      ],
      "abstract": "Personal service robots are increasingly used in domestic settings to assist\nolder adults and people requiring support. Effective operation involves not\nonly physical interaction but also the ability to interpret dynamic\nenvironments, understand tasks, and choose appropriate actions based on\ncontext. This requires integrating both hardware components (e.g. sensors,\nactuators) and software systems capable of reasoning about tasks, environments,\nand robot capabilities. Frameworks such as the Robot Operating System (ROS)\nprovide open-source tools that help connect low-level hardware with\nhigher-level functionalities. However, real-world deployments remain tightly\ncoupled to specific platforms. As a result, solutions are often isolated and\nhard-coded, limiting interoperability, reusability, and knowledge sharing.\nOntologies and knowledge graphs offer a structured way to represent tasks,\nenvironments, and robot capabilities. Existing ontologies, such as the\nSocio-physical Model of Activities (SOMA) and the Descriptive Ontology for\nLinguistic and Cognitive Engineering (DOLCE), provide models for activities,\nspatial relationships, and reasoning structures. However, they often focus on\nspecific domains and do not fully capture the connection between environment,\naction, robot capabilities, and system-level integration. In this work, we\npropose the Ontology for roBOts and acTions (OntoBOT), which extends existing\nontologies to provide a unified representation of tasks, actions, environments,\nand capabilities. Our contributions are twofold: (1) we unify these aspects\ninto a cohesive ontology to support formal reasoning about task execution, and\n(2) we demonstrate its generalizability by evaluating competency questions\nacross four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how\nOntoBOT enables context-aware reasoning, task-oriented execution, and knowledge\nsharing in service robotics.",
      "pdf_url": "http://arxiv.org/pdf/2509.22434v1",
      "published": "2025-09-26T14:53:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22434v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Partial Parameter Updates for Efficient Distributed Training",
      "authors": [
        "Anastasiia Filippova",
        "Angelos Katharopoulos",
        "David Grangier",
        "Ronan Collobert"
      ],
      "abstract": "We introduce a memory- and compute-efficient method for low-communication\ndistributed training. Existing methods reduce communication by performing\nmultiple local updates between infrequent global synchronizations. We\ndemonstrate that their efficiency can be significantly improved by restricting\nbackpropagation: instead of updating all the parameters, each node updates only\na fixed subset while keeping the remainder frozen during local steps. This\nconstraint substantially reduces peak memory usage and training FLOPs, while a\nfull forward pass over all parameters eliminates the need for cross-node\nactivation exchange. Experiments on a $1.3$B-parameter language model trained\nacross $32$ nodes show that our method matches the perplexity of prior\nlow-communication approaches under identical token and bandwidth budgets while\nreducing training FLOPs and peak memory.",
      "pdf_url": "http://arxiv.org/pdf/2509.22418v1",
      "published": "2025-09-26T14:39:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.22418v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}