{
  "last_updated": "2025-12-04T00:53:50.654158",
  "papers": [
    {
      "title": "PPTArena: A Benchmark for Agentic PowerPoint Editing",
      "authors": [
        "Michael Ofengenden",
        "Yunze Man",
        "Ziqi Pang",
        "Yu-Xiong Wang"
      ],
      "abstract": "We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.",
      "pdf_url": "https://arxiv.org/pdf/2512.03042v1",
      "published": "2025-12-02T18:59:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03042v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
      "authors": [
        "Zeqi Xiao",
        "Yiwei Zhao",
        "Lingxiao Li",
        "Yushi Lan",
        "Yu Ning",
        "Rahul Garg",
        "Roshni Cooper",
        "Mohammad H. Taghavi",
        "Xingang Pan"
      ],
      "abstract": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2512.03040v1",
      "published": "2025-12-02T18:59:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03040v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
      "authors": [
        "Mengchen Zhang",
        "Qi Chen",
        "Tong Wu",
        "Zihan Liu",
        "Dahua Lin"
      ],
      "abstract": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
      "pdf_url": "https://arxiv.org/pdf/2512.03036v1",
      "published": "2025-12-02T18:56:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03036v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control",
      "authors": [
        "Yuxuan Mu",
        "Ziyu Zhang",
        "Yi Shi",
        "Minami Matsumoto",
        "Kotaro Imamura",
        "Guy Tevet",
        "Chuan Guo",
        "Michael Taylor",
        "Chang Shu",
        "Pengcheng Xi",
        "Xue Bin Peng"
      ],
      "abstract": "Data-driven motion priors that can guide agents toward producing naturalistic behaviors play a pivotal role in creating life-like virtual characters. Adversarial imitation learning has been a highly effective method for learning motion priors from reference motion data. However, adversarial priors, with few exceptions, need to be retrained for each new controller, thereby limiting their reusability and necessitating the retention of the reference motion data when training on downstream tasks. In this work, we present Score-Matching Motion Priors (SMP), which leverages pre-trained motion diffusion models and score distillation sampling (SDS) to create reusable task-agnostic motion priors. SMPs can be pre-trained on a motion dataset, independent of any control policy or task. Once trained, SMPs can be kept frozen and reused as general-purpose reward functions to train policies to produce naturalistic behaviors for downstream tasks. We show that a general motion prior trained on large-scale datasets can be repurposed into a variety of style-specific priors. Furthermore SMP can compose different styles to synthesize new styles not present in the original dataset. Our method produces high-quality motion comparable to state-of-the-art adversarial imitation learning methods through reusable and modular motion priors. We demonstrate the effectiveness of SMP across a diverse suite of control tasks with physically simulated humanoid characters. Video demo available at https://youtu.be/ravlZJteS20",
      "pdf_url": "https://arxiv.org/pdf/2512.03028v1",
      "published": "2025-12-02T18:54:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03028v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models",
      "authors": [
        "Saeid Jamshidi",
        "Kawser Wazed Nafi",
        "Arghavan Moradi Dakhel",
        "Negar Shahabi",
        "Foutse Khomh"
      ],
      "abstract": "The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.03026v1",
      "published": "2025-12-02T18:52:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03026v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LORE: A Large Generative Model for Search Relevance",
      "authors": [
        "Chenji Lu",
        "Zhuo Chen",
        "Hui Zhao",
        "Zhiyuan Zeng",
        "Gang Zhao",
        "Junjie Ren",
        "Ruicong Xu",
        "Haoran Li",
        "Songyan Liu",
        "Pengjie Wang",
        "Jian Xu",
        "Bo Zheng"
      ],
      "abstract": "Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.",
      "pdf_url": "https://arxiv.org/pdf/2512.03025v1",
      "published": "2025-12-02T18:50:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03025v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "TokenPowerBench: Benchmarking the Power Consumption of LLM Inference",
      "authors": [
        "Chenxu Niu",
        "Wei Zhang",
        "Jie Li",
        "Yongjian Zhao",
        "Tongyang Wang",
        "Xi Wang",
        "Yong Chen"
      ],
      "abstract": "Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.",
      "pdf_url": "https://arxiv.org/pdf/2512.03024v1",
      "published": "2025-12-02T18:50:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03024v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.DC"
      ]
    },
    {
      "title": "Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge",
      "authors": [
        "Hamid Dadkhahi",
        "Firas Trabelsi",
        "Parker Riley",
        "Juraj Juraska",
        "Mehdi Mirzazadeh"
      ],
      "abstract": "Thinking Large Language Models (LLMs) used as judges for pairwise preferences remain noisy at the single-sample level, and common aggregation rules (majority vote, soft self-consistency, or instruction-based self-aggregation) are inconsistent when ties are allowed. We study inference-time compute (ITC) for evaluators that generate n independent thinking-rating samples per item, and propose a principled, distribution-calibrated aggregation scheme. Our method models three-way preferences with a Bradley-Terry-Davidson formulation on rating counts, leveraging both polarity (margin among non-ties) and decisiveness (non-tie rate) to distinguish narrow margins from strong consensus. Across various evaluation benchmarks, our approach consistently reduces MAE and increases pairwise accuracy versus standard baselines, and when evaluated against human-consensus meta-labels, matches or exceeds individual human raters. These results show that carefully allocating ITC and aggregating with distribution-aware methods turns noisy individual model judgments into reliable ratings for evaluation.",
      "pdf_url": "https://arxiv.org/pdf/2512.03019v1",
      "published": "2025-12-02T18:46:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03019v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "In-Context Sync-LoRA for Portrait Video Editing",
      "authors": [
        "Sagi Polaczek",
        "Or Patashnik",
        "Ali Mahdavi-Amiri",
        "Daniel Cohen-Or"
      ],
      "abstract": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
      "pdf_url": "https://arxiv.org/pdf/2512.03013v1",
      "published": "2025-12-02T18:40:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03013v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?",
      "authors": [
        "Dawei Li",
        "Abdullah Alnaibari",
        "Arslan Bisharat",
        "Manny Sandoval",
        "Deborah Hall",
        "Yasin Silva",
        "Huan Liu"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.",
      "pdf_url": "https://arxiv.org/pdf/2512.03005v1",
      "published": "2025-12-02T18:31:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03005v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Invasive Context Engineering to Control Large Language Models",
      "authors": [
        "Thomas Rivasseau"
      ],
      "abstract": "Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.",
      "pdf_url": "https://arxiv.org/pdf/2512.03001v1",
      "published": "2025-12-02T18:25:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.03001v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic",
      "authors": [
        "Muyu Pan",
        "Dheeraj Kodakandla",
        "Mahfuza Farooque"
      ],
      "abstract": "Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.",
      "pdf_url": "https://arxiv.org/pdf/2512.02987v1",
      "published": "2025-12-02T18:03:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02987v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding",
      "authors": [
        "Paul Barbaste",
        "Olivier Oullier",
        "Xavier Vasques"
      ],
      "abstract": "Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.",
      "pdf_url": "https://arxiv.org/pdf/2512.02978v1",
      "published": "2025-12-02T17:56:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02978v1",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Lumos: Let there be Language Model System Certification",
      "authors": [
        "Isha Chaudhary",
        "Vedaant Jain",
        "Avaljot Singh",
        "Kavya Sachdeva",
        "Sayan Ranu",
        "Gagandeep Singh"
      ],
      "abstract": "We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.",
      "pdf_url": "https://arxiv.org/pdf/2512.02966v1",
      "published": "2025-12-02T17:44:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02966v1",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
      "authors": [
        "Lanxiang Hu",
        "Abhilash Shankarampeta",
        "Yixin Huang",
        "Zilin Dai",
        "Haoyang Yu",
        "Yujie Zhao",
        "Haoqiang Kang",
        "Daniel Zhao",
        "Tajana Rosing",
        "Hao Zhang"
      ],
      "abstract": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.",
      "pdf_url": "https://arxiv.org/pdf/2512.02942v1",
      "published": "2025-12-02T17:11:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02942v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis",
      "authors": [
        "Yancheng Zhang",
        "Guangyu Sun",
        "Chen Chen"
      ],
      "abstract": "Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.",
      "pdf_url": "https://arxiv.org/pdf/2512.02932v1",
      "published": "2025-12-02T17:01:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02932v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning",
      "authors": [
        "Zhonghao He",
        "Tianyi Qiu",
        "Hirokazu Shirado",
        "Maarten Sap"
      ],
      "abstract": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.",
      "pdf_url": "https://arxiv.org/pdf/2512.02914v1",
      "published": "2025-12-02T16:34:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02914v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "In Silico Development of Psychometric Scales: Feasibility of Representative Population Data Simulation with LLMs",
      "authors": [
        "Enrico Cipriani",
        "Pavel Okopnyi",
        "Danilo Menicucci",
        "Simone Grassini"
      ],
      "abstract": "Developing and validating psychometric scales requires large samples, multiple testing phases, and substantial resources. Recent advances in Large Language Models (LLMs) enable the generation of synthetic participant data by prompting models to answer items while impersonating individuals of specific demographic profiles, potentially allowing in silico piloting before real data collection. Across four preregistered studies (N = circa 300 each), we tested whether LLM-simulated datasets can reproduce the latent structures and measurement properties of human responses. In Studies 1-2, we compared LLM-generated data with real datasets for two validated scales; in Studies 3-4, we created new scales using EFA on simulated data and then examined whether these structures generalized to newly collected human samples. Simulated datasets replicated the intended factor structures in three of four studies and showed consistent configural and metric invariance, with scalar invariance achieved for the two newly developed scales. However, correlation-based tests revealed substantial differences between real and synthetic datasets, and notable discrepancies appeared in score distributions and variances. Thus, while LLMs capture group-level latent structures, they do not approximate individual-level data properties. Simulated datasets also showed full internal invariance across gender. Overall, LLM-generated data appear useful for early-stage, group-level psychometric prototyping, but not as substitutes for individual-level validation. We discuss methodological limitations, risks of bias and data pollution, and ethical considerations related to in silico psychometric simulations.",
      "pdf_url": "https://arxiv.org/pdf/2512.02910v1",
      "published": "2025-12-02T16:26:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02910v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "authors": [
        "Fan Yang",
        "Kaihao Zhang"
      ],
      "abstract": "Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.",
      "pdf_url": "https://arxiv.org/pdf/2512.02906v1",
      "published": "2025-12-02T16:22:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02906v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Towards a fully differentiable digital twin for solar cells",
      "authors": [
        "Marie Louise Schubert",
        "Houssam Metni",
        "Jan David Fischbach",
        "Benedikt Zerulla",
        "Marjan Krstić",
        "Ulrich W. Paetzold",
        "Seyedamir Orooji",
        "Olivier J. J. Ronsin",
        "Yasin Ameslon",
        "Jens Harting",
        "Thomas Kirchartz",
        "Sandheep Ravishankar",
        "Chris Dreessen",
        "Eunchi Kim",
        "Christian Sprau",
        "Mohamed Hussein",
        "Alexander Colsmann",
        "Karen Forberich",
        "Klaus Jäger",
        "Pascal Friederich",
        "Carsten Rockstuhl"
      ],
      "abstract": "Maximizing energy yield (EY) - the total electric energy generated by a solar cell within a year at a specific location - is crucial in photovoltaics (PV), especially for emerging technologies. Computational methods provide the necessary insights and guidance for future research. However, existing simulations typically focus on only isolated aspects of solar cells. This lack of consistency highlights the need for a framework unifying all computational levels, from material to cell properties, for accurate prediction and optimization of EY prediction. To address this challenge, a differentiable digital twin, Sol(Di)$^2$T, is introduced to enable comprehensive end-to-end optimization of solar cells. The workflow starts with material properties and morphological processing parameters, followed by optical and electrical simulations. Finally, climatic conditions and geographic location are incorporated to predict the EY. Each step is either intrinsically differentiable or replaced with a machine-learned surrogate model, enabling not only accurate EY prediction but also gradient-based optimization with respect to input parameters. Consequently, Sol(Di)$^2$T extends EY predictions to previously unexplored conditions. Demonstrated for an organic solar cell, the proposed framework marks a significant step towards tailoring solar cells for specific applications while ensuring maximal performance.",
      "pdf_url": "https://arxiv.org/pdf/2512.02904v1",
      "published": "2025-12-02T16:20:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02904v1",
      "categories": [
        "physics.comp-ph",
        "cs.AI"
      ]
    },
    {
      "title": "VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling",
      "authors": [
        "Weiqi Li",
        "Quande Zhang",
        "Ruifeng Zhai",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "abstract": "Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.",
      "pdf_url": "https://arxiv.org/pdf/2512.02902v1",
      "published": "2025-12-02T16:16:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02902v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization",
      "authors": [
        "Feiyu Wang",
        "Xinyu Tan",
        "Bokai Huang",
        "Yihao Zhang",
        "Guoan Wang",
        "Peizhuang Cong",
        "Tong Yang"
      ],
      "abstract": "Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.",
      "pdf_url": "https://arxiv.org/pdf/2512.02901v1",
      "published": "2025-12-02T16:14:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02901v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits",
      "authors": [
        "Pedro Orvalho",
        "Marta Kwiatkowska",
        "Mikoláš Janota",
        "Vasco Manquinho"
      ],
      "abstract": "Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.\n  This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).",
      "pdf_url": "https://arxiv.org/pdf/2512.02898v1",
      "published": "2025-12-02T16:04:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02898v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LO",
        "cs.SC"
      ]
    },
    {
      "title": "OptPO: Optimal Rollout Allocation for Test-time Policy Optimization",
      "authors": [
        "Youkang Wang",
        "Jian Wang",
        "Rubing Chen",
        "Tianyi Zeng",
        "Xiao-Yong Wei",
        "Qing Li"
      ],
      "abstract": "Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.",
      "pdf_url": "https://arxiv.org/pdf/2512.02882v1",
      "published": "2025-12-02T15:38:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02882v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "The future of AI in critical mineral exploration",
      "authors": [
        "Jef Caers"
      ],
      "abstract": "The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage",
      "pdf_url": "https://arxiv.org/pdf/2512.02879v1",
      "published": "2025-12-02T15:37:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02879v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GraphMatch: Fusing Language and Graph Representations in a Dynamic Two-Sided Work Marketplace",
      "authors": [
        "Mikołaj Sacha",
        "Hammad Jafri",
        "Mattie Terzolo",
        "Ayan Sinha",
        "Andrew Rabinovich"
      ],
      "abstract": "Recommending matches in a text-rich, dynamic two-sided marketplace presents unique challenges due to evolving content and interaction graphs. We introduce GraphMatch, a new large-scale recommendation framework that fuses pre-trained language models with graph neural networks to overcome these challenges. Unlike prior approaches centered on standalone models, GraphMatch is a comprehensive recipe built on powerful text encoders and GNNs working in tandem. It employs adversarial negative sampling alongside point-in-time subgraph training to learn representations that capture both the fine-grained semantics of evolving text and the time-sensitive structure of the graph. We evaluated extensively on interaction data from Upwork, a leading labor marketplace, at large scale, and discuss our approach towards low-latency inference suitable for real-time use. In our experiments, GraphMatch outperforms language-only and graph-only baselines on matching tasks while being efficient at runtime. These results demonstrate that unifying language and graph representations yields a highly effective solution to text-rich, dynamic two-sided recommendations, bridging the gap between powerful pretrained LMs and large-scale graphs in practice.",
      "pdf_url": "https://arxiv.org/pdf/2512.02849v1",
      "published": "2025-12-02T15:02:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02849v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages",
      "authors": [
        "Lechen Zhang",
        "Yusheng Zhou",
        "Tolga Ergen",
        "Lajanugen Logeswaran",
        "Moontae Lee",
        "David Jurgens"
      ],
      "abstract": "System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.",
      "pdf_url": "https://arxiv.org/pdf/2512.02841v1",
      "published": "2025-12-02T14:54:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02841v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
      "authors": [
        "Yifan Li",
        "Yingda Yin",
        "Lingting Zhu",
        "Weikai Chen",
        "Shengju Qian",
        "Xin Wang",
        "Yanwei Fu"
      ],
      "abstract": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
      "pdf_url": "https://arxiv.org/pdf/2512.02835v1",
      "published": "2025-12-02T14:44:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02835v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "authors": [
        "Siyuan Yang",
        "Yang Zhang",
        "Haoran He",
        "Ling Pan",
        "Xiu Li",
        "Chenjia Bai",
        "Xuelong Li"
      ],
      "abstract": "Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \\textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.",
      "pdf_url": "https://arxiv.org/pdf/2512.02834v1",
      "published": "2025-12-02T14:42:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02834v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models",
      "authors": [
        "Ihab Ahmed",
        "Denis Krompaß",
        "Cheng Feng",
        "Volker Tresp"
      ],
      "abstract": "We investigate input normalization methods for Time-Series Foundation Models (TSFMs). While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical. Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity. Through systematic evaluation across four architecturally diverse TSFMs, we empirically establish REVIN as the most efficient approach, reducing zero-shot MASE by 89\\% relative to an un-normalized baseline and by 44\\% versus other normalization methods, while matching the best in-domain accuracy (0.84 MASE) without any dataset-level preprocessing -- yielding the highest accuracy-efficiency trade-off. Yet its effect utilization depends on architectural design choices and optimization objective, particularly with respect to training loss scale sensitivity and model type (probabilistic, point-forecast, or LLM-based models).",
      "pdf_url": "https://arxiv.org/pdf/2512.02833v1",
      "published": "2025-12-02T14:39:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02833v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Defense That Attacks: How Robust Models Become Better Attackers",
      "authors": [
        "Mohamed Awad",
        "Mahmoud Akrm",
        "Walid Gomaa"
      ],
      "abstract": "Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.",
      "pdf_url": "https://arxiv.org/pdf/2512.02830v1",
      "published": "2025-12-02T14:38:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02830v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity",
      "authors": [
        "Haoming Liu",
        "Jinnuo Liu",
        "Yanhao Li",
        "Liuyang Bai",
        "Yunkai Ji",
        "Yuanhe Guo",
        "Shenji Wan",
        "Hongyi Wen"
      ],
      "abstract": "Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.",
      "pdf_url": "https://arxiv.org/pdf/2512.02826v1",
      "published": "2025-12-02T14:34:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02826v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control",
      "authors": [
        "Yongrui Yu",
        "Zhongzhen Huang",
        "Linjie Mu",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Radiology reporting is an essential yet time-consuming and error-prone task for radiologists in clinical examinations, especially for volumetric medical images. Rigorous quality control is also critical but tedious, ensuring that the final report meets clinical standards. Existing automated approaches, including radiology report generation methods and medical vision-language models, focus mainly on the report generation phase and neglect the crucial quality control procedure, limiting their capability to provide comprehensive support to radiologists. We propose Radiologist Copilot, an agentic AI assistant equipped with orchestrated tools designed for automated radiology reporting with quality control. Leveraging large language models as the reasoning backbone, the agentic system autonomously selects tools, plans, and executes actions, emulating the behavior of radiologists throughout the holistic radiology reporting process. The orchestrated tools include region localization, think with image paradigm directed region analysis planning, strategic template selection for report generation, quality assessment and feedback-driven adaptive refinement for quality control. Therefore, Radiologist Copilot facilitates accurate, complete, and efficient radiology reporting, assisting radiologists and improving clinical efficiency. Experimental results demonstrate that Radiologist Copilot significantly surpasses other state-of-the-art methods in radiology reporting. The source code will be released upon acceptance.",
      "pdf_url": "https://arxiv.org/pdf/2512.02814v1",
      "published": "2025-12-02T14:25:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02814v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents",
      "authors": [
        "Zijie Lin",
        "Qilin Cai",
        "Liang Shen",
        "Mingjun Xiao"
      ],
      "abstract": "Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\\% and 13\\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.",
      "pdf_url": "https://arxiv.org/pdf/2512.02812v1",
      "published": "2025-12-02T14:24:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02812v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms",
      "authors": [
        "Shyam prasad reddy Kaitha",
        "Hongrui Yu"
      ],
      "abstract": "Multi-robot task allocation in construction automation has traditionally relied on optimization methods such as Dynamic Programming and Reinforcement Learning. This research introduces the LangGraph-based Task Allocation Agent (LTAA), an LLM-driven framework that integrates phase-adaptive allocation strategies, multi-stage validation with hierarchical retries, and dynamic prompting for efficient robot coordination. Although recent LLM approaches show potential for construction robotics, they largely lack rigorous validation and benchmarking against established algorithms. This paper presents the first systematic comparison of LLM-based task allocation with traditional methods in construction scenarios.The study validates LLM feasibility through SMART-LLM replication and addresses implementation challenges using a Self-Corrective Agent Architecture. LTAA leverages natural-language reasoning combined with structured validation mechanisms, achieving major computational gains reducing token usage by 94.6% and allocation time by 86% through dynamic prompting. The framework adjusts its strategy across phases: emphasizing execution feasibility early and workload balance in later allocations.The authors evaluate LTAA against Dynamic Programming, Q-learning, and Deep Q-Network (DQN) baselines using construction operations from the TEACh human-robot collaboration dataset. In the Heavy Excels setting, where robots have strong task specializations, LTAA achieves 77% task completion with superior workload balance, outperforming all traditional methods. These findings show that LLM-based reasoning with structured validation can match established optimization algorithms while offering additional advantages such as interpretability, adaptability, and the ability to update task logic without retraining.",
      "pdf_url": "https://arxiv.org/pdf/2512.02810v1",
      "published": "2025-12-02T14:23:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02810v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Perception of AI-Generated Music -- The Role of Composer Identity, Personality Traits, Music Preferences, and Perceived Humanness",
      "authors": [
        "David Stammer",
        "Hannah Strauss",
        "Peter Knees"
      ],
      "abstract": "The rapid rise of AI-generated art has sparked debate about potential biases in how audiences perceive and evaluate such works. This study investigates how composer information and listener characteristics shape the perception of AI-generated music, adopting a mixed-method approach. Using a diverse set of stimuli across various genres from two AI music models, we examine effects of perceived authorship on liking and emotional responses, and explore how attitudes toward AI, personality traits, and music-related variables influence evaluations. We further assess the influence of perceived humanness and analyze open-ended responses to uncover listener criteria for judging AI-generated music. Attitudes toward AI proved to be the best predictor of both liking and emotional intensity of AI-generated music. This quantitative finding was complemented by qualitative themes from our thematic analysis, which identified ethical, cultural, and contextual considerations as important criteria in listeners' evaluations of AI-generated music. Our results offer a nuanced view of how people experience music created by AI tools and point to key factors and methodological considerations for future research on music perception in human-AI interaction.",
      "pdf_url": "https://arxiv.org/pdf/2512.02785v1",
      "published": "2025-12-02T13:59:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02785v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys",
      "authors": [
        "Jiahao Zhao",
        "Shuaixing Zhang",
        "Nan Xu",
        "Lei Wang"
      ],
      "abstract": "LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.",
      "pdf_url": "https://arxiv.org/pdf/2512.02763v1",
      "published": "2025-12-02T13:42:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02763v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Reasoning-Aware Multimodal Fusion for Hateful Video Detection",
      "authors": [
        "Shuonan Yang",
        "Tailin Chen",
        "Jiangbei Yue",
        "Guangliang Cheng",
        "Jianbo Jiao",
        "Zeyu Fu"
      ],
      "abstract": "Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.",
      "pdf_url": "https://arxiv.org/pdf/2512.02743v1",
      "published": "2025-12-02T13:24:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02743v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Framework for Causal Concept-based Model Explanations",
      "authors": [
        "Anna Rodum Bjøru",
        "Jacob Lysnæs-Larsen",
        "Oskar Jørgensen",
        "Inga Strümke",
        "Helge Langseth"
      ],
      "abstract": "This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.",
      "pdf_url": "https://arxiv.org/pdf/2512.02735v1",
      "published": "2025-12-02T13:19:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02735v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Self-Improving AI Agents through Self-Play",
      "authors": [
        "Przemyslaw Chojecki"
      ],
      "abstract": "We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow.\n  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.\n  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.02731v1",
      "published": "2025-12-02T13:13:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02731v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions",
      "authors": [
        "Yifan Zhou",
        "Takehiko Ohkawa",
        "Guwenxiao Zhou",
        "Kanoko Goto",
        "Takumi Hirose",
        "Yusuke Sekikawa",
        "Nakamasa Inoue"
      ],
      "abstract": "Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.",
      "pdf_url": "https://arxiv.org/pdf/2512.02727v1",
      "published": "2025-12-02T13:01:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02727v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping",
      "authors": [
        "Md Abdul Kadir",
        "Sai Suresh Macharla Vasu",
        "Sidharth S. Nair",
        "Daniel Sonntag"
      ],
      "abstract": "Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \\textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.",
      "pdf_url": "https://arxiv.org/pdf/2512.02726v1",
      "published": "2025-12-02T13:00:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02726v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "StockMem: An Event-Reflection Memory Framework for Stock Forecasting",
      "authors": [
        "He Wang",
        "Wenyilin Xiao",
        "Songqiao Han",
        "Hailiang Huang"
      ],
      "abstract": "Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.",
      "pdf_url": "https://arxiv.org/pdf/2512.02720v1",
      "published": "2025-12-02T12:53:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02720v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs",
      "authors": [
        "Julian Ma",
        "Jun Wang",
        "Zafeirios Fountas"
      ],
      "abstract": "Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.",
      "pdf_url": "https://arxiv.org/pdf/2512.02719v1",
      "published": "2025-12-02T12:51:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02719v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "q-bio.NC"
      ]
    },
    {
      "title": "Menta: A Small Language Model for On-Device Mental Health Prediction",
      "authors": [
        "Tianyi Zhang",
        "Xiangyuan Xue",
        "Lingyan Ruan",
        "Shiya Fu",
        "Feng Xia",
        "Simon D'Alfonso",
        "Vassilis Kostakos",
        "Hong Jia"
      ],
      "abstract": "Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/",
      "pdf_url": "https://arxiv.org/pdf/2512.02716v1",
      "published": "2025-12-02T12:47:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02716v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs",
      "authors": [
        "Theodoros Aivalis",
        "Iraklis A. Klampanos",
        "Antonis Troumpoukis",
        "Joemon M. Jose"
      ],
      "abstract": "As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.",
      "pdf_url": "https://arxiv.org/pdf/2512.02713v1",
      "published": "2025-12-02T12:45:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02713v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Empirical Assessment of the Perception of Software Product Line Engineering by an SME before Migrating its Code Base",
      "authors": [
        "Thomas Georges",
        "Marianne Huchard",
        "Mélanie König",
        "Clémentine Nebut",
        "Chouki Tibermacine"
      ],
      "abstract": "Migrating a set of software variants into a software product line (SPL) is an expensive and potentially challenging endeavor. Indeed, SPL engineering can significantly impact a company's development process and often requires changes to established developer practices. The work presented in this paper stems from a collaboration with a Small and Medium-sized Enterprise (SME) that decided to migrate its existing code base into an SPL. In this study, we conducted an in-depth evaluation of the company's current development processes and practices, as well as the anticipated benefits and risks associated with the migration. Key stakeholders involved in software development participated in this evaluation to provide insight into their perceptions of the migration and their potential resistance to change. This paper describes the design of the interviews conducted with these stakeholders and presents an analysis of the results. Among the qualitative findings, we observed that all participants, regardless of their role in the development process, identified benefits of the migration relevant to their own activities. Furthermore, our results suggest that an effective risk mitigation strategy involves keeping stakeholders informed and engaged throughout the process, preserving as many good practices as possible, and actively involving them in the migration to ensure a smooth transition and minimize potential challenges.",
      "pdf_url": "https://arxiv.org/pdf/2512.02707v1",
      "published": "2025-12-02T12:39:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02707v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding",
      "authors": [
        "Hyeongseop Rha",
        "Jeong Hun Yeo",
        "Junil Won",
        "Se Jin Park",
        "Yong Man Ro"
      ],
      "abstract": "In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.",
      "pdf_url": "https://arxiv.org/pdf/2512.02699v1",
      "published": "2025-12-02T12:29:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02699v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation",
      "authors": [
        "Daiki Shirafuji",
        "Tatsuhiko Saito",
        "Yasutomo Kimura"
      ],
      "abstract": "Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.",
      "pdf_url": "https://arxiv.org/pdf/2512.02689v1",
      "published": "2025-12-02T12:18:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02689v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions",
      "authors": [
        "Piercosma Bisconti",
        "Marcello Galisai",
        "Federico Pierucci",
        "Marcantonio Bracale",
        "Matteo Prandi"
      ],
      "abstract": "This paper examines why safety mechanisms designed for human-model interaction do not scale to environments where large language models (LLMs) interact with each other. Most current governance practices still rely on single-agent safety containment, prompts, fine-tuning, and moderation layers that constrain individual model behavior but leave the dynamics of multi-model interaction ungoverned. These mechanisms assume a dyadic setting: one model responding to one user under stable oversight. Yet research and industrial development are rapidly shifting toward LLM-to-LLM ecosystems, where outputs are recursively reused as inputs across chains of agents. In such systems, local compliance can aggregate into collective failure even when every model is individually aligned. We propose a conceptual transition from model-level safety to system-level safety, introducing the framework of the Emergent Systemic Risk Horizon (ESRH) to formalize how instability arises from interaction structure rather than from isolated misbehavior. The paper contributes (i) a theoretical account of collective risk in interacting LLMs, (ii) a taxonomy connecting micro, meso, and macro-level failure modes, and (iii) a design proposal for InstitutionalAI, an architecture for embedding adaptive oversight within multi-agent systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.02682v1",
      "published": "2025-12-02T12:06:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.02682v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    }
  ]
}