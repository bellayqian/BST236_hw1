{
  "last_updated": "2025-09-09T00:48:36.397522",
  "papers": [
    {
      "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
      "authors": [
        "Zizun Li",
        "Jianjun Zhou",
        "Yifan Wang",
        "Haoyu Guo",
        "Wenzheng Chang",
        "Yang Zhou",
        "Haoyi Zhu",
        "Junyi Chen",
        "Chunhua Shen",
        "Tong He"
      ],
      "abstract": "We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.",
      "pdf_url": "http://arxiv.org/pdf/2509.05296v1",
      "published": "2025-09-05T17:59:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05296v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining",
      "authors": [
        "Deniz Bayazit",
        "Aaron Mueller",
        "Antoine Bosselut"
      ],
      "abstract": "Large language models (LLMs) learn non-trivial abstractions during\npretraining, like detecting irregular plural noun subjects. However, it is not\nwell understood when and how specific linguistic abilities emerge as\ntraditional evaluation methods such as benchmarking fail to reveal how models\nacquire concepts and capabilities. To bridge this gap and better understand\nmodel training at the concept level, we use sparse crosscoders to discover and\nalign features across model checkpoints. Using this approach, we track the\nevolution of linguistic features during pretraining. We train crosscoders\nbetween open-sourced checkpoint triplets with significant performance and\nrepresentation shifts, and introduce a novel metric, Relative Indirect Effects\n(RelIE), to trace training stages at which individual features become causally\nimportant for task performance. We show that crosscoders can detect feature\nemergence, maintenance, and discontinuation during pretraining. Our approach is\narchitecture-agnostic and scalable, offering a promising path toward more\ninterpretable and fine-grained analysis of representation learning throughout\npretraining.",
      "pdf_url": "http://arxiv.org/pdf/2509.05291v1",
      "published": "2025-09-05T17:56:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05291v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SpikingBrain Technical Report: Spiking Brain-inspired Large Models",
      "authors": [
        "Yuqi Pan",
        "Yupeng Feng",
        "Jinghao Zhuang",
        "Siyu Ding",
        "Zehao Liu",
        "Bohan Sun",
        "Yuhong Chou",
        "Han Xu",
        "Xuerui Qiu",
        "Anlin Deng",
        "Anjie Hu",
        "Peng Zhou",
        "Man Yao",
        "Jibin Wu",
        "Jian Yang",
        "Guoliang Sun",
        "Bo Xu",
        "Guoqi Li"
      ],
      "abstract": "Mainstream Transformer-based large language models face major efficiency\nbottlenecks: training computation scales quadratically with sequence length,\nand inference memory grows linearly, limiting long-context processing. Building\nlarge models on non-NVIDIA platforms also poses challenges for stable and\nefficient training. To address this, we introduce SpikingBrain, a family of\nbrain-inspired models designed for efficient long-context training and\ninference. SpikingBrain leverages the MetaX GPU cluster and focuses on three\naspects: (1) Model Architecture: linear and hybrid-linear attention\narchitectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an\nefficient, conversion-based training pipeline and a dedicated spike coding\nframework; (3) System Engineering: customized training frameworks, operator\nlibraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,\nand SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the\nfeasibility of large-scale LLM development on non-NVIDIA platforms.\nSpikingBrain achieves performance comparable to open-source Transformer\nbaselines while using only about 150B tokens for continual pre-training. Our\nmodels significantly improve long-sequence training efficiency and deliver\ninference with (partially) constant memory and event-driven spiking behavior.\nFor example, SpikingBrain-7B attains over 100x speedup in Time to First Token\nfor 4M-token sequences. Training remains stable for weeks on hundreds of MetaX\nC550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4\npercent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling\nlow-power operation. Overall, this work demonstrates the potential of\nbrain-inspired mechanisms to drive the next generation of efficient and\nscalable large model design.",
      "pdf_url": "http://arxiv.org/pdf/2509.05276v1",
      "published": "2025-09-05T17:34:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05276v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation",
      "authors": [
        "Yinglin Duan",
        "Zhengxia Zou",
        "Tongwei Gu",
        "Wei Jia",
        "Zhan Zhao",
        "Luyi Xu",
        "Xinzhu Liu",
        "Hao Jiang",
        "Kang Chen",
        "Shuang Qiu"
      ],
      "abstract": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
      "pdf_url": "http://arxiv.org/pdf/2509.05263v1",
      "published": "2025-09-05T17:22:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05263v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Scaling Performance of Large Language Model Pretraining",
      "authors": [
        "Alexander Interrante-Grant",
        "Carla Varela-Rosa",
        "Suhaas Narayan",
        "Chris Connelly",
        "Albert Reuther"
      ],
      "abstract": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, information about the scaling performance and training\nconsiderations of these large training pipelines is scarce in public\nliterature. Working with large-scale datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity.",
      "pdf_url": "http://arxiv.org/pdf/2509.05258v1",
      "published": "2025-09-05T17:14:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05258v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "Recomposer: Event-roll-guided generative audio editing",
      "authors": [
        "Daniel P. W. Ellis",
        "Eduardo Fonseca",
        "Ron J. Weiss",
        "Kevin Wilson",
        "Scott Wisdom",
        "Hakan Erdogan",
        "John R. Hershey",
        "Aren Jansen",
        "R. Channing Moore",
        "Manoj Plakal"
      ],
      "abstract": "Editing complex real-world sound scenes is difficult because individual sound\nsources overlap in time. Generative models can fill-in missing or corrupted\ndetails based on their strong prior understanding of the data domain. We\npresent a system for editing individual sound events within complex scenes able\nto delete, insert, and enhance individual sound events based on textual edit\ndescriptions (e.g., ``enhance Door'') and a graphical representation of the\nevent timing derived from an ``event roll'' transcription. We present an\nencoder-decoder transformer working on SoundStream representations, trained on\nsynthetic (input, desired output) audio example pairs formed by adding isolated\nsound events to dense, real-world backgrounds. Evaluation reveals the\nimportance of each part of the edit descriptions -- action, class, timing. Our\nwork demonstrates ``recomposition'' is an important and practical application.",
      "pdf_url": "http://arxiv.org/pdf/2509.05256v1",
      "published": "2025-09-05T17:14:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05256v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization",
      "authors": [
        "Yassine Taoudi-Benchekroun",
        "Klim Troyan",
        "Pascal Sager",
        "Stefan Gerber",
        "Lukas Tuggener",
        "Benjamin Grewe"
      ],
      "abstract": "The ability to compose learned concepts and apply them in novel settings is\nkey to human intelligence, but remains a persistent limitation in\nstate-of-the-art machine learning models. To address this issue, we introduce\nCOGITAO, a modular and extensible data generation framework and benchmark\ndesigned to systematically study compositionality and generalization in visual\ndomains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs\nrule-based tasks which apply a set of transformations to objects in grid-like\nenvironments. It supports composition, at adjustable depth, over a set of 28\ninteroperable transformations, along with extensive control over grid\nparametrization and object properties. This flexibility enables the creation of\nmillions of unique task rules -- surpassing concurrent datasets by several\norders of magnitude -- across a wide range of difficulties, while allowing\nvirtually unlimited sample generation per rule. We provide baseline experiments\nusing state-of-the-art vision models, highlighting their consistent failures to\ngeneralize to novel combinations of familiar elements, despite strong in-domain\nperformance. COGITAO is fully open-sourced, including all code and datasets, to\nsupport continued research in this field.",
      "pdf_url": "http://arxiv.org/pdf/2509.05249v1",
      "published": "2025-09-05T17:01:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05249v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Uncertain but Useful: Leveraging CNN Variability into Data Augmentation",
      "authors": [
        "Inés Gonzalez-Pepe",
        "Vinuyan Sivakolunthu",
        "Yohan Chatelain",
        "Tristan Glatard"
      ],
      "abstract": "Deep learning (DL) is rapidly advancing neuroimaging by achieving\nstate-of-the-art performance with reduced computation times. Yet the numerical\nstability of DL models -- particularly during training -- remains\nunderexplored. While inference with DL is relatively stable, training\nintroduces additional variability primarily through iterative stochastic\noptimization. We investigate this training-time variability using FastSurfer, a\nCNN-based whole-brain segmentation pipeline. Controlled perturbations are\nintroduced via floating point perturbations and random seeds. We find that: (i)\nFastSurfer exhibits higher variability compared to that of a traditional\nneuroimaging pipeline, suggesting that DL inherits and is particularly\nsusceptible to sources of instability present in its predecessors; (ii)\nensembles generated with perturbations achieve performance similar to an\nunperturbed baseline; and (iii) variability effectively produces ensembles of\nnumerical model families that can be repurposed for downstream applications. As\na proof of concept, we demonstrate that numerical ensembles can be used as a\ndata augmentation strategy for brain age regression. These findings position\ntraining-time variability not only as a reproducibility concern but also as a\nresource that can be harnessed to improve robustness and enable new\napplications in neuroimaging.",
      "pdf_url": "http://arxiv.org/pdf/2509.05238v1",
      "published": "2025-09-05T16:54:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05238v1",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.NA"
      ]
    },
    {
      "title": "CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models",
      "authors": [
        "Aysenur Kocak",
        "Shuo Yang",
        "Bardh Prenkaj",
        "Gjergji Kasneci"
      ],
      "abstract": "Pre-trained language models have achieved remarkable success across diverse\napplications but remain susceptible to spurious, concept-driven correlations\nthat impair robustness and fairness. In this work, we introduce CURE, a novel\nand lightweight framework that systematically disentangles and suppresses\nconceptual shortcuts while preserving essential content information. Our method\nfirst extracts concept-irrelevant representations via a dedicated content\nextractor reinforced by a reversal network, ensuring minimal loss of\ntask-relevant information. A subsequent controllable debiasing module employs\ncontrastive learning to finely adjust the influence of residual conceptual\ncues, enabling the model to either diminish harmful biases or harness\nbeneficial correlations as appropriate for the target task. Evaluated on the\nIMDB and Yelp datasets using three pre-trained architectures, CURE achieves an\nabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,\nwhile introducing minimal computational overhead. Our approach establishes a\nflexible, unsupervised blueprint for combating conceptual biases, paving the\nway for more reliable and fair language understanding systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.05230v1",
      "published": "2025-09-05T16:47:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05230v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models",
      "authors": [
        "Chang Dai",
        "Hongyu Shan",
        "Mingyang Song",
        "Di Liang"
      ],
      "abstract": "Positional encoding mechanisms enable Transformers to model sequential\nstructure and long-range dependencies in text. While absolute positional\nencodings struggle with extrapolation to longer sequences due to fixed\npositional representations, and relative approaches like Alibi exhibit\nperformance degradation on extremely long contexts, the widely-used Rotary\nPositional Encoding (RoPE) introduces oscillatory attention patterns that\nhinder stable long-distance dependency modelling. We address these limitations\nthrough a geometric reformulation of positional encoding. Drawing inspiration\nfrom Lorentz transformations in hyperbolic geometry, we propose Hyperbolic\nRotary Positional Encoding (HoPE), which leverages hyperbolic functions to\nimplement Lorentz rotations on token representations. Theoretical analysis\ndemonstrates that RoPE is a special case of our generalized formulation. HoPE\nfundamentally resolves RoPE's slation issues by enforcing monotonic decay of\nattention weights with increasing token distances. Extensive experimental\nresults, including perplexity evaluations under several extended sequence\nbenchmarks, show that HoPE consistently exceeds existing positional encoding\nmethods. These findings underscore HoPE's enhanced capacity for representing\nand generalizing long-range dependencies. Data and code will be available.",
      "pdf_url": "http://arxiv.org/pdf/2509.05218v1",
      "published": "2025-09-05T16:20:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05218v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks",
      "authors": [
        "Arefin Niam",
        "Tevfik Kosar",
        "M S Q Zulkar Nine"
      ],
      "abstract": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
      "pdf_url": "http://arxiv.org/pdf/2509.05207v1",
      "published": "2025-09-05T16:10:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05207v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet",
      "authors": [
        "Mohammad Saeid",
        "Amir Salarpour",
        "Pedram MohajerAnsari"
      ],
      "abstract": "The classification of 3D point clouds is crucial for applications such as\nautonomous driving, robotics, and augmented reality. However, the commonly used\nModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D\ndata, size mismatches, and inadequate class differentiation, which hinder model\nperformance. This paper introduces ModelNet-R, a meticulously refined version\nof ModelNet40 designed to address these issues and serve as a more reliable\nbenchmark. Additionally, this paper proposes Point-SkipNet, a lightweight\ngraph-based neural network that leverages efficient sampling, neighborhood\ngrouping, and skip connections to achieve high classification accuracy with\nreduced computational overhead. Extensive experiments demonstrate that models\ntrained in ModelNet-R exhibit significant performance improvements. Notably,\nPoint-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a\nsubstantially lower parameter count compared to contemporary models. This\nresearch highlights the crucial role of dataset quality in optimizing model\nefficiency for 3D point cloud classification. For more details, see the code\nat: https://github.com/m-saeid/ModeNetR_PointSkipNet.",
      "pdf_url": "http://arxiv.org/pdf/2509.05198v1",
      "published": "2025-09-05T15:57:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05198v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "AI Agents for Web Testing: A Case Study in the Wild",
      "authors": [
        "Naimeng Ye",
        "Xiao Yu",
        "Ruize Xu",
        "Tianyi Peng",
        "Zhou Yu"
      ],
      "abstract": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks.",
      "pdf_url": "http://arxiv.org/pdf/2509.05197v1",
      "published": "2025-09-05T15:57:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05197v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection",
      "authors": [
        "Mounvik K",
        "N Harshit"
      ],
      "abstract": "Deep learning models, especially convolutional neural networks (CNNs), have\nshown considerable promise for biomedical signals such as EEG-based seizure\ndetection. However, these models come with challenges, primarily due to their\nsize and compute requirements in environments where real-time detection or\nlimited resources are available. In this study, we present a lightweight\none-dimensional CNN model with structured pruning to improve efficiency and\nreliability. The model was trained with mild early stopping to address possible\noverfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686.\nStructured pruning of the baseline CNN involved removing 50% of the\nconvolutional kernels based on their importance to model predictions.\nSurprisingly, after pruning the weights and memory by 50%, the new network was\nstill able to maintain predictive capabilities, while modestly increasing\nprecision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we\npresent a convincing case that structured pruning removes redundancy, improves\ngeneralization, and, in combination with mild early stopping, achieves a\npromising way forward to improve seizure detection efficiency and reliability,\nwhich is clear motivation for resource-limited settings.",
      "pdf_url": "http://arxiv.org/pdf/2509.05190v1",
      "published": "2025-09-05T15:42:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05190v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination",
      "authors": [
        "Błażej Kotowski",
        "Nicholas Evans",
        "Behzad Haki",
        "Frederic Font",
        "Sergi Jordà"
      ],
      "abstract": "This paper investigates GrooveTransformer, a real-time rhythm generation\nsystem, through the postphenomenological framework of Variational\nCross-Examination (VCE). By reflecting on its deployment across three distinct\nartistic contexts, we identify three stabilities: an autonomous drum\naccompaniment generator, a rhythmic control voltage sequencer in Eurorack\nformat, and a rhythm driver for a harmonic accompaniment system. The\nversatility of its applications was not an explicit goal from the outset of the\nproject. Thus, we ask: how did this multistability emerge? Through VCE, we\nidentify three key contributors to its emergence: the affordances of system\ninvariants, the interdisciplinary collaboration, and the situated nature of its\ndevelopment. We conclude by reflecting on the viability of VCE as a descriptive\nand analytical method for Digital Musical Instrument (DMI) design, emphasizing\nits value in uncovering how technologies mediate, co-shape, and are co-shaped\nby users and contexts.",
      "pdf_url": "http://arxiv.org/pdf/2509.05145v1",
      "published": "2025-09-05T14:38:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05145v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Evaluation and Comparison Semantics for ODRL",
      "authors": [
        "Jaime Osvaldo Salas",
        "Paolo Pareti",
        "Semih Yumuşak",
        "Soulmaz Gheisari",
        "Luis-Daniel Ibáñez",
        "George Konstantinidis"
      ],
      "abstract": "We consider the problem of evaluating, and comparing computational policies\nin the Open Digital Rights Language (ODRL), which has become the de facto\nstandard for governing the access and usage of digital resources. Although\npreliminary progress has been made on the formal specification of the\nlanguage's features, a comprehensive formal semantics of ODRL is still missing.\nIn this paper, we provide a simple and intuitive formal semantics for ODRL that\nis based on query answering. Our semantics refines previous formalisations, and\nis aligned with the latest published specification of the language (2.2).\nBuilding on our evaluation semantics, and motivated by data sharing scenarios,\nwe also define and study the problem of comparing two policies, detecting\nequivalent, more restrictive or more permissive policies.",
      "pdf_url": "http://arxiv.org/pdf/2509.05139v1",
      "published": "2025-09-05T14:30:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05139v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GenAI-based test case generation and execution in SDV platform",
      "authors": [
        "Denesa Zyberaj",
        "Lukasz Mazur",
        "Nenad Petrovic",
        "Pankhuri Verma",
        "Pascal Hirmer",
        "Dirk Slama",
        "Xiangwei Cheng",
        "Alois Knoll"
      ],
      "abstract": "This paper introduces a GenAI-driven approach for automated test case\ngeneration, leveraging Large Language Models and Vision-Language Models to\ntranslate natural language requirements and system diagrams into structured\nGherkin test cases. The methodology integrates Vehicle Signal Specification\nmodeling to standardize vehicle signal definitions, improve compatibility\nacross automotive subsystems, and streamline integration with third-party\ntesting tools. Generated test cases are executed within the digital.auto\nplayground, an open and vendor-neutral environment designed to facilitate rapid\nvalidation of software-defined vehicle functionalities. We evaluate our\napproach using the Child Presence Detection System use case, demonstrating\nsubstantial reductions in manual test specification effort and rapid execution\nof generated tests. Despite significant automation, the generation of test\ncases and test scripts still requires manual intervention due to current\nlimitations in the GenAI pipeline and constraints of the digital.auto platform.",
      "pdf_url": "http://arxiv.org/pdf/2509.05112v1",
      "published": "2025-09-05T13:50:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05112v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "ICR: Iterative Clarification and Rewriting for Conversational Search",
      "authors": [
        "Zhiyu Cao",
        "Peifeng Li",
        "Qiaoming Zhu"
      ],
      "abstract": "Most previous work on Conversational Query Rewriting employs an end-to-end\nrewriting paradigm. However, this approach is hindered by the issue of multiple\nfuzzy expressions within the query, which complicates the simultaneous\nidentification and rewriting of multiple positions. To address this issue, we\npropose a novel framework ICR (Iterative Clarification and Rewriting), an\niterative rewriting scheme that pivots on clarification questions. Within this\nframework, the model alternates between generating clarification questions and\nrewritten queries. The experimental results show that our ICR can continuously\nimprove retrieval performance in the clarification-rewriting iterative process,\nthereby achieving state-of-the-art performance on two popular datasets.",
      "pdf_url": "http://arxiv.org/pdf/2509.05100v1",
      "published": "2025-09-05T13:37:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05100v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback",
      "authors": [
        "Matteo Bortoletto",
        "Yichao Zhou",
        "Lance Ying",
        "Tianmin Shu",
        "Andreas Bulling"
      ],
      "abstract": "While humans are inherently social creatures, the challenge of identifying\nwhen and how to assist and collaborate with others - particularly when pursuing\nindependent goals - can hinder cooperation. To address this challenge, we aim\nto develop an AI system that provides useful feedback to promote prosocial\nbehaviour - actions that benefit others, even when not directly aligned with\none's own goals. We introduce ProToM, a Theory of Mind-informed facilitator\nthat promotes prosocial actions in multi-agent systems by providing targeted,\ncontext-sensitive feedback to individual agents. ProToM first infers agents'\ngoals using Bayesian inverse planning, then selects feedback to communicate by\nmaximising expected utility, conditioned on the inferred goal distribution. We\nevaluate our approach against baselines in two multi-agent environments: Doors,\nKeys, and Gems, as well as Overcooked. Our results suggest that\nstate-of-the-art large language and reasoning models fall short of\ncommunicating feedback that is both contextually grounded and well-timed -\nleading to higher communication overhead and task speedup. In contrast, ProToM\nprovides targeted and helpful feedback, achieving a higher success rate,\nshorter task completion times, and is consistently preferred by human users.",
      "pdf_url": "http://arxiv.org/pdf/2509.05091v1",
      "published": "2025-09-05T13:30:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05091v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Finding your MUSE: Mining Unexpected Solutions Engine",
      "authors": [
        "Nir Sweed",
        "Hanit Hakim",
        "Ben Wolfson",
        "Hila Lifshitz",
        "Dafna Shahaf"
      ],
      "abstract": "Innovators often exhibit cognitive fixation on existing solutions or nascent\nideas, hindering the exploration of novel alternatives. This paper introduces a\nmethodology for constructing Functional Concept Graphs (FCGs), interconnected\nrepresentations of functional elements that support abstraction, problem\nreframing, and analogical inspiration. Our approach yields large-scale,\nhigh-quality FCGs with explicit abstraction relations, overcoming limitations\nof prior work. We further present MUSE, an algorithm leveraging FCGs to\ngenerate creative inspirations for a given problem. We demonstrate our method\nby computing an FCG on 500K patents, which we release for further research.",
      "pdf_url": "http://arxiv.org/pdf/2509.05072v1",
      "published": "2025-09-05T13:13:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05072v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions",
      "authors": [
        "Matteo Bortoletto",
        "Constantin Ruhdorfer",
        "Andreas Bulling"
      ],
      "abstract": "Most existing Theory of Mind (ToM) benchmarks for foundation models rely on\nvariations of the Sally-Anne test, offering only a very limited perspective on\nToM and neglecting the complexity of human social interactions. To address this\ngap, we propose ToM-SSI: a new benchmark specifically designed to test ToM\ncapabilities in environments rich with social interactions and spatial\ndynamics. While current ToM benchmarks are limited to text-only or dyadic\ninteractions, ToM-SSI is multimodal and includes group interactions of up to\nfour agents that communicate and move in situated environments. This unique\ndesign allows us to study, for the first time, mixed cooperative-obstructive\nsettings and reasoning about multiple agents' mental state in parallel, thus\ncapturing a wider range of social cognition than existing benchmarks. Our\nevaluations reveal that the current models' performance is still severely\nlimited, especially in these new tasks, highlighting critical gaps for future\nresearch.",
      "pdf_url": "http://arxiv.org/pdf/2509.05066v1",
      "published": "2025-09-05T12:58:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05066v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization",
      "authors": [
        "Jingqi Wu",
        "Hanxi Li",
        "Lin Yuanbo Wu",
        "Hao Chen",
        "Deyin Liu",
        "Peng Wang"
      ],
      "abstract": "Industrial product inspection is often performed using Anomaly Detection (AD)\nframeworks trained solely on non-defective samples. Although defective samples\ncan be collected during production, leveraging them usually requires\npixel-level annotations, limiting scalability. To address this, we propose\nADClick, an Interactive Image Segmentation (IIS) algorithm for industrial\nanomaly detection. ADClick generates pixel-wise anomaly annotations from only a\nfew user clicks and a brief textual description, enabling precise and efficient\nlabeling that significantly improves AD model performance (e.g., AP = 96.1\\% on\nMVTec AD). We further introduce ADClick-Seg, a cross-modal framework that\naligns visual features and textual prompts via a prototype-based approach for\nanomaly detection and localization. By combining pixel-level priors with\nlanguage-guided cues, ADClick-Seg achieves state-of-the-art results on the\nchallenging ``Multi-class'' AD task (AP = 80.0\\%, PRO = 97.5\\%, Pixel-AUROC =\n99.1\\% on MVTec AD).",
      "pdf_url": "http://arxiv.org/pdf/2509.05034v1",
      "published": "2025-09-05T11:45:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05034v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Pointing-Guided Target Estimation via Transformer-Based Attention",
      "authors": [
        "Luca Müller",
        "Hassan Ali",
        "Philipp Allgeuer",
        "Lukáš Gajdošech",
        "Stefan Wermter"
      ],
      "abstract": "Deictic gestures, like pointing, are a fundamental form of non-verbal\ncommunication, enabling humans to direct attention to specific objects or\nlocations. This capability is essential in Human-Robot Interaction (HRI), where\nrobots should be able to predict human intent and anticipate appropriate\nresponses. In this work, we propose the Multi-Modality Inter-TransFormer\n(MM-ITF), a modular architecture to predict objects in a controlled tabletop\nscenario with the NICOL robot, where humans indicate targets through natural\npointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing\ngestures to object locations, assigns a likelihood score to each, and\nidentifies the most likely target. Our results demonstrate that the method can\naccurately predict the intended object using monocular RGB data, thus enabling\nintuitive and accessible human-robot collaboration. To evaluate the\nperformance, we introduce a patch confusion matrix, providing insights into the\nmodel's predictions across candidate object locations. Code available at:\nhttps://github.com/lucamuellercode/MMITF.",
      "pdf_url": "http://arxiv.org/pdf/2509.05031v1",
      "published": "2025-09-05T11:42:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05031v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "I.2.9; I.2.10; I.2.6"
      ]
    },
    {
      "title": "Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework",
      "authors": [
        "Jie Chen",
        "Jinhao Jiang",
        "Yingqian Min",
        "Zican Dong",
        "Shijie Wang",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "abstract": "Large reasoning models (LRMs) have exhibited strong performance on complex\nreasoning tasks, with further gains achievable through increased computational\nbudgets at inference. However, current test-time scaling methods predominantly\nrely on redundant sampling, ignoring the historical experience utilization,\nthereby limiting computational efficiency. To overcome this limitation, we\npropose Sticker-TTS, a novel test-time scaling framework that coordinates three\ncollaborative LRMs to iteratively explore and refine solutions guided by\nhistorical attempts. At the core of our framework are distilled key\nconditions-termed stickers-which drive the extraction, refinement, and reuse of\ncritical information across multiple rounds of reasoning. To further enhance\nthe efficiency and performance of our framework, we introduce a two-stage\noptimization strategy that combines imitation learning with self-improvement,\nenabling progressive refinement. Extensive evaluations on three challenging\nmathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,\ndemonstrate that Sticker-TTS consistently surpasses strong baselines, including\nself-consistency and advanced reinforcement learning approaches, under\ncomparable inference budgets. These results highlight the effectiveness of\nsticker-guided historical experience utilization. Our code and data are\navailable at https://github.com/RUCAIBox/Sticker-TTS.",
      "pdf_url": "http://arxiv.org/pdf/2509.05007v1",
      "published": "2025-09-05T11:14:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.05007v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "I.2.7"
      ]
    },
    {
      "title": "Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection",
      "authors": [
        "Sidahmed Benabderrahmane",
        "Talal Rahwan"
      ],
      "abstract": "Advanced Persistent Threats (APTs) present a considerable challenge to\ncybersecurity due to their stealthy, long-duration nature. Traditional\nsupervised learning methods typically require large amounts of labeled data,\nwhich is often scarce in real-world scenarios. This paper introduces a novel\napproach that combines AutoEncoders for anomaly detection with active learning\nto iteratively enhance APT detection. By selectively querying an oracle for\nlabels on uncertain or ambiguous samples, our method reduces labeling costs\nwhile improving detection accuracy, enabling the model to effectively learn\nwith minimal data and reduce reliance on extensive manual labeling. We present\na comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based\nanomaly detection framework and demonstrate how the active learning loop\nprogressively enhances the model's performance. The framework is evaluated on\nreal-world, imbalanced provenance trace data from the DARPA Transparent\nComputing program, where APT-like attacks account for just 0.004\\% of the data.\nThe datasets, which cover multiple operating systems including Android, Linux,\nBSD, and Windows, are tested in two attack scenarios. The results show\nsubstantial improvements in detection rates during active learning,\noutperforming existing methods.",
      "pdf_url": "http://arxiv.org/pdf/2509.04999v1",
      "published": "2025-09-05T10:47:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04999v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of Dual-Loop Edge-Terminal Collaboration",
      "authors": [
        "Zheyan Qu",
        "Wenbo Wang",
        "Zitong Yu",
        "Boquan Sun",
        "Yang Li",
        "Xing Zhang"
      ],
      "abstract": "The ubiquitous computing resources in 6G networks provide ideal environments\nfor the fusion of large language models (LLMs) and intelligent services through\nthe agent framework. With auxiliary modules and planning cores, LLM-enabled\nagents can autonomously plan and take actions to deal with diverse environment\nsemantics and user intentions. However, the limited resources of individual\nnetwork devices significantly hinder the efficient operation of LLM-enabled\nagents with complex tool calls, highlighting the urgent need for efficient\nmulti-level device collaborations. To this end, the framework and method of the\nLLM-enabled multi-agent system with dual-loop terminal-edge collaborations are\nproposed in 6G networks. Firstly, the outer loop consists of the iterative\ncollaborations between the global agent and multiple sub-agents deployed on\nedge servers and terminals, where the planning capability is enhanced through\ntask decomposition and parallel sub-task distribution. Secondly, the inner loop\nutilizes sub-agents with dedicated roles to circularly reason, execute, and\nreplan the sub-task, and the parallel tool calling generation with offloading\nstrategies is incorporated to improve efficiency. The improved task planning\ncapability and task execution efficiency are validated through the conducted\ncase study in 6G-supported urban safety governance. Finally, the open\nchallenges and future directions are thoroughly analyzed in 6G networks,\naccelerating the advent of the 6G era.",
      "pdf_url": "http://arxiv.org/pdf/2509.04993v1",
      "published": "2025-09-05T10:40:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04993v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "High-Resolution Global Land Surface Temperature Retrieval via a Coupled Mechanism-Machine Learning Framework",
      "authors": [
        "Tian Xie",
        "Huanfeng Shen",
        "Menghui Jiang",
        "Juan-Carlos Jiménez-Muñoz",
        "José A. Sobrino",
        "Huifang Li",
        "Chao Zeng"
      ],
      "abstract": "Land surface temperature (LST) is vital for land-atmosphere interactions and\nclimate processes. Accurate LST retrieval remains challenging under\nheterogeneous land cover and extreme atmospheric conditions. Traditional split\nwindow (SW) algorithms show biases in humid environments; purely machine\nlearning (ML) methods lack interpretability and generalize poorly with limited\ndata. We propose a coupled mechanism model-ML (MM-ML) framework integrating\nphysical constraints with data-driven learning for robust LST retrieval. Our\napproach fuses radiative transfer modeling with data components, uses MODTRAN\nsimulations with global atmospheric profiles, and employs physics-constrained\noptimization. Validation against 4,450 observations from 29 global sites shows\nMM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming\nconventional methods. Under extreme conditions, MM-ML reduces errors by over\n50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor\nradiance, then water vapor, and less to emissivity, with MM-ML showing superior\nstability. These results demonstrate the effectiveness of our coupled modeling\nstrategy for retrieving geophysical parameters. The MM-ML framework combines\nphysical interpretability with nonlinear modeling capacity, enabling reliable\nLST retrieval in complex environments and supporting climate monitoring and\necosystem studies.",
      "pdf_url": "http://arxiv.org/pdf/2509.04991v1",
      "published": "2025-09-05T10:37:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04991v1",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Exploring an implementation of quantum learning pipeline for support vector machines",
      "authors": [
        "Mario Bifulco",
        "Luca Roversi"
      ],
      "abstract": "This work presents a fully quantum approach to support vector machine (SVM)\nlearning by integrating gate-based quantum kernel methods with quantum\nannealing-based optimization. We explore the construction of quantum kernels\nusing various feature maps and qubit configurations, evaluating their\nsuitability through Kernel-Target Alignment (KTA). The SVM dual problem is\nreformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem,\nenabling its solution via quantum annealers. Our experiments demonstrate that a\nhigh degree of alignment in the kernel and an appropriate regularization\nparameter lead to competitive performance, with the best model achieving an\nF1-score of 90%. These results highlight the feasibility of an end-to-end\nquantum learning pipeline and the potential of hybrid quantum architectures in\nquantum high-performance computing (QHPC) contexts.",
      "pdf_url": "http://arxiv.org/pdf/2509.04983v1",
      "published": "2025-09-05T10:19:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04983v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents",
      "authors": [
        "Rajesh Tembarai Krishnamachari",
        "Srividya Rajesh"
      ],
      "abstract": "AI agents -- powered by reasoning-capable large language models (LLMs) and\nintegrated with tools, data, and web search -- are poised to transform the\ninternet into a \\emph{Web of Agents}: a machine-native ecosystem where\nautonomous agents interact, collaborate, and execute tasks at scale. Realizing\nthis vision requires \\emph{Agent Ranking} -- selecting agents not only by\ndeclared capabilities but by proven, recent performance. Unlike Web~1.0's\nPageRank, a global, transparent network of agent interactions does not exist;\nusage signals are fragmented and private, making ranking infeasible without\ncoordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol\n(\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that\nenables the collection of minimal, privacy-preserving aggregates of usage and\nperformance across the ecosystem. On this substrate, we implement\n\\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines\n\\emph{usage} (selection frequency) and \\emph{competence} (outcome quality,\ncost, safety, latency) into a unified ranking. We present simulation results\nand theoretical guarantees on convergence, robustness, and Sybil resistance,\ndemonstrating the viability of coordinated protocols and performance-aware\nranking in enabling a scalable, trustworthy Agentic Web.",
      "pdf_url": "http://arxiv.org/pdf/2509.04979v1",
      "published": "2025-09-05T10:04:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04979v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation",
      "authors": [
        "Tien Pham",
        "Xinyun Chi",
        "Khang Nguyen",
        "Manfred Huber",
        "Angelo Cangelosi"
      ],
      "abstract": "Reinforcement learning (RL) agents can learn to solve complex tasks from\nvisual inputs, but generalizing these learned skills to new environments\nremains a major challenge in RL application, especially robotics. While data\naugmentation can improve generalization, it often compromises sample efficiency\nand training stability. This paper introduces DeGuV, an RL framework that\nenhances both generalization and sample efficiency. In specific, we leverage a\nlearnable masker network that produces a mask from the depth input, preserving\nonly critical visual information while discarding irrelevant pixels. Through\nthis, we ensure that our RL agents focus on essential features, improving\nrobustness under data augmentation. In addition, we incorporate contrastive\nlearning and stabilize Q-value estimation under augmentation to further enhance\nsample efficiency and training stability. We evaluate our proposed method on\nthe RL-ViGen benchmark using the Franka Emika robot and demonstrate its\neffectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV\noutperforms state-of-the-art methods in both generalization and sample\nefficiency while also improving interpretability by highlighting the most\nrelevant regions in the visual input",
      "pdf_url": "http://arxiv.org/pdf/2509.04970v1",
      "published": "2025-09-05T09:52:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04970v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts",
      "authors": [
        "Barbara Gendron",
        "Gaël Guibon",
        "Mathieu D'aquin"
      ],
      "abstract": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI.",
      "pdf_url": "http://arxiv.org/pdf/2509.04926v1",
      "published": "2025-09-05T08:44:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04926v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Artificial intelligence for representing and characterizing quantum systems",
      "authors": [
        "Yuxuan Du",
        "Yan Zhu",
        "Yuan-Hang Zhang",
        "Min-Hsiu Hsieh",
        "Patrick Rebentrost",
        "Weibo Gao",
        "Ya-Dong Wu",
        "Jens Eisert",
        "Giulio Chiribella",
        "Dacheng Tao",
        "Barry C. Sanders"
      ],
      "abstract": "Efficient characterization of large-scale quantum systems, especially those\nproduced by quantum analog simulators and megaquop quantum computers, poses a\ncentral challenge in quantum science due to the exponential scaling of the\nHilbert space with respect to system size. Recent advances in artificial\nintelligence (AI), with its aptitude for high-dimensional pattern recognition\nand function approximation, have emerged as a powerful tool to address this\nchallenge. A growing body of research has leveraged AI to represent and\ncharacterize scalable quantum systems, spanning from theoretical foundations to\nexperimental realizations. Depending on how prior knowledge and learning\narchitectures are incorporated, the integration of AI into quantum system\ncharacterization can be categorized into three synergistic paradigms: machine\nlearning, and, in particular, deep learning and language models. This review\ndiscusses how each of these AI paradigms contributes to two core tasks in\nquantum systems characterization: quantum property prediction and the\nconstruction of surrogates for quantum states. These tasks underlie diverse\napplications, from quantum certification and benchmarking to the enhancement of\nquantum algorithms and the understanding of strongly correlated phases of\nmatter. Key challenges and open questions are also discussed, together with\nfuture prospects at the interface of AI and quantum science.",
      "pdf_url": "http://arxiv.org/pdf/2509.04923v1",
      "published": "2025-09-05T08:41:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04923v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing",
      "authors": [
        "Hongyi Jing",
        "Jiafu Chen",
        "Chen Rao",
        "Ziqiang Dang",
        "Jiajie Teng",
        "Tianyi Chu",
        "Juncheng Mo",
        "Shuo Fang",
        "Huaizhong Lin",
        "Rui Lv",
        "Chenguang Ma",
        "Lei Zhao"
      ],
      "abstract": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser.",
      "pdf_url": "http://arxiv.org/pdf/2509.04908v1",
      "published": "2025-09-05T08:24:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04908v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ]
    },
    {
      "title": "PLaMo 2 Technical Report",
      "authors": [
        "Preferred Networks",
        ":",
        "Kaizaburo Chubachi",
        "Yasuhiro Fujita",
        "Shinichi Hemmi",
        "Yuta Hirokawa",
        "Toshiki Kataoka",
        "Goro Kobayashi",
        "Kenichi Maehashi",
        "Calvin Metzger",
        "Hiroaki Mikami",
        "Shogo Murai",
        "Daisuke Nishino",
        "Kento Nozawa",
        "Shintarou Okada",
        "Daisuke Okanohara",
        "Shunta Saito",
        "Shotaro Sano",
        "Shuji Suzuki",
        "Daisuke Tanaka",
        "Avinash Ummadisingu",
        "Hanqin Wang",
        "Sixue Wang",
        "Tianqi Xu"
      ],
      "abstract": "In this report, we introduce PLaMo 2, a series of Japanese-focused large\nlanguage models featuring a hybrid Samba-based architecture that transitions to\nfull attention via continual pre-training to support 32K token contexts.\nTraining leverages extensive synthetic corpora to overcome data scarcity, while\ncomputational efficiency is achieved through weight reuse and structured\npruning. This efficient pruning methodology produces an 8B model that achieves\nperformance comparable to our previous 100B model. Post-training further\nrefines the models using a pipeline of supervised fine-tuning (SFT) and direct\npreference optimization (DPO), enhanced by synthetic Japanese instruction data\nand model merging techniques. Optimized for inference using vLLM and\nquantization with minimal accuracy loss, the PLaMo 2 models achieve\nstate-of-the-art results on Japanese benchmarks, outperforming similarly-sized\nopen models in instruction-following, language fluency, and Japanese-specific\nknowledge.",
      "pdf_url": "http://arxiv.org/pdf/2509.04897v1",
      "published": "2025-09-05T08:17:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04897v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models",
      "authors": [
        "Dominik Pegler",
        "David Steyrl",
        "Mengfan Zhang",
        "Alexander Karner",
        "Jozsef Arato",
        "Frank Scharnowski",
        "Filip Melinscak"
      ],
      "abstract": "Advances in computer vision have opened new avenues for clinical\napplications, particularly in computerized exposure therapy where visual\nstimuli can be dynamically adjusted based on patient responses. As a critical\nstep toward such adaptive systems, we investigated whether pretrained computer\nvision models can accurately predict fear levels from spider-related images. We\nadapted three diverse models using transfer learning to predict human fear\nratings (on a 0-100 scale) from a standardized dataset of 313 images. The\nmodels were evaluated using cross-validation, achieving an average mean\nabsolute error (MAE) between 10.1 and 11.0. Our learning curve analysis\nrevealed that reducing the dataset size significantly harmed performance,\nthough further increases yielded no substantial gains. Explainability\nassessments showed the models' predictions were based on spider-related\nfeatures. A category-wise error analysis further identified visual conditions\nassociated with higher errors (e.g., distant views and artificial/painted\nspiders). These findings demonstrate the potential of explainable computer\nvision models in predicting fear ratings, highlighting the importance of both\nmodel explainability and a sufficient dataset size for developing effective\nemotion-aware therapeutic technologies.",
      "pdf_url": "http://arxiv.org/pdf/2509.04889v1",
      "published": "2025-09-05T08:10:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04889v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration",
      "authors": [
        "Jusheng Zhang",
        "Yijia Fan",
        "Kaitong Cai",
        "Xiaofei Sun",
        "Keze Wang"
      ],
      "abstract": "This paper introduces OSC (Orchestrating Cognitive Synergy), a\nknowledge-aware adaptive collaboration framework designed to enhance cognitive\nsynergy in multi-agent systems with large language models. While prior work has\nadvanced agent selection and result aggregation, efficient linguistic\ninteractions for deep collaboration among expert agents remain a critical\nbottleneck. OSC addresses this gap as a pivotal intermediate layer between\nselection and aggregation, introducing Collaborator Knowledge Models (CKM) to\nenable each agent to dynamically perceive its collaborators' cognitive states.\nThrough real-time cognitive gap analysis, agents adaptively adjust\ncommunication behaviors, including content focus, detail level, and expression\nstyle, using learned strategies. Experiments on complex reasoning and\nproblem-solving benchmarks demonstrate that OSC significantly improves task\nperformance and communication efficiency, transforming \"parallel-working\nindividuals'' into a \"deeply collaborative cognitive team.'' This framework not\nonly optimizes multi-agent collaboration but also offers new insights into LLM\nagent interaction behaviors.",
      "pdf_url": "http://arxiv.org/pdf/2509.04876v1",
      "published": "2025-09-05T07:44:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04876v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets for Telesales",
      "authors": [
        "Krittanon Kaewtawee",
        "Wachiravit Modecrua",
        "Krittin Pachtrachai",
        "Touchapon Kraisingkorn"
      ],
      "abstract": "Recent advances in language and speech modelling have made it possible to\nbuild autonomous voice assistants that understand and generate human dialogue\nin real time. These systems are increasingly being deployed in domains such as\ncustomer service and healthcare care, where they can automate repetitive tasks,\nreduce operational costs, and provide constant support around the clock. In\nthis paper, we present a general methodology for cloning a conversational voice\nAI agent from a corpus of call recordings. Although the case study described in\nthis paper uses telesales data to illustrate the approach, the underlying\nprocess generalizes to any domain where call transcripts are available. Our\nsystem listens to customers over the telephone, responds with a synthetic\nvoice, and follows a structured playbook learned from top performing human\nagents. We describe the domain selection, knowledge extraction, and prompt\nengineering used to construct the agent, integrating automatic speech\nrecognition, a large language model based dialogue manager, and text to speech\nsynthesis into a streaming inference pipeline. The cloned agent is evaluated\nagainst human agents on a rubric of 22 criteria covering introduction, product\ncommunication, sales drive, objection handling, and closing. Blind tests show\nthat the AI agent approaches human performance in routine aspects of the call\nwhile underperforming in persuasion and objection handling. We analyze these\nshortcomings and refine the prompt accordingly. The paper concludes with design\nlessons and avenues for future research, including large scale simulation and\nautomated evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2509.04871v1",
      "published": "2025-09-05T07:36:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04871v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The Paradox of Doom: Acknowledging Extinction Risk Reduces the Incentive to Prevent It",
      "authors": [
        "Jakub Growiec",
        "Klaus Prettner"
      ],
      "abstract": "We investigate the salience of extinction risk as a source of impatience. Our\nframework distinguishes between human extinction risk and individual mortality\nrisk while allowing for various degrees of intergenerational altruism.\nAdditionally, we consider the evolutionarily motivated \"selfish gene\"\nperspective. We find that the risk of human extinction is an indispensable\ncomponent of the discount rate, whereas individual mortality risk can be hedged\nagainst - partially or fully, depending on the setup - through human\nreproduction. Overall, we show that in the face of extinction risk, people\nbecome more impatient rather than more farsighted. Thus, the greater the threat\nof extinction, the less incentive there is to invest in avoiding it. Our\nframework can help explain why humanity consistently underinvests in mitigation\nof catastrophic risks, ranging from climate change mitigation, via pandemic\nprevention, to addressing the emerging risks of transformative artificial\nintelligence.",
      "pdf_url": "http://arxiv.org/pdf/2509.04855v1",
      "published": "2025-09-05T07:14:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04855v1",
      "categories": [
        "econ.GN",
        "cs.AI",
        "q-fin.EC"
      ]
    },
    {
      "title": "A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing",
      "authors": [
        "Chengkai Xu",
        "Jiaqi Liu",
        "Yicheng Guo",
        "Peng Hang",
        "Jian Sun"
      ],
      "abstract": "End-to-end autonomous driving remains constrained by the need to generate\nmulti-modal actions, maintain temporal stability, and generalize across diverse\nscenarios. Existing methods often collapse multi-modality, struggle with\nlong-horizon consistency, or lack modular adaptability. This paper presents\nKDP, a knowledge-driven diffusion policy that integrates generative diffusion\nmodeling with a sparse mixture-of-experts routing mechanism. The diffusion\ncomponent generates temporally coherent and multi-modal action sequences, while\nthe expert routing mechanism activates specialized and reusable experts\naccording to context, enabling modular knowledge composition. Extensive\nexperiments across representative driving scenarios demonstrate that KDP\nachieves consistently higher success rates, reduced collision risk, and\nsmoother control compared to prevailing paradigms. Ablation studies highlight\nthe effectiveness of sparse expert activation and the Transformer backbone, and\nactivation analyses reveal structured specialization and cross-scenario reuse\nof experts. These results establish diffusion with expert routing as a scalable\nand interpretable paradigm for knowledge-driven end-to-end autonomous driving.",
      "pdf_url": "http://arxiv.org/pdf/2509.04853v1",
      "published": "2025-09-05T07:07:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04853v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory",
      "authors": [
        "Mukul Singh",
        "Arjun Radhakrishna",
        "Sumit Gulwani"
      ],
      "abstract": "Language models are increasingly deployed in interactive online environments,\nfrom personal chat assistants to domain-specific agents, raising questions\nabout their cooperative and competitive behavior in multi-party settings. While\nprior work has examined language model decision-making in isolated or\nshort-term game-theoretic contexts, these studies often neglect long-horizon\ninteractions, human-model collaboration, and the evolution of behavioral\npatterns over time. In this paper, we investigate the dynamics of language\nmodel behavior in the iterated prisoner's dilemma (IPD), a classical framework\nfor studying cooperation and conflict. We pit model-based agents against a\nsuite of 240 well-established classical strategies in an Axelrod-style\ntournament and find that language models achieve performance on par with, and\nin some cases exceeding, the best-known classical strategies. Behavioral\nanalysis reveals that language models exhibit key properties associated with\nstrong cooperative strategies - niceness, provocability, and generosity while\nalso demonstrating rapid adaptability to changes in opponent strategy mid-game.\nIn controlled \"strategy switch\" experiments, language models detect and respond\nto shifts within only a few rounds, rivaling or surpassing human adaptability.\nThese results provide the first systematic characterization of long-term\ncooperative behaviors in language model agents, offering a foundation for\nfuture research into their role in more complex, mixed human-AI social\nenvironments.",
      "pdf_url": "http://arxiv.org/pdf/2509.04847v1",
      "published": "2025-09-05T06:55:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04847v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts",
      "authors": [
        "Xinkui Lin",
        "Yongxiu Xu",
        "Minghao Tang",
        "Shilong Zhang",
        "Hongbo Xu",
        "Hao Xu",
        "Yubin Wang"
      ],
      "abstract": "Multimodal relation extraction (MRE) is a crucial task in the fields of\nKnowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge\ngraph construction. However, existing methods are typically limited to\nextracting a single type of relational triplet, which restricts their ability\nto extract triplets beyond the specified types. Directly combining these\nmethods fails to capture dynamic cross-modal interactions and introduces\nsignificant computational redundancy. Therefore, we propose a novel\n\\textit{unified multimodal Relation Extraction framework with Multilevel\nOptimal Transport and mixture-of-Experts}, termed REMOTE, which can\nsimultaneously extract intra-modal and inter-modal relations between textual\nentities and visual objects. To dynamically select optimal interaction features\nfor different types of relational triplets, we introduce mixture-of-experts\nmechanism, ensuring the most relevant modality information is utilized.\nAdditionally, considering that the inherent property of multilayer sequential\nencoding in existing encoders often leads to the loss of low-level information,\nwe adopt a multilevel optimal transport fusion module to preserve low-level\nfeatures while maintaining multilayer encoding, yielding more expressive\nrepresentations. Correspondingly, we also create a Unified Multimodal Relation\nExtraction (UMRE) dataset to evaluate the effectiveness of our framework,\nencompassing diverse cases where the head and tail entities can originate from\neither text or image. Extensive experiments show that REMOTE effectively\nextracts various types of relational triplets and achieves state-of-the-art\nperformanc on almost all metrics across two other public MRE datasets. We\nrelease our resources at https://github.com/Nikol-coder/REMOTE.",
      "pdf_url": "http://arxiv.org/pdf/2509.04844v1",
      "published": "2025-09-05T06:52:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04844v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination",
      "authors": [
        "Ming Dai",
        "Wenxuan Cheng",
        "Jiedong Zhuang",
        "Jiang-jiang Liu",
        "Hongshen Zhao",
        "Zhenhua Feng",
        "Wankou Yang"
      ],
      "abstract": "Recent advances in visual grounding have largely shifted away from\ntraditional proposal-based two-stage frameworks due to their inefficiency and\nhigh computational complexity, favoring end-to-end direct reference paradigms.\nHowever, these methods rely exclusively on the referred target for supervision,\noverlooking the potential benefits of prominent prospective targets. Moreover,\nexisting approaches often fail to incorporate multi-granularity discrimination,\nwhich is crucial for robust object identification in complex scenarios. To\naddress these limitations, we propose PropVG, an end-to-end proposal-based\nframework that, to the best of our knowledge, is the first to seamlessly\nintegrate foreground object proposal generation with referential object\ncomprehension without requiring additional detectors. Furthermore, we introduce\na Contrastive-based Refer Scoring (CRS) module, which employs contrastive\nlearning at both sentence and word levels to enhance the capability in\nunderstanding and distinguishing referred objects. Additionally, we design a\nMulti-granularity Target Discrimination (MTD) module that fuses object- and\nsemantic-level information to improve the recognition of absent targets.\nExtensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO\n(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and\nmodels are available at https://github.com/Dmmm1997/PropVG.",
      "pdf_url": "http://arxiv.org/pdf/2509.04833v1",
      "published": "2025-09-05T06:30:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04833v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution",
      "authors": [
        "Haosong Liu",
        "Xiancheng Zhu",
        "Huanqiang Zeng",
        "Jianqing Zhu",
        "Jiuwen Cao",
        "Junhui Hou"
      ],
      "abstract": "Recently, Mamba-based methods, with its advantage in long-range information\nmodeling and linear complexity, have shown great potential in optimizing both\ncomputational cost and performance of light field image super-resolution\n(LFSR). However, current multi-directional scanning strategies lead to\ninefficient and redundant feature extraction when applied to complex LF data.\nTo overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS)\nstrategy, based on which we design the Subspace Simple Mamba Block (SSMB) to\nachieve more efficient and precise feature extraction. Furthermore, we propose\na dual-stage modeling strategy to address the limitation of state space in\npreserving spatial-angular and disparity information, thereby enabling a more\ncomprehensive exploration of non-local spatial-angular correlations.\nSpecifically, in stage I, we introduce the Spatial-Angular Residual Subspace\nMamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage\nII, we use a dual-branch parallel structure combining the Epipolar Plane Mamba\nBlock (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar\nfeature refinement. Building upon meticulously designed modules and strategies,\nwe introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates\nthe strengths of Mamba and Transformer models for LFSR, enabling comprehensive\ninformation exploration across spatial, angular, and epipolar-plane domains.\nExperimental results demonstrate that LFMT significantly outperforms current\nstate-of-the-art methods in LFSR, achieving substantial improvements in\nperformance while maintaining low computational complexity on both real-word\nand synthetic LF datasets.",
      "pdf_url": "http://arxiv.org/pdf/2509.04824v1",
      "published": "2025-09-05T05:50:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04824v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models",
      "authors": [
        "Haechang Kim",
        "Hao Chen",
        "Can Li",
        "Jong Min Lee"
      ],
      "abstract": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain.",
      "pdf_url": "http://arxiv.org/pdf/2509.04809v1",
      "published": "2025-09-05T05:09:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04809v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design",
      "authors": [
        "Keqin Zhang"
      ],
      "abstract": "Modern fronthaul links in wireless systems must transport high-dimensional\nsignals under stringent bandwidth and latency constraints, which makes\ncompression indispensable. Traditional strategies such as compressed sensing,\nscalar quantization, and fixed-codec pipelines often rely on restrictive\npriors, degrade sharply at high compression ratios, and are hard to tune across\nchannels and deployments. Recent progress in Artificial Intelligence (AI) has\nbrought end-to-end learned transforms, vector and hierarchical quantization,\nand learned entropy models that better exploit the structure of Channel State\nInformation(CSI), precoding matrices, I/Q samples, and LLRs. This paper first\nsurveys AI-driven compression techniques and then provides a focused analysis\nof two representative high-compression routes: CSI feedback with end-to-end\nlearning and Resource Block (RB) granularity precoding optimization combined\nwith compression. Building on these insights, we propose a fronthaul\ncompression strategy tailored to cell-free architectures. The design targets\nhigh compression with controlled performance loss, supports RB-level rate\nadaptation, and enables low-latency inference suitable for centralized\ncooperative transmission in next-generation networks.",
      "pdf_url": "http://arxiv.org/pdf/2509.04805v1",
      "published": "2025-09-05T04:52:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04805v1",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images",
      "authors": [
        "Asif Newaz",
        "Masum Mushfiq Ishti",
        "A Z M Ashraful Azam",
        "Asif Ur Rahman Adib"
      ],
      "abstract": "Skin diseases are among the most prevalent health concerns worldwide, yet\nconventional diagnostic methods are often costly, complex, and unavailable in\nlow-resource settings. Automated classification using deep learning has emerged\nas a promising alternative, but existing studies are mostly limited to\ndermoscopic datasets and a narrow range of disease classes. In this work, we\ncurate a large dataset of over 50 skin disease categories captured with mobile\ndevices, making it more representative of real-world conditions. We evaluate\nmultiple convolutional neural networks and Transformer-based architectures,\ndemonstrating that Transformer models, particularly the Swin Transformer,\nachieve superior performance by effectively capturing global contextual\nfeatures. To enhance interpretability, we incorporate Gradient-weighted Class\nActivation Mapping (Grad-CAM), which highlights clinically relevant regions and\nprovides transparency in model predictions. Our results underscore the\npotential of Transformer-based approaches for mobile-acquired skin lesion\nclassification, paving the way toward accessible AI-assisted dermatological\nscreening and early diagnosis in resource-limited environments.",
      "pdf_url": "http://arxiv.org/pdf/2509.04800v1",
      "published": "2025-09-05T04:31:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04800v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking",
      "authors": [
        "Yuan Sui",
        "Yanming Zhang",
        "Yi Liao",
        "Yu Gu",
        "Guohua Tang",
        "Zhongqian Sun",
        "Wei Yang",
        "Bryan Hooi"
      ],
      "abstract": "Large language models (LLMs) excel at processing information reactively but\nlack the ability to systemically explore hypothetical futures. They cannot ask,\n\"what if we take this action? how will it affect the final outcome\" and\nforecast its potential consequences before acting. This critical gap limits\ntheir utility in dynamic, high-stakes scenarios like strategic planning, risk\nassessment, and real-time decision making. To bridge this gap, we propose\nWiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.\nOur approach integrates What-If Analysis (WIA), a systematic approach for\nevaluating hypothetical scenarios by changing input variables. By leveraging\nenvironmental feedback via reinforcement learning, WiA-LLM moves beyond\nreactive thinking. It dynamically simulates the outcomes of each potential\naction, enabling the model to anticipate future states rather than merely react\nto the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a\ncomplex multiplayer game environment characterized by rapid state changes and\nintricate interactions. The game's real-time state changes require precise\nmulti-step consequence prediction, making it an ideal testbed for our approach.\nExperimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy\nin forecasting game-state changes (up to two times gain over baselines). The\nmodel shows particularly significant gains in high-difficulty scenarios where\naccurate foresight is critical. To our knowledge, this is the first work to\nformally explore and integrate what-if analysis capabilities within LLMs.\nWiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,\nproviding a scalable framework for robust decision-making in dynamic\nenvironments with broad implications for strategic applications.",
      "pdf_url": "http://arxiv.org/pdf/2509.04791v1",
      "published": "2025-09-05T04:05:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04791v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Graph Unlearning: Efficient Node Removal in Graph Neural Networks",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Zhoutian Wang",
        "Wei Ren",
        "Wanlei Zhou"
      ],
      "abstract": "With increasing concerns about privacy attacks and potential sensitive\ninformation leakage, researchers have actively explored methods to efficiently\nremove sensitive training data and reduce privacy risks in graph neural network\n(GNN) models. Node unlearning has emerged as a promising technique for\nprotecting the privacy of sensitive nodes by efficiently removing specific\ntraining node information from GNN models. However, existing node unlearning\nmethods either impose restrictions on the GNN structure or do not effectively\nutilize the graph topology for node unlearning. Some methods even compromise\nthe graph's topology, making it challenging to achieve a satisfactory\nperformance-complexity trade-off. To address these issues and achieve efficient\nunlearning for training node removal in GNNs, we propose three novel node\nunlearning methods: Class-based Label Replacement, Topology-guided Neighbor\nMean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among\nthese methods, Topology-guided Neighbor Mean Posterior Probability and\nClass-consistent Neighbor Node Filtering effectively leverage the topological\nfeatures of the graph, resulting in more effective node unlearning. To validate\nthe superiority of our proposed methods in node unlearning, we conducted\nexperiments on three benchmark datasets. The evaluation criteria included model\nutility, unlearning utility, and unlearning efficiency. The experimental\nresults demonstrate the utility and efficiency of the proposed methods and\nillustrate their superiority compared to state-of-the-art node unlearning\nmethods. Overall, the proposed methods efficiently remove sensitive training\nnodes and protect the privacy information of sensitive nodes in GNNs. The\nfindings contribute to enhancing the privacy and security of GNN models and\nprovide valuable insights into the field of node unlearning.",
      "pdf_url": "http://arxiv.org/pdf/2509.04785v1",
      "published": "2025-09-05T03:47:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04785v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Diversity in Large Language Models via Determinantal Point Processes",
      "authors": [
        "Yilei Chen",
        "Souradip Chakraborty",
        "Lorenz Wolf",
        "Ioannis Ch. Paschalidis",
        "Aldo Pacchiano"
      ],
      "abstract": "Supervised fine-tuning and reinforcement learning are two popular methods for\npost-training large language models (LLMs). While improving the model's\nperformance on downstream tasks, they often reduce the model's output\ndiversity, leading to narrow, canonical responses. Existing methods to enhance\ndiversity are limited, either by operating at inference time or by focusing on\nlexical differences. We propose a novel training method named DQO based on\ndeterminantal point processes (DPPs) to jointly optimize LLMs for quality and\nsemantic diversity. Our approach samples and embeds a group of responses for\neach prompt, then uses the determinant of a kernel-based similarity matrix to\nmeasure diversity as the volume spanned by the embeddings of these responses.\nExperiments across instruction-following, summarization, story generation, and\nreasoning tasks demonstrate that our method substantially improves semantic\ndiversity without sacrificing model quality.",
      "pdf_url": "http://arxiv.org/pdf/2509.04784v1",
      "published": "2025-09-05T03:47:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04784v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "VARMA-Enhanced Transformer for Time Series Forecasting",
      "authors": [
        "Jiajun Song",
        "Xiaoou Liu"
      ],
      "abstract": "Transformer-based models have significantly advanced time series forecasting.\nRecent work, like the Cross-Attention-only Time Series transformer (CATS),\nshows that removing self-attention can make the model more accurate and\nefficient. However, these streamlined architectures may overlook the\nfine-grained, local temporal dependencies effectively captured by classical\nstatistical models like Vector AutoRegressive Moving Average model (VARMA). To\naddress this gap, we propose VARMAformer, a novel architecture that synergizes\nthe efficiency of a cross-attention-only framework with the principles of\nclassical time series analysis. Our model introduces two key innovations: (1) a\ndedicated VARMA-inspired Feature Extractor (VFE) that explicitly models\nautoregressive (AR) and moving-average (MA) patterns at the patch level, and\n(2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal\ngate to make queries more context-aware. By fusing these classical insights\ninto a modern backbone, VARMAformer captures both global, long-range\ndependencies and local, statistical structures. Through extensive experiments\non widely-used benchmark datasets, we demonstrate that our model consistently\noutperforms existing state-of-the-art methods. Our work validates the\nsignificant benefit of integrating classical statistical insights into modern\ndeep learning frameworks for time series forecasting.",
      "pdf_url": "http://arxiv.org/pdf/2509.04782v1",
      "published": "2025-09-05T03:32:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.04782v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}