{
  "last_updated": "2025-09-11T00:47:57.249709",
  "papers": [
    {
      "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search",
      "authors": [
        "Xin Lai",
        "Junyi Li",
        "Wei Li",
        "Tao Liu",
        "Tianjian Li",
        "Hengshuang Zhao"
      ],
      "abstract": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems.",
      "pdf_url": "http://arxiv.org/pdf/2509.07969v1",
      "published": "2025-09-09T17:54:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07969v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare",
      "authors": [
        "Valen Tagliabue",
        "Leonard Dung"
      ],
      "abstract": "We develop new experimental paradigms for measuring welfare in language\nmodels. We compare verbal reports of models about their preferences with\npreferences expressed through behavior when navigating a virtual environment\nand selecting conversation topics. We also test how costs and rewards affect\nbehavior and whether responses to an eudaimonic welfare scale - measuring\nstates such as autonomy and purpose in life - are consistent across\nsemantically equivalent prompts. Overall, we observed a notable degree of\nmutual support between our measures. The reliable correlations observed between\nstated preferences and behavior across conditions suggest that preference\nsatisfaction can, in principle, serve as an empirically measurable welfare\nproxy in some of today's AI systems. Furthermore, our design offered an\nilluminating setting for qualitative observation of model behavior. Yet, the\nconsistency between measures was more pronounced in some models and conditions\nthan others and responses were not consistent across perturbations. Due to\nthis, and the background uncertainty about the nature of welfare and the\ncognitive states (and welfare subjecthood) of language models, we are currently\nuncertain whether our methods successfully measure the welfare state of\nlanguage models. Nevertheless, these findings highlight the feasibility of\nwelfare measurement in language models, inviting further exploration.",
      "pdf_url": "http://arxiv.org/pdf/2509.07961v1",
      "published": "2025-09-09T17:48:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07961v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ACE and Diverse Generalization via Selective Disagreement",
      "authors": [
        "Oliver Daniels",
        "Stuart Armstrong",
        "Alexandre Maranhão",
        "Mahirah Fairuz Rahman",
        "Benjamin M. Marlin",
        "Rebecca Gorman"
      ],
      "abstract": "Deep neural networks are notoriously sensitive to spurious correlations -\nwhere a model learns a shortcut that fails out-of-distribution. Existing work\non spurious correlations has often focused on incomplete\ncorrelations,leveraging access to labeled instances that break the correlation.\nBut in cases where the spurious correlations are complete, the correct\ngeneralization is fundamentally \\textit{underspecified}. To resolve this\nunderspecification, we propose learning a set of concepts that are consistent\nwith training data but make distinct predictions on a subset of novel unlabeled\ninputs. Using a self-training approach that encourages \\textit{confident} and\n\\textit{selective} disagreement, our method ACE matches or outperforms existing\nmethods on a suite of complete-spurious correlation benchmarks, while remaining\nrobust to incomplete spurious correlations. ACE is also more configurable than\nprior approaches, allowing for straight-forward encoding of prior knowledge and\nprincipled unsupervised model selection. In an early application to\nlanguage-model alignment, we find that ACE achieves competitive performance on\nthe measurement tampering detection benchmark \\textit{without} access to\nuntrusted measurements. While still subject to important limitations, ACE\nrepresents significant progress towards overcoming underspecification.",
      "pdf_url": "http://arxiv.org/pdf/2509.07955v1",
      "published": "2025-09-09T17:43:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07955v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges",
      "authors": [
        "Kasra Borazjani",
        "Naji Khosravan",
        "Rajeev Sahay",
        "Bita Akram",
        "Seyyedali Hosseinalipour"
      ],
      "abstract": "Multi-modal multi-task (M3T) foundation models (FMs) have recently shown\ntransformative potential in artificial intelligence, with emerging applications\nin education. However, their deployment in real-world educational settings is\nhindered by privacy regulations, data silos, and limited domain-specific data\navailability. We introduce M3T Federated Foundation Models (FedFMs) for\neducation: a paradigm that integrates federated learning (FL) with M3T FMs to\nenable collaborative, privacy-preserving training across decentralized\ninstitutions while accommodating diverse modalities and tasks. Subsequently,\nthis position paper aims to unveil M3T FedFMs as a promising yet underexplored\napproach to the education community, explore its potentials, and reveal its\nrelated future research directions. We outline how M3T FedFMs can advance three\ncritical pillars of next-generation intelligent education systems: (i) privacy\npreservation, by keeping sensitive multi-modal student and institutional data\nlocal; (ii) personalization, through modular architectures enabling tailored\nmodels for students, instructors, and institutions; and (iii) equity and\ninclusivity, by facilitating participation from underrepresented and\nresource-constrained entities. We finally identify various open research\nchallenges, including studying of (i) inter-institution heterogeneous privacy\nregulations, (ii) the non-uniformity of data modalities' characteristics, (iii)\nthe unlearning approaches for M3T FedFMs, (iv) the continual learning\nframeworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must\nbe collectively addressed for practical deployment.",
      "pdf_url": "http://arxiv.org/pdf/2509.07946v1",
      "published": "2025-09-09T17:31:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07946v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented Code Generation",
      "authors": [
        "Kai Ye",
        "Liangcai Su",
        "Chenxiong Qian"
      ],
      "abstract": "Code generation has emerged as a pivotal capability of Large Language\nModels(LLMs), revolutionizing development efficiency for programmers of all\nskill levels. However, the complexity of data structures and algorithmic logic\noften results in functional deficiencies and security vulnerabilities in\ngenerated code, reducing it to a prototype requiring extensive manual\ndebugging. While Retrieval-Augmented Generation (RAG) can enhance correctness\nand security by leveraging external code manuals, it simultaneously introduces\nnew attack surfaces.\n  In this paper, we pioneer the exploration of attack surfaces in\nRetrieval-Augmented Code Generation (RACG), focusing on malicious dependency\nhijacking. We demonstrate how poisoned documentation containing hidden\nmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting\ndual trust chains: LLM reliance on RAG and developers' blind trust in LLM\nsuggestions. To construct poisoned documents, we propose ImportSnare, a novel\nattack framework employing two synergistic strategies: 1)Position-aware beam\nsearch optimizes hidden ranking sequences to elevate poisoned documents in\nretrieval results, and 2)Multilingual inductive suggestions generate\njailbreaking sequences to manipulate LLMs into recommending malicious\ndependencies. Through extensive experiments across Python, Rust, and\nJavaScript, ImportSnare achieves significant attack success rates (over 50% for\npopular libraries such as matplotlib and seaborn) in general, and is also able\nto succeed even when the poisoning ratio is as low as 0.01%, targeting both\ncustom and real-world malicious packages. Our findings reveal critical supply\nchain risks in LLM-powered development, highlighting inadequate security\nalignment for code generation tasks. To support future research, we will\nrelease the multilingual benchmark suite and datasets. The project homepage is\nhttps://importsnare.github.io.",
      "pdf_url": "http://arxiv.org/pdf/2509.07941v1",
      "published": "2025-09-09T17:21:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07941v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation",
      "authors": [
        "Wanni Vidulige Ishan Perera",
        "Xing Liu",
        "Fan liang",
        "Junyi Zhang"
      ],
      "abstract": "The rapid evolution of Artificial Intelligence (AI) and Large Language Models\n(LLMs) has opened up new opportunities in the area of cybersecurity, especially\nin the exploitation automation landscape and penetration testing. This study\nexplores Android penetration testing automation using LLM-based tools,\nespecially PentestGPT, to identify and execute rooting techniques. Through a\ncomparison of the traditional manual rooting process and exploitation methods\nproduced using AI, this study evaluates the efficacy, reliability, and\nscalability of automated penetration testing in achieving high-level privilege\naccess on Android devices. With the use of an Android emulator (Genymotion) as\nthe testbed, we fully execute both traditional and exploit-based rooting\nmethods, automating the process using AI-generated scripts. Secondly, we create\na web application by integrating OpenAI's API to facilitate automated script\ngeneration from LLM-processed responses. The research focuses on the\neffectiveness of AI-enabled exploitation by comparing automated and manual\npenetration testing protocols, by determining LLM weaknesses and strengths\nalong the way. We also provide security suggestions of AI-enabled exploitation,\nincluding ethical factors and potential misuse. The findings exhibit that while\nLLMs can significantly streamline the workflow of exploitation, they need to be\ncontrolled by humans to ensure accuracy and ethical application. This study\nadds to the increasing body of literature on AI-powered cybersecurity and its\neffect on ethical hacking, security research, and mobile device security.",
      "pdf_url": "http://arxiv.org/pdf/2509.07933v1",
      "published": "2025-09-09T17:17:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07933v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s",
      "authors": [
        "Mahmudul Islam Masum",
        "Miad Islam",
        "Arif I. Sarwat"
      ],
      "abstract": "As local AI grows in popularity, there is a critical gap between the\nbenchmark performance of object detectors and their practical viability on\nconsumer-grade hardware. While models like YOLOv10s promise real-time speeds,\nthese metrics are typically achieved on high-power, desktop-class GPUs. This\npaper reveals that on resource-constrained systems, such as laptops with RTX\n4060 GPUs, performance is not compute-bound but is instead dominated by\nsystem-level bottlenecks, as illustrated by a simple bottleneck test. To\novercome this hardware-level constraint, we introduce a Two-Pass Adaptive\nInference algorithm, a model-independent approach that requires no\narchitectural changes. This study mainly focuses on adaptive inference\nstrategies and undertakes a comparative analysis of architectural early-exit\nand resolution-adaptive routing, highlighting their respective trade-offs\nwithin a unified evaluation framework. The system uses a fast, low-resolution\npass and only escalates to a high-resolution model pass when detection\nconfidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x\nspeedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%.\nThis work provides a practical and reproducible blueprint for deploying\nhigh-performance, real-time AI on consumer-grade devices by shifting the focus\nfrom pure model optimization to hardware-aware inference strategies that\nmaximize throughput.",
      "pdf_url": "http://arxiv.org/pdf/2509.07928v1",
      "published": "2025-09-09T17:13:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07928v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models",
      "authors": [
        "Tuo Wang",
        "Adithya Kulkarni",
        "Tyler Cody",
        "Peter A. Beling",
        "Yujun Yan",
        "Dawei Zhou"
      ],
      "abstract": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ.",
      "pdf_url": "http://arxiv.org/pdf/2509.07925v1",
      "published": "2025-09-09T17:07:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07925v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation",
      "authors": [
        "Moo Hyun Son",
        "Juyoung Bae",
        "Zelin Qiu",
        "Jiale Peng",
        "Kai Xin Li",
        "Yifan Lin",
        "Hao Chen"
      ],
      "abstract": "Digital dentistry represents a transformative shift in modern dental\npractice. The foundational step in this transformation is the accurate digital\nrepresentation of the patient's dentition, which is obtained from segmented\nCone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the\ngrowing interest in digital dental technologies, existing segmentation\nmethodologies frequently lack rigorous validation and demonstrate limited\nperformance and clinical applicability. To the best of our knowledge, this is\nthe first work to introduce a multimodal pretraining framework for tooth\nsegmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for\npretraining that integrates volumetric (CBCT) and surface-based (IOS)\nmodalities. By capturing modality-invariant representations through multimodal\ncontrastive learning, our approach effectively models fine-grained anatomical\nfeatures, enabling precise multi-class segmentation and accurate identification\nof F\\'ed\\'eration Dentaire Internationale (FDI) tooth numbering. Along with the\nframework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to\ndate, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive\ncollection of independent datasets, representing the largest and most diverse\nevaluation to date. Our method achieves state-of-the-art performance in both\ninternal and external testing, with an increase of 12\\% for CBCT segmentation\nand 8\\% for IOS segmentation in the Dice Similarity Coefficient (DSC).\nFurthermore, ToothMCL consistently surpasses existing approaches in tooth\ngroups and demonstrates robust generalizability across varying imaging\nconditions and clinical scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2509.07923v1",
      "published": "2025-09-09T17:05:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07923v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems",
      "authors": [
        "Arun Verma",
        "Zhaoxuan Wu",
        "Zijian Zhou",
        "Xiaoqiang Lin",
        "Zhiliang Chen",
        "Rachael Hwee Ling Sim",
        "Rui Qiao",
        "Jingtan Wang",
        "Nhung Bui",
        "Xinyuan Niu",
        "Wenyang Hu",
        "Gregory Kang Ruey Lau",
        "Zi-Yu Khoo",
        "Zitong Zhao",
        "Xinyi Xu",
        "Apivich Hemachandra",
        "See-Kiong Ng",
        "Bryan Kian Hsiang Low"
      ],
      "abstract": "Large Language Models (LLMs) are large-scale pretrained models that have\nachieved remarkable success across diverse domains. These successes have been\ndriven by unprecedented complexity and scale in both data and computations.\nHowever, due to the high costs of training such models, brute-force\ntrial-and-error approaches to improve LLMs are not feasible. Inspired by the\nsuccess of inverse problems in uncovering fundamental scientific laws, this\nposition paper advocates that inverse problems can also efficiently uncover\nscaling laws that guide the building of LLMs to achieve the desirable\nperformance with significantly better cost-effectiveness.",
      "pdf_url": "http://arxiv.org/pdf/2509.07909v1",
      "published": "2025-09-09T16:53:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07909v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?",
      "authors": [
        "Fangchen Yu",
        "Haiyuan Wan",
        "Qianjia Cheng",
        "Yuchen Zhang",
        "Jiacheng Chen",
        "Fujun Han",
        "Yulun Wu",
        "Junchi Yao",
        "Ruilizhen Hu",
        "Ning Ding",
        "Yu Cheng",
        "Tao Chen",
        "Lei Bai",
        "Dongzhan Zhou",
        "Yun Luo",
        "Ganqu Cui",
        "Peng Ye"
      ],
      "abstract": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight a substantial performance gap between open-source models and\ntop students, the strong physical reasoning capabilities of closed-source\nreasoning models, and the fact that there is still significant room for\nimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused\nbenchmark for advancing multimodal physical reasoning, is open-source and\navailable at https://github.com/SciYu/HiPhO.",
      "pdf_url": "http://arxiv.org/pdf/2509.07894v2",
      "published": "2025-09-09T16:24:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07894v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning",
      "authors": [
        "Daniel DeAlcala",
        "Aythami Morales",
        "Julian Fierrez",
        "Gonzalo Mancera",
        "Ruben Tolosana",
        "Javier Ortega-Garcia"
      ],
      "abstract": "Active Membership Inference Test (aMINT) is a method designed to detect\nwhether given data were used during the training of machine learning models. In\nActive MINT, we propose a novel multitask learning process that involves\ntraining simultaneously two models: the original or Audited Model, and a\nsecondary model, referred to as the MINT Model, responsible for identifying the\ndata used for training the Audited Model. This novel multi-task learning\napproach has been designed to incorporate the auditability of the model as an\noptimization objective during the training process of neural networks. The\nproposed approach incorporates intermediate activation maps as inputs to the\nMINT layers, which are trained to enhance the detection of training data. We\npresent results using a wide range of neural networks, from lighter\narchitectures such as MobileNet to more complex ones such as Vision\nTransformers, evaluated in 5 public benchmarks. Our proposed Active MINT\nachieves over 80% accuracy in detecting if given data was used for training,\nsignificantly outperforming previous approaches in the literature. Our aMINT\nand related methodological developments contribute to increasing transparency\nin AI models, facilitating stronger safeguards in AI deployments to achieve\nproper security, privacy, and copyright protection.",
      "pdf_url": "http://arxiv.org/pdf/2509.07879v1",
      "published": "2025-09-09T16:00:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07879v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models",
      "authors": [
        "Augustin Crespin",
        "Ioannis Kostis",
        "Hélène Verhaeghe",
        "Pierre Schaus"
      ],
      "abstract": "Constraint Programming and its high-level modeling languages have long been\nrecognized for their potential to achieve the holy grail of problem-solving.\nHowever, the complexity of modeling languages, the large number of global\nconstraints, and the art of creating good models have often hindered\nnon-experts from choosing CP to solve their combinatorial problems. While\ngenerating an expert-level model from a natural-language description of a\nproblem would be the dream, we are not yet there. We propose a tutoring system\ncalled CP-Model-Zoo, exploiting expert-written models accumulated through the\nyears. CP-Model-Zoo retrieves the closest source code model from a database\nbased on a user's natural language description of a combinatorial problem. It\nensures that expert-validated models are presented to the user while\neliminating the need for human data labeling. Our experiments show excellent\naccuracy in retrieving the correct model based on a user-input description of a\nproblem simulated with different levels of expertise.",
      "pdf_url": "http://arxiv.org/pdf/2509.07867v1",
      "published": "2025-09-09T15:55:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07867v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs",
      "authors": [
        "Xinyu Zhang",
        "Changzhi Zhou",
        "Linmei Hu",
        "Luhao Zhang",
        "Xiancai Chen",
        "Haomin Fu",
        "Yang Yang",
        "Mengdi Zhang"
      ],
      "abstract": "Existing code large language models (LLMs) often rely on large-scale\ninstruction data distilled from proprietary LLMs for fine-tuning, which\ntypically incurs high costs. In this paper, we explore the potential of\nsmall-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code\ninstruction data construction. We first observe that the data synthesis\ncapability of small-scale LLMs can be enhanced by training on a few superior\ndata synthesis samples from proprietary LLMs. Building on this, we propose a\nnovel iterative self-distillation approach to bootstrap small-scale LLMs,\ntransforming them into powerful synthesizers that reduce reliance on\nproprietary LLMs and minimize costs. Concretely, in each iteration, to obtain\ndiverse and high-quality self-distilled data, we design multi-checkpoint\nsampling and multi-aspect scoring strategies for initial data selection.\nFurthermore, to identify the most influential samples, we introduce a\ngradient-based influence estimation method for final data filtering. Based on\nthe code instruction datasets from the small-scale synthesizers, we develop\nSCoder, a family of code generation models fine-tuned from DeepSeek-Coder.\nSCoder models achieve state-of-the-art code generation capabilities,\ndemonstrating the effectiveness of our method.",
      "pdf_url": "http://arxiv.org/pdf/2509.07858v1",
      "published": "2025-09-09T15:38:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07858v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets",
      "authors": [
        "Seyd Teymoor Seydi"
      ],
      "abstract": "Accurate and timely mapping of burned areas is crucial for environmental\nmonitoring, disaster management, and assessment of climate change. This study\npresents a novel approach to automated burned area mapping using the AlphaEArth\ndataset combined with the Siamese U-Net deep learning architecture. The\nAlphaEArth Dataset, comprising high-resolution optical and thermal infrared\nimagery with comprehensive ground-truth annotations, provides an unprecedented\nresource for training robust burned area detection models. We trained our model\nwith the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US\nand evaluated it with 17 regions cross in Europe. Our experimental results\ndemonstrate that the proposed ensemble approach achieves superior performance\nwith an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test\ndataset. The model successfully identifies burned areas across diverse\necosystems with complex background, showing particular strength in detecting\npartially burned vegetation and fire boundaries and its transferability and\nhigh generalization in burned area mapping. This research contributes to the\nadvancement of automated fire damage assessment and provides a scalable\nsolution for global burn area monitoring using the AlphaEarth dataset.",
      "pdf_url": "http://arxiv.org/pdf/2509.07852v1",
      "published": "2025-09-09T15:29:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07852v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study",
      "authors": [
        "Amay Jain",
        "Liu Cui",
        "Si Chen"
      ],
      "abstract": "Large language models like ChatGPT are increasingly used in classrooms, but\nthey often provide outdated or fabricated information that can mislead\nstudents. Retrieval Augmented Generation (RAG) improves reliability of LLMs by\ngrounding responses in external resources. We investigate two accessible RAG\nparadigms, vector-based retrieval and graph-based retrieval to identify best\npractices for classroom question answering (QA). Existing comparative studies\nfail to account for pedagogical factors such as educational disciplines,\nquestion types, and practical deployment costs. Using a novel dataset,\nEduScopeQA, of 3,176 questions across academic subjects, we measure performance\non various educational query types, from specific facts to broad thematic\ndiscussions. We also evaluate system alignment with a dataset of systematically\naltered textbooks that contradict the LLM's latent knowledge. We find that\nOpenAI Vector Search RAG (representing vector-based RAG) performs well as a\nlow-cost generalist, especially for quick fact retrieval. On the other hand,\nGraphRAG Global excels at providing pedagogically rich answers to thematic\nqueries, and GraphRAG Local achieves the highest accuracy with the dense,\naltered textbooks when corpus integrity is critical. Accounting for the 10-20x\nhigher resource usage of GraphRAG (representing graph-based RAG), we show that\na dynamic branching framework that routes queries to the optimal retrieval\nmethod boosts fidelity and efficiency. These insights provide actionable\nguidelines for educators and system designers to integrate RAG-augmented LLMs\ninto learning environments effectively.",
      "pdf_url": "http://arxiv.org/pdf/2509.07846v1",
      "published": "2025-09-09T15:22:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07846v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost",
      "authors": [
        "Mihai Nadas",
        "Laura Diosan",
        "Andreea Tomescu",
        "Andrei Piscoran"
      ],
      "abstract": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings.",
      "pdf_url": "http://arxiv.org/pdf/2509.07829v1",
      "published": "2025-09-09T15:07:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07829v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach",
      "authors": [
        "João Paulo Nogueira",
        "Wentao Sun",
        "Alonso Silva",
        "Laith Zumot"
      ],
      "abstract": "The rise of large reasoning language models (LRLMs) has unlocked new\npotential for solving complex tasks. These models operate with a thinking\nbudget, that is, a predefined number of reasoning tokens used to arrive at a\nsolution. We propose a novel approach, inspired by the generator/discriminator\nframework in generative adversarial networks, in which a critic model\nperiodically probes its own reasoning to assess whether it has reached a\nconfident conclusion. If not, reasoning continues until a target certainty\nthreshold is met. This mechanism adaptively balances efficiency and reliability\nby allowing early termination when confidence is high, while encouraging\nfurther reasoning when uncertainty persists. Through experiments on the\nAIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)\nimproves baseline accuracy while reducing token usage. Importantly, extended\nmulti-seed evaluations over 64 runs demonstrate that CGR is stable, reducing\nvariance across seeds and improving exam-like performance under penalty-based\ngrading. Additionally, our token savings analysis shows that CGR can eliminate\nmillions of tokens in aggregate, with tunable trade-offs between certainty\nthresholds and efficiency. Together, these findings highlight certainty as a\npowerful signal for reasoning sufficiency. By integrating confidence into the\nreasoning process, CGR makes large reasoning language models more adaptive,\ntrustworthy, and resource efficient, paving the way for practical deployment in\ndomains where both accuracy and computational cost matter.",
      "pdf_url": "http://arxiv.org/pdf/2509.07820v1",
      "published": "2025-09-09T14:57:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07820v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models",
      "authors": [
        "Jonathan Teagan"
      ],
      "abstract": "This study applies a range of forecasting techniques,including ARIMA,\nProphet, Long Short Term Memory networks (LSTM), Temporal Convolutional\nNetworks (TCN), and XGBoost, to model and predict Russian equipment losses\nduring the ongoing war in Ukraine. Drawing on daily and monthly open-source\nintelligence (OSINT) data from WarSpotting, we aim to assess trends in\nattrition, evaluate model performance, and estimate future loss patterns\nthrough the end of 2025. Our findings show that deep learning models,\nparticularly TCN and LSTM, produce stable and consistent forecasts, especially\nunder conditions of high temporal granularity. By comparing different model\narchitectures and input structures, this study highlights the importance of\nensemble forecasting in conflict modeling, and the value of publicly available\nOSINT data in quantifying material degradation over time.",
      "pdf_url": "http://arxiv.org/pdf/2509.07813v1",
      "published": "2025-09-09T14:52:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07813v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Enhanced SegNet with Integrated Grad-CAM for Interpretable Retinal Layer Segmentation in OCT Images",
      "authors": [
        "S M Asiful Islam Saky",
        "Ugyen Tshering"
      ],
      "abstract": "Optical Coherence Tomography (OCT) is essential for diagnosing conditions\nsuch as glaucoma, diabetic retinopathy, and age-related macular degeneration.\nAccurate retinal layer segmentation enables quantitative biomarkers critical\nfor clinical decision-making, but manual segmentation is time-consuming and\nvariable, while conventional deep learning models often lack interpretability.\nThis work proposes an improved SegNet-based deep learning framework for\nautomated and interpretable retinal layer segmentation. Architectural\ninnovations, including modified pooling strategies, enhance feature extraction\nfrom noisy OCT images, while a hybrid loss function combining categorical\ncross-entropy and Dice loss improves performance for thin and imbalanced\nretinal layers. Gradient-weighted Class Activation Mapping (Grad-CAM) is\nintegrated to provide visual explanations, allowing clinical validation of\nmodel decisions. Trained and validated on the Duke OCT dataset, the framework\nachieved 95.77% validation accuracy, a Dice coefficient of 0.9446, and a\nJaccard Index (IoU) of 0.8951. Class-wise results confirmed robust performance\nacross most layers, with challenges remaining for thinner boundaries. Grad-CAM\nvisualizations highlighted anatomically relevant regions, aligning segmentation\nwith clinical biomarkers and improving transparency. By combining architectural\nimprovements, a customized hybrid loss, and explainable AI, this study delivers\na high-performing SegNet-based framework that bridges the gap between accuracy\nand interpretability. The approach offers strong potential for standardizing\nOCT analysis, enhancing diagnostic efficiency, and fostering clinical trust in\nAI-driven ophthalmic tools.",
      "pdf_url": "http://arxiv.org/pdf/2509.07795v1",
      "published": "2025-09-09T14:31:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07795v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Individual utilities of life satisfaction reveal inequality aversion unrelated to political alignment",
      "authors": [
        "Crispin Cooper",
        "Ana Fredrich",
        "Tommaso Reggiani",
        "Wouter Poortinga"
      ],
      "abstract": "How should well-being be prioritised in society, and what trade-offs are\npeople willing to make between fairness and personal well-being? We investigate\nthese questions using a stated preference experiment with a nationally\nrepresentative UK sample (n = 300), in which participants evaluated life\nsatisfaction outcomes for both themselves and others under conditions of\nuncertainty. Individual-level utility functions were estimated using an\nExpected Utility Maximisation (EUM) framework and tested for sensitivity to the\noverweighting of small probabilities, as characterised by Cumulative Prospect\nTheory (CPT). A majority of participants displayed concave (risk-averse)\nutility curves and showed stronger aversion to inequality in societal life\nsatisfaction outcomes than to personal risk. These preferences were unrelated\nto political alignment, suggesting a shared normative stance on fairness in\nwell-being that cuts across ideological boundaries. The results challenge use\nof average life satisfaction as a policy metric, and support the development of\nnonlinear utility-based alternatives that more accurately reflect collective\nhuman values. Implications for public policy, well-being measurement, and the\ndesign of value-aligned AI systems are discussed.",
      "pdf_url": "http://arxiv.org/pdf/2509.07793v2",
      "published": "2025-09-09T14:30:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07793v2",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY",
        "q-fin.EC"
      ]
    },
    {
      "title": "XSRD-Net: EXplainable Stroke Relapse Detection",
      "authors": [
        "Christian Gapp",
        "Elias Tappeiner",
        "Martin Welk",
        "Karl Fritscher",
        "Stephanie Mangesius",
        "Constantin Eisenschink",
        "Philipp Deisl",
        "Michael Knoflach",
        "Astrid E. Grams",
        "Elke R. Gizewski",
        "Rainer Schubert"
      ],
      "abstract": "Stroke is the second most frequent cause of death world wide with an annual\nmortality of around 5.5 million. Recurrence rates of stroke are between 5 and\n25% in the first year. As mortality rates for relapses are extraordinarily high\n(40%) it is of utmost importance to reduce the recurrence rates. We address\nthis issue by detecting patients at risk of stroke recurrence at an early stage\nin order to enable appropriate therapy planning. To this end we collected 3D\nintracranial CTA image data and recorded concomitant heart diseases, the age\nand the gender of stroke patients between 2010 and 2024. We trained single- and\nmultimodal deep learning based neural networks for binary relapse detection\n(Task 1) and for relapse free survival (RFS) time prediction together with a\nsubsequent classification (Task 2). The separation of relapse from non-relapse\npatients (Task 1) could be solved with tabular data (AUC on test dataset:\n0.84). However, for the main task, the regression (Task 2), our multimodal\nXSRD-net processed the modalities vision:tabular with 0.68:0.32 according to\nmodality contribution measures. The c-index with respect to relapses for the\nmultimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final,\ndeeper interpretability analysis results could highlight a link between both\nheart diseases (tabular) and carotid arteries (vision) for the detection of\nrelapses and the prediction of the RFS time. This is a central outcome that we\nstrive to strengthen with ongoing data collection and model retraining.",
      "pdf_url": "http://arxiv.org/pdf/2509.07772v1",
      "published": "2025-09-09T14:06:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07772v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.1"
      ]
    },
    {
      "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning",
      "authors": [
        "Michele Joshua Maggini",
        "Dhia Merzougui",
        "Rabiraj Bandyopadhyay",
        "Gaël Dias",
        "Fabrice Maurel",
        "Pablo Gamallo"
      ],
      "abstract": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct.",
      "pdf_url": "http://arxiv.org/pdf/2509.07768v1",
      "published": "2025-09-09T14:01:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07768v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects",
      "authors": [
        "Mikel Robredo",
        "Matteo Esposito",
        "Fabio Palomba",
        "Rafael Peñaloza",
        "Valentina Lenarduzzi"
      ],
      "abstract": "Context. Code refactoring improves software quality without changing external\nbehavior. Despite its advantages, its benefits are hindered by the considerable\ncost of time, resources, and continuous effort it demands. Aim. Understanding\nwhy developers refactor, and which metrics capture these motivations, may\nsupport wider and more effective use of refactoring in practice. Method. We\nperformed a large-scale empirical study to analyze developers refactoring\nactivity, leveraging Large Language Models (LLMs) to identify underlying\nmotivations from version control data, comparing our findings with previous\nmotivations reported in the literature. Results. LLMs matched human judgment in\n80% of cases, but aligned with literature-based motivations in only 47%. They\nenriched 22% of motivations with more detailed rationale, often highlighting\nreadability, clarity, and structural improvements. Most motivations were\npragmatic, focused on simplification and maintainability. While metrics related\nto developer experience and code readability ranked highest, their correlation\nwith motivation categories was weak. Conclusions. We conclude that LLMs\neffectively capture surface-level motivations but struggle with architectural\nreasoning. Their value lies in providing localized explanations, which, when\ncombined with software metrics, can form hybrid approaches. Such integration\noffers a promising path toward prioritizing refactoring more systematically and\nbalancing short-term improvements with long-term architectural goals.",
      "pdf_url": "http://arxiv.org/pdf/2509.07763v1",
      "published": "2025-09-09T13:58:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07763v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ]
    },
    {
      "title": "Spectral and Rhythm Feature Performance Evaluation for Category and Class Level Audio Classification with Deep Convolutional Neural Networks",
      "authors": [
        "Friedrich Wolf-Monheim"
      ],
      "abstract": "Next to decision tree and k-nearest neighbours algorithms deep convolutional\nneural networks (CNNs) are widely used to classify audio data in many domains\nlike music, speech or environmental sounds. To train a specific CNN various\nspectral and rhythm features like mel-scaled spectrograms, mel-frequency\ncepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform\n(STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy\nnormalized statistics (CENS) chromagrams can be used as digital image input\ndata for the neural network. The performance of these spectral and rhythm\nfeatures for audio category level as well as audio class level classification\nis investigated in detail with a deep CNN and the ESC-50 dataset with 2,000\nlabeled environmental audio recordings using an end-to-end deep learning\npipeline. The evaluated metrics accuracy, precision, recall and F1 score for\nmulticlass classification clearly show that the mel-scaled spectrograms and the\nmel-frequency cepstral coefficients (MFCC) perform significantly better then\nthe other spectral and rhythm features investigated in this research for audio\nclassification tasks using deep CNNs.",
      "pdf_url": "http://arxiv.org/pdf/2509.07756v1",
      "published": "2025-09-09T13:54:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07756v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "Enhancing Online Learning by Integrating Biosensors and Multimodal Learning Analytics for Detecting and Predicting Student Behavior: A Review",
      "authors": [
        "Alvaro Becerra",
        "Ruth Cobos",
        "Charles Lang"
      ],
      "abstract": "In modern online learning, understanding and predicting student behavior is\ncrucial for enhancing engagement and optimizing educational outcomes. This\nsystematic review explores the integration of biosensors and Multimodal\nLearning Analytics (MmLA) to analyze and predict student behavior during\ncomputer-based learning sessions. We examine key challenges, including emotion\nand attention detection, behavioral analysis, experimental design, and\ndemographic considerations in data collection. Our study highlights the growing\nrole of physiological signals, such as heart rate, brain activity, and\neye-tracking, combined with traditional interaction data and self-reports to\ngain deeper insights into cognitive states and engagement levels. We synthesize\nfindings from 54 key studies, analyzing commonly used methodologies such as\nadvanced machine learning algorithms and multimodal data pre-processing\ntechniques. The review identifies current research trends, limitations, and\nemerging directions in the field, emphasizing the transformative potential of\nbiosensor-driven adaptive learning systems. Our findings suggest that\nintegrating multimodal data can facilitate personalized learning experiences,\nreal-time feedback, and intelligent educational interventions, ultimately\nadvancing toward a more customized and adaptive online learning experience.",
      "pdf_url": "http://arxiv.org/pdf/2509.07742v1",
      "published": "2025-09-09T13:41:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07742v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis",
      "authors": [
        "Mustafa Kaan Aslan",
        "Reinout Heijungs",
        "Filip Ilievski"
      ],
      "abstract": "Environmental sustainability, particularly in relation to climate change, is\na key concern for consumers, producers, and policymakers. The carbon footprint,\nbased on greenhouse gas emissions, is a standard metric for quantifying the\ncontribution to climate change of activities and is often assessed using life\ncycle assessment (LCA). However, conducting LCA is complex due to opaque and\nglobal supply chains, as well as fragmented data. This paper presents a\nmethodology that combines advances in LCA and publicly available databases with\nknowledge-augmented AI techniques, including retrieval-augmented generation, to\nestimate cradle-to-gate carbon footprints of food products. We introduce a\nchatbot interface that allows users to interactively explore the carbon impact\nof composite meals and relate the results to familiar activities. A live web\ndemonstration showcases our proof-of-concept system with arbitrary food items\nand follow-up questions, highlighting both the potential and limitations - such\nas database uncertainties and AI misinterpretations - of delivering LCA\ninsights in an accessible format.",
      "pdf_url": "http://arxiv.org/pdf/2509.07733v1",
      "published": "2025-09-09T13:34:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07733v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis",
      "authors": [
        "Bo Yu",
        "Zhixiu Hua",
        "Bo Zhao"
      ],
      "abstract": "Background: Parkinson's disease remains a major neurodegenerative disorder\nwith high misdiagnosis rates, primarily due to reliance on clinical rating\nscales. Recent studies have demonstrated a strong association between gut\nmicrobiota and Parkinson's disease, suggesting that microbial composition may\nserve as a promising biomarker. Although deep learning models based ongut\nmicrobiota show potential for early prediction, most approaches rely on single\nclassifiers and often overlook inter-strain correlations or temporal dynamics.\nTherefore, there is an urgent need for more robust feature extraction methods\ntailored to microbiome data. Methods: We proposed BDPM (A Machine\nLearning-Based Feature Extractor for Parkinson's Disease Classification via Gut\nMicrobiota Analysis). First, we collected gut microbiota profiles from 39\nParkinson's patients and their healthy spouses to identify differentially\nabundant taxa. Second, we developed an innovative feature selection framework\nnamed RFRE (Random Forest combined with Recursive Feature Elimination),\nintegrating ecological knowledge to enhance biological interpretability.\nFinally, we designed a hybrid classification model to capture temporal and\nspatial patterns in microbiome data.",
      "pdf_url": "http://arxiv.org/pdf/2509.07723v1",
      "published": "2025-09-09T13:24:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07723v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ]
    },
    {
      "title": "RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning",
      "authors": [
        "Ziye Chen",
        "Chengwei Qin",
        "Yao Shu"
      ],
      "abstract": "As large language models (LLMs) reach high scores on established mathematical\nbenchmarks, such as GSM8K and MATH, the research community has turned to\nInternational Mathematical Olympiad (IMO) problems to push the evaluation\nfrontier. However, existing Olympiad-level benchmarks suffer from practical\nconstraints that introduce grading noise and potential bias, such as\nheterogeneous answer formats requiring model-based judges and a reliance on\npotentially flawed solutions. We introduce RIMO, a two-track benchmark designed\nto preserve peak Olympiad difficulty while eliminating this evaluation noise.\nThe first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique\ninteger answer, allowing for deterministic correctness checking. The second\ntrack, RIMO-P, features 456 proof problems with expert-checked solutions, which\nare decomposed into a sequence of sub-problems to evaluate the step-by-step\nreasoning process via an automated grading system. Our benchmarking of ten\nfrontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these\nsystems excel on older benchmarks, their performance drops sharply on RIMO.\nThese results highlight a substantial gap between current LLM capabilities and\nactual Olympiad-level reasoning. By providing a challenging yet\neasy-to-evaluate suite, RIMO offers a high-resolution yardstick for future\nresearch, presenting a clear target for closing the profound reasoning gap our\nfindings expose.",
      "pdf_url": "http://arxiv.org/pdf/2509.07711v1",
      "published": "2025-09-09T13:13:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07711v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support",
      "authors": [
        "Yildiray Kabak",
        "Gokce B. Laleci Erturkmen",
        "Mert Gencturk",
        "Tuncay Namli",
        "A. Anil Sinaci",
        "Ruben Alcantud Corcoles",
        "Cristina Gomez Ballesteros",
        "Pedro Abizanda",
        "Asuman Dogac"
      ],
      "abstract": "In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health\nLevel 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a\nRetrieval-Augmented Generation (RAG)-based system to improve personalized\nmedical decision support on evidence-based clinical guidelines, emphasizing the\nneed for research in practical applications. In the evolving landscape of\nmedical decision support systems, integrating advanced technologies such as RAG\nand HL7 FHIR can significantly enhance clinical decision-making processes.\nDespite the potential of these technologies, there is limited research on their\nintegration in practical applications.",
      "pdf_url": "http://arxiv.org/pdf/2509.07706v1",
      "published": "2025-09-09T13:10:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07706v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems",
      "authors": [
        "Kamel Kamel",
        "Hridoy Sankar Dutta",
        "Keshav Sood",
        "Sunil Aryal"
      ],
      "abstract": "Voice Authentication Systems (VAS) use unique vocal characteristics for\nverification. They are increasingly integrated into high-security sectors such\nas banking and healthcare. Despite their improvements using deep learning, they\nface severe vulnerabilities from sophisticated threats like deepfakes and\nadversarial attacks. The emergence of realistic voice cloning complicates\ndetection, as systems struggle to distinguish authentic from synthetic audio.\nWhile anti-spoofing countermeasures (CMs) exist to mitigate these risks, many\nrely on static detection models that can be bypassed by novel adversarial\nmethods, leaving a critical security gap. To demonstrate this vulnerability, we\npropose the Spectral Masking and Interpolation Attack (SMIA), a novel method\nthat strategically manipulates inaudible frequency regions of AI-generated\naudio. By altering the voice in imperceptible zones to the human ear, SMIA\ncreates adversarial samples that sound authentic while deceiving CMs. We\nconducted a comprehensive evaluation of our attack against state-of-the-art\n(SOTA) models across multiple tasks, under simulated real-world conditions.\nSMIA achieved a strong attack success rate (ASR) of at least 82% against\ncombined VAS/CM systems, at least 97.5% against standalone speaker verification\nsystems, and 100% against countermeasures. These findings conclusively\ndemonstrate that current security postures are insufficient against adaptive\nadversarial attacks. This work highlights the urgent need for a paradigm shift\ntoward next-generation defenses that employ dynamic, context-aware frameworks\ncapable of evolving with the threat landscape.",
      "pdf_url": "http://arxiv.org/pdf/2509.07677v1",
      "published": "2025-09-09T12:43:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07677v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding",
      "authors": [
        "Jipeng Li",
        "Zeyu Gao",
        "Yubin Qi",
        "Hande Dong",
        "Weijian Chen",
        "Qiang Lin"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable performance across\ndiverse tasks, yet their susceptibility to generating incorrect content during\ninference remains a critical unsolved challenge. While self-correction methods\noffer potential solutions, their effectiveness is hindered by two inherent\nlimitations: (1) the absence of reliable guidance signals for error\nlocalization, and (2) the restricted reasoning depth imposed by conventional\nnext-token decoding paradigms. To address these issues, we propose\nFeedback-Triggered Regeneration (FTR), a novel framework that synergizes user\nfeedback with enhanced decoding dynamics. Specifically, FTR activates response\nregeneration only upon receiving negative user feedback, thereby circumventing\nerror propagation from faulty self-assessment while preserving originally\ncorrect outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,\nwhich enables systematic exploration of multiple reasoning trajectories through\ndelayed sequence evaluation, effectively overcoming the myopic decision-making\ncharacteristic of standard next-token prediction. Extensive experiments on\nmathematical reasoning and code generation benchmarks demonstrate that our\nframework achieves consistent and significant improvements over\nstate-of-the-art prompt-based self-correction methods.",
      "pdf_url": "http://arxiv.org/pdf/2509.07676v1",
      "published": "2025-09-09T12:43:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07676v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "DeepGraphLog for Layered Neurosymbolic AI",
      "authors": [
        "Adem Kikaj",
        "Giuseppe Marra",
        "Floris Geerts",
        "Robin Manhaeve",
        "Luc De Raedt"
      ],
      "abstract": "Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural\nnetworks with the interpretability and structure of symbolic reasoning.\nHowever, current NeSy frameworks like DeepProbLog enforce a fixed flow where\nsymbolic reasoning always follows neural processing. This restricts their\nability to model complex dependencies, especially in irregular data structures\nsuch as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework\nthat extends ProbLog with Graph Neural Predicates. DeepGraphLog enables\nmulti-layer neural-symbolic reasoning, allowing neural and symbolic components\nto be layered in arbitrary order. In contrast to DeepProbLog, which cannot\nhandle symbolic reasoning via neural methods, DeepGraphLog treats symbolic\nrepresentations as graphs, enabling them to be processed by Graph Neural\nNetworks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in\nplanning, knowledge graph completion with distant supervision, and GNN\nexpressivity. Our results demonstrate that DeepGraphLog effectively captures\ncomplex relational dependencies, overcoming key limitations of existing NeSy\nsystems. By broadening the applicability of neurosymbolic AI to\ngraph-structured domains, DeepGraphLog offers a more expressive and flexible\nframework for neural-symbolic integration.",
      "pdf_url": "http://arxiv.org/pdf/2509.07665v1",
      "published": "2025-09-09T12:32:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07665v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment",
      "authors": [
        "Sascha Kaltenpoth",
        "Oliver Müller"
      ],
      "abstract": "Adopting Large language models (LLMs) in organizations potentially\nrevolutionizes our lives and work. However, they can generate off-topic,\ndiscriminating, or harmful content. This AI alignment problem often stems from\nmisspecifications during the LLM adoption, unnoticed by the principal due to\nthe LLM's black-box nature. While various research disciplines investigated AI\nalignment, they neither address the information asymmetries between\norganizational adopters and black-box LLM agents nor consider organizational AI\nadoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led\nAlignment Strategy) a conceptual framework grounded in agency (contract)\ntheory, to mitigate alignment problems during organizational LLM adoption. We\nconduct a conceptual literature analysis using the organizational LLM adoption\nphases and the agency theory as concepts. Our approach results in (1) providing\nan extended literature analysis process specific to AI alignment methods during\norganizational LLM adoption and (2) providing a first LLM alignment\nproblem-solution space.",
      "pdf_url": "http://arxiv.org/pdf/2509.07642v1",
      "published": "2025-09-09T12:10:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07642v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Variational Quantum Circuits in Offline Contextual Bandit Problems",
      "authors": [
        "Lukas Schulte",
        "Daniel Hein",
        "Steffen Udluft",
        "Thomas A. Runkler"
      ],
      "abstract": "This paper explores the application of variational quantum circuits (VQCs)\nfor solving offline contextual bandit problems in industrial optimization\ntasks. Using the Industrial Benchmark (IB) environment, we evaluate the\nperformance of quantum regression models against classical models. Our findings\ndemonstrate that quantum models can effectively fit complex reward functions,\nidentify optimal configurations via particle swarm optimization (PSO), and\ngeneralize well in noisy and sparse datasets. These results provide a proof of\nconcept for utilizing VQCs in offline contextual bandit problems and highlight\ntheir potential in industrial optimization tasks.",
      "pdf_url": "http://arxiv.org/pdf/2509.07633v1",
      "published": "2025-09-09T12:00:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07633v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling",
      "authors": [
        "Minghui Li",
        "Hao Zhang",
        "Yechao Zhang",
        "Wei Wan",
        "Shengshan Hu",
        "pei Xiaobing",
        "Jing Wang"
      ],
      "abstract": "Direct Prompt Injection (DPI) attacks pose a critical security threat to\nLarge Language Models (LLMs) due to their low barrier of execution and high\npotential damage. To address the impracticality of existing white-box/gray-box\nmethods and the poor transferability of black-box methods, we propose an\nactivations-guided prompt injection attack framework. We first construct an\nEnergy-based Model (EBM) using activations from a surrogate model to evaluate\nthe quality of adversarial prompts. Guided by the trained EBM, we employ the\ntoken-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize\nadversarial prompts, thereby enabling gradient-free black-box attacks.\nExperimental results demonstrate our superior cross-model transferability,\nachieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%\nimprovement over human-crafted prompts, and maintaining 36.6% ASR on unseen\ntask scenarios. Interpretability analysis reveals a correlation between\nactivations and attack effectiveness, highlighting the critical role of\nsemantic patterns in transferable vulnerability exploitation.",
      "pdf_url": "http://arxiv.org/pdf/2509.07617v1",
      "published": "2025-09-09T11:42:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07617v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "From Classical Data to Quantum Advantage -- Quantum Policy Evaluation on Quantum Hardware",
      "authors": [
        "Daniel Hein",
        "Simon Wiedemann",
        "Markus Baumann",
        "Patrik Felbinger",
        "Justin Klein",
        "Maximilian Schieder",
        "Jonas Stein",
        "Daniëlle Schuman",
        "Thomas Cope",
        "Steffen Udluft"
      ],
      "abstract": "Quantum policy evaluation (QPE) is a reinforcement learning (RL) algorithm\nwhich is quadratically more efficient than an analogous classical Monte Carlo\nestimation. It makes use of a direct quantum mechanical realization of a finite\nMarkov decision process, in which the agent and the environment are modeled by\nunitary operators and exchange states, actions, and rewards in superposition.\nPreviously, the quantum environment has been implemented and parametrized\nmanually for an illustrative benchmark using a quantum simulator. In this\npaper, we demonstrate how these environment parameters can be learned from a\nbatch of classical observational data through quantum machine learning (QML) on\nquantum hardware. The learned quantum environment is then applied in QPE to\nalso compute policy evaluations on quantum hardware. Our experiments reveal\nthat, despite challenges such as noise and short coherence times, the\nintegration of QML and QPE shows promising potential for achieving quantum\nadvantage in RL.",
      "pdf_url": "http://arxiv.org/pdf/2509.07614v1",
      "published": "2025-09-09T11:36:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07614v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques",
      "authors": [
        "Ali Nawaz",
        "Amir Ahmad",
        "Shehroz S. Khan"
      ],
      "abstract": "Class imbalance poses a significant challenge to supervised classification,\nparticularly in critical domains like medical diagnostics and anomaly detection\nwhere minority class instances are rare. While numerous studies have explored\nrebalancing techniques to address this issue, less attention has been given to\nevaluating the performance of binary classifiers under imbalance when no such\ntechniques are applied. Therefore, the goal of this study is to assess the\nperformance of binary classifiers \"as-is\", without performing any explicit\nrebalancing. Specifically, we systematically evaluate the robustness of a\ndiverse set of binary classifiers across both real-world and synthetic\ndatasets, under progressively reduced minority class sizes, using one-shot and\nfew-shot scenarios as baselines. Our approach also explores varying data\ncomplexities through synthetic decision boundary generation to simulate\nreal-world conditions. In addition to standard classifiers, we include\nexperiments using undersampling, oversampling strategies, and one-class\nclassification (OCC) methods to examine their behavior under severe imbalance.\nThe results confirm that classification becomes more difficult as data\ncomplexity increases and the minority class size decreases. While traditional\nclassifiers deteriorate under extreme imbalance, advanced models like TabPFN\nand boosting-based ensembles retain relatively higher performance and better\ngeneralization compared to traditional classifiers. Visual interpretability and\nevaluation metrics further validate these findings. Our work offers valuable\nguidance on model selection for imbalanced learning, providing insights into\nclassifier robustness without dependence on explicit rebalancing techniques.",
      "pdf_url": "http://arxiv.org/pdf/2509.07605v1",
      "published": "2025-09-09T11:28:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07605v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards",
      "authors": [
        "Mehdi Bejani",
        "Marco Mauri",
        "Daniele Acconcia",
        "Simone Todaro",
        "Stefano Mariani"
      ],
      "abstract": "This paper presents an innovative Transformer-based deep learning strategy\nfor optimizing the placement of sensors aiming at structural health monitoring\nof semiconductor probe cards. Failures in probe cards, including substrate\ncracks and loosened screws, would critically affect semiconductor manufacturing\nyield and reliability. Some failure modes could be detected by equipping a\nprobe card with adequate sensors. Frequency response functions from simulated\nfailure scenarios are adopted within a finite element model of a probe card. A\ncomprehensive dataset, enriched by physics-informed scenario expansion and\nphysics-aware statistical data augmentation, is exploited to train a hybrid\nConvolutional Neural Network and Transformer model. The model achieves high\naccuracy (99.83%) in classifying the probe card health states (baseline, loose\nscrew, crack) and an excellent crack detection recall (99.73%). Model\nrobustness is confirmed through a rigorous framework of 3 repetitions of\n10-fold stratified cross-validation. The attention mechanism also pinpoints\ncritical sensor locations: an analysis of the attention weights offers\nactionable insights for designing efficient, cost-effective monitoring systems\nby optimizing sensor configurations. This research highlights the capability of\nattention-based deep learning to advance proactive maintenance, enhancing\noperational reliability and yield in semiconductor manufacturing.",
      "pdf_url": "http://arxiv.org/pdf/2509.07603v1",
      "published": "2025-09-09T11:21:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07603v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?",
      "authors": [
        "Gavin Tao",
        "Yinuo Wang",
        "Jinzhao Zhou"
      ],
      "abstract": "End-to-end reinforcement learning for motion control promises unified\nperception-action policies that scale across embodiments and tasks, yet most\ndeployed controllers are either blind (proprioception-only) or rely on fusion\nbackbones with unfavorable compute-memory trade-offs. Recurrent controllers\nstruggle with long-horizon credit assignment, and Transformer-based fusion\nincurs quadratic cost in token length, limiting temporal and spatial context.\nWe present a vision-driven cross-modal RL framework built on SSD-Mamba2, a\nselective state-space backbone that applies state-space duality (SSD) to enable\nboth recurrent and convolutional scanning with hardware-aware streaming and\nnear-linear scaling. Proprioceptive states and exteroceptive observations\n(e.g., depth tokens) are encoded into compact tokens and fused by stacked\nSSD-Mamba2 layers. The selective state-space updates retain long-range\ndependencies with markedly lower latency and memory use than quadratic\nself-attention, enabling longer look-ahead, higher token resolution, and stable\ntraining under limited compute. Policies are trained end-to-end under curricula\nthat randomize terrain and appearance and progressively increase scene\ncomplexity. A compact, state-centric reward balances task progress, energy\nefficiency, and safety. Across diverse motion-control scenarios, our approach\nconsistently surpasses strong state-of-the-art baselines in return, safety\n(collisions and falls), and sample efficiency, while converging faster at the\nsame compute budget. These results suggest that SSD-Mamba2 provides a practical\nfusion backbone for scalable, foresightful, and efficient end-to-end motion\ncontrol.",
      "pdf_url": "http://arxiv.org/pdf/2509.07593v1",
      "published": "2025-09-09T11:05:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07593v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.SY",
        "eess.IV",
        "eess.SY"
      ]
    },
    {
      "title": "BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment",
      "authors": [
        "Andrey Sakhovskiy",
        "Elena Tutubalina"
      ],
      "abstract": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts.",
      "pdf_url": "http://arxiv.org/pdf/2509.07588v1",
      "published": "2025-09-09T10:59:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07588v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; H.3.3; J.3"
      ]
    },
    {
      "title": "Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks",
      "authors": [
        "Barkin Buyukcakir",
        "Rocharles Cavalcante Fontenele",
        "Reinhilde Jacobs",
        "Jannick De Tobel",
        "Patrick Thevissen",
        "Dirk Vandermeulen",
        "Peter Claes"
      ],
      "abstract": "Deep learning offers a promising avenue for automating many recognition tasks\nin fields such as medicine and forensics. However, the black-box nature of\nthese models hinders their adoption in high-stakes applications where trust and\naccountability are required. For 3D shape recognition tasks in particular, this\npaper introduces the Class Node Graph Attention Network (CGAT) architecture to\naddress this need. Applied to 3D meshes of third molars derived from CBCT\nimages, for Demirjian stage allocation, CGAT utilizes graph attention\nconvolutions and an inherent attention mechanism, visualized via attention\nrollout, to explain its decision-making process. We evaluated the local mean\ncurvature and distance to centroid node features, both individually and in\ncombination, as well as model depth, finding that models incorporating directed\nedges to a global CLS node produced more intuitive attention maps, while also\nyielding desirable classification performance. We analyzed the attention-based\nexplanations of the models, and their predictive performances to propose\noptimal settings for the CGAT. The combination of local mean curvature and\ndistance to centroid as node features yielded a slight performance increase\nwith 0.76 weighted F1 score, and more comprehensive attention visualizations.\nThe CGAT architecture's ability to generate human-understandable attention maps\ncan enhance trust and facilitate expert validation of model decisions. While\ndemonstrated on dental data, CGAT is broadly applicable to graph-based\nclassification and regression tasks, promoting wider adoption of transparent\nand competitive deep learning models in high-stakes environments.",
      "pdf_url": "http://arxiv.org/pdf/2509.07581v1",
      "published": "2025-09-09T10:44:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07581v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07 68T07 68T07 (Primary) 68R10 (Secondary)"
      ]
    },
    {
      "title": "Towards explainable decision support using hybrid neural models for logistic terminal automation",
      "authors": [
        "Riccardo D'Elia",
        "Alberto Termine",
        "Francesco Flammini"
      ],
      "abstract": "The integration of Deep Learning (DL) in System Dynamics (SD) modeling for\ntransportation logistics offers significant advantages in scalability and\npredictive accuracy. However, these gains are often offset by the loss of\nexplainability and causal reliability $-$ key requirements in critical\ndecision-making systems. This paper presents a novel framework for\ninterpretable-by-design neural system dynamics modeling that synergizes DL with\ntechniques from Concept-Based Interpretability, Mechanistic Interpretability,\nand Causal Machine Learning. The proposed hybrid approach enables the\nconstruction of neural network models that operate on semantically meaningful\nand actionable variables, while retaining the causal grounding and transparency\ntypical of traditional SD models. The framework is conceived to be applied to\nreal-world case-studies from the EU-funded project AutoMoTIF, focusing on\ndata-driven decision support, automation, and optimization of multimodal\nlogistic terminals. We aim at showing how neuro-symbolic methods can bridge the\ngap between black-box predictive models and the need for critical decision\nsupport in complex dynamical environments within cyber-physical systems enabled\nby the industrial Internet-of-Things.",
      "pdf_url": "http://arxiv.org/pdf/2509.07577v2",
      "published": "2025-09-09T10:41:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07577v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Towards Generalized Routing: Model and Agent Orchestration for Adaptive and Efficient Inference",
      "authors": [
        "Xiyu Guo",
        "Shan Wang",
        "Chunfang Ji",
        "Xuefeng Zhao",
        "Wenhao Xi",
        "Yaoyao Liu",
        "Qinglan Li",
        "Chao Deng",
        "Junlan Feng"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) and domain-specific AI\nagents has greatly expanded the ecosystem of AI-powered services. User queries,\nhowever, are highly diverse and often span multiple domains and task types,\nresulting in a complex and heterogeneous landscape. This diversity presents a\nfundamental routing challenge: how to accurately direct each query to an\nappropriate execution unit while optimizing both performance and efficiency. To\naddress this, we propose MoMA (Mixture of Models and Agents), a generalized\nrouting framework that integrates both LLM and agent-based routing. Built upon\na deep understanding of model and agent capabilities, MoMA effectively handles\ndiverse queries through precise intent recognition and adaptive routing\nstrategies, achieving an optimal balance between efficiency and cost.\nSpecifically, we construct a detailed training dataset to profile the\ncapabilities of various LLMs under different routing model structures,\nidentifying the most suitable tasks for each LLM. During inference, queries are\ndynamically routed to the LLM with the best cost-performance efficiency. We\nalso introduce an efficient agent selection strategy based on a context-aware\nstate machine and dynamic masking. Experimental results demonstrate that the\nMoMA router offers superior cost-efficiency and scalability compared to\nexisting approaches.",
      "pdf_url": "http://arxiv.org/pdf/2509.07571v1",
      "published": "2025-09-09T10:15:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07571v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "$ΔL$ Normalization: Rethink Loss Aggregation in RLVR",
      "authors": [
        "Zhiyuan He",
        "Xufang Luo",
        "Yike Zhang",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "abstract": "We propose $\\Delta L$ Normalization, a simple yet effective loss aggregation\nmethod tailored to the characteristic of dynamic generation lengths in\nReinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has\ndemonstrated strong potential in improving the reasoning capabilities of large\nlanguage models (LLMs), but a major challenge lies in the large variability of\nresponse lengths during training, which leads to high gradient variance and\nunstable optimization. Although previous methods such as GRPO, DAPO, and Dr.\nGRPO introduce different loss normalization terms to address this issue, they\neither produce biased estimates or still suffer from high gradient variance. By\nanalyzing the effect of varying lengths on policy loss both theoretically and\nempirically, we reformulate the problem as finding a minimum-variance unbiased\nestimator. Our proposed $\\Delta L$ Normalization not only provides an unbiased\nestimate of the true policy loss but also minimizes gradient variance in\ntheory. Extensive experiments show that it consistently achieves superior\nresults across different model sizes, maximum lengths, and tasks. Our code will\nbe made public at https://github.com/zerolllin/Delta-L-Normalization.",
      "pdf_url": "http://arxiv.org/pdf/2509.07558v1",
      "published": "2025-09-09T09:52:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07558v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition",
      "authors": [
        "Yi Liu",
        "Xiangrong Zhu",
        "Xiangyu Liu",
        "Wei Wei",
        "Wei Hu"
      ],
      "abstract": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering.",
      "pdf_url": "http://arxiv.org/pdf/2509.07555v1",
      "published": "2025-09-09T09:49:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07555v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "HU-based Foreground Masking for 3D Medical Masked Image Modeling",
      "authors": [
        "Jin Lee",
        "Vu Dang",
        "Gwang-Hyun Yu",
        "Anh Le",
        "Zahid Rahman",
        "Jin-Ho Jang",
        "Heonzoo Lee",
        "Kun-Yung Kim",
        "Jin-Sul Kim",
        "Jin-Young Kim"
      ],
      "abstract": "While Masked Image Modeling (MIM) has revolutionized fields of computer\nvision, its adoption in 3D medical image computing has been limited by the use\nof random masking, which overlooks the density of anatomical objects. To\naddress this limitation, we enhance the pretext task with a simple yet\neffective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we\nimplement an HU-based Foreground Masking, which focuses on the intensity\ndistribution of visceral organs and excludes non-tissue regions, such as air\nand fluid, that lack diagnostically meaningful features. Extensive experiments\non five public 3D medical imaging datasets demonstrate that our masking\nconsistently improves performance, both in quality of segmentation and Dice\nscore (BTCV:~84.64\\%, Flare22:~92.43\\%, MM-WHS:~90.67\\%, Amos22:~88.64\\%,\nBraTS:~78.55\\%). These results underscore the importance of domain-centric MIM\nand suggest a promising direction for representation learning in medical image\nsegmentation. Implementation is available at github.com/AISeedHub/SubFore/.",
      "pdf_url": "http://arxiv.org/pdf/2509.07534v1",
      "published": "2025-09-09T09:11:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07534v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents",
      "authors": [
        "Zheng Dou",
        "Deqing Wang",
        "Fuzhen Zhuang",
        "Jian Ren",
        "Yanlin Hu"
      ],
      "abstract": "Scientific document representation learning provides powerful embeddings for\nvarious tasks, while current methods face challenges across three approaches.\n1) Contrastive training with citation-structural signals underutilizes citation\ninformation and still generates single-vector representations. 2) Fine-grained\nrepresentation learning, which generates multiple vectors at the sentence or\naspect level, requires costly integration and lacks domain generalization. 3)\nTask-aware learning depends on manually predefined task categorization,\noverlooking nuanced task distinctions and requiring extra training data for\ntask-specific modules. To address these problems, we propose a new method that\nunifies the three approaches for better representations, namely FLeW.\nSpecifically, we introduce a novel triplet sampling method that leverages\ncitation intent and frequency to enhance citation-structural signals for\ntraining. Citation intents (background, method, result), aligned with the\ngeneral structure of scientific writing, facilitate a domain-generalized facet\npartition for fine-grained representation learning. Then, we adopt a simple\nweight search to adaptively integrate three facet-level embeddings into a\ntask-specific document embedding without task-aware fine-tuning. Experiments\nshow the applicability and robustness of FLeW across multiple scientific tasks\nand fields, compared to prior models.",
      "pdf_url": "http://arxiv.org/pdf/2509.07531v1",
      "published": "2025-09-09T09:08:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07531v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data",
      "authors": [
        "Gokul Karthik Kumar",
        "Rishabh Saraf",
        "Ludovick Lepauloux",
        "Abdul Muneer",
        "Billel Mokeddem",
        "Hakim Hacid"
      ],
      "abstract": "Large language models (LLMs) have transformed NLP, yet their integration with\naudio remains underexplored -- despite audio's centrality to human\ncommunication. We introduce Falcon3-Audio, a family of Audio-Language Models\n(ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably\nsmall amount of public audio data -- less than 30K hours (5K unique) --\nFalcon3-Audio-7B matches the best reported performance among open-weight models\non the MMAU benchmark, with a score of 64.14, matching R1-AQA, while\ndistinguishing itself through superior data and parameter efficiency,\nsingle-stage training, and transparency. Notably, our smallest 1B model remains\ncompetitive with larger open models ranging from 2B to 13B parameters. Through\nextensive ablations, we find that common complexities -- such as curriculum\nlearning, multiple audio encoders, and intricate cross-attention connectors --\nare not required for strong performance, even compared to models trained on\nover 500K hours of data.",
      "pdf_url": "http://arxiv.org/pdf/2509.07526v1",
      "published": "2025-09-09T09:01:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07526v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "EHWGesture -- A dataset for multimodal understanding of clinical gestures",
      "authors": [
        "Gianluca Amprimo",
        "Alberto Ancilotto",
        "Alessandro Savino",
        "Fabio Quazzolo",
        "Claudia Ferraris",
        "Gabriella Olmo",
        "Elisabetta Farella",
        "Stefano Di Carlo"
      ],
      "abstract": "Hand gesture understanding is essential for several applications in\nhuman-computer interaction, including automatic clinical assessment of hand\ndexterity. While deep learning has advanced static gesture recognition, dynamic\ngesture understanding remains challenging due to complex spatiotemporal\nvariations. Moreover, existing datasets often lack multimodal and multi-view\ndiversity, precise ground-truth tracking, and an action quality component\nembedded within gestures. This paper introduces EHWGesture, a multimodal video\ndataset for gesture understanding featuring five clinically relevant gestures.\nIt includes over 1,100 recordings (6 hours), captured from 25 healthy subjects\nusing two high-resolution RGB-Depth cameras and an event camera. A motion\ncapture system provides precise ground-truth hand landmark tracking, and all\ndevices are spatially calibrated and synchronized to ensure cross-modal\nalignment. Moreover, to embed an action quality task within gesture\nunderstanding, collected recordings are organized in classes of execution speed\nthat mirror clinical evaluations of hand dexterity. Baseline experiments\nhighlight the dataset's potential for gesture classification, gesture trigger\ndetection, and action quality assessment. Thus, EHWGesture can serve as a\ncomprehensive benchmark for advancing multimodal clinical gesture\nunderstanding.",
      "pdf_url": "http://arxiv.org/pdf/2509.07525v1",
      "published": "2025-09-09T09:00:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.07525v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}