{
  "last_updated": "2025-12-24T00:54:55.879045",
  "papers": [
    {
      "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
      "authors": [
        "Junze Ye",
        "Daniel Tawfik",
        "Alex J. Goodell",
        "Nikhil V. Kotha",
        "Mark K. Buyyounouski",
        "Mohsen Bayati"
      ],
      "abstract": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.",
      "pdf_url": "https://arxiv.org/pdf/2512.19691v1",
      "published": "2025-12-22T18:59:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19691v1",
      "categories": [
        "cs.AI",
        "stat.AP"
      ]
    },
    {
      "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "authors": [
        "Hanyang Kong",
        "Xingyi Yang",
        "Xiaoxu Zheng",
        "Xinchao Wang"
      ],
      "abstract": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
      "pdf_url": "https://arxiv.org/pdf/2512.19678v1",
      "published": "2025-12-22T18:53:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19678v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "authors": [
        "Yuqiao Tan",
        "Minzheng Wang",
        "Shizhu He",
        "Huanxuan Liao",
        "Chengfeng Zhao",
        "Qiunan Lu",
        "Tian Liang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
      "pdf_url": "https://arxiv.org/pdf/2512.19673v1",
      "published": "2025-12-22T18:51:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19673v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "authors": [
        "Argha Kamal Samanta",
        "Harshika Goyal",
        "Vasudha Joshi",
        "Tushar Mungle",
        "Pabitra Mitra"
      ],
      "abstract": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
      "pdf_url": "https://arxiv.org/pdf/2512.19663v1",
      "published": "2025-12-22T18:41:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19663v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Clustering with Label Consistency",
      "authors": [
        "Diptarka Chakraborty",
        "Hendrik Fichtenberger",
        "Bernhard Haeupler",
        "Silvio Lattanzi",
        "Ashkan Norouzi-Fard",
        "Ola Svensson"
      ],
      "abstract": "Designing efficient, effective, and consistent metric clustering algorithms is a significant challenge attracting growing attention. Traditional approaches focus on the stability of cluster centers; unfortunately, this neglects the real-world need for stable point labels, i.e., stable assignments of points to named sets (clusters). In this paper, we address this gap by initiating the study of label-consistent metric clustering. We first introduce a new notion of consistency, measuring the label distance between two consecutive solutions. Then, armed with this new definition, we design new consistent approximation algorithms for the classical $k$-center and $k$-median problems.",
      "pdf_url": "https://arxiv.org/pdf/2512.19654v1",
      "published": "2025-12-22T18:32:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19654v1",
      "categories": [
        "cs.DS",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring the features used for summary evaluation by Human and GPT",
      "authors": [
        "Zahra Sadeghi",
        "Evangelos Milios",
        "Frank Rudzicz"
      ],
      "abstract": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
      "pdf_url": "https://arxiv.org/pdf/2512.19620v1",
      "published": "2025-12-22T17:54:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19620v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
      "authors": [
        "Artemis Panagopoulou",
        "Aveek Purohit",
        "Achin Kulshrestha",
        "Soroosh Yazdani",
        "Mohit Goyal"
      ],
      "abstract": "While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",
      "pdf_url": "https://arxiv.org/pdf/2512.19609v1",
      "published": "2025-12-22T17:45:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19609v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
      "authors": [
        "Kirill Djebko",
        "Tom Baumann",
        "Erik Dilger",
        "Frank Puppe",
        "Sergio Montenegro"
      ],
      "abstract": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.",
      "pdf_url": "https://arxiv.org/pdf/2512.19576v1",
      "published": "2025-12-22T17:00:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19576v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ]
    },
    {
      "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
      "authors": [
        "Angjelin Hila"
      ],
      "abstract": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.",
      "pdf_url": "https://arxiv.org/pdf/2512.19570v1",
      "published": "2025-12-22T16:52:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19570v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty",
      "authors": [
        "Lapo Santarlasci",
        "Armando Rungi",
        "Loredana Fattorini",
        "Nestor Maslej"
      ],
      "abstract": "Artificial intelligence has become a key arena of global technological competition and a central concern for Europe's quest for technological sovereignty. This paper analyzes global AI patenting from 2010 to 2023 to assess Europe's position in an increasingly bipolar innovation landscape dominated by the United States and China. Using linked patent, firm, ownership, and citation data, we examine the geography, specialization, and international diffusion of AI innovation. We find a highly concentrated patent landscape: China leads in patent volumes, while the United States dominates in citation impact and technological influence. Europe accounts for a limited share of AI patents but exhibits signals of relatively high patent quality. Technological proximity reveals global convergence toward U.S. innovation trajectories, with Europe remaining fragmented rather than forming an autonomous pole. Gravity-model estimates show that cross-border AI knowledge flows are driven primarily by technological capability and specialization, while geographic and institutional factors play a secondary role. EU membership does not significantly enhance intra-European knowledge diffusion, suggesting that technological capacity, rather than political integration, underpins participation in global AI innovation networks.",
      "pdf_url": "https://arxiv.org/pdf/2512.19569v1",
      "published": "2025-12-22T16:52:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19569v1",
      "categories": [
        "econ.GN",
        "cs.AI"
      ]
    },
    {
      "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
      "authors": [
        "Yanliang Huang",
        "Xia Yan",
        "Peiran Yin",
        "Zhenduo Zhang",
        "Zeyan Shao",
        "Youran Wang",
        "Haoliang Huang",
        "Matthias Althoff"
      ],
      "abstract": "Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.",
      "pdf_url": "https://arxiv.org/pdf/2512.19564v1",
      "published": "2025-12-22T16:46:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19564v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "authors": [
        "Martin Sedlacek",
        "Pavlo Yefanov",
        "Georgy Ponimatkin",
        "Jai Bardhan",
        "Simon Pilc",
        "Mederic Fourmy",
        "Evangelos Kazakos",
        "Cees G. M. Snoek",
        "Josef Sivic",
        "Vladimir Petrik"
      ],
      "abstract": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm",
      "pdf_url": "https://arxiv.org/pdf/2512.19562v1",
      "published": "2025-12-22T16:44:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19562v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "BabyFlow: 3D modeling of realistic and expressive infant faces",
      "authors": [
        "Antonia Alomar",
        "Mireia Masias",
        "Marius George Linguraru",
        "Federico M. Sukno",
        "Gemma Piella"
      ],
      "abstract": "Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.",
      "pdf_url": "https://arxiv.org/pdf/2512.19560v1",
      "published": "2025-12-22T16:42:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19560v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations",
      "authors": [
        "Lawrence Krukrubo",
        "Julius Odede",
        "Olawande Olusegun"
      ],
      "abstract": "Current approaches to Explainable AI (XAI) face a \"Scalability-Stability Dilemma.\" Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel \"Asymmetry of Discovery.\" When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad \"Safety Nets\" (retention patterns) but struggle to capture specific \"Risk Traps\" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of \"Rule Writers\" to \"Exception Handlers.\"",
      "pdf_url": "https://arxiv.org/pdf/2512.19557v1",
      "published": "2025-12-22T16:40:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19557v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
      "authors": [
        "Yongxin Wang",
        "Zhicheng Yang",
        "Meng Cao",
        "Mingfei Han",
        "Haokun Lin",
        "Yingying Zhu",
        "Xiaojun Chang",
        "Xiaodan Liang"
      ],
      "abstract": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
      "pdf_url": "https://arxiv.org/pdf/2512.19554v1",
      "published": "2025-12-22T16:34:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19554v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
      "authors": [
        "Jiawen Wang",
        "Jingjing Wang Tianyang Chen",
        "Min Zhang",
        "Guodong Zhou"
      ],
      "abstract": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.",
      "pdf_url": "https://arxiv.org/pdf/2512.19551v1",
      "published": "2025-12-22T16:31:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19551v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "authors": [
        "Moritz Böhle",
        "Amélie Royer",
        "Juliette Marrie",
        "Edouard Grave",
        "Patrick Pérez"
      ],
      "abstract": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
      "pdf_url": "https://arxiv.org/pdf/2512.19535v1",
      "published": "2025-12-22T16:21:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19535v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement",
      "authors": [
        "Hongsheng Xing",
        "Qiuxin Si"
      ],
      "abstract": "Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $>25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.",
      "pdf_url": "https://arxiv.org/pdf/2512.19530v1",
      "published": "2025-12-22T16:19:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19530v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
      "authors": [
        "Li Puyin",
        "Tiange Xiang",
        "Ella Mao",
        "Shirley Wei",
        "Xinye Chen",
        "Adnan Masood",
        "Li Fei-fei",
        "Ehsan Adeli"
      ],
      "abstract": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
      "pdf_url": "https://arxiv.org/pdf/2512.19526v1",
      "published": "2025-12-22T16:18:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19526v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
      "authors": [
        "Xueming Yan",
        "Bo Yin",
        "Yaochu Jin"
      ],
      "abstract": "Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.",
      "pdf_url": "https://arxiv.org/pdf/2512.19516v1",
      "published": "2025-12-22T16:08:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19516v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
      "authors": [
        "Ziyang Song",
        "Zelin Zang",
        "Zuyao Chen",
        "Xusheng Liang",
        "Dong Yi",
        "Jinlin Wu",
        "Hongbin Liu",
        "Jiebo Luo"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
      "pdf_url": "https://arxiv.org/pdf/2512.19512v1",
      "published": "2025-12-22T16:06:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19512v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast",
      "authors": [
        "Hongliang Li",
        "Nong Zhang",
        "Zhewen Xu",
        "Xiang Li",
        "Changzheng Liu",
        "Chongbo Zhao",
        "Jie Wu"
      ],
      "abstract": "Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.",
      "pdf_url": "https://arxiv.org/pdf/2512.19506v1",
      "published": "2025-12-22T16:00:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19506v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset",
      "authors": [
        "Nikita Volzhin",
        "Soowhan Yoon"
      ],
      "abstract": "The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.",
      "pdf_url": "https://arxiv.org/pdf/2512.19494v1",
      "published": "2025-12-22T15:49:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19494v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
      "authors": [
        "Katharina Stengg",
        "Christian Macho",
        "Martin Pinzger"
      ],
      "abstract": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
      "pdf_url": "https://arxiv.org/pdf/2512.19481v1",
      "published": "2025-12-22T15:32:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19481v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications",
      "authors": [
        "Lorenzo Capelli",
        "Leandro de Souza Rosa",
        "Gianluca Setti",
        "Mauro Mangia",
        "Riccardo Rovatti"
      ],
      "abstract": "The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.",
      "pdf_url": "https://arxiv.org/pdf/2512.19472v1",
      "published": "2025-12-22T15:25:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19472v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "An Agentic Framework for Autonomous Materials Computation",
      "authors": [
        "Zeyu Xia",
        "Jinzhe Ma",
        "Congjie Zheng",
        "Shufei Zhang",
        "Yuqiang Li",
        "Hang Su",
        "P. Hu",
        "Changshui Zhang",
        "Xingao Gong",
        "Wanli Ouyang",
        "Lei Bai",
        "Dongzhan Zhou",
        "Mao Su"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
      "pdf_url": "https://arxiv.org/pdf/2512.19458v1",
      "published": "2025-12-22T15:03:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19458v1",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ]
    },
    {
      "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
      "authors": [
        "Jinwei Chi",
        "Ke Wang",
        "Yu Chen",
        "Xuanye Lin",
        "Qiang Xu"
      ],
      "abstract": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
      "pdf_url": "https://arxiv.org/pdf/2512.19456v1",
      "published": "2025-12-22T15:01:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19456v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation",
      "authors": [
        "Fei Ge",
        "Ying Huang",
        "Jie Liu",
        "Guixuan Zhang",
        "Zhi Zeng",
        "Shuwu Zhang",
        "Hu Guan"
      ],
      "abstract": "Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.",
      "pdf_url": "https://arxiv.org/pdf/2512.19438v1",
      "published": "2025-12-22T14:36:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19438v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Attention Is Not What You Need",
      "authors": [
        "Zhang Chong"
      ],
      "abstract": "We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.\n  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.\n  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2512.19428v1",
      "published": "2025-12-22T14:29:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19428v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.AG"
      ]
    },
    {
      "title": "Research Program: Theory of Learning in Dynamical Systems",
      "authors": [
        "Elad Hazan",
        "Shai Shalev Shwartz",
        "Nathan Srebro"
      ],
      "abstract": "Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.19410v1",
      "published": "2025-12-22T14:05:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19410v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
      "authors": [
        "Runze Li",
        "Yuwen Zhai",
        "Bo Xu",
        "LiWu Xu",
        "Nian Shi",
        "Wei Zhang",
        "Ran Lin",
        "Liang Wang"
      ],
      "abstract": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.",
      "pdf_url": "https://arxiv.org/pdf/2512.19396v1",
      "published": "2025-12-22T13:42:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19396v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition",
      "authors": [
        "Yueyao Chen",
        "Kai-Ni Wang",
        "Dario Tayupo",
        "Arnaud Huaulm'e",
        "Krystel Nyangoh Timoh",
        "Pierre Jannin",
        "Qi Dou"
      ],
      "abstract": "Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.\n  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.\n  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.\n  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.",
      "pdf_url": "https://arxiv.org/pdf/2512.19387v1",
      "published": "2025-12-22T13:36:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19387v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "authors": [
        "Xueming Yan",
        "Boyan Xu",
        "Yaochu Jin",
        "Lixian Xiao",
        "Wenlong Ye",
        "Runyang Cai",
        "Zeqi Zheng",
        "Jingfa Liu",
        "Aimin Yang"
      ],
      "abstract": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER",
      "pdf_url": "https://arxiv.org/pdf/2512.19379v1",
      "published": "2025-12-22T13:23:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19379v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture",
      "authors": [
        "Christian Hägg",
        "Kathlén Kohn",
        "Giovanni Luca Marchetti",
        "Boris Shapiro"
      ],
      "abstract": "We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).",
      "pdf_url": "https://arxiv.org/pdf/2512.19367v1",
      "published": "2025-12-22T13:09:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19367v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ]
    },
    {
      "title": "Learning General Policies with Policy Gradient Methods",
      "authors": [
        "Simon Ståhlberg",
        "Blai Bonet",
        "Hector Geffner"
      ],
      "abstract": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
      "pdf_url": "https://arxiv.org/pdf/2512.19366v1",
      "published": "2025-12-22T13:08:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19366v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "First-Order Representation Languages for Goal-Conditioned RL",
      "authors": [
        "Simon Ståhlberg",
        "Hector Geffner"
      ],
      "abstract": "First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.",
      "pdf_url": "https://arxiv.org/pdf/2512.19355v1",
      "published": "2025-12-22T12:54:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19355v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
      "authors": [
        "A. B. M. Ashikur Rahman",
        "Saeed Anwar",
        "Muhammad Usman",
        "Irfan Ahmad",
        "Ajmal Mian"
      ],
      "abstract": "Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \\textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.",
      "pdf_url": "https://arxiv.org/pdf/2512.19350v1",
      "published": "2025-12-22T12:49:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19350v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
      "authors": [
        "JiaWei Zhu",
        "ZiHeng Liu"
      ],
      "abstract": "Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.",
      "pdf_url": "https://arxiv.org/pdf/2512.19349v1",
      "published": "2025-12-22T12:48:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19349v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Alternative positional encoding functions for neural transformers",
      "authors": [
        "Ezequiel Lopez-Rubio",
        "Macoris Decena-Gimenez",
        "Rafael Marcos Luque-Baena"
      ],
      "abstract": "A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.",
      "pdf_url": "https://arxiv.org/pdf/2512.19323v1",
      "published": "2025-12-22T12:17:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19323v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MAGIC: Achieving Superior Model Merging via Magnitude Calibration",
      "authors": [
        "Yayuan Li",
        "Jian Zhang",
        "Jintao Guo",
        "Zihan Cheng",
        "Lei Qi",
        "Yinghuan Shi",
        "Yang Gao"
      ],
      "abstract": "The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC",
      "pdf_url": "https://arxiv.org/pdf/2512.19320v1",
      "published": "2025-12-22T12:13:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19320v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
      "authors": [
        "A. A. Gde Yogi Pramana",
        "Jason Ray",
        "Anthony Jaya",
        "Michael Wijaya"
      ],
      "abstract": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.19317v1",
      "published": "2025-12-22T12:07:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19317v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture",
      "authors": [
        "Hui Li",
        "Jiayue Lyu",
        "Fu-Yun Wang",
        "Kaihui Cheng",
        "Siyu Zhu",
        "Jingdong Wang"
      ],
      "abstract": "This paper studies the training-testing discrepancy (a.k.a. exposure bias) problem for improving the diffusion models. During training, the input of a prediction network at one training timestep is the corresponding ground-truth noisy data that is an interpolation of the noise and the data, and during testing, the input is the generated noisy data. We present a novel training approach, named MixFlow, for improving the performance. Our approach is motivated by the Slow Flow phenomenon: the ground-truth interpolation that is the nearest to the generated noisy data at a given sampling timestep is observed to correspond to a higher-noise timestep (termed slowed timestep), i.e., the corresponding ground-truth timestep is slower than the sampling timestep. MixFlow leverages the interpolations at the slowed timesteps, named slowed interpolation mixture, for post-training the prediction network for each training timestep. Experiments over class-conditional image generation (including SiT, REPA, and RAE) and text-to-image generation validate the effectiveness of our approach. Our approach MixFlow over the RAE models achieve strong generation results on ImageNet: 1.43 FID (without guidance) and 1.10 (with guidance) at 256 x 256, and 1.55 FID (without guidance) and 1.10 (with guidance) at 512 x 512.",
      "pdf_url": "https://arxiv.org/pdf/2512.19311v1",
      "published": "2025-12-22T12:00:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19311v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
      "authors": [
        "Haoyu Jiang",
        "Fanjie Zeng",
        "Boan Qu",
        "Xiaojie Lin",
        "Wei Zhong"
      ],
      "abstract": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.",
      "pdf_url": "https://arxiv.org/pdf/2512.19299v1",
      "published": "2025-12-22T11:43:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19299v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
      "authors": [
        "Linzhi Chen",
        "Yang Sun",
        "Hongru Wei",
        "Yuqi Chen"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.",
      "pdf_url": "https://arxiv.org/pdf/2512.19297v1",
      "published": "2025-12-22T11:40:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19297v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6",
      "authors": [
        "Jiaao Wu",
        "Xian Zhang",
        "Fan Yang",
        "Yinpeng Dong"
      ],
      "abstract": "We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.",
      "pdf_url": "https://arxiv.org/pdf/2512.19287v1",
      "published": "2025-12-22T11:30:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19287v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals",
      "authors": [
        "Chang Dong",
        "Jianfeng Tao",
        "Chengliang Liu"
      ],
      "abstract": "Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.",
      "pdf_url": "https://arxiv.org/pdf/2512.19280v1",
      "published": "2025-12-22T11:24:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19280v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation",
      "authors": [
        "Ivan DeAndres-Tame",
        "Chengwei Ye",
        "Ruben Tolosana",
        "Ruben Vera-Rodriguez",
        "Shiqi Yu"
      ],
      "abstract": "Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.",
      "pdf_url": "https://arxiv.org/pdf/2512.19275v1",
      "published": "2025-12-22T11:19:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19275v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study",
      "authors": [
        "Carla Crivoi",
        "Radu Tudor Ionescu"
      ],
      "abstract": "We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.",
      "pdf_url": "https://arxiv.org/pdf/2512.19253v1",
      "published": "2025-12-22T10:40:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19253v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
      "authors": [
        "Do Minh Duc",
        "Quan Xuan Truong",
        "Nguyen Tat Dat",
        "Nguyen Van Vinh"
      ],
      "abstract": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
      "pdf_url": "https://arxiv.org/pdf/2512.19247v1",
      "published": "2025-12-22T10:29:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19247v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models",
      "authors": [
        "Mingxu Zhang",
        "Dazhong Shen",
        "Qi Zhang",
        "Ying Sun"
      ],
      "abstract": "Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.",
      "pdf_url": "https://arxiv.org/pdf/2512.19240v1",
      "published": "2025-12-22T10:21:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.19240v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}