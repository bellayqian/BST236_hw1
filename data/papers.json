{
  "last_updated": "2025-09-12T00:46:15.372515",
  "papers": [
    {
      "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
      "authors": [
        "Kaiyan Zhang",
        "Yuxin Zuo",
        "Bingxiang He",
        "Youbang Sun",
        "Runze Liu",
        "Che Jiang",
        "Yuchen Fan",
        "Kai Tian",
        "Guoli Jia",
        "Pengfei Li",
        "Yu Fu",
        "Xingtai Lv",
        "Yuchen Zhang",
        "Sihang Zeng",
        "Shang Qu",
        "Haozhan Li",
        "Shijie Wang",
        "Yuru Wang",
        "Xinwei Long",
        "Fangfu Liu",
        "Xiang Xu",
        "Jiaze Ma",
        "Xuekai Zhu",
        "Ermo Hua",
        "Yihao Liu",
        "Zonglin Li",
        "Huayu Chen",
        "Xiaoye Qu",
        "Yafu Li",
        "Weize Chen",
        "Zhenzhao Yuan",
        "Junqi Gao",
        "Dong Li",
        "Zhiyuan Ma",
        "Ganqu Cui",
        "Zhiyuan Liu",
        "Biqing Qi",
        "Ning Ding",
        "Bowen Zhou"
      ],
      "abstract": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
      "pdf_url": "http://arxiv.org/pdf/2509.08827v1",
      "published": "2025-09-10T17:59:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08827v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation",
      "authors": [
        "Joachim Baumann",
        "Paul Röttger",
        "Aleksandra Urman",
        "Albert Wendsjö",
        "Flor Miriam Plaza-del-Arco",
        "Johannes B. Gruber",
        "Dirk Hovy"
      ],
      "abstract": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.",
      "pdf_url": "http://arxiv.org/pdf/2509.08825v1",
      "published": "2025-09-10T17:58:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08825v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "QCardEst/QCardCorr: Quantum Cardinality Estimation and Correction",
      "authors": [
        "Tobias Winker",
        "Jinghua Groppe",
        "Sven Groppe"
      ],
      "abstract": "Cardinality estimation is an important part of query optimization in DBMS. We\ndevelop a Quantum Cardinality Estimation (QCardEst) approach using Quantum\nMachine Learning with a Hybrid Quantum-Classical Network. We define a compact\nencoding for turning SQL queries into a quantum state, which requires only\nqubits equal to the number of tables in the query. This allows the processing\nof a complete query with a single variational quantum circuit (VQC) on current\nhardware. In addition, we compare multiple classical post-processing layers to\nturn the probability vector output of VQC into a cardinality value. We\nintroduce Quantum Cardinality Correction QCardCorr, which improves classical\ncardinality estimators by multiplying the output with a factor generated by a\nVQC to improve the cardinality estimation. With QCardCorr, we have an\nimprovement over the standard PostgreSQL optimizer of 6.37 times for JOB-light\nand 8.66 times for STATS. For JOB-light we even outperform MSCN by a factor of\n3.47.",
      "pdf_url": "http://arxiv.org/pdf/2509.08817v1",
      "published": "2025-09-10T17:49:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08817v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ]
    },
    {
      "title": "Merge-of-Thought Distillation",
      "authors": [
        "Zhanming Shen",
        "Zeyu Qin",
        "Zenan Huang",
        "Hao Chen",
        "Jiaqi Hu",
        "Yihong Zhuang",
        "Guoshan Lu",
        "Gang Chen",
        "Junbo Zhao"
      ],
      "abstract": "Efficient reasoning distillation for long chain-of-thought (CoT) models is\nincreasingly constrained by the assumption of a single oracle teacher, despite\npractical availability of multiple candidate teachers and growing CoT corpora.\nWe revisit teacher selection and observe that different students have different\n\"best teachers,\" and even for the same student the best teacher can vary across\ndatasets. Therefore, to unify multiple teachers' reasoning abilities into\nstudent with overcoming conflicts among various teachers' supervision, we\npropose Merge-of-Thought Distillation (MoT), a lightweight framework that\nalternates between teacher-specific supervised fine-tuning branches and\nweight-space merging of the resulting student variants. On competition math\nbenchmarks, using only about 200 high-quality CoT samples, applying MoT to a\nQwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,\nQWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT\nconsistently outperforms the best single-teacher distillation and the naive\nmulti-teacher union, raises the performance ceiling while mitigating\noverfitting, and shows robustness to distribution-shifted and peer-level\nteachers. Moreover, MoT reduces catastrophic forgetting, improves general\nreasoning beyond mathematics and even cultivates a better teacher, indicating\nthat consensus-filtered reasoning features transfer broadly. These results\nposition MoT as a simple, scalable route to efficiently distilling long CoT\ncapabilities from diverse teachers into compact students.",
      "pdf_url": "http://arxiv.org/pdf/2509.08814v2",
      "published": "2025-09-10T17:46:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08814v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages",
      "authors": [
        "Hailay Kidu Teklehaymanot",
        "Dren Fazlija",
        "Wolfgang Nejdl"
      ],
      "abstract": "Subword-based tokenization methods often fail to preserve morphological\nboundaries, a limitation especially pronounced in low-resource, morphologically\ncomplex languages such as those written in the Geez script. To address this, we\npresent MoVoC (Morpheme-aware Subword Vocabulary Construction) and train\nMoVoC-Tok, a tokenizer that integrates supervised morphological analysis into\nthe subword vocabulary. This hybrid segmentation approach combines\nmorpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological\nintegrity while maintaining lexical meaning. To tackle resource scarcity, we\ncurate and release manually annotated morpheme data for four Geez script\nlanguages and a morpheme-aware vocabulary for two of them. While the proposed\ntokenization method does not lead to significant gains in automatic translation\nquality, we observe consistent improvements in intrinsic metrics, MorphoScore,\nand Boundary Precision, highlighting the value of morphology-aware segmentation\nin enhancing linguistic fidelity and token efficiency. Our morpheme-annotated\ndatasets and tokenizer will be publicly available to support further research\nin low-resource, morphologically rich languages. Our code and data are\navailable on GitHub: https://github.com/hailaykidu/MoVoC",
      "pdf_url": "http://arxiv.org/pdf/2509.08812v1",
      "published": "2025-09-10T17:45:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08812v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; I.2.6; H.3.3"
      ]
    },
    {
      "title": "Scaling Truth: The Confidence Paradox in AI Fact-Checking",
      "authors": [
        "Ihsan A. Qazi",
        "Zohaib Khan",
        "Abdullah Ghani",
        "Agha A. Raza",
        "Zafar A. Qazi",
        "Wassay Sajjad",
        "Ayesha Ali",
        "Asher Javaid",
        "Muhammad Abdullah Sohail",
        "Abdul H. Azeemi"
      ],
      "abstract": "The rise of misinformation underscores the need for scalable and reliable\nfact-checking solutions. Large language models (LLMs) hold promise in\nautomating fact verification, yet their effectiveness across global contexts\nremains uncertain. We systematically evaluate nine established LLMs across\nmultiple categories (open/closed-source, multiple sizes, diverse architectures,\nreasoning-based) using 5,000 claims previously assessed by 174 professional\nfact-checking organizations across 47 languages. Our methodology tests model\ngeneralizability on claims postdating training cutoffs and four prompting\nstrategies mirroring both citizen and professional fact-checker interactions,\nwith over 240,000 human annotations as ground truth. Findings reveal a\nconcerning pattern resembling the Dunning-Kruger effect: smaller, accessible\nmodels show high confidence despite lower accuracy, while larger models\ndemonstrate higher accuracy but lower confidence. This risks systemic bias in\ninformation verification, as resource-constrained organizations typically use\nsmaller models. Performance gaps are most pronounced for non-English languages\nand claims originating from the Global South, threatening to widen existing\ninformation inequalities. These results establish a multilingual benchmark for\nfuture research and provide an evidence base for policy aimed at ensuring\nequitable access to trustworthy, AI-assisted fact-checking.",
      "pdf_url": "http://arxiv.org/pdf/2509.08803v1",
      "published": "2025-09-10T17:36:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08803v1",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "PianoVAM: A Multimodal Piano Performance Dataset",
      "authors": [
        "Yonghyun Kim",
        "Junhyung Park",
        "Joonhyung Bae",
        "Kirak Kim",
        "Taegyun Kwon",
        "Alexander Lerch",
        "Juhan Nam"
      ],
      "abstract": "The multimodal nature of music performance has driven increasing interest in\ndata beyond the audio domain within the music information retrieval (MIR)\ncommunity. This paper introduces PianoVAM, a comprehensive piano performance\ndataset that includes videos, audio, MIDI, hand landmarks, fingering labels,\nand rich metadata. The dataset was recorded using a Disklavier piano, capturing\naudio and MIDI from amateur pianists during their daily practice sessions,\nalongside synchronized top-view videos in realistic and varied performance\nconditions. Hand landmarks and fingering labels were extracted using a\npretrained hand pose estimation model and a semi-automated fingering annotation\nalgorithm. We discuss the challenges encountered during data collection and the\nalignment process across different modalities. Additionally, we describe our\nfingering annotation method based on hand landmarks extracted from videos.\nFinally, we present benchmarking results for both audio-only and audio-visual\npiano transcription using the PianoVAM dataset and discuss additional potential\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2509.08800v1",
      "published": "2025-09-10T17:35:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08800v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ]
    },
    {
      "title": "Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making",
      "authors": [
        "Anup Tuladhar",
        "Araz Minhas",
        "Adam Kirton",
        "Eli Kinney-Lang"
      ],
      "abstract": "We present a preliminary experimental platform that explores how narrative\nelements might shape AI decision-making by combining reinforcement learning\n(RL) with language model reasoning. While AI systems can now both make\ndecisions and engage in narrative reasoning, these capabilities have mostly\nbeen studied separately. Our platform attempts to bridge this gap using a\ndual-system architecture to examine how narrative frameworks could influence\nreward-based learning. The system comprises a reinforcement learning policy\nthat suggests actions based on past experience, and a language model that\nprocesses these suggestions through different narrative frameworks to guide\ndecisions. This setup enables initial experimentation with narrative elements\nwhile maintaining consistent environment and reward structures. We implement\nthis architecture in a configurable gridworld environment, where agents receive\nboth policy suggestions and information about their surroundings. The\nplatform's modular design facilitates controlled testing of environmental\ncomplexity, narrative parameters, and the interaction between reinforcement\nlearning and narrative-based decisions. Our logging system captures basic\ndecision metrics, from RL policy values to language model reasoning to action\nselection patterns. While preliminary, this implementation provides a\nfoundation for studying how different narrative frameworks might affect\nreward-based decisions and exploring potential interactions between\noptimization-based learning and symbolic reasoning in AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.08785v1",
      "published": "2025-09-10T17:14:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08785v1",
      "categories": [
        "cs.AI",
        "cs.MA",
        "stat.ML"
      ]
    },
    {
      "title": "An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using Mobile-Captured Skin Images",
      "authors": [
        "Asif Newaz",
        "Asif Ur Rahman Adib",
        "Rajit Sahil",
        "Mashfique Mehzad"
      ],
      "abstract": "Background: Arsenicosis is a serious public health concern in South and\nSoutheast Asia, primarily caused by long-term consumption of\narsenic-contaminated water. Its early cutaneous manifestations are clinically\nsignificant but often underdiagnosed, particularly in rural areas with limited\naccess to dermatologists. Automated, image-based diagnostic solutions can\nsupport early detection and timely interventions.\n  Methods: In this study, we propose an end-to-end framework for arsenicosis\ndiagnosis using mobile phone-captured skin images. A dataset comprising 20\nclasses and over 11000 images of arsenic-induced and other dermatological\nconditions was curated. Multiple deep learning architectures, including\nconvolutional neural networks (CNNs) and Transformer-based models, were\nbenchmarked for arsenicosis detection. Model interpretability was integrated\nvia LIME and Grad-CAM, while deployment feasibility was demonstrated through a\nweb-based diagnostic tool.\n  Results: Transformer-based models significantly outperformed CNNs, with the\nSwin Transformer achieving the best results (86\\\\% accuracy). LIME and Grad-CAM\nvisualizations confirmed that the models attended to lesion-relevant regions,\nincreasing clinical transparency and aiding in error analysis. The framework\nalso demonstrated strong performance on external validation samples, confirming\nits ability to generalize beyond the curated dataset.\n  Conclusion: The proposed framework demonstrates the potential of deep\nlearning for non-invasive, accessible, and explainable diagnosis of arsenicosis\nfrom mobile-acquired images. By enabling reliable image-based screening, it can\nserve as a practical diagnostic aid in rural and resource-limited communities,\nwhere access to dermatologists is scarce, thereby supporting early detection\nand timely intervention.",
      "pdf_url": "http://arxiv.org/pdf/2509.08780v1",
      "published": "2025-09-10T17:08:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08780v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform",
      "authors": [
        "Zhaoxun \"Lorenz\" Liu",
        "Wagner H. Souza",
        "Jay Han",
        "Amin Madani"
      ],
      "abstract": "Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,\naccurate patient-hospital allocation decisions under extreme pressure. Here, we\ndeveloped and validated a deep reinforcement learning-based decision-support AI\nagent to optimize patient transfer decisions during simulated MCIs by balancing\npatient acuity levels, specialized care requirements, hospital capacities, and\ntransport logistics. To integrate this AI agent, we developed MasTER, a\nweb-accessible command dashboard for MCI management simulations. Through a\ncontrolled user study with 30 participants (6 trauma experts and 24\nnon-experts), we evaluated three interaction approaches with the AI agent\n(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI\nscenarios in the Greater Toronto Area. Results demonstrate that increasing AI\ninvolvement significantly improves decision quality and consistency. The AI\nagent outperforms trauma surgeons (p < 0.001) and enables non-experts to\nachieve expert-level performance when assisted, contrasting sharply with their\nsignificantly inferior unassisted performance (p < 0.001). These findings\nestablish the potential for our AI-driven decision support to enhance both MCI\npreparedness training and real-world emergency response management.",
      "pdf_url": "http://arxiv.org/pdf/2509.08756v1",
      "published": "2025-09-10T16:46:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08756v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning",
      "authors": [
        "Zhiheng Xi",
        "Jixuan Huang",
        "Chenyang Liao",
        "Baodai Huang",
        "Honglin Guo",
        "Jiaqi Liu",
        "Rui Zheng",
        "Junjie Ye",
        "Jiazheng Zhang",
        "Wenxiang Chen",
        "Wei He",
        "Yiwen Ding",
        "Guanyu Li",
        "Zehui Chen",
        "Zhengyin Du",
        "Xuesong Yao",
        "Yufei Xu",
        "Jiecao Chen",
        "Tao Gui",
        "Zuxuan Wu",
        "Qi Zhang",
        "Xuanjing Huang",
        "Yu-Gang Jiang"
      ],
      "abstract": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
      "pdf_url": "http://arxiv.org/pdf/2509.08755v1",
      "published": "2025-09-10T16:46:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08755v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Learning Turbulent Flows with Generative Models: Super-resolution, Forecasting, and Sparse Flow Reconstruction",
      "authors": [
        "Vivek Oommen",
        "Siavash Khodakarami",
        "Aniruddha Bora",
        "Zhicheng Wang",
        "George Em Karniadakis"
      ],
      "abstract": "Neural operators are promising surrogates for dynamical systems but when\ntrained with standard L2 losses they tend to oversmooth fine-scale turbulent\nstructures. Here, we show that combining operator learning with generative\nmodeling overcomes this limitation. We consider three practical turbulent-flow\nchallenges where conventional neural operators fail: spatio-temporal\nsuper-resolution, forecasting, and sparse flow reconstruction. For Schlieren\njet super-resolution, an adversarially trained neural operator (adv-NO) reduces\nthe energy-spectrum error by 15x while preserving sharp gradients at neural\noperator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NO\ntrained on only 160 timesteps from a single trajectory forecasts accurately for\nfive eddy-turnover times and offers 114x wall-clock speed-up at inference than\nthe baseline diffusion-based forecasters, enabling near-real-time rollouts. For\nreconstructing cylinder wake flows from highly sparse Particle Tracking\nVelocimetry-like inputs, a conditional generative model infers full 3D velocity\nand pressure fields with correct phase alignment and statistics. These advances\nenable accurate reconstruction and forecasting at low compute cost, bringing\nnear-real-time analysis and control within reach in experimental and\ncomputational fluid mechanics. See our project page:\nhttps://vivekoommen.github.io/Gen4Turb/",
      "pdf_url": "http://arxiv.org/pdf/2509.08752v1",
      "published": "2025-09-10T16:42:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08752v1",
      "categories": [
        "physics.flu-dyn",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FinZero: Launching Multi-modal Financial Time Series Forecast with Large Reasoning Model",
      "authors": [
        "Yanlong Wang",
        "Jian Xu",
        "Fei Ma",
        "Hongkang Zhang",
        "Hang Yu",
        "Tiantian Gao",
        "Yu Wang",
        "Haochen You",
        "Shao-Lun Huang",
        "Danny Dongning Sun",
        "Xiao-Ping Zhang"
      ],
      "abstract": "Financial time series forecasting is both highly significant and challenging.\nPrevious approaches typically standardized time series data before feeding it\ninto forecasting models, but this encoding process inherently leads to a loss\nof important information. Moreover, past time series models generally require\nfixed numbers of variables or lookback window lengths, which further limits the\nscalability of time series forecasting. Besides, the interpretability and the\nuncertainty in forecasting remain areas requiring further research, as these\nfactors directly impact the reliability and practical value of predictions. To\naddress these issues, we first construct a diverse financial image-text dataset\n(FVLDB) and develop the Uncertainty-adjusted Group Relative Policy Optimization\n(UARPO) method to enable the model not only output predictions but also analyze\nthe uncertainty of those predictions. We then proposed FinZero, a multimodal\npre-trained model finetuned by UARPO to perform reasoning, prediction, and\nanalytical understanding on the FVLDB financial time series. Extensive\nexperiments validate that FinZero exhibits strong adaptability and scalability.\nAfter fine-tuning with UARPO, FinZero achieves an approximate 13.48\\%\nimprovement in prediction accuracy over GPT-4o in the high-confidence group,\ndemonstrating the effectiveness of reinforcement learning fine-tuning in\nmultimodal large model, including in financial time series forecasting tasks.",
      "pdf_url": "http://arxiv.org/pdf/2509.08742v1",
      "published": "2025-09-10T16:32:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08742v1",
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ]
    },
    {
      "title": "DEQuify your force field: More efficient simulations using deep equilibrium models",
      "authors": [
        "Andreas Burger",
        "Luca Thiede",
        "Alán Aspuru-Guzik",
        "Nandita Vijaykumar"
      ],
      "abstract": "Machine learning force fields show great promise in enabling more accurate\nmolecular dynamics simulations compared to manually derived ones. Much of the\nprogress in recent years was driven by exploiting prior knowledge about\nphysical systems, in particular symmetries under rotation, translation, and\nreflections. In this paper, we argue that there is another important piece of\nprior information that, thus fa,r hasn't been explored: Simulating a molecular\nsystem is necessarily continuous, and successive states are therefore extremely\nsimilar. Our contribution is to show that we can exploit this information by\nrecasting a state-of-the-art equivariant base model as a deep equilibrium\nmodel. This allows us to recycle intermediate neural network features from\nprevious time steps, enabling us to improve both accuracy and speed by\n$10\\%-20\\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ\nbase model. The training is also much more memory efficient, allowing us to\ntrain more expressive models on larger systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.08734v1",
      "published": "2025-09-10T16:23:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08734v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates",
      "authors": [
        "Hyunjun Kim",
        "Junwoo Ha",
        "Sangyoon Yu",
        "Haon Park"
      ],
      "abstract": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming.",
      "pdf_url": "http://arxiv.org/pdf/2509.08729v1",
      "published": "2025-09-10T16:17:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08729v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Explainability of CNN Based Classification Models for Acoustic Signal",
      "authors": [
        "Zubair Faruqui",
        "Mackenzie S. McIntire",
        "Rahul Dubey",
        "Jay McEntee"
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a critical tool for\ninterpreting the predictions of complex deep learning models. While XAI has\nbeen increasingly applied in various domains within acoustics, its use in\nbioacoustics, which involves analyzing audio signals from living organisms,\nremains relatively underexplored. In this paper, we investigate the\nvocalizations of a bird species with strong geographic variation throughout its\nrange in North America. Audio recordings were converted into spectrogram images\nand used to train a deep Convolutional Neural Network (CNN) for classification,\nachieving an accuracy of 94.8\\%. To interpret the model's predictions, we\napplied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT,\nGrad-CAM) XAI techniques. These techniques produced different but complementary\nexplanations, and when their explanations were considered together, they\nprovided more complete and interpretable insights into the model's\ndecision-making. This work highlights the importance of using a combination of\nXAI techniques to improve trust and interoperability, not only in broader\nacoustics signal analysis but also argues for broader applicability in\ndifferent domain specific tasks.",
      "pdf_url": "http://arxiv.org/pdf/2509.08717v1",
      "published": "2025-09-10T16:11:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08717v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems",
      "authors": [
        "Ziming Luo",
        "Atoosa Kasirzadeh",
        "Nihar B. Shah"
      ],
      "abstract": "AI scientist systems, capable of autonomously executing the full research\nworkflow from hypothesis generation and experimentation to paper writing, hold\nsignificant potential for accelerating scientific discovery. However, the\ninternal workflow of these systems have not been closely examined. This lack of\nscrutiny poses a risk of introducing flaws that could undermine the integrity,\nreliability, and trustworthiness of their research outputs. In this paper, we\nidentify four potential failure modes in contemporary AI scientist systems:\ninappropriate benchmark selection, data leakage, metric misuse, and post-hoc\nselection bias. To examine these risks, we design controlled experiments that\nisolate each failure mode while addressing challenges unique to evaluating AI\nscientist systems. Our assessment of two prominent open-source AI scientist\nsystems reveals the presence of several failures, across a spectrum of\nseverity, which can be easily overlooked in practice. Finally, we demonstrate\nthat access to trace logs and code from the full automated workflow enables far\nmore effective detection of such failures than examining the final paper alone.\nWe thus recommend journals and conferences evaluating AI-generated research to\nmandate submission of these artifacts alongside the paper to ensure\ntransparency, accountability, and reproducibility.",
      "pdf_url": "http://arxiv.org/pdf/2509.08713v1",
      "published": "2025-09-10T16:04:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08713v1",
      "categories": [
        "cs.AI",
        "cs.DL"
      ]
    },
    {
      "title": "One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases",
      "authors": [
        "Shalima Binta Manir",
        "Tim Oates"
      ],
      "abstract": "We introduce a novel Theory of Mind (ToM) framework inspired by dual-process\ntheories from cognitive science, integrating a fast, habitual graph-based\nreasoning system (System 1), implemented via graph convolutional networks\n(GCNs), and a slower, context-sensitive meta-adaptive learning system (System\n2), driven by meta-learning techniques. Our model dynamically balances\nintuitive and deliberative reasoning through a learned context gate mechanism.\nWe validate our architecture on canonical false-belief tasks and systematically\nexplore its capacity to replicate hallmark cognitive biases associated with\ndual-process theory, including anchoring, cognitive-load fatigue, framing\neffects, and priming effects. Experimental results demonstrate that our\ndual-process approach closely mirrors human adaptive behavior, achieves robust\ngeneralization to unseen contexts, and elucidates cognitive mechanisms\nunderlying reasoning biases. This work bridges artificial intelligence and\ncognitive theory, paving the way for AI systems exhibiting nuanced, human-like\nsocial cognition and adaptive decision-making capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2509.08705v1",
      "published": "2025-09-10T15:55:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08705v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals",
      "authors": [
        "Stefan Podgorski",
        "Sourav Garg",
        "Mehdi Hosseinzadeh",
        "Lachlan Mares",
        "Feras Dayoub",
        "Ian Reid"
      ],
      "abstract": "Visual navigation in robotics traditionally relies on globally-consistent 3D\nmaps or learned controllers, which can be computationally expensive and\ndifficult to generalize across diverse environments. In this work, we present a\nnovel RGB-only, object-level topometric navigation pipeline that enables\nzero-shot, long-horizon robot navigation without requiring 3D maps or\npre-trained controllers. Our approach integrates global topological path\nplanning with local metric trajectory control, allowing the robot to navigate\ntowards object-level sub-goals while avoiding obstacles. We address key\nlimitations of previous methods by continuously predicting local trajectory\nusing monocular depth and traversability estimation, and incorporating an\nauto-switching mechanism that falls back to a baseline controller when\nnecessary. The system operates using foundational models, ensuring open-set\napplicability without the need for domain-specific fine-tuning. We demonstrate\nthe effectiveness of our method in both simulated environments and real-world\ntests, highlighting its robustness and deployability. Our approach outperforms\nexisting state-of-the-art methods, offering a more adaptable and effective\nsolution for visual navigation in open-set environments. The source code is\nmade publicly available: https://github.com/podgorki/TANGO.",
      "pdf_url": "http://arxiv.org/pdf/2509.08699v1",
      "published": "2025-09-10T15:43:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08699v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference",
      "authors": [
        "Guoqing Ma",
        "Jia Zhu",
        "Hanghui Guo",
        "Weijie Shi",
        "Jiawei Shen",
        "Jingjiang Liu",
        "Yidan Liang"
      ],
      "abstract": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.08682v1",
      "published": "2025-09-10T15:22:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08682v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network",
      "authors": [
        "Liangjin Liu",
        "Haoyang Zheng",
        "Pei Zhou"
      ],
      "abstract": "Isolated Sign Language Recognition (ISLR) is challenged by gestures that are\nmorphologically similar yet semantically distinct, a problem rooted in the\ncomplex interplay between hand shape and motion trajectory. Existing methods,\noften relying on a single reference frame, struggle to resolve this geometric\nambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a\ndual-reference, dual-stream architecture that decouples and models gesture\nmorphology and trajectory in separate, complementary coordinate systems. Our\napproach utilizes a wrist-centric frame for view-invariant shape analysis and a\nfacial-centric frame for context-aware trajectory modeling. These streams are\nprocessed by specialized networks-a topology-aware graph convolution for shape\nand a Finsler geometry-based encoder for trajectory-and are integrated via a\ngeometry-driven optimal transport fusion mechanism. DSLNet sets a new\nstate-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the\nchallenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with\nsignificantly fewer parameters than competing models.",
      "pdf_url": "http://arxiv.org/pdf/2509.08661v1",
      "published": "2025-09-10T14:58:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08661v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.m; I.2.0"
      ]
    },
    {
      "title": "Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions",
      "authors": [
        "Amirhossein Taherpour",
        "Abbas Taherpour",
        "Tamer Khattab"
      ],
      "abstract": "This paper presents a feature-based Partially Observable Markov Decision\nProcess (POMDP) framework for quantum network routing, combining belief-state\nplanning with Graph Neural Networks (GNNs) to address partial observability,\ndecoherence, and scalability challenges in dynamic quantum systems. Our\napproach encodes complex quantum network dynamics, including entanglement\ndegradation and time-varying channel noise, into a low-dimensional feature\nspace, enabling efficient belief updates and scalable policy learning. The core\nof our framework is a hybrid GNN-POMDP architecture that processes\ngraph-structured representations of entangled links to learn routing policies,\ncoupled with a noise-adaptive mechanism that fuses POMDP belief updates with\nGNN outputs for robust decision making. We provide a theoretical analysis\nestablishing guarantees for belief convergence, policy improvement, and\nrobustness to noise. Experiments on simulated quantum networks with up to 100\nnodes demonstrate significant improvements in routing fidelity and entanglement\ndelivery rates compared to state-of-the-art baselines, particularly under high\ndecoherence and nonstationary conditions.",
      "pdf_url": "http://arxiv.org/pdf/2509.08654v1",
      "published": "2025-09-10T14:50:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08654v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ]
    },
    {
      "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations",
      "authors": [
        "Ron F. Del Rosario",
        "Klaudia Krawiecka",
        "Christian Schroeder de Witt"
      ],
      "abstract": "As Large Language Model (LLM) agents become increasingly capable of\nautomating complex, multi-step tasks, the need for robust, secure, and\npredictable architectural patterns is paramount. This paper provides a\ncomprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic\ndesign that separates strategic planning from tactical execution. We explore\nthe foundational principles of P-t-E, detailing its core components - the\nPlanner and the Executor - and its architectural advantages in predictability,\ncost-efficiency, and reasoning quality over reactive patterns like ReAct\n(Reason + Act). A central focus is placed on the security implications of this\ndesign, particularly its inherent resilience to indirect prompt injection\nattacks by establishing control-flow integrity. We argue that while P-t-E\nprovides a strong foundation, a defense-in-depth strategy is necessary, and we\ndetail essential complementary controls such as the Principle of Least\nPrivilege, task-scoped tool access, and sandboxed code execution. To make these\nprinciples actionable, this guide provides detailed implementation blueprints\nand working code references for three leading agentic frameworks: LangChain\n(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing\nthe P-t-E pattern is analyzed, highlighting unique features like LangGraph's\nstateful graphs for re-planning, CrewAI's declarative tool scoping for\nsecurity, and AutoGen's built-in Docker sandboxing. Finally, we discuss\nadvanced patterns, including dynamic re-planning loops, parallel execution with\nDirected Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop\n(HITL) verification, to offer a complete strategic blueprint for architects,\ndevelopers, and security engineers aiming to build production-grade, resilient,\nand trustworthy LLM agents.",
      "pdf_url": "http://arxiv.org/pdf/2509.08646v1",
      "published": "2025-09-10T14:41:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08646v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts",
      "authors": [
        "Lauren H. Cooke",
        "Matthias Jung",
        "Jan M. Brendel",
        "Nora M. Kerkovits",
        "Borek Foldyna",
        "Michael T. Lu",
        "Vineet K. Raghu"
      ],
      "abstract": "Chest radiographs (CXRs) are among the most common tests in medicine.\nAutomated image interpretation may reduce radiologists\\' workload and expand\naccess to diagnostic expertise. Deep learning multi-task and foundation models\nhave shown strong performance for CXR interpretation but are vulnerable to\nshortcut learning, where models rely on spurious and off-target correlations\nrather than clinically relevant features to make decisions. We introduce\nRoentMod, a counterfactual image editing framework that generates anatomically\nrealistic CXRs with user-specified, synthetic pathology while preserving\nunrelated anatomical features of the original scan. RoentMod combines an\nopen-source medical image generator (RoentGen) with an image-to-image\nmodification model without requiring retraining. In reader studies with\nboard-certified radiologists and radiology residents, RoentMod-produced images\nappeared realistic in 93\\% of cases, correctly incorporated the specified\nfinding in 89-99\\% of cases, and preserved native anatomy comparable to real\nfollow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task\nand foundation models frequently exploit off-target pathology as shortcuts,\nlimiting their specificity. Incorporating RoentMod-generated counterfactual\nimages during training mitigated this vulnerability, improving model\ndiscrimination across multiple pathologies by 3-19\\% AUC in internal validation\nand by 1-11\\% for 5 out of 6 tested pathologies in external testing. These\nfindings establish RoentMod as a broadly applicable tool for probing and\ncorrecting shortcut learning in medical AI. By enabling controlled\ncounterfactual interventions, RoentMod enhances the robustness and\ninterpretability of CXR interpretation models and provides a generalizable\nstrategy for improving foundation models in medical imaging.",
      "pdf_url": "http://arxiv.org/pdf/2509.08640v1",
      "published": "2025-09-10T14:35:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08640v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "I.4, I.2, J.3"
      ]
    },
    {
      "title": "UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation",
      "authors": [
        "Zhihao Zhao",
        "Yinzheng Zhao",
        "Junjie Yang",
        "Xiangtong Yao",
        "Quanmin Liang",
        "Daniel Zapp",
        "Kai Huang",
        "Nassir Navab",
        "M. Ali Nasseri"
      ],
      "abstract": "Significant advancements in AI-driven multimodal medical image diagnosis have\nled to substantial improvements in ophthalmic disease identification in recent\nyears. However, acquiring paired multimodal ophthalmic images remains\nprohibitively expensive. While fundus photography is simple and cost-effective,\nthe limited availability of OCT data and inherent modality imbalance hinder\nfurther progress. Conventional approaches that rely solely on fundus or textual\nfeatures often fail to capture fine-grained spatial information, as each\nimaging modality provides distinct cues about lesion predilection sites. In\nthis study, we propose a novel unpaired multimodal framework \\UOPSL that\nutilizes extensive OCT-derived spatial priors to dynamically identify\npredilection sites, enhancing fundus image-based disease recognition. Our\napproach bridges unpaired fundus and OCTs via extended disease text\ndescriptions. Initially, we employ contrastive learning on a large corpus of\nunpaired OCT and fundus images while simultaneously learning the predilection\nsites matrix in the OCT latent space. Through extensive optimization, this\nmatrix captures lesion localization patterns within the OCT feature space.\nDuring the fine-tuning or inference phase of the downstream classification task\nbased solely on fundus images, where paired OCT data is unavailable, we\neliminate OCT input and utilize the predilection sites matrix to assist in\nfundus image classification learning. Extensive experiments conducted on 9\ndiverse datasets across 28 critical categories demonstrate that our framework\noutperforms existing benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2509.08624v1",
      "published": "2025-09-10T14:19:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08624v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4.10"
      ]
    },
    {
      "title": "OTESGN: Optimal Transport-Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis",
      "authors": [
        "Xinfeng Liao",
        "Xuanqi Chen",
        "Lianxi Wang",
        "Jiahuan Yang",
        "Zhuowei Chen",
        "Ziying Rong"
      ],
      "abstract": "Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and\ndetermine their sentiment polarity. While dependency trees combined with\ncontextual semantics provide structural cues, existing approaches often rely on\ndot-product similarity and fixed graphs, which limit their ability to capture\nnonlinear associations and adapt to noisy contexts. To address these\nlimitations, we propose the Optimal Transport-Enhanced Syntactic-Semantic Graph\nNetwork (OTESGN), a model that jointly integrates structural and distributional\nsignals. Specifically, a Syntactic Graph-Aware Attention module models global\ndependencies with syntax-guided masking, while a Semantic Optimal Transport\nAttention module formulates aspect-opinion association as a distribution\nmatching problem solved via the Sinkhorn algorithm. An Adaptive Attention\nFusion mechanism balances heterogeneous features, and contrastive\nregularization enhances robustness. Extensive experiments on three benchmark\ndatasets (Rest14, Laptop14, and Twitter) demonstrate that OTESGN delivers\nstate-of-the-art performance. Notably, it surpasses competitive baselines by up\nto +1.30 Macro-F1 on Laptop14 and +1.01 on Twitter. Ablation studies and\nvisualization analyses further highlight OTESGN's ability to capture\nfine-grained sentiment associations and suppress noise from irrelevant context.",
      "pdf_url": "http://arxiv.org/pdf/2509.08612v2",
      "published": "2025-09-10T14:08:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08612v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques",
      "authors": [
        "Alireza Sameh",
        "Mehrdad Rostami",
        "Mourad Oussalah",
        "Vahid Farrahi"
      ],
      "abstract": "Purpose: We compared the performance of deep learning (DL) and classical\nmachine learning (ML) algorithms for the classification of 24-hour movement\nbehavior into sleep, sedentary, light intensity physical activity (LPA), and\nmoderate-to-vigorous intensity physical activity (MVPA). Methods: Open-access\ndata from 151 adults wearing a wrist-worn accelerometer (Axivity-AX3) was used.\nParticipants were randomly divided into training, validation, and test sets\n(121, 15, and 15 participants each). Raw acceleration signals were segmented\ninto non-overlapping 10-second windows, and then a total of 104 handcrafted\nfeatures were extracted. Four DL algorithms-Long Short-Term Memory (LSTM),\nBidirectional Long Short-Term Memory (BiLSTM), Gated Recurrent Units (GRU), and\nOne-Dimensional Convolutional Neural Network (1D-CNN)-were trained using raw\nacceleration signals and with handcrafted features extracted from these signals\nto predict 24-hour movement behavior categories. The handcrafted features were\nalso used to train classical ML algorithms, namely Random Forest (RF), Support\nVector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression\n(LR), Artificial Neural Network (ANN), and Decision Tree (DT) for classifying\n24-hour movement behavior intensities. Results: LSTM, BiLSTM, and GRU showed an\noverall accuracy of approximately 85% when trained with raw acceleration\nsignals, and 1D-CNN an overall accuracy of approximately 80%. When trained on\nhandcrafted features, the overall accuracy for both DL and classical ML\nalgorithms ranged from 70% to 81%. Overall, there was a higher confusion in\nclassification of MVPA and LPA, compared to sleep and sedentary categories.\nConclusion: DL methods with raw acceleration signals had only slightly better\nperformance in predicting 24-hour movement behavior intensities, compared to\nwhen DL and classical ML were trained with handcrafted features.",
      "pdf_url": "http://arxiv.org/pdf/2509.08606v1",
      "published": "2025-09-10T14:04:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08606v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications",
      "authors": [
        "Anran Li",
        "Lingfei Qian",
        "Mengmeng Du",
        "Yu Yin",
        "Yan Hu",
        "Zihao Sun",
        "Yihang Fu",
        "Erica Stutz",
        "Xuguang Ai",
        "Qianqian Xie",
        "Rui Zhu",
        "Jimin Huang",
        "Yifan Yang",
        "Siru Liu",
        "Yih-Chung Tham",
        "Lucila Ohno-Machado",
        "Hyunghoon Cho",
        "Zhiyong Lu",
        "Hua Xu",
        "Qingyu Chen"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.",
      "pdf_url": "http://arxiv.org/pdf/2509.08604v1",
      "published": "2025-09-10T14:02:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08604v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "No-Knowledge Alarms for Misaligned LLMs-as-Judges",
      "authors": [
        "Andrés Corrada-Emmanuel"
      ],
      "abstract": "If we use LLMs as judges to evaluate the complex decisions of other LLMs, who\nor what monitors the judges? Infinite monitoring chains are inevitable whenever\nwe do not know the ground truth of the decisions by experts and we do not want\nto trust them. One way to ameliorate our evaluation uncertainty is to exploit\nthe use of logical consistency between disagreeing experts. By observing how\nLLM judges agree and disagree while grading other LLMs, we can compute the only\npossible evaluations of their grading ability. For example, if two LLM judges\ndisagree on which tasks a third one completed correctly, they cannot both be\n100\\% correct in their judgments. This logic can be formalized as a Linear\nProgramming problem in the space of integer response counts for any finite\ntest. We use it here to develop no-knowledge alarms for misaligned LLM judges.\nThe alarms can detect, with no false positives, that at least one member or\nmore of an ensemble of judges are violating a user specified grading ability\nrequirement.",
      "pdf_url": "http://arxiv.org/pdf/2509.08593v1",
      "published": "2025-09-10T13:46:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08593v1",
      "categories": [
        "cs.AI",
        "stat.ML",
        "90C05, 68T27",
        "I.2.3; F.4.1"
      ]
    },
    {
      "title": "Interpretability as Alignment: Making Internal Understanding a Design Principle",
      "authors": [
        "Aadit Sengupta",
        "Pratinav Seth",
        "Vinay Kumar Sankarapu"
      ],
      "abstract": "Large neural models are increasingly deployed in high-stakes settings,\nraising concerns about whether their behavior reliably aligns with human\nvalues. Interpretability provides a route to internal transparency by revealing\nthe computations that drive outputs. We argue that interpretability especially\nmechanistic approaches should be treated as a design principle for alignment,\nnot an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer\nintuitive but correlational explanations, while mechanistic techniques like\ncircuit tracing or activation patching yield causal insight into internal\nfailures, including deceptive or misaligned reasoning that behavioral methods\nlike RLHF, red teaming, or Constitutional AI may overlook. Despite these\nadvantages, interpretability faces challenges of scalability, epistemic\nuncertainty, and mismatches between learned representations and human concepts.\nOur position is that progress on safe and trustworthy AI will depend on making\ninterpretability a first-class objective of AI research and development,\nensuring that systems are not only effective but also auditable, transparent,\nand aligned with human intent.",
      "pdf_url": "http://arxiv.org/pdf/2509.08592v1",
      "published": "2025-09-10T13:45:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08592v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models",
      "authors": [
        "Garry Yang",
        "Zizhe Chen",
        "Man Hon Wong",
        "Haoyu Lei",
        "Yongqiang Chen",
        "Zhenguo Li",
        "Kaiwen Zhou",
        "James Cheng"
      ],
      "abstract": "Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos.",
      "pdf_url": "http://arxiv.org/pdf/2509.08538v2",
      "published": "2025-09-10T12:34:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08538v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Agents of Discovery",
      "authors": [
        "Sascha Diefenbacher",
        "Anna Hallin",
        "Gregor Kasieczka",
        "Michael Krämer",
        "Anne Lauscher",
        "Tim Lukas"
      ],
      "abstract": "The substantial data volumes encountered in modern particle physics and other\ndomains of fundamental physics research allow (and require) the use of\nincreasingly complex data analysis tools and workflows. While the use of\nmachine learning (ML) tools for data analysis has recently proliferated, these\ntools are typically special-purpose algorithms that rely, for example, on\nencoded physics knowledge to reach optimal performance. In this work, we\ninvestigate a new and orthogonal direction: Using recent progress in large\nlanguage models (LLMs) to create a team of agents -- instances of LLMs with\nspecific subtasks -- that jointly solve data analysis-based research problems\nin a way similar to how a human researcher might: by creating code to operate\nstandard tools and libraries (including ML systems) and by building on results\nof previous iterations. If successful, such agent-based systems could be\ndeployed to automate routine analysis components to counteract the increasing\ncomplexity of modern tool chains. To investigate the capabilities of\ncurrent-generation commercial LLMs, we consider the task of anomaly detection\nvia the publicly available and highly-studied LHC Olympics dataset. Several\ncurrent models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated\nand their stability tested. Overall, we observe the capacity of the agent-based\nsystem to solve this data analysis problem. The best agent-created solutions\nmirror the performance of human state-of-the-art results.",
      "pdf_url": "http://arxiv.org/pdf/2509.08535v1",
      "published": "2025-09-10T12:25:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08535v1",
      "categories": [
        "hep-ph",
        "cs.AI",
        "cs.LG",
        "hep-ex",
        "physics.data-an"
      ]
    },
    {
      "title": "AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution",
      "authors": [
        "Felix Mächtle",
        "Nils Loose",
        "Jan-Niclas Serr",
        "Jonas Sander",
        "Thomas Eisenbarth"
      ],
      "abstract": "Symbolic execution is a powerful technique for software testing, but suffers\nfrom limitations when encountering external functions, such as native methods\nor third-party libraries. Existing solutions often require additional context,\nexpensive SMT solvers, or manual intervention to approximate these functions\nthrough symbolic stubs. In this work, we propose a novel approach to\nautomatically generate symbolic stubs for external functions during symbolic\nexecution that leverages Genetic Programming. When the symbolic executor\nencounters an external function, AutoStub generates training data by executing\nthe function on randomly generated inputs and collecting the outputs. Genetic\nProgramming then derives expressions that approximate the behavior of the\nfunction, serving as symbolic stubs. These automatically generated stubs allow\nthe symbolic executor to continue the analysis without manual intervention,\nenabling the exploration of program paths that were previously intractable. We\ndemonstrate that AutoStub can automatically approximate external functions with\nover 90% accuracy for 55% of the functions evaluated, and can infer\nlanguage-specific behaviors that reveal edge cases crucial for software\ntesting.",
      "pdf_url": "http://arxiv.org/pdf/2509.08524v1",
      "published": "2025-09-10T12:08:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08524v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast Marching Tree for Dynamic Replanning",
      "authors": [
        "Soheil Espahbodini Nia"
      ],
      "abstract": "Path planning in dynamic environments remains a core challenge in robotics,\nespecially as autonomous systems are deployed in unpredictable spaces such as\nwarehouses and public roads. While algorithms like Fast Marching Tree\n(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their\nsingle-pass design prevents path revisions which are essential for real-time\nadaptation. On the other hand, full replanning is often too computationally\nexpensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching\nTree algorithm that enables efficient and consistent replanning in dynamic\nenvironments. We revisit the neighbor selection rule of FMT$^{*}$ and\ndemonstrate that a minimal change overcomes its single-pass limitation,\nenabling the algorithm to update cost-to-come values upon discovering better\nconnections without sacrificing asymptotic optimality or computational\nefficiency. By maintaining a cost-ordered priority queue and applying a\nselective update condition that uses an expanding neighbor to identify and\ntrigger the re-evaluation of any node with a potentially suboptimal path,\nFMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the\nenvironment evolves. This targeted strategy preserves the inherent efficiency\nof FMT$^{*}$ while enabling robust adaptation to changes in obstacle\nconfiguration. FMT$^{x}$ is proven to recover an asymptotically optimal\nsolution after environmental changes. Experimental results demonstrate that\nFMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more\nswiftly to dynamic events with lower computational overhead and thus offering a\nmore effective solution for real-time robotic navigation in unpredictable\nworlds.",
      "pdf_url": "http://arxiv.org/pdf/2509.08521v1",
      "published": "2025-09-10T11:57:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08521v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "I.2.9; I.2.8"
      ]
    },
    {
      "title": "Variational Rank Reduction Autoencoders for Generative",
      "authors": [
        "Alicia Tierz",
        "Jad Mounayer",
        "Beatriz Moya",
        "Francisco Chinesta"
      ],
      "abstract": "Generative thermal design for complex geometries is fundamental in many areas\nof engineering, yet it faces two main challenges: the high computational cost\nof high-fidelity simulations and the limitations of conventional generative\nmodels. Approaches such as autoencoders (AEs) and variational autoencoders\n(VAEs) often produce unstructured latent spaces with discontinuities, which\nrestricts their capacity to explore designs and generate physically consistent\nsolutions.\n  To address these limitations, we propose a hybrid framework that combines\nVariational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks\n(DeepONets). The VRRAE introduces a truncated SVD within the latent space,\nleading to continuous, interpretable, and well-structured representations that\nmitigate posterior collapse and improve geometric reconstruction. The DeepONet\nthen exploits this compact latent encoding in its branch network, together with\nspatial coordinates in the trunk network, to predict temperature gradients\nefficiently and accurately.\n  This hybrid approach not only enhances the quality of generated geometries\nand the accuracy of gradient prediction, but also provides a substantial\nadvantage in inference efficiency compared to traditional numerical solvers.\nOverall, the study underscores the importance of structured latent\nrepresentations for operator learning and highlights the potential of combining\ngenerative models and operator networks in thermal design and broader\nengineering applications.",
      "pdf_url": "http://arxiv.org/pdf/2509.08515v1",
      "published": "2025-09-10T11:45:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08515v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making",
      "authors": [
        "Kechen Jiao",
        "Zhirui Fang",
        "Jiahao Liu",
        "Bei Li",
        "Qifan Wang",
        "Xinyu Liu",
        "Junhao Ruan",
        "Zhongjian Qiao",
        "Yifan Zhu",
        "Yaxin Xu",
        "Jingang Wang",
        "Xiu Li"
      ],
      "abstract": "Using effective generalization capabilities of vision language models (VLMs)\nin context-specific dynamic tasks for embodied artificial intelligence remains\na significant challenge. Although supervised fine-tuned models can better align\nwith the real physical world, they still exhibit sluggish responses and\nhallucination issues in dynamically changing environments, necessitating\nfurther alignment. Existing post-SFT methods, reliant on reinforcement learning\nand chain-of-thought (CoT) approaches, are constrained by sparse rewards and\naction-only optimization, resulting in low sample efficiency, poor consistency,\nand model degradation. To address these issues, this paper proposes\nThought-Centric Preference Optimization (TCPO) for effective embodied\ndecision-making. Specifically, TCPO introduces a stepwise preference-based\noptimization approach, transforming sparse reward signals into richer step\nsample pairs. It emphasizes the alignment of the model's intermediate reasoning\nprocess, mitigating the problem of model degradation. Moreover, by\nincorporating Action Policy Consistency Constraint (APC), it further imposes\nconsistency constraints on the model output. Experiments in the ALFWorld\nenvironment demonstrate an average success rate of 26.67%, achieving a 6%\nimprovement over RL4VLM and validating the effectiveness of our approach in\nmitigating model degradation after fine-tuning. These results highlight the\npotential of integrating preference-based learning techniques with CoT\nprocesses to enhance the decision-making capabilities of vision-language models\nin embodied agents.",
      "pdf_url": "http://arxiv.org/pdf/2509.08500v1",
      "published": "2025-09-10T11:16:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08500v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants",
      "authors": [
        "Benjamin Sturgeon",
        "Daniel Samuelson",
        "Jacob Haimes",
        "Jacy Reese Anthis"
      ],
      "abstract": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets.",
      "pdf_url": "http://arxiv.org/pdf/2509.08494v1",
      "published": "2025-09-10T11:10:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08494v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Send to which account? Evaluation of an LLM-based Scambaiting System",
      "authors": [
        "Hossein Siadati",
        "Haadi Jafarian",
        "Sima Jafarikhah"
      ],
      "abstract": "Scammers are increasingly harnessing generative AI(GenAI) technologies to\nproduce convincing phishing content at scale, amplifying financial fraud and\nundermining public trust. While conventional defenses, such as detection\nalgorithms, user training, and reactive takedown efforts remain important, they\noften fall short in dismantling the infrastructure scammers depend on,\nincluding mule bank accounts and cryptocurrency wallets. To bridge this gap, a\nproactive and emerging strategy involves using conversational honeypots to\nengage scammers and extract actionable threat intelligence. This paper presents\nthe first large-scale, real-world evaluation of a scambaiting system powered by\nlarge language models (LLMs). Over a five-month deployment, the system\ninitiated over 2,600 engagements with actual scammers, resulting in a dataset\nof more than 18,700 messages. It achieved an Information Disclosure Rate (IDR)\nof approximately 32%, successfully extracting sensitive financial information\nsuch as mule accounts. Additionally, the system maintained a Human Acceptance\nRate (HAR) of around 70%, indicating strong alignment between LLM-generated\nresponses and human operator preferences. Alongside these successes, our\nanalysis reveals key operational challenges. In particular, the system\nstruggled with engagement takeoff: only 48.7% of scammers responded to the\ninitial seed message sent by defenders. These findings highlight the need for\nfurther refinement and provide actionable insights for advancing the design of\nautomated scambaiting systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.08493v1",
      "published": "2025-09-10T11:08:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08493v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "K.6.5; I.2.7"
      ]
    },
    {
      "title": "A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models",
      "authors": [
        "Edwine Nabahirwa",
        "Wei Song",
        "Minghua Zhang",
        "Yi Fang",
        "Zhou Ni"
      ],
      "abstract": "Underwater object detection (UOD) is vital to diverse marine applications,\nincluding oceanographic research, underwater robotics, and marine conservation.\nHowever, UOD faces numerous challenges that compromise its performance. Over\nthe years, various methods have been proposed to address these issues, but they\noften fail to fully capture the complexities of underwater environments. This\nreview systematically categorizes UOD challenges into five key areas: Image\nquality degradation, target-related issues, data-related challenges,\ncomputational and processing constraints, and limitations in detection\nmethodologies. To address these challenges, we analyze the progression from\ntraditional image processing and object detection techniques to modern\napproaches. Additionally, we explore the potential of large vision-language\nmodels (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated\nin other domains. We also present case studies, including synthetic dataset\ngeneration using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review\nidentifies three key insights: (i) Current UOD methods are insufficient to\nfully address challenges like image degradation and small object detection in\ndynamic underwater environments. (ii) Synthetic data generation using LVLMs\nshows potential for augmenting datasets but requires further refinement to\nensure realism and applicability. (iii) LVLMs hold significant promise for UOD,\nbut their real-time application remains under-explored, requiring further\nresearch on optimization techniques.",
      "pdf_url": "http://arxiv.org/pdf/2509.08490v1",
      "published": "2025-09-10T11:01:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08490v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation",
      "authors": [
        "Kaleem Ahmad"
      ],
      "abstract": "Prompt-driven image analysis converts a single natural-language instruction\ninto multiple steps: locate, segment, edit, and describe. We present a\npractical case study of a unified pipeline that combines open-vocabulary\ndetection, promptable segmentation, text-conditioned inpainting, and\nvision-language description into a single workflow. The system works end to end\nfrom a single prompt, retains intermediate artifacts for transparent debugging\n(such as detections, masks, overlays, edited images, and before and after\ncomposites), and provides the same functionality through an interactive UI and\na scriptable CLI for consistent, repeatable runs. We highlight integration\nchoices that reduce brittleness, including threshold adjustments, mask\ninspection with light morphology, and resource-aware defaults. In a small,\nsingle-word prompt segment, detection and segmentation produced usable masks in\nover 90% of cases with an accuracy above 85% based on our criteria. On a\nhigh-end GPU, inpainting makes up 60 to 75% of total runtime under typical\nguidance and sampling settings, which highlights the need for careful tuning.\nThe study offers implementation-guided advice on thresholds, mask tightness,\nand diffusion parameters, and details version pinning, artifact logging, and\nseed control to support replay. Our contribution is a transparent, reliable\npattern for assembling modern vision and multimodal models behind a single\nprompt, with clear guardrails and operational practices that improve\nreliability in object replacement, scene augmentation, and removal.",
      "pdf_url": "http://arxiv.org/pdf/2509.08489v1",
      "published": "2025-09-10T11:00:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08489v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition",
      "authors": [
        "Jing-Tong Tzeng",
        "Carlos Busso",
        "Chi-Chun Lee"
      ],
      "abstract": "Speech emotion recognition (SER) plays a critical role in building\nemotion-aware speech systems, but its performance degrades significantly under\nnoisy conditions. Although speech enhancement (SE) can improve robustness, it\noften introduces artifacts that obscure emotional cues and adds computational\noverhead to the pipeline. Multi-task learning (MTL) offers an alternative by\njointly optimizing SE and SER tasks. However, conventional shared-backbone\nmodels frequently suffer from gradient interference and representational\nconflicts between tasks. To address these challenges, we propose the Sparse\nMixture-of-Experts Representation Integration Technique (Sparse MERIT), a\nflexible MTL framework that applies frame-wise expert routing over\nself-supervised speech representations. Sparse MERIT incorporates task-specific\ngating networks that dynamically select from a shared pool of experts for each\nframe, enabling parameter-efficient and task-adaptive representation learning.\nExperiments on the MSP-Podcast corpus show that Sparse MERIT consistently\noutperforms baseline models on both SER and SE tasks. Under the most\nchallenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT\nimproves SER F1-macro by an average of 12.0% over a baseline relying on a SE\npre-processing strategy, and by 3.4% over a naive MTL baseline, with\nstatistical significance on unseen noise conditions. For SE, Sparse MERIT\nimproves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and\nby 20.0% over the naive MTL baseline. These results demonstrate that Sparse\nMERIT provides robust and generalizable performance for both emotion\nrecognition and enhancement tasks in noisy environments.",
      "pdf_url": "http://arxiv.org/pdf/2509.08470v1",
      "published": "2025-09-10T10:18:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08470v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "Adversarial Attacks Against Automated Fact-Checking: A Survey",
      "authors": [
        "Fanzhen Liu",
        "Alsharif Abuadbba",
        "Kristen Moore",
        "Surya Nepal",
        "Cecile Paris",
        "Jia Wu",
        "Jian Yang",
        "Quan Z. Sheng"
      ],
      "abstract": "In an era where misinformation spreads freely, fact-checking (FC) plays a\ncrucial role in verifying claims and promoting reliable information. While\nautomated fact-checking (AFC) has advanced significantly, existing systems\nremain vulnerable to adversarial attacks that manipulate or generate claims,\nevidence, or claim-evidence pairs. These attacks can distort the truth, mislead\ndecision-makers, and ultimately undermine the reliability of FC models. Despite\ngrowing research interest in adversarial attacks against AFC systems, a\ncomprehensive, holistic overview of key challenges remains lacking. These\nchallenges include understanding attack strategies, assessing the resilience of\ncurrent models, and identifying ways to enhance robustness. This survey\nprovides the first in-depth review of adversarial attacks targeting FC,\ncategorizing existing attack methodologies and evaluating their impact on AFC\nsystems. Additionally, we examine recent advancements in adversary-aware\ndefenses and highlight open research questions that require further\nexploration. Our findings underscore the urgent need for resilient FC\nframeworks capable of withstanding adversarial manipulations in pursuit of\npreserving high verification accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2509.08463v1",
      "published": "2025-09-10T10:10:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08463v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics",
      "authors": [
        "Dikshant Sagar",
        "Kaiwen Yu",
        "Alejandro Yankelevich",
        "Jianming Bian",
        "Pierre Baldi"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics.",
      "pdf_url": "http://arxiv.org/pdf/2509.08461v2",
      "published": "2025-09-10T10:07:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08461v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "hep-ex"
      ]
    },
    {
      "title": "DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation",
      "authors": [
        "Charuka Herath",
        "Yogachandran Rahulamathavan",
        "Varuna De Silva",
        "Sangarapillai Lambotharan"
      ],
      "abstract": "Federated Learning (FL) enables decentralized model training without sharing\nraw data, offering strong privacy guarantees. However, existing FL protocols\nstruggle to defend against Byzantine participants, maintain model utility under\nnon-independent and identically distributed (non-IID) data, and remain\nlightweight for edge devices. Prior work either assumes trusted hardware, uses\nexpensive cryptographic tools, or fails to address privacy and robustness\nsimultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated\nLearning framework that addresses these limitations using a group-based secure\naggregation approach. Unlike LSFL, which assumes non-colluding semi-honest\nservers, DSFL removes this dependency by revealing a key vulnerability: privacy\nleakage through client-server collusion. DSFL introduces three key innovations:\n(1) a dual-server secure aggregation protocol that protects updates without\nencryption or key exchange, (2) a group-wise credit-based filtering mechanism\nto isolate Byzantine clients based on deviation scores, and (3) a dynamic\nreward-penalty system for enforcing fair participation. DSFL is evaluated on\nMNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in\nboth IID and non-IID settings. It consistently outperforms existing baselines,\nincluding LSFL, homomorphic encryption methods, and differential privacy\napproaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and\n68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar\nthreats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB\ncommunication per round.",
      "pdf_url": "http://arxiv.org/pdf/2509.08449v1",
      "published": "2025-09-10T09:47:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08449v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting",
      "authors": [
        "Ivan Stoyanov",
        "Fabian Bongratz",
        "Christian Wachinger"
      ],
      "abstract": "Accurate forecasting of individualized, high-resolution cortical thickness\n(CTh) trajectories is essential for detecting subtle cortical changes,\nproviding invaluable insights into neurodegenerative processes and facilitating\nearlier and more precise intervention strategies. However, CTh forecasting is a\nchallenging task due to the intricate non-Euclidean geometry of the cerebral\ncortex and the need to integrate multi-modal data for subject-specific\npredictions. To address these challenges, we introduce the Spherical Brownian\nBridge Diffusion Model (SBDM). Specifically, we propose a bidirectional\nconditional Brownian bridge diffusion process to forecast CTh trajectories at\nthe vertex level of registered cortical surfaces. Our technical contribution\nincludes a new denoising model, the conditional spherical U-Net (CoS-UNet),\nwhich combines spherical convolutions and dense cross-attention to integrate\ncortical surfaces and tabular conditions seamlessly. Compared to previous\napproaches, SBDM achieves significantly reduced prediction errors, as\ndemonstrated by our experiments based on longitudinal datasets from the ADNI\nand OASIS. Additionally, we demonstrate SBDM's ability to generate individual\nfactual and counterfactual CTh trajectories, offering a novel framework for\nexploring hypothetical scenarios of cortical development.",
      "pdf_url": "http://arxiv.org/pdf/2509.08442v1",
      "published": "2025-09-10T09:40:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08442v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ]
    },
    {
      "title": "Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking",
      "authors": [
        "Keisuke Toida",
        "Taigo Sakai",
        "Naoki Kato",
        "Kazutoyo Yokota",
        "Takeshi Nakamura",
        "Kazuhiro Hotta"
      ],
      "abstract": "Multi-View Multi-Object Tracking (MVMOT) is essential for applications such\nas surveillance, autonomous driving, and sports analytics. However, maintaining\nconsistent object identities across multiple cameras remains challenging due to\nviewpoint changes, lighting variations, and occlusions, which often lead to\ntracking errors.Recent methods project features from multiple cameras into a\nunified Bird's-Eye-View (BEV) space to improve robustness against occlusion.\nHowever, this projection introduces feature distortion and non-uniform density\ncaused by variations in object scale with distance. These issues degrade the\nquality of the fused representation and reduce detection and tracking\naccuracy.To address these problems, we propose SCFusion, a framework that\ncombines three techniques to improve multi-view feature integration. First, it\napplies a sparse transformation to avoid unnatural interpolation during\nprojection. Next, it performs density-aware weighting to adaptively fuse\nfeatures based on spatial confidence and camera distance. Finally, it\nintroduces a multi-view consistency loss that encourages each camera to learn\ndiscriminative features independently before fusion.Experiments show that\nSCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9%\non WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline\nmethod TrackTacular. These results demonstrate that SCFusion effectively\nmitigates the limitations of conventional BEV projection and provides a robust\nand accurate solution for multi-view object detection and tracking.",
      "pdf_url": "http://arxiv.org/pdf/2509.08421v1",
      "published": "2025-09-10T09:06:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08421v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "An Iterative LLM Framework for SIBT utilizing RAG-based Adaptive Weight Optimization",
      "authors": [
        "Zhuo Xiao",
        "Qinglong Yao",
        "Jingjing Wang",
        "Fugen Zhou",
        "Bo Liu",
        "Haitao Sun",
        "Zhe Ji",
        "Yuliang Jiang",
        "Junjie Wang",
        "Qiuwen Wu"
      ],
      "abstract": "Seed implant brachytherapy (SIBT) is an effective cancer treatment modality;\nhowever, clinical planning often relies on manual adjustment of objective\nfunction weights, leading to inefficiencies and suboptimal results. This study\nproposes an adaptive weight optimization framework for SIBT planning, driven by\nlarge language models (LLMs). A locally deployed DeepSeek-R1 LLM is integrated\nwith an automatic planning algorithm in an iterative loop. Starting with fixed\nweights, the LLM evaluates plan quality and recommends new weights in the next\niteration. This process continues until convergence criteria are met, after\nwhich the LLM conducts a comprehensive evaluation to identify the optimal plan.\nA clinical knowledge base, constructed and queried via retrieval-augmented\ngeneration (RAG), enhances the model's domain-specific reasoning. The proposed\nmethod was validated on 23 patient cases, showing that the LLM-assisted\napproach produces plans that are comparable to or exceeding clinically approved\nand fixed-weight plans, in terms of dose homogeneity for the clinical target\nvolume (CTV) and sparing of organs at risk (OARs). The study demonstrates the\npotential use of LLMs in SIBT planning automation.",
      "pdf_url": "http://arxiv.org/pdf/2509.08407v1",
      "published": "2025-09-10T08:54:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08407v1",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Semantic Causality-Aware Vision-Based 3D Occupancy Prediction",
      "authors": [
        "Dubing Chen",
        "Huan Zheng",
        "Yucheng Zhou",
        "Xianfei Li",
        "Wenlong Liao",
        "Tao He",
        "Pai Peng",
        "Jianbing Shen"
      ],
      "abstract": "Vision-based 3D semantic occupancy prediction is a critical task in 3D vision\nthat integrates volumetric 3D reconstruction with semantic understanding.\nExisting methods, however, often rely on modular pipelines. These modules are\ntypically optimized independently or use pre-configured inputs, leading to\ncascading errors. In this paper, we address this limitation by designing a\nnovel causal loss that enables holistic, end-to-end supervision of the modular\n2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D\nsemantic causality, this loss regulates the gradient flow from 3D voxel\nrepresentations back to the 2D features. Consequently, it renders the entire\npipeline differentiable, unifying the learning process and making previously\nnon-trainable components fully learnable. Building on this principle, we\npropose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises\nthree components guided by our causal loss: Channel-Grouped Lifting for\nadaptive semantic mapping, Learnable Camera Offsets for enhanced robustness\nagainst camera perturbations, and Normalized Convolution for effective feature\npropagation. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance on the Occ3D benchmark, demonstrating significant\nrobustness to camera perturbations and improved 2D-to-3D semantic consistency.",
      "pdf_url": "http://arxiv.org/pdf/2509.08388v1",
      "published": "2025-09-10T08:29:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08388v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Efficient Decoding Methods for Language Models on Encrypted Data",
      "authors": [
        "Matan Avitan",
        "Moran Baruch",
        "Nir Drucker",
        "Itamar Zimerman",
        "Yoav Goldberg"
      ],
      "abstract": "Large language models (LLMs) power modern AI applications, but processing\nsensitive data on untrusted servers raises privacy concerns. Homomorphic\nencryption (HE) enables computation on encrypted data for secure inference.\nHowever, neural text generation requires decoding methods like argmax and\nsampling, which are non-polynomial and thus computationally expensive under\nencryption, creating a significant performance bottleneck. We introduce cutmax,\nan HE-friendly argmax algorithm that reduces ciphertext operations compared to\nprior methods, enabling practical greedy decoding under encryption. We also\npropose the first HE-compatible nucleus (top-p) sampling method, leveraging\ncutmax for efficient stochastic decoding with provable privacy guarantees. Both\ntechniques are polynomial, supporting efficient inference in privacy-preserving\nsettings. Moreover, their differentiability facilitates gradient-based\nsequence-level optimization as a polynomial alternative to straight-through\nestimators. We further provide strong theoretical guarantees for cutmax,\nproving it converges globally to a unique two-level fixed point, independent of\nthe input values beyond the identity of the maximizer, which explains its rapid\nconvergence in just a few iterations. Evaluations on realistic LLM outputs show\nlatency reductions of 24x-35x over baselines, advancing secure text generation.",
      "pdf_url": "http://arxiv.org/pdf/2509.08383v1",
      "published": "2025-09-10T08:23:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08383v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model",
      "authors": [
        "Yu Cheng Chih",
        "Yong Hao Hou"
      ],
      "abstract": "Deploying large language models (LLMs) for structured data extraction in\ndomains such as financial compliance reporting, legal document analytics, and\nmultilingual knowledge base construction is often impractical for smaller teams\ndue to the high cost of running large architectures and the difficulty of\npreparing large, high-quality datasets. Most recent instruction-tuning studies\nfocus on seven-billion-parameter or larger models, leaving limited evidence on\nwhether much smaller models can work reliably under low-resource, multi-task\nconditions. This work presents ETLCH, a billion-parameter LLaMA-based model\nfine-tuned with low-rank adaptation on only a few hundred to one thousand\nsamples per task for JSON extraction, knowledge graph extraction, and named\nentity recognition. Despite its small scale, ETLCH outperforms strong baselines\nacross most evaluation metrics, with substantial gains observed even at the\nlowest data scale. These findings demonstrate that well-tuned small models can\ndeliver stable and accurate structured outputs at a fraction of the\ncomputational cost, enabling cost-effective and reliable information extraction\npipelines in resource-constrained environments.",
      "pdf_url": "http://arxiv.org/pdf/2509.08381v1",
      "published": "2025-09-10T08:19:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.08381v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}