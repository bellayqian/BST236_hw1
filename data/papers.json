{
  "last_updated": "2025-08-06T00:58:09.332078",
  "papers": [
    {
      "title": "D2PPO: Diffusion Policy Policy Optimization with Dispersive Loss",
      "authors": [
        "Guowei Zou",
        "Weibing Li",
        "Hejun Wu",
        "Yukun Qian",
        "Yuhang Wang",
        "Haitao Wang"
      ],
      "abstract": "Diffusion policies excel at robotic manipulation by naturally modeling\nmultimodal action distributions in high-dimensional spaces. Nevertheless,\ndiffusion policies suffer from diffusion representation collapse: semantically\nsimilar observations are mapped to indistinguishable features, ultimately\nimpairing their ability to handle subtle but critical variations required for\ncomplex robotic manipulation. To address this problem, we propose D2PPO\n(Diffusion Policy Policy Optimization with Dispersive Loss). D2PPO introduces\ndispersive loss regularization that combats representation collapse by treating\nall hidden representations within each batch as negative pairs. D2PPO compels\nthe network to learn discriminative representations of similar observations,\nthereby enabling the policy to identify subtle yet crucial differences\nnecessary for precise manipulation. In evaluation, we find that early-layer\nregularization benefits simple tasks, while late-layer regularization sharply\nenhances performance on complex manipulation tasks. On RoboMimic benchmarks,\nD2PPO achieves an average improvement of 22.7% in pre-training and 26.1% after\nfine-tuning, setting new SOTA results. In comparison with SOTA, results of\nreal-world experiments on a Franka Emika Panda robot show the excitingly high\nsuccess rate of our method. The superiority of our method is especially evident\nin complex tasks. Project page: https://guowei-zou.github.io/d2ppo/",
      "pdf_url": "http://arxiv.org/pdf/2508.02644v1",
      "published": "2025-08-04T17:33:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02644v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "An Efficient Continuous-Time MILP for Integrated Aircraft Hangar Scheduling and Layout",
      "authors": [
        "Shayan Farhang Pazhooh",
        "Hossein Shams Shemirani"
      ],
      "abstract": "Efficient management of aircraft maintenance hangars is a critical\noperational challenge, involving complex, interdependent decisions regarding\naircraft scheduling and spatial allocation. This paper introduces a novel\ncontinuous-time mixed-integer linear programming (MILP) model to solve this\nintegrated spatio-temporal problem. By treating time as a continuous variable,\nour formulation overcomes the scalability limitations of traditional\ndiscrete-time approaches. The performance of the exact model is benchmarked\nagainst a constructive heuristic, and its practical applicability is\ndemonstrated through a custom-built visualization dashboard. Computational\nresults are compelling: the model solves instances with up to 25 aircraft to\nproven optimality, often in mere seconds, and for large-scale cases of up to 40\naircraft, delivers high-quality solutions within known optimality gaps. In all\ntested scenarios, the resulting solutions consistently and significantly\noutperform the heuristic, which highlights the framework's substantial economic\nbenefits and provides valuable managerial insights into the trade-off between\nsolution time and optimality.",
      "pdf_url": "http://arxiv.org/pdf/2508.02640v1",
      "published": "2025-08-04T17:25:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02640v1",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.CE",
        "90C11 (Primary), 90B35, 90C27 (Secondary)"
      ]
    },
    {
      "title": "Actionable Counterfactual Explanations Using Bayesian Networks and Path Planning with Applications to Environmental Quality Improvement",
      "authors": [
        "Enrique Valero-Leal",
        "Pedro Larrañaga",
        "Concha Bielza"
      ],
      "abstract": "Counterfactual explanations study what should have changed in order to get an\nalternative result, enabling end-users to understand machine learning\nmechanisms with counterexamples. Actionability is defined as the ability to\ntransform the original case to be explained into a counterfactual one. We\ndevelop a method for actionable counterfactual explanations that, unlike\npredecessors, does not directly leverage training data. Rather, data is only\nused to learn a density estimator, creating a search landscape in which to\napply path planning algorithms to solve the problem and masking the endogenous\ndata, which can be sensitive or private. We put special focus on estimating the\ndata density using Bayesian networks, demonstrating how their enhanced\ninterpretability is useful in high-stakes scenarios in which fairness is\nraising concern. Using a synthetic benchmark comprised of 15 datasets, our\nproposal finds more actionable and simpler counterfactuals than the current\nstate-of-the-art algorithms. We also test our algorithm with a real-world\nEnvironmental Protection Agency dataset, facilitating a more efficient and\nequitable study of policies to improve the quality of life in United States of\nAmerica counties. Our proposal captures the interaction of variables, ensuring\nequity in decisions, as policies to improve certain domains of study (air,\nwater quality, etc.) can be detrimental in others. In particular, the\nsociodemographic domain is often involved, where we find important variables\nrelated to the ongoing housing crisis that can potentially have a severe\nnegative impact on communities.",
      "pdf_url": "http://arxiv.org/pdf/2508.02634v1",
      "published": "2025-08-04T17:20:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02634v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging Questions for Agentic E-Commerce",
      "authors": [
        "Amine Allouah",
        "Omar Besbes",
        "Josué D Figueroa",
        "Yash Kanoria",
        "Akshit Kumar"
      ],
      "abstract": "Online marketplaces will be transformed by autonomous AI agents acting on\nbehalf of consumers. Rather than humans browsing and clicking,\nvision-language-model (VLM) agents can parse webpages, evaluate products, and\ntransact. This raises a fundamental question: what do AI agents buy, and why?\nWe develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent\nwith a fully programmable mock marketplace to study this question. We first\nconduct basic rationality checks in the context of simple tasks, and then, by\nrandomizing product positions, prices, ratings, reviews, sponsored tags, and\nplatform endorsements, we obtain causal estimates of how frontier VLMs actually\nshop. Models show strong but heterogeneous position effects: all favor the top\nrow, yet different models prefer different columns, undermining the assumption\nof a universal \"top\" rank. They penalize sponsored tags and reward\nendorsements. Sensitivities to price, ratings, and reviews are directionally\nhuman-like but vary sharply in magnitude across models. Motivated by scenarios\nwhere sellers use AI agents to optimize product listings, we show that a\nseller-side agent that makes minor tweaks to product descriptions, targeting AI\nbuyer preferences, can deliver substantial market-share gains if AI-mediated\nshopping dominates. We also find that modal product choices can differ across\nmodels and, in some cases, demand may concentrate on a few select products,\nraising competition questions. Together, our results illuminate how AI agents\nmay behave in e-commerce settings and surface concrete seller strategy,\nplatform design, and regulatory questions in an AI-mediated ecosystem.",
      "pdf_url": "http://arxiv.org/pdf/2508.02630v1",
      "published": "2025-08-04T17:19:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02630v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.MA",
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents",
      "authors": [
        "Yibin Liu",
        "Zhixuan Liang",
        "Zanxin Chen",
        "Tianxing Chen",
        "Mengkang Hu",
        "Wanxi Dong",
        "Congsheng Xu",
        "Zhaoming Han",
        "Yusen Qin",
        "Yao Mu"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.",
      "pdf_url": "http://arxiv.org/pdf/2508.02629v1",
      "published": "2025-08-04T17:18:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02629v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "AutoML-Med: A Framework for Automated Machine Learning in Medical Tabular Data",
      "authors": [
        "Riccardo Francia",
        "Maurizio Leone",
        "Giorgio Leonardi",
        "Stefania Montani",
        "Marzio Pennisi",
        "Manuel Striani",
        "Sandra D'Alfonso"
      ],
      "abstract": "Medical datasets are typically affected by issues such as missing values,\nclass imbalance, a heterogeneous feature types, and a high number of features\nversus a relatively small number of samples, preventing machine learning models\nfrom obtaining proper results in classification and regression tasks. This\npaper introduces AutoML-Med, an Automated Machine Learning tool specifically\ndesigned to address these challenges, minimizing user intervention and\nidentifying the optimal combination of preprocessing techniques and predictive\nmodels. AutoML-Med's architecture incorporates Latin Hypercube Sampling (LHS)\nfor exploring preprocessing methods, trains models using selected metrics, and\nutilizes Partial Rank Correlation Coefficient (PRCC) for fine-tuned\noptimization of the most influential preprocessing steps. Experimental results\ndemonstrate AutoML-Med's effectiveness in two different clinical settings,\nachieving higher balanced accuracy and sensitivity, which are crucial for\nidentifying at-risk patients, compared to other state-of-the-art tools.\nAutoML-Med's ability to improve prediction results, especially in medical\ndatasets with sparse data and class imbalance, highlights its potential to\nstreamline Machine Learning applications in healthcare.",
      "pdf_url": "http://arxiv.org/pdf/2508.02625v1",
      "published": "2025-08-04T17:13:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02625v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction",
      "authors": [
        "Enrico De Santis",
        "Antonello Rizzi"
      ],
      "abstract": "This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research.",
      "pdf_url": "http://arxiv.org/pdf/2508.02622v1",
      "published": "2025-08-04T17:10:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02622v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research",
      "authors": [
        "Yinghao Zhu",
        "Yifan Qi",
        "Zixiang Wang",
        "Lei Gu",
        "Dehao Sui",
        "Haoran Hu",
        "Xichen Zhang",
        "Ziyi He",
        "Liantao Ma",
        "Lequan Yu"
      ],
      "abstract": "The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery.",
      "pdf_url": "http://arxiv.org/pdf/2508.02621v1",
      "published": "2025-08-04T17:08:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02621v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Meta-RAG on Large Codebases Using Code Summarization",
      "authors": [
        "Vali Tawosia",
        "Salwa Alamir",
        "Xiaomo Liu",
        "Manuela Veloso"
      ],
      "abstract": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.",
      "pdf_url": "http://arxiv.org/pdf/2508.02611v1",
      "published": "2025-08-04T17:01:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02611v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Entity Representation Learning Through Onsite-Offsite Graph for Pinterset Ads",
      "authors": [
        "Jiayin Jin",
        "Zhimeng Pan",
        "Yang Tang",
        "Jiarui Feng",
        "Kungang Li",
        "Chongyuan Xiang",
        "Jiacheng Li",
        "Runze Su",
        "Siping Ji",
        "Han Sun",
        "Ling Leng",
        "Prathibha Deshikachar"
      ],
      "abstract": "Graph Neural Networks (GNN) have been extensively applied to industry\nrecommendation systems, as seen in models like GraphSage\\cite{GraphSage},\nTwHIM\\cite{TwHIM}, LiGNN\\cite{LiGNN} etc. In these works, graphs were\nconstructed based on users' activities on the platforms, and various graph\nmodels were developed to effectively learn node embeddings. In addition to\nusers' onsite activities, their offsite conversions are crucial for Ads models\nto capture their shopping interest. To better leverage offsite conversion data\nand explore the connection between onsite and offsite activities, we\nconstructed a large-scale heterogeneous graph based on users' onsite ad\ninteractions and opt-in offsite conversion activities. Furthermore, we\nintroduced TransRA (TransR\\cite{TransR} with Anchors), a novel Knowledge Graph\nEmbedding (KGE) model, to more efficiently integrate graph embeddings into Ads\nranking models. However, our Ads ranking models initially struggled to directly\nincorporate Knowledge Graph Embeddings (KGE), and only modest gains were\nobserved during offline experiments. To address this challenge, we employed the\nLarge ID Embedding Table technique and innovated an attention based KGE\nfinetuning approach within the Ads ranking models. As a result, we observed a\nsignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)\nprediction models. Moreover, this framework has been deployed in Pinterest's\nAds Engagement Model and contributed to $2.69\\%$ CTR lift and $1.34\\%$ CPC\nreduction. We believe the techniques presented in this paper can be leveraged\nby other large-scale industrial models.",
      "pdf_url": "http://arxiv.org/pdf/2508.02609v1",
      "published": "2025-08-04T17:00:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02609v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis in Low-Data Regimes",
      "authors": [
        "Siyi Liu",
        "Yujia Zheng",
        "Yongqi Zhang"
      ],
      "abstract": "The application of machine learning on tabular data in specialized domains is\nseverely limited by data scarcity. While generative models offer a solution,\ntraditional methods falter in low-data regimes, and recent Large Language\nModels (LLMs) often ignore the explicit dependency structure of tabular data,\nleading to low-fidelity synthetics. To address these limitations, we introduce\nStructSynth, a novel framework that integrates the generative power of LLMs\nwith robust structural control. StructSynth employs a two-stage architecture.\nFirst, it performs explicit structure discovery to learn a Directed Acyclic\nGraph (DAG) from the available data. Second, this learned structure serves as a\nhigh-fidelity blueprint to steer the LLM's generation process, forcing it to\nadhere to the learned feature dependencies and thereby ensuring the generated\ndata respects the underlying structure by design. Our extensive experiments\ndemonstrate that StructSynth produces synthetic data with significantly higher\nstructural integrity and downstream utility than state-of-the-art methods. It\nproves especially effective in challenging low-data scenarios, successfully\nnavigating the trade-off between privacy preservation and statistical fidelity.",
      "pdf_url": "http://arxiv.org/pdf/2508.02601v1",
      "published": "2025-08-04T16:55:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02601v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition",
      "authors": [
        "Catalina Gomez",
        "Lalithkumar Seenivasan",
        "Xinrui Zou",
        "Jeewoo Yoon",
        "Sirui Chu",
        "Ariel Leong",
        "Patrick Kramer",
        "Yu-Chun Ku",
        "Jose L. Porras",
        "Alejandro Martin-Gomez",
        "Masaru Ishii",
        "Mathias Unberath"
      ],
      "abstract": "Traditional surgical skill acquisition relies heavily on expert feedback, yet\ndirect access is limited by faculty availability and variability in subjective\nassessments. While trainees can practice independently, the lack of\npersonalized, objective, and quantitative feedback reduces the effectiveness of\nself-directed learning. Recent advances in computer vision and machine learning\nhave enabled automated surgical skill assessment, demonstrating the feasibility\nof automatic competency evaluation. However, it is unclear whether such\nArtificial Intelligence (AI)-driven feedback can contribute to skill\nacquisition. Here, we examine the effectiveness of explainable AI\n(XAI)-generated feedback in surgical training through a human-AI study. We\ncreate a simulation-based training framework that utilizes XAI to analyze\nvideos and extract surgical skill proxies related to primitive actions. Our\nintervention provides automated, user-specific feedback by comparing trainee\nperformance to expert benchmarks and highlighting deviations from optimal\nexecution through understandable proxies for actionable guidance. In a\nprospective user study with medical students, we compare the impact of\nXAI-guided feedback against traditional video-based coaching on task outcomes,\ncognitive load, and trainees' perceptions of AI-assisted learning. Results\nshowed improved cognitive load and confidence post-intervention. While no\ndifferences emerged between the two feedback types in reducing performance gaps\nor practice adjustments, trends in the XAI group revealed desirable effects\nwhere participants more closely mimicked expert practice. This work encourages\nthe study of explainable AI in surgical education and the development of\ndata-driven, adaptive feedback mechanisms that could transform learning\nexperiences and competency assessment.",
      "pdf_url": "http://arxiv.org/pdf/2508.02593v1",
      "published": "2025-08-04T16:48:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02593v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules",
      "authors": [
        "Yilun Liu",
        "Yunpu Ma",
        "Yuetian Lu",
        "Shuo Chen",
        "Zifeng Ding",
        "Volker Tresp"
      ],
      "abstract": "Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among\ntheir specialized experts, which existing Parameter- Efficient Fine-Tuning\n(PEFT) strategies fail to leverage. This motivates us to investigate whether\nadaptation modules themselves should incorporate routing mechanisms to align\nwith MoE's multi-expert architecture. We analyze dynamics of core components\nwhen applying PEFT to MoE language models and examine how different routing\nstrategies affect adaptation effectiveness. Extensive experiments adapting\nOLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks\nvalidate the performance and efficiency of our routed approach. We identify the\noptimal configurations for different scenarios and provide empirical analyses\nwith practical insights to facilitate better PEFT and MoE applications.",
      "pdf_url": "http://arxiv.org/pdf/2508.02587v1",
      "published": "2025-08-04T16:43:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02587v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification",
      "authors": [
        "Ming Pok Ng",
        "Junqi Jiang",
        "Gabriel Freedman",
        "Antonio Rago",
        "Francesca Toni"
      ],
      "abstract": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs.",
      "pdf_url": "http://arxiv.org/pdf/2508.02584v1",
      "published": "2025-08-04T16:40:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02584v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge",
      "authors": [
        "Lei Zan",
        "Keli Zhang",
        "Ruichu Cai",
        "Lujia Pan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone.",
      "pdf_url": "http://arxiv.org/pdf/2508.02583v1",
      "published": "2025-08-04T16:39:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02583v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare",
      "authors": [
        "Eman Alamoudi",
        "Ellis Solaiman"
      ],
      "abstract": "Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling.",
      "pdf_url": "http://arxiv.org/pdf/2508.02574v1",
      "published": "2025-08-04T16:28:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02574v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ]
    },
    {
      "title": "Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification",
      "authors": [
        "Javier Fumanal-Idocin",
        "Raquel Fernandez-Peralta",
        "Javier Andreu-Perez"
      ],
      "abstract": "Dynamic feature selection (DFS) offers a compelling alternative to\ntraditional, static feature selection by adapting the selected features to each\nindividual sample. Unlike classical methods that apply a uniform feature set,\nDFS customizes feature selection per sample, providing insight into the\ndecision-making process for each case. DFS is especially significant in\nsettings where decision transparency is key, i.e., clinical decisions; however,\nexisting methods use opaque models, which hinder their applicability in\nreal-life scenarios. This paper introduces a novel approach leveraging a\nrule-based system as a base classifier for the DFS process, which enhances\ndecision interpretability compared to neural estimators. We also show how this\nmethod provides a quantitative measure of uncertainty for each feature query\nand can make the feature selection process computationally lighter by\nconstraining the feature search space. We also discuss when greedy selection of\nconditional mutual information is equivalent to selecting features that\nminimize the difference with respect to the global model predictions. Finally,\nwe demonstrate the competitive performance of our rule-based DFS approach\nagainst established and state-of-the-art greedy and RL methods, which are\nmostly considered opaque, compared to our explainable rule-based system.",
      "pdf_url": "http://arxiv.org/pdf/2508.02566v1",
      "published": "2025-08-04T16:21:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02566v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Stakeholder Perspectives on Humanistic Implementation of Computer Perception in Healthcare: A Qualitative Study",
      "authors": [
        "Kristin M. Kostick-Quenet",
        "Meghan E. Hurley",
        "Syed Ayaz",
        "John Herrington",
        "Casey Zampella",
        "Julia Parish-Morris",
        "Birkan Tunç",
        "Gabriel Lázaro-Muñoz",
        "J. S. Blumenthal-Barby",
        "Eric A. Storch"
      ],
      "abstract": "Computer perception (CP) technologies (digital phenotyping, affective\ncomputing and related passive sensing approaches) offer unprecedented\nopportunities to personalize healthcare, but provoke concerns about privacy,\nbias and the erosion of empathic, relationship-centered practice. A\ncomprehensive understanding of perceived risks, benefits, and implementation\nchallenges from those who design, deploy and experience these tools in\nreal-world settings remains elusive. This study provides the first\nevidence-based account of key stakeholder perspectives on the relational,\ntechnical, and governance challenges raised by the integration of CP\ntechnologies into patient care. We conducted in-depth, semi-structured\ninterviews with 102 stakeholders: adolescent patients and their caregivers,\nfrontline clinicians, technology developers, and ethics, legal, policy or\nphilosophy scholars. Transcripts underwent thematic analysis by a\nmultidisciplinary team; reliability was enhanced through double coding and\nconsensus adjudication. Stakeholders articulated seven interlocking concern\ndomains: (1) trustworthiness and data integrity; (2) patient-specific\nrelevance; (3) utility and workflow integration; (4) regulation and governance;\n(5) privacy and data protection; (6) direct and indirect patient harms; and (7)\nphilosophical critiques of reductionism. To operationalize humanistic\nsafeguards, we propose \"personalized roadmaps\": co-designed plans that\npredetermine which metrics will be monitored, how and when feedback is shared,\nthresholds for clinical action, and procedures for reconciling discrepancies\nbetween algorithmic inferences and lived experience. By translating these\ninsights into personalized roadmaps, we offer a practical framework for\ndevelopers, clinicians and policymakers seeking to harness continuous\nbehavioral data while preserving the humanistic core of care.",
      "pdf_url": "http://arxiv.org/pdf/2508.02550v1",
      "published": "2025-08-04T16:01:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02550v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "The KG-ER Conceptual Schema Language",
      "authors": [
        "Enrico Franconi",
        "Benoît Groz",
        "Jan Hidders",
        "Nina Pardal",
        "Sławek Staworko",
        "Jan Van den Bussche",
        "Piotr Wieczorek"
      ],
      "abstract": "We propose KG-ER, a conceptual schema language for knowledge graphs that\ndescribes the structure of knowledge graphs independently of their\nrepresentation (relational databases, property graphs, RDF) while helping to\ncapture the semantics of the information stored in a knowledge graph.",
      "pdf_url": "http://arxiv.org/pdf/2508.02548v1",
      "published": "2025-08-04T16:01:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02548v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "68P15"
      ]
    },
    {
      "title": "What are you sinking? A geometric approach on attention sink",
      "authors": [
        "Valeria Ruscio",
        "Umberto Nanni",
        "Fabrizio Silvestri"
      ],
      "abstract": "Attention sink (AS) is a consistent pattern in transformer attention maps\nwhere certain tokens (often special tokens or positional anchors)\ndisproportionately attract attention from other tokens. We show that in\ntransformers, AS is not an architectural artifact, but it is the manifestation\nof a fundamental geometric principle: the establishment of reference frames\nthat anchor representational spaces. We analyze several architectures and\nidentify three distinct reference frame types, centralized, distributed, and\nbidirectional, that correlate with the attention sink phenomenon. We show that\nthey emerge during the earliest stages of training as optimal solutions to the\nproblem of establishing stable coordinate systems in high-dimensional spaces.\nWe show the influence of architecture components, particularly position\nencoding implementations, on the specific type of reference frame. This\nperspective transforms our understanding of transformer attention mechanisms\nand provides insights for both architecture design and the relationship with\nAS.",
      "pdf_url": "http://arxiv.org/pdf/2508.02546v1",
      "published": "2025-08-04T15:59:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02546v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Automatic Identification of Machine Learning-Specific Code Smells",
      "authors": [
        "Peter Hamfelt",
        "Ricardo Britto",
        "Lincoln Rocha",
        "Camilo Almendra"
      ],
      "abstract": "Machine learning (ML) has rapidly grown in popularity, becoming vital to many\nindustries. Currently, the research on code smells in ML applications lacks\ntools and studies that address the identification and validity of ML-specific\ncode smells. This work investigates suitable methods and tools to design and\ndevelop a static code analysis tool (MLpylint) based on code smell criteria.\nThis research employed the Design Science Methodology. In the problem\nidentification phase, a literature review was conducted to identify ML-specific\ncode smells. In solution design, a secondary literature review and\nconsultations with experts were performed to select methods and tools for\nimplementing the tool. We evaluated the tool on data from 160 open-source ML\napplications sourced from GitHub. We also conducted a static validation through\nan expert survey involving 15 ML professionals. The results indicate the\neffectiveness and usefulness of the MLpylint. We aim to extend our current\napproach by investigating ways to introduce MLpylint seamlessly into\ndevelopment workflows, fostering a more productive and innovative developer\nenvironment.",
      "pdf_url": "http://arxiv.org/pdf/2508.02541v1",
      "published": "2025-08-04T15:51:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02541v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Accurate and Interpretable Postmenstrual Age Prediction via Multimodal Large Language Model",
      "authors": [
        "Qifan Chen",
        "Jin Cui",
        "Cindy Duan",
        "Yushuo Han",
        "Yifei Shi"
      ],
      "abstract": "Accurate estimation of postmenstrual age (PMA) at scan is crucial for\nassessing neonatal development and health. While deep learning models have\nachieved high accuracy in predicting PMA from brain MRI, they often function as\nblack boxes, offering limited transparency and interpretability in clinical\ndecision support. In this work, we address the dual challenge of accuracy and\ninterpretability by adapting a multimodal large language model (MLLM) to\nperform both precise PMA prediction and clinically relevant explanation\ngeneration. We introduce a parameter-efficient fine-tuning (PEFT) strategy\nusing instruction tuning and Low-Rank Adaptation (LoRA) applied to the\nQwen2.5-VL-7B model. The model is trained on four 2D cortical surface\nprojection maps derived from neonatal MRI scans. By employing distinct prompts\nfor training and inference, our approach enables the MLLM to handle a\nregression task during training and generate clinically relevant explanations\nduring inference. The fine-tuned model achieves a low prediction error with a\n95 percent confidence interval of 0.78 to 1.52 weeks, while producing\ninterpretable outputs grounded in developmental features, marking a significant\nstep toward transparent and trustworthy AI systems in perinatal neuroscience.",
      "pdf_url": "http://arxiv.org/pdf/2508.02525v1",
      "published": "2025-08-04T15:35:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02525v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Test-time Prompt Intervention",
      "authors": [
        "Chenxu Yang",
        "Qingyi Si",
        "Mz Dai",
        "Dingyu Yao",
        "Mingyu Zheng",
        "Minghui Chen",
        "Zheng Lin",
        "Weiping Wang"
      ],
      "abstract": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2508.02511v1",
      "published": "2025-08-04T15:17:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02511v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms",
      "authors": [
        "Xiaowei Yuan",
        "Lei Jin",
        "Haoxin Zhang",
        "Yan Gao",
        "Yi Wu",
        "Yao Hu",
        "Ziyang Huang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments.",
      "pdf_url": "http://arxiv.org/pdf/2508.02506v1",
      "published": "2025-08-04T15:14:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02506v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling",
      "authors": [
        "Maxime Bouscary",
        "Saurabh Amin"
      ],
      "abstract": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems.",
      "pdf_url": "http://arxiv.org/pdf/2508.02503v1",
      "published": "2025-08-04T15:11:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02503v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "PHM-Bench: A Domain-Specific Benchmarking Framework for Systematic Evaluation of Large Models in Prognostics and Health Management",
      "authors": [
        "Puyu Yang",
        "Laifa Tao",
        "Zijian Huang",
        "Haifei Liu",
        "Wenyan Cao",
        "Hao Ji",
        "Jianan Qiu",
        "Qixuan Huang",
        "Xuanyuan Su",
        "Yuhang Xie",
        "Jun Zhang",
        "Shangyu Li",
        "Chen Lu",
        "Zhixuan Lian"
      ],
      "abstract": "With the rapid advancement of generative artificial intelligence, large\nlanguage models (LLMs) are increasingly adopted in industrial domains, offering\nnew opportunities for Prognostics and Health Management (PHM). These models\nhelp address challenges such as high development costs, long deployment cycles,\nand limited generalizability. However, despite the growing synergy between PHM\nand LLMs, existing evaluation methodologies often fall short in structural\ncompleteness, dimensional comprehensiveness, and evaluation granularity. This\nhampers the in-depth integration of LLMs into the PHM domain. To address these\nlimitations, this study proposes PHM-Bench, a novel three-dimensional\nevaluation framework for PHM-oriented large models. Grounded in the triadic\nstructure of fundamental capability, core task, and entire lifecycle, PHM-Bench\nis tailored to the unique demands of PHM system engineering. It defines\nmulti-level evaluation metrics spanning knowledge comprehension, algorithmic\ngeneration, and task optimization. These metrics align with typical PHM tasks,\nincluding condition monitoring, fault diagnosis, RUL prediction, and\nmaintenance decision-making. Utilizing both curated case sets and publicly\navailable industrial datasets, our study enables multi-dimensional evaluation\nof general-purpose and domain-specific models across diverse PHM tasks.\nPHM-Bench establishes a methodological foundation for large-scale assessment of\nLLMs in PHM and offers a critical benchmark to guide the transition from\ngeneral-purpose to PHM-specialized models.",
      "pdf_url": "http://arxiv.org/pdf/2508.02490v1",
      "published": "2025-08-04T15:01:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02490v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration",
      "authors": [
        "Hyunjn An",
        "Yongwon Kim",
        "Wonduk Seo",
        "Joonil Park",
        "Daye Kang",
        "Changhoon Oh",
        "Dokyun Kim",
        "Seunghyun Lee"
      ],
      "abstract": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design.",
      "pdf_url": "http://arxiv.org/pdf/2508.02470v1",
      "published": "2025-08-04T14:36:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02470v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "cs.SE"
      ]
    },
    {
      "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs",
      "authors": [
        "Daniele Cipollone",
        "Egor Bogomolov",
        "Arie van Deursen",
        "Maliheh Izadi"
      ],
      "abstract": "Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance.",
      "pdf_url": "http://arxiv.org/pdf/2508.02455v1",
      "published": "2025-08-04T14:20:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02455v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for Local-Life Service Recommendation",
      "authors": [
        "Zhaoyu Hu",
        "Hao Guo",
        "Yuan Tian",
        "Erpeng Xue",
        "Jianyang Wang",
        "Xianyang Qi",
        "Hongxiang Lin",
        "Lei Wang",
        "Sheng Chen"
      ],
      "abstract": "In the context of the booming digital economy, recommendation systems, as a\nkey link connecting users and numerous services, face challenges in modeling\nuser behavior sequences on local-life service platforms, including the sparsity\nof long sequences and strong spatio-temporal dependence. Such challenges can be\naddressed by drawing an analogy to the forgetting process in human memory. This\nis because users' responses to recommended content follow the recency effect\nand the cyclicality of memory. By exploring this, this paper introduces the\nforgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM)\nwith long sequences for local-life service recommendation. STIM integrates\nthree key components: a dynamic masking module based on the forgetting curve,\nwhich is used to extract both recent spatiotemporal features and periodic\nspatiotemporal features; a query-based mixture of experts (MoE) approach that\ncan adaptively activate expert networks under different dynamic masks, enabling\nthe collaborative modeling of time, location, and items; and a hierarchical\nmulti-interest network unit, which captures multi-interest representations by\nmodeling the hierarchical interactions between the shallow and deep semantics\nof users' recent behaviors. By introducing the STIM method, we conducted online\nA/B tests and achieved a 1.54\\% improvement in gross transaction volume (GTV).\nIn addition, extended offline experiments also showed improvements. STIM has\nbeen deployed in a large-scale local-life service recommendation system,\nserving hundreds of millions of daily active users in core application\nscenarios.",
      "pdf_url": "http://arxiv.org/pdf/2508.02451v1",
      "published": "2025-08-04T14:16:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02451v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Assessing the Reliability and Validity of Large Language Models for Automated Assessment of Student Essays in Higher Education",
      "authors": [
        "Andrea Gaggioli",
        "Giuseppe Casaburi",
        "Leonardo Ercolani",
        "Francesco Collova'",
        "Pietro Torre",
        "Fabrizio Davide"
      ],
      "abstract": "This study investigates the reliability and validity of five advanced Large\nLanguage Models (LLMs), Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral\n24B, for automated essay scoring in a real world higher education context. A\ntotal of 67 Italian-language student essays, written as part of a university\npsychology course, were evaluated using a four-criterion rubric (Pertinence,\nCoherence, Originality, Feasibility). Each model scored all essays across three\nprompt replications to assess intra-model stability. Human-LLM agreement was\nconsistently low and non-significant (Quadratic Weighted Kappa), and\nwithin-model reliability across replications was similarly weak (median\nKendall's W < 0.30). Systematic scoring divergences emerged, including a\ntendency to inflate Coherence and inconsistent handling of context-dependent\ndimensions. Inter-model agreement analysis revealed moderate convergence for\nCoherence and Originality, but negligible concordance for Pertinence and\nFeasibility. Although limited in scope, these findings suggest that current\nLLMs may struggle to replicate human judgment in tasks requiring disciplinary\ninsight and contextual sensitivity. Human oversight remains critical when\nevaluating open-ended academic work, particularly in interpretive domains.",
      "pdf_url": "http://arxiv.org/pdf/2508.02442v1",
      "published": "2025-08-04T14:02:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02442v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Multimodal Large Language Models for End-to-End Affective Computing: Benchmarking and Boosting with Generative Knowledge Prompting",
      "authors": [
        "Miaosen Luo",
        "Jiesen Long",
        "Zequn Li",
        "Yunying Yang",
        "Yuncheng Jiang",
        "Sijie Mai"
      ],
      "abstract": "Multimodal Affective Computing (MAC) aims to recognize and interpret human\nemotions by integrating information from diverse modalities such as text,\nvideo, and audio. Recent advancements in Multimodal Large Language Models\n(MLLMs) have significantly reshaped the landscape of MAC by offering a unified\nframework for processing and aligning cross-modal information. However,\npractical challenges remain, including performance variability across complex\nMAC tasks and insufficient understanding of how architectural designs and data\ncharacteristics impact affective analysis. To address these gaps, we conduct a\nsystematic benchmark evaluation of state-of-the-art open-source MLLMs capable\nof concurrently processing audio, visual, and textual modalities across\nmultiple established MAC datasets. Our evaluation not only compares the\nperformance of these MLLMs but also provides actionable insights into model\noptimization by analyzing the influence of model architectures and dataset\nproperties. Furthermore, we propose a novel hybrid strategy that combines\ngenerative knowledge prompting with supervised fine-tuning to enhance MLLMs'\naffective computing capabilities. Experimental results demonstrate that this\nintegrated approach significantly improves performance across various MAC\ntasks, offering a promising avenue for future research and development in this\nfield. Our code is released on https://github.com/LuoMSen/MLLM-MAC.",
      "pdf_url": "http://arxiv.org/pdf/2508.02429v1",
      "published": "2025-08-04T13:49:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02429v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through Composing Ready-to-Use Models",
      "authors": [
        "Tung-Thuy Pham",
        "Duy-Quan Luong",
        "Minh-Quan Duong",
        "Trung-Hieu Nguyen",
        "Thu-Trang Nguyen",
        "Son Nguyen",
        "Hieu Dinh Vo"
      ],
      "abstract": "Composable AI offers a scalable and effective paradigm for tackling complex\nAI tasks by decomposing them into sub-tasks and solving each sub-task using\nready-to-use well-trained models. However, systematically evaluating methods\nunder this setting remains largely unexplored. In this paper, we introduce\nCABENCH, the first public benchmark comprising 70 realistic composable AI\ntasks, along with a curated pool of 700 models across multiple modalities and\ndomains. We also propose an evaluation framework to enable end-to-end\nassessment of composable AI solutions. To establish initial baselines, we\nprovide human-designed reference solutions and compare their performance with\ntwo LLM-based approaches. Our results illustrate the promise of composable AI\nin addressing complex real-world problems while highlighting the need for\nmethods that can fully unlock its potential by automatically generating\neffective execution pipelines.",
      "pdf_url": "http://arxiv.org/pdf/2508.02427v1",
      "published": "2025-08-04T13:48:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02427v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Multi-Class Human/Object Detection on Robot Manipulators using Proprioceptive Sensing",
      "authors": [
        "Justin Hehli",
        "Marco Heiniger",
        "Maryam Rezayati",
        "Hans Wernher van de Venn"
      ],
      "abstract": "In physical human-robot collaboration (pHRC) settings, humans and robots\ncollaborate directly in shared environments. Robots must analyze interactions\nwith objects to ensure safety and facilitate meaningful workflows. One critical\naspect is human/object detection, where the contacted object is identified.\nPast research introduced binary machine learning classifiers to distinguish\nbetween soft and hard objects. This study improves upon those results by\nevaluating three-class human/object detection models, offering more detailed\ncontact analysis. A dataset was collected using the Franka Emika Panda robot\nmanipulator, exploring preprocessing strategies for time-series analysis.\nModels including LSTM, GRU, and Transformers were trained on these datasets.\nThe best-performing model achieved 91.11\\% accuracy during real-time testing,\ndemonstrating the feasibility of multi-class detection models. Additionally, a\ncomparison of preprocessing strategies suggests a sliding window approach is\noptimal for this task.",
      "pdf_url": "http://arxiv.org/pdf/2508.02425v1",
      "published": "2025-08-04T13:45:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02425v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement Learning",
      "authors": [
        "Akshay Dodwadmath",
        "Setareh Maghsudi"
      ],
      "abstract": "Stackelberg games and their resulting equilibria have received increasing\nattention in the multi-agent reinforcement learning literature. Each stage of a\ntraditional Stackelberg game involves a leader(s) acting first, followed by the\nfollowers. In situations where the roles of leader(s) and followers can be\ninterchanged, the designated role can have considerable advantages, for\nexample, in first-mover advantage settings. Then the question arises: Who\nshould be the leader and when? A bias in the leader selection process can lead\nto unfair outcomes. This problem is aggravated if the agents are\nself-interested and care only about their goals and rewards. We formally define\nthis leader selection problem and show its relation to fairness in agents'\nreturns. Furthermore, we propose a multi-agent reinforcement learning framework\nthat maximizes fairness by integrating mediators. Mediators have previously\nbeen used in the simultaneous action setting with varying levels of control,\nsuch as directly performing agents' actions or just recommending them. Our\nframework integrates mediators in the Stackelberg setting with minimal control\n(leader selection). We show that the presence of mediators leads to\nself-interested agents taking fair actions, resulting in higher overall\nfairness in agents' returns.",
      "pdf_url": "http://arxiv.org/pdf/2508.02421v1",
      "published": "2025-08-04T13:42:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02421v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time Series Analysis",
      "authors": [
        "Xiao Wang",
        "Hao Si",
        "Fan Zhang",
        "Xiaoya Zhou",
        "Dengdi Sun",
        "Wanli Lyu",
        "Qingquan Yang",
        "Jin Tang"
      ],
      "abstract": "Multivariate time series analysis has long been one of the key research\ntopics in the field of artificial intelligence. However, analyzing complex time\nseries data remains a challenging and unresolved problem due to its high\ndimensionality, dynamic nature, and complex interactions among variables.\nInspired by the strong structural modeling capability of hypergraphs, this\npaper proposes a novel hypergraph-based time series transformer backbone\nnetwork, termed HGTS-Former, to address the multivariate coupling in time\nseries data. Specifically, given the multivariate time series signal, we first\nnormalize and embed each patch into tokens. Then, we adopt the multi-head\nself-attention to enhance the temporal representation of each patch. The\nhierarchical hypergraphs are constructed to aggregate the temporal patterns\nwithin each channel and fine-grained relations between different variables.\nAfter that, we convert the hyperedge into node features through the EdgeToNode\nmodule and adopt the feed-forward network to further enhance the output\nfeatures. Extensive experiments conducted on two multivariate time series tasks\nand eight datasets fully validated the effectiveness of our proposed\nHGTS-Former. The source code will be released on\nhttps://github.com/Event-AHU/Time_Series_Analysis.",
      "pdf_url": "http://arxiv.org/pdf/2508.02411v1",
      "published": "2025-08-04T13:33:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02411v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera Fusion",
      "authors": [
        "Yimeng Liu",
        "Maolin Gan",
        "Huaili Zeng",
        "Li Liu",
        "Younsuk Dong",
        "Zhichao Cao"
      ],
      "abstract": "Leaf Wetness Duration (LWD), the time that water remains on leaf surfaces, is\ncrucial in the development of plant diseases. Existing LWD detection lacks\nstandardized measurement techniques, and variations across different plant\ncharacteristics limit its effectiveness. Prior research proposes diverse\napproaches, but they fail to measure real natural leaves directly and lack\nresilience in various environmental conditions. This reduces the precision and\nrobustness, revealing a notable practical application and effectiveness gap in\nreal-world agricultural settings. This paper presents Hydra, an innovative\napproach that integrates millimeter-wave (mm-Wave) radar with camera technology\nto detect leaf wetness by determining if there is water on the leaf. We can\nmeasure the time to determine the LWD based on this detection. Firstly, we\ndesign a Convolutional Neural Network (CNN) to selectively fuse multiple\nmm-Wave depth images with an RGB image to generate multiple feature images.\nThen, we develop a transformer-based encoder to capture the inherent connection\namong the multiple feature images to generate a feature map, which is further\nfed to a classifier for detection. Moreover, we augment the dataset during\ntraining to generalize our model. Implemented using a frequency-modulated\ncontinuous-wave (FMCW) radar within the 76 to 81 GHz band, Hydra's performance\nis meticulously evaluated on plants, demonstrating the potential to classify\nleaf wetness with up to 96% accuracy across varying scenarios. Deploying Hydra\nin the farm, including rainy, dawn, or poorly light nights, it still achieves\nan accuracy rate of around 90%.",
      "pdf_url": "http://arxiv.org/pdf/2508.02409v1",
      "published": "2025-08-04T13:33:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02409v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation",
      "authors": [
        "Xiaolin Lin",
        "Jingcun Wang",
        "Olga Kondrateva",
        "Yiyu Shi",
        "Bing Li",
        "Grace Li Zhang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
      "pdf_url": "http://arxiv.org/pdf/2508.02401v1",
      "published": "2025-08-04T13:26:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02401v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Inference-time Scaling for Diffusion-based Audio Super-resolution",
      "authors": [
        "Yizhu Jin",
        "Zhen Ye",
        "Zeyue Tian",
        "Haohe Liu",
        "Qiuqiang Kong",
        "Yike Guo",
        "Wei Xue"
      ],
      "abstract": "Diffusion models have demonstrated remarkable success in generative tasks,\nincluding audio super-resolution (SR). In many applications like movie\npost-production and album mastering, substantial computational budgets are\navailable for achieving superior audio quality. However, while existing\ndiffusion approaches typically increase sampling steps to improve quality, the\nperformance remains fundamentally limited by the stochastic nature of the\nsampling process, leading to high-variance and quality-limited outputs. Here,\nrather than simply increasing the number of sampling steps, we propose a\ndifferent paradigm through inference-time scaling for SR, which explores\nmultiple solution trajectories during the sampling process. Different\ntask-specific verifiers are developed, and two search algorithms, including the\nrandom search and zero-order search for SR, are introduced. By actively guiding\nthe exploration of the high-dimensional solution space through\nverifier-algorithm combinations, we enable more robust and higher-quality\noutputs. Through extensive validation across diverse audio domains (speech,\nmusic, sound effects) and frequency ranges, we demonstrate consistent\nperformance gains, achieving improvements of up to 9.70% in aesthetics, 5.88%\nin speaker similarity, 15.20% in word error rate, and 46.98% in spectral\ndistance for speech SR from 4kHz to 24kHz, showcasing the effectiveness of our\napproach. Audio samples are available at:\nhttps://racerk.github.io/tt-scale-audiosr/.",
      "pdf_url": "http://arxiv.org/pdf/2508.02391v1",
      "published": "2025-08-04T13:17:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02391v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering",
      "authors": [
        "Xu Wang",
        "Shengeng Tang",
        "Fei Wang",
        "Lechao Cheng",
        "Dan Guo",
        "Feng Xue",
        "Richang Hong"
      ],
      "abstract": "Generating semantically coherent and visually accurate talking faces requires\nbridging the gap between linguistic meaning and facial articulation. Although\naudio-driven methods remain prevalent, their reliance on high-quality paired\naudio visual data and the inherent ambiguity in mapping acoustics to lip motion\npose significant challenges in terms of scalability and robustness. To address\nthese issues, we propose Text2Lip, a viseme-centric framework that constructs\nan interpretable phonetic-visual bridge by embedding textual input into\nstructured viseme sequences. These mid-level units serve as a linguistically\ngrounded prior for lip motion prediction. Furthermore, we design a progressive\nviseme-audio replacement strategy based on curriculum learning, enabling the\nmodel to gradually transition from real audio to pseudo-audio reconstructed\nfrom enhanced viseme features via cross-modal attention. This allows for robust\ngeneration in both audio-present and audio-free scenarios. Finally, a\nlandmark-guided renderer synthesizes photorealistic facial videos with accurate\nlip synchronization. Extensive evaluations show that Text2Lip outperforms\nexisting approaches in semantic fidelity, visual realism, and modality\nrobustness, establishing a new paradigm for controllable and flexible talking\nface generation. Our project homepage is https://plyon1.github.io/Text2Lip/.",
      "pdf_url": "http://arxiv.org/pdf/2508.02362v1",
      "published": "2025-08-04T12:50:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02362v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera",
      "authors": [
        "Byeonggyu Park",
        "Hee-Yeun Kim",
        "Byonghyok Choi",
        "Hansang Cho",
        "Byungkwan Kim",
        "Soomok Lee",
        "Mingu Jeon",
        "Seong-Woo Kim"
      ],
      "abstract": "Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban\nenvironments poses a significant challenge for autonomous driving systems.\nWhile mmWave radar has demonstrated potential for detecting objects in such\nscenarios, the 2D radar point cloud (PCD) data is susceptible to distortions\ncaused by multipath reflections, making accurate spatial inference difficult.\nAdditionally, although camera images provide high-resolution visual\ninformation, they lack depth perception and cannot directly observe objects in\nNLoS regions. In this paper, we propose a novel framework that interprets radar\nPCD through road layout inferred from camera for localization of NLoS\npedestrians. The proposed method leverages visual information from the camera\nto interpret 2D radar PCD, enabling spatial scene reconstruction. The\neffectiveness of the proposed approach is validated through experiments\nconducted using a radar-camera system mounted on a real vehicle. The\nlocalization performance is evaluated using a dataset collected in outdoor NLoS\ndriving environments, demonstrating the practical applicability of the method.",
      "pdf_url": "http://arxiv.org/pdf/2508.02348v1",
      "published": "2025-08-04T12:31:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02348v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems",
      "authors": [
        "Xingchen Zou",
        "Yuhao Yang",
        "Zheng Chen",
        "Xixuan Hao",
        "Yiqi Chen",
        "Chao Huang",
        "Yuxuan Liang"
      ],
      "abstract": "Traffic signal control (TSC) is vital for mitigating congestion and\nsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation\nmodel with human-like reasoning for TSC systems. Our model is developed through\nself-exploration and iteration of reinforced large language models (LLMs) with\nexpert guidance in a simulated traffic environment. Compared to traditional\nreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers\nthree significant advantages. First, Traffic-R1 delivers zero-shot\ngeneralisation, transferring unchanged to new road networks and\nout-of-distribution incidents by utilizing its internal traffic control\npolicies and human-like reasoning. Second, its 3B-parameter architecture is\nlightweight enough for real-time inference on mobile-class chips, enabling\nlarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSC\nprocess and facilitates multi-intersection communication through its\nself-iteration and a new synchronous communication network. Extensive\nbenchmarks demonstrate that Traffic-R1 sets a new state of the art,\noutperforming strong baselines and training-intensive RL controllers. In\npractice, the model now manages signals for more than 55,000 drivers daily,\nshortening average queues by over 5% and halving operator workload. Our\ncheckpoint is available at https://huggingface.co/Season998/Traffic-R1.",
      "pdf_url": "http://arxiv.org/pdf/2508.02344v1",
      "published": "2025-08-04T12:25:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02344v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MicroMix: Efficient Mixed-Precision Quantization with Microscaling Formats for Large Language Models",
      "authors": [
        "Wenyuan Liu",
        "Haoqian Meng",
        "Yilun Luo",
        "Peng Zhang",
        "Xindian Ma"
      ],
      "abstract": "Quantization significantly accelerates inference in large language models\n(LLMs) by replacing original high-precision matrices with low-precision\ncounterparts. Recent advances in weight-activation quantization have primarily\nfocused on mapping both weights and activations to the INT4 format. Although\nthe new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x\nspeedup over FP16, existing INT4-based kernels fail to fully exploit this\ncapability due to mismatched data formats. To bridge this gap, we propose\nMicroMix, a co-designed mixed-precision quantization algorithm and matrix\nmultiplication kernel based on Microscaling (MX) data formats. Tailored for the\nBlackwell architecture, the MicroMix kernel supports arbitrary combinations of\nMXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a\nfavorable trade-off between accuracy and efficiency for each linear layer, we\nintroduce quantization thresholds that identify activation elements where\nlower-precision formats (MXFP4 or MXFP6) incur excessive quantization error.\nOur algorithm selectively allocates higher-precision channels to preserve\naccuracy while maintaining compute efficiency. MicroMix achieves competitive or\nsuperior performance across diverse downstream tasks, including zero-shot and\nfew-shot learning, language modeling, code generation, and mathematical\nreasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX\n5090) GPUs, our kernel delivers at least 20% faster execution than\nTensorRT-FP8. Furthermore, when applied to various Llama and Qwen models,\nMicroMix consistently improves prefill latency and memory efficiency across a\nrange of batch sizes compared to TensorRT baselines. Our code is available at\nhttps://github.com/lwy2020/MicroMix.",
      "pdf_url": "http://arxiv.org/pdf/2508.02343v1",
      "published": "2025-08-04T12:22:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02343v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo",
      "authors": [
        "Qianli Ma",
        "Yaowei Zheng",
        "Zhelun Shi",
        "Zhongkai Zhao",
        "Bin Jia",
        "Ziyue Huang",
        "Zhiqi Lin",
        "Youjie Li",
        "Jiacheng Yang",
        "Yanghua Peng",
        "Zhi Zhang",
        "Xin Liu"
      ],
      "abstract": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2508.02317v2",
      "published": "2025-08-04T11:33:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02317v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "A Survey on Data Security in Large Language Models",
      "authors": [
        "Kang Chen",
        "Xiuze Zhou",
        "Yuanguo Lin",
        "Jinhe Su",
        "Yuanhui Yu",
        "Li Shen",
        "Fan Lin"
      ],
      "abstract": "Large Language Models (LLMs), now a foundation in advancing natural language\nprocessing, power applications such as text generation, machine translation,\nand conversational systems. Despite their transformative potential, these\nmodels inherently rely on massive amounts of training data, often collected\nfrom diverse and uncurated sources, which exposes them to serious data security\nrisks. Harmful or malicious data can compromise model behavior, leading to\nissues such as toxic output, hallucinations, and vulnerabilities to threats\nsuch as prompt injection or data poisoning. As LLMs continue to be integrated\ninto critical real-world systems, understanding and addressing these\ndata-centric security risks is imperative to safeguard user trust and system\nreliability. This survey offers a comprehensive overview of the main data\nsecurity risks facing LLMs and reviews current defense strategies, including\nadversarial training, RLHF, and data augmentation. Additionally, we categorize\nand analyze relevant datasets used for assessing robustness and security across\ndifferent domains, providing guidance for future research. Finally, we\nhighlight key research directions that focus on secure model updates,\nexplainability-driven defenses, and effective governance frameworks, aiming to\npromote the safe and responsible development of LLM technology. This work aims\nto inform researchers, practitioners, and policymakers, driving progress toward\ndata security in LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2508.02312v1",
      "published": "2025-08-04T11:28:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02312v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment",
      "authors": [
        "Guofu Xie",
        "Yunsheng Shi",
        "Hongtao Tian",
        "Ting Yao",
        "Xiao Zhang"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2508.02298v1",
      "published": "2025-08-04T11:06:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02298v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment",
      "authors": [
        "Wentao Zhang",
        "Yilei Zhao",
        "Chuqiao Zong",
        "Xinrun Wang",
        "Bo An"
      ],
      "abstract": "Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}.",
      "pdf_url": "http://arxiv.org/pdf/2508.02292v1",
      "published": "2025-08-04T11:02:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02292v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Flexible Automatic Identification and Removal (FAIR)-Pruner: An Efficient Neural Network Pruning Method",
      "authors": [
        "Chenqing Lin",
        "Mostafa Hussien",
        "Chengyao Yu",
        "Mohamed Cheriet",
        "Osama Abdelrahman",
        "Ruixing Ming"
      ],
      "abstract": "Neural network pruning is a critical compression technique that facilitates\nthe deployment of large-scale neural networks on resource-constrained edge\ndevices, typically by identifying and eliminating redundant or insignificant\nparameters to reduce computational and memory overhead. This paper proposes the\nFlexible Automatic Identification and Removal (FAIR)-Pruner, a novel method for\nneural network structured pruning. Specifically, FAIR-Pruner first evaluates\nthe importance of each unit (e.g., neuron or channel) through the Utilization\nScore quantified by the Wasserstein distance. To reflect the performance\ndegradation after unit removal, it then introduces the Reconstruction Error,\nwhich is computed via the Taylor expansion of the loss function. Finally,\nFAIR-Pruner identifies superfluous units with negligible impact on model\nperformance by controlling the proposed Tolerance of Difference, which measures\ndifferences between unimportant units and those that cause performance\ndegradation. A major advantage of FAIR-Pruner lies in its capacity to\nautomatically determine the layer-wise pruning rates, which yields a more\nefficient subnetwork structure compared to applying a uniform pruning rate.\nAnother advantage of the FAIR-Pruner is its great one-shot performance without\npost-pruning fine-tuning. Furthermore, with utilization scores and\nreconstruction errors, users can flexibly obtain pruned models under different\npruning ratios. Comprehensive experimental validation on diverse benchmark\ndatasets (e.g., ImageNet) and various neural network architectures (e.g., VGG)\ndemonstrates that FAIR-Pruner achieves significant model compression while\nmaintaining high accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.02291v1",
      "published": "2025-08-04T10:59:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02291v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Dialogue Systems Engineering: A Survey and Future Directions",
      "authors": [
        "Mikio Nakano",
        "Hironori Takeuchi",
        "Sadahiro Yoshikawa",
        "Yoichi Matsuyama",
        "Kazunori Komatani"
      ],
      "abstract": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering.",
      "pdf_url": "http://arxiv.org/pdf/2508.02279v1",
      "published": "2025-08-04T10:49:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02279v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "CellForge: Agentic Design of Virtual Cell Models",
      "authors": [
        "Xiangru Tang",
        "Zhuoyun Yu",
        "Jiapeng Chen",
        "Yan Cui",
        "Daniel Shao",
        "Weixu Wang",
        "Fang Wu",
        "Yuchen Zhuang",
        "Wenqi Shi",
        "Zhi Huang",
        "Arman Cohan",
        "Xihong Lin",
        "Fabian Theis",
        "Smita Krishnaswamy",
        "Mark Gerstein"
      ],
      "abstract": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
      "pdf_url": "http://arxiv.org/pdf/2508.02276v1",
      "published": "2025-08-04T10:43:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02276v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "q-bio.QM"
      ]
    },
    {
      "title": "Dynaword: From One-shot to Continuously Developed Datasets",
      "authors": [
        "Kenneth Enevoldsen",
        "Kristian Nørgaard Jensen",
        "Jan Kostkan",
        "Balázs Szabó",
        "Márton Kardos",
        "Kirten Vad",
        "Johan Heinsen",
        "Andrea Blasi Núñez",
        "Gianluca Barmina",
        "Jacob Nielsen",
        "Rasmus Larsen",
        "Peter Vahlstrup",
        "Per Møldrup Dalum",
        "Desmond Elliott",
        "Lukas Galke",
        "Peter Schneider-Kamp",
        "Kristoffer Nielbo"
      ],
      "abstract": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution.",
      "pdf_url": "http://arxiv.org/pdf/2508.02271v2",
      "published": "2025-08-04T10:30:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.02271v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}