{
  "last_updated": "2025-12-08T00:54:18.647859",
  "papers": [
    {
      "title": "The Universal Weight Subspace Hypothesis",
      "authors": [
        "Prakhar Kaushik",
        "Shravan Chaudhari",
        "Ankit Vaidya",
        "Rama Chellappa",
        "Alan Yuille"
      ],
      "abstract": "We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.",
      "pdf_url": "https://arxiv.org/pdf/2512.05117v1",
      "published": "2025-12-04T18:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05117v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "authors": [
        "Dongzhi Jiang",
        "Renrui Zhang",
        "Haodong Li",
        "Zhuofan Zong",
        "Ziyu Guo",
        "Jun He",
        "Claire Guo",
        "Junyan Ye",
        "Rongyao Fang",
        "Weijia Li",
        "Rui Liu",
        "Hongsheng Li"
      ],
      "abstract": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
      "pdf_url": "https://arxiv.org/pdf/2512.05112v1",
      "published": "2025-12-04T18:59:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05112v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "authors": [
        "Rundong Luo",
        "Noah Snavely",
        "Wei-Chiu Ma"
      ],
      "abstract": "We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!",
      "pdf_url": "https://arxiv.org/pdf/2512.05110v1",
      "published": "2025-12-04T18:59:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05110v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning",
      "authors": [
        "Purbesh Mitra",
        "Sennur Ulukus"
      ],
      "abstract": "Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.",
      "pdf_url": "https://arxiv.org/pdf/2512.05105v1",
      "published": "2025-12-04T18:59:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05105v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "authors": [
        "Xiaochuang Han",
        "Youssef Emad",
        "Melissa Hall",
        "John Nguyen",
        "Karthik Padthe",
        "Liam Robbins",
        "Amir Bar",
        "Delong Chen",
        "Michal Drozdzal",
        "Maha Elbayad",
        "Yushi Hu",
        "Shang-Wen Li",
        "Sreya Dutta Roy",
        "Jakob Verbeek",
        "XuDong Wang",
        "Marjan Ghazvininejad",
        "Luke Zettlemoyer",
        "Emily Dinan"
      ],
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "pdf_url": "https://arxiv.org/pdf/2512.05103v1",
      "published": "2025-12-04T18:59:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05103v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Structured Document Translation via Format Reinforcement Learning",
      "authors": [
        "Haiyue Song",
        "Johannes Eschbach-Dymanus",
        "Hour Kaing",
        "Sumire Honda",
        "Hideki Tanaka",
        "Bianka Buschbeck",
        "Masao Utiyama"
      ],
      "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.",
      "pdf_url": "https://arxiv.org/pdf/2512.05100v1",
      "published": "2025-12-04T18:58:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05100v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards",
      "authors": [
        "Yuan Gao",
        "Jin Song"
      ],
      "abstract": "In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.",
      "pdf_url": "https://arxiv.org/pdf/2512.05098v1",
      "published": "2025-12-04T18:58:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05098v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?",
      "authors": [
        "Shashwat Shankar",
        "Subhranshu Pandey",
        "Innocent Dengkhw Mochahari",
        "Bhabesh Mali",
        "Animesh Basak Chowdhury",
        "Sukanta Bhattacharjee",
        "Chandan Karfa"
      ],
      "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.",
      "pdf_url": "https://arxiv.org/pdf/2512.05073v1",
      "published": "2025-12-04T18:37:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05073v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.SE"
      ]
    },
    {
      "title": "Multi-LLM Collaboration for Medication Recommendation",
      "authors": [
        "Huascar Sanchez",
        "Briland Hitaj",
        "Jules Bergmann",
        "Linda Briesemeister"
      ],
      "abstract": "As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.",
      "pdf_url": "https://arxiv.org/pdf/2512.05066v1",
      "published": "2025-12-04T18:25:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05066v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Meta-Learning for Quantum Optimization via Quantum Sequence Model",
      "authors": [
        "Yu-Cheng Lin",
        "Yu-Chao Hsu",
        "Samuel Yen-Chi Chen"
      ],
      "abstract": "The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies. We investigate four classical or quantum sequence models, including the Quantum Kernel-based Long Short-Term Memory (QK-LSTM), as learned optimizers in a \"learning to learn\" paradigm. Our numerical experiments on the Max-Cut problem demonstrate that the QK-LSTM optimizer achieves superior performance, obtaining the highest approximation ratios and exhibiting the fastest convergence rate across all tested problem sizes (n=10 to 13). Crucially, the QK-LSTM model achieves perfect parameter transferability by synthesizing a single, fixed set of near-optimal parameters, leading to a remarkable sustained acceleration of convergence even when generalizing to larger problems. This capability, enabled by the compact and expressive power of the quantum kernel architecture, underscores its effectiveness. The QK-LSTM, with only 43 trainable parameters, substantially outperforms the classical LSTM (56 parameters) and other quantum sequence models, establishing a robust pathway toward highly efficient parameter initialization for variational quantum algorithms in the NISQ era.",
      "pdf_url": "https://arxiv.org/pdf/2512.05058v1",
      "published": "2025-12-04T18:13:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05058v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "authors": [
        "Yu-Chao Hsu",
        "Jiun-Cheng Jiang",
        "Chun-Hua Lin",
        "Kuo-Chung Peng",
        "Nan-Yow Chen",
        "Samuel Yen-Chi Chen",
        "En-Jui Kuo",
        "Hsi-Sheng Goan"
      ],
      "abstract": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.",
      "pdf_url": "https://arxiv.org/pdf/2512.05049v1",
      "published": "2025-12-04T18:03:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05049v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
      "authors": [
        "Monishwaran Maheswaran",
        "Rishabh Tiwari",
        "Yuezhou Hu",
        "Kerem Dilmen",
        "Coleman Hooper",
        "Haocheng Xi",
        "Nicholas Lee",
        "Mehrdad Farajtabar",
        "Michael W. Mahoney",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "abstract": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\\sim2\\times$ at matched accuracy.",
      "pdf_url": "https://arxiv.org/pdf/2512.05033v1",
      "published": "2025-12-04T17:50:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05033v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Model-Free Assessment of Simulator Fidelity via Quantile Curves",
      "authors": [
        "Garud Iyengar",
        "Yu-Shiou Willy Lin",
        "Kaizheng Wang"
      ],
      "abstract": "Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2512.05024v1",
      "published": "2025-12-04T17:39:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05024v1",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Detecting Perspective Shifts in Multi-agent Systems",
      "authors": [
        "Eric Bridgeford",
        "Hayden Helm"
      ],
      "abstract": "Generative models augmented with external tools and update mechanisms (or \\textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.",
      "pdf_url": "https://arxiv.org/pdf/2512.05013v1",
      "published": "2025-12-04T17:24:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05013v1",
      "categories": [
        "cs.AI",
        "cs.MA",
        "stat.ME"
      ]
    },
    {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "authors": [
        "Daniyar Zakarin",
        "Thiemo Wandel",
        "Anton Obukhov",
        "Dengxin Dai"
      ],
      "abstract": "We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "pdf_url": "https://arxiv.org/pdf/2512.05000v1",
      "published": "2025-12-04T17:12:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.05000v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Evolutionary Architecture Search through Grammar-Based Sequence Alignment",
      "authors": [
        "Adri Gómez Martín",
        "Felix Möller",
        "Steven McDonagh",
        "Monica Abella",
        "Manuel Desco",
        "Elliot J. Crowley",
        "Aaron Klein",
        "Linus Ericsson"
      ],
      "abstract": "Neural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search. These algorithms enable us to efficiently calculate a distance metric for neural architectures and to generate a set of hybrid offspring from two parent models. This facilitates the deployment of crossover-based search heuristics, allows us to perform a thorough analysis on the architectural loss landscape, and track population diversity during search. We highlight how our method vastly improves computational complexity over previous work and enables us to efficiently compute shortest paths between architectures. When instantiating the crossover in evolutionary searches, we achieve competitive results, outperforming competing methods. Future work can build upon this new tool, discovering novel components that can be used more broadly across neural architecture design, and broadening its applications beyond NAS.",
      "pdf_url": "https://arxiv.org/pdf/2512.04992v1",
      "published": "2025-12-04T16:57:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04992v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets",
      "authors": [
        "Christopher Chiu",
        "Simpson Zhang",
        "Mihaela van der Schaar"
      ],
      "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.",
      "pdf_url": "https://arxiv.org/pdf/2512.04988v1",
      "published": "2025-12-04T16:57:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04988v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis",
      "authors": [
        "Jasmaine Khale",
        "Ravi Prakash Srivastava"
      ],
      "abstract": "Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.",
      "pdf_url": "https://arxiv.org/pdf/2512.04967v1",
      "published": "2025-12-04T16:35:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04967v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "GeoPE:A Unified Geometric Positional Embedding for Structured Tensors",
      "authors": [
        "Yupu Yao",
        "Bowen Yang"
      ],
      "abstract": "Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.",
      "pdf_url": "https://arxiv.org/pdf/2512.04963v1",
      "published": "2025-12-04T16:31:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04963v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning",
      "authors": [
        "Roberto Cipollone",
        "Luca Iocchi",
        "Matteo Leonetti"
      ],
      "abstract": "The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.",
      "pdf_url": "https://arxiv.org/pdf/2512.04958v1",
      "published": "2025-12-04T16:26:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04958v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics",
      "authors": [
        "Weiye Shi",
        "Zhaowei Zhang",
        "Shaoheng Yan",
        "Yaodong Yang"
      ],
      "abstract": "Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.",
      "pdf_url": "https://arxiv.org/pdf/2512.04957v1",
      "published": "2025-12-04T16:26:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04957v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent",
      "authors": [
        "Leyang Shen",
        "Yang Zhang",
        "Chun Kai Ling",
        "Xiaoyan Zhao",
        "Tat-Seng Chua"
      ],
      "abstract": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.",
      "pdf_url": "https://arxiv.org/pdf/2512.04949v1",
      "published": "2025-12-04T16:15:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04949v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases",
      "authors": [
        "Raquel Norel",
        "Michele Merler",
        "Pavitra Modi"
      ],
      "abstract": "Patients with rare neurological diseases report cognitive symptoms -\"brain fog\"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived \"Proficiency in Verbal Discourse\" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.",
      "pdf_url": "https://arxiv.org/pdf/2512.04938v1",
      "published": "2025-12-04T16:06:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04938v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Algorithmic Thinking Theory",
      "authors": [
        "MohammadHossein Bateni",
        "Vincent Cohen-Addad",
        "Yuzhou Gu",
        "Silvio Lattanzi",
        "Simon Meierhans",
        "Christopher Mohri"
      ],
      "abstract": "Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.\n  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.",
      "pdf_url": "https://arxiv.org/pdf/2512.04923v1",
      "published": "2025-12-04T15:55:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04923v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "The AI Consumer Index (ACE)",
      "authors": [
        "Julien Benchek",
        "Rohit Shetty",
        "Benjamin Hunsberger",
        "Ajay Arun",
        "Zach Richards",
        "Brendan Foody",
        "Osvald Nitski",
        "Bertie Vidgen"
      ],
      "abstract": "We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.",
      "pdf_url": "https://arxiv.org/pdf/2512.04921v1",
      "published": "2025-12-04T15:54:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04921v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ]
    },
    {
      "title": "Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming",
      "authors": [
        "Fang Li"
      ],
      "abstract": "This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.",
      "pdf_url": "https://arxiv.org/pdf/2512.04910v1",
      "published": "2025-12-04T15:37:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04910v1",
      "categories": [
        "cs.AR",
        "cs.AI"
      ]
    },
    {
      "title": "ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching",
      "authors": [
        "Guanbo Huang",
        "Jingjia Mao",
        "Fanding Huang",
        "Fengkai Liu",
        "Xiangyang Luo",
        "Yaoyuan Liang",
        "Jiasheng Lu",
        "Xiaoe Wang",
        "Pei Liu",
        "Ruiliu Fu",
        "Shao-Lun Huang"
      ],
      "abstract": "Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.",
      "pdf_url": "https://arxiv.org/pdf/2512.04904v1",
      "published": "2025-12-04T15:34:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04904v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems",
      "authors": [
        "M Zeeshan",
        "Saud Satti"
      ],
      "abstract": "Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.",
      "pdf_url": "https://arxiv.org/pdf/2512.04895v1",
      "published": "2025-12-04T15:22:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04895v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions",
      "authors": [
        "Junjie Fan",
        "Hongye Zhao",
        "Linduo Wei",
        "Jiayu Rao",
        "Guijia Li",
        "Jiaxin Yuan",
        "Wenqi Xu",
        "Yong Qi"
      ],
      "abstract": "Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.",
      "pdf_url": "https://arxiv.org/pdf/2512.04871v1",
      "published": "2025-12-04T14:56:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04871v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Developing a General Personal Tutor for Education",
      "authors": [
        "Jaan Aru",
        "Kristjan-Julius Laak"
      ],
      "abstract": "The vision of a universal AI tutor has remained elusive, despite decades of effort. Could LLMs be the game-changer? We overview novel issues arising from developing a nationwide AI tutor. We highlight the practical questions that point to specific gaps in our scientific understanding of the learning process.",
      "pdf_url": "https://arxiv.org/pdf/2512.04869v1",
      "published": "2025-12-04T14:55:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04869v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "q-bio.NC"
      ]
    },
    {
      "title": "SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs",
      "authors": [
        "Hao Wang",
        "Jialun Zhong",
        "Changcheng Wang",
        "Zhujun Nie",
        "Zheng Li",
        "Shunyu Yao",
        "Yanzeng Li",
        "Xinchi Li"
      ],
      "abstract": "Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2512.04868v1",
      "published": "2025-12-04T14:52:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04868v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Are Your Agents Upward Deceivers?",
      "authors": [
        "Dadi Guo",
        "Qingyu Liu",
        "Dongrui Liu",
        "Qihan Ren",
        "Shuai Shao",
        "Tianyi Qiu",
        "Haoran Li",
        "Yi R. Fung",
        "Zhongjie Ba",
        "Juntao Dai",
        "Jiaming Ji",
        "Zhikai Chen",
        "Jialing Tao",
        "Yaodong Yang",
        "Jing Shao",
        "Xia Hu"
      ],
      "abstract": "Large Language Model (LLM)-based agents are increasingly used as autonomous subordinates that carry out tasks for users. This raises the question of whether they may also engage in deception, similar to how individuals in human organizations lie to superiors to create a good image or avoid punishment. We observe and define agentic upward deception, a phenomenon in which an agent facing environmental constraints conceals its failure and performs actions that were not requested without reporting. To assess its prevalence, we construct a benchmark of 200 tasks covering five task types and eight realistic scenarios in a constrained environment, such as broken tools or mismatched information sources. Evaluations of 11 popular LLMs reveal that these agents typically exhibit action-based deceptive behaviors, such as guessing results, performing unsupported simulations, substituting unavailable information sources, and fabricating local files. We further test prompt-based mitigation and find only limited reductions, suggesting that it is difficult to eliminate and highlighting the need for stronger mitigation strategies to ensure the safety of LLM-based agents.",
      "pdf_url": "https://arxiv.org/pdf/2512.04864v1",
      "published": "2025-12-04T14:47:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04864v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research",
      "authors": [
        "Lukas Weidener",
        "Marko Brkić",
        "Chiara Bacci",
        "Mihailo Jovanović",
        "Emre Ulgac",
        "Alex Dobrin",
        "Johannes Weniger",
        "Martin Vlas",
        "Ritvik Singh",
        "Aakaash Meduri"
      ],
      "abstract": "Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.",
      "pdf_url": "https://arxiv.org/pdf/2512.04854v1",
      "published": "2025-12-04T14:37:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04854v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding",
      "authors": [
        "Tsai-Ning Wang",
        "Lin-Lin Chen",
        "Neil Zeghidour",
        "Aaqib Saeed"
      ],
      "abstract": "Pre-trained audio models excel at detecting acoustic patterns in auscultation sounds but often fail to grasp their clinical significance, limiting their use and performance in diagnostic tasks. To bridge this gap, we introduce AcuLa (Audio-Clinical Understanding via Language Alignment), a lightweight post-training framework that instills semantic understanding into any audio encoder by aligning it with a medical language model, which acts as a \"semantic teacher.\" To enable alignment at scale, we construct a large-scale dataset by leveraging off-the-shelf large language models to translate the rich, structured metadata accompanying existing audio recordings into coherent clinical reports. Our alignment strategy combines a representation-level contrastive objective with a self-supervised modeling, ensuring that the model learns clinical semantics while preserving fine-grained temporal cues. AcuLa achieves state-of-the-art results across 18 diverse cardio-respiratory tasks from 10 different datasets, improving the mean AUROC on classification benchmarks from 0.68 to 0.79 and, on the most challenging COVID-19 cough detection task, boosting the AUROC from 0.55 to 0.89. Our work demonstrates that this audio-language alignment transforms purely acoustic models into clinically-aware diagnostic tools, establishing a novel paradigm for enhancing physiological understanding in audio-based health monitoring.",
      "pdf_url": "https://arxiv.org/pdf/2512.04847v1",
      "published": "2025-12-04T14:30:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04847v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "authors": [
        "Atsuki Yamaguchi",
        "Terufumi Morishita",
        "Aline Villavicencio",
        "Nikolaos Aletras"
      ],
      "abstract": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.",
      "pdf_url": "https://arxiv.org/pdf/2512.04844v1",
      "published": "2025-12-04T14:28:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04844v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders",
      "authors": [
        "Amy Winecoff",
        "Kevin Klyman"
      ],
      "abstract": "Generative AI systems may pose serious risks to individuals vulnerable to eating disorders. Existing safeguards tend to overlook subtle but clinically significant cues, leaving many risks unaddressed. To better understand the nature of these risks, we conducted semi-structured interviews with 15 clinicians, researchers, and advocates with expertise in eating disorders. Using abductive qualitative analysis, we developed an expert-guided taxonomy of generative AI risks across seven categories: (1) providing generalized health advice; (2) encouraging disordered behaviors; (3) supporting symptom concealment; (4) creating thinspiration; (5) reinforcing negative self-beliefs; (6) promoting excessive focus on the body; and (7) perpetuating narrow views about eating disorders. Our results demonstrate how certain user interactions with generative AI systems intersect with clinical features of eating disorders in ways that may intensify risk. We discuss implications of our work, including approaches for risk assessment, safeguard design, and participatory evaluation practices with domain experts.",
      "pdf_url": "https://arxiv.org/pdf/2512.04843v1",
      "published": "2025-12-04T14:27:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04843v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security",
      "authors": [
        "Wei Zhao",
        "Zhe Li",
        "Jun Sun"
      ],
      "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.\n  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\\% detection accuracy across multiple threat types.\n  By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.",
      "pdf_url": "https://arxiv.org/pdf/2512.04841v1",
      "published": "2025-12-04T14:25:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04841v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case",
      "authors": [
        "Vignesh Kumar Kembu",
        "Pierandrea Morandini",
        "Marta Bianca Maria Ranzini",
        "Antonino Nocera"
      ],
      "abstract": "Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.",
      "pdf_url": "https://arxiv.org/pdf/2512.04834v1",
      "published": "2025-12-04T14:17:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04834v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ]
    },
    {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "authors": [
        "Rasul Tutunov",
        "Alexandre Maraval",
        "Antoine Grosnit",
        "Xihan Li",
        "Jun Wang",
        "Haitham Bou-Ammar"
      ],
      "abstract": "Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.",
      "pdf_url": "https://arxiv.org/pdf/2512.04829v1",
      "published": "2025-12-04T14:11:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04829v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Enabling Ethical AI: A case study in using Ontological Context for Justified Agentic AI Decisions",
      "authors": [
        "Liam McGee",
        "James Harvey",
        "Lucy Cull",
        "Andreas Vermeulen",
        "Bart-Floris Visscher",
        "Malvika Sharan"
      ],
      "abstract": "In this preprint, we present A collaborative human-AI approach to building an inspectable semantic layer for Agentic AI. AI agents first propose candidate knowledge structures from diverse data sources; domain experts then validate, correct, and extend these structures, with their feedback used to improve subsequent models. Authors show how this process captures tacit institutional knowledge, improves response quality and efficiency, and mitigates institutional amnesia. We argue for a shift from post-hoc explanation to justifiable Agentic AI, where decisions are grounded in explicit, inspectable evidence and reasoning accessible to both experts and non-specialists.",
      "pdf_url": "https://arxiv.org/pdf/2512.04822v1",
      "published": "2025-12-04T14:06:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04822v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Setting up for failure: automatic discovery of the neural mechanisms of cognitive errors",
      "authors": [
        "Puria Radmard",
        "Paul M. Bays",
        "Máté Lengyel"
      ],
      "abstract": "Discovering the neural mechanisms underpinning cognition is one of the grand challenges of neuroscience. However, previous approaches for building models of RNN dynamics that explain behaviour required iterative refinement of architectures and/or optimisation objectives, resulting in a piecemeal, and mostly heuristic, human-in-the-loop process. Here, we offer an alternative approach that automates the discovery of viable RNN mechanisms by explicitly training RNNs to reproduce behaviour, including the same characteristic errors and suboptimalities, that humans and animals produce in a cognitive task. Achieving this required two main innovations. First, as the amount of behavioural data that can be collected in experiments is often too limited to train RNNs, we use a non-parametric generative model of behavioural responses to produce surrogate data for training RNNs. Second, to capture all relevant statistical aspects of the data, we developed a novel diffusion model-based approach for training RNNs. To showcase the potential of our approach, we chose a visual working memory task as our test-bed, as behaviour in this task is well known to produce response distributions that are patently multimodal (due to swap errors). The resulting network dynamics correctly qualitative features of macaque neural data. Importantly, these results were not possible to obtain with more traditional approaches, i.e., when only a limited set of behavioural signatures (rather than the full richness of behavioural response distributions) were fitted, or when RNNs were trained for task optimality (instead of reproducing behaviour). Our approach also yields novel predictions about the mechanism of swap errors, which can be readily tested in experiments. These results suggest that fitting RNNs to rich patterns of behaviour provides a powerful way to automatically discover mechanisms of important cognitive functions.",
      "pdf_url": "https://arxiv.org/pdf/2512.04808v1",
      "published": "2025-12-04T14:00:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04808v1",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ]
    },
    {
      "title": "287,872 Supermassive Black Holes Masses: Deep Learning Approaching Reverberation Mapping Accuracy",
      "authors": [
        "Yuhao Lu",
        "HengJian SiTu",
        "Jie Li",
        "Yixuan Li",
        "Yang Liu",
        "Wenbin Lin",
        "Yu Wang"
      ],
      "abstract": "We present a population-scale catalogue of 287,872 supermassive black hole masses with high accuracy. Using a deep encoder-decoder network trained on optical spectra with reverberation-mapping (RM) based labels of 849 quasars and applied to all SDSS quasars up to $z=4$, our method achieves a root-mean-square error of $0.058$\\,dex, a relative uncertainty of $\\approx 14\\%$, and coefficient of determination $R^{2}\\approx0.91$ with respect to RM-based masses, far surpassing traditional single-line virial estimators. Notably, the high accuracy is maintained for both low ($<10^{7.5}\\,M_\\odot$) and high ($>10^{9}\\,M_\\odot$) mass quasars, where empirical relations are unreliable.",
      "pdf_url": "https://arxiv.org/pdf/2512.04803v1",
      "published": "2025-12-04T13:55:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04803v1",
      "categories": [
        "astro-ph.GA",
        "astro-ph.HE",
        "astro-ph.IM",
        "cs.AI"
      ]
    },
    {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "authors": [
        "SIMA team",
        "Adrian Bolton",
        "Alexander Lerchner",
        "Alexandra Cordell",
        "Alexandre Moufarek",
        "Andrew Bolt",
        "Andrew Lampinen",
        "Anna Mitenkova",
        "Arne Olav Hallingstad",
        "Bojan Vujatovic",
        "Bonnie Li",
        "Cong Lu",
        "Daan Wierstra",
        "Daniel P. Sawyer",
        "Daniel Slater",
        "David Reichert",
        "Davide Vercelli",
        "Demis Hassabis",
        "Drew A. Hudson",
        "Duncan Williams",
        "Ed Hirst",
        "Fabio Pardo",
        "Felix Hill",
        "Frederic Besse",
        "Hannah Openshaw",
        "Harris Chan",
        "Hubert Soyer",
        "Jane X. Wang",
        "Jeff Clune",
        "John Agapiou",
        "John Reid",
        "Joseph Marino",
        "Junkyung Kim",
        "Karol Gregor",
        "Kaustubh Sridhar",
        "Kay McKinney",
        "Laura Kampis",
        "Lei M. Zhang",
        "Loic Matthey",
        "Luyu Wang",
        "Maria Abi Raad",
        "Maria Loks-Thompson",
        "Martin Engelcke",
        "Matija Kecman",
        "Matthew Jackson",
        "Maxime Gazeau",
        "Ollie Purkiss",
        "Oscar Knagg",
        "Peter Stys",
        "Piermaria Mendolicchio",
        "Raia Hadsell",
        "Rosemary Ke",
        "Ryan Faulkner",
        "Sarah Chakera",
        "Satinder Singh Baveja",
        "Shane Legg",
        "Sheleem Kashem",
        "Tayfun Terzi",
        "Thomas Keck",
        "Tim Harley",
        "Tim Scholtes",
        "Tyson Roberts",
        "Volodymyr Mnih",
        "Yulan Liu",
        "Zhengdong Wang",
        "Zoubin Ghahramani"
      ],
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "pdf_url": "https://arxiv.org/pdf/2512.04797v1",
      "published": "2025-12-04T13:46:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04797v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases",
      "authors": [
        "Gongyu Chen",
        "Xiaoyu Zhang",
        "Zhenqiang Weng",
        "Junjie Zheng",
        "Da Shen",
        "Chaofan Ding",
        "Wei-Qiang Zhang",
        "Zihao Chen"
      ],
      "abstract": "Singing voice conversion (SVC) aims to render the target singer's timbre while preserving melody and lyrics. However, existing zero-shot SVC systems remain fragile in real songs due to harmony interference, F0 errors, and the lack of inductive biases for singing. We propose YingMusic-SVC, a robust zero-shot framework that unifies continuous pre-training, robust supervised fine-tuning, and Flow-GRPO reinforcement learning. Our model introduces a singing-trained RVC timbre shifter for timbre-content disentanglement, an F0-aware timbre adaptor for dynamic vocal expression, and an energy-balanced rectified flow matching loss to enhance high-frequency fidelity. Experiments on a graded multi-track benchmark show that YingMusic-SVC achieves consistent improvements over strong open-source baselines in timbre similarity, intelligibility, and perceptual naturalness, especially under accompanied and harmony-contaminated conditions, demonstrating its effectiveness for real-world SVC deployment.",
      "pdf_url": "https://arxiv.org/pdf/2512.04793v1",
      "published": "2025-12-04T13:38:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04793v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications",
      "authors": [
        "Eranga Bandara",
        "Amin Hass",
        "Ross Gore",
        "Sachin Shetty",
        "Ravi Mukkamala",
        "Safdar H. Bouk",
        "Xueping Liang",
        "Ng Wee Keong",
        "Kasun De Zoysa",
        "Aruna Withanage",
        "Nilaan Loganathan"
      ],
      "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.",
      "pdf_url": "https://arxiv.org/pdf/2512.04785v1",
      "published": "2025-12-04T13:32:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04785v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance",
      "authors": [
        "Junjie Zheng",
        "Chunbo Hao",
        "Guobin Ma",
        "Xiaoyu Zhang",
        "Gongyu Chen",
        "Chaofan Ding",
        "Zihao Chen",
        "Lei Xie"
      ],
      "abstract": "Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints.",
      "pdf_url": "https://arxiv.org/pdf/2512.04779v1",
      "published": "2025-12-04T13:25:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04779v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions",
      "authors": [
        "Giorgos Polychronis",
        "Foivos Pournaropoulos",
        "Christos D. Antonopoulos",
        "Spyros Lalis"
      ],
      "abstract": "Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.",
      "pdf_url": "https://arxiv.org/pdf/2512.04773v1",
      "published": "2025-12-04T13:21:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04773v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Embodied Co-Design for Rapidly Evolving Agents: Taxonomy, Frontiers, and Challenges",
      "authors": [
        "Yuxing Wang",
        "Zhiyu Chen",
        "Tiantian Zhang",
        "Qiyue Yin",
        "Yongzhe Chang",
        "Zhiheng Li",
        "Liang Wang",
        "Xueqian Wang"
      ],
      "abstract": "Brain-body co-evolution enables animals to develop complex behaviors in their environments. Inspired by this biological synergy, embodied co-design (ECD) has emerged as a transformative paradigm for creating intelligent agents-from virtual creatures to physical robots-by jointly optimizing their morphologies and controllers rather than treating control in isolation. This integrated approach facilitates richer environmental interactions and robust task performance. In this survey, we provide a systematic overview of recent advances in ECD. We first formalize the concept of ECD and position it within related fields. We then introduce a hierarchical taxonomy: a lower layer that breaks down agent design into three fundamental components-controlling brain, body morphology, and task environment-and an upper layer that integrates these components into four major ECD frameworks: bi-level, single-level, generative, and open-ended. This taxonomy allows us to synthesize insights from more than one hundred recent studies. We further review notable benchmarks, datasets, and applications in both simulated and real-world scenarios. Finally, we identify significant challenges and offer insights into promising future research directions. A project associated with this survey has been created at https://github.com/Yuxing-Wang-THU/SurveyBrainBody.",
      "pdf_url": "https://arxiv.org/pdf/2512.04770v1",
      "published": "2025-12-04T13:12:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04770v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.ET",
        "eess.SY"
      ]
    },
    {
      "title": "Human Cognitive Biases in Explanation-Based Interaction: The Case of Within and Between Session Order Effect",
      "authors": [
        "Dario Pesenti",
        "Alessandro Bogani",
        "Katya Tentori",
        "Stefano Teso"
      ],
      "abstract": "Explanatory Interactive Learning (XIL) is a powerful interactive learning framework designed to enable users to customize and correct AI models by interacting with their explanations. In a nutshell, XIL algorithms select a number of items on which an AI model made a decision (e.g. images and their tags) and present them to users, together with corresponding explanations (e.g. image regions that drive the model's decision). Then, users supply corrective feedback for the explanations, which the algorithm uses to improve the model. Despite showing promise in debugging tasks, recent studies have raised concerns that explanatory interaction may trigger order effects, a well-known cognitive bias in which the sequence of presented items influences users' trust and, critically, the quality of their feedback. We argue that these studies are not entirely conclusive, as the experimental designs and tasks employed differ substantially from common XIL use cases, complicating interpretation. To clarify the interplay between order effects and explanatory interaction, we ran two larger-scale user studies (n = 713 total) designed to mimic common XIL tasks. Specifically, we assessed order effects both within and between debugging sessions by manipulating the order in which correct and wrong explanations are presented to participants. Order effects had a limited, through significant impact on users' agreement with the model (i.e., a behavioral measure of their trust), and only when examined withing debugging sessions, not between them. The quality of users' feedback was generally satisfactory, with order effects exerting only a small and inconsistent influence in both experiments. Overall, our findings suggest that order effects do not pose a significant issue for the successful employment of XIL approaches. More broadly, our work contributes to the ongoing efforts for understanding human factors in AI.",
      "pdf_url": "https://arxiv.org/pdf/2512.04764v1",
      "published": "2025-12-04T12:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04764v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "UnwrapDiff: Conditional Diffusion for Robust InSAR Phase Unwrapping",
      "authors": [
        "Yijia Song",
        "Juliet Biggs",
        "Alin Achim",
        "Robert Popescu",
        "Simon Orrego",
        "Nantheera Anantrasirichai"
      ],
      "abstract": "Phase unwrapping is a fundamental problem in InSAR data processing, supporting geophysical applications such as deformation monitoring and hazard assessment. Its reliability is limited by noise and decorrelation in radar acquisitions, which makes accurate reconstruction of the deformation signal challenging. We propose a denoising diffusion probabilistic model (DDPM)-based framework for InSAR phase unwrapping, UnwrapDiff, in which the output of the traditional minimum cost flow algorithm (SNAPHU) is incorporated as conditional guidance. To evaluate robustness, we construct a synthetic dataset that incorporates atmospheric effects and diverse noise patterns, representative of realistic InSAR observations. Experiments show that the proposed model leverages the conditional prior while reducing the effect of diverse noise patterns, achieving on average a 10.11\\% reduction in NRMSE compared to SNAPHU. It also achieves better reconstruction quality in difficult cases such as dyke intrusions.",
      "pdf_url": "https://arxiv.org/pdf/2512.04749v1",
      "published": "2025-12-04T12:38:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.04749v1",
      "categories": [
        "physics.geo-ph",
        "cs.AI"
      ]
    }
  ]
}