{
  "last_updated": "2025-09-23T00:46:50.987566",
  "papers": [
    {
      "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation",
      "authors": [
        "Jane Luo",
        "Xin Zhang",
        "Steven Liu",
        "Jie Wu",
        "Yiming Huang",
        "Yangyu Huang",
        "Chengyu Yin",
        "Ying Xin",
        "Jianfeng Liu",
        "Yuefeng Zhan",
        "Hao Sun",
        "Qi Chen",
        "Scarlett Li",
        "Mao Yang"
      ],
      "abstract": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.",
      "pdf_url": "http://arxiv.org/pdf/2509.16198v1",
      "published": "2025-09-19T17:58:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16198v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation",
      "authors": [
        "Luca Della Libera",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "abstract": "Neural audio codecs are a fundamental component of modern generative audio\npipelines. Although recent codecs achieve strong low-bitrate reconstruction and\nprovide powerful representations for downstream tasks, most are non-streamable,\nlimiting their use in real-time applications. We present FocalCodec-Stream, a\nhybrid codec based on focal modulation that compresses speech into a single\nbinary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our\napproach combines multi-stage causal distillation of WavLM with targeted\narchitectural improvements, including a lightweight refiner module that\nenhances quality under latency constraints. Experiments show that\nFocalCodec-Stream outperforms existing streamable codecs at comparable\nbitrates, while preserving both semantic and acoustic information. The result\nis a favorable trade-off between reconstruction quality, downstream task\nperformance, latency, and efficiency. Code and checkpoints will be released at\nhttps://github.com/lucadellalib/focalcodec.",
      "pdf_url": "http://arxiv.org/pdf/2509.16195v1",
      "published": "2025-09-19T17:57:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16195v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs",
      "authors": [
        "Jinghao Zhang",
        "Sihang Jiang",
        "Shiwei Guo",
        "Shisong Chen",
        "Yanghua Xiao",
        "Hongwei Feng",
        "Jiaqing Liang",
        "Minggui HE",
        "Shimin Tao",
        "Hongxia Ma"
      ],
      "abstract": "As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture",
      "pdf_url": "http://arxiv.org/pdf/2509.16188v1",
      "published": "2025-09-19T17:47:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16188v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Accelerating Atomic Fine Structure Determination with Graph Reinforcement Learning",
      "authors": [
        "M. Ding",
        "V. -A. Darvariu",
        "A. N. Ryabtsev",
        "N. Hawes",
        "J. C. Pickering"
      ],
      "abstract": "Atomic data determined by analysis of observed atomic spectra are essential\nfor plasma diagnostics. For each low-ionisation open d- and f-subshell atomic\nspecies, around $10^3$ fine structure level energies can be determined through\nyears of analysis of $10^4$ observable spectral lines. We propose the\nautomation of this task by casting the analysis procedure as a Markov decision\nprocess and solving it by graph reinforcement learning using reward functions\nlearned on historical human decisions. In our evaluations on existing spectral\nline lists and theoretical calculations for Co II and Nd II-III, hundreds of\nlevel energies were computed within hours, agreeing with published values in\n95% of cases for Co II and 54-87% for Nd II-III. As the current efficiency in\natomic fine structure determination struggles to meet growing atomic data\ndemands from astronomy and fusion science, our new artificial intelligence\napproach sets the stage for closing this gap.",
      "pdf_url": "http://arxiv.org/pdf/2509.16184v1",
      "published": "2025-09-19T17:44:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16184v1",
      "categories": [
        "physics.atom-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Fast OTSU Thresholding Using Bisection Method",
      "authors": [
        "Sai Varun Kodathala"
      ],
      "abstract": "The Otsu thresholding algorithm represents a fundamental technique in image\nsegmentation, yet its computational efficiency is severely limited by\nexhaustive search requirements across all possible threshold values. This work\npresents an optimized implementation that leverages the bisection method to\nexploit the unimodal characteristics of the between-class variance function.\nOur approach reduces the computational complexity from O(L) to O(log L)\nevaluations while preserving segmentation accuracy. Experimental validation on\n48 standard test images demonstrates a 91.63% reduction in variance\ncomputations and 97.21% reduction in algorithmic iterations compared to\nconventional exhaustive search. The bisection method achieves exact threshold\nmatches in 66.67% of test cases, with 95.83% exhibiting deviations within 5\ngray levels. The algorithm maintains universal convergence within theoretical\nlogarithmic bounds while providing deterministic performance guarantees\nsuitable for real-time applications. This optimization addresses critical\ncomputational bottlenecks in large-scale image processing systems without\ncompromising the theoretical foundations or segmentation quality of the\noriginal Otsu method.",
      "pdf_url": "http://arxiv.org/pdf/2509.16179v1",
      "published": "2025-09-19T17:40:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16179v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ]
    },
    {
      "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks",
      "authors": [
        "Het Patel",
        "Muzammil Allie",
        "Qian Zhang",
        "Jia Chen",
        "Evangelos E. Papalexakis"
      ],
      "abstract": "Vision language models (VLMs) excel in multimodal understanding but are prone\nto adversarial attacks. Existing defenses often demand costly retraining or\nsignificant architecture changes. We introduce a lightweight defense using\ntensor decomposition suitable for any pre-trained VLM, requiring no retraining.\nBy decomposing and reconstructing vision encoder representations, it filters\nadversarial noise while preserving meaning. Experiments with CLIP on COCO and\nFlickr30K show improved robustness. On Flickr30K, it restores 12.3\\%\nperformance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On\nCOCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%.\nAnalysis shows Tensor Train decomposition with low rank (8-32) and low residual\nstrength ($\\alpha=0.1-0.2$) is optimal. This method is a practical,\nplug-and-play solution with minimal overhead for existing VLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.16163v1",
      "published": "2025-09-19T17:16:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16163v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Network-Based Detection of Autism Spectrum Disorder Using Sustainable and Non-invasive Salivary Biomarkers",
      "authors": [
        "Janayna M. Fernandes",
        "Robinson Sabino-Silva",
        "Murillo G. Carneiro"
      ],
      "abstract": "Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying\nearly diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy,\nwe developed GANet, a genetic algorithm-based network optimization framework\nleveraging PageRank and Degree for importance-based feature characterization.\nGANet systematically optimizes network structure to extract meaningful patterns\nfrom high-dimensional spectral data. It achieved superior performance compared\nto linear discriminant analysis, support vector machines, and deep learning\nmodels, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74\nharmonic mean. These results demonstrate GANet's potential as a robust,\nbio-inspired, non-invasive tool for precise ASD detection and broader\nspectral-based health applications.",
      "pdf_url": "http://arxiv.org/pdf/2509.16126v1",
      "published": "2025-09-19T16:24:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16126v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
      "authors": [
        "Kaiwen Zheng",
        "Huayu Chen",
        "Haotian Ye",
        "Haoxiang Wang",
        "Qinsheng Zhang",
        "Kai Jiang",
        "Hang Su",
        "Stefano Ermon",
        "Jun Zhu",
        "Ming-Yu Liu"
      ],
      "abstract": "Online reinforcement learning (RL) has been central to post-training language\nmodels, but its extension to diffusion models remains challenging due to\nintractable likelihoods. Recent works discretize the reverse sampling process\nto enable GRPO-style training, yet they inherit fundamental drawbacks,\nincluding solver restrictions, forward-reverse inconsistency, and complicated\nintegration with classifier-free guidance (CFG). We introduce Diffusion\nNegative-aware FineTuning (DiffusionNFT), a new online RL paradigm that\noptimizes diffusion models directly on the forward process via flow matching.\nDiffusionNFT contrasts positive and negative generations to define an implicit\npolicy improvement direction, naturally incorporating reinforcement signals\ninto the supervised learning objective. This formulation enables training with\narbitrary black-box solvers, eliminates the need for likelihood estimation, and\nrequires only clean images rather than sampling trajectories for policy\noptimization. DiffusionNFT is up to $25\\times$ more efficient than FlowGRPO in\nhead-to-head comparisons, while being CFG-free. For instance, DiffusionNFT\nimproves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO\nachieves 0.95 with over 5k steps and additional CFG employment. By leveraging\nmultiple reward models, DiffusionNFT significantly boosts the performance of\nSD3.5-Medium in every benchmark tested.",
      "pdf_url": "http://arxiv.org/pdf/2509.16117v1",
      "published": "2025-09-19T16:09:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16117v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses",
      "authors": [
        "Fangyi Yu",
        "Nabeel Seedat",
        "Dasha Herrmannova",
        "Frank Schilder",
        "Jonathan Richard Schwarz"
      ],
      "abstract": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains.",
      "pdf_url": "http://arxiv.org/pdf/2509.16093v1",
      "published": "2025-09-19T15:36:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16093v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model",
      "authors": [
        "Pengteng Li",
        "Pinhao Song",
        "Wuyang Li",
        "Weiyu Guo",
        "Huizai Yao",
        "Yijie Xu",
        "Dugang Liu",
        "Hui Xiong"
      ],
      "abstract": "We introduce SEE&TREK, the first training-free prompting framework tailored\nto enhance the spatial understanding of Multimodal Large Language Models\n(MLLMS) under vision-only constraints. While prior efforts have incorporated\nmodalities like depth or point clouds to improve spatial reasoning, purely\nvisualspatial understanding remains underexplored. SEE&TREK addresses this gap\nby focusing on two core principles: increasing visual diversity and motion\nreconstruction. For visual diversity, we conduct Maximum Semantic Richness\nSampling, which employs an off-the-shell perception model to extract\nsemantically rich keyframes that capture scene structure. For motion\nreconstruction, we simulate visual trajectories and encode relative spatial\npositions into keyframes to preserve both spatial relations and temporal\ncoherence. Our method is training&GPU-free, requiring only a single forward\npass, and can be seamlessly integrated into existing MLLM'S. Extensive\nexperiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently\nboosts various MLLM S performance across diverse spatial reasoning tasks with\nthe most +3.5% improvement, offering a promising path toward stronger spatial\nintelligence.",
      "pdf_url": "http://arxiv.org/pdf/2509.16087v1",
      "published": "2025-09-19T15:30:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16087v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Communications to Circulations: 3D Wind Field Retrieval and Real-Time Prediction Using 5G GNSS Signals and Deep Learning",
      "authors": [
        "Yuchen Ye",
        "Hong Liang",
        "Chaoxia Yuan",
        "Mingyu Li",
        "Aoqi Zhou",
        "Chunqing Shang",
        "Hua Cai",
        "Peixi Liu",
        "Kezuan Wang",
        "Yifeng Zheng"
      ],
      "abstract": "Accurate atmospheric wind field information is crucial for various\napplications, including weather forecasting, aviation safety, and disaster risk\nreduction. However, obtaining high spatiotemporal resolution wind data remains\nchallenging due to limitations in traditional in-situ observations and remote\nsensing techniques, as well as the computational expense and biases of\nnumerical weather prediction (NWP) models. This paper introduces G-WindCast, a\nnovel deep learning framework that leverages signal strength variations from 5G\nGlobal Navigation Satellite System (GNSS) signals to retrieve and forecast\nthree-dimensional (3D) atmospheric wind fields. The framework utilizes Forward\nNeural Networks (FNN) and Transformer networks to capture complex, nonlinear,\nand spatiotemporal relationships between GNSS-derived features and wind\ndynamics. Our preliminary results demonstrate promising accuracy in both wind\nretrieval and short-term wind forecasting (up to 30 minutes lead time), with\nskill scores comparable to high-resolution NWP outputs in certain scenarios.\nThe model exhibits robustness across different forecast horizons and pressure\nlevels, and its predictions for wind speed and direction show superior\nagreement with observations compared to concurrent ERA5 reanalysis data.\nFurthermore, we show that the system can maintain excellent performance for\nlocalized forecasting even with a significantly reduced number of GNSS stations\n(e.g., around 100), highlighting its cost-effectiveness and scalability. This\ninterdisciplinary approach underscores the transformative potential of\nexploiting non-traditional data sources and deep learning for advanced\nenvironmental monitoring and real-time atmospheric applications.",
      "pdf_url": "http://arxiv.org/pdf/2509.16068v1",
      "published": "2025-09-19T15:17:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16068v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T07",
        "I.2.1"
      ]
    },
    {
      "title": "Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers",
      "authors": [
        "Krati Saxena",
        "Federico Jurado Ruiz",
        "Guido Manzi",
        "Dianbo Liu",
        "Alex Lamb"
      ],
      "abstract": "Attention mechanisms have become integral in AI, significantly enhancing\nmodel performance and scalability by drawing inspiration from human cognition.\nConcurrently, the Attention Schema Theory (AST) in cognitive science posits\nthat individuals manage their attention by creating a model of the attention\nitself, effectively allocating cognitive resources. Inspired by AST, we\nintroduce ASAC (Attention Schema-based Attention Control), which integrates the\nattention schema concept into artificial neural networks. Our initial\nexperiments focused on embedding the ASAC module within transformer\narchitectures. This module employs a Vector-Quantized Variational AutoEncoder\n(VQVAE) as both an attention abstractor and controller, facilitating precise\nattention management. By explicitly modeling attention allocation, our approach\naims to enhance system efficiency. We demonstrate ASAC's effectiveness in both\nthe vision and NLP domains, highlighting its ability to improve classification\naccuracy and expedite the learning process. Our experiments with vision\ntransformers across various datasets illustrate that the attention controller\nnot only boosts classification accuracy but also accelerates learning.\nFurthermore, we have demonstrated the model's robustness and generalization\ncapabilities across noisy and out-of-distribution datasets. In addition, we\nhave showcased improved performance in multi-task settings. Quick experiments\nreveal that the attention schema-based module enhances resilience to\nadversarial attacks, optimizes attention to improve learning efficiency, and\nfacilitates effective transfer learning and learning from fewer examples. These\npromising results establish a connection between cognitive science and machine\nlearning, shedding light on the efficient utilization of attention mechanisms\nin AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.16058v1",
      "published": "2025-09-19T15:08:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16058v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Compose by Focus: Scene Graph-based Atomic Skills",
      "authors": [
        "Han Qi",
        "Changhe Chen",
        "Heng Yang"
      ],
      "abstract": "A key requirement for generalist robots is compositional generalization - the\nability to combine atomic skills to solve complex, long-horizon tasks. While\nprior work has primarily focused on synthesizing a planner that sequences\npre-learned skills, robust execution of the individual skills themselves\nremains challenging, as visuomotor policies often fail under distribution\nshifts induced by scene composition. To address this, we introduce a scene\ngraph-based representation that focuses on task-relevant objects and relations,\nthereby mitigating sensitivity to irrelevant variation. Building on this idea,\nwe develop a scene-graph skill learning framework that integrates graph neural\nnetworks with diffusion-based imitation learning, and further combine \"focused\"\nscene-graph skills with a vision-language model (VLM) based task planner.\nExperiments in both simulation and real-world manipulation tasks demonstrate\nsubstantially higher success rates than state-of-the-art baselines,\nhighlighting improved robustness and compositional generalization in\nlong-horizon tasks.",
      "pdf_url": "http://arxiv.org/pdf/2509.16053v1",
      "published": "2025-09-19T15:03:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16053v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech",
      "authors": [
        "Sang Hoon Woo",
        "Sehun Lee",
        "Kang-wook Kim",
        "Gunhee Kim"
      ],
      "abstract": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT",
      "pdf_url": "http://arxiv.org/pdf/2509.16028v1",
      "published": "2025-09-19T14:34:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16028v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning",
      "authors": [
        "Hong-Yun Lin",
        "Jhen-Ke Lin",
        "Chung-Chun Wang",
        "Hao-Chien Lu",
        "Berlin Chen"
      ],
      "abstract": "Spoken Language Assessment (SLA) estimates a learner's oral proficiency from\nspontaneous speech. The growing population of L2 English speakers has\nintensified the demand for reliable SLA, a critical component of Computer\nAssisted Language Learning (CALL). Existing efforts often rely on cascaded\npipelines, which are prone to error propagation, or end-to-end models that\noften operate on a short audio window, which might miss discourse-level\nevidence. This paper introduces a novel multimodal foundation model approach\nthat performs session-level evaluation in a single pass. Our approach couples\nmulti-target learning with a frozen, Whisper ASR model-based speech prior for\nacoustic-aware calibration, allowing for jointly learning holistic and\ntrait-level objectives of SLA without resorting to handcrafted features. By\ncoherently processing the entire response session of an L2 speaker, the model\nexcels at predicting holistic oral proficiency. Experiments conducted on the\nSpeak & Improve benchmark demonstrate that our proposed approach outperforms\nthe previous state-of-the-art cascaded system and exhibits robust cross-part\ngeneralization, producing a compact deployable grader that is tailored for CALL\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2509.16025v1",
      "published": "2025-09-19T14:33:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16025v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "AI Methods for Permutation Circuit Synthesis Across Generic Topologies",
      "authors": [
        "Victor Villar",
        "Juan Cruz-Benito",
        "Ismael Faro",
        "David Kremer"
      ],
      "abstract": "This paper investigates artificial intelligence (AI) methodologies for the\nsynthesis and transpilation of permutation circuits across generic topologies.\nOur approach uses Reinforcement Learning (RL) techniques to achieve\nnear-optimal synthesis of permutation circuits up to 25 qubits. Rather than\ndeveloping specialized models for individual topologies, we train a\nfoundational model on a generic rectangular lattice, and employ masking\nmechanisms to dynamically select subsets of topologies during the synthesis.\nThis enables the synthesis of permutation circuits on any topology that can be\nembedded within the rectangular lattice, without the need to re-train the\nmodel. In this paper we show results for 5x5 lattice and compare them to\nprevious AI topology-oriented models and classical methods, showing that they\noutperform classical heuristics, and match previous specialized AI models, and\nperforms synthesis even for topologies that were not seen during training. We\nfurther show that the model can be fine tuned to strengthen the performance for\nselected topologies of interest. This methodology allows a single trained model\nto efficiently synthesize circuits across diverse topologies, allowing its\npractical integration into transpilation workflows.",
      "pdf_url": "http://arxiv.org/pdf/2509.16020v1",
      "published": "2025-09-19T14:28:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16020v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation",
      "authors": [
        "Qi Wang",
        "Shituo Ma",
        "Guoxin Yu",
        "Hanyang Peng",
        "Yue Yu"
      ],
      "abstract": "Voice cloning for Text-to-Speech (TTS) aims to generate expressive and\npersonalized speech from text using limited data from a target speaker.\nFederated Learning (FL) offers a collaborative and privacy-preserving framework\nfor this task, but existing approaches suffer from high communication costs and\ntend to suppress stylistic heterogeneity, resulting in insufficient\npersonalization. To address these issues, we propose Fed-PISA, which stands for\nFederated Personalized Identity-Style Adaptation. To minimize communication\ncosts, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism:\nthe speaker's timbre is retained locally through a private ID-LoRA, while only\na lightweight style-LoRA is transmitted to the server, thereby minimizing\nparameter exchange. To harness heterogeneity, our aggregation method, inspired\nby collaborative filtering, is introduced to create custom models for each\nclient by learning from stylistically similar peers. Experiments show that\nFed-PISA improves style expressivity, naturalness, and speaker similarity,\noutperforming standard federated baselines with minimal communication costs.",
      "pdf_url": "http://arxiv.org/pdf/2509.16010v1",
      "published": "2025-09-19T14:24:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.16010v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Towards Sharper Object Boundaries in Self-Supervised Depth Estimation",
      "authors": [
        "Aurélien Cecille",
        "Stefan Duffner",
        "Franck Davoine",
        "Rémi Agier",
        "Thibault Neveu"
      ],
      "abstract": "Accurate monocular depth estimation is crucial for 3D scene understanding,\nbut existing methods often blur depth at object boundaries, introducing\nspurious intermediate 3D points. While achieving sharp edges usually requires\nvery fine-grained supervision, our method produces crisp depth discontinuities\nusing only self-supervision. Specifically, we model per-pixel depth as a\nmixture distribution, capturing multiple plausible depths and shifting\nuncertainty from direct regression to the mixture weights. This formulation\nintegrates seamlessly into existing pipelines via variance-aware loss functions\nand uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show\nthat our method achieves up to 35% higher boundary sharpness and improves point\ncloud quality compared to state-of-the-art baselines.",
      "pdf_url": "http://arxiv.org/pdf/2509.15987v1",
      "published": "2025-09-19T13:53:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15987v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions",
      "authors": [
        "Xinchen Wan",
        "Jinhua Liang",
        "Huan Zhang"
      ],
      "abstract": "Existing digital mental wellness tools often overlook the nuanced emotional\nstates underlying everyday challenges. For example, pre-sleep anxiety affects\nmore than 1.5 billion people worldwide, yet current approaches remain largely\nstatic and \"one-size-fits-all\", failing to adapt to individual needs. In this\nwork, we present EmoHeal, an end-to-end system that delivers personalized,\nthree-stage supportive narratives. EmoHeal detects 27 fine-grained emotions\nfrom user text with a fine-tuned XLM-RoBERTa model, mapping them to musical\nparameters via a knowledge graph grounded in music therapy principles (GEMS,\niso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to\nguide users from their current state toward a calmer one\n(\"match-guide-target\"). A within-subjects study (N=40) demonstrated significant\nsupportive effects, with participants reporting substantial mood improvement\n(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,\np<0.001). A strong correlation between perceived accuracy and therapeutic\noutcome (r=0.72, p<0.001) validates our fine-grained approach. These findings\nestablish the viability of theory-driven, emotion-aware digital wellness tools\nand provides a scalable AI blueprint for operationalizing music therapy\nprinciples.",
      "pdf_url": "http://arxiv.org/pdf/2509.15986v1",
      "published": "2025-09-19T13:52:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15986v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations",
      "authors": [
        "Yujie Zhu",
        "Charles A. Hepburn",
        "Matthew Thorpe",
        "Giovanni Montana"
      ],
      "abstract": "In reinforcement learning with sparse rewards, demonstrations can accelerate\nlearning, but determining when to imitate them remains challenging. We propose\nSmooth Policy Regularisation from Demonstrations (SPReD), a framework that\naddresses the fundamental question: when should an agent imitate a\ndemonstration versus follow its own policy? SPReD uses ensemble methods to\nexplicitly model Q-value distributions for both demonstration and policy\nactions, quantifying uncertainty for comparisons. We develop two complementary\nuncertainty-aware methods: a probabilistic approach estimating the likelihood\nof demonstration superiority, and an advantage-based approach scaling imitation\nby statistical significance. Unlike prevailing methods (e.g. Q-filter) that\nmake binary imitation decisions, SPReD applies continuous,\nuncertainty-proportional regularisation weights, reducing gradient variance\nduring training. Despite its computational simplicity, SPReD achieves\nremarkable gains in experiments across eight robotics tasks, outperforming\nexisting approaches by up to a factor of 14 in complex tasks while maintaining\nrobustness to demonstration quality and quantity. Our code is available at\nhttps://github.com/YujieZhu7/SPReD.",
      "pdf_url": "http://arxiv.org/pdf/2509.15981v1",
      "published": "2025-09-19T13:47:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15981v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ]
    },
    {
      "title": "Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation",
      "authors": [
        "Lorenzo Cirillo",
        "Claudio Schiavella",
        "Lorenzo Papa",
        "Paolo Russo",
        "Irene Amerini"
      ],
      "abstract": "Explainable artificial intelligence is increasingly employed to understand\nthe decision-making process of deep learning models and create trustworthiness\nin their adoption. However, the explainability of Monocular Depth Estimation\n(MDE) remains largely unexplored despite its wide deployment in real-world\napplications. In this work, we study how to analyze MDE networks to map the\ninput image to the predicted depth map. More in detail, we investigate\nwell-established feature attribution methods, Saliency Maps, Integrated\nGradients, and Attention Rollout on different computationally complex models\nfor MDE: METER, a lightweight network, and PixelFormer, a deep network. We\nassess the quality of the generated visual explanations by selectively\nperturbing the most relevant and irrelevant pixels, as identified by the\nexplainability methods, and analyzing the impact of these perturbations on the\nmodel's output. Moreover, since existing evaluation metrics can have some\nlimitations in measuring the validity of visual explanations for MDE, we\nadditionally introduce the Attribution Fidelity. This metric evaluates the\nreliability of the feature attribution by assessing their consistency with the\npredicted depth map. Experimental results demonstrate that Saliency Maps and\nIntegrated Gradients have good performance in highlighting the most important\ninput features for MDE lightweight and deep models, respectively. Furthermore,\nwe show that Attribution Fidelity effectively identifies whether an\nexplainability method fails to produce reliable visual maps, even in scenarios\nwhere conventional metrics might suggest satisfactory results.",
      "pdf_url": "http://arxiv.org/pdf/2509.15980v1",
      "published": "2025-09-19T13:45:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15980v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "BEFT: Bias-Efficient Fine-Tuning of Language Models",
      "authors": [
        "Baichuan Huang",
        "Ananth Balashankar",
        "Amir Aminifar"
      ],
      "abstract": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks.",
      "pdf_url": "http://arxiv.org/pdf/2509.15974v1",
      "published": "2025-09-19T13:35:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15974v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation",
      "authors": [
        "Chao Yu",
        "Yuanqing Wang",
        "Zhen Guo",
        "Hao Lin",
        "Si Xu",
        "Hongzhi Zang",
        "Quanlu Zhang",
        "Yongji Wu",
        "Chunyang Zhu",
        "Junhao Hu",
        "Zixiao Huang",
        "Mingjie Wei",
        "Yuqing Xie",
        "Ke Yang",
        "Bo Dai",
        "Zhexuan Xu",
        "Xiangyuan Wang",
        "Xu Fu",
        "Zhihao Liu",
        "Kang Chen",
        "Weilin Liu",
        "Gang Liu",
        "Boxun Li",
        "Jianlei Yang",
        "Zhi Yang",
        "Guohao Dai",
        "Yu Wang"
      ],
      "abstract": "Reinforcement learning (RL) has demonstrated immense potential in advancing\nartificial general intelligence, agentic intelligence, and embodied\nintelligence. However, the inherent heterogeneity and dynamicity of RL\nworkflows often lead to low hardware utilization and slow training on existing\nsystems. In this paper, we present RLinf, a high-performance RL training system\nbased on our key observation that the major roadblock to efficient RL training\nlies in system flexibility. To maximize flexibility and efficiency, RLinf is\nbuilt atop a novel RL system design paradigm called macro-to-micro flow\ntransformation (M2Flow), which automatically breaks down high-level,\neasy-to-compose RL workflows at both the temporal and spatial dimensions, and\nrecomposes them into optimized execution flows. Supported by RLinf worker's\nadaptive communication capability, we devise context switching and elastic\npipelining to realize M2Flow transformation, and a profiling-guided scheduling\npolicy to generate optimal execution plans. Extensive evaluations on both\nreasoning RL and embodied RL tasks demonstrate that RLinf consistently\noutperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in\nend-to-end training throughput.",
      "pdf_url": "http://arxiv.org/pdf/2509.15965v1",
      "published": "2025-09-19T13:24:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15965v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework",
      "authors": [
        "Tianyu Li",
        "Yan Xin",
        "Jianzhong",
        "Zhang"
      ],
      "abstract": "Reliable channel estimation (CE) is fundamental for robust communication in\ndynamic wireless environments, where models must generalize across varying\nconditions such as signal-to-noise ratios (SNRs), the number of resource blocks\n(RBs), and channel profiles. Traditional deep learning (DL)-based methods\nstruggle to generalize effectively across such diverse settings, particularly\nunder multitask and zero-shot scenarios. In this work, we propose MoE-CE, a\nflexible mixture-of-experts (MoE) framework designed to enhance the\ngeneralization capability of DL-based CE methods. MoE-CE provides an\nappropriate inductive bias by leveraging multiple expert subnetworks, each\nspecialized in distinct channel characteristics, and a learned router that\ndynamically selects the most relevant experts per input. This architecture\nenhances model capacity and adaptability without a proportional rise in\ncomputational cost while being agnostic to the choice of the backbone model and\nthe learning algorithm. Through extensive experiments on synthetic datasets\ngenerated under diverse SNRs, RB numbers, and channel profiles, including\nmultitask and zero-shot evaluations, we demonstrate that MoE-CE consistently\noutperforms conventional DL approaches, achieving significant performance gains\nwhile maintaining efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2509.15964v1",
      "published": "2025-09-19T13:23:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15964v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Structured Information for Improving Spatial Relationships in Text-to-Image Generation",
      "authors": [
        "Sander Schildermans",
        "Chang Tian",
        "Ying Jiao",
        "Marie-Francine Moens"
      ],
      "abstract": "Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing\nspatial relationships described in natural language prompts remains a major\nchallenge. Prior efforts have addressed this issue through prompt optimization,\nspatially grounded generation, and semantic refinement. This work introduces a\nlightweight approach that augments prompts with tuple-based structured\ninformation, using a fine-tuned language model for automatic conversion and\nseamless integration into T2I pipelines. Experimental results demonstrate\nsubstantial improvements in spatial accuracy, without compromising overall\nimage quality as measured by Inception Score. Furthermore, the automatically\ngenerated tuples exhibit quality comparable to human-crafted tuples. This\nstructured information provides a practical and portable solution to enhance\nspatial relationships in T2I generation, addressing a key limitation of current\nlarge-scale generative systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.15962v1",
      "published": "2025-09-19T13:20:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15962v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration",
      "authors": [
        "Zhuoyue Zhang",
        "Haitong Xu"
      ],
      "abstract": "Autonomous navigation in maritime domains is accelerating alongside advances\nin artificial intelligence, sensing, and connectivity. Opaque decision-making\nand poorly calibrated human-automation interaction remain key barriers to safe\nadoption. This article synthesizes 100 studies on automation transparency for\nMaritime Autonomous Surface Ships (MASS) spanning situation awareness (SA),\nhuman factors, interface design, and regulation. We (i) map the\nGuidance-Navigation-Control stack to shore-based operational modes -- remote\nsupervision (RSM) and remote control (RCM) -- and identify where human unsafe\ncontrol actions (Human-UCAs) concentrate in handover and emergency loops; (ii)\nsummarize evidence that transparency features (decision rationales,\nalternatives, confidence/uncertainty, and rule-compliance indicators) improve\nunderstanding and support trust calibration, though reliability and\npredictability often dominate trust; (iii) distill design strategies for\ntransparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI\npresentation (textual/graphical overlays, color coding, conversational and\nimmersive UIs), and engineer-facing processes (resilient interaction design,\nvalidation, and standardization). We integrate methods for Human-UCA\nidentification (STPA-Cog + IDAC), quantitative trust/SA assessment, and\noperator workload monitoring, and outline regulatory and rule-based\nimplications including COLREGs formalization and route exchange. We conclude\nwith an adaptive transparency framework that couples operator state estimation\nwith explainable decision support to reduce cognitive overload and improve\ntakeover timeliness. The review highlights actionable figure-of-merit displays\n(e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs\n(rule traceability, confidence), and training pipelines (HIL/MIL, simulation)\nas near-term levers for safer MASS operations.",
      "pdf_url": "http://arxiv.org/pdf/2509.15959v1",
      "published": "2025-09-19T13:18:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15959v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol",
      "authors": [
        "Kanato Masayoshi",
        "Masahiro Hashimoto",
        "Ryoichi Yokoyama",
        "Naoki Toda",
        "Yoshifumi Uwamino",
        "Shogo Fukuda",
        "Ho Namkoong",
        "Masahiro Jinzaki"
      ],
      "abstract": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.",
      "pdf_url": "http://arxiv.org/pdf/2509.15957v1",
      "published": "2025-09-19T13:17:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15957v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.IR"
      ]
    },
    {
      "title": "Compose Yourself: Average-Velocity Flow Matching for One-Step Speech Enhancement",
      "authors": [
        "Gang Yang",
        "Yue Lei",
        "Wenxin Tai",
        "Jin Wu",
        "Jia Chen",
        "Ting Zhong",
        "Fan Zhou"
      ],
      "abstract": "Diffusion and flow matching (FM) models have achieved remarkable progress in\nspeech enhancement (SE), yet their dependence on multi-step generation is\ncomputationally expensive and vulnerable to discretization errors. Recent\nadvances in one-step generative modeling, particularly MeanFlow, provide a\npromising alternative by reformulating dynamics through average velocity\nfields. In this work, we present COSE, a one-step FM framework tailored for SE.\nTo address the high training overhead of Jacobian-vector product (JVP)\ncomputations in MeanFlow, we introduce a velocity composition identity to\ncompute average velocity efficiently, eliminating expensive computation while\npreserving theoretical consistency and achieving competitive enhancement\nquality. Extensive experiments on standard benchmarks show that COSE delivers\nup to 5x faster sampling and reduces training cost by 40%, all without\ncompromising speech quality. Code is available at\nhttps://github.com/ICDM-UESTC/COSE.",
      "pdf_url": "http://arxiv.org/pdf/2509.15952v1",
      "published": "2025-09-19T13:07:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15952v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching",
      "authors": [
        "Graham Clyne",
        "Guillaume Couairon",
        "Guillaume Gastineau",
        "Claire Monteleoni",
        "Anastase Charantonis"
      ],
      "abstract": "Climate projections have uncertainties related to components of the climate\nsystem and their interactions. A typical approach to quantifying these\nuncertainties is to use climate models to create ensembles of repeated\nsimulations under different initial conditions. Due to the complexity of these\nsimulations, generating such ensembles of projections is computationally\nexpensive. In this work, we present ArchesClimate, a deep learning-based\nclimate model emulator that aims to reduce this cost. ArchesClimate is trained\non decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution\nof approximately 2.5x1.25 degrees. We train a flow matching model following\nArchesWeatherGen, which we adapt to predict near-term climate. Once trained,\nthe model generates states at a one-month lead time and can be used to\nauto-regressively emulate climate model simulations of any length. We show that\nfor up to 10 years, these generations are stable and physically consistent. We\nalso show that for several important climate variables, ArchesClimate generates\nsimulations that are interchangeable with the IPSL model. This work suggests\nthat climate model emulators could significantly reduce the cost of climate\nmodel simulations.",
      "pdf_url": "http://arxiv.org/pdf/2509.15942v1",
      "published": "2025-09-19T12:53:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15942v1",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ]
    },
    {
      "title": "A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning",
      "authors": [
        "Shaopeng Zhai",
        "Qi Zhang",
        "Tianyi Zhang",
        "Fuxian Huang",
        "Haoran Zhang",
        "Ming Zhou",
        "Shengzhe Zhang",
        "Litao Liu",
        "Sixu Lin",
        "Jiangmiao Pang"
      ],
      "abstract": "Robotic real-world reinforcement learning (RL) with vision-language-action\n(VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient\nexploration. We introduce VLAC, a general process reward model built upon\nInternVL and trained on large scale heterogeneous datasets. Given pairwise\nobservations and a language goal, it outputs dense progress delta and done\nsignal, eliminating task-specific reward engineering, and supports one-shot\nin-context transfer to unseen tasks and environments. VLAC is trained on\nvision-language datasets to strengthen perception, dialogic and reasoning\ncapabilities, together with robot and human trajectories data that ground\naction generation and progress estimation, and additionally strengthened to\nreject irrelevant prompts as well as detect regression or stagnation by\nconstructing large numbers of negative and semantically mismatched samples.\nWith prompt control, a single VLAC model alternately generating reward and\naction tokens, unifying critic and policy. Deployed inside an asynchronous\nreal-world RL loop, we layer a graded human-in-the-loop protocol (offline\ndemonstration replay, return and explore, human guided explore) that\naccelerates exploration and stabilizes early learning. Across four distinct\nreal-world manipulation tasks, VLAC lifts success rates from about 30\\% to\nabout 90\\% within 200 real-world interaction episodes; incorporating\nhuman-in-the-loop interventions yields a further 50% improvement in sample\nefficiency and achieves up to 100% final success.",
      "pdf_url": "http://arxiv.org/pdf/2509.15937v1",
      "published": "2025-09-19T12:44:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15937v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "The Alignment Bottleneck",
      "authors": [
        "Wenjun Cao"
      ],
      "abstract": "Large language models improve with scale, yet feedback-based alignment still\nexhibits systematic deviations from intended behavior. Motivated by bounded\nrationality in economics and cognitive science, we view judgment as\nresource-limited and feedback as a constrained channel. On this basis, we model\nthe loop as a two-stage cascade $U \\to H \\to Y$ given $S$, with cognitive\ncapacity $C_{\\text{cog}|S}$ and average total capacity\n$\\bar{C}_{\\text{tot}|S}$. Our main result is a capacity-coupled Alignment\nPerformance Interval. It pairs a data size-independent Fano lower bound proved\non a separable codebook mixture with a PAC-Bayes upper bound whose KL term is\ncontrolled by the same channel via $m \\, \\bar{C}_{\\text{tot}|S}$. The PAC-Bayes\nbound becomes an upper bound on the same true risk when the canonical\nobservable loss is used and the dataset is drawn from the same mixture. Under\nthese matched conditions, both limits are governed by a single capacity.\nConsequences include that, with value complexity and capacity fixed, adding\nlabels alone cannot cross the bound; attaining lower risk on more complex\ntargets requires capacity that grows with $\\log M$; and once useful signal\nsaturates capacity, further optimization tends to fit channel regularities,\nconsistent with reports of sycophancy and reward hacking. The analysis views\nalignment as interface engineering: measure and allocate limited capacity,\nmanage task complexity, and decide where information is spent.",
      "pdf_url": "http://arxiv.org/pdf/2509.15932v1",
      "published": "2025-09-19T12:38:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15932v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT",
        "stat.ML"
      ]
    },
    {
      "title": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search",
      "authors": [
        "Zhiyu Mou",
        "Yiqin Lv",
        "Miao Xu",
        "Cheems Wang",
        "Yixiu Mao",
        "Qichen Ye",
        "Chao Li",
        "Rongquan Bai",
        "Chuan Yu",
        "Jian Xu",
        "Bo Zheng"
      ],
      "abstract": "Auto-bidding is an essential tool for advertisers to enhance their\nadvertising performance. Recent progress has shown that AI-Generated Bidding\n(AIGB), which formulates the auto-bidding as a trajectory generation task and\ntrains a conditional diffusion-based planner on offline data, achieves superior\nand stable performance compared to typical offline reinforcement learning\n(RL)-based auto-bidding methods. However, existing AIGB methods still encounter\na performance bottleneck due to their neglect of fine-grained generation\nquality evaluation and inability to explore beyond static datasets. To address\nthis, we propose AIGB-Pearl (\\emph{Planning with EvAluator via RL}), a novel\nmethod that integrates generative planning and policy optimization. The key to\nAIGB-Pearl is to construct a non-bootstrapped \\emph{trajectory evaluator} to\nassign rewards and guide policy search, enabling the planner to optimize its\ngeneration quality iteratively through interaction. Furthermore, to enhance\ntrajectory evaluator accuracy in offline settings, we incorporate three key\ntechniques: (i) a Large Language Model (LLM)-based architecture for better\nrepresentational capacity, (ii) hybrid point-wise and pair-wise losses for\nbetter score learning, and (iii) adaptive integration of expert feedback for\nbetter generalization ability. Extensive experiments on both simulated and\nreal-world advertising systems demonstrate the state-of-the-art performance of\nour approach.",
      "pdf_url": "http://arxiv.org/pdf/2509.15927v1",
      "published": "2025-09-19T12:30:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15927v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds",
      "authors": [
        "Remo Sasso",
        "Michelangelo Conserva",
        "Dominik Jeurissen",
        "Paulo Rauber"
      ],
      "abstract": "While reinforcement learning from scratch has shown impressive results in\nsolving sequential decision-making tasks with efficient simulators, real-world\napplications with expensive interactions require more sample-efficient agents.\nFoundation models (FMs) are natural candidates to improve sample efficiency as\nthey possess broad knowledge and reasoning capabilities, but it is yet unclear\nhow to effectively integrate them into the reinforcement learning framework. In\nthis paper, we anticipate and, most importantly, evaluate two promising\nstrategies. First, we consider the use of foundation world models (FWMs) that\nexploit the prior knowledge of FMs to enable training and evaluating agents\nwith simulated interactions. Second, we consider the use of foundation agents\n(FAs) that exploit the reasoning capabilities of FMs for decision-making. We\nevaluate both approaches empirically in a family of grid-world environments\nthat are suitable for the current generation of large language models (LLMs).\nOur results suggest that improvements in LLMs already translate into better\nFWMs and FAs; that FAs based on current LLMs can already provide excellent\npolicies for sufficiently simple environments; and that the coupling of FWMs\nand reinforcement learning agents is highly promising for more complex settings\nwith partial observability and stochastic elements.",
      "pdf_url": "http://arxiv.org/pdf/2509.15915v1",
      "published": "2025-09-19T12:10:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15915v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05",
        "I.2.6; I.2.8"
      ]
    },
    {
      "title": "An Equivariant Graph Network for Interpretable Nanoporous Materials Design",
      "authors": [
        "Zhenhao Zhou",
        "Salman Bin Kashif",
        "Dawei Feng",
        "Jin-Hu Dou",
        "Kaihang Shi",
        "Tao Deng",
        "Zhenpeng Yao"
      ],
      "abstract": "Nanoporous materials hold promise for diverse sustainable applications, yet\ntheir vast chemical space poses challenges for efficient design. Machine\nlearning offers a compelling pathway to accelerate the exploration, but\nexisting models lack either interpretability or fidelity for elucidating the\ncorrelation between crystal geometry and property. Here, we report a\nthree-dimensional periodic space sampling method that decomposes large\nnanoporous structures into local geometrical sites for combined property\nprediction and site-wise contribution quantification. Trained with a\nconstructed database and retrieved datasets, our model achieves\nstate-of-the-art accuracy and data efficiency for property prediction on gas\nstorage, separation, and electrical conduction. Meanwhile, this approach\nenables the interpretation of the prediction and allows for accurate\nidentification of significant local sites for targeted properties. Through\nidentifying transferable high-performance sites across diverse nanoporous\nframeworks, our model paves the way for interpretable, symmetry-aware\nnanoporous materials design, which is extensible to other materials, like\nmolecular crystals and beyond.",
      "pdf_url": "http://arxiv.org/pdf/2509.15908v1",
      "published": "2025-09-19T12:04:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15908v1",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ]
    },
    {
      "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions",
      "authors": [
        "Frederic Kirstein",
        "Sonu Kumar",
        "Terry Ruas",
        "Bela Gipp"
      ],
      "abstract": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization.",
      "pdf_url": "http://arxiv.org/pdf/2509.15901v1",
      "published": "2025-09-19T11:58:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15901v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and AI Methods for Childhood Leukemia Prediction",
      "authors": [
        "Henning Höfener",
        "Farina Kock",
        "Martina Pontones",
        "Tabita Ghete",
        "David Pfrang",
        "Nicholas Dickel",
        "Meik Kunz",
        "Daniela P. Schacherer",
        "David A. Clunie",
        "Andrey Fedorov",
        "Max Westphal",
        "Markus Metzler"
      ],
      "abstract": "Leukemia diagnosis primarily relies on manual microscopic analysis of bone\nmarrow morphology supported by additional laboratory parameters, making it\ncomplex and time consuming. While artificial intelligence (AI) solutions have\nbeen proposed, most utilize private datasets and only cover parts of the\ndiagnostic pipeline. Therefore, we present a large, high-quality, publicly\navailable leukemia bone marrow dataset spanning the entire diagnostic process,\nfrom cell detection to diagnosis. Using this dataset, we further propose\nmethods for cell detection, cell classification, and diagnosis prediction. The\ndataset comprises 246 pediatric patients with diagnostic, clinical and\nlaboratory information, over 40 000 cells with bounding box annotations and\nmore than 28 000 of these with high-quality class labels, making it the most\ncomprehensive dataset publicly available. Evaluation of the AI models yielded\nan average precision of 0.96 for the cell detection, an area under the curve of\n0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean\nF1-score of 0.90 for the diagnosis prediction using predicted cell counts.\nWhile the proposed approaches demonstrate their usefulness for AI-assisted\ndiagnostics, the dataset will foster further research and development in the\nfield, ultimately contributing to more precise diagnoses and improved patient\noutcomes.",
      "pdf_url": "http://arxiv.org/pdf/2509.15895v1",
      "published": "2025-09-19T11:48:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15895v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "MoAngelo: Motion-Aware Neural Surface Reconstruction for Dynamic Scenes",
      "authors": [
        "Mohamed Ebbed",
        "Zorah Lähner"
      ],
      "abstract": "Dynamic scene reconstruction from multi-view videos remains a fundamental\nchallenge in computer vision. While recent neural surface reconstruction\nmethods have achieved remarkable results in static 3D reconstruction, extending\nthese approaches with comparable quality for dynamic scenes introduces\nsignificant computational and representational challenges. Existing dynamic\nmethods focus on novel-view synthesis, therefore, their extracted meshes tend\nto be noisy. Even approaches aiming for geometric fidelity often result in too\nsmooth meshes due to the ill-posedness of the problem. We present a novel\nframework for highly detailed dynamic reconstruction that extends the static 3D\nreconstruction method NeuralAngelo to work in dynamic settings. To that end, we\nstart with a high-quality template scene reconstruction from the initial frame\nusing NeuralAngelo, and then jointly optimize deformation fields that track the\ntemplate and refine it based on the temporal sequence. This flexible template\nallows updating the geometry to include changes that cannot be modeled with the\ndeformation field, for instance occluded parts or the changes in the topology.\nWe show superior reconstruction accuracy in comparison to previous\nstate-of-the-art methods on the ActorsHQ dataset.",
      "pdf_url": "http://arxiv.org/pdf/2509.15892v1",
      "published": "2025-09-19T11:43:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15892v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation",
      "authors": [
        "Senkang Hu",
        "Xudong Han",
        "Jinqi Jiang",
        "Yihang Tao",
        "Zihan Fang",
        "Sam Tak Wu Kwong",
        "Yuguang Fang"
      ],
      "abstract": "Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models.",
      "pdf_url": "http://arxiv.org/pdf/2509.15888v1",
      "published": "2025-09-19T11:35:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15888v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning",
      "authors": [
        "Xiaosheng Long",
        "Hanyu Wang",
        "Zhentao Song",
        "Kun Luo",
        "Hongde Liu"
      ],
      "abstract": "Recent retrieval-augmented image captioning methods incorporate external\nknowledge to compensate for the limitations in comprehending complex scenes.\nHowever, current approaches face challenges in relation modeling: (1) the\nrepresentation of semantic prompts is too coarse-grained to capture\nfine-grained relationships; (2) these methods lack explicit modeling of image\nobjects and their semantic relationships. To address these limitations, we\npropose RACap, a relation-aware retrieval-augmented model for image captioning,\nwhich not only mines structured relation semantics from retrieval captions, but\nalso identifies heterogeneous objects from the image. RACap effectively\nretrieves structured relation features that contain heterogeneous visual\ninformation to enhance the semantic consistency and relational expressiveness.\nExperimental results show that RACap, with only 10.8M trainable parameters,\nachieves superior performance compared to previous lightweight captioning\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2509.15883v1",
      "published": "2025-09-19T11:29:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15883v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration",
      "authors": [
        "Xingmei Wang",
        "Xiaoyu Hu",
        "Chengkai Huang",
        "Ziyan Zeng",
        "Guohao Nie",
        "Quan Z. Sheng",
        "Lina Yao"
      ],
      "abstract": "Bridging 2D and 3D sensor modalities is critical for robust perception in\nautonomous systems. However, image-to-point cloud (I2P) registration remains\nchallenging due to the semantic-geometric gap between texture-rich but\ndepth-ambiguous images and sparse yet metrically precise point clouds, as well\nas the tendency of existing methods to converge to local optima. To overcome\nthese limitations, we introduce CrossI2P, a self-supervised framework that\nunifies cross-modal learning and two-stage registration in a single end-to-end\npipeline. First, we learn a geometric-semantic fused embedding space via\ndual-path contrastive learning, enabling annotation-free, bidirectional\nalignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine\nregistration paradigm: a global stage establishes superpoint-superpixel\ncorrespondences through joint intra-modal context and cross-modal interaction\nmodeling, followed by a geometry-constrained point-level refinement for precise\nregistration. Third, we employ a dynamic training mechanism with gradient\nnormalization to balance losses for feature alignment, correspondence\nrefinement, and pose estimation. Extensive experiments demonstrate that\nCrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry\nbenchmark and by 37.9% on nuScenes, significantly improving both accuracy and\nrobustness.",
      "pdf_url": "http://arxiv.org/pdf/2509.15882v1",
      "published": "2025-09-19T11:29:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15882v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction",
      "authors": [
        "Manajit Das",
        "Ajnabiul Hoque",
        "Mayank Baranwal",
        "Raghavan B. Sunoj"
      ],
      "abstract": "Prediction of complete step-by-step chemical reaction mechanisms (CRMs)\nremains a major challenge. Whereas the traditional approaches in CRM tasks rely\non expert-driven experiments or costly quantum chemical computations,\ncontemporary deep learning (DL) alternatives ignore key intermediates and\nmechanistic steps and often suffer from hallucinations. We present DeepMech, an\ninterpretable graph-based DL framework employing atom- and bond-level\nattention, guided by generalized templates of mechanistic operations (TMOps),\nto generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K\natom-mapped and mass-balanced elementary steps), DeepMech achieves\n98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in\ncomplete CRM tasks, besides maintaining high fidelity even in\nout-of-distribution scenarios as well as in predicting side and/or byproducts.\nExtension to multistep CRMs relevant to prebiotic chemistry, demonstrates the\nability of DeepMech in effectively reconstructing pathways from simple\nprimordial substrates to complex biomolecules such as serine and aldopentose.\nAttention analysis identifies reactive atoms/bonds in line with chemical\nintuition, rendering our model interpretable and suitable for reaction design.",
      "pdf_url": "http://arxiv.org/pdf/2509.15872v1",
      "published": "2025-09-19T11:14:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15872v1",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network",
      "authors": [
        "Rikuto Kotoge",
        "Zheng Chen",
        "Tasuku Kimura",
        "Yasuko Matsubara",
        "Takufumi Yanagisawa",
        "Haruhiko Kishima",
        "Yasushi Sakurai"
      ],
      "abstract": "Dynamic GNNs, which integrate temporal and spatial features in\nElectroencephalography (EEG) data, have shown great potential in automating\nseizure detection. However, fully capturing the underlying dynamics necessary\nto represent brain states, such as seizure and non-seizure, remains a\nnon-trivial task and presents two fundamental challenges. First, most existing\ndynamic GNN methods are built on temporally fixed static graphs, which fail to\nreflect the evolving nature of brain connectivity during seizure progression.\nSecond, current efforts to jointly model temporal signals and graph structures\nand, more importantly, their interactions remain nascent, often resulting in\ninconsistent performance. To address these challenges, we present the first\ntheoretical analysis of these two problems, demonstrating the effectiveness and\nnecessity of explicit dynamic modeling and time-then-graph dynamic GNN method.\nBuilding on these insights, we propose EvoBrain, a novel seizure detection\nmodel that integrates a two-stream Mamba architecture with a GCN enhanced by\nLaplacian Positional Encoding, following neurological insights. Moreover,\nEvoBrain incorporates explicitly dynamic graph structures, allowing both nodes\nand edges to evolve over time. Our contributions include (a) a theoretical\nanalysis proving the expressivity advantage of explicit dynamic modeling and\ntime-then-graph over other approaches, (b) a novel and efficient model that\nsignificantly improves AUROC by 23% and F1 score by 30%, compared with the\ndynamic GNN baseline, and (c) broad evaluations of our method on the\nchallenging early seizure prediction tasks.",
      "pdf_url": "http://arxiv.org/pdf/2509.15857v1",
      "published": "2025-09-19T10:47:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15857v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring",
      "authors": [
        "Giovanni De Gasperis",
        "Sante Dino Facchini"
      ],
      "abstract": "Industrial monitoring systems, especially when deployed in Industry 4.0\nenvironments, are experiencing a shift in paradigm from traditional rule-based\narchitectures to data-driven approaches leveraging machine learning and\nartificial intelligence. This study presents a comparison between these two\nmethodologies, analyzing their respective strengths, limitations, and\napplication scenarios, and proposes a basic framework to evaluate their key\nproperties. Rule-based systems offer high interpretability, deterministic\nbehavior, and ease of implementation in stable environments, making them ideal\nfor regulated industries and safety-critical applications. However, they face\nchallenges with scalability, adaptability, and performance in complex or\nevolving contexts. Conversely, data-driven systems excel in detecting hidden\nanomalies, enabling predictive maintenance and dynamic adaptation to new\nconditions. Despite their high accuracy, these models face challenges related\nto data availability, explainability, and integration complexity. The paper\nsuggests hybrid solutions as a possible promising direction, combining the\ntransparency of rule-based logic with the analytical power of machine learning.\nOur hypothesis is that the future of industrial monitoring lies in intelligent,\nsynergic systems that leverage both expert knowledge and data-driven insights.\nThis dual approach enhances resilience, operational efficiency, and trust,\npaving the way for smarter and more flexible industrial environments.",
      "pdf_url": "http://arxiv.org/pdf/2509.15848v1",
      "published": "2025-09-19T10:31:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15848v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Diversity of Structured Domains via k-Kemeny Scores",
      "authors": [
        "Piotr Faliszewski",
        "Krzysztof Sornat",
        "Stanisław Szufa",
        "Tomasz Wąs"
      ],
      "abstract": "In the k-Kemeny problem, we are given an ordinal election, i.e., a collection\nof votes ranking the candidates from best to worst, and we seek the smallest\nnumber of swaps of adjacent candidates that ensure that the election has at\nmost k different rankings. We study this problem for a number of structured\ndomains, including the single-peaked, single-crossing, group-separable, and\nEuclidean ones. We obtain two kinds of results: (1) We show that k-Kemeny\nremains intractable under most of these domains, even for k=2, and (2) we use\nk-Kemeny to rank these domains in terms of their diversity.",
      "pdf_url": "http://arxiv.org/pdf/2509.15812v1",
      "published": "2025-09-19T09:40:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15812v1",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning",
      "authors": [
        "Sara Rajaee",
        "Rochelle Choenni",
        "Ekaterina Shutova",
        "Christof Monz"
      ],
      "abstract": "While the reasoning abilities of large language models (LLMs) continue to\nadvance, it remains unclear how such ability varies across languages in\nmultilingual LLMs and whether different languages produce reasoning paths that\ncomplement each other. To investigate this question, we train a reward model to\nrank generated responses for a given question across languages. Our results\nshow that our cross-lingual reward model substantially improves mathematical\nreasoning performance compared to using reward modeling within a single\nlanguage, benefiting even high-resource languages. While English often exhibits\nthe highest performance in multilingual models, we find that cross-lingual\nsampling particularly benefits English under low sampling budgets. Our findings\nreveal new opportunities to improve multilingual reasoning by leveraging the\ncomplementary strengths of diverse languages.",
      "pdf_url": "http://arxiv.org/pdf/2509.15811v1",
      "published": "2025-09-19T09:38:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15811v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering",
      "authors": [
        "Chen Wang",
        "Zeyuan Ma",
        "Zhiguang Cao",
        "Yue-Jiao Gong"
      ],
      "abstract": "To relieve intensive human-expertise required to design optimization\nalgorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage\ngeneralization strength of meta-learning to train neural network-based\nalgorithm design policies over a predefined training problem set, which\nautomates the adaptability of the low-level optimizers on unseen problem\ninstances. Currently, a common training problem set choice in existing MetaBBOs\nis well-known benchmark suites CoCo-BBOB. Although such choice facilitates the\nMetaBBO's development, problem instances in CoCo-BBOB are more or less limited\nin diversity, raising the risk of overfitting of MetaBBOs, which might further\nresults in poor generalization. In this paper, we propose an instance\ngeneration approach, termed as \\textbf{LSRE}, which could generate diverse\ntraining problem instances for MetaBBOs to learn more generalizable policies.\nLSRE first trains an autoencoder which maps high-dimensional problem features\ninto a 2-dimensional latent space. Uniform-grid sampling in this latent space\nleads to hidden representations of problem instances with sufficient diversity.\nBy leveraging a genetic-programming approach to search function formulas with\nminimal L2-distance to these hidden representations, LSRE reverse engineers a\ndiversified problem set, termed as \\textbf{Diverse-BBO}. We validate the\neffectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe\ntheir generalization performances on either synthetic or realistic scenarios.\nExtensive experimental results underscore the superiority of Diverse-BBO to\nexisting training set choices in MetaBBOs. Further ablation studies not only\ndemonstrate the effectiveness of design choices in LSRE, but also reveal\ninteresting insights on instance diversity and MetaBBO's generalization.",
      "pdf_url": "http://arxiv.org/pdf/2509.15810v1",
      "published": "2025-09-19T09:37:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15810v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models",
      "authors": [
        "Fangjian Shen",
        "Zifeng Liang",
        "Chao Wang",
        "Wushao Wen"
      ],
      "abstract": "Text-to-image (T2I) models exhibit a significant yet under-explored \"brand\nbias\", a tendency to generate contents featuring dominant commercial brands\nfrom generic prompts, posing ethical and legal risks. We propose CIDER, a\nnovel, model-agnostic framework to mitigate bias at inference-time through\nprompt refinement to avoid costly retraining. CIDER uses a lightweight detector\nto identify branded content and a Vision-Language Model (VLM) to generate\nstylistically divergent alternatives. We introduce the Brand Neutrality Score\n(BNS) to quantify this issue and perform extensive experiments on leading T2I\nmodels. Results show CIDER significantly reduces both explicit and implicit\nbiases while maintaining image quality and aesthetic appeal. Our work offers a\npractical solution for more original and equitable content, contributing to the\ndevelopment of trustworthy generative AI.",
      "pdf_url": "http://arxiv.org/pdf/2509.15803v1",
      "published": "2025-09-19T09:30:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15803v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding",
      "authors": [
        "Kehua Chen"
      ],
      "abstract": "Current state-of-the-art video understanding methods typically struggle with\ntwo critical challenges: (1) the computational infeasibility of processing\nevery frame in dense video content and (2) the difficulty in identifying\nsemantically significant frames through naive uniform sampling strategies. In\nthis paper, we propose a novel video understanding framework, called\nChronoForge-RL, which combines Temporal Apex Distillation (TAD) and\nKeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle these\nissues. Concretely, we introduce a differentiable keyframe selection mechanism\nthat systematically identifies semantic inflection points through a three-stage\nprocess to enhance computational efficiency while preserving temporal\ninformation. Then, two particular modules are proposed to enable effective\ntemporal reasoning: Firstly, TAD leverages variation scoring, inflection\ndetection, and prioritized distillation to select the most informative frames.\nSecondly, we introduce KF-GRPO which implements a contrastive learning paradigm\nwith a saliency-enhanced reward mechanism that explicitly incentivizes models\nto leverage both frame content and temporal relationships. Finally, our\nproposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBench\ncompared to baseline methods, clearly surpassing previous approaches while\nenabling our 7B parameter model to achieve performance comparable to 72B\nparameter alternatives.",
      "pdf_url": "http://arxiv.org/pdf/2509.15800v1",
      "published": "2025-09-19T09:27:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15800v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control",
      "authors": [
        "Max Studt",
        "Georg Schildbach"
      ],
      "abstract": "Achieving safe and coordinated behavior in dynamic, constraint-rich\nenvironments remains a major challenge for learning-based control. Pure\nend-to-end learning often suffers from poor sample efficiency and limited\nreliability, while model-based methods depend on predefined references and\nstruggle to generalize. We propose a hierarchical framework that combines\ntactical decision-making via reinforcement learning (RL) with low-level\nexecution through Model Predictive Control (MPC). For the case of multi-agent\nsystems this means that high-level policies select abstract targets from\nstructured regions of interest (ROIs), while MPC ensures dynamically feasible\nand safe motion. Tested on a predator-prey benchmark, our approach outperforms\nend-to-end and shielding-based RL baselines in terms of reward, safety, and\nconsistency, underscoring the benefits of combining structured learning with\nmodel-based control.",
      "pdf_url": "http://arxiv.org/pdf/2509.15799v1",
      "published": "2025-09-19T09:27:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15799v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "math.OC"
      ]
    },
    {
      "title": "Monte Carlo Tree Diffusion with Multiple Experts for Protein Design",
      "authors": [
        "Xuefeng Liu",
        "Mingxuan Cao",
        "Songhao Jiang",
        "Xiao Luo",
        "Xiaotian Duan",
        "Mengdi Wang",
        "Tobin R. Sosnick",
        "Jinbo Xu",
        "Rick Stevens"
      ],
      "abstract": "The goal of protein design is to generate amino acid sequences that fold into\nfunctional structures with desired properties. Prior methods combining\nautoregressive language models with Monte Carlo Tree Search (MCTS) struggle\nwith long-range dependencies and suffer from an impractically large search\nspace. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts,\nwhich integrates masked diffusion models with tree search to enable multi-token\nplanning and efficient exploration. Unlike autoregressive planners, MCTD-ME\nuses biophysical-fidelity-enhanced diffusion denoising as the rollout engine,\njointly revising multiple positions and scaling to large sequence spaces. It\nfurther leverages experts of varying capacities to enrich exploration, guided\nby a pLDDT-based masking schedule that targets low-confidence regions while\npreserving reliable residues. We propose a novel multi-expert selection rule\n(PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse\nfolding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and\nunguided baselines in both sequence recovery (AAR) and structural similarity\n(scTM), with gains increasing for longer proteins and benefiting from\nmulti-expert guidance. More generally, the framework is model-agnostic and\napplicable beyond inverse folding, including de novo protein engineering and\nmulti-objective molecular generation.",
      "pdf_url": "http://arxiv.org/pdf/2509.15796v1",
      "published": "2025-09-19T09:24:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15796v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ]
    }
  ]
}