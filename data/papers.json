{
  "last_updated": "2025-07-21T01:00:10.898835",
  "papers": [
    {
      "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding",
      "authors": [
        "Shihao Wang",
        "Guo Chen",
        "De-an Huang",
        "Zhiqi Li",
        "Minghan Li",
        "Guilin Li",
        "Jose M. Alvarez",
        "Lei Zhang",
        "Zhiding Yu"
      ],
      "abstract": "Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding.",
      "pdf_url": "http://arxiv.org/pdf/2507.13353v1",
      "published": "2025-07-17T17:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13353v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning",
      "authors": [
        "Senqiao Yang",
        "Junyi Li",
        "Xin Lai",
        "Bei Yu",
        "Hengshuang Zhao",
        "Jiaya Jia"
      ],
      "abstract": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
      "pdf_url": "http://arxiv.org/pdf/2507.13348v1",
      "published": "2025-07-17T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13348v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Imbalance in Balance: Online Concept Balancing in Generation Models",
      "authors": [
        "Yukai Shi",
        "Jiarong Ou",
        "Rui Chen",
        "Haotian Yang",
        "Jiahao Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai"
      ],
      "abstract": "In visual generation tasks, the responses and combinations of complex\nconcepts often lack stability and are error-prone, which remains an\nunder-explored area. In this paper, we attempt to explore the causal factors\nfor poor concept responses through elaborately designed experiments. We also\ndesign a concept-wise equalization loss function (IMBA loss) to address this\nissue. Our proposed method is online, eliminating the need for offline dataset\nprocessing, and requires minimal code changes. In our newly proposed complex\nconcept benchmark Inert-CompBench and two other public test sets, our method\nsignificantly enhances the concept response capability of baseline models and\nyields highly competitive results with only a few codes.",
      "pdf_url": "http://arxiv.org/pdf/2507.13345v1",
      "published": "2025-07-17T17:59:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13345v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Latent Policy Steering with Embodiment-Agnostic Pretrained World Models",
      "authors": [
        "Yiqi Wang",
        "Mrinal Verghese",
        "Jeff Schneider"
      ],
      "abstract": "Learning visuomotor policies via imitation has proven effective across a wide\nrange of robotic domains. However, the performance of these policies is heavily\ndependent on the number of training demonstrations, which requires expensive\ndata collection in the real world. In this work, we aim to reduce data\ncollection efforts when learning visuomotor robot policies by leveraging\nexisting or cost-effective data from a wide range of embodiments, such as\npublic robot datasets and the datasets of humans playing with objects (human\ndata from play). Our approach leverages two key insights. First, we use optic\nflow as an embodiment-agnostic action representation to train a World Model\n(WM) across multi-embodiment datasets, and finetune it on a small amount of\nrobot data from the target embodiment. Second, we develop a method, Latent\nPolicy Steering (LPS), to improve the output of a behavior-cloned policy by\nsearching in the latent space of the WM for better action sequences. In real\nworld experiments, we observe significant improvements in the performance of\npolicies trained with a small amount of data (over 50% relative improvement\nwith 30 demonstrations and over 20% relative improvement with 50\ndemonstrations) by combining the policy with a WM pretrained on two thousand\nepisodes sampled from the existing Open X-embodiment dataset across different\nrobots or a cost-effective human dataset from play.",
      "pdf_url": "http://arxiv.org/pdf/2507.13340v1",
      "published": "2025-07-17T17:57:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13340v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming",
      "authors": [
        "Gal Beniamini",
        "Yuval Dor",
        "Alon Vinnikov",
        "Shir Granot Peled",
        "Or Weinstein",
        "Or Sharir",
        "Noam Wies",
        "Tomer Nussbaum",
        "Ido Ben Shaul",
        "Tomer Zekharya",
        "Yoav Levine",
        "Shai Shalev-Shwartz",
        "Amnon Shashua"
      ],
      "abstract": "Frontier AI models demonstrate formidable breadth of knowledge. But how close\nare they to true human -- or superhuman -- expertise? Genuine experts can\ntackle the hardest problems and push the boundaries of scientific\nunderstanding. To illuminate the limits of frontier model capabilities, we turn\naway from contrived competitive programming puzzles, and instead focus on\nreal-life research problems.\n  We construct FormulaOne, a benchmark that lies at the intersection of graph\ntheory, logic, and algorithms, all well within the training distribution of\nfrontier models. Our problems are incredibly demanding, requiring an array of\nreasoning steps. The dataset has three key properties. First, it is of\ncommercial interest and relates to practical large-scale optimisation problems,\nsuch as those arising in routing, scheduling, and network design. Second, it is\ngenerated from the highly expressive framework of Monadic Second-Order (MSO)\nlogic on graphs, paving the way toward automatic problem generation at scale;\nideal for building RL environments. Third, many of our problems are intimately\nrelated to the frontier of theoretical computer science, and to central\nconjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As\nsuch, any significant algorithmic progress on our dataset, beyond known\nresults, could carry profound theoretical implications.\n  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on\nFormulaOne, solving less than 1% of the questions, even when given 10 attempts\nand explanatory fewshot examples -- highlighting how far they remain from\nexpert-level understanding in some domains. To support further research, we\nadditionally curate FormulaOne-Warmup, offering a set of simpler tasks, from\nthe same distribution. We release the full corpus along with a comprehensive\nevaluation framework.",
      "pdf_url": "http://arxiv.org/pdf/2507.13337v1",
      "published": "2025-07-17T17:53:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13337v1",
      "categories": [
        "cs.AI",
        "cs.CC",
        "math.LO"
      ]
    },
    {
      "title": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It",
      "authors": [
        "Yulu Qin",
        "Dheeraj Varghese",
        "Adam Dahlgren Lindström",
        "Lucia Donatelli",
        "Kanishka Misra",
        "Najoung Kim"
      ],
      "abstract": "Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic.",
      "pdf_url": "http://arxiv.org/pdf/2507.13328v1",
      "published": "2025-07-17T17:47:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13328v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark",
      "authors": [
        "Junsu Kim",
        "Naeun Kim",
        "Jaeho Lee",
        "Incheol Park",
        "Dongyoon Han",
        "Seungryul Baek"
      ],
      "abstract": "The reasoning-based pose estimation (RPE) benchmark has emerged as a widely\nadopted evaluation standard for pose-aware multimodal large language models\n(MLLMs). Despite its significance, we identified critical reproducibility and\nbenchmark-quality issues that hinder fair and consistent quantitative\nevaluations. Most notably, the benchmark utilizes different image indices from\nthose of the original 3DPW dataset, forcing researchers into tedious and\nerror-prone manual matching processes to obtain accurate ground-truth (GT)\nannotations for quantitative metrics (\\eg, MPJPE, PA-MPJPE). Furthermore, our\nanalysis reveals several inherent benchmark-quality limitations, including\nsignificant image redundancy, scenario imbalance, overly simplistic poses, and\nambiguous textual descriptions, collectively undermining reliable evaluations\nacross diverse scenarios. To alleviate manual effort and enhance\nreproducibility, we carefully refined the GT annotations through meticulous\nvisual matching and publicly release these refined annotations as an\nopen-source resource, thereby promoting consistent quantitative evaluations and\nfacilitating future advancements in human pose-aware multimodal reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2507.13314v1",
      "published": "2025-07-17T17:33:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13314v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations",
      "authors": [
        "Carlos Arriaga",
        "Gonzalo Martínez",
        "Eneko Sendin",
        "Javier Conde",
        "Pedro Reviriego"
      ],
      "abstract": "The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use.",
      "pdf_url": "http://arxiv.org/pdf/2507.13302v1",
      "published": "2025-07-17T17:11:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13302v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research",
      "authors": [
        "Yilun Zhao",
        "Weiyuan Chen",
        "Zhijian Xu",
        "Manasi Patwardhan",
        "Yixin Liu",
        "Chengye Wang",
        "Lovekesh Vig",
        "Arman Cohan"
      ],
      "abstract": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.",
      "pdf_url": "http://arxiv.org/pdf/2507.13300v1",
      "published": "2025-07-17T17:09:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13300v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Formal Verification of LLM-Generated Code from Natural Language Prompts",
      "authors": [
        "Aaron Councilman",
        "David Fu",
        "Aryan Gupta",
        "Chengxiao Wang",
        "David Grove",
        "Yu-Xiong Wang",
        "Vikram Adve"
      ],
      "abstract": "In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%.",
      "pdf_url": "http://arxiv.org/pdf/2507.13290v1",
      "published": "2025-07-17T16:54:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13290v1",
      "categories": [
        "cs.PL",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour",
      "authors": [
        "Emma M. A. Harrison"
      ],
      "abstract": "Robots are increasingly integrated across industries, particularly in\nhealthcare. However, many valuable applications for quadrupedal robots remain\noverlooked. This research explores the effectiveness of three reinforcement\nlearning algorithms in training a simulated quadruped robot for autonomous\nnavigation and obstacle avoidance. The goal is to develop a robotic guide dog\nsimulation capable of path following and obstacle avoidance, with long-term\npotential for real-world assistance to guide dogs and visually impaired\nindividuals. It also seeks to expand research into medical 'pets', including\nrobotic guide and alert dogs.\n  A comparative analysis of thirteen related research papers shaped key\nevaluation criteria, including collision detection, pathfinding algorithms,\nsensor usage, robot type, and simulation platforms. The study focuses on sensor\ninputs, collision frequency, reward signals, and learning progression to\ndetermine which algorithm best supports robotic navigation in complex\nenvironments.\n  Custom-made environments were used to ensure fair evaluation of all three\nalgorithms under controlled conditions, allowing consistent data collection.\nResults show that Proximal Policy Optimization (PPO) outperformed Deep\nQ-Network (DQN) and Q-learning across all metrics, particularly in average and\nmedian steps to goal per episode.\n  By analysing these results, this study contributes to robotic navigation, AI\nand medical robotics, offering insights into the feasibility of AI-driven\nquadruped mobility and its role in assistive robotics.",
      "pdf_url": "http://arxiv.org/pdf/2507.13277v1",
      "published": "2025-07-17T16:38:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13277v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management",
      "authors": [
        "Luis Gasco",
        "Hermenegildo Fabregat",
        "Laura García-Sardiña",
        "Paula Estrella",
        "Daniel Deniz",
        "Alvaro Rodrigo",
        "Rabih Zbib"
      ],
      "abstract": "Advances in natural language processing and large language models are driving\na major transformation in Human Capital Management, with a growing interest in\nbuilding smart systems based on language technologies for talent acquisition,\nupskilling strategies, and workforce planning. However, the adoption and\nprogress of these technologies critically depend on the development of reliable\nand fair models, properly evaluated on public data and open benchmarks, which\nhave so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation\ncampaign focused on skill and job title intelligence. The lab consists of two\ntasks: Task A - Multilingual Job Title Matching, covering English, Spanish,\nGerman, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.\nBoth corpora were built from real job applications, carefully anonymized, and\nmanually annotated to reflect the complexity and diversity of real-world labor\nmarket data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered\nthe evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most\nsystems relied on information retrieval techniques built with multilingual\nencoder-based models fine-tuned with contrastive learning, and several of them\nincorporated large language models for data augmentation or re-ranking. The\nresults show that the training strategies have a larger effect than the size of\nthe model alone. TalentCLEF provides the first public benchmark in this field\nand encourages the development of robust, fair, and transferable language\ntechnologies for the labor market.",
      "pdf_url": "http://arxiv.org/pdf/2507.13275v1",
      "published": "2025-07-17T16:33:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13275v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation",
      "authors": [
        "Jiazheng Li",
        "Hong Lu",
        "Kaiyue Wen",
        "Zaiwen Yang",
        "Jiaxuan Gao",
        "Hongzhou Lin",
        "Yi Wu",
        "Jingzhao Zhang"
      ],
      "abstract": "Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL.",
      "pdf_url": "http://arxiv.org/pdf/2507.13266v1",
      "published": "2025-07-17T16:21:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13266v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50"
      ]
    },
    {
      "title": "Voxtral",
      "authors": [
        "Alexander H. Liu",
        "Andy Ehrenberg",
        "Andy Lo",
        "Clément Denoix",
        "Corentin Barreau",
        "Guillaume Lample",
        "Jean-Malo Delignon",
        "Khyathi Raghavi Chandu",
        "Patrick von Platen",
        "Pavankumar Reddy Muddireddy",
        "Sanchit Gandhi",
        "Soham Ghosh",
        "Srijan Mishra",
        "Thomas Foubert",
        "Abhinav Rastogi",
        "Adam Yang",
        "Albert Q. Jiang",
        "Alexandre Sablayrolles",
        "Amélie Héliou",
        "Amélie Martin",
        "Anmol Agarwal",
        "Antoine Roux",
        "Arthur Darcet",
        "Arthur Mensch",
        "Baptiste Bout",
        "Baptiste Rozière",
        "Baudouin De Monicault",
        "Chris Bamford",
        "Christian Wallenwein",
        "Christophe Renaudin",
        "Clémence Lanfranchi",
        "Darius Dabert",
        "Devendra Singh Chaplot",
        "Devon Mizelle",
        "Diego de las Casas",
        "Elliot Chane-Sane",
        "Emilien Fugier",
        "Emma Bou Hanna",
        "Gabrielle Berrada",
        "Gauthier Delerce",
        "Gauthier Guinet",
        "Georgii Novikov",
        "Guillaume Martin",
        "Himanshu Jaju",
        "Jan Ludziejewski",
        "Jason Rute",
        "Jean-Hadrien Chabran",
        "Jessica Chudnovsky",
        "Joachim Studnia",
        "Joep Barmentlo",
        "Jonas Amar",
        "Josselin Somerville Roberts",
        "Julien Denize",
        "Karan Saxena",
        "Karmesh Yadav",
        "Kartik Khandelwal",
        "Kush Jain",
        "Lélio Renard Lavaud",
        "Léonard Blier",
        "Lingxiao Zhao",
        "Louis Martin",
        "Lucile Saulnier",
        "Luyu Gao",
        "Marie Pellat",
        "Mathilde Guillaumin",
        "Mathis Felardos",
        "Matthieu Dinot",
        "Maxime Darrin",
        "Maximilian Augustin",
        "Mickaël Seznec",
        "Neha Gupta",
        "Nikhil Raghuraman",
        "Olivier Duchenne",
        "Patricia Wang",
        "Patryk Saffer",
        "Paul Jacob",
        "Paul Wambergue",
        "Paula Kurylowicz",
        "Philomène Chagniot",
        "Pierre Stock",
        "Pravesh Agrawal",
        "Rémi Delacourt",
        "Romain Sauvestre",
        "Roman Soletskyi",
        "Sagar Vaze",
        "Sandeep Subramanian",
        "Saurabh Garg",
        "Shashwat Dalal",
        "Siddharth Gandhi",
        "Sumukh Aithal",
        "Szymon Antoniak",
        "Teven Le Scao",
        "Thibault Schueller",
        "Thibaut Lavril",
        "Thomas Robert",
        "Thomas Wang",
        "Timothée Lacroix",
        "Tom Bewley",
        "Valeriia Nemychnikova",
        "Victor Paltz",
        "Virgile Richard",
        "Wen-Ding Li",
        "William Marshall",
        "Xuanyu Zhang",
        "Yihan Wan",
        "Yunhao Tang"
      ],
      "abstract": "We present Voxtral Mini and Voxtral Small, two multimodal audio chat models.\nVoxtral is trained to comprehend both spoken audio and text documents,\nachieving state-of-the-art performance across a diverse range of audio\nbenchmarks, while preserving strong text capabilities. Voxtral Small\noutperforms a number of closed-source models, while being small enough to run\nlocally. A 32K context window enables the model to handle audio files up to 40\nminutes in duration and long multi-turn conversations. We also contribute three\nbenchmarks for evaluating speech understanding models on knowledge and trivia.\nBoth Voxtral models are released under Apache 2.0 license.",
      "pdf_url": "http://arxiv.org/pdf/2507.13264v1",
      "published": "2025-07-17T16:17:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13264v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Merge Kernel for Bayesian Optimization on Permutation Space",
      "authors": [
        "Zikai Xie",
        "Linjiang Chen"
      ],
      "abstract": "Bayesian Optimization (BO) algorithm is a standard tool for black-box\noptimization problems. The current state-of-the-art BO approach for permutation\nspaces relies on the Mallows kernel-an $\\Omega(n^2)$ representation that\nexplicitly enumerates every pairwise comparison. Inspired by the close\nrelationship between the Mallows kernel and pairwise comparison, we propose a\nnovel framework for generating kernel functions on permutation space based on\nsorting algorithms. Within this framework, the Mallows kernel can be viewed as\na special instance derived from bubble sort. Further, we introduce the\n\\textbf{Merge Kernel} constructed from merge sort, which replaces the quadratic\ncomplexity with $\\Theta(n\\log n)$ to achieve the lowest possible complexity.\nThe resulting feature vector is significantly shorter, can be computed in\nlinearithmic time, yet still efficiently captures meaningful permutation\ndistances. To boost robustness and right-invariance without sacrificing\ncompactness, we further incorporate three lightweight, task-agnostic\ndescriptors: (1) a shift histogram, which aggregates absolute element\ndisplacements and supplies a global misplacement signal; (2) a split-pair line,\nwhich encodes selected long-range comparisons by aligning elements across the\ntwo halves of the whole permutation; and (3) sliding-window motifs, which\nsummarize local order patterns that influence near-neighbor objectives. Our\nempirical evaluation demonstrates that the proposed kernel consistently\noutperforms the state-of-the-art Mallows kernel across various permutation\noptimization benchmarks. Results confirm that the Merge Kernel provides a more\ncompact yet more effective solution for Bayesian optimization in permutation\nspace.",
      "pdf_url": "http://arxiv.org/pdf/2507.13263v2",
      "published": "2025-07-17T16:12:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13263v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy",
      "authors": [
        "Yiting Yang",
        "Hao Luo",
        "Yuan Sun",
        "Qingsen Yan",
        "Haokui Zhang",
        "Wei Dong",
        "Guoqing Wang",
        "Peng Wang",
        "Yang Yang",
        "Hengtao Shen"
      ],
      "abstract": "A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained\nVision Transformers (ViT) involves freezing the majority of the backbone\nparameters and solely learning low-rank adaptation weight matrices to\naccommodate downstream tasks. These low-rank matrices are commonly derived\nthrough the multiplication structure of down-projection and up-projection\nmatrices, exemplified by methods such as LoRA and Adapter. In this work, we\nobserve an approximate orthogonality among any two row or column vectors within\nany weight matrix of the backbone parameters; however, this property is absent\nin the vectors of the down/up-projection matrices. Approximate orthogonality\nimplies a reduction in the upper bound of the model's generalization error,\nsignifying that the model possesses enhanced generalization capability. If the\nfine-tuned down/up-projection matrices were to exhibit this same property as\nthe pre-trained backbone matrices, could the generalization capability of\nfine-tuned ViTs be further augmented? To address this question, we propose an\nApproximately Orthogonal Fine-Tuning (AOFT) strategy for representing the\nlow-rank weight matrices. This strategy employs a single learnable vector to\ngenerate a set of approximately orthogonal vectors, which form the\ndown/up-projection matrices, thereby aligning the properties of these matrices\nwith those of the backbone. Extensive experimental results demonstrate that our\nmethod achieves competitive performance across a range of downstream image\nclassification tasks, confirming the efficacy of the enhanced generalization\ncapability embedded in the down/up-projection matrices.",
      "pdf_url": "http://arxiv.org/pdf/2507.13260v1",
      "published": "2025-07-17T16:09:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13260v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Automating Steering for Safe Multimodal Large Language Models",
      "authors": [
        "Lyucheng Wu",
        "Mengru Wang",
        "Ziwen Xu",
        "Tri Cao",
        "Nay Oo",
        "Bryan Hooi",
        "Shumin Deng"
      ],
      "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.",
      "pdf_url": "http://arxiv.org/pdf/2507.13255v1",
      "published": "2025-07-17T16:04:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13255v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models",
      "authors": [
        "Ashray Gupta",
        "Rohan Joseph",
        "Sunny Rai"
      ],
      "abstract": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.",
      "pdf_url": "http://arxiv.org/pdf/2507.13238v1",
      "published": "2025-07-17T15:47:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13238v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "VITA: Vision-to-Action Flow Matching Policy",
      "authors": [
        "Dechen Gao",
        "Boqi Zhao",
        "Andrew Lee",
        "Ian Chuang",
        "Hanchu Zhou",
        "Hang Wang",
        "Zhe Zhao",
        "Junshan Zhang",
        "Iman Soltani"
      ],
      "abstract": "We present VITA, a Vision-To-Action flow matching policy that evolves latent\nvisual representations into latent actions for visuomotor control. Traditional\nflow matching and diffusion policies sample from standard source distributions\n(e.g., Gaussian noise) and require additional conditioning mechanisms like\ncross-attention to condition action generation on visual information, creating\ntime and space overheads. VITA proposes a novel paradigm that treats latent\nimages as the flow source, learning an inherent mapping from vision to action\nwhile eliminating separate conditioning modules and preserving generative\nmodeling capabilities. Learning flows between fundamentally different\nmodalities like vision and action is challenging due to sparse action data\nlacking semantic structures and dimensional mismatches between high-dimensional\nvisual representations and raw actions. We address this by creating a\nstructured action latent space via an autoencoder as the flow matching target,\nup-sampling raw actions to match visual representation shapes. Crucially, we\nsupervise flow matching with both encoder targets and final action outputs\nthrough flow latent decoding, which backpropagates action reconstruction loss\nthrough sequential flow matching ODE solving steps for effective end-to-end\nlearning. Implemented as simple MLP layers, VITA is evaluated on challenging\nbi-manual manipulation tasks on the ALOHA platform, including 5 simulation and\n2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or\nmatches state-of-the-art generative policies while reducing inference latency\nby 50-130% compared to conventional flow matching policies requiring different\nconditioning mechanisms or complex architectures. To our knowledge, VITA is the\nfirst MLP-only flow matching policy capable of solving complex bi-manual\nmanipulation tasks like those in ALOHA benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2507.13231v1",
      "published": "2025-07-17T15:41:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13231v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation",
      "authors": [
        "Junhong Min",
        "Youngpil Jeon",
        "Jimin Kim",
        "Minyong Choi"
      ],
      "abstract": "The pursuit of a generalizable stereo matching model, capable of performing\nacross varying resolutions and disparity ranges without dataset-specific\nfine-tuning, has revealed a fundamental trade-off. Iterative local search\nmethods achieve high scores on constrained benchmarks, but their core mechanism\ninherently limits the global consistency required for true generalization. On\nthe other hand, global matching architectures, while theoretically more robust,\nhave been historically rendered infeasible by prohibitive computational and\nmemory costs. We resolve this dilemma with $S^2M^2$: a global matching\narchitecture that achieves both state-of-the-art accuracy and high efficiency\nwithout relying on cost volume filtering or deep refinement stacks. Our design\nintegrates a multi-resolution transformer for robust long-range correspondence,\ntrained with a novel loss function that concentrates probability on feasible\nmatches. This approach enables a more robust joint estimation of disparity,\nocclusion, and confidence. $S^2M^2$ establishes a new state of the art on the\nMiddlebury v3 and ETH3D benchmarks, significantly outperforming prior methods\nacross most metrics while reconstructing high-quality details with competitive\nefficiency.",
      "pdf_url": "http://arxiv.org/pdf/2507.13229v1",
      "published": "2025-07-17T15:40:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13229v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Synthesizing Reality: Leveraging the Generative AI-Powered Platform Midjourney for Construction Worker Detection",
      "authors": [
        "Hongyang Zhao",
        "Tianyu Liang",
        "Sina Davari",
        "Daeho Kim"
      ],
      "abstract": "While recent advancements in deep neural networks (DNNs) have substantially\nenhanced visual AI's capabilities, the challenge of inadequate data diversity\nand volume remains, particularly in construction domain. This study presents a\nnovel image synthesis methodology tailored for construction worker detection,\nleveraging the generative-AI platform Midjourney. The approach entails\ngenerating a collection of 12,000 synthetic images by formulating 3000\ndifferent prompts, with an emphasis on image realism and diversity. These\nimages, after manual labeling, serve as a dataset for DNN training. Evaluation\non a real construction image dataset yielded promising results, with the model\nattaining average precisions (APs) of 0.937 and 0.642 at\nintersection-over-union (IoU) thresholds of 0.5 and 0.5 to 0.95, respectively.\nNotably, the model demonstrated near-perfect performance on the synthetic\ndataset, achieving APs of 0.994 and 0.919 at the two mentioned thresholds.\nThese findings reveal both the potential and weakness of generative AI in\naddressing DNN training data scarcity.",
      "pdf_url": "http://arxiv.org/pdf/2507.13221v1",
      "published": "2025-07-17T15:35:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13221v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Higher-Order Pattern Unification Modulo Similarity Relations",
      "authors": [
        "Besik Dundua",
        "Temur Kutsia"
      ],
      "abstract": "The combination of higher-order theories and fuzzy logic can be useful in\ndecision-making tasks that involve reasoning across abstract functions and\npredicates, where exact matches are often rare or unnecessary. Developing\nefficient reasoning and computational techniques for such a combined formalism\npresents a significant challenge. In this paper, we adopt a more\nstraightforward approach aiming at integrating two well-established and\ncomputationally well-behaved components: higher-order patterns on one side and\nfuzzy equivalences expressed through similarity relations based on minimum\nT-norm on the other. We propose a unification algorithm for higher-order\npatterns modulo these similarity relations and prove its termination,\nsoundness, and completeness. This unification problem, like its crisp\ncounterpart, is unitary. The algorithm computes a most general unifier with the\nhighest degree of approximation when the given terms are unifiable.",
      "pdf_url": "http://arxiv.org/pdf/2507.13208v1",
      "published": "2025-07-17T15:18:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13208v1",
      "categories": [
        "cs.AI",
        "cs.LO",
        "math.LO",
        "03B70 (Primary) 68T37, 68T27, 68Q42, 03B40, 68V15 (Secondary)",
        "F.4.1; I.2.3"
      ]
    },
    {
      "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era",
      "authors": [
        "Matthew E. Brophy"
      ],
      "abstract": "The advancement of powerful yet opaque large language models (LLMs)\nnecessitates a fundamental revision of the philosophical criteria used to\nevaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the\nassumption of transparent architectures, which LLMs defy due to their\nstochastic outputs and opaque internal states. This paper argues that\ntraditional ethical criteria are pragmatically obsolete for LLMs due to this\nmismatch. Engaging with core themes in the philosophy of technology, this paper\nproffers a revised set of ten functional criteria to evaluate LLM-based\nartificial moral agents: moral concordance, context sensitivity, normative\nintegrity, metaethical awareness, system resilience, trustworthiness,\ncorrigibility, partial transparency, functional autonomy, and moral\nimagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating\nMoral Agency through Large Language Systems), aim to steer AMAs toward greater\nalignment and beneficial societal integration in the coming years. We\nillustrate these criteria using hypothetical scenarios involving an autonomous\npublic bus (APB) to demonstrate their practical applicability in morally\nsalient contexts.",
      "pdf_url": "http://arxiv.org/pdf/2507.13175v1",
      "published": "2025-07-17T14:39:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13175v1",
      "categories": [
        "cs.AI",
        "68T27, 03B42 68T27, 03B4268T27, 03B42 68T27, 03B42 68T27, 03B42\n  68T27, 03B42 68T27, 03B42 68T27, 03B4268T27, 03B42",
        "I.2.0; I.2.9; K.4.1"
      ]
    },
    {
      "title": "Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback",
      "authors": [
        "Suzie Kim",
        "Hye-Bin Shin",
        "Seong-Whan Lee"
      ],
      "abstract": "Conventional reinforcement learning (RL) ap proaches often struggle to learn\neffective policies under sparse reward conditions, necessitating the manual\ndesign of complex, task-specific reward functions. To address this limitation,\nrein forcement learning from human feedback (RLHF) has emerged as a promising\nstrategy that complements hand-crafted rewards with human-derived evaluation\nsignals. However, most existing RLHF methods depend on explicit feedback\nmechanisms such as button presses or preference labels, which disrupt the\nnatural interaction process and impose a substantial cognitive load on the\nuser. We propose a novel reinforcement learning from implicit human feedback\n(RLIHF) framework that utilizes non-invasive electroencephalography (EEG)\nsignals, specifically error-related potentials (ErrPs), to provide continuous,\nimplicit feedback without requiring explicit user intervention. The proposed\nmethod adopts a pre-trained decoder to transform raw EEG signals into\nprobabilistic reward components, en abling effective policy learning even in\nthe presence of sparse external rewards. We evaluate our approach in a\nsimulation environment built on the MuJoCo physics engine, using a Kinova Gen2\nrobotic arm to perform a complex pick-and-place task that requires avoiding\nobstacles while manipulating target objects. The results show that agents\ntrained with decoded EEG feedback achieve performance comparable to those\ntrained with dense, manually designed rewards. These findings validate the\npotential of using implicit neural feedback for scalable and human-aligned\nreinforcement learning in interactive robotics.",
      "pdf_url": "http://arxiv.org/pdf/2507.13171v1",
      "published": "2025-07-17T14:35:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13171v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks",
      "authors": [
        "Kutub Uddin",
        "Awais Khan",
        "Muhammad Umar Farooq",
        "Khalid Malik"
      ],
      "abstract": "Audio plays a crucial role in applications like speaker verification,\nvoice-enabled smart devices, and audio conferencing. However, audio\nmanipulations, such as deepfakes, pose significant risks by enabling the spread\nof misinformation. Our empirical analysis reveals that existing methods for\ndetecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,\nparticularly those attacked using generative adversarial networks. In this\narticle, we propose a novel collaborative learning method called SHIELD to\ndefend against generative AF attacks. To expose AF signatures, we integrate an\nauxiliary generative model, called the defense (DF) generative model, which\nfacilitates collaborative learning by combining input and output. Furthermore,\nwe design a triplet model to capture correlations for real and AF attacked\naudios with real-generated and attacked-generated audios using auxiliary\ngenerative models. The proposed SHIELD strengthens the defense against\ngenerative AF attacks and achieves robust performance across various generative\nmodels. The proposed AF significantly reduces the average detection accuracy\nfrom 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,\nand from 98.41% to 51.18% for HalfTruth for three different generative models.\nThe proposed SHIELD mechanism is robust against AF attacks and achieves an\naverage accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,\nand 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and\nHalfTruth datasets, respectively.",
      "pdf_url": "http://arxiv.org/pdf/2507.13170v1",
      "published": "2025-07-17T14:33:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13170v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "Prompt Injection 2.0: Hybrid AI Threats",
      "authors": [
        "Jeremy McHugh",
        "Kristina Šekrst",
        "Jon Cefalu"
      ],
      "abstract": "Prompt injection attacks, where malicious input is designed to manipulate AI\nsystems into ignoring their original instructions and following unauthorized\ncommands instead, were first discovered by Preamble, Inc. in May 2022 and\nresponsibly disclosed to OpenAI. Over the last three years, these attacks have\ncontinued to pose a critical security threat to LLM-integrated systems. The\nemergence of agentic AI systems, where LLMs autonomously perform multistep\ntasks through tools and coordination with other agents, has fundamentally\ntransformed the threat landscape. Modern prompt injection attacks can now\ncombine with traditional cybersecurity exploits to create hybrid threats that\nsystematically evade traditional security controls. This paper presents a\ncomprehensive analysis of Prompt Injection 2.0, examining how prompt injections\nintegrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),\nand other web security vulnerabilities to bypass traditional security measures.\nWe build upon Preamble's foundational research and mitigation technologies,\nevaluating them against contemporary threats, including AI worms, multi-agent\ninfections, and hybrid cyber-AI attacks. Our analysis incorporates recent\nbenchmarks that demonstrate how traditional web application firewalls, XSS\nfilters, and CSRF tokens fail against AI-enhanced attacks. We also present\narchitectural solutions that combine prompt isolation, runtime security, and\nprivilege separation with novel threat detection capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2507.13169v1",
      "published": "2025-07-17T14:33:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13169v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models",
      "authors": [
        "Arian Mousakhan",
        "Sudhanshu Mittal",
        "Silvio Galesso",
        "Karim Farid",
        "Thomas Brox"
      ],
      "abstract": "Existing world models for autonomous driving struggle with long-horizon\ngeneration and generalization to challenging scenarios. In this work, we\ndevelop a model using simple design choices, and without additional supervision\nor sensors, such as maps, depth, or multiple cameras. We show that our model\nyields state-of-the-art performance, despite having only 469M parameters and\nbeing trained on 280h of video data. It particularly stands out in difficult\nscenarios like turning maneuvers and urban traffic. We test whether discrete\ntoken models possibly have advantages over continuous models based on flow\nmatching. To this end, we set up a hybrid tokenizer that is compatible with\nboth approaches and allows for a side-by-side comparison. Our study concludes\nin favor of the continuous autoregressive model, which is less brittle on\nindividual design choices and more powerful than the model built on discrete\ntokens. Code, models and qualitative results are publicly available at\nhttps://lmb-freiburg.github.io/orbis.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2507.13162v1",
      "published": "2025-07-17T14:29:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13162v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities",
      "authors": [
        "Hao Sun",
        "Mihaela van der Schaar"
      ],
      "abstract": "In the era of Large Language Models (LLMs), alignment has emerged as a\nfundamental yet challenging problem in the pursuit of more reliable,\ncontrollable, and capable machine intelligence. The recent success of reasoning\nmodels and conversational AI systems has underscored the critical role of\nreinforcement learning (RL) in enhancing these systems, driving increased\nresearch interest at the intersection of RL and LLM alignment. This paper\nprovides a comprehensive review of recent advances in LLM alignment through the\nlens of inverse reinforcement learning (IRL), emphasizing the distinctions\nbetween RL techniques employed in LLM alignment and those in conventional RL\ntasks. In particular, we highlight the necessity of constructing neural reward\nmodels from human data and discuss the formal and practical implications of\nthis paradigm shift. We begin by introducing fundamental concepts in RL to\nprovide a foundation for readers unfamiliar with the field. We then examine\nrecent advances in this research agenda, discussing key challenges and\nopportunities in conducting IRL for LLM alignment. Beyond methodological\nconsiderations, we explore practical aspects, including datasets, benchmarks,\nevaluation metrics, infrastructure, and computationally efficient training and\ninference techniques. Finally, we draw insights from the literature on\nsparse-reward RL to identify open questions and potential research directions.\nBy synthesizing findings from diverse studies, we aim to provide a structured\nand critical overview of the field, highlight unresolved challenges, and\noutline promising future directions for improving LLM alignment through RL and\nIRL techniques.",
      "pdf_url": "http://arxiv.org/pdf/2507.13158v1",
      "published": "2025-07-17T14:22:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13158v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models",
      "authors": [
        "Xiangyu Dong",
        "Haoran Zhao",
        "Jiang Gao",
        "Haozhou Li",
        "Xiaoguang Ma",
        "Yaoming Zhou",
        "Fuhai Chen",
        "Juan Liu"
      ],
      "abstract": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.",
      "pdf_url": "http://arxiv.org/pdf/2507.13152v1",
      "published": "2025-07-17T14:13:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13152v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model",
      "authors": [
        "Maulana Bisyir Azhari",
        "David Hyunchul Shim"
      ],
      "abstract": "Learning-based monocular visual odometry (VO) poses robustness,\ngeneralization, and efficiency challenges in robotics. Recent advances in\nvisual foundation models, such as DINOv2, have improved robustness and\ngeneralization in various vision tasks, yet their integration in VO remains\nlimited due to coarse feature granularity. In this paper, we present DINO-VO, a\nfeature-based VO system leveraging DINOv2 visual foundation model for its\nsparse feature matching. To address the integration challenge, we propose a\nsalient keypoints detector tailored to DINOv2's coarse features. Furthermore,\nwe complement DINOv2's robust-semantic features with fine-grained geometric\nfeatures, resulting in more localizable representations. Finally, a\ntransformer-based matcher and differentiable pose estimation layer enable\nprecise camera motion estimation by learning good matches. Against prior\ndetector-descriptor networks like SuperPoint, DINO-VO demonstrates greater\nrobustness in challenging environments. Furthermore, we show superior accuracy\nand generalization of the proposed feature descriptors against standalone\nDINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on\nthe TartanAir and KITTI datasets and is competitive on EuRoC dataset, while\nrunning efficiently at 72 FPS with less than 1GB of memory usage on a single\nGPU. Moreover, it performs competitively against Visual SLAM systems on outdoor\ndriving scenarios, showcasing its generalization capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2507.13145v1",
      "published": "2025-07-17T14:09:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13145v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "From Roots to Rewards: Dynamic Tree Reasoning with RL",
      "authors": [
        "Ahmed Bahloul",
        "Simon Malberg"
      ],
      "abstract": "Modern language models address complex questions through chain-of-thought\n(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,\n2021), yet struggle with error propagation and knowledge integration.\nTree-structured reasoning methods, particularly the Probabilistic\nTree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues\nby decomposing questions into hierarchical structures and selecting answers\nthrough confidence-weighted aggregation of parametric and retrieved knowledge\n(Yao et al., 2023). However, ProbTree's static implementation introduces two\nkey limitations: (1) the reasoning tree is fixed during the initial\nconstruction phase, preventing dynamic adaptation to intermediate results, and\n(2) each node requires exhaustive evaluation of all possible solution\nstrategies, creating computational inefficiency. We present a dynamic\nreinforcement learning (Sutton and Barto, 2018) framework that transforms\ntree-based reasoning into an adaptive process. Our approach incrementally\nconstructs the reasoning tree based on real-time confidence estimates, while\nlearning optimal policies for action selection (decomposition, retrieval, or\naggregation). This maintains ProbTree's probabilistic rigor while improving\nboth solution quality and computational efficiency through selective expansion\nand focused resource allocation. The work establishes a new paradigm for\ntreestructured reasoning that balances the reliability of probabilistic\nframeworks with the flexibility required for real-world question answering\nsystems.",
      "pdf_url": "http://arxiv.org/pdf/2507.13142v2",
      "published": "2025-07-17T14:06:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13142v2",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data",
      "authors": [
        "Junseong Lee",
        "Jaegwan Cho",
        "Yoonju Cho",
        "Seoyoon Choi",
        "Yejin Shin"
      ],
      "abstract": "The study \"Prediction of Highway Traffic Flow Based on Artificial\nIntelligence Algorithms Using California Traffic Data\" presents a machine\nlearning-based traffic flow prediction model to address global traffic\ncongestion issues. The research utilized 30-second interval traffic data from\nCalifornia Highway 78 over a five-month period from July to November 2022,\nanalyzing a 7.24 km westbound section connecting \"Melrose Dr\" and \"El-Camino\nReal\" in the San Diego area. The study employed Multiple Linear Regression\n(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals\nranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance\nmetrics, the analysis revealed that both MLR and RF models performed optimally\nwith 10-minute data collection intervals. These findings are expected to\ncontribute to future traffic congestion solutions and efficient traffic\nmanagement.",
      "pdf_url": "http://arxiv.org/pdf/2507.13112v1",
      "published": "2025-07-17T13:27:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13112v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training",
      "authors": [
        "Adithyavairavan Murali",
        "Balakumar Sundaralingam",
        "Yu-Wei Chao",
        "Wentao Yuan",
        "Jun Yamada",
        "Mark Carlson",
        "Fabio Ramos",
        "Stan Birchfield",
        "Dieter Fox",
        "Clemens Eppner"
      ],
      "abstract": "Grasping is a fundamental robot skill, yet despite significant research\nadvancements, learning-based 6-DOF grasping approaches are still not turnkey\nand struggle to generalize across different embodiments and in-the-wild\nsettings. We build upon the recent success on modeling the object-centric grasp\ngeneration process as an iterative diffusion process. Our proposed framework,\nGraspGen, consists of a DiffusionTransformer architecture that enhances grasp\ngeneration, paired with an efficient discriminator to score and filter sampled\ngrasps. We introduce a novel and performant on-generator training recipe for\nthe discriminator. To scale GraspGen to both objects and grippers, we release a\nnew simulated dataset consisting of over 53 million grasps. We demonstrate that\nGraspGen outperforms prior methods in simulations with singulated objects\nacross different grippers, achieves state-of-the-art performance on the\nFetchBench grasping benchmark, and performs well on a real robot with noisy\nvisual observations.",
      "pdf_url": "http://arxiv.org/pdf/2507.13097v1",
      "published": "2025-07-17T13:09:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13097v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "MUPAX: Multidimensional Problem Agnostic eXplainable AI",
      "authors": [
        "Vincenzo Dentamaro",
        "Felice Franchini",
        "Giuseppe Pirlo",
        "Irina Voiculescu"
      ],
      "abstract": "Robust XAI techniques should ideally be simultaneously deterministic, model\nagnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM\nAGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability\ntechnique, with guaranteed convergency. MUPAX measure theoretic formulation\ngives principled feature importance attribution through structured perturbation\nanalysis that discovers inherent input patterns and eliminates spurious\nrelationships. We evaluate MUPAX on an extensive range of data modalities and\ntasks: audio classification (1D), image classification (2D), volumetric medical\nimage analysis (3D), and anatomical landmark detection, demonstrating dimension\nagnostic effectiveness. The rigorous convergence guarantees extend to any loss\nfunction and arbitrary dimensions, making MUPAX applicable to virtually any\nproblem context for AI. By contrast with other XAI methods that typically\ndecrease performance when masking, MUPAX not only preserves but actually\nenhances model accuracy by capturing only the most important patterns of the\noriginal data. Extensive benchmarking against the state of the XAI art\ndemonstrates MUPAX ability to generate precise, consistent and understandable\nexplanations, a crucial step towards explainable and trustworthy AI systems.\nThe source code will be released upon publication.",
      "pdf_url": "http://arxiv.org/pdf/2507.13090v1",
      "published": "2025-07-17T12:59:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13090v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities",
      "authors": [
        "Liuyi Wang",
        "Xinyuan Xia",
        "Hui Zhao",
        "Hanqing Wang",
        "Tai Wang",
        "Yilun Chen",
        "Chengju Liu",
        "Qijun Chen",
        "Jiangmiao Pang"
      ],
      "abstract": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2507.13019v1",
      "published": "2025-07-17T11:46:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13019v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming",
      "authors": [
        "Roger Xavier Lera-Leri",
        "Filippo Bistaffa",
        "Athina Georgara",
        "Juan Antonio Rodriguez-Aguilar"
      ],
      "abstract": "Following the recent push for trustworthy AI, there has been an increasing\ninterest in developing contrastive explanation techniques for optimisation,\nespecially concerning the solution of specific decision-making processes\nformalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic\napproach for building contrastive explanations for MILPs based on constraint\nreasoning techniques. First, we show how to encode the queries a user makes\nabout the solution of an MILP problem as additional constraints. Then, we\ndetermine the reasons that constitute the answer to the user's query by\ncomputing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set\nof constraints. Finally, we represent our explanation as a \"graph of reasons\"\nconstructed from the IIS, which helps the user understand the structure among\nthe reasons that answer their query. We test our method on instances of\nwell-known optimisation problems to evaluate the empirical hardness of\ncomputing explanations.",
      "pdf_url": "http://arxiv.org/pdf/2507.13007v1",
      "published": "2025-07-17T11:25:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13007v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SMART: Relation-Aware Learning of Geometric Representations for Knowledge Graphs",
      "authors": [
        "Kossi Amouzouvi",
        "Bowen Song",
        "Andrea Coletta",
        "Luigi Bellomarini",
        "Jens Lehmann",
        "Sahar Vahdati"
      ],
      "abstract": "Knowledge graph representation learning approaches provide a mapping between\nsymbolic knowledge in the form of triples in a knowledge graph (KG) and their\nfeature vectors. Knowledge graph embedding (KGE) models often represent\nrelations in a KG as geometric transformations. Most state-of-the-art (SOTA)\nKGE models are derived from elementary geometric transformations (EGTs), such\nas translation, scaling, rotation, and reflection, or their combinations. These\ngeometric transformations enable the models to effectively preserve specific\nstructural and relational patterns of the KG. However, the current use of EGTs\nby KGEs remains insufficient without considering relation-specific\ntransformations. Although recent models attempted to address this problem by\nensembling SOTA baseline models in different ways, only a single or composite\nversion of geometric transformations are used by such baselines to represent\nall the relations. In this paper, we propose a framework that evaluates how\nwell each relation fits with different geometric transformations. Based on this\nranking, the model can: (1) assign the best-matching transformation to each\nrelation, or (2) use majority voting to choose one transformation type to apply\nacross all relations. That is, the model learns a single relation-specific EGT\nin low dimensional vector space through an attention mechanism. Furthermore, we\nuse the correlation between relations and EGTs, which are learned in a low\ndimension, for relation embeddings in a high dimensional vector space. The\neffectiveness of our models is demonstrated through comprehensive evaluations\non three benchmark KGs as well as a real-world financial KG, witnessing a\nperformance comparable to leading models",
      "pdf_url": "http://arxiv.org/pdf/2507.13001v1",
      "published": "2025-07-17T11:18:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.13001v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Teach Old SAEs New Domain Tricks with Boosting",
      "authors": [
        "Nikita Koriagin",
        "Yaroslav Aksenov",
        "Daniil Laptev",
        "Gleb Gerasimov",
        "Nikita Balagansky",
        "Daniil Gavrilov"
      ],
      "abstract": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2507.12990v1",
      "published": "2025-07-17T10:57:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12990v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A Translation of Probabilistic Event Calculus into Markov Decision Processes",
      "authors": [
        "Lyris Xu",
        "Fabio Aurelio D'Asaro",
        "Luke Dickens"
      ],
      "abstract": "Probabilistic Event Calculus (PEC) is a logical framework for reasoning about\nactions and their effects in uncertain environments, which enables the\nrepresentation of probabilistic narratives and computation of temporal\nprojections. The PEC formalism offers significant advantages in\ninterpretability and expressiveness for narrative reasoning. However, it lacks\nmechanisms for goal-directed reasoning. This paper bridges this gap by\ndeveloping a formal translation of PEC domains into Markov Decision Processes\n(MDPs), introducing the concept of \"action-taking situations\" to preserve PEC's\nflexible action semantics. The resulting PEC-MDP formalism enables the\nextensive collection of algorithms and theoretical tools developed for MDPs to\nbe applied to PEC's interpretable narrative domains. We demonstrate how the\ntranslation supports both temporal reasoning tasks and objective-driven\nplanning, with methods for mapping learned policies back into human-readable\nPEC representations, maintaining interpretability while extending PEC's\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2507.12989v1",
      "published": "2025-07-17T10:56:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12989v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps",
      "authors": [
        "Maximiliano Hormazábal Lagos",
        "Álvaro Bueno Sáez",
        "Héctor Cerezo-Costas",
        "Pedro Alonso Doval",
        "Jorge Alcalde Vesteiro"
      ],
      "abstract": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas\ny Respuestas sobre Tablas en Espa\\~nol (Questions and Answers about Tables in\nSpanish). Our solution obtains answers to the questions by implementing Python\ncode generation with LLMs that is used to filter and process the table. This\nsolution evolves from the MRT implementation for the Semeval 2025 related task.\nThe process consists of multiple steps: analyzing and understanding the content\nof the table, selecting the useful columns, generating instructions in natural\nlanguage, translating these instructions to code, running it, and handling\npotential errors or exceptions. These steps use open-source LLMs and\nfine-grained optimized prompts for each step. With this approach, we achieved\nan accuracy score of 85\\% in the task.",
      "pdf_url": "http://arxiv.org/pdf/2507.12981v1",
      "published": "2025-07-17T10:33:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12981v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints",
      "authors": [
        "Youssef Tawfilis",
        "Hossam Amer",
        "Minar El-Aasser",
        "Tallal Elshabrawy"
      ],
      "abstract": "Federated Learning has gained increasing attention for its ability to enable\nmultiple nodes to collaboratively train machine learning models without sharing\ntheir raw data. At the same time, Generative AI -- particularly Generative\nAdversarial Networks (GANs) -- have achieved remarkable success across a wide\nrange of domains, such as healthcare, security, and Image Generation. However,\ntraining generative models typically requires large datasets and significant\ncomputational resources, which are often unavailable in real-world settings.\nAcquiring such resources can be costly and inefficient, especially when many\nunderutilized devices -- such as IoT devices and edge devices -- with varying\ncapabilities remain idle. Moreover, obtaining large datasets is challenging due\nto privacy concerns and copyright restrictions, as most devices are unwilling\nto share their data. To address these challenges, we propose a novel approach\nfor decentralized GAN training that enables the utilization of distributed data\nand underutilized, low-capability devices while not sharing data in its raw\nform. Our approach is designed to tackle key challenges in decentralized\nenvironments, combining KLD-weighted Clustered Federated Learning to address\nthe issues of data heterogeneity and multi-domain datasets, with Heterogeneous\nU-Shaped split learning to tackle the challenge of device heterogeneity under\nstrict data sharing constraints -- ensuring that no labels or raw data, whether\nreal or synthetic, are ever shared between nodes. Experimental results shows\nthat our approach demonstrates consistent and significant improvements across\nkey performance metrics, where it achieves 1.1x -- 2.2x higher image generation\nscores, an average 10% boost in classification metrics (up to 50% in\nmulti-domain non-IID settings), in much lower latency compared to several\nbenchmarks. Find our code at https://github.com/youssefga28/HuSCF-GAN.",
      "pdf_url": "http://arxiv.org/pdf/2507.12979v1",
      "published": "2025-07-17T10:31:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12979v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Demographic-aware fine-grained classification of pediatric wrist fractures",
      "authors": [
        "Ammar Ahmed",
        "Ali Shariq Imran",
        "Zenun Kastrati",
        "Sher Muhammad Daudpota"
      ],
      "abstract": "Wrist pathologies are frequently observed, particularly among children who\nconstitute the majority of fracture cases. However, diagnosing these conditions\nis time-consuming and requires specialized expertise. Computer vision presents\na promising avenue, contingent upon the availability of extensive datasets, a\nnotable challenge in medical imaging. Therefore, reliance solely on one\nmodality, such as images, proves inadequate, especially in an era of diverse\nand plentiful data types. In this study, we employ a multifaceted approach to\naddress the challenge of recognizing wrist pathologies using an extremely\nlimited dataset. Initially, we approach the problem as a fine-grained\nrecognition task, aiming to identify subtle X-ray pathologies that conventional\nCNNs overlook. Secondly, we enhance network performance by fusing patient\nmetadata with X-ray images. Thirdly, rather than pre-training on a\ncoarse-grained dataset like ImageNet, we utilize weights trained on a\nfine-grained dataset. While metadata integration has been used in other medical\ndomains, this is a novel application for wrist pathologies. Our results show\nthat a fine-grained strategy and metadata integration improve diagnostic\naccuracy by 2% with a limited dataset and by over 10% with a larger\nfracture-focused dataset.",
      "pdf_url": "http://arxiv.org/pdf/2507.12964v2",
      "published": "2025-07-17T10:03:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12964v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an Application on the DermaMNIST Dataset",
      "authors": [
        "Nerma Kadric",
        "Amila Akagic",
        "Medina Kapo"
      ],
      "abstract": "Pigmented skin lesions represent localized areas of increased melanin and can\nindicate serious conditions like melanoma, a major contributor to skin cancer\nmortality. The MedMNIST v2 dataset, inspired by MNIST, was recently introduced\nto advance research in biomedical imaging and includes DermaMNIST, a dataset\nfor classifying pigmented lesions based on the HAM10000 dataset. This study\nassesses ResNet-50 and EfficientNetV2L models for multi-class classification\nusing DermaMNIST, employing transfer learning and various layer configurations.\nOne configuration achieves results that match or surpass existing methods. This\nstudy suggests that convolutional neural networks (CNNs) can drive progress in\nbiomedical image analysis, significantly enhancing diagnostic accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2507.12961v1",
      "published": "2025-07-17T10:00:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12961v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets",
      "authors": [
        "Zhichao Sheng",
        "Shilin Zhou",
        "Chen Gong",
        "Zhenghua Li"
      ],
      "abstract": "Spoken Language Understanding (SLU) plays a crucial role in speech-centric\nmultimedia applications, enabling machines to comprehend spoken language in\nscenarios such as meetings, interviews, and customer service interactions. SLU\nencompasses multiple tasks, including Automatic Speech Recognition (ASR),\nspoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).\nHowever, existing methods often rely on separate model architectures for\nindividual tasks such as spoken NER and SA, which increases system complexity,\nlimits cross-task interaction, and fails to fully exploit heterogeneous\ndatasets available across tasks. To address these limitations, we propose\nUniSLU, a unified framework that jointly models multiple SLU tasks within a\nsingle architecture. Specifically, we propose a unified representation for\ndiverse SLU tasks, enabling full utilization of heterogeneous datasets across\nmultiple tasks. Built upon this representation, we propose a unified generative\nmethod that jointly models ASR, spoken NER, and SA tasks, enhancing task\ninteractions and enabling seamless integration with large language models to\nharness their powerful generative capabilities. Extensive experiments on public\nSLU datasets demonstrate the effectiveness of our approach, achieving superior\nSLU performance compared to several benchmark methods, making it well-suited\nfor real-world speech-based multimedia scenarios. We will release all code and\nmodels at github to facilitate future research.",
      "pdf_url": "http://arxiv.org/pdf/2507.12951v1",
      "published": "2025-07-17T09:45:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12951v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "cs.SD"
      ]
    },
    {
      "title": "MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration",
      "authors": [
        "Shirui Zhao",
        "Jun Yin",
        "Lingyun Yao",
        "Martin Andraud",
        "Wannes Meert",
        "Marian Verhelst"
      ],
      "abstract": "An increasing number of applications are exploiting sampling-based algorithms\nfor planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)\nalgorithms form the computational backbone of this emerging branch of machine\nlearning. Unfortunately, the high computational cost limits their feasibility\nfor large-scale problems and real-world applications, and the existing MCMC\nacceleration solutions are either limited in hardware flexibility or fail to\nmaintain efficiency at the system level across a variety of end-to-end\napplications. This paper introduces \\textbf{MC$^2$A}, an algorithm-hardware\nco-design framework, enabling efficient and flexible optimization for MCMC\nacceleration. Firstly, \\textbf{MC$^2$A} analyzes the MCMC workload diversity\nthrough an extension of the processor performance roofline model with a 3rd\ndimension to derive the optimal balance between the compute, sampling and\nmemory parameters. Secondly, \\textbf{MC$^2$A} proposes a parametrized hardware\naccelerator architecture with flexible and efficient support of MCMC kernels\nwith a pipeline of ISA-programmable tree-structured processing units,\nreconfigurable samplers and a crossbar interconnect to support irregular\naccess. Thirdly, the core of \\textbf{MC$^2$A} is powered by a novel Gumbel\nsampler that eliminates exponential and normalization operations. In the\nend-to-end case study, \\textbf{MC$^2$A} achieves an overall {$307.6\\times$,\n$1.4\\times$, $2.0\\times$, $84.2\\times$} speedup compared to the CPU, GPU, TPU\nand state-of-the-art MCMC accelerator. Evaluated on various representative MCMC\nworkloads, this work demonstrates and exploits the feasibility of general\nhardware acceleration to popularize MCMC-based solutions in diverse application\ndomains.",
      "pdf_url": "http://arxiv.org/pdf/2507.12935v1",
      "published": "2025-07-17T09:20:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12935v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ]
    },
    {
      "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization",
      "authors": [
        "Dongyeun Lee",
        "Jiwan Hur",
        "Hyounguk Shon",
        "Jae Young Lee",
        "Junmo Kim"
      ],
      "abstract": "Diffusion models have achieved remarkable success in image generation but\ncome with significant computational costs, posing challenges for deployment in\nresource-constrained environments. Recent post-training quantization (PTQ)\nmethods have attempted to mitigate this issue by focusing on the iterative\nnature of diffusion models. However, these approaches often overlook outliers,\nleading to degraded performance at low bit-widths. In this paper, we propose a\nDMQ which combines Learned Equivalent Scaling (LES) and channel-wise\nPower-of-Two Scaling (PTS) to effectively address these challenges. Learned\nEquivalent Scaling optimizes channel-wise scaling factors to redistribute\nquantization difficulty between weights and activations, reducing overall\nquantization error. Recognizing that early denoising steps, despite having\nsmall quantization errors, crucially impact the final output due to error\naccumulation, we incorporate an adaptive timestep weighting scheme to\nprioritize these critical steps during learning. Furthermore, identifying that\nlayers such as skip connections exhibit high inter-channel variance, we\nintroduce channel-wise Power-of-Two Scaling for activations. To ensure robust\nselection of PTS factors even with small calibration set, we introduce a voting\nalgorithm that enhances reliability. Extensive experiments demonstrate that our\nmethod significantly outperforms existing works, especially at low bit-widths\nsuch as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image\ngeneration quality and model stability. The code is available at\nhttps://github.com/LeeDongYeun/dmq.",
      "pdf_url": "http://arxiv.org/pdf/2507.12933v1",
      "published": "2025-07-17T09:15:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12933v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Making Language Model a Hierarchical Classifier and Generator",
      "authors": [
        "Yihong Wang",
        "Zhonglin Jiang",
        "Ningyuan Xi",
        "Yue Zhao",
        "Qingqing Gu",
        "Xiyuan Chen",
        "Hao Wu",
        "Sheng Xu",
        "Hange Zhou",
        "Yong Chen",
        "Luo Ji"
      ],
      "abstract": "Decoder-only language models, such as GPT and LLaMA, generally decode on the\nlast layer. Motivated by human's hierarchical thinking capability, we propose\nthat a hierarchical decoder architecture could be built with different layers\ndecoding texts simultaneously. Due to limited time and computationally\nresources, we choose to adapt a pretrained language model into this form of\nhierarchical decoder. Language heads of the last layer are copied to different\nselected intermediate layers, and fine-tuned with different task inputs. By\nthorough experiments, we validate that these selective intermediate layers\ncould be adapted to speak meaningful and reasonable contents, and this paradigm\nof hierarchical decoder can obtain state-of-the-art performances on multiple\ntasks such as hierarchical text classification, classification-guided\ngeneration, and hierarchical text generation. This study suggests the\npossibility of a generalized hierarchical reasoner, pretraining from scratch.",
      "pdf_url": "http://arxiv.org/pdf/2507.12930v1",
      "published": "2025-07-17T09:09:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12930v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models",
      "authors": [
        "Yifan Xu",
        "Chao Zhang",
        "Hanqi Jiang",
        "Xiaoyan Wang",
        "Ruifei Ma",
        "Yiwei Li",
        "Zihao Wu",
        "Zeju Li",
        "Xiangde Liu"
      ],
      "abstract": "Advancements in foundation models have made it possible to conduct\napplications in various downstream tasks. Especially, the new era has witnessed\na remarkable capability to extend Large Language Models (LLMs) for tackling\ntasks of 3D scene understanding. Current methods rely heavily on 3D point\nclouds, but the 3D point cloud reconstruction of an indoor scene often results\nin information loss. Some textureless planes or repetitive patterns are prone\nto omission and manifest as voids within the reconstructed 3D point clouds.\nBesides, objects with complex structures tend to introduce distortion of\ndetails caused by misalignments between the captured images and the dense\nreconstructed point clouds. 2D multi-view images present visual consistency\nwith 3D point clouds and provide more detailed representations of scene\ncomponents, which can naturally compensate for these deficiencies. Based on\nthese insights, we propose Argus, a novel 3D multimodal framework that\nleverages multi-view images for enhanced 3D scene understanding with LLMs. In\ngeneral, Argus can be treated as a 3D Large Multimodal Foundation Model\n(3D-LMM) since it takes various modalities as input(text instructions, 2D\nmulti-view images, and 3D point clouds) and expands the capability of LLMs to\ntackle 3D tasks. Argus involves fusing and integrating multi-view images and\ncamera poses into view-as-scene features, which interact with the 3D features\nto create comprehensive and detailed 3D-aware scene embeddings. Our approach\ncompensates for the information loss while reconstructing 3D point clouds and\nhelps LLMs better understand the 3D world. Extensive experiments demonstrate\nthat our method outperforms existing 3D-LMMs in various downstream tasks.",
      "pdf_url": "http://arxiv.org/pdf/2507.12916v1",
      "published": "2025-07-17T09:02:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12916v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "An ultra-low-power CGRA for accelerating Transformers at the edge",
      "authors": [
        "Rohit Prasad"
      ],
      "abstract": "Transformers have revolutionized deep learning with applications in natural\nlanguage processing, computer vision, and beyond. However, their computational\ndemands make it challenging to deploy them on low-power edge devices. This\npaper introduces an ultra-low-power, Coarse-Grained Reconfigurable Array (CGRA)\narchitecture specifically designed to accelerate General Matrix Multiplication\n(GEMM) operations in transformer models tailored for the energy and resource\nconstraints of edge applications. The proposed architecture integrates a 4 x 4\narray of Processing Elements (PEs) for efficient parallel computation and\ndedicated 4 x 2 Memory Operation Blocks (MOBs) for optimized LOAD/STORE\noperations, reducing memory bandwidth demands and enhancing data reuse. A\nswitchless mesh torus interconnect network further minimizes power and latency\nby enabling direct communication between PEs and MOBs, eliminating the need for\ncentralized switching. Through its heterogeneous array design and efficient\ndataflow, this CGRA architecture addresses the unique computational needs of\ntransformers, offering a scalable pathway to deploy sophisticated machine\nlearning models on edge devices.",
      "pdf_url": "http://arxiv.org/pdf/2507.12904v1",
      "published": "2025-07-17T08:43:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12904v1",
      "categories": [
        "cs.AR",
        "cs.AI"
      ]
    },
    {
      "title": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks",
      "authors": [
        "Jian Yao",
        "Ran Cheng",
        "Kay Chen Tan"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) have led to substantial\nimprovements in the mathematical reasoning abilities of large language models\n(LLMs), as measured by standard benchmarks. However, these gains often persist\neven when models are trained with flawed signals, such as random or inverted\nrewards, raising a fundamental question: do such improvements reflect true\nreasoning, or are they merely artifacts of overfitting to benchmark-specific\npatterns? To address this question, we take an evaluation-centric perspective\nand identify two critical shortcomings in existing protocols. First,\n\\emph{benchmark contamination} arises from the public availability of test\nproblems, increasing the risk of data leakage. Second, \\emph{evaluation\nfragility} stems from the reliance on single-instance assessments, which are\nhighly sensitive to stochastic outputs and fail to capture reasoning\nconsistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic\nevaluation framework designed to probe genuine reasoning ability. By converting\nfixed numerical problems into symbolic templates and requiring models to solve\nmultiple instantiations of each, VAR-MATH enforces consistent reasoning across\nstructurally equivalent variants, thereby mitigating contamination and\nimproving evaluation robustness. We apply VAR-MATH to transform two popular\nbenchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and\nVAR-AIME24. Experimental results reveal substantial performance drops for\nRL-trained models on the variabilized versions, especially for smaller models,\nwith average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings\nsuggest that many existing RL methods rely on superficial heuristics and fail\nto generalize beyond specific numerical forms. Overall, VAR-MATH offers a\nprincipled, contamination-resistant evaluation paradigm for mathematical\nreasoning.",
      "pdf_url": "http://arxiv.org/pdf/2507.12885v1",
      "published": "2025-07-17T08:10:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12885v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    }
  ]
}