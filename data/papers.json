{
  "last_updated": "2025-12-21T00:59:22.708542",
  "papers": [
    {
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "authors": [
        "Jinjie Mai",
        "Chaoyang Wang",
        "Guocheng Gordon Qian",
        "Willi Menapace",
        "Sergey Tulyakov",
        "Bernard Ghanem",
        "Peter Wonka",
        "Ashkan Mirzaei"
      ],
      "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "pdf_url": "https://arxiv.org/pdf/2512.16920v1",
      "published": "2025-12-18T18:59:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16920v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DVGT: Driving Visual Geometry Transformer",
      "authors": [
        "Sicheng Zuo",
        "Zixun Xie",
        "Wenzhao Zheng",
        "Shaoqing Xu",
        "Fang Li",
        "Shengyin Jiang",
        "Long Chen",
        "Zhi-Xin Yang",
        "Jiwen Lu"
      ],
      "abstract": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.",
      "pdf_url": "https://arxiv.org/pdf/2512.16919v1",
      "published": "2025-12-18T18:59:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16919v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "authors": [
        "Qihao Liu",
        "Chengzhi Mao",
        "Yaojie Liu",
        "Alan Yuille",
        "Wen-Sheng Chu"
      ],
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
      "pdf_url": "https://arxiv.org/pdf/2512.16921v1",
      "published": "2025-12-18T18:59:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16921v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
      "authors": [
        "Qihao Liu",
        "Luoxin Ye",
        "Wufei Ma",
        "Yu-Cheng Chou",
        "Alan Yuille"
      ],
      "abstract": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2512.16917v1",
      "published": "2025-12-18T18:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16917v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "authors": [
        "Peter Chen",
        "Xiaopeng Li",
        "Ziniu Li",
        "Wotao Yin",
        "Xi Chen",
        "Tianyi Lin"
      ],
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "pdf_url": "https://arxiv.org/pdf/2512.16912v1",
      "published": "2025-12-18T18:59:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16912v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
      "authors": [
        "Andrew Wagenmaker",
        "Perry Dong",
        "Raymond Tsao",
        "Chelsea Finn",
        "Sergey Levine"
      ],
      "abstract": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
      "pdf_url": "https://arxiv.org/pdf/2512.16911v1",
      "published": "2025-12-18T18:59:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16911v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
      "authors": [
        "Mingfei Chen",
        "Yifan Wang",
        "Zhengqin Li",
        "Homanga Bharadhwaj",
        "Yujin Chen",
        "Chuan Qin",
        "Ziyi Kou",
        "Yuan Tian",
        "Eric Whitmire",
        "Rajinder Sodhi",
        "Hrvoje Benko",
        "Eli Shlizerman",
        "Yue Liu"
      ],
      "abstract": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
      "pdf_url": "https://arxiv.org/pdf/2512.16907v1",
      "published": "2025-12-18T18:59:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16907v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Impacts of Racial Bias in Historical Training Data for News AI",
      "authors": [
        "Rahul Bhargava",
        "Malene Hornstrup Jespersen",
        "Emily Boardman Ndulue",
        "Vivica Dsouza"
      ],
      "abstract": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
      "pdf_url": "https://arxiv.org/pdf/2512.16901v1",
      "published": "2025-12-18T18:56:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16901v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation",
      "authors": [
        "Haichao Zhang",
        "Yao Lu",
        "Lichen Wang",
        "Yunzhe Li",
        "Daiwei Chen",
        "Yunpeng Xu",
        "Yun Fu"
      ],
      "abstract": "Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.",
      "pdf_url": "https://arxiv.org/pdf/2512.16891v1",
      "published": "2025-12-18T18:52:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16891v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies",
      "authors": [
        "Astrid Brull",
        "Sara Aguti",
        "Véronique Bolduc",
        "Ying Hu",
        "Daniel M. Jimenez-Gutierrez",
        "Enrique Zuazua",
        "Joaquin Del-Rio",
        "Oleksii Sliusarenko",
        "Haiyan Zhou",
        "Francesco Muntoni",
        "Carsten G. Bönnemann",
        "Xabi Uribe-Etxebarria"
      ],
      "abstract": "The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.",
      "pdf_url": "https://arxiv.org/pdf/2512.16876v1",
      "published": "2025-12-18T18:44:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16876v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC"
      ]
    },
    {
      "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking",
      "authors": [
        "Tomáš Souček",
        "Pierre Fernandez",
        "Hady Elsahar",
        "Sylvestre-Alvise Rebuffi",
        "Valeriu Lacatusu",
        "Tuan Tran",
        "Tom Sander",
        "Alexandre Mourachko"
      ],
      "abstract": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.",
      "pdf_url": "https://arxiv.org/pdf/2512.16874v1",
      "published": "2025-12-18T18:42:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16874v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI",
      "authors": [
        "Otman A. Basir"
      ],
      "abstract": "Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.16873v1",
      "published": "2025-12-18T18:42:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16873v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Sequencing to Mitigate Catastrophic Forgetting in Continual Learning",
      "authors": [
        "Hesham G. Moussa",
        "Aroosa Hameed",
        "Arashmid Akhavain"
      ],
      "abstract": "To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.",
      "pdf_url": "https://arxiv.org/pdf/2512.16871v1",
      "published": "2025-12-18T18:40:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16871v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models",
      "authors": [
        "Jiabin Xue"
      ],
      "abstract": "Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: \"How to determine labels for truly future, unseen data points\". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.",
      "pdf_url": "https://arxiv.org/pdf/2512.16866v1",
      "published": "2025-12-18T18:37:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16866v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
      "authors": [
        "Zihan Zhou",
        "Animesh Garg",
        "Ajay Mandlekar",
        "Caelan Garrett"
      ],
      "abstract": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2512.16861v1",
      "published": "2025-12-18T18:32:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16861v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Distributional AGI Safety",
      "authors": [
        "Nenad Tomašev",
        "Matija Franklin",
        "Julian Jacobs",
        "Sébastien Krier",
        "Simon Osindero"
      ],
      "abstract": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.",
      "pdf_url": "https://arxiv.org/pdf/2512.16856v1",
      "published": "2025-12-18T18:29:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16856v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
      "authors": [
        "Khurram Khalil",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
      "pdf_url": "https://arxiv.org/pdf/2512.16855v1",
      "published": "2025-12-18T18:27:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16855v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation",
      "authors": [
        "Amita Kamath",
        "Kai-Wei Chang",
        "Ranjay Krishna",
        "Luke Zettlemoyer",
        "Yushi Hu",
        "Marjan Ghazvininejad"
      ],
      "abstract": "Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.",
      "pdf_url": "https://arxiv.org/pdf/2512.16853v1",
      "published": "2025-12-18T18:26:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16853v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy",
      "authors": [
        "Ripan Kumar Kundu",
        "Istiak Ahmed",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.",
      "pdf_url": "https://arxiv.org/pdf/2512.16851v1",
      "published": "2025-12-18T18:23:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16851v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Meta-RL Induces Exploration in Language Agents",
      "authors": [
        "Yulun Jiang",
        "Liangze Jiang",
        "Damien Teney",
        "Michael Moor",
        "Maria Brbic"
      ],
      "abstract": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
      "pdf_url": "https://arxiv.org/pdf/2512.16848v1",
      "published": "2025-12-18T18:22:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16848v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference",
      "authors": [
        "Harsh Vardhan Bansal"
      ],
      "abstract": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications",
      "pdf_url": "https://arxiv.org/pdf/2512.16843v1",
      "published": "2025-12-18T18:18:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16843v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
      "authors": [
        "Yuxin Ray Song",
        "Jinzhou Li",
        "Rao Fu",
        "Devin Murphy",
        "Kaichen Zhou",
        "Rishi Shiv",
        "Yaqi Li",
        "Haoyu Xiong",
        "Crystal Elaine Owens",
        "Yilun Du",
        "Yiyue Luo",
        "Xianyi Cheng",
        "Antonio Torralba",
        "Wojciech Matusik",
        "Paul Pu Liang"
      ],
      "abstract": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.",
      "pdf_url": "https://arxiv.org/pdf/2512.16842v1",
      "published": "2025-12-18T18:18:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16842v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Next-Generation License Plate Detection and Recognition System using YOLOv8",
      "authors": [
        "Arslan Amin",
        "Rafia Mumtaz",
        "Muhammad Jawad Bashir",
        "Syed Mohammad Hassan Zaidi"
      ],
      "abstract": "In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.",
      "pdf_url": "https://arxiv.org/pdf/2512.16826v1",
      "published": "2025-12-18T18:06:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16826v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
      "authors": [
        "William English",
        "Dominic Simon",
        "Sumit Kumar Jha",
        "Rickard Ewetz"
      ],
      "abstract": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
      "pdf_url": "https://arxiv.org/pdf/2512.16814v1",
      "published": "2025-12-18T17:55:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16814v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
      "authors": [
        "Bahman Abolhassani",
        "Tugba Erpek",
        "Kemal Davaslioglu",
        "Yalin E. Sagduyu",
        "Sastry Kompella"
      ],
      "abstract": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
      "pdf_url": "https://arxiv.org/pdf/2512.16813v1",
      "published": "2025-12-18T17:54:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16813v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
      "authors": [
        "Shubham Mishra",
        "Samyek Jain",
        "Gorang Mehrishi",
        "Shiv Tiwari",
        "Harsh Sharma",
        "Pratik Narang",
        "Dhruv Kumar"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.",
      "pdf_url": "https://arxiv.org/pdf/2512.16795v1",
      "published": "2025-12-18T17:27:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16795v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ]
    },
    {
      "title": "Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint",
      "authors": [
        "Endar Suprih Wihidayat",
        "Sieteng Soh",
        "Kwan-Wu Chin",
        "Duc-son Pham"
      ],
      "abstract": "In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.16792v1",
      "published": "2025-12-18T17:25:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16792v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals",
      "authors": [
        "Shuting Zhao",
        "Zeyu Xiao",
        "Xinrong Chen"
      ],
      "abstract": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/",
      "pdf_url": "https://arxiv.org/pdf/2512.16791v1",
      "published": "2025-12-18T17:25:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16791v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Mass Spectrum Analysis with ASP",
      "authors": [
        "Nils Küchenmeister",
        "Alex Ivliev",
        "Markus Krötzsch"
      ],
      "abstract": "We present a new use of Answer Set Programming (ASP) to discover the molecular structure of chemical samples based on the relative abundance of elements and structural fragments, as measured in mass spectrometry. To constrain the exponential search space for this combinatorial problem, we develop canonical representations of molecular structures and an ASP implementation that uses these definitions. We evaluate the correctness of our implementation over a large set of known molecular structures, and we compare its quality and performance to other ASP symmetry-breaking methods and to a commercial tool from analytical chemistry. Under consideration in Theory and Practice of Logic Programming (TPLP).",
      "pdf_url": "https://arxiv.org/pdf/2512.16780v1",
      "published": "2025-12-18T17:13:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16780v1",
      "categories": [
        "cs.LO",
        "cs.AI"
      ]
    },
    {
      "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation",
      "authors": [
        "William English",
        "Chase Walker",
        "Dominic Simon",
        "Rickard Ewetz"
      ],
      "abstract": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.",
      "pdf_url": "https://arxiv.org/pdf/2512.16770v1",
      "published": "2025-12-18T17:03:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16770v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?",
      "authors": [
        "Siqi Wang",
        "Chao Liang",
        "Yunfan Gao",
        "Erxin Yu",
        "Sen Li",
        "Yushi Li",
        "Jing Li",
        "Haofen Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., \"I am thirsty\") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling \"last-mile\" navigation challenges.",
      "pdf_url": "https://arxiv.org/pdf/2512.16755v1",
      "published": "2025-12-18T16:53:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16755v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
      "authors": [
        "Claudia Vale Oliveira",
        "Nelson Zagalo",
        "Filipe Silva",
        "Anabela Brandao",
        "Syeda Faryal Hussain Khurrum",
        "Joaquim Santos"
      ],
      "abstract": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
      "pdf_url": "https://arxiv.org/pdf/2512.16750v1",
      "published": "2025-12-18T16:45:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16750v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "TreeNet: A Light Weight Model for Low Bitrate Image Compression",
      "authors": [
        "Mahadev Prasad Panda",
        "Purnachandra Rao Makkena",
        "Srivatsa Prativadibhayankaram",
        "Siegfried Fößel",
        "André Kaup"
      ],
      "abstract": "Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.",
      "pdf_url": "https://arxiv.org/pdf/2512.16743v1",
      "published": "2025-12-18T16:40:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16743v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach",
      "authors": [
        "Yipeng Zhuang",
        "Yifeng Guo",
        "Yuewen Li",
        "Yuheng Wu",
        "Philip Leung-Ho Yu",
        "Tingting Song",
        "Zhiyong Wang",
        "Kunzhong Zhou",
        "Weifang Wang",
        "Li Zhuang"
      ],
      "abstract": "Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.",
      "pdf_url": "https://arxiv.org/pdf/2512.16739v1",
      "published": "2025-12-18T16:37:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16739v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities",
      "authors": [
        "Daniel Bramblett",
        "Rushang Karia",
        "Adrian Ciotinga",
        "Ruthvick Suresh",
        "Pulkit Verma",
        "YooJung Choi",
        "Siddharth Srivastava"
      ],
      "abstract": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.",
      "pdf_url": "https://arxiv.org/pdf/2512.16733v1",
      "published": "2025-12-18T16:32:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16733v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library",
      "authors": [
        "Oliver Stritzel",
        "Nick Hühnerbein",
        "Simon Rauch",
        "Itzel Zarate",
        "Lukas Fleischmann",
        "Moike Buck",
        "Attila Lischka",
        "Christian Frey"
      ],
      "abstract": "In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.",
      "pdf_url": "https://arxiv.org/pdf/2512.16715v1",
      "published": "2025-12-18T16:18:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16715v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems",
      "authors": [
        "Abhisek Ganguly"
      ],
      "abstract": "We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.16707v1",
      "published": "2025-12-18T16:12:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16707v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
      "authors": [
        "Giovanni Adorni"
      ],
      "abstract": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
      "pdf_url": "https://arxiv.org/pdf/2512.16701v1",
      "published": "2025-12-18T16:06:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16701v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning",
      "authors": [
        "Mahbub E Sobhani",
        "Md. Faiyaz Abdullah Sayeedi",
        "Mohammad Nehad Alam",
        "Proma Hossain Progga",
        "Swakkhar Shatabda"
      ],
      "abstract": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver",
      "pdf_url": "https://arxiv.org/pdf/2512.16698v1",
      "published": "2025-12-18T16:00:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16698v1",
      "categories": [
        "cs.AI",
        "cs.CG"
      ]
    },
    {
      "title": "Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm",
      "authors": [
        "Wisnu Uriawan",
        "Achmad Ajie Priyajie",
        "Angga Gustian",
        "Fikri Nur Hidayat",
        "Sendi Ahmad Rafiudin",
        "Muhamad Fikri Zaelani"
      ],
      "abstract": "This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.16694v1",
      "published": "2025-12-18T15:59:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16694v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray",
      "authors": [
        "Gonçalo Gaspar Alves",
        "Shekoufeh Gorgi Zadeh",
        "Andreas Husch",
        "Ben Bausch"
      ],
      "abstract": "Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.",
      "pdf_url": "https://arxiv.org/pdf/2512.16685v1",
      "published": "2025-12-18T15:50:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16685v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance",
      "authors": [
        "Jacob Reiss",
        "Shikshya Shiwakoti",
        "Samuel Goldsmith",
        "Ujjwal Pandit"
      ],
      "abstract": "In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2512.16661v1",
      "published": "2025-12-18T15:29:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16661v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking",
      "authors": [
        "Sangeeth B",
        "Serena Nicolazzo",
        "Deepa K.",
        "Vinod P"
      ],
      "abstract": "The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.",
      "pdf_url": "https://arxiv.org/pdf/2512.16658v1",
      "published": "2025-12-18T15:26:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16658v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Comprehensive AI Literacy: The Case for Centering Human Agency",
      "authors": [
        "Sri Yash Tadimalla",
        "Justin Cary",
        "Gordon Hull",
        "Jordan Register",
        "Daniel Maxwell",
        "David Pugalee",
        "Tina Heafner"
      ],
      "abstract": "The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.",
      "pdf_url": "https://arxiv.org/pdf/2512.16656v1",
      "published": "2025-12-18T15:25:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16656v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
      "authors": [
        "Jirui Yang",
        "Hengqi Guo",
        "Zhihui Lu",
        "Yi Zhao",
        "Yuansen Zhang",
        "Shijing Hu",
        "Qiang Duan",
        "Yinggui Wang",
        "Tao Wei"
      ],
      "abstract": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
      "pdf_url": "https://arxiv.org/pdf/2512.16650v1",
      "published": "2025-12-18T15:22:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16650v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam",
      "authors": [
        "Wisnu Uriawan",
        "Aria Octavian Hamza",
        "Ade Ripaldi Nuralim",
        "Adi Purnama",
        "Ahmad Juaeni Yunus",
        "Anissya Auliani Supriadi Putri"
      ],
      "abstract": "This research presents the implementation of a Sharia-compliant chatbot as an interactive medium for consulting Islamic questions, leveraging Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic embedding to ensure contextual and accurate responses. Utilizing the CRISP-DM methodology, the system processes a curated Islam QA dataset of 25,000 question-answer pairs from authentic sources like the Qur'an, Hadith, and scholarly fatwas, formatted in JSON for flexibility and scalability. The chatbot prototype, developed with a Flask API backend and Flutter-based mobile frontend, achieves 87% semantic accuracy in functional testing across diverse topics including fiqh, aqidah, ibadah, and muamalah, demonstrating its potential to enhance religious literacy, digital da'wah, and access to verified Islamic knowledge in the Industry 4.0 era. While effective for closed-domain queries, limitations such as static learning and dataset dependency highlight opportunities for future enhancements like continuous adaptation and multi-turn conversation support, positioning this innovation as a bridge between traditional Islamic scholarship and modern AI-driven consultation.",
      "pdf_url": "https://arxiv.org/pdf/2512.16644v1",
      "published": "2025-12-18T15:15:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16644v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game",
      "authors": [
        "Barna Pásztor",
        "Thomas Kleine Buening",
        "Andreas Krause"
      ],
      "abstract": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.",
      "pdf_url": "https://arxiv.org/pdf/2512.16626v1",
      "published": "2025-12-18T15:03:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16626v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "cs.MA",
        "stat.ML"
      ]
    },
    {
      "title": "Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents",
      "authors": [
        "Giulia Boato",
        "Andrea Montibeller",
        "Edward Delp",
        "Luisa Verdoliva",
        "Daniele Miorandi"
      ],
      "abstract": "AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.",
      "pdf_url": "https://arxiv.org/pdf/2512.16614v1",
      "published": "2025-12-18T14:52:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16614v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ]
    },
    {
      "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
      "authors": [
        "Iker García-Ferrero",
        "David Montero",
        "Roman Orus"
      ],
      "abstract": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
      "pdf_url": "https://arxiv.org/pdf/2512.16602v1",
      "published": "2025-12-18T14:43:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16602v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks",
      "authors": [
        "Shaohua Wu",
        "Tong Yu",
        "Shenling Wang",
        "Xudong Zhao"
      ],
      "abstract": "Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.",
      "pdf_url": "https://arxiv.org/pdf/2512.16586v1",
      "published": "2025-12-18T14:32:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.16586v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}