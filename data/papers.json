{
  "last_updated": "2025-02-23T00:48:33.085614",
  "papers": [
    {
      "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention",
      "authors": [
        "Shang Yang",
        "Junxian Guo",
        "Haotian Tang",
        "Qinghao Hu",
        "Guangxuan Xiao",
        "Jiaming Tang",
        "Yujun Lin",
        "Zhijian Liu",
        "Yao Lu",
        "Song Han"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
      "pdf_url": "http://arxiv.org/pdf/2502.14866v1",
      "published": "2025-02-20T18:59:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14866v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "cs.PF"
      ]
    },
    {
      "title": "Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework",
      "authors": [
        "Yuming Yang",
        "Jiang Zhong",
        "Li Jin",
        "Jingwang Huang",
        "Jingpeng Gao",
        "Qing Liu",
        "Yang Bai",
        "Jingyuan Zhang",
        "Rui Jiang",
        "Kaiwen Wei"
      ],
      "abstract": "Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning\ncapabilities by integrating external knowledge. However, existing benchmarks\nprimarily focus on simple image-text interactions, overlooking complex visual\nformats like charts that are prevalent in real-world applications. In this\nwork, we introduce a novel task, Chart-based MRAG, to address this limitation.\nTo semi-automatically generate high-quality evaluation samples, we propose\nCHARt-based document question-answering GEneration (CHARGE), a framework that\nproduces evaluation data through structured keypoint extraction, crossmodal\nverification, and keypoint-based generation. By combining CHARGE with expert\nvalidation, we construct Chart-MRAG Bench, a comprehensive benchmark for\nchart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8\ndomains from real-world documents. Our evaluation reveals three critical\nlimitations in current approaches: (1) unified multimodal embedding retrieval\nmethods struggles in chart-based scenarios, (2) even with ground-truth\nretrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87%\nCoverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality\nbias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are\nreleased at https://github.com/Nomothings/CHARGE.git.",
      "pdf_url": "http://arxiv.org/pdf/2502.14864v1",
      "published": "2025-02-20T18:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14864v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Interpretable Text Embeddings and Text Similarity Explanation: A Primer",
      "authors": [
        "Juri Opitz",
        "Lucas Möller",
        "Andrianos Michail",
        "Simon Clematide"
      ],
      "abstract": "Text embeddings and text embedding models are a backbone of many AI and NLP\nsystems, particularly those involving search. However, interpretability\nchallenges persist, especially in explaining obtained similarity scores, which\nis crucial for applications requiring transparency. In this paper, we give a\nstructured overview of interpretability methods specializing in explaining\nthose similarity scores, an emerging research area. We study the methods'\nindividual ideas and techniques, evaluating their potential for improving\ninterpretability of text embeddings and explaining predicted similarities.",
      "pdf_url": "http://arxiv.org/pdf/2502.14862v1",
      "published": "2025-02-20T18:59:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14862v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling",
      "authors": [
        "Weilin Zhao",
        "Tengyu Pan",
        "Xu Han",
        "Yudi Zhang",
        "Ao Sun",
        "Yuxiang Huang",
        "Kaihuo Zhang",
        "Weilun Zhao",
        "Yuxuan Li",
        "Jianyong Wang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2.",
      "pdf_url": "http://arxiv.org/pdf/2502.14856v1",
      "published": "2025-02-20T18:58:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14856v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Revealing and Mitigating Over-Attention in Knowledge Editing",
      "authors": [
        "Pinzheng Wang",
        "Zecheng Tang",
        "Keyan Zhou",
        "Juntao Li",
        "Qiaoming Zhu",
        "Min Zhang"
      ],
      "abstract": "Large Language Models have demonstrated superior performance across a wide\nrange of tasks, but they still exhibit undesirable errors due to incorrect\nknowledge learned from the training data. To avoid this, knowledge editing\nmethods emerged to precisely edit the specific model knowledge via efficiently\nmodifying a very small percentage of parameters. % However, those methods can\nlead to the problem of Specificity Failure: when the content related to the\nedited knowledge occurs in the context, it can inadvertently corrupt other\npre-existing knowledge. However, those methods can lead to the problem of\nSpecificity Failure, where the existing knowledge and capabilities are severely\ndegraded due to editing. Our preliminary indicates that Specificity Failure\nprimarily stems from the model's attention heads assigning excessive attention\nscores to entities related to the edited knowledge, thereby unduly focusing on\nspecific snippets within the context, which we denote as the Attention Drift\nphenomenon. To mitigate such Attention Drift issue, we introduce a simple yet\neffective method Selective Attention Drift Restriction}(SADR), which introduces\nan additional regularization term during the knowledge editing process to\nrestrict changes in the attention weight distribution, thereby preventing undue\nfocus on the edited entity. Experiments on five frequently used strong LLMs\ndemonstrate the effectiveness of our method, where SADR can significantly\nmitigate Specificity Failure in the predominant knowledge editing tasks.",
      "pdf_url": "http://arxiv.org/pdf/2502.14838v1",
      "published": "2025-02-20T18:51:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14838v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs",
      "authors": [
        "Tao Ji",
        "Bin Guo",
        "Yuanbin Wu",
        "Qipeng Guo",
        "Lixing Shen",
        "Zhan Chen",
        "Xipeng Qiu",
        "Qi Zhang",
        "Tao Gui"
      ],
      "abstract": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
      "pdf_url": "http://arxiv.org/pdf/2502.14837v1",
      "published": "2025-02-20T18:50:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14837v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
      "authors": [
        "Shangqing Tu",
        "Yucheng Wang",
        "Daniel Zhang-Li",
        "Yushi Bai",
        "Jifan Yu",
        "Yuhao Wu",
        "Lei Hou",
        "Huiqin Liu",
        "Zhiyuan Liu",
        "Bin Xu",
        "Juanzi Li"
      ],
      "abstract": "Existing Large Vision-Language Models (LVLMs) can process inputs with context\nlengths up to 128k visual and text tokens, yet they struggle to generate\ncoherent outputs beyond 1,000 words. We find that the primary limitation is the\nabsence of long output examples during supervised fine-tuning (SFT). To tackle\nthis issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158\nexamples, each with multiple input images, an instruction, and corresponding\noutputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that\nmaintain high-fidelity to the input images, we employ Direct Preference\nOptimization (DPO) to the SFT model. Given the high cost of collecting human\nfeedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which\nbreaks long outputs into segments and uses iterative corrections to form\npreference pairs with the original outputs. Additionally, we develop\nMMLongBench-Write, a benchmark featuring six tasks to evaluate the\nlong-generation capabilities of VLMs. Our 7B parameter model, trained with\nLongWriter-V-22k and IterDPO, achieves impressive performance on this\nbenchmark, outperforming larger proprietary models like GPT-4o. Code and data:\nhttps://github.com/THU-KEG/LongWriter-V",
      "pdf_url": "http://arxiv.org/pdf/2502.14834v1",
      "published": "2025-02-20T18:47:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14834v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Improving the Diffusability of Autoencoders",
      "authors": [
        "Ivan Skorokhodov",
        "Sharath Girish",
        "Benran Hu",
        "Willi Menapace",
        "Yanyu Li",
        "Rameen Abdal",
        "Sergey Tulyakov",
        "Aliaksandr Siarohin"
      ],
      "abstract": "Latent diffusion models have emerged as the leading approach for generating\nhigh-quality images and videos, utilizing compressed latent representations to\nreduce the computational burden of the diffusion process. While recent\nadvancements have primarily focused on scaling diffusion backbones and\nimproving autoencoder reconstruction quality, the interaction between these\ncomponents has received comparatively less attention. In this work, we perform\na spectral analysis of modern autoencoders and identify inordinate\nhigh-frequency components in their latent spaces, which are especially\npronounced in the autoencoders with a large bottleneck channel size. We\nhypothesize that this high-frequency component interferes with the\ncoarse-to-fine nature of the diffusion synthesis process and hinders the\ngeneration quality. To mitigate the issue, we propose scale equivariance: a\nsimple regularization strategy that aligns latent and RGB spaces across\nfrequencies by enforcing scale equivariance in the decoder. It requires minimal\ncode changes and only up to 20K autoencoder fine-tuning steps, yet\nsignificantly improves generation quality, reducing FID by 19% for image\ngeneration on ImageNet-1K 256x256 and FVD by at least 44% for video generation\non Kinetics-700 17x256x256.",
      "pdf_url": "http://arxiv.org/pdf/2502.14831v1",
      "published": "2025-02-20T18:45:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14831v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs",
      "authors": [
        "Danni Liu",
        "Jan Niehues"
      ],
      "abstract": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).",
      "pdf_url": "http://arxiv.org/pdf/2502.14830v1",
      "published": "2025-02-20T18:45:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14830v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Advanced Techniques for Visual Question Answering: A Comprehensive Comparison",
      "authors": [
        "Aiswarya Baby",
        "Tintu Thankom Koshy"
      ],
      "abstract": "Visual Question Answering (VQA) has emerged as a pivotal task in the\nintersection of computer vision and natural language processing, requiring\nmodels to understand and reason about visual content in response to natural\nlanguage questions. Analyzing VQA datasets is essential for developing robust\nmodels that can handle the complexities of multimodal reasoning. Several\napproaches have been developed to examine these datasets, each offering\ndistinct perspectives on question diversity, answer distribution, and\nvisual-textual correlations. Despite significant progress, existing VQA models\nface challenges related to dataset bias, limited model complexity, commonsense\nreasoning gaps, rigid evaluation methods, and generalization to real world\nscenarios. This paper presents a comprehensive comparative study of five\nadvanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling,\nBLIP-2, and OFA, each employing distinct methodologies to address these\nchallenges.",
      "pdf_url": "http://arxiv.org/pdf/2502.14827v1",
      "published": "2025-02-20T18:45:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14827v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ]
    },
    {
      "title": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables",
      "authors": [
        "Luis Antonio Gutiérrez Guanilo",
        "Mir Tafseer Nayeem",
        "Cristian López",
        "Davood Rafiei"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndiverse domains, yet their application in e-commerce remains underexplored due\nto a lack of domain-specific datasets. To address this gap, we introduce\neC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce,\nincluding detailed product attributes and user-specific queries. Leveraging\neC-Tab2Text, we focus on text generation from product tables, enabling LLMs to\nproduce high-quality, attribute-specific product reviews from structured\ntabular data. Fine-tuned models were rigorously evaluated using standard\nTable2Text metrics, alongside correctness, faithfulness, and fluency\nassessments. Our results demonstrate substantial improvements in generating\ncontextually accurate reviews, highlighting the transformative potential of\ntailored datasets and fine-tuning methodologies in optimizing e-commerce\nworkflows. This work highlights the potential of LLMs in e-commerce workflows\nand the essential role of domain-specific datasets in tailoring them to\nindustry-specific challenges.",
      "pdf_url": "http://arxiv.org/pdf/2502.14820v1",
      "published": "2025-02-20T18:41:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14820v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.HC"
      ]
    },
    {
      "title": "Optimizing Model Selection for Compound AI Systems",
      "authors": [
        "Lingjiao Chen",
        "Jared Quincy Davis",
        "Boris Hanin",
        "Peter Bailis",
        "Matei Zaharia",
        "James Zou",
        "Ion Stoica"
      ],
      "abstract": "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
      "pdf_url": "http://arxiv.org/pdf/2502.14815v1",
      "published": "2025-02-20T18:36:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14815v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image Analysis",
      "authors": [
        "Fadillah Maani",
        "Numan Saeed",
        "Tausifa Saleem",
        "Zaid Farooq",
        "Hussain Alasmawi",
        "Werner Diehl",
        "Ameera Mohammad",
        "Gareth Waring",
        "Saudabi Valappi",
        "Leanne Bricker",
        "Mohammad Yaqub"
      ],
      "abstract": "Foundation models are becoming increasingly effective in the medical domain,\noffering pre-trained models on large datasets that can be readily adapted for\ndownstream tasks. Despite progress, fetal ultrasound images remain a\nchallenging domain for foundation models due to their inherent complexity,\noften requiring substantial additional training and facing limitations due to\nthe scarcity of paired multimodal data. To overcome these challenges, here we\nintroduce FetalCLIP, a vision-language foundation model capable of generating\nuniversal representation of fetal ultrasound images. FetalCLIP was pre-trained\nusing a multimodal learning approach on a diverse dataset of 210,035 fetal\nultrasound images paired with text. This represents the largest paired dataset\nof its kind used for foundation model development to date. This unique training\napproach allows FetalCLIP to effectively learn the intricate anatomical\nfeatures present in fetal ultrasound images, resulting in robust\nrepresentations that can be used for a variety of downstream applications. In\nextensive benchmarking across a range of key fetal ultrasound applications,\nincluding classification, gestational age estimation, congenital heart defect\n(CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all\nbaselines while demonstrating remarkable generalizability and strong\nperformance even with limited labeled data. We plan to release the FetalCLIP\nmodel publicly for the benefit of the broader scientific community.",
      "pdf_url": "http://arxiv.org/pdf/2502.14807v1",
      "published": "2025-02-20T18:30:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14807v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language Models",
      "authors": [
        "Bernal Jiménez Gutiérrez",
        "Yiheng Shu",
        "Weijian Qi",
        "Sizhe Zhou",
        "Yu Su"
      ],
      "abstract": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Our\ncode and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.",
      "pdf_url": "http://arxiv.org/pdf/2502.14802v1",
      "published": "2025-02-20T18:26:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14802v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Survey on Text-Driven 360-Degree Panorama Generation",
      "authors": [
        "Hai Wang",
        "Xiaoyu Xiang",
        "Weihao Xia",
        "Jing-Hao Xue"
      ],
      "abstract": "The advent of text-driven 360-degree panorama generation, enabling the\nsynthesis of 360-degree panoramic images directly from textual descriptions,\nmarks a transformative advancement in immersive visual content creation. This\ninnovation significantly simplifies the traditionally complex process of\nproducing such content. Recent progress in text-to-image diffusion models has\naccelerated the rapid development in this emerging field. This survey presents\na comprehensive review of text-driven 360-degree panorama generation, offering\nan in-depth analysis of state-of-the-art algorithms and their expanding\napplications in 360-degree 3D scene generation. Furthermore, we critically\nexamine current limitations and propose promising directions for future\nresearch. A curated project page with relevant resources and research papers is\navailable at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.",
      "pdf_url": "http://arxiv.org/pdf/2502.14799v1",
      "published": "2025-02-20T18:19:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14799v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Rapid Word Learning Through Meta In-Context Learning",
      "authors": [
        "Wentao Wang",
        "Guangyuan Jiang",
        "Tal Linzen",
        "Brenden M. Lake"
      ],
      "abstract": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.",
      "pdf_url": "http://arxiv.org/pdf/2502.14791v1",
      "published": "2025-02-20T18:11:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14791v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Ray-Tracing for Conditionally Activated Neural Networks",
      "authors": [
        "Claudio Gallicchio",
        "Giuseppe Nuti"
      ],
      "abstract": "In this paper, we introduce a novel architecture for conditionally activated\nneural networks combining a hierarchical construction of multiple Mixture of\nExperts (MoEs) layers with a sampling mechanism that progressively converges to\nan optimized configuration of expert activation. This methodology enables the\ndynamic unfolding of the network's architecture, facilitating efficient\npath-specific training. Experimental results demonstrate that this approach\nachieves competitive accuracy compared to conventional baselines while\nsignificantly reducing the parameter count required for inference. Notably,\nthis parameter reduction correlates with the complexity of the input patterns,\na property naturally emerging from the network's operational dynamics without\nnecessitating explicit auxiliary penalty functions.",
      "pdf_url": "http://arxiv.org/pdf/2502.14788v1",
      "published": "2025-02-20T18:09:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14788v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
      "authors": [
        "Michael Tschannen",
        "Alexey Gritsenko",
        "Xiao Wang",
        "Muhammad Ferjad Naeem",
        "Ibrahim Alabdulmohsin",
        "Nikhil Parthasarathy",
        "Talfan Evans",
        "Lucas Beyer",
        "Ye Xia",
        "Basil Mustafa",
        "Olivier Hénaff",
        "Jeremiah Harmsen",
        "Andreas Steiner",
        "Xiaohua Zhai"
      ],
      "abstract": "We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B).",
      "pdf_url": "http://arxiv.org/pdf/2502.14786v1",
      "published": "2025-02-20T18:08:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14786v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Real-Time Device Reach Forecasting Using HLL and MinHash Data Sketches",
      "authors": [
        "Chandrashekar Muniyappa",
        "Kendall Willets",
        "Sriraman Krishnamoorthy"
      ],
      "abstract": "Predicting the right number of TVs (Device Reach) in real-time based on a\nuser-specified targeting attributes is imperative for running multi-million\ndollar ADs business. The traditional approach of SQL queries to join billions\nof records across multiple targeting dimensions is extremely slow. As a\nworkaround, many applications will have an offline process to crunch these\nnumbers and present the results after many hours. In our case, the solution was\nan offline process taking 24 hours to onboard a customer resulting in a\npotential loss of business. To solve this problem, we have built a new\nreal-time prediction system using MinHash and HyperLogLog (HLL) data sketches\nto compute the device reach at runtime when a user makes a request. However,\nexisting MinHash implementations do not solve the complex problem of multilevel\naggregation and intersection. This work will show how we have solved this\nproblem, in addition, we have improved MinHash algorithm to run 4 times faster\nusing Single Instruction Multiple Data (SIMD) vectorized operations for high\nspeed and accuracy with constant space to process billions of records. Finally,\nby experiments, we prove that the results are as accurate as traditional\noffline prediction system with an acceptable error rate of 5%.",
      "pdf_url": "http://arxiv.org/pdf/2502.14785v1",
      "published": "2025-02-20T18:05:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14785v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG",
        "60G25",
        "I.5.3"
      ]
    },
    {
      "title": "ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting",
      "authors": [
        "Abhijit Mishra",
        "Richard Noh",
        "Hsiang Fu",
        "Mingda Li",
        "Minji Kim"
      ],
      "abstract": "Efficient and privacy-preserving multimodal interaction is essential as AR,\nVR, and modern smartphones with powerful cameras become primary interfaces for\nhuman-computer communication. Existing powerful large vision-language models\n(VLMs) enabling multimodal interaction often rely on cloud-based processing,\nraising significant concerns about (1) visual privacy by transmitting sensitive\nvision data to servers, and (2) their limited real-time, on-device usability.\nThis paper explores Visual Instruction Rewriting, a novel approach that\ntransforms multimodal instructions into text-only commands, allowing seamless\nintegration of lightweight on-device instruction rewriter VLMs (250M\nparameters) with existing conversational AI systems, enhancing vision data\nprivacy. To achieve this, we present a dataset of over 39,000 examples across\n14 domains and develop a compact VLM, pretrained on image captioning datasets\nand fine-tuned for instruction rewriting. Experimental results, evaluated\nthrough NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic\nparsing analysis, demonstrate that even a quantized version of the model\n(<500MB storage footprint) can achieve effective instruction rewriting, thus\nenabling privacy-focused, multimodal AI applications.",
      "pdf_url": "http://arxiv.org/pdf/2502.14780v1",
      "published": "2025-02-20T18:01:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14780v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Harnessing PDF Data for Improving Japanese Large Multimodal Models",
      "authors": [
        "Jeonghun Baek",
        "Akiko Aizawa",
        "Kiyoharu Aizawa"
      ],
      "abstract": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nEnglish, but their effectiveness in Japanese remains limited due to the lack of\nhigh-quality training data. Current Japanese LMMs often rely on translated\nEnglish datasets, restricting their ability to capture Japan-specific cultural\nknowledge. To address this, we explore the potential of Japanese PDF data as a\ntraining resource, an area that remains largely underutilized. We introduce a\nfully automated pipeline that leverages pretrained models to extract image-text\npairs from PDFs through layout analysis, OCR, and vision-language pairing,\nremoving the need for manual annotation. Additionally, we construct instruction\ndata from extracted image-text pairs to enrich the training data. To evaluate\nthe effectiveness of PDF-derived data, we train Japanese LMMs and assess their\nperformance on the Japanese LMM Benchmark. Our results demonstrate substantial\nimprovements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.\nFurther analysis highlights the impact of PDF-derived data on various factors,\nsuch as model size and language models, reinforcing its value as a multimodal\nresource for Japanese LMMs. We plan to make the source code and data publicly\navailable upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2502.14778v1",
      "published": "2025-02-20T17:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14778v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Making Universal Policies Universal",
      "authors": [
        "Niklas Höpner",
        "David Kuric",
        "Herke van Hoof"
      ],
      "abstract": "The development of a generalist agent capable of solving a wide range of\nsequential decision-making tasks remains a significant challenge. We address\nthis problem in a cross-agent setup where agents share the same observation\nspace but differ in their action spaces. Our approach builds on the universal\npolicy framework, which decouples policy learning into two stages: a\ndiffusion-based planner that generates observation sequences and an inverse\ndynamics model that assigns actions to these plans. We propose a method for\ntraining the planner on a joint dataset composed of trajectories from all\nagents. This method offers the benefit of positive transfer by pooling data\nfrom different agents, while the primary challenge lies in adapting shared\nplans to each agent's unique constraints. We evaluate our approach on the\nBabyAI environment, covering tasks of varying complexity, and demonstrate\npositive transfer across agents. Additionally, we examine the planner's\ngeneralisation ability to unseen agents and compare our method to traditional\nimitation learning approaches. By training on a pooled dataset from multiple\nagents, our universal policy achieves an improvement of up to $42.20\\%$ in task\ncompletion accuracy compared to a policy trained on a dataset from a single\nagent.",
      "pdf_url": "http://arxiv.org/pdf/2502.14777v1",
      "published": "2025-02-20T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14777v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
      "authors": [
        "Tian Xie",
        "Zitian Gao",
        "Qingnan Ren",
        "Haoming Luo",
        "Yuqian Hong",
        "Bryan Dai",
        "Joey Zhou",
        "Kai Qiu",
        "Zhirong Wu",
        "Chong Luo"
      ],
      "abstract": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC.",
      "pdf_url": "http://arxiv.org/pdf/2502.14768v1",
      "published": "2025-02-20T17:49:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14768v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis",
      "authors": [
        "Priyanka Kargupta",
        "Ishika Agarwal",
        "Tal August",
        "Jiawei Han"
      ],
      "abstract": "With the exponential growth of research facilitated by modern technology and\nimproved accessibility, scientific discoveries have become increasingly\nfragmented within and across fields. This makes it challenging to assess the\nsignificance, novelty, incremental findings, and equivalent ideas between\nrelated works, particularly those from different research communities. Large\nlanguage models (LLMs) have recently demonstrated strong quantitative and\nqualitative reasoning abilities, and multi-agent LLM debates have shown promise\nin handling complex reasoning tasks by exploring diverse perspectives and\nreasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a\nframework which converts scientific papers into LLM personas that debate their\nrespective novelties. To emphasize structured, critical reasoning rather than\nfocusing solely on outcomes, ToD dynamically constructs a debate tree, enabling\nfine-grained analysis of independent novelty arguments within scholarly\narticles. Through experiments on scientific literature across various domains,\nevaluated by expert researchers, we demonstrate that ToD generates informative\narguments, effectively contrasts papers, and supports researchers in their\nliterature review.",
      "pdf_url": "http://arxiv.org/pdf/2502.14767v1",
      "published": "2025-02-20T17:43:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14767v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning",
      "authors": [
        "Juraj Vladika",
        "Ivana Hacajová",
        "Florian Matthes"
      ],
      "abstract": "Fact verification (FV) aims to assess the veracity of a claim based on\nrelevant evidence. The traditional approach for automated FV includes a\nthree-part pipeline relying on short evidence snippets and encoder-only\ninference models. More recent approaches leverage the multi-turn nature of LLMs\nto address FV as a step-by-step problem where questions inquiring additional\ncontext are generated and answered until there is enough information to make a\ndecision. This iterative method makes the verification process rational and\nexplainable. While these methods have been tested for encyclopedic claims,\nexploration on domain-specific and realistic claims is missing. In this work,\nwe apply an iterative FV system on three medical fact-checking datasets and\nevaluate it with multiple settings, including different LLMs, external web\nsearch, and structured reasoning using logic predicates. We demonstrate\nimprovements in the final performance over traditional approaches and the high\npotential of step-by-step FV systems for domain-specific claims.",
      "pdf_url": "http://arxiv.org/pdf/2502.14765v1",
      "published": "2025-02-20T17:40:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14765v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations",
      "authors": [
        "Haotian Zhai",
        "Connor Lawless",
        "Ellen Vitercik",
        "Liu Leqi"
      ],
      "abstract": "A fundamental problem in combinatorial optimization is identifying equivalent\nformulations, which can lead to more efficient solution strategies and deeper\ninsights into a problem's computational complexity. The need to automatically\nidentify equivalence between problem formulations has grown as optimization\ncopilots--systems that generate problem formulations from natural language\ndescriptions--have proliferated. However, existing approaches to checking\nformulation equivalence lack grounding, relying on simple heuristics which are\ninsufficient for rigorous validation. Inspired by Karp reductions, in this work\nwe introduce quasi-Karp equivalence, a formal criterion for determining when\ntwo optimization formulations are equivalent based on the existence of a\nmapping between their decision variables. We propose EquivaMap, a framework\nthat leverages large language models to automatically discover such mappings,\nenabling scalable and reliable equivalence verification. To evaluate our\napproach, we construct the first open-source dataset of equivalent optimization\nformulations, generated by applying transformations such as adding slack\nvariables or valid inequalities to existing formulations. Empirically,\nEquivaMap significantly outperforms existing methods, achieving substantial\nimprovements in correctly identifying formulation equivalence.",
      "pdf_url": "http://arxiv.org/pdf/2502.14760v1",
      "published": "2025-02-20T17:35:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14760v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.OC"
      ]
    },
    {
      "title": "On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems",
      "authors": [
        "Juraj Vladika",
        "Florian Matthes"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has emerged as an approach to augment\nlarge language models (LLMs) by reducing their reliance on static knowledge and\nimproving answer factuality. RAG retrieves relevant context snippets and\ngenerates an answer based on them. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is lacking, particularly regarding the\nideal size of provided context, and the choice of base LLM and retrieval\nmethod. To help guide development of robust RAG systems, we evaluate various\ncontext sizes, BM25 and semantic search as retrievers, and eight base LLMs.\nMoving away from the usual RAG evaluation with short answers, we explore the\nmore challenging long-form question answering in two domains, where a good\nanswer has to utilize the entire context. Our findings indicate that final QA\nperformance improves steadily with up to 15 snippets but stagnates or declines\nbeyond that. Finally, we show that different general-purpose LLMs excel in the\nbiomedical domain than the encyclopedic one, and that open-domain evidence\nretrieval in large corpora is challenging.",
      "pdf_url": "http://arxiv.org/pdf/2502.14759v1",
      "published": "2025-02-20T17:34:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14759v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders",
      "authors": [
        "Maya Varma",
        "Ashwin Kumar",
        "Rogier van der Sluijs",
        "Sophie Ostmeier",
        "Louis Blankemeier",
        "Pierre Chambon",
        "Christian Bluethgen",
        "Jip Prince",
        "Curtis Langlotz",
        "Akshay Chaudhari"
      ],
      "abstract": "Medical images are acquired at high resolutions with large fields of view in\norder to capture fine-grained features necessary for clinical decision-making.\nConsequently, training deep learning models on medical images can incur large\ncomputational costs. In this work, we address the challenge of downsizing\nmedical images in order to improve downstream computational efficiency while\npreserving clinically-relevant features. We introduce MedVAE, a family of six\nlarge-scale 2D and 3D autoencoders capable of encoding medical images as\ndownsized latent representations and decoding latent representations back to\nhigh-resolution images. We train MedVAE autoencoders using a novel two-stage\ntraining approach with 1,052,730 medical images. Across diverse tasks obtained\nfrom 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent\nrepresentations in place of high-resolution images when training downstream\nmodels can lead to efficiency benefits (up to 70x improvement in throughput)\nwhile simultaneously preserving clinically-relevant features and (2) MedVAE can\ndecode latent representations back to high-resolution images with high\nfidelity. Our work demonstrates that large-scale, generalizable autoencoders\ncan help address critical efficiency challenges in the medical domain. Our code\nis available at https://github.com/StanfordMIMI/MedVAE.",
      "pdf_url": "http://arxiv.org/pdf/2502.14753v1",
      "published": "2025-02-20T17:24:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14753v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Multi-Agent Coordination across Diverse Applications: A Survey",
      "authors": [
        "Lijun Sun",
        "Yijun Yang",
        "Qiqi Duan",
        "Yuhui Shi",
        "Chao Lyu",
        "Yu-Cheng Chang",
        "Chin-Teng Lin",
        "Yang Shen"
      ],
      "abstract": "Multi-agent coordination studies the underlying mechanism enabling the\ntrending spread of diverse multi-agent systems (MAS) and has received\nincreasing attention, driven by the expansion of emerging applications and\nrapid AI advances. This survey outlines the current state of coordination\nresearch across applications through a unified understanding that answers four\nfundamental coordination questions: (1) what is coordination; (2) why\ncoordination; (3) who to coordinate with; and (4) how to coordinate. Our\npurpose is to explore existing ideas and expertise in coordination and their\nconnections across diverse applications, while identifying and highlighting\nemerging and promising research directions. First, general coordination\nproblems that are essential to varied applications are identified and analyzed.\nSecond, a number of MAS applications are surveyed, ranging from widely studied\ndomains, e.g., search and rescue, warehouse automation and logistics, and\ntransportation systems, to emerging fields including humanoid and\nanthropomorphic robots, satellite systems, and large language models (LLMs).\nFinally, open challenges about the scalability, heterogeneity, and learning\nmechanisms of MAS are analyzed and discussed. In particular, we identify the\nhybridization of hierarchical and decentralized coordination, human-MAS\ncoordination, and LLM-based MAS as promising future directions.",
      "pdf_url": "http://arxiv.org/pdf/2502.14743v1",
      "published": "2025-02-20T17:12:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14743v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "YOLOv12: A Breakdown of the Key Architectural Features",
      "authors": [
        "Mujadded Al Rabbani Alif",
        "Muhammad Hussain"
      ],
      "abstract": "This paper presents an architectural analysis of YOLOv12, a significant\nadvancement in single-stage, real-time object detection building upon the\nstrengths of its predecessors while introducing key improvements. The model\nincorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and\nFlashAttention-driven area-based attention, improving feature extraction,\nenhanced efficiency, and robust detections. With multiple model variants,\nsimilar to its predecessors, YOLOv12 offers scalable solutions for both\nlatency-sensitive and high-accuracy applications. Experimental results manifest\nconsistent gains in mean average precision (mAP) and inference speed, making\nYOLOv12 a compelling choice for applications in autonomous systems, security,\nand real-time analytics. By achieving an optimal balance between computational\nefficiency and performance, YOLOv12 sets a new benchmark for real-time computer\nvision, facilitating deployment across diverse hardware platforms, from edge\ndevices to high-performance clusters.",
      "pdf_url": "http://arxiv.org/pdf/2502.14740v1",
      "published": "2025-02-20T17:08:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14740v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration",
      "authors": [
        "Minjie Hong",
        "Yan Xia",
        "Zehan Wang",
        "Jieming Zhu",
        "Ye Wang",
        "Sihang Cai",
        "Xiaoda Yang",
        "Quanyu Dai",
        "Zhenhua Dong",
        "Zhimeng Zhang",
        "Zhou Zhao"
      ],
      "abstract": "Large language models (LLMs) are increasingly leveraged as foundational\nbackbones in the development of advanced recommender systems, offering enhanced\ncapabilities through their extensive knowledge and reasoning. Existing\nllm-based recommender systems (RSs) often face challenges due to the\nsignificant differences between the linguistic semantics of pre-trained LLMs\nand the collaborative semantics essential for RSs. These systems use\npre-trained linguistic semantics but learn collaborative semantics from scratch\nvia the llm-Backbone. However, LLMs are not designed for recommendations,\nleading to inefficient collaborative learning, weak result correlations, and\npoor integration of traditional RS features. To address these challenges, we\npropose EAGER-LLM, a decoder-only llm-based generative recommendation framework\nthat integrates endogenous and exogenous behavioral and semantic information in\na non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich\nitem indices that integrates indexing sequences for exogenous signals, enabling\nefficient link-wide processing; 2)non-invasive multiscale alignment\nreconstruction tasks guide the model toward a deeper understanding of both\ncollaborative and semantic signals; 3)an annealing adapter designed to finely\nbalance the model's recommendation performance with its comprehension\ncapabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing\non three public benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2502.14735v1",
      "published": "2025-02-20T17:01:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14735v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models",
      "authors": [
        "Yifu Chen",
        "Shengpeng Ji",
        "Haoxiao Wang",
        "Ziqing Wang",
        "Siyu Chen",
        "Jinzheng He",
        "Jin Xu",
        "Zhou Zhao"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to\nits capacity to empower large language models (LLMs) to integrate external\nknowledge. However, existing RAG frameworks are primarily designed for\ntext-based LLMs and rely on Automatic Speech Recognition to process speech\ninput, which discards crucial audio information, risks transcription errors,\nand increases computational overhead. Therefore, we introduce WavRAG, the first\nretrieval augmented generation framework with native, end-to-end audio support.\nWavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw\naudio for both embedding and retrieval. 2) WavRAG integrates audio and text\ninto a unified knowledge representation. Specifically, we propose the\nWavRetriever to facilitate the retrieval from a text-audio hybrid knowledge\nbase, and further enhance the in-context capabilities of spoken dialogue models\nthrough the integration of chain-of-thought reasoning. In comparison to\nstate-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval\nperformance while delivering a 10x acceleration. Furthermore, WavRAG's unique\ntext-audio hybrid retrieval capability extends the boundaries of RAG to the\naudio modality.",
      "pdf_url": "http://arxiv.org/pdf/2502.14727v1",
      "published": "2025-02-20T16:54:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14727v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Ranking Joint Policies in Dynamic Games using Evolutionary Dynamics",
      "authors": [
        "Natalia Koliou",
        "George Vouros"
      ],
      "abstract": "Game-theoretic solution concepts, such as the Nash equilibrium, have been key\nto finding stable joint actions in multi-player games. However, it has been\nshown that the dynamics of agents' interactions, even in simple two-player\ngames with few strategies, are incapable of reaching Nash equilibria,\nexhibiting complex and unpredictable behavior. Instead, evolutionary approaches\ncan describe the long-term persistence of strategies and filter out transient\nones, accounting for the long-term dynamics of agents' interactions. Our goal\nis to identify agents' joint strategies that result in stable behavior, being\nresistant to changes, while also accounting for agents' payoffs, in dynamic\ngames. Towards this goal, and building on previous results, this paper proposes\ntransforming dynamic games into their empirical forms by considering agents'\nstrategies instead of agents' actions, and applying the evolutionary\nmethodology $\\alpha$-Rank to evaluate and rank strategy profiles according to\ntheir long-term dynamics. This methodology not only allows us to identify joint\nstrategies that are strong through agents' long-term interactions, but also\nprovides a descriptive, transparent framework regarding the high ranking of\nthese strategies. Experiments report on agents that aim to collaboratively\nsolve a stochastic version of the graph coloring problem. We consider different\nstyles of play as strategies to define the empirical game, and train policies\nrealizing these strategies, using the DQN algorithm. Then we run simulations to\ngenerate the payoff matrix required by $\\alpha$-Rank to rank joint strategies.",
      "pdf_url": "http://arxiv.org/pdf/2502.14724v1",
      "published": "2025-02-20T16:50:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14724v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "From Knowledge Generation to Knowledge Verification: Examining the BioMedical Generative Capabilities of ChatGPT",
      "authors": [
        "Ahmed Abdeen Hamed",
        "Byung Suk Lee"
      ],
      "abstract": "The generative capabilities of LLM models present opportunities in\naccelerating tasks and concerns with the authenticity of the knowledge it\nproduces. To address the concerns, we present a computational approach that\nsystematically evaluates the factual accuracy of biomedical knowledge that an\nLLM model has been prompted to generate. Our approach encompasses two\nprocesses: the generation of disease-centric associations and the verification\nof them using the semantic knowledge of the biomedical ontologies. Using\nChatGPT as the select LLM model, we designed a set of prompt-engineering\nprocesses to generate linkages between diseases, drugs, symptoms, and genes to\nestablish grounds for assessments. Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). The symptom term identification accuracy was\nnotably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO\nontologies accordingly. The verification of associations reveals literature\ncoverage rates of (89%-91%) among disease-drug and disease-gene associations.\nThe low identification accuracy for symptom terms also contributed to the\nverification of symptom-related associations (49%-62%).",
      "pdf_url": "http://arxiv.org/pdf/2502.14714v1",
      "published": "2025-02-20T16:39:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14714v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "I.2; I.2.4; I.2.7"
      ]
    },
    {
      "title": "Human Misperception of Generative-AI Alignment: A Laboratory Experiment",
      "authors": [
        "Kevin He",
        "Ran Shorrer",
        "Mengjia Xia"
      ],
      "abstract": "We conduct an incentivized laboratory experiment to study people's perception\nof generative artificial intelligence (GenAI) alignment in the context of\neconomic decision-making. Using a panel of economic problems spanning the\ndomains of risk, time preference, social preference, and strategic\ninteractions, we ask human subjects to make choices for themselves and to\npredict the choices made by GenAI on behalf of a human user. We find that\npeople overestimate the degree of alignment between GenAI's choices and human\nchoices. In every problem, human subjects' average prediction about GenAI's\nchoice is substantially closer to the average human-subject choice than it is\nto the GenAI choice. At the individual level, different subjects' predictions\nabout GenAI's choice in a given problem are highly correlated with their own\nchoices in the same problem. We explore the implications of people\noverestimating GenAI alignment in a simple theoretical model.",
      "pdf_url": "http://arxiv.org/pdf/2502.14708v1",
      "published": "2025-02-20T16:32:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14708v1",
      "categories": [
        "econ.TH",
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "Building reliable sim driving agents by scaling self-play",
      "authors": [
        "Daphne Cornelisse",
        "Aarav Pandya",
        "Kevin Joseph",
        "Joseph Suárez",
        "Eugene Vinitsky"
      ],
      "abstract": "Simulation agents are essential for designing and testing systems that\ninteract with humans, such as autonomous vehicles (AVs). These agents serve\nvarious purposes, from benchmarking AV performance to stress-testing the\nsystem's limits, but all use cases share a key requirement: reliability. A\nsimulation agent should behave as intended by the designer, minimizing\nunintended actions like collisions that can compromise the signal-to-noise\nratio of analyses. As a foundation for reliable sim agents, we propose scaling\nself-play to thousands of scenarios on the Waymo Open Motion Dataset under\nsemi-realistic limits on human perception and control. Training from scratch on\na single GPU, our agents nearly solve the full training set within a day. They\ngeneralize effectively to unseen test scenes, achieving a 99.8% goal completion\nrate with less than 0.8% combined collision and off-road incidents across\n10,000 held-out scenarios. Beyond in-distribution generalization, our agents\nshow partial robustness to out-of-distribution scenes and can be fine-tuned in\nminutes to reach near-perfect performance in those cases. Demonstrations of\nagent behaviors can be found at this link. We open-source both the pre-trained\nagents and the complete code base. Demonstrations of agent behaviors can be\nfound at \\url{https://sites.google.com/view/reliable-sim-agents}.",
      "pdf_url": "http://arxiv.org/pdf/2502.14706v1",
      "published": "2025-02-20T16:30:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14706v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting",
      "authors": [
        "Yuxuan Yang",
        "Dalin Zhang",
        "Yuxuan Liang",
        "Hua Lu",
        "Huan Li",
        "Gang Chen"
      ],
      "abstract": "Time Series Forecasting (TSF) is a crucial task in various domains, yet\nexisting TSF models rely heavily on high-quality data and insufficiently\nexploit all available data. This paper explores a novel self-supervised\napproach to re-label time series datasets by inherently constructing candidate\ndatasets. During the optimization of a simple reconstruction network,\nintermediates are used as pseudo labels in a self-supervised paradigm,\nimproving generalization for any predictor. We introduce the Self-Correction\nwith Adaptive Mask (SCAM), which discards overfitted components and selectively\nreplaces them with pseudo labels generated from reconstructions. Additionally,\nwe incorporate Spectral Norm Regularization (SNR) to further suppress\noverfitting from a loss landscape perspective. Our experiments on eleven\nreal-world datasets demonstrate that SCAM consistently improves the performance\nof various backbone models. This work offers a new perspective on constructing\ndatasets and enhancing the generalization of TSF models through self-supervised\nlearning.",
      "pdf_url": "http://arxiv.org/pdf/2502.14704v1",
      "published": "2025-02-20T16:29:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14704v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "General Uncertainty Estimation with Delta Variances",
      "authors": [
        "Simon Schmitt",
        "John Shawe-Taylor",
        "Hado van Hasselt"
      ],
      "abstract": "Decision makers may suffer from uncertainty induced by limited data. This may\nbe mitigated by accounting for epistemic uncertainty, which is however\nchallenging to estimate efficiently for large neural networks. To this extent\nwe investigate Delta Variances, a family of algorithms for epistemic\nuncertainty quantification, that is computationally efficient and convenient to\nimplement. It can be applied to neural networks and more general functions\ncomposed of neural networks. As an example we consider a weather simulator with\na neural-network-based step function inside -- here Delta Variances empirically\nobtain competitive results at the cost of a single gradient computation. The\napproach is convenient as it requires no changes to the neural network\narchitecture or training procedure. We discuss multiple ways to derive Delta\nVariances theoretically noting that special cases recover popular techniques\nand present a unified perspective on multiple related methods. Finally we\nobserve that this general perspective gives rise to a natural extension and\nempirically show its benefit.",
      "pdf_url": "http://arxiv.org/pdf/2502.14698v1",
      "published": "2025-02-20T16:22:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14698v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP",
        "stat.ML"
      ]
    },
    {
      "title": "seqKAN: Sequence processing with Kolmogorov-Arnold Networks",
      "authors": [
        "Tatiana Boura",
        "Stasinos Konstantopoulos"
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine\nlearning framework that is more interpretable and controllable than the\nmulti-layer perceptron. Various network architectures have been proposed within\nthe KAN framework targeting different tasks and application domains, including\nsequence processing.\n  This paper proposes seqKAN, a new KAN architecture for sequence processing.\nAlthough multiple sequence processing KAN architectures have already been\nproposed, we argue that seqKAN is more faithful to the core concept of the KAN\nframework. Furthermore, we empirically demonstrate that it achieves better\nresults.\n  The empirical evaluation is performed on generated data from a complex\nphysics problem on an interpolation and an extrapolation task. Using this\ndataset we compared seqKAN against a prior KAN network for timeseries\nprediction, recurrent deep networks, and symbolic regression. seqKAN\nsubstantially outperforms all architectures, particularly on the extrapolation\ndataset, while also being the most transparent.",
      "pdf_url": "http://arxiv.org/pdf/2502.14681v1",
      "published": "2025-02-20T16:10:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14681v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Data-Constrained Synthesis of Training Data for De-Identification",
      "authors": [
        "Thomas Vakili",
        "Aron Henriksson",
        "Hercules Dalianis"
      ],
      "abstract": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.",
      "pdf_url": "http://arxiv.org/pdf/2502.14677v1",
      "published": "2025-02-20T16:09:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14677v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "BP-SGCN: Behavioral Pseudo-Label Informed Sparse Graph Convolution Network for Pedestrian and Heterogeneous Trajectory Prediction",
      "authors": [
        "Ruochen Li",
        "Stamos Katsigiannis",
        "Tae-Kyun Kim",
        "Hubert P. H. Shum"
      ],
      "abstract": "Trajectory prediction allows better decision-making in applications of\nautonomous vehicles or surveillance by predicting the short-term future\nmovement of traffic agents. It is classified into pedestrian or heterogeneous\ntrajectory prediction. The former exploits the relatively consistent behavior\nof pedestrians, but is limited in real-world scenarios with heterogeneous\ntraffic agents such as cyclists and vehicles. The latter typically relies on\nextra class label information to distinguish the heterogeneous agents, but such\nlabels are costly to annotate and cannot be generalized to represent different\nbehaviors within the same class of agents. In this work, we introduce the\nbehavioral pseudo-labels that effectively capture the behavior distributions of\npedestrians and heterogeneous agents solely based on their motion features,\nsignificantly improving the accuracy of trajectory prediction. To implement the\nframework, we propose the Behavioral Pseudo-Label Informed Sparse Graph\nConvolution Network (BP-SGCN) that learns pseudo-labels and informs to a\ntrajectory predictor. For optimization, we propose a cascaded training scheme,\nin which we first learn the pseudo-labels in an unsupervised manner, and then\nperform end-to-end fine-tuning on the labels in the direction of increasing the\ntrajectory prediction accuracy. Experiments show that our pseudo-labels\neffectively model different behavior clusters and improve trajectory\nprediction. Our proposed BP-SGCN outperforms existing methods using both\npedestrian (ETH/UCY, pedestrian-only SDD) and heterogeneous agent datasets\n(SDD, Argoverse 1).",
      "pdf_url": "http://arxiv.org/pdf/2502.14676v1",
      "published": "2025-02-20T16:09:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14676v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Explanations of Deep Language Models Explain Language Representations in the Brain",
      "authors": [
        "Maryam Rahimi",
        "Yadollah Yaghoobzadeh",
        "Mohammad Reza Daliri"
      ],
      "abstract": "Recent advances in artificial intelligence have given rise to large language\nmodels (LLMs) that not only achieve human-like performance but also share\ncomputational principles with the brain's language processing mechanisms. While\nprevious research has primarily focused on aligning LLMs' internal\nrepresentations with neural activity, we introduce a novel approach that\nleverages explainable AI (XAI) methods to forge deeper connections between the\ntwo domains. Using attribution methods, we quantified how preceding words\ncontribute to an LLM's next-word predictions and employed these explanations to\npredict fMRI recordings from participants listening to the same narratives. Our\nfindings demonstrate that attribution methods robustly predict brain activity\nacross the language network, surpassing traditional internal representations in\nearly language areas. This alignment is hierarchical: early-layer explanations\ncorrespond to the initial stages of language processing in the brain, while\nlater layers align with more advanced stages. Moreover, the layers more\ninfluential on LLM next-word prediction$\\unicode{x2014}$those with higher\nattribution scores$\\unicode{x2014}$exhibited stronger alignment with neural\nactivity. This work establishes a bidirectional bridge between AI and\nneuroscience. First, we demonstrate that attribution methods offer a powerful\nlens for investigating the neural mechanisms of language comprehension,\nrevealing how meaning emerges from preceding context. Second, we propose using\nbrain alignment as a metric to evaluate the validity of attribution methods,\nproviding a framework for assessing their biological plausibility.",
      "pdf_url": "http://arxiv.org/pdf/2502.14671v1",
      "published": "2025-02-20T16:05:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14671v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.NC"
      ]
    },
    {
      "title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs",
      "authors": [
        "Yuchen Wu",
        "Liang Ding",
        "Li Shen",
        "Dacheng Tao"
      ],
      "abstract": "Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings.",
      "pdf_url": "http://arxiv.org/pdf/2502.14645v1",
      "published": "2025-02-20T15:32:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14645v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation",
      "authors": [
        "Angxiao Yue",
        "Zichong Wang",
        "Hongteng Xu"
      ],
      "abstract": "Protein backbone generation plays a central role in de novo protein design\nand is significant for many biological and medical applications. Although\ndiffusion and flow-based generative models provide potential solutions to this\nchallenging task, they often generate proteins with undesired designability and\nsuffer computational inefficiency. In this study, we propose a novel rectified\nquaternion flow (ReQFlow) matching method for fast and high-quality protein\nbackbone generation. In particular, our method generates a local translation\nand a 3D rotation from random noise for each residue in a protein chain, which\nrepresents each 3D rotation as a unit quaternion and constructs its flow by\nspherical linear interpolation (SLERP) in an exponential format. We train the\nmodel by quaternion flow (QFlow) matching with guaranteed numerical stability\nand rectify the QFlow model to accelerate its inference and improve the\ndesignability of generated protein backbones, leading to the proposed ReQFlow\nmodel. Experiments show that ReQFlow achieves state-of-the-art performance in\nprotein backbone generation while requiring much fewer sampling steps and\nsignificantly less inference time (e.g., being 37x faster than RFDiffusion and\n62x faster than Genie2 when generating a backbone of length 300), demonstrating\nits effectiveness and efficiency. The code is available at\nhttps://github.com/AngxiaoYue/ReQFlow.",
      "pdf_url": "http://arxiv.org/pdf/2502.14637v1",
      "published": "2025-02-20T15:20:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14637v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors",
      "authors": [
        "Yuguo Yin",
        "Yuxin Xie",
        "Wenyuan Yang",
        "Dongchao Yang",
        "Jinghan Ru",
        "Xianwei Zhuang",
        "Liming Liang",
        "Yuexian Zou"
      ],
      "abstract": "Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to\nretrieve audio clips or multilingual texts from databases. However, existing\nML-ATR schemes suffer from inconsistencies for instance similarity matching\nacross languages. We theoretically analyze the inconsistency in terms of both\nmultilingual modal alignment direction error and weight error, and propose the\ntheoretical weight error upper bound for quantifying the inconsistency. Based\non the analysis of the weight error upper bound, we find that the inconsistency\nproblem stems from the data distribution error caused by random sampling of\nlanguages. We propose a consistent ML-ATR scheme using 1-to-k contrastive\nlearning and audio-English co-anchor contrastive learning, aiming to mitigate\nthe negative impact of data distribution error on recall and consistency in\nML-ATR. Experimental results on the translated AudioCaps and Clotho datasets\nshow that our scheme achieves state-of-the-art performance on recall and\nconsistency metrics for eight mainstream languages, including English. Our code\nwill be available at https://github.com/ATRI-ACL/ATRI-ACL.",
      "pdf_url": "http://arxiv.org/pdf/2502.14627v1",
      "published": "2025-02-20T15:06:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14627v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison for Semantic Similarity",
      "authors": [
        "Xinghan Pan"
      ],
      "abstract": "This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines.",
      "pdf_url": "http://arxiv.org/pdf/2502.14620v1",
      "published": "2025-02-20T14:58:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14620v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; I.7.3"
      ]
    },
    {
      "title": "Reward Models Identify Consistency, Not Causality",
      "authors": [
        "Yuhui Xu",
        "Hanze Dong",
        "Lei Wang",
        "Caiming Xiong",
        "Junnan Li"
      ],
      "abstract": "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2502.14619v1",
      "published": "2025-02-20T14:57:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14619v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A Theory for Conditional Generative Modeling on Multiple Data Sources",
      "authors": [
        "Rongzhen Wang",
        "Yan Zhang",
        "Chenyu Zheng",
        "Chongxuan Li",
        "Guoqiang Wu"
      ],
      "abstract": "The success of large generative models has driven a paradigm shift,\nleveraging massive multi-source data to enhance model capabilities. However,\nthe interaction among these sources remains theoretically underexplored. This\npaper takes the first step toward a rigorous analysis of multi-source training\nin conditional generative modeling, where each condition represents a distinct\ndata source. Specifically, we establish a general distribution estimation error\nbound in average total variation distance for conditional maximum likelihood\nestimation based on the bracketing number. Our result shows that when source\ndistributions share certain similarities and the model is expressive enough,\nmulti-source training guarantees a sharper bound than single-source training.\nWe further instantiate the general theory on conditional Gaussian estimation\nand deep generative models including autoregressive and flexible energy-based\nmodels, by characterizing their bracketing numbers. The results highlight that\nthe number of sources and similarity among source distributions improve the\nadvantage of multi-source training. Simulations and real-world experiments\nvalidate our theory. Code is available at:\n\\url{https://github.com/ML-GSAI/Multi-Source-GM}.",
      "pdf_url": "http://arxiv.org/pdf/2502.14583v1",
      "published": "2025-02-20T14:13:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14583v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Statistical Case Against Empirical Human-AI Alignment",
      "authors": [
        "Julian Rodemann",
        "Esteban Garces Arias",
        "Christoph Luther",
        "Christoph Jansen",
        "Thomas Augustin"
      ],
      "abstract": "Empirical human-AI alignment aims to make AI systems act in line with\nobserved human behavior. While noble in its goals, we argue that empirical\nalignment can inadvertently introduce statistical biases that warrant caution.\nThis position paper thus advocates against naive empirical alignment, offering\nprescriptive alignment and a posteriori empirical alignment as alternatives. We\nsubstantiate our principled argument by tangible examples like human-centric\ndecoding of language models.",
      "pdf_url": "http://arxiv.org/pdf/2502.14581v1",
      "published": "2025-02-20T14:12:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14581v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "stat.OT"
      ]
    },
    {
      "title": "Factor Graph-based Interpretable Neural Networks",
      "authors": [
        "Yicong Li",
        "Kuanjiu Zhou",
        "Shuo Yu",
        "Qiang Zhang",
        "Renqiang Luo",
        "Xiaodong Li",
        "Feng Xia"
      ],
      "abstract": "Comprehensible neural network explanations are foundations for a better\nunderstanding of decisions, especially when the input data are infused with\nmalicious perturbations. Existing solutions generally mitigate the impact of\nperturbations through adversarial training, yet they fail to generate\ncomprehensible explanations under unknown perturbations. To address this\nchallenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network,\nwhich is capable of generating comprehensible explanations under unknown\nperturbations. Instead of retraining like previous solutions, the proposed\nAGAIN directly integrates logical rules by which logical errors in explanations\nare identified and rectified during inference. Specifically, we construct the\nfactor graph to express logical rules between explanations and categories. By\ntreating logical rules as exogenous knowledge, AGAIN can identify\nincomprehensible explanations that violate real-world logic. Furthermore, we\npropose an interactive intervention switch strategy rectifying explanations\nbased on the logical guidance from the factor graph without learning\nperturbations, which overcomes the inherent limitation of adversarial\ntraining-based methods in defending only against known perturbations.\nAdditionally, we theoretically demonstrate the effectiveness of employing\nfactor graph by proving that the comprehensibility of explanations is strongly\ncorrelated with factor graph. Extensive experiments are conducted on three\ndatasets and experimental results illustrate the superior performance of AGAIN\ncompared to state-of-the-art baselines.",
      "pdf_url": "http://arxiv.org/pdf/2502.14572v1",
      "published": "2025-02-20T13:56:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.14572v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}