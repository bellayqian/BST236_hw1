{
  "last_updated": "2025-10-15T00:49:52.976706",
  "papers": [
    {
      "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images",
      "authors": [
        "Chengqi Duan",
        "Kaiyue Sun",
        "Rongyao Fang",
        "Manyuan Zhang",
        "Yan Feng",
        "Ying Luo",
        "Yufang Liu",
        "Ke Wang",
        "Peng Pei",
        "Xunliang Cai",
        "Hongsheng Li",
        "Yi Ma",
        "Xihui Liu"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
      "pdf_url": "http://arxiv.org/pdf/2510.11718v1",
      "published": "2025-10-13T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11718v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Adversarial Attacks Leverage Interference Between Features in Superposition",
      "authors": [
        "Edward Stevinson",
        "Lucas Prieto",
        "Melih Barsbey",
        "Tolga Birdal"
      ],
      "abstract": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs.",
      "pdf_url": "http://arxiv.org/pdf/2510.11709v1",
      "published": "2025-10-13T17:59:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11709v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering",
      "authors": [
        "Arjun Sahney",
        "Ram Gorthi",
        "Cezary ≈Åastowski",
        "Javier Vega"
      ],
      "abstract": "We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints.",
      "pdf_url": "http://arxiv.org/pdf/2510.11694v1",
      "published": "2025-10-13T17:54:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11694v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Scaling Language-Centric Omnimodal Representation Learning",
      "authors": [
        "Chenghao Xiao",
        "Hou Pong Chan",
        "Hao Zhang",
        "Weiwen Xu",
        "Mahani Aljunied",
        "Yu Rong"
      ],
      "abstract": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
      "pdf_url": "http://arxiv.org/pdf/2510.11693v1",
      "published": "2025-10-13T17:53:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11693v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation",
      "authors": [
        "Maggie Wang",
        "Stephen Tian",
        "Aiden Swann",
        "Ola Shorinwa",
        "Jiajun Wu",
        "Mac Schwager"
      ],
      "abstract": "Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ .",
      "pdf_url": "http://arxiv.org/pdf/2510.11689v1",
      "published": "2025-10-13T17:51:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11689v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities",
      "authors": [
        "Zicheng Liu",
        "Lige Huang",
        "Jie Zhang",
        "Dongrui Liu",
        "Yuan Tian",
        "Jing Shao"
      ],
      "abstract": "The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models.",
      "pdf_url": "http://arxiv.org/pdf/2510.11688v1",
      "published": "2025-10-13T17:50:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11688v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Representation-Based Exploration for Language Models: From Test-Time to Post-Training",
      "authors": [
        "Jens Tuyls",
        "Dylan J. Foster",
        "Akshay Krishnamurthy",
        "Jordan T. Ash"
      ],
      "abstract": "Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening.",
      "pdf_url": "http://arxiv.org/pdf/2510.11686v1",
      "published": "2025-10-13T17:49:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11686v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models",
      "authors": [
        "Nianyi Lin",
        "Jiajie Zhang",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks. Our codes\nand models are available at\n\\href{https://github.com/THU-KEG/BGPO}{https://github.com/THU-KEG/BGPO}.",
      "pdf_url": "http://arxiv.org/pdf/2510.11683v2",
      "published": "2025-10-13T17:47:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11683v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Ego-Vision World Model for Humanoid Contact Planning",
      "authors": [
        "Hang Liu",
        "Yuman Gao",
        "Sangli Teng",
        "Yufeng Chi",
        "Yakun Sophia Shao",
        "Zhongyu Li",
        "Maani Ghaffari",
        "Koushil Sreenath"
      ],
      "abstract": "Enabling humanoid robots to exploit physical contact, rather than simply\navoid collisions, is crucial for autonomy in unstructured environments.\nTraditional optimization-based planners struggle with contact complexity, while\non-policy reinforcement learning (RL) is sample-inefficient and has limited\nmulti-task ability. We propose a framework combining a learned world model with\nsampling-based Model Predictive Control (MPC), trained on a demonstration-free\noffline dataset to predict future outcomes in a compressed latent space. To\naddress sparse contact rewards and sensor noise, the MPC uses a learned\nsurrogate value function for dense, robust planning. Our single, scalable model\nsupports contact-aware tasks, including wall support after perturbation,\nblocking incoming objects, and traversing height-limited arches, with improved\ndata efficiency and multi-task capability over on-policy RL. Deployed on a\nphysical humanoid, our system achieves robust, real-time contact planning from\nproprioception and ego-centric depth images. Website:\nhttps://ego-vcp.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2510.11682v1",
      "published": "2025-10-13T17:47:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11682v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Accelerated stochastic first-order method for convex optimization under heavy-tailed noise",
      "authors": [
        "Chuan He",
        "Zhaosong Lu"
      ],
      "abstract": "We study convex composite optimization problems, where the objective function\nis given by the sum of a prox-friendly function and a convex function whose\nsubgradients are estimated under heavy-tailed noise. Existing work often\nemploys gradient clipping or normalization techniques in stochastic first-order\nmethods to address heavy-tailed noise. In this paper, we demonstrate that a\nvanilla stochastic algorithm -- without additional modifications such as\nclipping or normalization -- can achieve optimal complexity for these problems.\nIn particular, we establish that an accelerated stochastic proximal subgradient\nmethod achieves a first-order oracle complexity that is universally optimal for\nsmooth, weakly smooth, and nonsmooth convex optimization, as well as for\nstochastic convex optimization under heavy-tailed noise. Numerical experiments\nare further provided to validate our theoretical results.",
      "pdf_url": "http://arxiv.org/pdf/2510.11676v1",
      "published": "2025-10-13T17:45:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11676v1",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG",
        "stat.ML",
        "49M05, 49M37, 90C25, 90C30"
      ]
    },
    {
      "title": "FACE: Faithful Automatic Concept Extraction",
      "authors": [
        "Dipkamal Bhusal",
        "Michael Clifford",
        "Sara Rampazzi",
        "Nidhi Rastogi"
      ],
      "abstract": "Interpreting deep neural networks through concept-based explanations offers a\nbridge between low-level features and high-level human-understandable\nsemantics. However, existing automatic concept discovery methods often fail to\nalign these extracted concepts with the model's true decision-making process,\nthereby compromising explanation faithfulness. In this work, we propose FACE\n(Faithful Automatic Concept Extraction), a novel framework that augments\nNon-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence\nregularization term to ensure alignment between the model's original and\nconcept-based predictions. Unlike prior methods that operate solely on encoder\nactivations, FACE incorporates classifier supervision during concept learning,\nenforcing predictive consistency and enabling faithful explanations. We provide\ntheoretical guarantees showing that minimizing the KL divergence bounds the\ndeviation in predictive distributions, thereby promoting faithful local\nlinearity in the learned concept space. Systematic evaluations on ImageNet,\nCOCO, and CelebA datasets demonstrate that FACE outperforms existing methods\nacross faithfulness and sparsity metrics.",
      "pdf_url": "http://arxiv.org/pdf/2510.11675v1",
      "published": "2025-10-13T17:44:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11675v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI",
      "authors": [
        "Shijie Xia",
        "Yuhan Sun",
        "Pengfei Liu"
      ],
      "abstract": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2510.11661v1",
      "published": "2025-10-13T17:35:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11661v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
      "authors": [
        "Yi Yang",
        "Kefan Gu",
        "Yuqing Wen",
        "Hebei Li",
        "Yucheng Zhao",
        "Tiancai Wang",
        "Xudong Liu"
      ],
      "abstract": "While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets. The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/.",
      "pdf_url": "http://arxiv.org/pdf/2510.11660v2",
      "published": "2025-10-13T17:34:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11660v2",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection",
      "authors": [
        "Daniel Berhane Araya",
        "Duoduo Liao"
      ],
      "abstract": "Financial markets face growing threats from misinformation that can trigger\nbillions in losses in minutes. Most existing approaches lack transparency in\ntheir decision-making and provide limited attribution to credible sources. We\nintroduce FinVet, a novel multi-agent framework that integrates two\nRetrieval-Augmented Generation (RAG) pipelines with external fact-checking\nthrough a confidence-weighted voting mechanism. FinVet employs adaptive\nthree-tier processing that dynamically adjusts verification strategies based on\nretrieval confidence, from direct metadata extraction to hybrid reasoning to\nfull model-based analysis. Unlike existing methods, FinVet provides\nevidence-backed verdicts, source attribution, confidence scores, and explicit\nuncertainty flags when evidence is insufficient. Experimental evaluation on the\nFinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a\n10.4% improvement over the best individual pipeline (fact-check pipeline) and\n37% improvement over standalone RAG approaches.",
      "pdf_url": "http://arxiv.org/pdf/2510.11654v1",
      "published": "2025-10-13T17:31:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11654v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model",
      "authors": [
        "Prasanna Mayilvahanan",
        "Ricardo Dominguez-Olmedo",
        "Thadd√§us Wiedemer",
        "Wieland Brendel"
      ],
      "abstract": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.",
      "pdf_url": "http://arxiv.org/pdf/2510.11653v1",
      "published": "2025-10-13T17:30:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11653v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection",
      "authors": [
        "Krittin Chaowakarn",
        "Paramin Sangwongngam",
        "Nang Htet Htet Aung",
        "Chalie Charoenlarpnopparut"
      ],
      "abstract": "Recent studies in 3D object detection for autonomous vehicles aim to enrich\nfeatures through the utilization of multi-modal setups or the extraction of\nlocal patterns within LiDAR point clouds. However, multi-modal methods face\nsignificant challenges in feature alignment, and gaining features locally can\nbe oversimplified for complex 3D object detection tasks. In this paper, we\npropose a novel model, NV3D, which utilizes local features acquired from voxel\nneighbors, as normal vectors computed per voxel basis using K-nearest neighbors\n(KNN) and principal component analysis (PCA). This informative feature enables\nNV3D to determine the relationship between the surface and pertinent target\nentities, including cars, pedestrians, or cyclists. During the normal vector\nextraction process, NV3D offers two distinct sampling strategies: normal vector\ndensity-based sampling and FOV-aware bin-based sampling, allowing elimination\nof up to 55% of data while maintaining performance. In addition, we applied\nelement-wise attention fusion, which accepts voxel features as the query and\nvalue and normal vector features as the key, similar to the attention\nmechanism. Our method is trained on the KITTI dataset and has demonstrated\nsuperior performance in car and cyclist detection owing to their spatial\nshapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%\nmean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%\nand 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in\ncar detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of\nvoxels being filtered out.",
      "pdf_url": "http://arxiv.org/pdf/2510.11632v1",
      "published": "2025-10-13T17:13:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11632v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.2.6; I.2.9; I.2.10; I.4.8; I.4.10; I.5.1; I.5.4"
      ]
    },
    {
      "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
      "authors": [
        "Tobias Preintner",
        "Weixuan Yuan",
        "Adrian K√∂nig",
        "Thomas B√§ck",
        "Elena Raponi",
        "Niki van Stein"
      ],
      "abstract": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.",
      "pdf_url": "http://arxiv.org/pdf/2510.11631v1",
      "published": "2025-10-13T17:12:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11631v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Attention Factors for Statistical Arbitrage",
      "authors": [
        "Elliot L. Epstein",
        "Rose Wang",
        "Jaewon Choi",
        "Markus Pelger"
      ],
      "abstract": "Statistical arbitrage exploits temporal price differences between similar\nassets. We develop a framework to jointly identify similar assets through\nfactors, identify mispricing and form a trading policy that maximizes\nrisk-adjusted performance after trading costs. Our Attention Factors are\nconditional latent factors that are the most useful for arbitrage trading. They\nare learned from firm characteristic embeddings that allow for complex\ninteractions. We identify time-series signals from the residual portfolios of\nour factors with a general sequence model. Estimating factors and the arbitrage\ntrading strategy jointly is crucial to maximize profitability after trading\ncosts. In a comprehensive empirical study we show that our Attention Factor\nmodel achieves an out-of-sample Sharpe ratio above 4 on the largest U.S.\nequities over a 24-year period. Our one-step solution yields an unprecedented\nSharpe ratio of 2.3 net of transaction costs. We show that weak factors are\nimportant for arbitrage trading.",
      "pdf_url": "http://arxiv.org/pdf/2510.11616v1",
      "published": "2025-10-13T16:56:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11616v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.CP",
        "I.2.0"
      ]
    },
    {
      "title": "LLM-Oriented Token-Adaptive Knowledge Distillation",
      "authors": [
        "Xurong Xie",
        "Zhucun Xue",
        "Jiafu Wu",
        "Jian Li",
        "Yabiao Wang",
        "Xiaobin Hu",
        "Yong Liu",
        "Jiangning Zhang"
      ],
      "abstract": "Knowledge distillation (KD) is a key technique for compressing large-scale\nlanguage models (LLMs), yet prevailing logit-based methods typically employ\nstatic strategies that are misaligned with the dynamic learning process of\nstudent models. These methods typically treat all tokens indiscriminately and\napply a single, fixed temperature, resulting in suboptimal knowledge transfer.\nTo address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge\nDistillation (AdaKD), a novel framework that adapts the distillation process to\nthe real-time learning state of each token. AdaKD consists of two synergistic\nmodules driven by a unified token difficulty metric. First, our Loss-Driven\nAdaptive Token Focusing (LATF) module dynamically adjusts the distillation\nfocus by monitoring the student's learning stability, concentrating\ncomputational resources on the most valuable tokens at each training phase.\nSecond, we introduce Inverse Difficulty Temperature Scaling (IDTS), a\ncounterintuitive yet effective token-level temperature strategy. It employs low\ntemperatures for difficult tokens for targeted error correction, and high\ntemperatures for easy tokens to encourage students to learn from the teacher's\ncomplete and smooth output distribution, thereby enhancing generalization. As a\nplug-and-play framework, AdaKD can consistently improve the performance of\nvarious distillation methods on multiple model architectures and benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2510.11615v1",
      "published": "2025-10-13T16:55:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11615v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems",
      "authors": [
        "Shiqi Zhang",
        "Xinbei Ma",
        "Yunqing Xu",
        "Zouying Cao",
        "Pengrui Lu",
        "Haobo Yuan",
        "Tiancheng Shen",
        "Zhuosheng Zhang",
        "Hai Zhao",
        "Ming-Hsuan Yang"
      ],
      "abstract": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.",
      "pdf_url": "http://arxiv.org/pdf/2510.11608v1",
      "published": "2025-10-13T16:47:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11608v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce",
      "authors": [
        "Sanjula De Alwis",
        "Indrajith Ekanayake"
      ],
      "abstract": "In online retail, customer acquisition typically incurs higher costs than\ncustomer retention, motivating firms to invest in churn analytics. However,\nmany contemporary churn models operate as opaque black boxes, limiting insight\ninto the determinants of attrition, the timing of retention opportunities, and\nthe identification of high-risk customer segments. Accordingly, the emphasis\nshould shift from prediction alone to the design of personalized retention\nstrategies grounded in interpretable evidence. This study advances a\nthree-component framework that integrates explainable AI to quantify feature\ncontributions, survival analysis to model time-to-event churn risk, and RFM\nprofiling to segment customers by transactional behaviour. In combination,\nthese methods enable the attribution of churn drivers, estimation of\nintervention windows, and prioritization of segments for targeted actions,\nthereby supporting strategies that reduce attrition and strengthen customer\nloyalty.",
      "pdf_url": "http://arxiv.org/pdf/2510.11604v1",
      "published": "2025-10-13T16:44:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11604v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping",
      "authors": [
        "Marc Brinner",
        "Sina Zarrie√ü"
      ],
      "abstract": "We propose SemCSE-Multi, a novel unsupervised framework for generating\nmultifaceted embeddings of scientific abstracts, evaluated in the domains of\ninvasion biology and medicine. These embeddings capture distinct, individually\nspecifiable aspects in isolation, thus enabling fine-grained and controllable\nsimilarity assessments as well as adaptive, user-driven visualizations of\nscientific domains. Our approach relies on an unsupervised procedure that\nproduces aspect-specific summarizing sentences and trains embedding models to\nmap semantically related summaries to nearby positions in the embedding space.\nWe then distill these aspect-specific embedding capabilities into a unified\nembedding model that directly predicts multiple aspect embeddings from a\nscientific abstract in a single, efficient forward pass. In addition, we\nintroduce an embedding decoding pipeline that decodes embeddings back into\nnatural language descriptions of their associated aspects. Notably, we show\nthat this decoding remains effective even for unoccupied regions in\nlow-dimensional visualizations, thus offering vastly improved interpretability\nin user-centric settings.",
      "pdf_url": "http://arxiv.org/pdf/2510.11599v1",
      "published": "2025-10-13T16:38:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11599v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Reproducibility: The New Frontier in AI Governance",
      "authors": [
        "Israel Mason-Williams",
        "Gabryel Mason-Williams"
      ],
      "abstract": "AI policymakers are responsible for delivering effective governance\nmechanisms that can provide safe, aligned and trustworthy AI development.\nHowever, the information environment offered to policymakers is characterised\nby an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and\ncreating deep uncertainty and divides on which risks should be prioritised from\na governance perspective. We posit that the current publication speeds in AI\ncombined with the lack of strong scientific standards, via weak reproducibility\nprotocols, effectively erodes the power of policymakers to enact meaningful\npolicy and governance protocols. Our paper outlines how AI research could adopt\nstricter reproducibility guidelines to assist governance endeavours and improve\nconsensus on the AI risk landscape. We evaluate the forthcoming reproducibility\ncrisis within AI research through the lens of crises in other scientific\ndomains; providing a commentary on how adopting preregistration, increased\nstatistical power and negative result publication reproducibility protocols can\nenable effective AI governance. While we maintain that AI governance must be\nreactive due to AI's significant societal implications we argue that\npolicymakers and governments must consider reproducibility protocols as a core\ntool in the governance arsenal and demand higher standards for AI research.\nCode to replicate data and figures:\nhttps://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance",
      "pdf_url": "http://arxiv.org/pdf/2510.11595v1",
      "published": "2025-10-13T16:34:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11595v1",
      "categories": [
        "cs.AI",
        "cs.GL"
      ]
    },
    {
      "title": "Hierarchical Qubit-Merging Transformer for Quantum Error Correction",
      "authors": [
        "Seong-Joon Park",
        "Hee-Youl Kwak",
        "Yongjune Kim"
      ],
      "abstract": "For reliable large-scale quantum computation, a quantum error correction\n(QEC) scheme must effectively resolve physical errors to protect logical\ninformation. Leveraging recent advances in deep learning, neural network-based\ndecoders have emerged as a promising approach to enhance the reliability of\nQEC. We propose the Hierarchical Qubit-Merging Transformer (HQMT), a novel and\ngeneral decoding framework that explicitly leverages the structural graph of\nstabilizer codes to learn error correlations across multiple scales. Our\narchitecture first computes attention locally on structurally related groups of\nstabilizers and then systematically merges these qubit-centric representations\nto build a global view of the error syndrome. The proposed HQMT achieves\nsubstantially lower logical error rates for surface codes by integrating a\ndedicated qubit-merging layer within the transformer architecture. Across\nvarious code distances, HQMT significantly outperforms previous neural\nnetwork-based QEC decoders as well as a powerful belief propagation with\nordered statistics decoding (BP+OSD) baseline. This hierarchical approach\nprovides a scalable and effective framework for surface code decoding,\nadvancing the realization of reliable quantum computing.",
      "pdf_url": "http://arxiv.org/pdf/2510.11593v1",
      "published": "2025-10-13T16:31:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11593v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents",
      "authors": [
        "Jiateng Liu",
        "Zhenhailong Wang",
        "Xiaojiang Huang",
        "Yingjie Li",
        "Xing Fan",
        "Xiang Li",
        "Chenlei Guo",
        "Ruhi Sarikaya",
        "Heng Ji"
      ],
      "abstract": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata.",
      "pdf_url": "http://arxiv.org/pdf/2510.11588v1",
      "published": "2025-10-13T16:30:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11588v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Characterizing Web Search in The Age of Generative AI",
      "authors": [
        "Elisabeth Kirsten",
        "Jost Grosse Perdekamp",
        "Mihir Upadhyay",
        "Krishna P. Gummadi",
        "Muhammad Bilal Zafar"
      ],
      "abstract": "The advent of LLMs has given rise to a new type of web search: Generative\nsearch, where LLMs retrieve web pages related to a query and generate a single,\ncoherent text as a response. This output modality stands in stark contrast to\ntraditional web search, where results are returned as a ranked list of\nindependent web pages. In this paper, we ask: Along what dimensions do\ngenerative search outputs differ from traditional web search? We compare\nGoogle, a traditional web search engine, with four generative search engines\nfrom two providers (Google and OpenAI) across queries from four domains. Our\nanalysis reveals intriguing differences. Most generative search engines cover a\nwider range of sources compared to web search. Generative search engines vary\nin the degree to which they rely on internal knowledge contained within the\nmodel parameters v.s. external knowledge retrieved from the web. Generative\nsearch engines surface varying sets of concepts, creating new opportunities for\nenhancing search diversity and serendipity. Our results also highlight the need\nfor revisiting evaluation criteria for web search in the age of Generative AI.",
      "pdf_url": "http://arxiv.org/pdf/2510.11560v1",
      "published": "2025-10-13T16:04:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11560v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products",
      "authors": [
        "Komal Gupta",
        "Aditya Shrivastava"
      ],
      "abstract": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta.",
      "pdf_url": "http://arxiv.org/pdf/2510.11558v1",
      "published": "2025-10-13T16:00:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11558v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation",
      "authors": [
        "Yuchen Yan",
        "Zhihua Liu",
        "Hao Wang",
        "Weiming Li",
        "Xiaoshuai Hao"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance\nLarge Language Models (LLMs) by integrating external knowledge sources.\nHowever, multi-hop questions, which require the identification of multiple\nknowledge targets to form a synthesized answer, raise new challenges for RAG\nsystems. Under the multi-hop settings, existing methods often struggle to fully\nunderstand the questions with complex semantic structures and are susceptible\nto irrelevant noise during the retrieval of multiple information targets. To\naddress these limitations, we propose a novel graph representation learning\nframework for multi-hop question retrieval. We first introduce a\nMulti-information Level Knowledge Graph (Multi-L KG) to model various\ninformation levels for a more comprehensive understanding of multi-hop\nquestions. Based on this, we design a Query-Specific Graph Neural Network\n(QSGNN) for representation learning on the Multi-L KG. QSGNN employs\nintra/inter-level message passing mechanisms, and in each message passing the\ninformation aggregation is guided by the query, which not only facilitates\nmulti-granular information aggregation but also significantly reduces the\nimpact of noise. To enhance its ability to learn robust representations, we\nfurther propose two synthesized data generation strategies for pre-training the\nQSGNN. Extensive experimental results demonstrate the effectiveness of our\nframework in multi-hop scenarios, especially in high-hop questions the\nimprovement can reach 33.8\\%. The code is available at:\nhttps://github.com/Jerry2398/QSGNN.",
      "pdf_url": "http://arxiv.org/pdf/2510.11541v1",
      "published": "2025-10-13T15:41:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11541v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding Interactions with LLMs",
      "authors": [
        "Manaal Basha",
        "Aime√™ M. Ribeiro",
        "Jeena Javahar",
        "Cleidson R. B. de Souza",
        "Gema Rodr√≠guez-P√©rez"
      ],
      "abstract": "Understanding how developers interact with code generation tools (CGTs)\nrequires detailed, real-time data on programming behavior which is often\ndifficult to collect without disrupting workflow. We present\n\\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed\nto capture fine-grained interaction events from within the Visual Studio Code\n(VS Code) editor. \\textit{CodeWatcher} logs semantically meaningful events such\nas insertions made by CGTs, deletions, copy-paste actions, and focus shifts,\nenabling continuous monitoring of developer activity without modifying user\nworkflows. The system comprises a VS Code plugin, a Python-based RESTful API,\nand a MongoDB backend, all containerized for scalability and ease of\ndeployment. By structuring and timestamping each event, \\textit{CodeWatcher}\nenables post-hoc reconstruction of coding sessions and facilitates rich\nbehavioral analyses, including how and when CGTs are used during development.\nThis infrastructure is crucial for supporting research on responsible AI,\ndeveloper productivity, and the human-centered evaluation of CGTs. Please find\nthe demo, diagrams, and tool here: https://osf.io/j2kru/overview.",
      "pdf_url": "http://arxiv.org/pdf/2510.11536v1",
      "published": "2025-10-13T15:39:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11536v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "A Flexible Multi-Agent Deep Reinforcement Learning Framework for Dynamic Routing and Scheduling of Latency-Critical Services",
      "authors": [
        "Vincenzo Norman Vitale",
        "Antonia Maria Tulino",
        "Andreas F. Molisch",
        "Jaime Llorca"
      ],
      "abstract": "Timely delivery of delay-sensitive information over dynamic, heterogeneous\nnetworks is increasingly essential for a range of interactive applications,\nsuch as industrial automation, self-driving vehicles, and augmented reality.\nHowever, most existing network control solutions target only average delay\nperformance, falling short of providing strict End-to-End (E2E) peak latency\nguarantees. This paper addresses the challenge of reliably delivering packets\nwithin application-imposed deadlines by leveraging recent advancements in\nMulti-Agent Deep Reinforcement Learning (MA-DRL). After introducing the\nDelay-Constrained Maximum-Throughput (DCMT) dynamic network control problem,\nand highlighting the limitations of current solutions, we present a novel\nMA-DRL network control framework that leverages a centralized routing and\ndistributed scheduling architecture. The proposed framework leverages critical\nnetworking domain knowledge for the design of effective MA-DRL strategies based\non the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) technique, where\ncentralized routing and distributed scheduling agents dynamically assign paths\nand schedule packet transmissions according to packet lifetimes, thereby\nmaximizing on-time packet delivery. The generality of the proposed framework\nallows integrating both data-driven \\blue{Deep Reinforcement Learning (DRL)}\nagents and traditional rule-based policies in order to strike the right balance\nbetween performance and learning complexity. Our results confirm the\nsuperiority of the proposed framework with respect to traditional stochastic\noptimization-based approaches and provide key insights into the role and\ninterplay between data-driven DRL agents and new rule-based policies for both\nefficient and high-performance control of latency-critical services.",
      "pdf_url": "http://arxiv.org/pdf/2510.11535v1",
      "published": "2025-10-13T15:38:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11535v1",
      "categories": [
        "cs.NI",
        "cs.AI"
      ]
    },
    {
      "title": "Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns During Programming Tasks",
      "authors": [
        "Jeena Javahar",
        "Tanya Budhrani",
        "Manaal Basha",
        "Cleidson R. B. de Souza",
        "Ivan Beschastnikh",
        "Gema Rodriguez-Perez"
      ],
      "abstract": "The use of AI code-generation tools is becoming increasingly common, making\nit important to understand how software developers are adopting these tools. In\nthis study, we investigate how developers engage with Amazon's CodeWhisperer,\nan LLM-based code-generation tool. We conducted two user studies with two\ngroups of 10 participants each, interacting with CodeWhisperer - the first to\nunderstand which interactions were critical to capture and the second to\ncollect low-level interaction data using a custom telemetry plugin. Our\nmixed-methods analysis identified four behavioral patterns: 1) incremental code\nrefinement, 2) explicit instruction using natural language comments, 3)\nbaseline structuring with model suggestions, and 4) integrative use with\nexternal sources. We provide a comprehensive analysis of these patterns .",
      "pdf_url": "http://arxiv.org/pdf/2510.11516v1",
      "published": "2025-10-13T15:22:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11516v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference",
      "authors": [
        "Jianhao Yuan",
        "Fabio Pizzati",
        "Francesco Pinto",
        "Lars Kunze",
        "Ivan Laptev",
        "Paul Newman",
        "Philip Torr",
        "Daniele De Martini"
      ],
      "abstract": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.",
      "pdf_url": "http://arxiv.org/pdf/2510.11512v1",
      "published": "2025-10-13T15:19:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11512v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Automatic Music Sample Identification with Multi-Track Contrastive Learning",
      "authors": [
        "Alain Riou",
        "Joan Serr√†",
        "Yuki Mitsufuji"
      ],
      "abstract": "Sampling, the technique of reusing pieces of existing audio tracks to create\nnew music content, is a very common practice in modern music production. In\nthis paper, we tackle the challenging task of automatic sample identification,\nthat is, detecting such sampled content and retrieving the material from which\nit originates. To do so, we adopt a self-supervised learning approach that\nleverages a multi-track dataset to create positive pairs of artificial mixes,\nand design a novel contrastive learning objective. We show that such method\nsignificantly outperforms previous state-of-the-art baselines, that is robust\nto various genres, and that scales well when increasing the number of noise\nsongs in the reference database. In addition, we extensively analyze the\ncontribution of the different components of our training pipeline and\nhighlight, in particular, the need for high-quality separated stems for this\ntask.",
      "pdf_url": "http://arxiv.org/pdf/2510.11507v1",
      "published": "2025-10-13T15:17:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11507v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "People use fast, flat goal-directed simulation to reason about novel problems",
      "authors": [
        "Katherine M. Collins",
        "Cedegao E. Zhang",
        "Lionel Wong",
        "Mauricio Barba da Costa",
        "Graham Todd",
        "Adrian Weller",
        "Samuel J. Cheyette",
        "Thomas L. Griffiths",
        "Joshua B. Tenenbaum"
      ],
      "abstract": "Games have long been a microcosm for studying planning and reasoning in both\nnatural and artificial intelligence, especially with a focus on expert-level or\neven super-human play. But real life also pushes human intelligence along a\ndifferent frontier, requiring people to flexibly navigate decision-making\nproblems that they have never thought about before. Here, we use novice\ngameplay to study how people make decisions and form judgments in new problem\nsettings. We show that people are systematic and adaptively rational in how\nthey play a game for the first time, or evaluate a game (e.g., how fair or how\nfun it is likely to be) before they have played it even once. We explain these\ncapacities via a computational cognitive model that we call the \"Intuitive\nGamer\". The model is based on mechanisms of fast and flat (depth-limited)\ngoal-directed probabilistic simulation--analogous to those used in Monte Carlo\ntree-search models of expert game-play, but scaled down to use very few\nstochastic samples, simple goal heuristics for evaluating actions, and no deep\nsearch. In a series of large-scale behavioral studies with over 1000\nparticipants and 121 two-player strategic board games (almost all novel to our\nparticipants), our model quantitatively captures human judgments and decisions\nvarying the amount and kind of experience people have with a game--from no\nexperience at all (\"just thinking\"), to a single round of play, to indirect\nexperience watching another person and predicting how they should play--and\ndoes so significantly better than much more compute-intensive expert-level\nmodels. More broadly, our work offers new insights into how people rapidly\nevaluate, act, and make suggestions when encountering novel problems, and could\ninform the design of more flexible and human-like AI systems that can determine\nnot just how to solve new tasks, but whether a task is worth thinking about at\nall.",
      "pdf_url": "http://arxiv.org/pdf/2510.11503v1",
      "published": "2025-10-13T15:12:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11503v1",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "Offline Reinforcement Learning with Generative Trajectory Policies",
      "authors": [
        "Xinsong Feng",
        "Leshu Tang",
        "Chenan Wang",
        "Haipeng Chen"
      ],
      "abstract": "Generative models have emerged as a powerful class of policies for offline\nreinforcement learning (RL) due to their ability to capture complex,\nmulti-modal behaviors. However, existing methods face a stark trade-off: slow,\niterative models like diffusion policies are computationally expensive, while\nfast, single-step models like consistency policies often suffer from degraded\nperformance. In this paper, we demonstrate that it is possible to bridge this\ngap. The key to moving beyond the limitations of individual methods, we argue,\nlies in a unifying perspective that views modern generative models, including\ndiffusion, flow matching, and consistency models, as specific instances of\nlearning a continuous-time generative trajectory governed by an Ordinary\nDifferential Equation (ODE). This principled foundation provides a clearer\ndesign space for generative policies in RL and allows us to propose Generative\nTrajectory Policies (GTPs), a new and more general policy paradigm that learns\nthe entire solution map of the underlying ODE. To make this paradigm practical\nfor offline RL, we further introduce two key theoretically principled\nadaptations. Empirical results demonstrate that GTP achieves state-of-the-art\nperformance on D4RL benchmarks - it significantly outperforms prior generative\npolicies, achieving perfect scores on several notoriously hard AntMaze tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.11499v1",
      "published": "2025-10-13T15:06:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11499v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
      "authors": [
        "Zhiwei Jin",
        "Xiaohui Song",
        "Nan Wang",
        "Yafei Liu",
        "Chao Li",
        "Xin Li",
        "Ruichen Wang",
        "Zhihao Li",
        "Qi Qi",
        "Long Cheng",
        "Dongze Hao",
        "Quanlong Zheng",
        "Yanhao Zhang",
        "Haobo Ji",
        "Jian Ma",
        "Zhitong Zheng",
        "Zhenyi Lin",
        "Haolin Deng",
        "Xin Zou",
        "Xiaojie Yin",
        "Ruilin Wang",
        "Liankai Cai",
        "Haijing Liu",
        "Yuqing Qiu",
        "Ke Chen",
        "Zixian Li",
        "Chi Xie",
        "Huafei Li",
        "Chenxing Li",
        "Chuangchuang Wang",
        "Kai Tang",
        "Zhiguang Zhu",
        "Kai Tang",
        "Wenmei Gao",
        "Rui Wang",
        "Jun Wu",
        "Chao Liu",
        "Qin Xie",
        "Chen Chen",
        "Haonan Lu"
      ],
      "abstract": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
      "pdf_url": "http://arxiv.org/pdf/2510.11496v2",
      "published": "2025-10-13T15:04:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11496v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Investigating Large Language Models' Linguistic Abilities for Text Preprocessing",
      "authors": [
        "Marco Braga",
        "Gian Carlo Milanese",
        "Gabriella Pasi"
      ],
      "abstract": "Text preprocessing is a fundamental component of Natural Language Processing,\ninvolving techniques such as stopword removal, stemming, and lemmatization to\nprepare text as input for further processing and analysis. Despite the\ncontext-dependent nature of the above techniques, traditional methods usually\nignore contextual information. In this paper, we investigate the idea of using\nLarge Language Models (LLMs) to perform various preprocessing tasks, due to\ntheir ability to take context into account without requiring extensive\nlanguage-specific annotated resources. Through a comprehensive evaluation on\nweb-sourced data, we compare LLM-based preprocessing (specifically stopword\nremoval, lemmatization and stemming) to traditional algorithms across multiple\ntext classification tasks in six European languages. Our analysis indicates\nthat LLMs are capable of replicating traditional stopword removal,\nlemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,\nrespectively. Additionally, we show that ML algorithms trained on texts\npreprocessed by LLMs achieve an improvement of up to 6% with respect to the\n$F_1$ measure compared to traditional techniques. Our code, prompts, and\nresults are publicly available at\nhttps://github.com/GianCarloMilanese/llm_pipeline_wi-iat.",
      "pdf_url": "http://arxiv.org/pdf/2510.11482v1",
      "published": "2025-10-13T14:53:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11482v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning",
      "authors": [
        "Ardian Selmonaj",
        "Giacomo Del Rio",
        "Adrian Schneider",
        "Alessandro Antonucci"
      ],
      "abstract": "Achieving mission objectives in a realistic simulation of aerial combat is\nhighly challenging due to imperfect situational awareness and nonlinear flight\ndynamics. In this work, we introduce a novel 3D multi-agent air combat\nenvironment and a Hierarchical Multi-Agent Reinforcement Learning framework to\ntackle these challenges. Our approach combines heterogeneous agent dynamics,\ncurriculum learning, league-play, and a newly adapted training algorithm. To\nthis end, the decision-making process is organized into two abstraction levels:\nlow-level policies learn precise control maneuvers, while high-level policies\nissue tactical commands based on mission objectives. Empirical results show\nthat our hierarchical approach improves both learning efficiency and combat\nperformance in complex dogfight scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2510.11474v1",
      "published": "2025-10-13T14:44:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11474v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Iterative Amortized Inference: Unifying In-Context Learning and Learned Optimizers",
      "authors": [
        "Sarthak Mittal",
        "Divyat Mahajan",
        "Guillaume Lajoie",
        "Mohammad Pezeshki"
      ],
      "abstract": "Modern learning systems increasingly rely on amortized learning - the idea of\nreusing computation or inductive biases shared across tasks to enable rapid\ngeneralization to novel problems. This principle spans a range of approaches,\nincluding meta-learning, in-context learning, prompt tuning, learned optimizers\nand more. While motivated by similar goals, these approaches differ in how they\nencode and leverage task-specific information, often provided as in-context\nexamples. In this work, we propose a unified framework which describes how such\nmethods differ primarily in the aspects of learning they amortize - such as\ninitializations, learned updates, or predictive mappings - and how they\nincorporate task data at inference. We introduce a taxonomy that categorizes\namortized models into parametric, implicit, and explicit regimes, based on\nwhether task adaptation is externalized, internalized, or jointly modeled.\nBuilding on this view, we identify a key limitation in current approaches: most\nmethods struggle to scale to large datasets because their capacity to process\ntask data at inference (e.g., context length) is often limited. To address\nthis, we propose iterative amortized inference, a class of models that refine\nsolutions step-by-step over mini-batches, drawing inspiration from stochastic\noptimization. Our formulation bridges optimization-based meta-learning with\nforward-pass amortization in models like LLMs, offering a scalable and\nextensible foundation for general-purpose task adaptation.",
      "pdf_url": "http://arxiv.org/pdf/2510.11471v1",
      "published": "2025-10-13T14:40:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11471v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model",
      "authors": [
        "Yisen Gao",
        "Jiaxin Bai",
        "Yi Huang",
        "Xingcheng Fu",
        "Qingyun Sun",
        "Yangqiu Song"
      ],
      "abstract": "Deductive and abductive reasoning are two critical paradigms for analyzing\nknowledge graphs, enabling applications from financial query answering to\nscientific discovery. Deductive reasoning on knowledge graphs usually involves\nretrieving entities that satisfy a complex logical query, while abductive\nreasoning generates plausible logical hypotheses from observations. Despite\ntheir clear synergistic potential, where deduction can validate hypotheses and\nabduction can uncover deeper logical patterns, existing methods address them in\nisolation. To bridge this gap, we propose DARK, a unified framework for\nDeductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion\nmodel capable of capturing the bidirectional relationship between queries and\nconclusions, DARK has two key innovations. First, to better leverage deduction\nfor hypothesis refinement during abductive reasoning, we introduce a\nself-reflective denoising process that iteratively generates and validates\ncandidate hypotheses against the observed conclusion. Second, to discover\nricher logical associations, we propose a logic-exploration reinforcement\nlearning approach that simultaneously masks queries and conclusions, enabling\nthe model to explore novel reasoning compositions. Extensive experiments on\nmultiple benchmark knowledge graphs show that DARK achieves state-of-the-art\nperformance on both deductive and abductive reasoning tasks, demonstrating the\nsignificant benefits of our unified approach.",
      "pdf_url": "http://arxiv.org/pdf/2510.11462v1",
      "published": "2025-10-13T14:34:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11462v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization",
      "authors": [
        "Beining Wang",
        "Weihang Su",
        "Hongtao Tian",
        "Tao Yang",
        "Yujia Zhou",
        "Ting Yao",
        "Qingyao Ai",
        "Yiqun Liu"
      ],
      "abstract": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is\na critical yet challenging task. The dominant paradigm, outcome-supervised\nreinforcement learning (RLVR), rewards only correct final answers, often\npropagating flawed reasoning and suffering from sparse reward signals. While\nprocess-level reward models (PRMs) provide denser, step-by-step feedback, they\nlack generalizability and interpretability, requiring task-specific\nsegmentation of the reasoning process. To this end, we propose the\nDimension-level Reward Model (DRM), a new supervision framework that bridges\nthe gap between these two approaches. DRM evaluates the quality of a reasoning\nprocess along three fundamental, complementary, and interpretable dimensions:\nConfidence for uncertainty calibration, Relevance for semantic alignment, and\nCoherence for logical consistency. Together, these dimensions capture aspects\nbeyond final answer correctness and enable interpretable assessment without\nrequiring ground truth answers. Experimental results show that DRM provides\neffective supervision signals, guides the optimization of LLMs and enhances\ntheir reasoning ability. In particular, DRM-supervised training achieves\nconsistent gains on both in-distribution and out-of-distribution open-domain\ntasks, including mathematics, question answering, code execution, and puzzles.\nOur findings demonstrate that multidimensional supervision of the reasoning\nprocess can improve the generalized reasoning ability of LLMs beyond the\ntraining distribution.",
      "pdf_url": "http://arxiv.org/pdf/2510.11457v1",
      "published": "2025-10-13T14:29:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11457v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Audio-Maestro: Enhancing Large Audio-Language Models with Tool-Augmented Reasoning",
      "authors": [
        "Kuan-Yi Lee",
        "Tsung-En Lin",
        "Hung-Yi Lee"
      ],
      "abstract": "Recent advancements in large multimodal models (LMMs) have shown strong\ncapabilities in audio understanding. However, most systems rely solely on\nend-to-end reasoning, limiting interpretability and accuracy for tasks that\nrequire structured knowledge or specialized signal analysis. In this work, we\npresent Audio-Maestro -- a tool-augmented audio reasoning framework that\nenables audio-language models to autonomously call external tools and integrate\ntheir timestamped outputs into the reasoning process. This design allows the\nmodel to analyze, transform, and interpret audio signals through specialized\ntools rather than relying solely on end-to-end inference. Experiments show that\nAudio-Maestro consistently improves general audio reasoning performance:\nGemini-2.5-flash's average accuracy on MMAU-Test rises from 67.4% to 72.1%,\nDeSTA-2.5 from 58.3% to 62.8%, and GPT-4o from 60.8% to 63.9%. To our\nknowledge, Audio-Maestro is the first framework to integrate structured tool\noutput into the large audio language model reasoning process.",
      "pdf_url": "http://arxiv.org/pdf/2510.11454v1",
      "published": "2025-10-13T14:25:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11454v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder to Improve Cardiac Disease Detection of Wearable ECG Devices",
      "authors": [
        "Xinyan Guan",
        "Yongfan Lai",
        "Jiarui Jin",
        "Jun Li",
        "Haoyu Wang",
        "Qinghao Zhao",
        "Deyun Zhang",
        "Shijia Geng",
        "Shenda Hong"
      ],
      "abstract": "Twelve-lead electrocardiograms (ECGs) are the clinical gold standard for\ncardiac diagnosis, providing comprehensive spatial coverage of the heart\nnecessary to detect conditions such as myocardial infarction (MI). However,\ntheir lack of portability limits continuous and large-scale use. Three-lead ECG\nsystems are widely used in wearable devices due to their simplicity and\nmobility, but they often fail to capture pathologies in unmeasured regions. To\naddress this, we propose WearECG, a Variational Autoencoder (VAE) method that\nreconstructs twelve-lead ECGs from three leads: II, V1, and V5. Our model\nincludes architectural improvements to better capture temporal and spatial\ndependencies in ECG signals. We evaluate generation quality using MSE, MAE, and\nFrechet Inception Distance (FID), and assess clinical validity via a Turing\ntest with expert cardiologists. To further validate diagnostic utility, we\nfine-tune ECGFounder, a large-scale pretrained ECG model, on a multi-label\nclassification task involving over 40 cardiac conditions, including six\ndifferent myocardial infarction locations, using both real and generated\nsignals. Experiments on the MIMIC dataset show that our method produces\nphysiologically realistic and diagnostically informative signals, with robust\nperformance in downstream tasks. This work demonstrates the potential of\ngenerative modeling for ECG reconstruction and its implications for scalable,\nlow-cost cardiac screening.",
      "pdf_url": "http://arxiv.org/pdf/2510.11442v1",
      "published": "2025-10-13T14:14:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11442v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05",
        "I.2.6; I.2.7"
      ]
    },
    {
      "title": "KnowRL: Teaching Language Models to Know What They Know",
      "authors": [
        "Sahil Kale",
        "Devendra Singh Dhami"
      ],
      "abstract": "Truly reliable AI requires more than simply scaling up knowledge; it demands\nthe ability to know what it knows and when it does not. Yet recent research\nshows that even the best LLMs misjudge their own competence in more than one in\nfive cases, making any response born of such internal uncertainty impossible to\nfully trust. Inspired by self-improvement reinforcement learning techniques\nthat require minimal data, we present a simple but powerful framework KnowRL\nthat strengthens a model's internal understanding of its own feasibility\nboundaries, enabling safer and more responsible behaviour. Our framework\ncombines two components: (i) introspection, where the model generates and\nclassifies tasks it judges feasible or infeasible, and (ii) consensus-based\nrewarding, where stability of self-knowledge assessment is reinforced through\ninternal agreement. By using internally generated data, this design strengthens\nconsistency in self-knowledge and entirely avoids costly external supervision.\nIn experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved\nself-knowledge, validated by both intrinsic self-consistency and extrinsic\nbenchmarking. With nothing more than a small seed set and no external\nsupervision, our method drove gains as high as 28% in accuracy and 12% in F1,\noutperforming baselines in just a few iterations. Our framework essentially\nunlocks the untapped capacity of LLMs to self-improve their knowledge\nawareness, opening the door to reliable, more accountable AI and safer\ndeployment in critical applications. Owing to its simplicity and independence\nfrom external effort, we encourage applying this reliability-enhancing process\nto all future models.",
      "pdf_url": "http://arxiv.org/pdf/2510.11407v1",
      "published": "2025-10-13T13:47:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11407v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Living Off the LLM: How LLMs Will Change Adversary Tactics",
      "authors": [
        "Sean Oesch",
        "Jack Hutchins",
        "Luke Koch",
        "Kevin Kurian"
      ],
      "abstract": "In living off the land attacks, malicious actors use legitimate tools and\nprocesses already present on a system to avoid detection. In this paper, we\nexplore how the on-device LLMs of the future will become a security concern as\nthreat actors integrate LLMs into their living off the land attack pipeline and\nways the security community may mitigate this threat.",
      "pdf_url": "http://arxiv.org/pdf/2510.11398v1",
      "published": "2025-10-13T13:41:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11398v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
      "authors": [
        "Junpeng Liu",
        "Yuzhong Zhao",
        "Bowen Cao",
        "Jiayu Ding",
        "Yilin Jia",
        "Tengchao Lv",
        "Yupan Huang",
        "Shaohan Huang",
        "Nan Yang",
        "Li Dong",
        "Lei Cui",
        "Tao Ge",
        "Xun Wang",
        "Huitian Jiao",
        "Sun Mao",
        "FNU Kartik",
        "Si-Qing Chen",
        "Wai Lam",
        "Furu Wei"
      ],
      "abstract": "Recent advances in agentic workflows have enabled the automation of tasks\nsuch as professional document generation. However, they primarily focus on\ntextual quality, neglecting visual structure and style, which are crucial for\nreadability and engagement. This gap arises mainly from the absence of suitable\nreward models to guide agentic workflows toward producing documents with\nstronger structural and stylistic quality. To address this, we propose\nDocReward, a document reward model that evaluates documents based on their\nstructure and style. We construct a multi-domain dataset DocPair of 117K paired\ndocuments, covering 32 domains and 267 document types, each including a high-\nand low-professionalism document with identical content but different structure\nand style. This enables the model to evaluate professionalism comprehensively,\nand in a textual-quality-agnostic way. DocReward is trained using the\nBradley-Terry loss to score documents, penalizing predictions that contradict\nthe annotated ranking. To assess the performance of reward models, we create a\ntest dataset containing document bundles ranked by well-educated human\nevaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6\nand 19.4 percentage points, respectively, demonstrating its superiority over\nbaselines. In an extrinsic evaluation of document generation, DocReward\nachieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%\nwin rate, demonstrating its utility in guiding generation agents toward\nproducing human-preferred documents.",
      "pdf_url": "http://arxiv.org/pdf/2510.11391v1",
      "published": "2025-10-13T13:36:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11391v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Medical Interpretability and Knowledge Maps of Large Language Models",
      "authors": [
        "Razvan Marinescu",
        "Victoria-Elisabeth Gruber",
        "Diego Fajardo"
      ],
      "abstract": "We present a systematic study of medical-domain interpretability in Large\nLanguage Models (LLMs). We study how the LLMs both represent and process\nmedical knowledge through four different interpretability techniques: (1) UMAP\nprojections of intermediate activations, (2) gradient-based saliency with\nrespect to the model weights, (3) layer lesioning/removal and (4) activation\npatching. We present knowledge maps of five LLMs which show, at a\ncoarse-resolution, where knowledge about patient's ages, medical symptoms,\ndiseases and drugs is stored in the models. In particular for Llama3.3-70B, we\nfind that most medical knowledge is processed in the first half of the model's\nlayers. In addition, we find several interesting phenomena: (i) age is often\nencoded in a non-linear and sometimes discontinuous manner at intermediate\nlayers in the models, (ii) the disease progression representation is\nnon-monotonic and circular at certain layers of the model, (iii) in\nLlama3.3-70B, drugs cluster better by medical specialty rather than mechanism\nof action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B\nhave activations that collapse at intermediate layers but recover by the final\nlayers. These results can guide future research on fine-tuning, un-learning or\nde-biasing LLMs for medical tasks by suggesting at which layers in the model\nthese techniques should be applied.",
      "pdf_url": "http://arxiv.org/pdf/2510.11390v1",
      "published": "2025-10-13T13:34:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11390v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AI-Driven anemia diagnosis: A review of advanced models and techniques",
      "authors": [
        "Abdullah Al Mahmud",
        "Prangon Chowdhury",
        "Mohammed Borhan Uddin",
        "Khaled Eabne Delowar",
        "Tausifur Rahman Talha",
        "Bijoy Dewanjee"
      ],
      "abstract": "Anemia, a condition marked by insufficient levels of red blood cells or\nhemoglobin, remains a widespread health issue affecting millions of individuals\nglobally. Accurate and timely diagnosis is essential for effective management\nand treatment of anemia. In recent years, there has been a growing interest in\nthe use of artificial intelligence techniques, i.e., machine learning (ML) and\ndeep learning (DL) for the detection, classification, and diagnosis of anemia.\nThis paper provides a systematic review of the recent advancements in this\nfield, with a focus on various models applied to anemia detection. The review\nalso compares these models based on several performance metrics, including\naccuracy, sensitivity, specificity, and precision. By analyzing these metrics,\nthe paper evaluates the strengths and limitation of discussed models in\ndetecting and classifying anemia, emphasizing the importance of addressing\nthese factors to improve diagnostic accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2510.11380v1",
      "published": "2025-10-13T13:22:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11380v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning",
      "authors": [
        "Dean L. Slack",
        "Noura Al Moubayed"
      ],
      "abstract": "Although large language models excel across many tasks, they can memorise\ntraining data and thereby expose private or copyrighted text. Most defences\ntarget the pre-training stage, leaving memorisation during fine-tuning,\nespecially for domain adaptation and instruction tuning, poorly understood. We\nfine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on\ncommon evaluation datasets and track verbatim memorisation throughout training.\nWe find that memorisation increases dramatically in the first few epochs, often\nsignificantly before either validation perplexity or evaluation performance is\noptimised. We use a simple but effective n-gram memorisation score which\nreliably precedes verbatim memorisation; using it as an early-stopping\ncriterion mitigates memorisation with minimal performance loss. Further, we\nintroduce an n-gram-aware loss regulariser and show that it reduces\nmemorisation across all model families tested by up to 40% while minimising\nevaluation performance trade-offs when compared to an existing memorisation\nmitigation strategy. These results yield practical, scalable insights into\nmemorisation dynamics during language model fine-tuning.",
      "pdf_url": "http://arxiv.org/pdf/2510.11372v1",
      "published": "2025-10-13T13:12:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11372v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers",
      "authors": [
        "Wenhan Ma",
        "Hailin Zhang",
        "Liang Zhao",
        "Yifan Song",
        "Yudong Wang",
        "Zhifang Sui",
        "Fuli Luo"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.",
      "pdf_url": "http://arxiv.org/pdf/2510.11370v1",
      "published": "2025-10-13T13:11:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.11370v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    }
  ]
}