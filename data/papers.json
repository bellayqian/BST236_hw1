{
  "last_updated": "2025-10-20T00:53:52.892392",
  "papers": [
    {
      "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
      "authors": [
        "Hadi Alzayer",
        "Yunzhi Zhang",
        "Chen Geng",
        "Jia-Bin Huang",
        "Jiajun Wu"
      ],
      "abstract": "We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.",
      "pdf_url": "http://arxiv.org/pdf/2510.14981v1",
      "published": "2025-10-16T17:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14981v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale",
      "authors": [
        "Haiwen Diao",
        "Mingxuan Li",
        "Silei Wu",
        "Linjun Dai",
        "Xiaohua Wang",
        "Hanming Deng",
        "Lewei Lu",
        "Dahua Lin",
        "Ziwei Liu"
      ],
      "abstract": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
      "pdf_url": "http://arxiv.org/pdf/2510.14979v1",
      "published": "2025-10-16T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14979v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Agentic Design of Compositional Machines",
      "authors": [
        "Wenqian Zhang",
        "Weiyang Liu",
        "Zhen Liu"
      ],
      "abstract": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2510.14980v1",
      "published": "2025-10-16T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14980v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ]
    },
    {
      "title": "Terra: Explorable Native 3D World Model with Point Latents",
      "authors": [
        "Yuanhui Huang",
        "Weiliang Chen",
        "Wenzhao Zheng",
        "Xin Tao",
        "Pengfei Wan",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.",
      "pdf_url": "http://arxiv.org/pdf/2510.14977v1",
      "published": "2025-10-16T17:59:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14977v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
      "authors": [
        "Hengyuan Xu",
        "Wei Cheng",
        "Peng Xing",
        "Yixiao Fang",
        "Shuhan Wu",
        "Rui Wang",
        "Xianfang Zeng",
        "Daxin Jiang",
        "Gang Yu",
        "Xingjun Ma",
        "Yu-Gang Jiang"
      ],
      "abstract": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
      "pdf_url": "http://arxiv.org/pdf/2510.14975v1",
      "published": "2025-10-16T17:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14975v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
      "authors": [
        "Hansheng Chen",
        "Kai Zhang",
        "Hao Tan",
        "Leonidas Guibas",
        "Gordon Wetzstein",
        "Sai Bi"
      ],
      "abstract": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard $\\ell_2$ flow matching loss. By simply\nmimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
      "pdf_url": "http://arxiv.org/pdf/2510.14974v1",
      "published": "2025-10-16T17:59:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14974v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
      "authors": [
        "Quan Nguyen-Tri",
        "Mukul Ranjan",
        "Zhiqiang Shen"
      ],
      "abstract": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.14973v1",
      "published": "2025-10-16T17:59:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14973v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
      "authors": [
        "Yinxi Li",
        "Yuntian Deng",
        "Pengyu Nie"
      ],
      "abstract": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.14972v1",
      "published": "2025-10-16T17:59:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14972v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.PL",
        "cs.SE"
      ]
    },
    {
      "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training",
      "authors": [
        "Yiming Wang",
        "Da Yin",
        "Yuedong Cui",
        "Ruichen Zheng",
        "Zhiqian Li",
        "Zongyu Lin",
        "Di Wu",
        "Xueqing Wu",
        "Chenchen Ye",
        "Yu Zhou",
        "Kai-Wei Chang"
      ],
      "abstract": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
      "pdf_url": "http://arxiv.org/pdf/2510.14969v1",
      "published": "2025-10-16T17:59:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14969v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
      "authors": [
        "Mingxuan Yan",
        "Yuping Wang",
        "Zechun Liu",
        "Jiachen Li"
      ],
      "abstract": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io.",
      "pdf_url": "http://arxiv.org/pdf/2510.14968v1",
      "published": "2025-10-16T17:59:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14968v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents",
      "authors": [
        "Guoqing Wang",
        "Sunhao Dai",
        "Guangze Ye",
        "Zeyu Gan",
        "Wei Yao",
        "Yong Deng",
        "Xiaofeng Wu",
        "Zhenzhe Ying"
      ],
      "abstract": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2510.14967v1",
      "published": "2025-10-16T17:59:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14967v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "C4D: 4D Made from 3D through Dual Correspondences",
      "authors": [
        "Shizun Wang",
        "Zhenxiang Jiang",
        "Xingyi Yang",
        "Xinchao Wang"
      ],
      "abstract": "Recovering 4D from monocular video, which jointly estimates dynamic geometry\nand camera poses, is an inevitably challenging problem. While recent\npointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great\nprogress in reconstructing static scenes, directly applying them to dynamic\nscenes leads to inaccurate results. This discrepancy arises because moving\nobjects violate multi-view geometric constraints, disrupting the\nreconstruction. To address this, we introduce C4D, a framework that leverages\ntemporal Correspondences to extend existing 3D reconstruction formulation to\n4D. Specifically, apart from predicting pointmaps, C4D captures two types of\ncorrespondences: short-term optical flow and long-term point tracking. We train\na dynamic-aware point tracker that provides additional mobility information,\nfacilitating the estimation of motion masks to separate moving elements from\nthe static background, thus offering more reliable guidance for dynamic scenes.\nFurthermore, we introduce a set of dynamic scene optimization objectives to\nrecover per-frame 3D geometry and camera parameters. Simultaneously, the\ncorrespondences lift 2D trajectories into smooth 3D trajectories, enabling\nfully integrated 4D reconstruction. Experiments show that our framework\nachieves complete 4D recovery and demonstrates strong performance across\nmultiple downstream tasks, including depth estimation, camera pose estimation,\nand point tracking. Project Page: https://littlepure2333.github.io/C4D",
      "pdf_url": "http://arxiv.org/pdf/2510.14960v1",
      "published": "2025-10-16T17:59:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14960v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions",
      "authors": [
        "Lizhi Yang",
        "Blake Werner",
        "Massimiliano de Sa Aaron D. Ames"
      ],
      "abstract": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter.",
      "pdf_url": "http://arxiv.org/pdf/2510.14959v1",
      "published": "2025-10-16T17:58:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14959v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "RealDPO: Real or Not Real, that is the Preference",
      "authors": [
        "Guo Cheng",
        "Danni Yang",
        "Ziqi Huang",
        "Jianlou Si",
        "Chenyang Si",
        "Ziwei Liu"
      ],
      "abstract": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
      "pdf_url": "http://arxiv.org/pdf/2510.14955v1",
      "published": "2025-10-16T17:58:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14955v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion",
      "authors": [
        "Blake Werner",
        "Lizhi Yang",
        "Aaron D. Ames"
      ],
      "abstract": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion.",
      "pdf_url": "http://arxiv.org/pdf/2510.14947v1",
      "published": "2025-10-16T17:56:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14947v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics",
      "authors": [
        "Yuxing Lu",
        "Xukai Zhao",
        "J. Ben Tamo",
        "Micky C. Nnamdi",
        "Rui Peng",
        "Shuang Zeng",
        "Xingyu Hu",
        "Jinzhuo Wang",
        "May D. Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\ngeneral text; however, their proficiency in specialized scientific domains that\nrequire deep, interconnected knowledge remains largely uncharacterized.\nMetabolomics presents unique challenges with its complex biochemical pathways,\nheterogeneous identifier systems, and fragmented databases. To systematically\nevaluate LLM capabilities in this domain, we introduce MetaBench, the first\nbenchmark for metabolomics assessment. Curated from authoritative public\nresources, MetaBench evaluates five capabilities essential for metabolomics\nresearch: knowledge, understanding, grounding, reasoning, and research. Our\nevaluation of 25 open- and closed-source LLMs reveals distinct performance\npatterns across metabolomics tasks: while models perform well on text\ngeneration tasks, cross-database identifier grounding remains challenging even\nwith retrieval augmentation. Model performance also decreases on long-tail\nmetabolites with sparse annotations. With MetaBench, we provide essential\ninfrastructure for developing and evaluating metabolomics AI systems, enabling\nsystematic progress toward reliable computational tools for metabolomics\nresearch.",
      "pdf_url": "http://arxiv.org/pdf/2510.14944v1",
      "published": "2025-10-16T17:55:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14944v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ]
    },
    {
      "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
      "authors": [
        "Wenkai Yang",
        "Weijie Liu",
        "Ruobing Xie",
        "Yiju Guo",
        "Lulu Wu",
        "Saiyong Yang",
        "Yankai Lin"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
      "pdf_url": "http://arxiv.org/pdf/2510.14943v1",
      "published": "2025-10-16T17:55:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14943v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning",
      "authors": [
        "Yao Zhang",
        "Yu Wu",
        "Haowei Zhang",
        "Weiguo Li",
        "Haokun Chen",
        "Jingpei Wu",
        "Guohao Li",
        "Zhen Han",
        "Volker Tresp"
      ],
      "abstract": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2510.14942v1",
      "published": "2025-10-16T17:54:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14942v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Circuit Insights: Towards Interpretability Beyond Activations",
      "authors": [
        "Elena Golimblevskaia",
        "Aakriti Jain",
        "Bruno Puri",
        "Ammar Ibrahim",
        "Wojciech Samek",
        "Sebastian Lapuschkin"
      ],
      "abstract": "The fields of explainable AI and mechanistic interpretability aim to uncover\nthe internal structure of neural networks, with circuit discovery as a central\ntool for understanding model computations. Existing approaches, however, rely\non manual inspection and remain limited to toy tasks. Automated\ninterpretability offers scalability by analyzing isolated features and their\nactivations, but it often misses interactions between features and depends\nstrongly on external LLMs and dataset quality. Transcoders have recently made\nit possible to separate feature attributions into input-dependent and\ninput-invariant components, providing a foundation for more systematic circuit\nanalysis. Building on this, we propose WeightLens and CircuitLens, two\ncomplementary methods that go beyond activation-based analysis. WeightLens\ninterprets features directly from their learned weights, removing the need for\nexplainer models or datasets while matching or exceeding the performance of\nexisting methods on context-independent features. CircuitLens captures how\nfeature activations arise from interactions between components, revealing\ncircuit-level dynamics that activation-only approaches cannot identify.\nTogether, these methods increase interpretability robustness and enhance\nscalable mechanistic analysis of circuits while maintaining efficiency and\nquality.",
      "pdf_url": "http://arxiv.org/pdf/2510.14936v1",
      "published": "2025-10-16T17:49:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14936v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models",
      "authors": [
        "Akira Okutomi"
      ],
      "abstract": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we find that fragile internal dynamics correlate with miscalibration\nand hallucination, while critique-style prompts show mixed effects on\ncalibration and hallucination. These results suggest a structural bridge\nbetween Kantian self-limitation and feedback control, offering a principled\nlens for diagnosing -- and selectively reducing -- overconfidence in reasoning\nsystems. This is a preliminary version; supplementary experiments and broader\nreplication will be reported in a future revision.",
      "pdf_url": "http://arxiv.org/pdf/2510.14925v1",
      "published": "2025-10-16T17:40:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14925v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG",
      "authors": [
        "Annisaa Fitri Nurfidausi",
        "Eleonora Mancini",
        "Paolo Torroni"
      ],
      "abstract": "Depression is a widespread mental health disorder, yet its automatic\ndetection remains challenging. Prior work has explored unimodal and multimodal\napproaches, with multimodal systems showing promise by leveraging complementary\nsignals. However, existing studies are limited in scope, lack systematic\ncomparisons of features, and suffer from inconsistent evaluation protocols. We\naddress these gaps by systematically exploring feature representations and\nmodelling strategies across EEG, together with speech and text. We evaluate\nhandcrafted features versus pre-trained embeddings, assess the effectiveness of\ndifferent neural encoders, compare unimodal, bimodal, and trimodal\nconfigurations, and analyse fusion strategies with attention to the role of\nEEG. Consistent subject-independent splits are applied to ensure robust,\nreproducible benchmarking. Our results show that (i) the combination of EEG,\nspeech and text modalities enhances multimodal detection, (ii) pretrained\nembeddings outperform handcrafted features, and (iii) carefully designed\ntrimodal models achieve state-of-the-art performance. Our work lays the\ngroundwork for future research in multimodal depression detection.",
      "pdf_url": "http://arxiv.org/pdf/2510.14922v1",
      "published": "2025-10-16T17:39:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14922v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ]
    },
    {
      "title": "Predicting Task Performance with Context-aware Scaling Laws",
      "authors": [
        "Kyle Montgomery",
        "David Park",
        "Jianhong Tu",
        "Michael Bendersky",
        "Beliz Gunel",
        "Dawn Song",
        "Chenguang Wang"
      ],
      "abstract": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling.",
      "pdf_url": "http://arxiv.org/pdf/2510.14919v1",
      "published": "2025-10-16T17:35:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14919v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Budget-aware Test-time Scaling via Discriminative Verification",
      "authors": [
        "Kyle Montgomery",
        "Sijun Tan",
        "Yuqi Chen",
        "Siyuan Zhuang",
        "Tianjun Zhang",
        "Raluca Ada Popa",
        "Chenguang Wang"
      ],
      "abstract": "Test-time scaling is a powerful strategy for boosting the performance of\nlarge language models on complex reasoning tasks. While state-of-the-art\napproaches often employ generative verifiers to select the best solution from a\npool of candidates, this method incurs prohibitive computational costs,\nlimiting its practicality. In this work, we shift the focus to a more\nbudget-aware paradigm: discriminative verification. We conduct a thorough\nempirical analysis and demonstrate that while discriminative verifiers may\nunderperform in isolation, combining them with self-consistency in a hybrid\napproach creates a powerful and efficient test-time scaling mechanism. Notably,\nunder a fixed compute budget, this hybrid approach surpasses state-of-the-art\ngenerative verification by a significant margin: achieving up to 15.3\\% higher\naccuracy on AIME2025. Our findings establish that for practical, real-world\napplications, budget-aware scaling with discriminative verifiers is not only a\n\"free\" upgrade over self-consistency, but also a more effective and efficient\nalternative to costly generative techniques. Code is available at\nhttps://github.com/wang-research-lab/verification.",
      "pdf_url": "http://arxiv.org/pdf/2510.14913v1",
      "published": "2025-10-16T17:30:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14913v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos",
      "authors": [
        "Gabriel Fiastre",
        "Antoine Yang",
        "Cordelia Schmid"
      ],
      "abstract": "Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.",
      "pdf_url": "http://arxiv.org/pdf/2510.14904v1",
      "published": "2025-10-16T17:20:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14904v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
      "authors": [
        "Aayush Karan",
        "Yilun Du"
      ],
      "abstract": "Frontier reasoning models have exhibited incredible capabilities across a\nwide array of disciplines, driven by posttraining large language models (LLMs)\nwith reinforcement learning (RL). However, despite the widespread success of\nthis paradigm, much of the literature has been devoted to disentangling truly\nnovel behaviors that emerge during RL but are not present in the base models.\nIn our work, we approach this question from a different angle, instead asking\nwhether comparable reasoning capabilites can be elicited from base models at\ninference time by pure sampling, without any additional training. Inspired by\nMarkov chain Monte Carlo (MCMC) techniques for sampling from sharpened\ndistributions, we propose a simple iterative sampling algorithm leveraging the\nbase models' own likelihoods. Over different base models, we show that our\nalgorithm offers substantial boosts in reasoning that nearly match and even\noutperform those from RL on a wide variety of single-shot tasks, including\nMATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in\ndiversity over multiple samples that is characteristic of RL-posttraining.\nCrucially, our method does not require training, curated datasets, or a\nverifier, suggesting broad applicability beyond easily verifiable domains.",
      "pdf_url": "http://arxiv.org/pdf/2510.14901v1",
      "published": "2025-10-16T17:18:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14901v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates",
      "authors": [
        "Wen-Kwang Tsao",
        "Yao-Ching Yu",
        "Chien-Ming Huang"
      ],
      "abstract": "The Enterprise Intelligence Platform must integrate logs from numerous\nthird-party vendors in order to perform various downstream tasks. However,\nvendor documentation is often unavailable at test time. It is either misplaced,\nmismatched, poorly formatted, or incomplete, which makes schema mapping\nchallenging. We introduce a reinforcement learning agent that can self-improve\nwithout labeled examples or model weight updates. During inference, the agent:\n1) Identifies ambiguous field-mapping attempts. 2) Generates targeted\nweb-search queries to gather external evidence. 3) Applies a confidence-based\nreward to iteratively refine its mappings. To demonstrate this concept, we\nconverted Microsoft Defender for Endpoint logs into a common schema. Our method\nincreased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\%\nover 100 iterations using GPT-4o. At the same time, it reduced the number of\nlow-confidence mappings requiring expert review by 85\\%. This new approach\nprovides an evidence-driven, transparent method for solving future industry\nproblems, paving the way for more robust, accountable, scalable, efficient,\nflexible, adaptable, and collaborative solutions.",
      "pdf_url": "http://arxiv.org/pdf/2510.14900v1",
      "published": "2025-10-16T17:17:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14900v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media",
      "authors": [
        "Soorya Ram Shimgekar",
        "Ruining Zhao",
        "Agam Goyal",
        "Violeta J. Rodriguez",
        "Paul A. Bloom",
        "Hari Sundaram",
        "Koustuv Saha"
      ],
      "abstract": "On social media, many individuals experiencing suicidal ideation (SI) do not\ndisclose their distress explicitly. Instead, signs may surface indirectly\nthrough everyday posts or peer interactions. Detecting such implicit signals\nearly is critical but remains challenging. We frame early and implicit SI as a\nforward-looking prediction task and develop a computational framework that\nmodels a user's information environment, consisting of both their longitudinal\nposting histories as well as the discourse of their socially proximal peers. We\nadopted a composite network centrality measure to identify top neighbors of a\nuser, and temporally aligned the user's and neighbors' interactions --\nintegrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a\nReddit study of 1,000 (500 Case and 500 Control) users, our approach improves\nearly and implicit SI detection by 15% over individual-only baselines. These\nfindings highlight that peer interactions offer valuable predictive signals and\ncarry broader implications for designing early detection systems that capture\nindirect as well as masked expressions of risk in online environments.",
      "pdf_url": "http://arxiv.org/pdf/2510.14889v1",
      "published": "2025-10-16T17:09:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14889v1",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC"
      ]
    },
    {
      "title": "Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards",
      "authors": [
        "Sarah Liaw",
        "Benjamin Plaut"
      ],
      "abstract": "In high-stakes AI applications, even a single action can cause irreparable\ndamage. However, nearly all of sequential decision-making theory assumes that\nall errors are recoverable (e.g., by bounding rewards). Standard bandit\nalgorithms that explore aggressively may cause irreparable damage when this\nassumption fails. Some prior work avoids irreparable errors by asking for help\nfrom a mentor, but a mentor may not always be available. In this work, we\nformalize a model of learning with unbounded rewards without a mentor as a\ntwo-action contextual bandit with an abstain option: at each round the agent\nobserves an input and chooses either to abstain (always 0 reward) or to commit\n(execute a preexisting task policy). Committing yields rewards that are\nupper-bounded but can be arbitrarily negative, and the commit reward is assumed\nLipschitz in the input. We propose a caution-based algorithm that learns when\nnot to learn: it chooses a trusted region and commits only where the available\nevidence does not already certify harm. Under these conditions and i.i.d.\ninputs, we establish sublinear regret guarantees, theoretically demonstrating\nthe effectiveness of cautious exploration for deploying learning agents safely\nin high-stakes environments.",
      "pdf_url": "http://arxiv.org/pdf/2510.14884v1",
      "published": "2025-10-16T17:01:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14884v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The Gatekeeper Knows Enough",
      "authors": [
        "Fikresilase Wondmeneh Abebayew"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as autonomous agents,\nyet their practical utility is fundamentally constrained by a limited context\nwindow and state desynchronization resulting from the LLMs' stateless nature\nand inefficient context management. These limitations lead to unreliable\noutput, unpredictable behavior, and inefficient resource usage, particularly\nwhen interacting with large, structured, and sensitive knowledge systems such\nas codebases and documents. To address these challenges, we introduce the\nGatekeeper Protocol, a novel, domain-agnostic framework that governs\nagent-system interactions. Our protocol mandates that the agent first operate\nand reason on a minimalist, low-fidelity \"latent state\" representation of the\nsystem to strategically request high-fidelity context on demand. All\ninteractions are mediated through a unified JSON format that serves as a\ndeclarative, state-synchronized protocol, ensuring the agent's model of the\nsystem remains verifiably grounded in the system's reality. We demonstrate the\nefficacy of this protocol with Sage, a reference implementation of the\nGatekeeper Protocol for software development. Our results show that this\napproach significantly increases agent reliability, improves computational\nefficiency by minimizing token consumption, and enables scalable interaction\nwith complex systems, creating a foundational methodology for building more\nrobust, predictable, and grounded AI agents for any structured knowledge\ndomain.",
      "pdf_url": "http://arxiv.org/pdf/2510.14881v1",
      "published": "2025-10-16T17:00:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14881v1",
      "categories": [
        "cs.AI",
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "Predicting kernel regression learning curves from only raw data statistics",
      "authors": [
        "Dhruva Karkada",
        "Joseph Turnbull",
        "Yuxi Liu",
        "James B. Simon"
      ],
      "abstract": "We study kernel regression with common rotation-invariant kernels on real\ndatasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical\nframework that predicts learning curves (test risk vs. sample size) from only\ntwo measurements: the empirical data covariance matrix and an empirical\npolynomial decomposition of the target function $f_*$. The key new idea is an\nanalytical approximation of a kernel's eigenvalues and eigenfunctions with\nrespect to an anisotropic data distribution. The eigenfunctions resemble\nHermite polynomials of the data, so we call this approximation the Hermite\neigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find\nthat real image data is often \"Gaussian enough\" for the HEA to hold well in\npractice, enabling us to predict learning curves by applying prior results\nrelating kernel eigenstructure to test risk. Extending beyond kernel\nregression, we empirically find that MLPs in the feature-learning regime learn\nHermite polynomials in the order predicted by the HEA. Our HEA framework is a\nproof of concept that an end-to-end theory of learning which maps dataset\nstructure all the way to model performance is possible for nontrivial learning\nalgorithms on real datasets.",
      "pdf_url": "http://arxiv.org/pdf/2510.14878v1",
      "published": "2025-10-16T16:57:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14878v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Benchmarking Multimodal Large Language Models for Face Recognition",
      "authors": [
        "Hatef Otroshi Shahreza",
        "Sébastien Marcel"
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved remarkable performance\nacross diverse vision-and-language tasks. However, their potential in face\nrecognition remains underexplored. In particular, the performance of\nopen-source MLLMs needs to be evaluated and compared with existing face\nrecognition models on standard benchmarks with similar protocol. In this work,\nwe present a systematic benchmark of state-of-the-art MLLMs for face\nrecognition on several face recognition datasets, including LFW, CALFW, CPLFW,\nCFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich\nsemantic cues useful for face-related tasks, they lag behind specialized models\nin high-precision recognition scenarios in zero-shot applications. This\nbenchmark provides a foundation for advancing MLLM-based face recognition,\noffering insights for the design of next-generation models with higher accuracy\nand generalization. The source code of our benchmark is publicly available in\nthe project page.",
      "pdf_url": "http://arxiv.org/pdf/2510.14866v1",
      "published": "2025-10-16T16:42:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14866v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans",
      "authors": [
        "Le Cong",
        "Zaixi Zhang",
        "Xiaotong Wang",
        "Yin Di",
        "Ruofan Jin",
        "Michal Gerasimiuk",
        "Yinkai Wang",
        "Ravi K. Dinesh",
        "David Smerkous",
        "Alex Smerkous",
        "Xuekun Wu",
        "Shilong Liu",
        "Peishan Li",
        "Yi Zhu",
        "Simran Serrao",
        "Ning Zhao",
        "Imran A. Mohammad",
        "John B. Sunwoo",
        "Joseph C. Wu",
        "Mengdi Wang"
      ],
      "abstract": "Modern science advances fastest when thought meets action. LabOS represents\nthe first AI co-scientist that unites computational reasoning with physical\nexperimentation through multimodal perception, self-evolving agents, and\nEntended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model\nAI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see\nwhat scientists see, understand experimental context, and assist in real-time\nexecution. Across applications--from cancer immunotherapy target discovery to\nstem-cell engineering -- LabOS shows that AI can move beyond computational\ndesign to participation, turning the laboratory into an intelligent,\ncollaborative environment where human and machine discovery evolve together.",
      "pdf_url": "http://arxiv.org/pdf/2510.14861v1",
      "published": "2025-10-16T16:36:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14861v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
      "authors": [
        "Zhuo-Yang Song"
      ],
      "abstract": "The generate-filter-refine (iterative) paradigm based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.14846v2",
      "published": "2025-10-16T16:18:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14846v2",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ]
    },
    {
      "title": "Boosting Instruction Following at Scale",
      "authors": [
        "Ben Elder",
        "Evelyn Duesterwald",
        "Vinod Muthusamy"
      ],
      "abstract": "A typical approach developers follow to influence an LLM's behavior in an\napplication is through careful manipulation of the prompt, such as by adding or\nmodifying instructions. However, merely adding more instructions provides\nlittle assurance that they will actually be followed. We introduce Instruction\nBoosting as a post-generation method to increase the reliability of LLM prompt\ninstructions. We show that Instruction Boosting improves the instruction\nfollowing rate by up to 7 points for two instructions and up to 4 points for\nten instructions. To demonstrate these results we introduce SCALEDIF, a\nbenchmark with a scaled instruction volume of up to ten instructions per data\nsample. We also present an analysis of the commonly observed trend that\nperformance degrades as more instructions are added. We show that an important\nfactor contributing to this trend is the degree of tension and conflict that\narises as the number of instructions is increased. We contribute a quantitative\nconflict scoring tool that explains the observed performance trends and\nprovides feedback to developers on the impact that additional prompt\ninstructions have on a model's performance.",
      "pdf_url": "http://arxiv.org/pdf/2510.14842v1",
      "published": "2025-10-16T16:15:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14842v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning",
      "authors": [
        "Kun Lei",
        "Huanyu Li",
        "Dongjie Yu",
        "Zhenyu Wei",
        "Lingxiao Guo",
        "Zhennan Jiang",
        "Ziyu Wang",
        "Shiyu Liang",
        "Huazhe Xu"
      ],
      "abstract": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours.",
      "pdf_url": "http://arxiv.org/pdf/2510.14830v1",
      "published": "2025-10-16T16:07:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14830v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning",
      "authors": [
        "Jinrui Liu",
        "Bingyan Nie",
        "Boyu Li",
        "Yaran Chen",
        "Yuze Wang",
        "Shunsen He",
        "Haoran Li"
      ],
      "abstract": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.",
      "pdf_url": "http://arxiv.org/pdf/2510.14828v1",
      "published": "2025-10-16T16:04:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14828v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Agentic NL2SQL to Reduce Computational Costs",
      "authors": [
        "Dominik Jehle",
        "Lennart Purucker",
        "Frank Hutter"
      ],
      "abstract": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)\nhas recently been empowered by large language models (LLMs). Using LLMs to\nperform NL2SQL methods on a large collection of SQL databases necessitates\nprocessing large quantities of meta-information about the databases, which in\nturn results in lengthy prompts with many tokens and high processing costs. To\naddress this challenge, we introduce Datalake Agent, an agentic system designed\nto enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing\ndirect solvers for NL2SQL that call the LLM once with all meta-information in\nthe prompt, the Datalake Agent employs an interactive loop to reduce the\nutilized meta-information. Within the loop, the LLM is used in a reasoning\nframework that selectively requests only the necessary information to solve a\ntable question answering task. We evaluate the Datalake Agent on a collection\nof 23 databases with 100 table question answering tasks. The Datalake Agent\nreduces the tokens used by the LLM by up to 87\\% and thus allows for\nsubstantial cost reductions while maintaining competitive performance.",
      "pdf_url": "http://arxiv.org/pdf/2510.14808v1",
      "published": "2025-10-16T15:42:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14808v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SimKO: Simple Pass@K Policy Optimization",
      "authors": [
        "Ruotian Peng",
        "Yi Ren",
        "Zhouliang Yu",
        "Weiyang Liu",
        "Yandong Wen"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration.",
      "pdf_url": "http://arxiv.org/pdf/2510.14807v1",
      "published": "2025-10-16T15:40:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14807v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks",
      "authors": [
        "Pedro R. A. S. Bassi",
        "Xinze Zhou",
        "Wenxuan Li",
        "Szymon Płotka",
        "Jieneng Chen",
        "Qi Chen",
        "Zheren Zhu",
        "Jakub Prządo",
        "Ibrahim E. Hamacı",
        "Sezgin Er",
        "Yuhan Wang",
        "Ashwin Kumar",
        "Bjoern Menze",
        "Jarosław B. Ćwikła",
        "Yuyin Zhou",
        "Akshay S. Chaudhari",
        "Curtis P. Langlotz",
        "Sergio Decherchi",
        "Andrea Cavalli",
        "Kang Wang",
        "Yang Yang",
        "Alan L. Yuille",
        "Zongwei Zhou"
      ],
      "abstract": "Early tumor detection save lives. Each year, more than 300 million computed\ntomography (CT) scans are performed worldwide, offering a vast opportunity for\neffective cancer screening. However, detecting small or early-stage tumors on\nthese CT scans remains challenging, even for experts. Artificial intelligence\n(AI) models can assist by highlighting suspicious regions, but training such\nmodels typically requires extensive tumor masks--detailed, voxel-wise outlines\nof tumors manually drawn by radiologists. Drawing these masks is costly,\nrequiring years of effort and millions of dollars. In contrast, nearly every CT\nscan in clinical practice is already accompanied by medical reports describing\nthe tumor's size, number, appearance, and sometimes, pathology\nresults--information that is rich, abundant, and often underutilized for AI\ntraining. We introduce R-Super, which trains AI to segment tumors that match\ntheir descriptions in medical reports. This approach scales AI training with\nlarge collections of readily available medical reports, substantially reducing\nthe need for manually drawn tumor masks. When trained on 101,654 reports, AI\nmodels achieved performance comparable to those trained on 723 masks. Combining\nreports and masks further improved sensitivity by +13% and specificity by +8%,\nsurpassing radiologists in detecting five of the seven tumor types. Notably,\nR-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,\nbladder, uterus, and esophagus, for which no public masks or AI models\npreviously existed. This study challenges the long-held belief that\nlarge-scale, labor-intensive tumor mask creation is indispensable, establishing\na scalable and accessible path toward early detection across diverse tumor\ntypes.\n  We plan to release our trained models, code, and dataset at\nhttps://github.com/MrGiovanni/R-Super",
      "pdf_url": "http://arxiv.org/pdf/2510.14803v1",
      "published": "2025-10-16T15:35:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14803v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images",
      "authors": [
        "Usama Sajjad",
        "Abdul Rehman Akbar",
        "Ziyu Su",
        "Deborah Knight",
        "Wendy L. Frankel",
        "Metin N. Gurcan",
        "Wei Chen",
        "Muhammad Khalid Khan Niazi"
      ],
      "abstract": "Colorectal cancer (CRC) remains the third most prevalent malignancy globally,\nwith approximately 154,000 new cases and 54,000 projected deaths anticipated\nfor 2025. The recent advancement of foundation models in computational\npathology has been largely propelled by task agnostic methodologies that can\noverlook organ-specific crucial morphological patterns that represent distinct\nbiological processes that can fundamentally influence tumor behavior,\ntherapeutic response, and patient outcomes. The aim of this study is to develop\na novel, interpretable AI model, PRISM (Prognostic Representation of Integrated\nSpatial Morphology), that incorporates a continuous variability spectrum within\neach distinct morphology to characterize phenotypic diversity and reflecting\nthe principle that malignant transformation occurs through incremental\nevolutionary processes rather than abrupt phenotypic shifts. PRISM is trained\non 8.74 million histological images extracted from surgical resection specimens\nof 424 patients with stage III CRC. PRISM achieved superior prognostic\nperformance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;\nHR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific\nmethods by 15% and AI foundation models by ~23% accuracy. It showed\nsex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable\nperformance across clinicopathological subgroups, with minimal accuracy\nfluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,\nreplicating the Alliance cohort finding of no survival difference between\ntreatments.",
      "pdf_url": "http://arxiv.org/pdf/2510.14800v1",
      "published": "2025-10-16T15:32:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14800v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
      "authors": [
        "Manjie Xu",
        "Cheng Chen",
        "Xin Jia",
        "Jingyi Zhou",
        "Yongji Wu",
        "Zejian Wang",
        "Chi Zhang",
        "Kai Zuo",
        "Yibo Chen",
        "Xu Tang",
        "Yao Hu",
        "Yixin Zhu"
      ],
      "abstract": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms.",
      "pdf_url": "http://arxiv.org/pdf/2510.14788v1",
      "published": "2025-10-16T15:20:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14788v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning",
      "authors": [
        "Hwiyeol Jo",
        "Joosung Lee",
        "Jaehone Lee",
        "Sang-Woo Lee",
        "Joonsuk Park",
        "Kang Min Yoo"
      ],
      "abstract": "Evaluating generative models, such as large language models (LLMs), commonly\ninvolves question-answering tasks where the final answer is selected based on\nprobability of answer choices. On the other hand, for models requiring\nreasoning, the method of answer extraction plays a critical role. Our research\nreveals that the performance of reasoning models and their final answer\ndistributions are highly sensitive to the answer extraction algorithm employed.\nIn order to mitigate this, we propose a basic framework: Answer Regeneration.\nThe method uses an additional model inference, providing the prior input and\noutput prefaced by the prompt \"Answer:\". The final answer is then selected or\nextracted from the regenerated output. We show that this\nextraction-rule-agnostic approach exhibits improved performance and enhanced\nrobustness. Furthermore, we have applied this framework to general math\nproblems and open-ended question answering tasks. Our analysis and this\nframework could offer a more reliable results for model evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2510.14773v1",
      "published": "2025-10-16T15:09:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14773v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality",
      "authors": [
        "Giuseppe Lorenzo Catalano",
        "Agata Marta Soccini"
      ],
      "abstract": "Space exploration increasingly relies on Virtual Reality for several tasks,\nsuch as mission planning, multidisciplinary scientific analysis, and astronaut\ntraining. A key factor for the reliability of the simulations is having\naccurate 3D representations of planetary terrains. Extraterrestrial heightmaps\nderived from satellite imagery often contain missing values due to acquisition\nand transmission constraints. Mars is among the most studied planets beyond\nEarth, and its extensive terrain datasets make the Martian surface\nreconstruction a valuable task, although many areas remain unmapped. Deep\nlearning algorithms can support void-filling tasks; however, whereas Earth's\ncomprehensive datasets enables the use of conditional methods, such approaches\ncannot be applied to Mars. Current approaches rely on simpler interpolation\ntechniques which, however, often fail to preserve geometric coherence. In this\nwork, we propose a method for reconstructing the surface of Mars based on an\nunconditional diffusion model. Training was conducted on an augmented dataset\nof 12000 Martian heightmaps derived from NASA's HiRISE survey. A\nnon-homogeneous rescaling strategy captures terrain features across multiple\nscales before resizing to a fixed 128x128 model resolution. We compared our\nmethod against established void-filling and inpainting techniques, including\nInverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an\nevaluation set of 1000 samples. Results show that our approach consistently\noutperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)\nand perceptual similarity (29-81% on LPIPS) with the original data.",
      "pdf_url": "http://arxiv.org/pdf/2510.14765v1",
      "published": "2025-10-16T15:02:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14765v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes",
      "authors": [
        "Yunwen Li",
        "Shuangshuang Ying",
        "Xingwei Qu",
        "Xin Li",
        "Sheng Jin",
        "Minghao Liu",
        "Zhoufutu Wen",
        "Tianyu Zheng",
        "Xeron Du",
        "Qiguang Chen",
        "Jiajun Shi",
        "Wangchunshu Zhou",
        "Jiazhan Feng",
        "Wanjun Zhong",
        "Libo Qin",
        "Stephen Huang",
        "Wanxiang Che",
        "Chenghua Lin",
        "Eli Zhang"
      ],
      "abstract": "Large language models exhibit systematic deficiencies in creative writing,\nparticularly in non-English contexts where training data is scarce and lacks\nprocess-level supervision. We present COIG-Writer, a novel Chinese creative\nwriting dataset that captures both diverse outputs and their underlying thought\nprocesses through systematic reverse-engineering of high-quality texts. Unlike\nexisting datasets that provide only input-output pairs, COIG-Writer comprises\n1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a\nreverse-engineered prompt, (2) detailed creative reasoning documenting\ndecision-making processes, and (3) the final text. Through comprehensive\nexperiments, we identify a two-component model of creative writing: narrative\nlogic (provided by process supervision) and linguistic expression (maintained\nby general-purpose data). Our findings reveal three critical insights: (1)\nProcess supervision is highly effective but requires stabilization with general\ndata. A ratio of at least one creative sample to twelve general samples is\nneeded to achieve optimal performance; below this threshold, the win rate\nprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities\nare culturally-bound with no cross-lingual transfer (89.26pp gap between\nChinese and English performance), and (3) lexical diversity inversely\ncorrelates with creative quality (TTR paradox), suggesting high diversity\nsignals compensatory behavior for logical deficiencies. These findings\nestablish that creative excellence emerges from the interaction between logical\nscaffolding and linguistic grounding, analogous to how mathematical reasoning\nenhances but cannot replace linguistic competence in foundation models.",
      "pdf_url": "http://arxiv.org/pdf/2510.14763v1",
      "published": "2025-10-16T15:01:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14763v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries",
      "authors": [
        "Divyat Mahajan",
        "Sachin Goyal",
        "Badr Youbi Idrissi",
        "Mohammad Pezeshki",
        "Ioannis Mitliagkas",
        "David Lopez-Paz",
        "Kartik Ahuja"
      ],
      "abstract": "Next-token prediction (NTP) has driven the success of large language models\n(LLMs), but it struggles with long-horizon reasoning, planning, and creative\nwriting, with these limitations largely attributed to teacher-forced training.\nMulti-token prediction (MTP) partially mitigates these issues by predicting\nseveral future tokens at once, but it mostly captures short-range dependencies\nand offers limited improvement. We propose future summary prediction (FSP),\nwhich trains an auxiliary head to predict a compact representation of the\nlong-term future, preserving information relevant for long-form generations. We\nexplore two variants of FSP: handcrafted summaries, for example, a bag of words\nsummary of the future of the sequence, and learned summaries, which use\nembeddings produced by a reverse language model trained from right to left.\nLarge-scale pretraining experiments (3B and 8B-parameter models) demonstrate\nthat FSP provides improvements over both NTP and MTP across math, reasoning,\nand coding benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2510.14751v1",
      "published": "2025-10-16T14:52:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14751v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
      "authors": [
        "Simone Carnemolla",
        "Matteo Pennisi",
        "Sarinda Samarasinghe",
        "Giovanni Bellitto",
        "Simone Palazzo",
        "Daniela Giordano",
        "Mubarak Shah",
        "Concetto Spampinato"
      ],
      "abstract": "Understanding and explaining the behavior of machine learning models is\nessential for building transparent and trustworthy AI systems. We introduce\nDEXTER, a data-free framework that employs diffusion models and large language\nmodels to generate global, textual explanations of visual classifiers. DEXTER\noperates by optimizing text prompts to synthesize class-conditional images that\nstrongly activate a target classifier. These synthetic samples are then used to\nelicit detailed natural language reports that describe class-specific decision\npatterns and biases. Unlike prior work, DEXTER enables natural language\nexplanation about a classifier's decision process without access to training\ndata or ground-truth labels. We demonstrate DEXTER's flexibility across three\ntasks-activation maximization, slice discovery and debiasing, and bias\nexplanation-each illustrating its ability to uncover the internal mechanisms of\nvisual classifiers. Quantitative and qualitative evaluations, including a user\nstudy, show that DEXTER produces accurate, interpretable outputs. Experiments\non ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms\nexisting approaches in global model explanation and class-level bias reporting.\nCode is available at https://github.com/perceivelab/dexter.",
      "pdf_url": "http://arxiv.org/pdf/2510.14741v1",
      "published": "2025-10-16T14:43:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14741v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.m"
      ]
    },
    {
      "title": "Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling",
      "authors": [
        "Alexandru Meterez",
        "Depen Morwani",
        "Jingfeng Wu",
        "Costin-Andrei Oncescu",
        "Cengiz Pehlevan",
        "Sham Kakade"
      ],
      "abstract": "Increasing the batch size during training -- a ''batch ramp'' -- is a\npromising strategy to accelerate large language model pretraining. While for\nSGD, doubling the batch size can be equivalent to halving the learning rate,\nthe optimal strategy for adaptive optimizers like Adam is less clear. As a\nresult, any batch-ramp scheduling, if used at all, is typically tuned\nheuristically. This work develops a principled framework for batch-size\nscheduling and introduces Seesaw: whenever a standard scheduler would halve the\nlearning rate, Seesaw instead multiplies it by $1/\\sqrt{2}$ and doubles the\nbatch size, preserving loss dynamics while reducing serial steps.\nTheoretically, we provide, to our knowledge, the first finite-sample proof of\nequivalence between learning-rate decay and batch-size ramp-up for SGD on noisy\nlinear regression, and we extend this equivalence to normalized SGD, a\ntractable proxy for Adam, under a variance-dominated regime observed in\npractice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla\nscale using a constant (critical) batch size, Seesaw matches cosine decay at\nequal FLOPs while reducing wall-clock time by $\\approx 36\\%$, approaching the\ntheoretical limit implied by our analysis.",
      "pdf_url": "http://arxiv.org/pdf/2510.14717v1",
      "published": "2025-10-16T14:17:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14717v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "title": "Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models",
      "authors": [
        "Tingyu Lin",
        "Armin Dadras",
        "Florian Kleber",
        "Robert Sablatnig"
      ],
      "abstract": "Camera movement conveys spatial and narrative information essential for\nunderstanding video content. While recent camera movement classification (CMC)\nmethods perform well on modern datasets, their generalization to historical\nfootage remains unexplored. This paper presents the first systematic evaluation\nof deep video CMC models on archival film material. We summarize representative\nmethods and datasets, highlighting differences in model design and label\ndefinitions. Five standard video classification models are assessed on the\nHISTORIAN dataset, which includes expert-annotated World War II footage. The\nbest-performing model, Video Swin Transformer, achieves 80.25% accuracy,\nshowing strong convergence despite limited training data. Our findings\nhighlight the challenges and potential of adapting existing models to\nlow-quality video and motivate future work combining diverse input modalities\nand temporal architectures.",
      "pdf_url": "http://arxiv.org/pdf/2510.14713v1",
      "published": "2025-10-16T14:11:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14713v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery",
      "authors": [
        "Caleb Robinson",
        "Kimberly T. Goetz",
        "Christin B. Khan",
        "Meredith Sackett",
        "Kathleen Leonard",
        "Rahul Dodhia",
        "Juan M. Lavista Ferres"
      ],
      "abstract": "Effective monitoring of whale populations is critical for conservation, but\ntraditional survey methods are expensive and difficult to scale. While prior\nwork has shown that whales can be identified in very high-resolution (VHR)\nsatellite imagery, large-scale automated detection remains challenging due to a\nlack of annotated imagery, variability in image quality and environmental\nconditions, and the cost of building robust machine learning pipelines over\nmassive remote sensing archives. We present a semi-automated approach for\nsurfacing possible whale detections in VHR imagery using a statistical anomaly\ndetection method that flags spatial outliers, i.e. \"interesting points\". We\npair this detector with a web-based labeling interface designed to enable\nexperts to quickly annotate the interesting points. We evaluate our system on\nthree benchmark scenes with known whale annotations and achieve recalls of\n90.3% to 96.4%, while reducing the area requiring expert inspection by up to\n99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method\ndoes not rely on labeled training data and offers a scalable first step toward\nfuture machine-assisted marine mammal monitoring from space. We have open\nsourced this pipeline at https://github.com/microsoft/whales.",
      "pdf_url": "http://arxiv.org/pdf/2510.14709v1",
      "published": "2025-10-16T14:10:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14709v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling",
      "authors": [
        "Jianghao Lin",
        "Yuanyuan Shi",
        "Xin Peng",
        "Renjie Ding",
        "Hairui Wang",
        "Yuxuan Peng",
        "Bizhe Bai",
        "Weixi Song",
        "Fengshuo Bai",
        "Huacan Chai",
        "Weinan Zhang",
        "Fei Huang",
        "Ying Wen"
      ],
      "abstract": "Large language models (LLMs) are increasingly demonstrating strong\ncapabilities as autonomous agents, with function calling serving as a core\nmechanism for interaction with the environment. Meanwhile, inference scaling\nhas become a cutting-edge technique to enhance LLM performance by allocating\nmore computational resources during the inference process. However, current\nresearch on inference scaling primarily focuses on unstructured output\ngeneration tasks, leaving its application in structured outputs, like function\ncalling, largely underexplored. To bridge this gap, we propose an inference\nscaling framework that combines fine-grained beam search with a process reward\nmodel, ToolPRM, which scores the internal steps of each single function call.\nTo train ToolPRM, we construct the first fine-grained intra-call process\nsupervision dataset, automatically annotated with function-masking techniques\nto provide step-level rewards for structured tool-use reasoning. Extensive\nexperiments demonstrate that ToolPRM beats the coarse-grained and outcome\nreward models in terms of predictive accuracy, indicating its stronger\ncapability in supervising the function calling inference process. Inference\nscaling technique equipped with ToolPRM also significantly improves the\nbackbone model performance across various function calling tasks and\nbenchmarks. More importantly, we reveal a key principle for applying inference\nscaling techniques to structured outputs: \"explore more but retain less\" due to\nthe unrecoverability characteristics of structured function calling generation.",
      "pdf_url": "http://arxiv.org/pdf/2510.14703v1",
      "published": "2025-10-16T14:06:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.14703v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}