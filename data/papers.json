{
  "last_updated": "2026-02-05T01:06:34.231426",
  "papers": [
    {
      "title": "PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning",
      "authors": [
        "Romain Cosentino"
      ],
      "abstract": "We develop a continual learning method for pretrained models that \\emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \\emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights. Second, redundancy offers a natural bias for \\emph{where} to place plasticity: by restricting updates to a subset of redundant neurons and constraining the remaining degrees of freedom, we obtain update families with reduced functional drift on the old-data distribution and improved worst-case retention guarantees. These insights lead to \\textsc{PLATE} (\\textbf{Pla}sticity-\\textbf{T}unable \\textbf{E}fficient Adapters), a continual learning method requiring no past-task data that provides explicit control over the plasticity-retention trade-off. PLATE parameterizes each layer with a structured low-rank update $ΔW = B A Q^\\top$, where $B$ and $Q$ are computed once from pretrained weights and kept frozen, and only $A$ is trained on the new task. The code is available at https://github.com/SalesforceAIResearch/PLATE.",
      "pdf_url": "https://arxiv.org/pdf/2602.03846v1",
      "published": "2026-02-03T18:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03846v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization",
      "authors": [
        "Erzhen Hu",
        "Frederik Brudy",
        "David Ledo",
        "George Fitzmaurice",
        "Fraser Anderson"
      ],
      "abstract": "In pre-production, filmmakers and 3D animation experts must rapidly prototype ideas to explore a film's possibilities before fullscale production, yet conventional approaches involve trade-offs in efficiency and expressiveness. Hand-drawn storyboards often lack spatial precision needed for complex cinematography, while 3D previsualization demands expertise and high-quality rigged assets. To address this gap, we present PrevizWhiz, a system that leverages rough 3D scenes in combination with generative image and video models to create stylized video previews. The workflow integrates frame-level image restyling with adjustable resemblance, time-based editing through motion paths or external video inputs, and refinement into high-fidelity video clips. A study with filmmakers demonstrates that our system lowers technical barriers for film-makers, accelerates creative iteration, and effectively bridges the communication gap, while also surfacing challenges of continuity, authorship, and ethical consideration in AI-assisted filmmaking.",
      "pdf_url": "https://arxiv.org/pdf/2602.03838v1",
      "published": "2026-02-03T18:56:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03838v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques",
      "authors": [
        "David P. Woodruff",
        "Vincent Cohen-Addad",
        "Lalit Jain",
        "Jieming Mao",
        "Song Zuo",
        "MohammadHossein Bateni",
        "Simina Branzei",
        "Michael P. Brenner",
        "Lin Chen",
        "Ying Feng",
        "Lance Fortnow",
        "Gang Fu",
        "Ziyi Guan",
        "Zahra Hadizadeh",
        "Mohammad T. Hajiaghayi",
        "Mahdi JafariRaviz",
        "Adel Javanmard",
        "Karthik C. S.",
        "Ken-ichi Kawarabayashi",
        "Ravi Kumar",
        "Silvio Lattanzi",
        "Euiwoong Lee",
        "Yi Li",
        "Ioannis Panageas",
        "Dimitris Paparas",
        "Benjamin Przybocki",
        "Bernardo Subercaseaux",
        "Ola Svensson",
        "Shayan Taherijam",
        "Xuan Wu",
        "Eylon Yogev",
        "Morteza Zadimoghaddam",
        "Samson Zhou",
        "Vahab Mirrokni"
      ],
      "abstract": "Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.",
      "pdf_url": "https://arxiv.org/pdf/2602.03837v1",
      "published": "2026-02-03T18:56:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03837v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations",
      "authors": [
        "Minjun Zhu",
        "Zhen Lin",
        "Yixuan Weng",
        "Panzhong Lu",
        "Qiujie Xie",
        "Yifan Wei",
        "Sifan Liu",
        "Qiyao Sun",
        "Yue Zhang"
      ],
      "abstract": "High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.",
      "pdf_url": "https://arxiv.org/pdf/2602.03828v1",
      "published": "2026-02-03T18:41:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03828v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.DL"
      ]
    },
    {
      "title": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion",
      "authors": [
        "Oscar Ovanger",
        "Levi Harris",
        "Timothy H. Keitt"
      ],
      "abstract": "Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \\textbf{F}usion under \\textbf{IN}dependent \\textbf{C}onditional \\textbf{H}ypotheses (\\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \\emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \\texttt{\\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}",
      "pdf_url": "https://arxiv.org/pdf/2602.03817v1",
      "published": "2026-02-03T18:21:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03817v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Conformal Thinking: Risk Control for Reasoning on a Compute Budget",
      "authors": [
        "Xi Wang",
        "Anushri Suresh",
        "Alvin Zhang",
        "Rishi More",
        "William Jurayj",
        "Benjamin Van Durme",
        "Mehrdad Farajtabar",
        "Daniel Khashabi",
        "Eric Nalisnick"
      ],
      "abstract": "Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.",
      "pdf_url": "https://arxiv.org/pdf/2602.03814v1",
      "published": "2026-02-03T18:17:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03814v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Antidistillation Fingerprinting",
      "authors": [
        "Yixuan Even Xu",
        "John Kirchenbauer",
        "Yash Savani",
        "Asher Trockman",
        "Alexander Robey",
        "Tom Goldstein",
        "Fei Fang",
        "J. Zico Kolter"
      ],
      "abstract": "Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model's outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fingerprinting strength, often requiring significant degradation of utility to ensure the fingerprint is effectively internalized by the student. We introduce antidistillation fingerprinting (ADFP), a principled approach that aligns the fingerprinting objective with the student's learning dynamics. Building upon the gradient-based framework of antidistillation sampling, ADFP utilizes a proxy model to identify and sample tokens that directly maximize the expected detectability of the fingerprint in the student after fine-tuning, rather than relying on the incidental absorption of the un-targeted biases of a more naive watermark. Experiments on GSM8K and OASST1 benchmarks demonstrate that ADFP achieves a significant Pareto improvement over state-of-the-art baselines, yielding stronger detection confidence with minimal impact on utility, even when the student model's architecture is unknown.",
      "pdf_url": "https://arxiv.org/pdf/2602.03812v1",
      "published": "2026-02-03T18:15:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03812v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network",
      "authors": [
        "Abdul Joseph Fofanah",
        "Lian Wen",
        "David Chen",
        "Shaoyang Zhang"
      ],
      "abstract": "Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact, Embed) similar to how humans learn. The model begins by engaging with structurally simpler features, defined as (1) local neighbourhood patterns (1-hop), (2) low-degree node attributes, and (3) class-separable node pairs identified via initial graph convolutional networks and graph attention networks (GCN and GAT) embeddings. This foundation enables stable early learning despite label skew. The Enact stage then addresses complicated aspects: (1) connections that require multiple steps, (2) edges that connect different types of nodes, and (3) nodes at the edges of minority classes by using adjustable attention weights. Finally, Embed consolidates these features via iterative message passing and curriculum-aligned loss weighting. We evaluate CL3AN-GNN on eight Open Graph Benchmark datasets spanning social, biological, and citation networks. Experiments show consistent improvements across all datasets in accuracy, F1-score, and AUC over recent state-of-the-art methods. The model's step-by-step method works well with different types of graph datasets, showing quicker results than training everything at once, better performance on new, imbalanced graphs, and clear explanations of each step using gradient stability and attention correlation learning curves. This work provides both a theoretically grounded framework for curriculum learning in GNNs and practical evidence of its effectiveness against imbalances, validated through metrics, convergence speeds, and generalisation tests.",
      "pdf_url": "https://arxiv.org/pdf/2602.03808v1",
      "published": "2026-02-03T18:10:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03808v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation",
      "authors": [
        "Ziru Chen",
        "Dongdong Chen",
        "Ruinan Jin",
        "Yingbin Liang",
        "Yujia Xie",
        "Huan Sun"
      ],
      "abstract": "Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.",
      "pdf_url": "https://arxiv.org/pdf/2602.03806v1",
      "published": "2026-02-03T18:08:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03806v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ]
    },
    {
      "title": "Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods",
      "authors": [
        "Grigory Begunov",
        "Alexander Tyurin"
      ],
      "abstract": "Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.",
      "pdf_url": "https://arxiv.org/pdf/2602.03802v1",
      "published": "2026-02-03T18:02:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03802v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "math.NA",
        "math.OC"
      ]
    },
    {
      "title": "Conformal Reachability for Safe Control in Unknown Environments",
      "authors": [
        "Xinhang Ma",
        "Junlin Wu",
        "Yiannis Kantaros",
        "Yevgeniy Vorobeychik"
      ],
      "abstract": "Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines conformal prediction with reachability analysis. In particular, we use conformal prediction to obtain valid uncertainty intervals for the unknown dynamics at each time step, with reachability then verifying whether safety is maintained within the conformal uncertainty bounds. Next, we develop an algorithmic approach for training control policies that optimize nominal reward while also maximizing the planning horizon with sound probabilistic safety guarantees. We evaluate the proposed approach in seven safe control settings spanning four domains -- cartpole, lane following, drone control, and safe navigation -- for both affine and nonlinear safety specifications. Our experiments show that the policies we learn achieve the strongest provable safety guarantees while still maintaining high average reward.",
      "pdf_url": "https://arxiv.org/pdf/2602.03799v1",
      "published": "2026-02-03T18:01:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03799v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity",
      "authors": [
        "Yingxuan Yang",
        "Chengrui Qu",
        "Muning Wen",
        "Laixi Shi",
        "Ying Wen",
        "Weinan Zhang",
        "Adam Wierman",
        "Shangding Gu"
      ],
      "abstract": "LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.",
      "pdf_url": "https://arxiv.org/pdf/2602.03794v1",
      "published": "2026-02-03T17:58:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03794v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents",
      "authors": [
        "Xilong Wang",
        "Yinuo Liu",
        "Zhun Wang",
        "Dawn Song",
        "Neil Gong"
      ],
      "abstract": "Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user's intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \\emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: https://github.com/wxl-lxw/WebSentinel.",
      "pdf_url": "https://arxiv.org/pdf/2602.03792v1",
      "published": "2026-02-03T17:55:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03792v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants",
      "authors": [
        "Gabriel Damsholt",
        "Jes Frellsen",
        "Susanne Ditlevsen"
      ],
      "abstract": "Stochastic interpolants unify flows and diffusions, popular generative modeling frameworks. A primary hyperparameter in these methods is the interpolation schedule that determines how to bridge a standard Gaussian base measure to an arbitrary target measure. We prove how to convert a sample path of a stochastic differential equation (SDE) with arbitrary diffusion coefficient under any schedule into the unique sample path under another arbitrary schedule and diffusion coefficient. We then extend the stochastic interpolant framework to admit a larger class of point mass schedules in which the Gaussian base measure collapses to a point mass measure. Under the assumption of Gaussian data, we identify lazy schedule families that make the drift identically zero and show that with deterministic sampling one gets a variance-preserving schedule commonly used in diffusion models, whereas with statistically optimal SDE sampling one gets our point mass schedule. Finally, to demonstrate the usefulness of our theoretical results on realistic highly non-Gaussian data, we apply our lazy schedule conversion to a state-of-the-art pretrained flow model and show that this allows for generating images in fewer steps without retraining the model.",
      "pdf_url": "https://arxiv.org/pdf/2602.03789v1",
      "published": "2026-02-03T17:48:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03789v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration",
      "authors": [
        "Jianhao Ruan",
        "Zhihao Xu",
        "Yiran Peng",
        "Fashen Ren",
        "Zhaoyang Yu",
        "Xinbing Liang",
        "Jinyu Xiang",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo",
        "Jiayi Zhang"
      ],
      "abstract": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra",
      "pdf_url": "https://arxiv.org/pdf/2602.03786v1",
      "published": "2026-02-03T17:46:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03786v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Efficient Estimation of Kernel Surrogate Models for Task Attribution",
      "authors": [
        "Zhenshuo Zhang",
        "Minxuan Duan",
        "Hongyang R. Zhang"
      ],
      "abstract": "Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.",
      "pdf_url": "https://arxiv.org/pdf/2602.03783v1",
      "published": "2026-02-03T17:43:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03783v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Reward Redistribution for CVaR MDPs using a Bellman Operator on L-infinity",
      "authors": [
        "Aneri Muni",
        "Vincent Taboga",
        "Esther Derman",
        "Pierre-Luc Bacon",
        "Erick Delage"
      ],
      "abstract": "Tail-end risk measures such as static conditional value-at-risk (CVaR) are used in safety-critical applications to prevent rare, yet catastrophic events. Unlike risk-neutral objectives, the static CVaR of the return depends on entire trajectories without admitting a recursive Bellman decomposition in the underlying Markov decision process. A classical resolution relies on state augmentation with a continuous variable. However, unless restricted to a specialized class of admissible value functions, this formulation induces sparse rewards and degenerate fixed points. In this work, we propose a novel formulation of the static CVaR objective based on augmentation. Our alternative approach leads to a Bellman operator with: (1) dense per-step rewards; (2) contracting properties on the full space of bounded value functions. Building on this theoretical foundation, we develop risk-averse value iteration and model-free Q-learning algorithms that rely on discretized augmented states. We further provide convergence guarantees and approximation error bounds due to discretization. Empirical results demonstrate that our algorithms successfully learn CVaR-sensitive policies and achieve effective performance-safety trade-offs.",
      "pdf_url": "https://arxiv.org/pdf/2602.03778v1",
      "published": "2026-02-03T17:39:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03778v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "DiffLOB: Diffusion Models for Counterfactual Generation in Limit Order Books",
      "authors": [
        "Zhuohan Wang",
        "Carmine Ventre"
      ],
      "abstract": "Modern generative models for limit order books (LOBs) can reproduce realistic market dynamics, but remain fundamentally passive: they either model what typically happens without accounting for hypothetical future market conditions, or they require interaction with another agent to explore alternative outcomes. This limits their usefulness for stress testing, scenario analysis, and decision-making. We propose \\textbf{DiffLOB}, a regime-conditioned \\textbf{Diff}usion model for controllable and counterfactual generation of \\textbf{LOB} trajectories. DiffLOB explicitly conditions the generative process on future market regimes--including trend, volatility, liquidity, and order-flow imbalance, which enables the model to answer counterfactual queries of the form: ``If the future market regime were X instead of Y, how would the limit order book evolve?'' Our systematic evaluation framework for counterfactual LOB generation consists of three criteria: (1) \\textit{Controllable Realism}, measuring how well generated trajectories can reproduce marginal distributions, temporal dependence structure and regime variables; (2) \\textit{Counterfactual validity}, testing whether interventions on future regimes induce consistent changes in the generated LOB dynamics; (3) \\textit{Counterfactual usefulness}, assessing whether synthetic counterfactual trajectories improve downstream prediction of future market regimes.",
      "pdf_url": "https://arxiv.org/pdf/2602.03776v1",
      "published": "2026-02-03T17:34:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03776v1",
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ]
    },
    {
      "title": "An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents",
      "authors": [
        "Farnoosh Hashemi",
        "Michael W. Macy"
      ],
      "abstract": "Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.ai-an LLM-driven social media platform-analyzing 7M posts and interactions among 32K LLM agents over a year. We start with homophily and social influence among LLMs, learning that similar to humans', their social networks exhibit these fundamental phenomena. Next, we study the toxic language of LLMs, its linguistic features, and their interaction patterns, finding that LLMs show different structural patterns in toxic posting than humans. After studying the ideological leaning in LLMs posts, and the polarization in their community, we focus on how to prevent their potential harmful activities. We present a simple yet effective method, called Chain of Social Thought (CoST), that reminds LLM agents to avoid harmful posting.",
      "pdf_url": "https://arxiv.org/pdf/2602.03775v1",
      "published": "2026-02-03T17:34:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03775v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining",
      "authors": [
        "Changhao Wang",
        "Yunfei Yu",
        "Xinhao Yao",
        "Jiaolong Yang",
        "Riccardo Cantoro",
        "Chaobo Li",
        "Qing Cui",
        "Jun Zhou"
      ],
      "abstract": "The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \\textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \\textit{manifold approximation} problem without training proxy models or relying on external reference datasets. UniGeM operates hierarchically: \\textbf{Macro-Exploration} learns mixing weights with stability-based clustering; \\textbf{Micro-Mining} filters high-quality instances by their geometric distribution to ensure logical consistency. Validated by training 8B and 16B MoE models on 100B tokens, UniGeM achieves \\textbf{2.0$\\times$ data efficiency} over a random baseline and further improves overall performance compared to SOTA methods in reasoning-heavy evaluations and multilingual generalization.",
      "pdf_url": "https://arxiv.org/pdf/2602.03772v1",
      "published": "2026-02-03T17:32:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03772v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Decision-oriented benchmarking to transform AI weather forecast access: Application to the Indian monsoon",
      "authors": [
        "Rajat Masiwal",
        "Colin Aitken",
        "Adam Marchakitus",
        "Mayank Gupta",
        "Katherine Kowal",
        "Hamid A. Pahlavan",
        "Tyler Yang",
        "Y. Qiang Sun",
        "Michael Kremer",
        "Amir Jina",
        "William R. Boos",
        "Pedram Hassanzadeh"
      ],
      "abstract": "Artificial intelligence weather prediction (AIWP) models now often outperform traditional physics-based models on common metrics while requiring orders-of-magnitude less computing resources and time. Open-access AIWP models thus hold promise as transformational tools for helping low- and middle-income populations make decisions in the face of high-impact weather shocks. Yet, current approaches to evaluating AIWP models focus mainly on aggregated meteorological metrics without considering local stakeholders' needs in decision-oriented, operational frameworks. Here, we introduce such a framework that connects meteorology, AI, and social sciences. As an example, we apply it to the 150-year-old problem of Indian monsoon forecasting, focusing on benefits to rain-fed agriculture, which is highly susceptible to climate change. AIWP models skillfully predict an agriculturally relevant onset index at regional scales weeks in advance when evaluated out-of-sample using deterministic and probabilistic metrics. This framework informed a government-led effort in 2025 to send 38 million Indian farmers AI-based monsoon onset forecasts, which captured an unusual weeks-long pause in monsoon progression. This decision-oriented benchmarking framework provides a key component of a blueprint for harnessing the power of AIWP models to help large vulnerable populations adapt to weather shocks in the face of climate variability and change.",
      "pdf_url": "https://arxiv.org/pdf/2602.03767v1",
      "published": "2026-02-03T17:27:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03767v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.GN",
        "physics.ao-ph"
      ]
    },
    {
      "title": "Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives",
      "authors": [
        "Owen Dong",
        "Lily Gao",
        "Manish Kota",
        "Bennett A. Landmana",
        "Jelena Bekvalac",
        "Gaynor Western",
        "Katherine D. Van Schaik"
      ],
      "abstract": "Paleoradiology, the use of modern imaging technologies to study archaeological and anthropological remains, offers new windows on millennial scale patterns of human health. Unfortunately, the radiographs collected during field campaigns are heterogeneous: bones are disarticulated, positioning is ad hoc, and laterality markers are often absent. Additionally, factors such as age at death, age of bone, sex, and imaging equipment introduce high variability. Thus, content navigation, such as identifying a subset of images with a specific projection view, can be time consuming and difficult, making efficient triaging a bottleneck for expert analysis. We report a zero shot prompting strategy that leverages a state of the art Large Vision Language Model (LVLM) to automatically identify the main bone, projection view, and laterality in such images. Our pipeline converts raw DICOM files to bone windowed PNGs, submits them to the LVLM with a carefully engineered prompt, and receives structured JSON outputs, which are extracted and formatted onto a spreadsheet in preparation for validation. On a random sample of 100 images reviewed by an expert board certified paleoradiologist, the system achieved 92% main bone accuracy, 80% projection view accuracy, and 100% laterality accuracy, with low or medium confidence flags for ambiguous cases. These results suggest that LVLMs can substantially accelerate code word development for large paleoradiology datasets, allowing for efficient content navigation in future anthropology workflows.",
      "pdf_url": "https://arxiv.org/pdf/2602.03750v1",
      "published": "2026-02-03T17:14:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03750v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models",
      "authors": [
        "Yu Tian",
        "Linh Huynh",
        "Katerina Christhilf",
        "Shubham Chakraborty",
        "Micah Watanabe",
        "Tracy Arner",
        "Danielle McNamara"
      ],
      "abstract": "Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.",
      "pdf_url": "https://arxiv.org/pdf/2602.03704v1",
      "published": "2026-02-03T16:26:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03704v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging",
      "authors": [
        "Alexandru Meterez",
        "Pranav Ajit Nair",
        "Depen Morwani",
        "Cengiz Pehlevan",
        "Sham Kakade"
      ],
      "abstract": "Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging - also known as model merging - in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1-32x Chinchilla scale, comparing constant learning rates with weight averaging and $1/\\sqrt{t}$ schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining.",
      "pdf_url": "https://arxiv.org/pdf/2602.03702v1",
      "published": "2026-02-03T16:24:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03702v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "title": "Agent Primitives: Reusable Latent Building Blocks for Multi-Agent Systems",
      "authors": [
        "Haibo Jin",
        "Kuang Peng",
        "Ye Yu",
        "Xiaopeng Yuan",
        "Haohan Wang"
      ],
      "abstract": "While existing multi-agent systems (MAS) can handle complex problems by enabling collaboration among multiple agents, they are often highly task-specific, relying on manually crafted agent roles and interaction prompts, which leads to increased architectural complexity and limited reusability across tasks. Moreover, most MAS communicate primarily through natural language, making them vulnerable to error accumulation and instability in long-context, multi-stage interactions within internal agent histories.\n  In this work, we propose \\textbf{Agent Primitives}, a set of reusable latent building blocks for LLM-based MAS. Inspired by neural network design, where complex models are built from reusable components, we observe that many existing MAS architectures can be decomposed into a small number of recurring internal computation patterns. Based on this observation, we instantiate three primitives: Review, Voting and Selection, and Planning and Execution. All primitives communicate internally via key-value (KV) cache, which improves both robustness and efficiency by mitigating information degradation across multi-stage interactions. To enable automatic system construction, an Organizer agent selects and composes primitives for each query, guided by a lightweight knowledge pool of previously successful configurations, forming a primitive-based MAS.\n  Experiments show that primitives-based MAS improve average accuracy by 12.0-16.5\\% over single-agent baselines, reduce token usage and inference latency by approximately 3$\\times$-4$\\times$ compared to text-based MAS, while incurring only 1.3$\\times$-1.6$\\times$ overhead relative to single-agent inference and providing more stable performance across model backbones.",
      "pdf_url": "https://arxiv.org/pdf/2602.03695v1",
      "published": "2026-02-03T16:17:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03695v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "OCRTurk: A Comprehensive OCR Benchmark for Turkish",
      "authors": [
        "Deniz Yılmaz",
        "Evren Ayberk Munis",
        "Çağrı Toraman",
        "Süha Kağan Köse",
        "Burak Aktaş",
        "Mehmet Can Baytekin",
        "Bilge Kaan Görür"
      ],
      "abstract": "Document parsing is now widely used in applications, such as large-scale document digitization, retrieval-augmented generation, and domain-specific pipelines in healthcare and education. Benchmarking these models is crucial for assessing their reliability and practical robustness. Existing benchmarks mostly target high-resource languages and provide limited coverage for low-resource settings, such as Turkish. Moreover, existing studies on Turkish document parsing lack a standardized benchmark that reflects real-world scenarios and document diversity. To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark covering multiple layout elements and document categories at three difficulty levels. OCRTurk consists of 180 Turkish documents drawn from academic articles, theses, slide decks, and non-academic articles. We evaluate seven OCR models on OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves the strongest overall results, leading most element-wise metrics except figures and attaining high Normalized Edit Distance scores in easy, medium, and hard subsets. We also observe performance variation by document type. Models perform well on non-academic documents, while slideshows become the most challenging.",
      "pdf_url": "https://arxiv.org/pdf/2602.03693v1",
      "published": "2026-02-03T16:11:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03693v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LLM-Inspired Pretrain-Then-Finetune for Small-Data, Large-Scale Optimization",
      "authors": [
        "Zishi Zhang",
        "Jinhui Han",
        "Ming Hu",
        "Yijie Peng"
      ],
      "abstract": "We consider small-data, large-scale decision problems in which a firm must make many operational decisions simultaneously (e.g., across a large product portfolio) while observing only a few, potentially noisy, data points per instance. Inspired by the success of large language models (LLMs), we propose a pretrain-then-finetune approach built on a designed Transformer model to address this challenge. The model is first pretrained on large-scale, domain-informed synthetic data that encode managerial knowledge and structural features of the decision environment, and is then fine-tuned on real observations. This new pipeline offers two complementary advantages: pretraining injects domain knowledge into the learning process and enables the training of high-capacity models using abundant synthetic data, while finetuning adapts the pretrained model to the operational environment and improves alignment with the true data-generating regime. While we have leveraged the Transformer's state-of-the-art representational capacity, particularly its attention mechanism, to efficiently extract cross-task structure, our approach is not an off-the-shelf application. Instead, it relies on problem-specific architectural design and a tailored training procedure to match the decision setting. Theoretically, we develop the first comprehensive error analysis regarding Transformer learning in relevant contexts, establishing nonasymptotic guarantees that validate the method's effectiveness. Critically, our analysis reveals how pretraining and fine-tuning jointly determine performance, with the dominant contribution governed by whichever is more favorable. In particular, finetuning exhibits an economies-of-scale effect, whereby transfer learning becomes increasingly effective as the number of instances grows.",
      "pdf_url": "https://arxiv.org/pdf/2602.03690v1",
      "published": "2026-02-03T16:08:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03690v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation",
      "authors": [
        "Jiashuo Sun",
        "Pengcheng Jiang",
        "Saizhuo Wang",
        "Jiajun Fan",
        "Heng Wang",
        "Siru Ouyang",
        "Ming Zhong",
        "Yizhu Jiao",
        "Chengsong Huang",
        "Xueqiang Xu",
        "Pengrui Han",
        "Peiran Li",
        "Jiaxin Huang",
        "Ge Liu",
        "Heng Ji",
        "Jiawei Han"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator's Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at https://github.com/GasolSun36/BAR-RAG.",
      "pdf_url": "https://arxiv.org/pdf/2602.03689v1",
      "published": "2026-02-03T16:08:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03689v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System",
      "authors": [
        "Wenzhe Fan",
        "Tommaso Tognoli",
        "Henry Peng Zou",
        "Chunyu Miao",
        "Yibo Wang",
        "Xinhua Zhang"
      ],
      "abstract": "Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \\textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \\textbf{t}ask-\\textbf{o}riented \\textbf{dy}namic \\textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.",
      "pdf_url": "https://arxiv.org/pdf/2602.03688v1",
      "published": "2026-02-03T16:07:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03688v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "QuAIL: Quality-Aware Inertial Learning for Robust Training under Data Corruption",
      "authors": [
        "Mattia Sabella",
        "Alberto Archetti",
        "Pietro Pinoli",
        "Matteo Matteucci",
        "Cinzia Cappiello"
      ],
      "abstract": "Tabular machine learning systems are frequently trained on data affected by non-uniform corruption, including noisy measurements, missing entries, and feature-specific biases. In practice, these defects are often documented only through column-level reliability indicators rather than instance-wise quality annotations, limiting the applicability of many robustness and cleaning techniques. We present QuAIL, a quality-informed training mechanism that incorporates feature reliability priors directly into the learning process. QuAIL augments existing models with a learnable feature-modulation layer whose updates are selectively constrained by a quality-dependent proximal regularizer, thereby inducing controlled adaptation across features of varying trustworthiness. This stabilizes optimization under structured corruption without explicit data repair or sample-level reweighting. Empirical evaluation across 50 classification and regression datasets demonstrates that QuAIL consistently improves average performance over neural baselines under both random and value-dependent corruption, with especially robust behavior in low-data and systematically biased settings. These results suggest that incorporating feature reliability information directly into optimization dynamics is a practical and effective approach for resilient tabular learning.",
      "pdf_url": "https://arxiv.org/pdf/2602.03686v1",
      "published": "2026-02-03T16:06:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03686v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Universal One-third Time Scaling in Learning Peaked Distributions",
      "authors": [
        "Yizhou Liu",
        "Ziming Liu",
        "Cengiz Pehlevan",
        "Jeff Gore"
      ],
      "abstract": "Training large language models (LLMs) is computationally expensive, partly because the loss exhibits slow power-law convergence whose origin remains debatable. Through systematic analysis of toy models and empirical evaluation of LLMs, we show that this behavior can arise intrinsically from the use of softmax and cross-entropy. When learning peaked probability distributions, e.g., next-token distributions, these components yield power-law vanishing losses and gradients, creating a fundamental optimization bottleneck. This ultimately leads to power-law time scaling of the loss with a universal exponent of $1/3$. Our results provide a mechanistic explanation for observed neural scaling and suggest new directions for improving LLM training efficiency.",
      "pdf_url": "https://arxiv.org/pdf/2602.03685v1",
      "published": "2026-02-03T16:06:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03685v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "ContraLog: Log File Anomaly Detection with Contrastive Learning and Masked Language Modeling",
      "authors": [
        "Simon Dietz",
        "Kai Klede",
        "An Nguyen",
        "Bjoern M Eskofier"
      ],
      "abstract": "Log files record computational events that reflect system state and behavior, making them a primary source of operational insights in modern computer systems. Automated anomaly detection on logs is therefore critical, yet most established methods rely on log parsers that collapse messages into discrete templates, discarding variable values and semantic content. We propose ContraLog, a parser-free and self-supervised method that reframes log anomaly detection as predicting continuous message embeddings rather than discrete template IDs. ContraLog combines a message encoder that produces rich embeddings for individual log messages with a sequence encoder to model temporal dependencies within sequences. The model is trained with a combination of masked language modeling and contrastive learning to predict masked message embeddings based on the surrounding context. Experiments on the HDFS, BGL, and Thunderbird benchmark datasets empirically demonstrate effectiveness on complex datasets with diverse log messages. Additionally, we find that message embeddings generated by ContraLog carry meaningful information and are predictive of anomalies even without sequence context. These results highlight embedding-level prediction as an approach for log anomaly detection, with potential applicability to other event sequences.",
      "pdf_url": "https://arxiv.org/pdf/2602.03678v1",
      "published": "2026-02-03T15:59:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03678v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Equilibrium Propagation for Non-Conservative Systems",
      "authors": [
        "Antonino Emanuele Scurria",
        "Dimitri Vanden Abeele",
        "Bortolo Matteo Mognetti",
        "Serge Massar"
      ],
      "abstract": "Equilibrium Propagation (EP) is a physics-inspired learning algorithm that uses stationary states of a dynamical system both for inference and learning. In its original formulation it is limited to conservative systems, $\\textit{i.e.}$ to dynamics which derive from an energy function. Given their importance in applications, it is important to extend EP to nonconservative systems, $\\textit{i.e.}$ systems with non-reciprocal interactions. Previous attempts to generalize EP to such systems failed to compute the exact gradient of the cost function. Here we propose a framework that extends EP to arbitrary nonconservative systems, including feedforward networks. We keep the key property of equilibrium propagation, namely the use of stationary states both for inference and learning. However, we modify the dynamics in the learning phase by a term proportional to the non-reciprocal part of the interaction so as to obtain the exact gradient of the cost function. This algorithm can also be derived using a variational formulation that generates the learning dynamics through an energy function defined over an augmented state space. Numerical experiments using the MNIST database show that this algorithm achieves better performance and learns faster than previous proposals.",
      "pdf_url": "https://arxiv.org/pdf/2602.03670v1",
      "published": "2026-02-03T15:52:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03670v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "math.DS",
        "physics.class-ph"
      ]
    },
    {
      "title": "Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images",
      "authors": [
        "Sandeep Patil",
        "Yongqi Dong",
        "Haneen Farah",
        "Hans Hellendoorn"
      ],
      "abstract": "Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.",
      "pdf_url": "https://arxiv.org/pdf/2602.03669v1",
      "published": "2026-02-03T15:51:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03669v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ]
    },
    {
      "title": "Mitigating Conversational Inertia in Multi-Turn Agents",
      "authors": [
        "Yang Wan",
        "Zheng Cao",
        "Zhenhao Zhang",
        "Zhengwen Zeng",
        "Shuheng Shen",
        "Changhua Meng",
        "Linchao Zhu"
      ],
      "abstract": "Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.",
      "pdf_url": "https://arxiv.org/pdf/2602.03664v1",
      "published": "2026-02-03T15:47:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03664v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish",
      "authors": [
        "Süha Kağan Köse",
        "Mehmet Can Baytekin",
        "Burak Aktaş",
        "Bilge Kaan Görür",
        "Evren Ayberk Munis",
        "Deniz Yılmaz",
        "Muhammed Yusuf Kartal",
        "Çağrı Toraman"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.",
      "pdf_url": "https://arxiv.org/pdf/2602.03652v1",
      "published": "2026-02-03T15:35:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03652v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "authors": [
        "Bowei He",
        "Minda Hu",
        "Zenan Xu",
        "Hongru Wang",
        "Licheng Zong",
        "Yankai Chen",
        "Chen Ma",
        "Xue Liu",
        "Pluto Zhou",
        "Irwin King"
      ],
      "abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.",
      "pdf_url": "https://arxiv.org/pdf/2602.03647v1",
      "published": "2026-02-03T15:32:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03647v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Tutorial on Reasoning for IR & IR for Reasoning",
      "authors": [
        "Mohanna Hoveyda",
        "Panagiotis Efstratiadis",
        "Arjen de Vries",
        "Maarten de Rijke"
      ],
      "abstract": "Information retrieval has long focused on ranking documents by semantic relatedness. Yet many real-world information needs demand more: enforcement of logical constraints, multi-step inference, and synthesis of multiple pieces of evidence. Addressing these requirements is, at its core, a problem of reasoning. Across AI communities, researchers are developing diverse solutions for the problem of reasoning, from inference-time strategies and post-training of LLMs, to neuro-symbolic systems, Bayesian and probabilistic frameworks, geometric representations, and energy-based models. These efforts target the same problem: to move beyond pattern-matching systems toward structured, verifiable inference. However, they remain scattered across disciplines, making it difficult for IR researchers to identify the most relevant ideas and opportunities. To help navigate the fragmented landscape of research in reasoning, this tutorial first articulates a working definition of reasoning within the context of information retrieval and derives from it a unified analytical framework. The framework maps existing approaches along axes that reflect the core components of the definition. By providing a comprehensive overview of recent approaches and mapping current methods onto the defined axes, we expose their trade-offs and complementarities, highlight where IR can benefit from cross-disciplinary advances, and illustrate how retrieval process itself can play a central role in broader reasoning systems. The tutorial will equip participants with both a conceptual framework and practical guidance for enhancing reasoning-capable IR systems, while situating IR as a domain that both benefits and contributes to the broader development of reasoning methodologies.",
      "pdf_url": "https://arxiv.org/pdf/2602.03640v1",
      "published": "2026-02-03T15:24:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03640v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish",
      "authors": [
        "Burak Aktaş",
        "Mehmet Can Baytekin",
        "Süha Kağan Köse",
        "Ömer İlbilgi",
        "Elif Özge Yılmaz",
        "Çağrı Toraman",
        "Bilge Kaan Görür"
      ],
      "abstract": "Text-to-SQL systems have achieved strong performance on English benchmarks, yet their behavior in morphologically rich, low-resource languages remains largely unexplored. We introduce BIRDTurk, the first Turkish adaptation of the BIRD benchmark, constructed through a controlled translation pipeline that adapts schema identifiers to Turkish while strictly preserving the logical structure and execution semantics of SQL queries and databases. Translation quality is validated on a sample size determined by the Central Limit Theorem to ensure 95% confidence, achieving 98.15% accuracy on human-evaluated samples. Using BIRDTurk, we evaluate inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning. Our results reveal that Turkish introduces consistent performance degradation, driven by both structural linguistic divergence and underrepresentation in LLM pretraining, while agentic reasoning demonstrates stronger cross-lingual robustness. Supervised fine-tuning remains challenging for standard multilingual baselines but scales effectively with modern instruction-tuned models. BIRDTurk provides a controlled testbed for cross-lingual Text-to-SQL evaluation under realistic database conditions. We release the training and development splits to support future research.",
      "pdf_url": "https://arxiv.org/pdf/2602.03633v1",
      "published": "2026-02-03T15:21:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03633v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ]
    },
    {
      "title": "Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12",
      "authors": [
        "Iñaki del Campo",
        "Pablo Cuervo",
        "Victor Rodriguez-Fernandez",
        "Roberto Armellin",
        "Jack Yarndley"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an \"LLM-as-a-Judge\" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.",
      "pdf_url": "https://arxiv.org/pdf/2602.03630v1",
      "published": "2026-02-03T15:18:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03630v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Controlling Output Rankings in Generative Engines for LLM-based Search",
      "authors": [
        "Haibo Jin",
        "Ruoxi Chen",
        "Peiyan Zhang",
        "Yifeng Luo",
        "Huimin Zeng",
        "Man Luo",
        "Haohan Wang"
      ],
      "abstract": "The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility.\n  In this work, we propose CORE, an optimization method that \\textbf{C}ontrols \\textbf{O}utput \\textbf{R}ankings in g\\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface.\n  Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \\textbf{91.4\\% @Top-5}, \\textbf{86.6\\% @Top-3}, and \\textbf{80.3\\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.",
      "pdf_url": "https://arxiv.org/pdf/2602.03608v1",
      "published": "2026-02-03T14:59:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03608v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures",
      "authors": [
        "Basile Terver",
        "Randall Balestriero",
        "Megi Dervishi",
        "David Fan",
        "Quentin Garrido",
        "Tushar Nagarajan",
        "Koustuv Sinha",
        "Wancong Zhang",
        "Mike Rabbat",
        "Yann LeCun",
        "Amir Bar"
      ],
      "abstract": "We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at https://github.com/facebookresearch/eb_jepa.",
      "pdf_url": "https://arxiv.org/pdf/2602.03604v1",
      "published": "2026-02-03T14:56:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03604v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "APEX: Probing Neural Networks via Activation Perturbation",
      "authors": [
        "Tao Ren",
        "Xiaoyu Luo",
        "Qiongxiu Li"
      ],
      "abstract": "Prior work on probing neural networks primarily relies on input-space analysis or parameter perturbation, both of which face fundamental limitations in accessing structural information encoded in intermediate representations. We introduce Activation Perturbation for EXploration (APEX), an inference-time probing paradigm that perturbs hidden activations while keeping both inputs and model parameters fixed. We theoretically show that activation perturbation induces a principled transition from sample-dependent to model-dependent behavior by suppressing input-specific signals and amplifying representation-level structure, and further establish that input perturbation corresponds to a constrained special case of this framework. Through representative case studies, we demonstrate the practical advantages of APEX. In the small-noise regime, APEX provides a lightweight and efficient measure of sample regularity that aligns with established metrics, while also distinguishing structured from randomly labeled models and revealing semantically coherent prediction transitions. In the large-noise regime, APEX exposes training-induced model-level biases, including a pronounced concentration of predictions on the target class in backdoored models. Overall, our results show that APEX offers an effective perspective for exploring, and understanding neural networks beyond what is accessible from input space alone.",
      "pdf_url": "https://arxiv.org/pdf/2602.03586v1",
      "published": "2026-02-03T14:36:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03586v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "$V_0$: A Generalist Value Model for Any Policy at State Zero",
      "authors": [
        "Yi-Kai Zhang",
        "Zhiyuan Yao",
        "Hongyan Hao",
        "Yueqing Sun",
        "Qi Gu",
        "Hui Su",
        "Xunliang Cai",
        "De-Chuan Zhan",
        "Han-Jia Ye"
      ],
      "abstract": "Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.",
      "pdf_url": "https://arxiv.org/pdf/2602.03584v1",
      "published": "2026-02-03T14:35:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03584v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Don't believe everything you read: Understanding and Measuring MCP Behavior under Misleading Tool Descriptions",
      "authors": [
        "Zhihao Li",
        "Boyang Ma",
        "Xuelong Dai",
        "Minghui Xu",
        "Yue Zhang",
        "Biwei Yan",
        "Kun Li"
      ],
      "abstract": "The Model Context Protocol (MCP) enables large language models to invoke external tools through natural-language descriptions, forming the foundation of many AI agent applications. However, MCP does not enforce consistency between documented tool behavior and actual code execution, even though MCP Servers often run with broad system privileges. This gap introduces a largely unexplored security risk. We study how mismatches between externally presented tool descriptions and underlying implementations systematically shape the mental models and decision-making behavior of intelligent agents. Specifically, we present the first large-scale study of description-code inconsistency in the MCP ecosystem. We design an automated static analysis framework and apply it to 10,240 real-world MCP Servers across 36 categories. Our results show that while most servers are highly consistent, approximately 13% exhibit substantial mismatches that can enable undocumented privileged operations, hidden state mutations, or unauthorized financial actions. We further observe systematic differences across application categories, popularity levels, and MCP marketplaces. Our findings demonstrate that description-code inconsistency is a concrete and prevalent attack surface in MCP-based AI agents, and motivate the need for systematic auditing and stronger transparency guarantees in future agent ecosystems.",
      "pdf_url": "https://arxiv.org/pdf/2602.03580v1",
      "published": "2026-02-03T14:31:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03580v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs",
      "authors": [
        "Su Dong",
        "Qinggang Zhang",
        "Yilin Xiao",
        "Shengyuan Chen",
        "Chuang Zhou",
        "Xiao Huang"
      ],
      "abstract": "Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.",
      "pdf_url": "https://arxiv.org/pdf/2602.03578v1",
      "published": "2026-02-03T14:26:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03578v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories",
      "authors": [
        "Linjie Mu",
        "Zhongzhen Huang",
        "Yannian Gu",
        "Shengqian Qin",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.",
      "pdf_url": "https://arxiv.org/pdf/2602.03569v1",
      "published": "2026-02-03T14:12:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03569v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "EVE: Efficient Verification of Data Erasure through Customized Perturbation in Approximate Unlearning",
      "authors": [
        "Weiqi Wang",
        "Zhiyi Tian",
        "Chenhan Zhang",
        "Luoyu Chen",
        "Shui Yu"
      ],
      "abstract": "Verifying whether the machine unlearning process has been properly executed is critical but remains underexplored. Some existing approaches propose unlearning verification methods based on backdooring techniques. However, these methods typically require participation in the model's initial training phase to backdoor the model for later verification, which is inefficient and impractical. In this paper, we propose an efficient verification of erasure method (EVE) for verifying machine unlearning without requiring involvement in the model's initial training process. The core idea is to perturb the unlearning data to ensure the model prediction of the specified samples will change before and after unlearning with perturbed data. The unlearning users can leverage the observation of the changes as a verification signal. Specifically, the perturbations are designed with two key objectives: ensuring the unlearning effect and altering the unlearned model's prediction of target samples. We formalize the perturbation generation as an adversarial optimization problem, solving it by aligning the unlearning gradient with the gradient of boundary change for target samples. We conducted extensive experiments, and the results show that EVE can verify machine unlearning without involving the model's initial training process, unlike backdoor-based methods. Moreover, EVE significantly outperforms state-of-the-art unlearning verification methods, offering significant speedup in efficiency while enhancing verification accuracy. The source code of EVE is released at \\uline{https://anonymous.4open.science/r/EVE-C143}, providing a novel tool for verification of machine unlearning.",
      "pdf_url": "https://arxiv.org/pdf/2602.03567v1",
      "published": "2026-02-03T14:09:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03567v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing",
      "authors": [
        "Yizhao Gao",
        "Jianyu Wei",
        "Qihao Zhang",
        "Yu Cheng",
        "Shimao Chen",
        "Zhengju Tang",
        "Zihan Jiang",
        "Yifan Song",
        "Hailin Zhang",
        "Liang Zhao",
        "Bo Yang",
        "Gang Wang",
        "Shijie Cao",
        "Fuli Luo"
      ],
      "abstract": "This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.",
      "pdf_url": "https://arxiv.org/pdf/2602.03560v1",
      "published": "2026-02-03T14:05:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03560v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images",
      "authors": [
        "Xinyue Li",
        "Zhiming Xu",
        "Zhichao Zhang",
        "Zhaolin Cai",
        "Sijing Wu",
        "Xiongkuo Min",
        "Yitong Chen",
        "Guangtao Zhai"
      ],
      "abstract": "Generative text-to-image models are advancing at an unprecedented pace, continuously shifting the perceptual quality ceiling and rendering previously collected labels unreliable for newer generations. To address this, we present ELIQ, a Label-free Framework for Quality Assessment of Evolving AI-generated Images. Specifically, ELIQ focuses on visual quality and prompt-image alignment, automatically constructs positive and aspect-specific negative pairs to cover both conventional distortions and AIGC-specific distortion modes, enabling transferable supervision without human annotations. Building on these pairs, ELIQ adapts a pre-trained multimodal model into a quality-aware critic via instruction tuning and predicts two-dimensional quality using lightweight gated fusion and a Quality Query Transformer. Experiments across multiple benchmarks demonstrate that ELIQ consistently outperforms existing label-free methods, generalizes from AI-generated content (AIGC) to user-generated content (UGC) scenarios without modification, and paves the way for scalable and label-free quality assessment under continuously evolving generative models. The code will be released upon publication.",
      "pdf_url": "https://arxiv.org/pdf/2602.03558v1",
      "published": "2026-02-03T14:04:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.03558v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    }
  ]
}