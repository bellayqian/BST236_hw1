{
  "last_updated": "2025-08-21T00:49:36.117804",
  "papers": [
    {
      "title": "ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents",
      "authors": [
        "Hanyu Lai",
        "Xiao Liu",
        "Yanxiao Zhao",
        "Han Xu",
        "Hanchen Zhang",
        "Bohao Jing",
        "Yanyu Ren",
        "Shuntian Yao",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "abstract": "We introduce ComputerRL, a framework for autonomous desktop intelligence that\nenables agents to operate complex digital workspaces skillfully. ComputerRL\nfeatures the API-GUI paradigm, which unifies programmatic API calls and direct\nGUI interaction to address the inherent mismatch between machine agents and\nhuman-centric desktop environments. Scaling end-to-end RL training is crucial\nfor improvement and generalization across diverse desktop tasks, yet remains\nchallenging due to environmental inefficiency and instability in extended\ntraining. To support scalable and robust training, we develop a distributed RL\ninfrastructure capable of orchestrating thousands of parallel virtual desktop\nenvironments to accelerate large-scale online RL. Furthermore, we propose\nEntropulse, a training strategy that alternates reinforcement learning with\nsupervised fine-tuning, effectively mitigating entropy collapse during extended\ntraining runs. We employ ComputerRL on open models GLM-4-9B-0414 and\nQwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B\nbased on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,\ndemonstrating significant improvements for general agents in desktop\nautomation. The algorithm and framework are adopted in building AutoGLM (Liu et\nal., 2024a)",
      "pdf_url": "http://arxiv.org/pdf/2508.14040v1",
      "published": "2025-08-19T17:59:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.14040v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation",
      "authors": [
        "Ken Deng",
        "Yunhan Yang",
        "Jingxiang Sun",
        "Xihui Liu",
        "Yebin Liu",
        "Ding Liang",
        "Yan-Pei Cao"
      ],
      "abstract": "Modern 3D generation methods can rapidly create shapes from sparse or single\nviews, but their outputs often lack geometric detail due to computational\nconstraints. We present DetailGen3D, a generative approach specifically\ndesigned to enhance these generated 3D shapes. Our key insight is to model the\ncoarse-to-fine transformation directly through data-dependent flows in latent\nspace, avoiding the computational overhead of large-scale 3D generative models.\nWe introduce a token matching strategy that ensures accurate spatial\ncorrespondence during refinement, enabling local detail synthesis while\npreserving global structure. By carefully designing our training data to match\nthe characteristics of synthesized coarse shapes, our method can effectively\nenhance shapes produced by various 3D generation and reconstruction approaches,\nfrom single-view to sparse multi-view inputs. Extensive experiments demonstrate\nthat DetailGen3D achieves high-fidelity geometric detail synthesis while\nmaintaining efficiency in training.",
      "pdf_url": "http://arxiv.org/pdf/2508.14036v1",
      "published": "2025-08-19T17:58:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.14036v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation",
      "authors": [
        "Dongyoon Hahm",
        "Taywon Min",
        "Woogyeol Jin",
        "Kimin Lee"
      ],
      "abstract": "Beyond simple text generation, Large Language Models (LLMs) have evolved into\nagentic systems capable of planning and interacting with external tools to\nsolve complex tasks. This evolution involves fine-tuning LLMs on agent-specific\ntasks to enhance their proficiency. However, safety concerns are frequently\noverlooked during this fine-tuning process. In this work, we show that aligned\nLLMs can become unintentionally misaligned, leading to a higher likelihood of\nexecuting harmful tasks and a reduced tendency to refuse them when fine-tuned\nto execute agentic tasks. To address these safety challenges, we propose Prefix\nINjection Guard (PING), a simple yet effective method that prepends\nautomatically generated natural language prefixes to agent responses, guiding\nthem to refuse harmful requests while preserving performance on benign tasks.\nSpecifically, we introduce an iterative approach that alternates between (1)\ngenerating candidate prefixes and (2) selecting those that optimize both task\nperformance and refusal behavior. Experimental results demonstrate that PING\nsignificantly enhances the safety of fine-tuned LLM agents without sacrificing\ntheir effectiveness. PING consistently outperforms existing prompting\napproaches across diverse benchmarks in both web navigation and code generation\ntasks. Our analysis of internal hidden states via linear probes reveals that\nprefix tokens are crucial for behavior modification, explaining the performance\ngains. WARNING: This paper contains contents that are unethical or offensive in\nnature.",
      "pdf_url": "http://arxiv.org/pdf/2508.14031v1",
      "published": "2025-08-19T17:53:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.14031v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Ask Good Questions for Large Language Models",
      "authors": [
        "Qi Wu",
        "Zhongqi Lu"
      ],
      "abstract": "Recent advances in large language models (LLMs) have significantly improved\nthe performance of dialog systems, yet current approaches often fail to provide\naccurate guidance of topic due to their inability to discern user confusion in\nrelated concepts. To address this, we introduce the Ask-Good-Question (AGQ)\nframework, which features an improved Concept-Enhanced Item Response Theory\n(CEIRT) model to better identify users' knowledge levels. Our contributions\ninclude applying the CEIRT model along with LLMs to directly generate guiding\nquestions based on the inspiring text, greatly improving information retrieval\nefficiency during the question & answer process. Through comparisons with other\nbaseline methods, our approach outperforms by significantly enhencing the\nusers' information retrieval experiences.",
      "pdf_url": "http://arxiv.org/pdf/2508.14025v1",
      "published": "2025-08-19T17:31:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.14025v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem",
      "authors": [
        "Christian Blum",
        "Pedro Pinacho-Davidson"
      ],
      "abstract": "The longest run subsequence (LRS) problem is an NP-hard combinatorial\noptimization problem belonging to the class of subsequence problems from\nbioinformatics. In particular, the problem plays a role in genome reassembly.\nIn this paper, we present a solution to the LRS problem using a Biased Random\nKey Genetic Algorithm (BRKGA). Our approach places particular focus on the\ncomputational efficiency of evaluating individuals, which involves converting\nvectors of gray values into valid solutions to the problem. For comparison\npurposes, a Max-Min Ant System is developed and implemented. This is in\naddition to the application of the integer linear programming solver CPLEX for\nsolving all considered problem instances. The computation results show that the\nproposed BRKGA is currently a state-of-the-art technique for the LRS problem.\nNevertheless, the results also show that there is room for improvement,\nespecially in the context of input strings based on large alphabet sizes.",
      "pdf_url": "http://arxiv.org/pdf/2508.14020v1",
      "published": "2025-08-19T17:27:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.14020v1",
      "categories": [
        "cs.AI",
        "cs.DM",
        "68T01",
        "I.2.8"
      ]
    },
    {
      "title": "Efficient Knowledge Graph Unlearning with Zeroth-order Information",
      "authors": [
        "Yang Xiao",
        "Ruimeng Ye",
        "Bohan Liu",
        "Xiaolong Ma",
        "Bo Hui"
      ],
      "abstract": "Due to regulations like the Right to be Forgotten, there is growing demand\nfor removing training data and its influence from models. Since full retraining\nis costly, various machine unlearning methods have been proposed. In this\npaper, we firstly present an efficient knowledge graph (KG) unlearning\nalgorithm. We remark that KG unlearning is nontrivial due to the distinctive\nstructure of KG and the semantic relations between entities. Also, unlearning\nby estimating the influence of removed components incurs significant\ncomputational overhead when applied to large-scale knowledge graphs. To this\nend, we define an influence function for KG unlearning and propose to\napproximate the model's sensitivity without expensive computation of\nfirst-order and second-order derivatives for parameter updates. Specifically,\nwe use Taylor expansion to estimate the parameter changes caused by data\nremoval. Given that the first-order gradients and second-order derivatives\ndominate the computational load, we use the Fisher matrices and zeroth-order\noptimization to approximate the inverse-Hessian vector product without\nconstructing the computational graphs. Our experimental results demonstrate\nthat the proposed method outperforms other state-of-the-art graph unlearning\nbaselines significantly in terms of unlearning efficiency and unlearning\nquality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.",
      "pdf_url": "http://arxiv.org/pdf/2508.14013v1",
      "published": "2025-08-19T17:22:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.14013v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Identity Leakage in Speaker De-Identification Systems",
      "authors": [
        "Seungmin Seo",
        "Oleg Aulov",
        "Afzal Godil",
        "Kevin Mangold"
      ],
      "abstract": "Speaker de-identification aims to conceal a speaker's identity while\npreserving intelligibility of the underlying speech. We introduce a benchmark\nthat quantifies residual identity leakage with three complementary error rates:\nequal error rate, cumulative match characteristic hit rate, and embedding-space\nsimilarity measured via canonical correlation analysis and Procrustes analysis.\nEvaluation results reveal that all state-of-the-art speaker de-identification\nsystems leak identity information. The highest performing system in our\nevaluation performs only slightly better than random guessing, while the lowest\nperforming system achieves a 45% hit rate within the top 50 candidates based on\nCMC. These findings highlight persistent privacy risks in current speaker\nde-identification technologies.",
      "pdf_url": "http://arxiv.org/pdf/2508.14012v1",
      "published": "2025-08-19T17:20:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.14012v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery",
      "authors": [
        "Mohammad Izadi",
        "Mehran Safayani"
      ],
      "abstract": "Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition\nmarked by disruptions in brain connectivity. Functional MRI (fMRI) offers a\nnon-invasive window into large-scale neural dynamics by measuring\nblood-oxygen-level-dependent (BOLD) signals across the brain. These signals can\nbe modeled as interactions among Regions of Interest (ROIs), which are grouped\ninto functional communities based on their underlying roles in brain function.\nEmerging evidence suggests that connectivity patterns within and between these\ncommunities are particularly sensitive to ASD-related alterations. Effectively\ncapturing these patterns and identifying interactions that deviate from typical\ndevelopment is essential for improving ASD diagnosis and enabling biomarker\ndiscovery. In this work, we introduce ASDFormer, a Transformer-based\narchitecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to\ncapture neural signatures associated with ASD. By integrating multiple\nspecialized expert branches with attention mechanisms, ASDFormer adaptively\nemphasizes different brain regions and connectivity patterns relevant to\nautism. This enables both improved classification performance and more\ninterpretable identification of disorder-related biomarkers. Applied to the\nABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and\nreveals robust insights into functional connectivity disruptions linked to ASD,\nhighlighting its potential as a tool for biomarker discovery.",
      "pdf_url": "http://arxiv.org/pdf/2508.14005v1",
      "published": "2025-08-19T17:05:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.14005v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation",
      "authors": [
        "Yifu Yuan",
        "Haiqin Cui",
        "Yaoting Huang",
        "Yibin Chen",
        "Fei Ni",
        "Zibin Dong",
        "Pengyi Li",
        "Yan Zheng",
        "Jianye Hao"
      ],
      "abstract": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which\nstems from data scarcity and embodiment heterogeneity. To address this, we\npioneer \"pointing\" as a unified, embodiment-agnostic intermediate\nrepresentation, defining four core embodied pointing abilities that bridge\nhigh-level vision-language comprehension with low-level action primitives. We\nintroduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed\nfor embodied reasoning and pointing. We use a wide range of embodied and\ngeneral visual reasoning datasets as sources to construct a large-scale\ndataset, Embodied-Points-200K, which supports key embodied pointing\ncapabilities. We then train Embodied-R1 using a two-stage Reinforced\nFine-tuning (RFT) curriculum with a specialized multi-task reward design.\nEmbodied-R1 achieves state-of-the-art performance on 11 embodied spatial and\npointing benchmarks. Critically, it demonstrates robust zero-shot\ngeneralization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%\nacross 8 real-world XArm tasks without any task-specific fine-tuning,\nrepresenting a 62% improvement over strong baselines. Furthermore, the model\nexhibits high robustness against diverse visual disturbances. Our work shows\nthat a pointing-centric representation, combined with an RFT training paradigm,\noffers an effective and generalizable pathway to closing the perception-action\ngap in robotics.",
      "pdf_url": "http://arxiv.org/pdf/2508.13998v1",
      "published": "2025-08-19T16:50:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13998v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization",
      "authors": [
        "Shaohua Duan",
        "Xinze Li",
        "Zhenghao Liu",
        "Xiaoyuan Yi",
        "Yukun Yan",
        "Shuo Wang",
        "Yu Gu",
        "Ge Yu",
        "Maosong Sun"
      ],
      "abstract": "Long-context modeling is critical for a wide range of real-world tasks,\nincluding long-context question answering, summarization, and complex reasoning\ntasks. Recent studies have explored fine-tuning Large Language Models (LLMs)\nwith synthetic data to enhance their long-context capabilities. However, the\neffectiveness of such approaches is often limited by the low diversity and\nfactual inconsistencies in the generated data. To address these challenges, we\npropose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)\nrollout strategy to identify the most informative chunks from the given long\ncontext for sampling high-quality and diverse responses and constructing\npreference data pairs for Direct Preference Optimization (DPO) training.\nSpecifically, we treat context chunks as arms of MAB, select chunks based on\ntheir expected reward scores to input into LLMs to generate responses, and\niteratively update these scores based on reward feedback. This exploration and\nexploitation process enables the model to focus on the most relevant context\nsegments, thereby generating and collecting high-quality and diverse responses.\nFinally, we collect these generated responses from the rollout process and\napply the DPO method to further optimize the LLM. Experimental results show\nthat LongMab-PO significantly improves the diversity and quality of preference\ndata pairs, achieving state-of-the-art performance on long-context reasoning\nbenchmarks. All code and data will be released on\nhttps://github.com/NEUIR/LongMab-PO.",
      "pdf_url": "http://arxiv.org/pdf/2508.13993v1",
      "published": "2025-08-19T16:33:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13993v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "The Social Context of Human-Robot Interactions",
      "authors": [
        "Sydney Thompson",
        "Kate Candon",
        "Marynel Vázquez"
      ],
      "abstract": "The Human-Robot Interaction (HRI) community often highlights the social\ncontext of an interaction as a key consideration when designing, implementing,\nand evaluating robot behavior. Unfortunately, researchers use the term \"social\ncontext\" in varied ways. This can lead to miscommunication, making it\nchallenging to draw connections between related work on understanding and\nmodeling the social contexts of human-robot interactions. To address this gap,\nwe survey the HRI literature for existing definitions and uses of the term\n\"social context\". Then, we propose a conceptual model for describing the social\ncontext of a human-robot interaction. We apply this model to existing work, and\nwe discuss a range of attributes of social contexts that can help researchers\nplan for interactions, develop behavior models for robots, and gain insights\nafter interactions have taken place. We conclude with a discussion of open\nresearch questions in relation to understanding and modeling the social\ncontexts of human-robot interactions.",
      "pdf_url": "http://arxiv.org/pdf/2508.13982v1",
      "published": "2025-08-19T16:15:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13982v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.MA",
        "I.2.9; I.2"
      ]
    },
    {
      "title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation",
      "authors": [
        "Jingquan Wang",
        "Andrew Negrut",
        "Harry Zhang",
        "Khailanii Slaton",
        "Shu Wang",
        "Radu Serban",
        "Jinlong Wu",
        "Dan Negrut"
      ],
      "abstract": "This contribution is concerned with the following issue: can pretrained large\nlanguage models (LLMs) be refined and customized to the point where they become\nvirtual assistants helping experts with the effective use of a simulation tool?\nIn this case study, the ``simulation tool'' considered is PyChrono, an open\nsource multi-physics dynamics engine for multibody systems. We present a\nframework for refining and customizing both open- and closed-source LLMs to\nharness the power of AI in generating scripts that perform PyChrono virtual\nexperiments. We refine and customize several classes of LLMs through a process\nthat leads to a quantifiable improvement in the quality of the generated\nPyChrono simulation scripts. These scripts can range from simple\nsingle-pendulum simulations to complex virtual experiments involving full\nvehicles on deformable terrain. While the generated scripts are rarely perfect,\nthey often serve as strong starting points for the user to modify and improve\non. Additionally, the LLM can answer specific API questions about the\nsimulator, or recommend modeling approaches. The framework discussed is general\nand can be applied to lower the entry barrier for simulation tools associated\nwith other application domains.",
      "pdf_url": "http://arxiv.org/pdf/2508.13975v1",
      "published": "2025-08-19T16:12:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13975v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation",
      "authors": [
        "Tianyi Niu",
        "Jaemin Cho",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "We investigate to what extent Multimodal Large Language Models (MLLMs) can\naccurately identify the orientation of input images rotated 0{\\deg}, 90{\\deg},\n180{\\deg}, and 270{\\deg}. This task demands robust visual reasoning\ncapabilities to detect rotational cues and contextualize spatial relationships\nwithin images, regardless of their orientation. To evaluate MLLMs on these\nabilities, we introduce RotBench -- a 350-image manually-filtered benchmark\ncomprising lifestyle, portrait, and landscape images. Despite the relatively\nsimple nature of this task, we show that several state-of-the-art open and\nproprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably\nidentify rotation in input images. Providing models with auxiliary information\n-- including captions, depth maps, and more -- or using chain-of-thought\nprompting offers only small and inconsistent improvements. Our results indicate\nthat most models are able to reliably identify right-side-up (0{\\deg}) images,\nwhile certain models are able to identify upside-down (180{\\deg}) images. None\ncan reliably distinguish between 90{\\deg} and 270{\\deg}. Simultaneously showing\nthe image rotated in different orientations leads to moderate performance gains\nfor reasoning models, while a modified setup using voting improves the\nperformance of weaker models. We further show that fine-tuning does not improve\nmodels' ability to distinguish 90{\\deg} and 270{\\deg} rotations, despite\nsubstantially improving the identification of 180{\\deg} images. Together, these\nresults reveal a significant gap between MLLMs' spatial reasoning capabilities\nand human perception in identifying rotation.",
      "pdf_url": "http://arxiv.org/pdf/2508.13968v1",
      "published": "2025-08-19T15:58:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13968v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K-12 Students?",
      "authors": [
        "Ruiwei Xiao",
        "Xinying Hou",
        "Ying-Jui Tseng",
        "Hsuan Nieu",
        "Guanze Liao",
        "John Stamper",
        "Kenneth R. Koedinger"
      ],
      "abstract": "As Artificial Intelligence (AI) becomes increasingly integrated into daily\nlife, there is a growing need to equip the next generation with the ability to\napply, interact with, evaluate, and collaborate with AI systems responsibly.\nPrior research highlights the urgent demand from K-12 educators to teach\nstudents the ethical and effective use of AI for learning. To address this\nneed, we designed an Large-Language Model (LLM)-based module to teach prompting\nliteracy. This includes scenario-based deliberate practice activities with\ndirect interaction with intelligent LLM agents, aiming to foster secondary\nschool students' responsible engagement with AI chatbots. We conducted two\niterations of classroom deployment in 11 authentic secondary education\nclassrooms, and evaluated 1) AI-based auto-grader's capability; 2) students'\nprompting performance and confidence changes towards using AI for learning; and\n3) the quality of learning and assessment materials. Results indicated that the\nAI-based auto-grader could grade student-written prompts with satisfactory\nquality. In addition, the instructional materials supported students in\nimproving their prompting skills through practice and led to positive shifts in\ntheir perceptions of using AI for learning. Furthermore, data from Study 1\ninformed assessment revisions in Study 2. Analyses of item difficulty and\ndiscrimination in Study 2 showed that True/False and open-ended questions could\nmeasure prompting literacy more effectively than multiple-choice questions for\nour target learners. These promising outcomes highlight the potential for\nbroader deployment and highlight the need for broader studies to assess\nlearning effectiveness and assessment design.",
      "pdf_url": "http://arxiv.org/pdf/2508.13962v1",
      "published": "2025-08-19T15:54:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13962v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "A Mechanism for Mutual Fairness in Cooperative Games with Replicable Resources -- Extended Version",
      "authors": [
        "Björn Filter",
        "Ralf Möller",
        "Özgür Lütfü Özçep"
      ],
      "abstract": "The latest developments in AI focus on agentic systems where artificial and\nhuman agents cooperate to realize global goals. An example is collaborative\nlearning, which aims to train a global model based on data from individual\nagents. A major challenge in designing such systems is to guarantee safety and\nalignment with human values, particularly a fair distribution of rewards upon\nachieving the global goal. Cooperative game theory offers useful abstractions\nof cooperating agents via value functions, which assign value to each\ncoalition, and via reward functions. With these, the idea of fair allocation\ncan be formalized by specifying fairness axioms and designing concrete\nmechanisms. Classical cooperative game theory, exemplified by the Shapley\nvalue, does not fully capture scenarios like collaborative learning, as it\nassumes nonreplicable resources, whereas data and models can be replicated.\nInfinite replicability requires a generalized notion of fairness, formalized\nthrough new axioms and mechanisms. These must address imbalances in reciprocal\nbenefits among participants, which can lead to strategic exploitation and\nunfair allocations. The main contribution of this paper is a mechanism and a\nproof that it fulfills the property of mutual fairness, formalized by the\nBalanced Reciprocity Axiom. It ensures that, for every pair of players, each\nbenefits equally from the participation of the other.",
      "pdf_url": "http://arxiv.org/pdf/2508.13960v1",
      "published": "2025-08-19T15:53:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13960v1",
      "categories": [
        "cs.GT",
        "cs.AI"
      ]
    },
    {
      "title": "Prompt Orchestration Markup Language",
      "authors": [
        "Yuge Zhang",
        "Nan Chen",
        "Jiahang Xu",
        "Yuqing Yang"
      ],
      "abstract": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2508.13948v1",
      "published": "2025-08-19T15:37:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13948v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ]
    },
    {
      "title": "The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management",
      "authors": [
        "Soumyadeep Dhar"
      ],
      "abstract": "The rise of autonomous, AI-driven agents in economic settings raises critical\nquestions about their emergent strategic behavior. This paper investigates\nthese dynamics in the cooperative context of a multi-echelon supply chain, a\nsystem famously prone to instabilities like the bullwhip effect. We conduct\ncomputational experiments with generative AI agents, powered by Large Language\nModels (LLMs), within a controlled supply chain simulation designed to isolate\ntheir behavioral tendencies. Our central finding is the \"collaboration\nparadox\": a novel, catastrophic failure mode where theoretically superior\ncollaborative AI agents, designed with Vendor-Managed Inventory (VMI)\nprinciples, perform even worse than non-AI baselines. We demonstrate that this\nparadox arises from an operational flaw where agents hoard inventory, starving\nthe system. We then show that resilience is only achieved through a synthesis\nof two distinct layers: high-level, AI-driven proactive policy-setting to\nestablish robust operational targets, and a low-level, collaborative execution\nprotocol with proactive downstream replenishment to maintain stability. Our\nfinal framework, which implements this synthesis, can autonomously generate,\nevaluate, and quantify a portfolio of viable strategic choices. The work\nprovides a crucial insight into the emergent behaviors of collaborative AI\nagents and offers a blueprint for designing stable, effective AI-driven systems\nfor business analytics.",
      "pdf_url": "http://arxiv.org/pdf/2508.13942v1",
      "published": "2025-08-19T15:31:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13942v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems",
      "authors": [
        "Matey Krastev",
        "Miklos Hamar",
        "Danilo Toapanta",
        "Jesse Brouwers",
        "Yibin Lei"
      ],
      "abstract": "This work revisits and extends synthetic query generation pipelines for\nNeural Information Retrieval (NIR) by leveraging the InPars Toolkit, a\nreproducible, end-to-end framework for generating training data using large\nlanguage models (LLMs). We first assess the reproducibility of the original\nInPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and\nvalidate their effectiveness using open-source reranker and generator models.\nBuilding on this foundation, we introduce two key extensions to the pipeline:\n(1) fine-tuning a query generator LLM via Contrastive Preference Optimization\n(CPO) to improve the signal quality in generated queries, and (2) replacing\nstatic prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts\nusing the DSPy framework. Our results show that both extensions reduce the need\nfor aggressive filtering while improving retrieval performance. All code,\nmodels, and synthetic datasets are publicly released to support further\nresearch at: \\href{https://github.com/danilotpnta/IR2-project}{this https URL}.",
      "pdf_url": "http://arxiv.org/pdf/2508.13930v1",
      "published": "2025-08-19T15:23:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13930v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control",
      "authors": [
        "SM Mazharul Islam",
        "Manfred Huber"
      ],
      "abstract": "A policy in deep reinforcement learning (RL), either deterministic or\nstochastic, is commonly parameterized as a Gaussian distribution alone,\nlimiting the learned behavior to be unimodal. However, the nature of many\npractical decision-making problems favors a multimodal policy that facilitates\nrobust exploration of the environment and thus to address learning challenges\narising from sparse rewards, complex dynamics, or the need for strategic\nadaptation to varying contexts. This issue is exacerbated in continuous control\ndomains where exploration usually takes place in the vicinity of the predicted\noptimal action, either through an additive Gaussian noise or the sampling\nprocess of a stochastic policy. In this paper, we introduce Categorical\nPolicies to model multimodal behavior modes with an intermediate categorical\ndistribution, and then generate output action that is conditioned on the\nsampled mode. We explore two sampling schemes that ensure differentiable\ndiscrete latent structure while maintaining efficient gradient-based\noptimization. By utilizing a latent categorical distribution to select the\nbehavior mode, our approach naturally expresses multimodality while remaining\nfully differentiable via the sampling tricks. We evaluate our multimodal policy\non a set of DeepMind Control Suite environments, demonstrating that through\nbetter exploration, our learned policies converge faster and outperform\nstandard Gaussian policies. Our results indicate that the Categorical\ndistribution serves as a powerful tool for structured exploration and\nmultimodal behavior representation in continuous control.",
      "pdf_url": "http://arxiv.org/pdf/2508.13922v1",
      "published": "2025-08-19T15:18:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13922v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback",
      "authors": [
        "Yihao Ang",
        "Yifan Bao",
        "Lei Jiang",
        "Jiajie Tao",
        "Anthony K. H. Tung",
        "Lukasz Szpruch",
        "Hao Ni"
      ],
      "abstract": "Time-series data is central to decision-making in financial markets, yet\nbuilding high-performing, interpretable, and auditable models remains a major\nchallenge. While Automated Machine Learning (AutoML) frameworks streamline\nmodel development, they often lack adaptability and responsiveness to\ndomain-specific needs and evolving objectives. Concurrently, Large Language\nModels (LLMs) have enabled agentic systems capable of reasoning, memory\nmanagement, and dynamic code generation, offering a path toward more flexible\nworkflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular\nagentic framework designed to automate and enhance time-series modeling\nworkflows for financial applications. The agent formalizes the pipeline as a\nstructured, iterative decision process across three stages: model selection,\ncode refinement, and fine-tuning, guided by contextual reasoning and\nexperimental feedback. Central to our architecture is a planner agent equipped\nwith structured knowledge banks, curated libraries of models and refinement\nstrategies, which guide exploration, while improving interpretability and\nreducing error propagation. \\textsf{TS-Agent} supports adaptive learning,\nrobust debugging, and transparent auditing, key requirements for high-stakes\nenvironments such as financial services. Empirical evaluations on diverse\nfinancial forecasting and synthetic data generation tasks demonstrate that\n\\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic\nbaselines, achieving superior accuracy, robustness, and decision traceability.",
      "pdf_url": "http://arxiv.org/pdf/2508.13915v1",
      "published": "2025-08-19T15:14:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13915v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Fisher-Orthogonal Projection Methods for Natural Gradient Descent with Large Batches",
      "authors": [
        "Yishun Lu",
        "Wesley Armour"
      ],
      "abstract": "Modern GPUs are equipped with large amounts of high-bandwidth memory,\nenabling them to support mini-batch sizes of up to tens of thousands of\ntraining samples. However, most existing optimizers struggle to perform\neffectively at such a large batch size. As batch size increases, gradient noise\ndecreases due to averaging over many samples, limiting the ability of\nfirst-order methods to escape sharp or suboptimal minima and reach the global\nminimum. Meanwhile, second-order methods like the natural gradient with\nKronecker-Factored Approximate Curvature (KFAC) often require excessively high\ndamping to remain stable at large batch sizes. This high damping effectively\nwashes out the curvature information that gives these methods their advantage,\nreducing their performance to that of simple gradient descent. In this paper,\nwe introduce Fisher-Orthogonal Projection (FOP), a novel technique that\nrestores the effectiveness of the second-order method at very large batch\nsizes, enabling scalable training with improved generalization and faster\nconvergence. FOP constructs a variance-aware update direction by leveraging\ngradients from two sub-batches, enhancing the average gradient with a component\nof the gradient difference that is orthogonal to the average under the\nFisher-metric.",
      "pdf_url": "http://arxiv.org/pdf/2508.13898v1",
      "published": "2025-08-19T15:02:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13898v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Improved Generalized Planning with LLMs through Strategy Refinement and Reflection",
      "authors": [
        "Katharina Stein",
        "Nils Hodel",
        "Daniel Fišer",
        "Jörg Hoffmann",
        "Michael Katz",
        "Alexander Koller"
      ],
      "abstract": "LLMs have recently been used to generate Python programs representing\ngeneralized plans in PDDL planning, i.e., plans that generalize across the\ntasks of a given PDDL domain. Previous work proposed a framework consisting of\nthree steps: the LLM first generates a summary and then a strategy for the\ndomain, both in natural language, and then implements that strategy as a Python\nprogram, that gets debugged on example planning tasks. In that work, only one\nstrategy is generated and passed directly to the program generation. If the\nstrategy is incorrect, its implementation will therefore result in an incorrect\ngeneralized plan. Here, we introduce an approach that generates the strategy in\nthe form of pseudocode and enables automatic debugging of the pseudocode, hence\nallowing us to identify and fix errors prior to the generation of the\ngeneralized plan itself. Additionally, we extend the Python debugging phase\nwith a reflection step prompting the LLM to pinpoint the reason for the\nobserved plan failure. Finally, we take inspiration from LLM code generation to\nproduce several program variants and pick the best one. Running experiments on\n17 benchmark domains, we show that these extensions substantially improve (and\nnever deteriorate) the quality of the generalized plans. In 12 of the domains,\nour best Python programs solve all tasks that can be generated with the\nrespective instance generator.",
      "pdf_url": "http://arxiv.org/pdf/2508.13876v1",
      "published": "2025-08-19T14:42:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13876v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer",
      "authors": [
        "Rathnam Vidushika Rasanji",
        "Jin Wei-Kocsis",
        "Jiansong Zhang",
        "Dongming Gan",
        "Ragu Athinarayanan",
        "Paul Asunda"
      ],
      "abstract": "Reinforcement learning (RL) has demonstrated great potential in robotic\noperations. However, its data-intensive nature and reliance on the Markov\nDecision Process (MDP) assumption limit its practical deployment in real-world\nscenarios involving complex dynamics and long-term temporal dependencies, such\nas multi-robot manipulation. Decision Transformers (DTs) have emerged as a\npromising offline alternative by leveraging causal transformers for sequence\nmodeling in RL tasks. However, their applications to multi-robot manipulations\nstill remain underexplored. To address this gap, we propose a novel framework,\nSymbolically-Guided Decision Transformer (SGDT), which integrates a\nneuro-symbolic mechanism with a causal transformer to enable deployable\nmulti-robot collaboration. In the proposed SGDT framework, a neuro-symbolic\nplanner generates a high-level task-oriented plan composed of symbolic\nsubgoals. Guided by these subgoals, a goal-conditioned decision transformer\n(GCDT) performs low-level sequential decision-making for multi-robot\nmanipulation. This hierarchical architecture enables structured, interpretable,\nand generalizable decision making in complex multi-robot collaboration tasks.\nWe evaluate the performance of SGDT across a range of task scenarios, including\nzero-shot and few-shot scenarios. To our knowledge, this is the first work to\nexplore DT-based technology for multi-robot manipulation.",
      "pdf_url": "http://arxiv.org/pdf/2508.13877v1",
      "published": "2025-08-19T14:42:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13877v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain Vessel Segmentation on Transcranial Color-coded Doppler",
      "authors": [
        "Wenxuan Zhang",
        "Shuai Li",
        "Xinyi Wang",
        "Yu Sun",
        "Hongyu Kang",
        "Pui Yuk Chryste Wan",
        "Yong-Ping Zheng",
        "Sai-Kit Lam"
      ],
      "abstract": "The Circle of Willis (CoW), vital for ensuring consistent blood flow to the\nbrain, is closely linked to ischemic stroke. Accurate assessment of the CoW is\nimportant for identifying individuals at risk and guiding appropriate clinical\nmanagement. Among existing imaging methods, Transcranial Color-coded Doppler\n(TCCD) offers unique advantages due to its radiation-free nature,\naffordability, and accessibility. However, reliable TCCD assessments depend\nheavily on operator expertise for identifying anatomical landmarks and\nperforming accurate angle correction, which limits its widespread adoption. To\naddress this challenge, we propose an AI-powered, real-time CoW\nauto-segmentation system capable of efficiently capturing cerebral arteries. No\nprior studies have explored AI-driven cerebrovascular segmentation using TCCD.\nIn this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO)\nnetwork tailored for TCCD data, designed to provide real-time guidance for\nbrain vessel segmentation in the CoW. We prospectively collected TCCD data\ncomprising 738 annotated frames and 3,419 labeled artery instances to establish\na high-quality dataset for model training and evaluation. The proposed AAW-YOLO\ndemonstrated strong performance in segmenting both ipsilateral and\ncontralateral CoW vessels, achieving an average Dice score of 0.901, IoU of\n0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame\ninference speed of 14.199 ms. This system offers a practical solution to reduce\nreliance on operator experience in TCCD-based cerebrovascular screening, with\npotential applications in routine clinical workflows and resource-constrained\nsettings. Future research will explore bilateral modeling and larger-scale\nvalidation.",
      "pdf_url": "http://arxiv.org/pdf/2508.13875v1",
      "published": "2025-08-19T14:41:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13875v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion",
      "authors": [
        "Zihan Liang",
        "Yufei Ma",
        "ZhiPeng Qian",
        "Huangyu Dai",
        "Zihan Wang",
        "Ben Chen",
        "Chenyi Lei",
        "Yuqing Ding",
        "Han Li"
      ],
      "abstract": "Current e-commerce multimodal retrieval systems face two key limitations:\nthey optimize for specific tasks with fixed modality pairings, and lack\ncomprehensive benchmarks for evaluating unified retrieval approaches. To\naddress these challenges, we introduce UniECS, a unified multimodal e-commerce\nsearch framework that handles all retrieval scenarios across image, text, and\ntheir combinations. Our work makes three key contributions. First, we propose a\nflexible architecture with a novel gated multimodal encoder that uses adaptive\nfusion mechanisms. This encoder integrates different modality representations\nwhile handling missing modalities. Second, we develop a comprehensive training\nstrategy to optimize learning. It combines cross-modal alignment loss (CMAL),\ncohesive local alignment loss (CLAL), intra-modal contrastive loss (IMCL), and\nadaptive loss weighting. Third, we create M-BEER, a carefully curated\nmultimodal benchmark containing 50K product pairs for e-commerce search\nevaluation. Extensive experiments demonstrate that UniECS consistently\noutperforms existing methods across four e-commerce benchmarks with fine-tuning\nor zero-shot evaluation. On our M-BEER bench, UniECS achieves substantial\nimprovements in cross-modal tasks (up to 28\\% gain in R@10 for text-to-image\nretrieval) while maintaining parameter efficiency (0.2B parameters) compared to\nlarger models like GME-Qwen2VL (2B) and MM-Embed (8B). Furthermore, we deploy\nUniECS in the e-commerce search platform of Kuaishou Inc. across two search\nscenarios, achieving notable improvements in Click-Through Rate (+2.74\\%) and\nRevenue (+8.33\\%). The comprehensive evaluation demonstrates the effectiveness\nof our approach in both experimental and real-world settings. Corresponding\ncodes, models and datasets will be made publicly available at\nhttps://github.com/qzp2018/UniECS.",
      "pdf_url": "http://arxiv.org/pdf/2508.13843v1",
      "published": "2025-08-19T14:06:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13843v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression",
      "authors": [
        "Mikołaj Janusz",
        "Tomasz Wojnar",
        "Yawei Li",
        "Luca Benini",
        "Kamil Adamczewski"
      ],
      "abstract": "Pruning is a core technique for compressing neural networks to improve\ncomputational efficiency. This process is typically approached in two ways:\none-shot pruning, which involves a single pass of training and pruning, and\niterative pruning, where pruning is performed over multiple cycles for\npotentially finer network refinement. Although iterative pruning has\nhistorically seen broader adoption, this preference is often assumed rather\nthan rigorously tested. Our study presents one of the first systematic and\ncomprehensive comparisons of these methods, providing rigorous definitions,\nbenchmarking both across structured and unstructured settings, and applying\ndifferent pruning criteria and modalities. We find that each method has\nspecific advantages: one-shot pruning proves more effective at lower pruning\nratios, while iterative pruning performs better at higher ratios. Building on\nthese findings, we advocate for patience-based pruning and introduce a hybrid\napproach that can outperform traditional methods in certain scenarios,\nproviding valuable insights for practitioners selecting a pruning strategy\ntailored to their goals and constraints. Source code is available at\nhttps://github.com/janumiko/pruning-benchmark.",
      "pdf_url": "http://arxiv.org/pdf/2508.13836v1",
      "published": "2025-08-19T13:57:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13836v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling",
      "authors": [
        "Insaf Nahri",
        "Romain Pinquié",
        "Philippe Véron",
        "Nicolas Bus",
        "Mathieu Thorel"
      ],
      "abstract": "This study explores the integration of Building Information Modeling (BIM)\nwith Natural Language Processing (NLP) to automate the extraction of\nrequirements from unstructured French Building Technical Specification (BTS)\ndocuments within the construction industry. Employing Named Entity Recognition\n(NER) and Relation Extraction (RE) techniques, the study leverages the\ntransformer-based model CamemBERT and applies transfer learning with the French\nlanguage model Fr\\_core\\_news\\_lg, both pre-trained on a large French corpus in\nthe general domain. To benchmark these models, additional approaches ranging\nfrom rule-based to deep learning-based methods are developed. For RE, four\ndifferent supervised models, including Random Forest, are implemented using a\ncustom feature vector. A hand-crafted annotated dataset is used to compare the\neffectiveness of NER approaches and RE models. Results indicate that CamemBERT\nand Fr\\_core\\_news\\_lg exhibited superior performance in NER, achieving\nF1-scores over 90\\%, while Random Forest proved most effective in RE, with an\nF1 score above 80\\%. The outcomes are intended to be represented as a knowledge\ngraph in future work to further enhance automatic verification systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.13833v1",
      "published": "2025-08-19T13:55:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13833v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration",
      "authors": [
        "Yifei Chen",
        "Guanting Dong",
        "Yutao Zhu",
        "Zhicheng Dou"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) technology has been widely applied in\nrecent years. However, despite the emergence of various RAG frameworks, a\nsingle RAG framework still cannot adapt well to a broad range of downstream\ntasks. Therefore, how to leverage the advantages of multiple RAG systems has\nbecome an area worth exploring. To address this issue, we have conducted a\ncomprehensive and systematic investigation into ensemble methods based on RAG\nsystems. Specifically, we have analyzed the RAG ensemble framework from both\ntheoretical and mechanistic analysis perspectives. From the theoretical\nanalysis, we provide the first explanation of the RAG ensemble framework from\nthe perspective of information entropy. In terms of mechanism analysis, we have\nexplored the RAG ensemble framework from both the pipeline and module levels.\nWe carefully select four different pipelines (Branching, Iterative, Loop, and\nAgentic) and three different modules (Generator, Retriever, and Reranker) to\nsolve seven different research questions. The experiments show that aggregating\nmultiple RAG systems is both generalizable and robust, whether at the pipeline\nlevel or the module level. Our work lays the foundation for similar research on\nthe multi-RAG system ensemble.",
      "pdf_url": "http://arxiv.org/pdf/2508.13828v1",
      "published": "2025-08-19T13:38:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13828v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The illusion of a perfect metric: Why evaluating AI's words is harder than it looks",
      "authors": [
        "Maria Paz Oliva",
        "Adriana Correia",
        "Ivan Vankov",
        "Viktor Botev"
      ],
      "abstract": "Evaluating Natural Language Generation (NLG) is crucial for the practical\nadoption of AI, but has been a longstanding research challenge. While human\nevaluation is considered the de-facto standard, it is expensive and lacks\nscalability. Practical applications have driven the development of various\nautomatic evaluation metrics (AEM), designed to compare the model output with\nhuman-written references, generating a score which approximates human judgment.\nOver time, AEMs have evolved from simple lexical comparisons, to semantic\nsimilarity models and, more recently, to LLM-based evaluators. However, it\nseems that no single metric has emerged as a definitive solution, resulting in\nstudies using different ones without fully considering the implications. This\npaper aims to show this by conducting a thorough examination of the\nmethodologies of existing metrics, their documented strengths and limitations,\nvalidation methods, and correlations with human judgment. We identify several\nkey challenges: metrics often capture only specific aspects of text quality,\ntheir effectiveness varies by task and dataset, validation practices remain\nunstructured, and correlations with human judgment are inconsistent.\nImportantly, we find that these challenges persist in the most recent type of\nmetric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented\nGeneration (RAG), an increasingly relevant task in academia and industry. Our\nfindings challenge the quest for the 'perfect metric'. We propose selecting\nmetrics based on task-specific needs and leveraging complementary evaluations\nand advocate that new metrics should focus on enhanced validation\nmethodologies.",
      "pdf_url": "http://arxiv.org/pdf/2508.13816v1",
      "published": "2025-08-19T13:22:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13816v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias",
      "authors": [
        "Koffi Ismael Ouattara",
        "Ioannis Krontiris",
        "Theo Dimitrakos",
        "Frank Kargl"
      ],
      "abstract": "As AI systems increasingly rely on training data, assessing dataset\ntrustworthiness has become critical, particularly for properties like fairness\nor bias that emerge at the dataset level. Prior work has used Subjective Logic\nto assess trustworthiness of individual data, but not to evaluate\ntrustworthiness properties that emerge only at the level of the dataset as a\nwhole. This paper introduces the first formal framework for assessing the\ntrustworthiness of AI training datasets, enabling uncertainty-aware evaluations\nof global properties such as bias. Built on Subjective Logic, our approach\nsupports trust propositions and quantifies uncertainty in scenarios where\nevidence is incomplete, distributed, and/or conflicting. We instantiate this\nframework on the trustworthiness property of bias, and we experimentally\nevaluate it based on a traffic sign recognition dataset. The results\ndemonstrate that our method captures class imbalance and remains interpretable\nand robust in both centralized and federated contexts.",
      "pdf_url": "http://arxiv.org/pdf/2508.13813v1",
      "published": "2025-08-19T13:17:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13813v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Quantifier Instantiations: To Mimic or To Revolt?",
      "authors": [
        "Jan Jakubův",
        "Mikoláš Janota"
      ],
      "abstract": "Quantified formulas pose a significant challenge for Satisfiability Modulo\nTheories (SMT) solvers due to their inherent undecidability. Existing\ninstantiation techniques, such as e-matching, syntax-guided, model-based,\nconflict-based, and enumerative methods, often complement each other. This\npaper introduces a novel instantiation approach that dynamically learns from\nthese techniques during solving. By treating observed instantiations as samples\nfrom a latent language, we use probabilistic context-free grammars to generate\nnew, similar terms. Our method not only mimics successful past instantiations\nbut also explores diversity by optionally inverting learned term probabilities,\naiming to balance exploitation and exploration in quantifier reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2508.13811v1",
      "published": "2025-08-19T13:16:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13811v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs",
      "authors": [
        "Juncheng Xie",
        "Hung-yi Lee"
      ],
      "abstract": "Controlling the length of text produced by large language models (LLMs)\nremains challenging: models frequently overshoot or undershoot explicit length\ninstructions because they cannot reliably keep an internal token count. We\npresent a prompt-based, one-shot strategy that compels an off-the-shelf LLM to\ngenerate exactly a desired number of tokens - words (English) or characters\n(Chinese) - without any fine-tuning or iterative sampling. The prompt appends\ncountdown markers and explicit counting rules so that the model \"writes while\ncounting.\" We evaluate on four settings: open-ended generation (1-1000 tokens),\nXSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH\nequal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps\nfrom below 30% under naive prompts to above 95% with our countdown prompt,\nsurpassing the popular draft-then-revise baseline, while judged answer quality\nis preserved. These results show that precise length control can be achieved\nthrough prompt engineering alone, offering a lightweight alternative to\ntraining- or decoding-based methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.13805v1",
      "published": "2025-08-19T13:12:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13805v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports",
      "authors": [
        "Enobong Adahada",
        "Isabel Sassoon",
        "Kate Hone",
        "Yongmin Li"
      ],
      "abstract": "We introduce Med-CTX, a fully transformer based multimodal framework for\nexplainable breast cancer ultrasound segmentation. We integrate clinical\nradiology reports to boost both performance and interpretability. Med-CTX\nachieves exact lesion delineation by using a dual-branch visual encoder that\ncombines ViT and Swin transformers, as well as uncertainty aware fusion.\nClinical language structured with BI-RADS semantics is encoded by\nBioClinicalBERT and combined with visual features utilising cross-modal\nattention, allowing the model to provide clinically grounded, model generated\nexplanations. Our methodology generates segmentation masks, uncertainty maps,\nand diagnostic rationales all at once, increasing confidence and transparency\nin computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice\nscore of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and\nSwin. Clinical text plays a key role in segmentation accuracy and explanation\nquality, as evidenced by ablation studies that show a -5.4% decline in Dice\nscore and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP\nscore: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new\nbar for trustworthy, multimodal medical architecture.",
      "pdf_url": "http://arxiv.org/pdf/2508.13796v1",
      "published": "2025-08-19T12:55:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13796v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web",
      "authors": [
        "Zihan Guo",
        "Yuanjian Zhou",
        "Chenyi Wang",
        "Linlin You",
        "Minjie Bian",
        "Weinan Zhang"
      ],
      "abstract": "The rapid development of large language models (LLMs) has significantly\npropelled the development of artificial intelligence (AI) agents, which are\nincreasingly evolving into diverse autonomous entities, advancing the LLM-based\nmulti-agent systems (LaMAS). However, current agentic ecosystems remain\nfragmented and closed. Establishing an interconnected and scalable paradigm for\nAgentic AI has become a critical prerequisite. Although Agentic Web proposes an\nopen architecture to break the ecosystem barriers, its implementation still\nfaces core challenges such as privacy protection, data management, and value\nmeasurement. Existing centralized or semi-centralized paradigms suffer from\ninherent limitations, making them inadequate for supporting large-scale,\nheterogeneous, and cross-domain autonomous interactions. To address these\nchallenges, this paper introduces the blockchain-enabled trustworthy Agentic\nWeb (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not\nonly offers a trustworthy and scalable infrastructure for LaMAS but also has\nthe potential to advance the Web paradigm from Web3 (centered on data\nownership) towards Web3.5, which emphasizes ownership of agent capabilities and\nthe monetization of intelligence. Beyond a systematic examination of the\nBetaWeb framework, this paper presents a five-stage evolutionary roadmap,\noutlining the path of LaMAS from passive execution to advanced collaboration\nand autonomous governance. We also conduct a comparative analysis of existing\nproducts and discuss key challenges of BetaWeb from multiple perspectives.\nUltimately, we argue that deep integration between blockchain and LaMAS can lay\nthe foundation for a resilient, trustworthy, and sustainably incentivized\ndigital ecosystem. A summary of the enabling technologies for each stage is\navailable at https://github.com/MatZaharia/BetaWeb.",
      "pdf_url": "http://arxiv.org/pdf/2508.13787v1",
      "published": "2025-08-19T12:43:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13787v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer",
      "authors": [
        "Yisu Liu",
        "Chenxing Li",
        "Wanqian Zhang",
        "Wenfu Wang",
        "Meng Yu",
        "Ruibo Fu",
        "Zheng Lin",
        "Weiping Wang",
        "Dong Yu"
      ],
      "abstract": "Controllable text-to-audio generation aims to synthesize audio from textual\ndescriptions while satisfying user-specified constraints, including event\ntypes, temporal sequences, and onset and offset timestamps. This enables\nprecise control over both the content and temporal structure of the generated\naudio. Despite recent progress, existing methods still face inherent trade-offs\namong accurate temporal localization, open-vocabulary scalability, and\npractical efficiency. To address these challenges, we propose DegDiT, a novel\ndynamic event graph-guided diffusion transformer framework for open-vocabulary\ncontrollable audio generation. DegDiT encodes the events in the description as\nstructured dynamic graphs. The nodes in each graph are designed to represent\nthree aspects: semantic features, temporal attributes, and inter-event\nconnections. A graph transformer is employed to integrate these nodes and\nproduce contextualized event embeddings that serve as guidance for the\ndiffusion model. To ensure high-quality and diverse training data, we introduce\na quality-balanced data selection pipeline that combines hierarchical event\nannotation with multi-criteria quality scoring, resulting in a curated dataset\nwith semantic diversity. Furthermore, we present consensus preference\noptimization, facilitating audio generation through consensus among multiple\nreward signals. Extensive experiments on AudioCondition, DESED, and AudioTime\ndatasets demonstrate that DegDiT achieves state-of-the-art performances across\na variety of objective and subjective evaluation metrics.",
      "pdf_url": "http://arxiv.org/pdf/2508.13786v1",
      "published": "2025-08-19T12:41:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13786v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images",
      "authors": [
        "Sebastian Ibarra",
        "Javier del Riego",
        "Alessandro Catanese",
        "Julian Cuba",
        "Julian Cardona",
        "Nataly Leon",
        "Jonathan Infante",
        "Karim Lekadir",
        "Oliver Diaz",
        "Richard Osuala"
      ],
      "abstract": "Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis\nand treatment. However, its reliance on contrast agents introduces safety\nconcerns, contraindications, increased cost, and workflow complexity. To this\nend, we present pre-contrast conditioned denoising diffusion probabilistic\nmodels to synthesize DCE-MRI, introducing, evaluating, and comparing a total of\n22 generative model variants in both single-breast and full breast settings.\nTowards enhancing lesion fidelity, we introduce both tumor-aware loss functions\nand explicit tumor segmentation mask conditioning. Using a public multicenter\ndataset and comparing to respective pre-contrast baselines, we observe that\nsubtraction image-based models consistently outperform post-contrast-based\nmodels across five complementary evaluation metrics. Apart from assessing the\nentire image, we also separately evaluate the region of interest, where both\ntumor-aware losses and segmentation mask inputs improve evaluation metrics. The\nlatter notably enhance qualitative results capturing contrast uptake, albeit\nassuming access to tumor localization inputs that are not guaranteed to be\navailable in screening settings. A reader study involving 2 radiologists and 4\nMRI technologists confirms the high realism of the synthetic images, indicating\nan emerging clinical potential of generative contrast-enhancement. We share our\ncodebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.",
      "pdf_url": "http://arxiv.org/pdf/2508.13776v1",
      "published": "2025-08-19T12:24:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13776v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API",
      "authors": [
        "Peer Trilcke",
        "Ingo Börner",
        "Henny Sluyter-Gäthje",
        "Daniil Skorinkin",
        "Frank Fischer",
        "Carsten Milling"
      ],
      "abstract": "This paper reports on the implementation and evaluation of a Model Context\nProtocol (MCP) server for DraCor, enabling Large Language Models (LLM) to\nautonomously interact with the DraCor API. We conducted experiments focusing on\ntool selection and application by the LLM, employing a qualitative approach\nthat includes systematic observation of prompts to understand how LLMs behave\nwhen using MCP tools, evaluating \"Tool Correctness\", \"Tool-Calling Efficiency\",\nand \"Tool-Use Reliability\". Our findings highlight the importance of \"Docstring\nEngineering\", defined as reflexively crafting tool documentation to optimize\nLLM-tool interaction. Our experiments demonstrate both the promise of agentic\nAI for research in Computational Literary Studies and the essential\ninfrastructure development needs for reliable Digital Humanities\ninfrastructures.",
      "pdf_url": "http://arxiv.org/pdf/2508.13774v1",
      "published": "2025-08-19T12:21:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13774v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "J.5; I.2"
      ]
    },
    {
      "title": "PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting",
      "authors": [
        "Tian Sun",
        "Yuqi Chen",
        "Weiwei Sun"
      ],
      "abstract": "Long-term time series forecasting (LTSF) is a fundamental task with\nwide-ranging applications. Although Transformer-based models have made\nsignificant breakthroughs in forecasting, their effectiveness for time series\nforecasting remains debatable. In this paper, we revisit the significance of\nself-attention and propose a simple yet effective mechanism, Periodic-Nested\nGroup Attention, namely PENGUIN. Our approach highlights the importance of\nexplicitly modeling periodic patterns and incorporating relative attention bias\nfor effective time series modeling. To this end, we introduce a periodic-nested\nrelative attention bias that captures periodic structures directly. To handle\nmultiple coexisting periodicities (e.g., daily and weekly cycles), we design a\ngrouped attention mechanism, where each group targets a specific periodicity\nusing a multi-query attention mechanism. Extensive experiments across diverse\nbenchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and\nTransformer-based models.",
      "pdf_url": "http://arxiv.org/pdf/2508.13773v1",
      "published": "2025-08-19T12:19:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13773v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models",
      "authors": [
        "James Meaden",
        "Michał Jarosz",
        "Piotr Jodłowski",
        "Grigori Melnik"
      ],
      "abstract": "Current code generation benchmarks focus primarily on functional correctness\nwhile overlooking two critical aspects of real-world programming: algorithmic\nefficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional\nProgramming ASSessment), a comprehensive evaluation framework that assesses\ncode generation across three dimensions: correctness, efficiency, and quality.\nCOMPASS consists of 50 competitive programming problems from real Codility\ncompetitions, providing authentic human baselines from 393,150 submissions.\nUnlike existing benchmarks that treat algorithmically inefficient solutions\nidentically to optimal ones provided they pass test cases, COMPASS\nsystematically evaluates runtime efficiency and code quality using\nindustry-standard analysis tools. Our evaluation of three leading\nreasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and\nOpenAI O4-Mini-High, reveals that models achieving high correctness scores do\nnot necessarily produce efficient algorithms or maintainable code. These\nfindings highlight the importance of evaluating more than just correctness to\ntruly understand the real-world capabilities of code generation models. COMPASS\nserves as a guiding framework, charting a path for future research toward AI\nsystems that are robust, reliable, and ready for production use.",
      "pdf_url": "http://arxiv.org/pdf/2508.13757v1",
      "published": "2025-08-19T11:55:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13757v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration",
      "authors": [
        "Zhicheng Yang",
        "Zhijiang Guo",
        "Yinya Huang",
        "Yongxin Wang",
        "Dongchun Xie",
        "Yiwei Wang",
        "Xiaodan Liang",
        "Jing Tang"
      ],
      "abstract": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.",
      "pdf_url": "http://arxiv.org/pdf/2508.13755v1",
      "published": "2025-08-19T11:51:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13755v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making",
      "authors": [
        "Liuxin Bao",
        "Zhihao Peng",
        "Xiaofei Zhou",
        "Runmin Cong",
        "Jiyong Zhang",
        "Yixuan Yuan"
      ],
      "abstract": "Medical Decision-Making (MDM) is a complex process requiring substantial\ndomain-specific expertise to effectively synthesize heterogeneous and\ncomplicated clinical information. While recent advancements in Large Language\nModels (LLMs) show promise in supporting MDM, single-LLM approaches are limited\nby their parametric knowledge constraints and static training corpora, failing\nto robustly integrate the clinical information. To address this challenge, we\npropose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)\nframework to enhance the accuracy and reliability of MDM systems. It operates\nin two stages: (i) expertise-aware agent recruitment and (ii) confidence- and\nadversarial-driven multi-agent collaboration. Specifically, in the first stage,\nwe use a publicly available corpus to construct an LLM expertise table for\ncapturing expertise-specific strengths of multiple LLMs across medical\ndepartment categories and query difficulty levels. This table enables the\nsubsequent dynamic selection of the optimal LLMs to act as medical expert\nagents for each medical query during the inference phase. In the second stage,\nwe employ selected agents to generate responses with self-assessed confidence\nscores, which are then integrated through the confidence fusion and adversarial\nvalidation to improve diagnostic reliability. We evaluate our EMRC framework on\nthree public MDM datasets, where the results demonstrate that our EMRC\noutperforms state-of-the-art single- and multi-LLM methods, achieving superior\ndiagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC\nachieves 74.45% accuracy, representing a 2.69% improvement over the\nbest-performing closed-source model GPT- 4-0613, which demonstrates the\neffectiveness of our expertise-aware agent recruitment strategy and the agent\ncomplementarity in leveraging each LLM's specialized capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2508.13754v1",
      "published": "2025-08-19T11:51:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13754v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks",
      "authors": [
        "Yeji Park",
        "Minyoung Lee",
        "Sanghyuk Chun",
        "Junsuk Choe"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) demonstrate strong performance on\nsingle-image tasks. However, we observe that their performance degrades\nsignificantly when handling multi-image inputs. This occurs because visual cues\nfrom different images become entangled in the model's output. We refer to this\nphenomenon as cross-image information leakage. To address this issue, we\npropose FOCUS, a training-free and architecture-agnostic decoding strategy that\nmitigates cross-image information leakage during inference. FOCUS sequentially\nmasks all but one image with random noise, guiding the model to focus on the\nsingle clean image. We repeat this process across all target images to obtain\nlogits under partially masked contexts. These logits are aggregated and then\ncontrastively refined using a noise-only reference input, which suppresses the\nleakage and yields more accurate outputs. FOCUS consistently improves\nperformance across four multi-image benchmarks and diverse LVLM families. This\ndemonstrates that FOCUS offers a general and practical solution for enhancing\nmulti-image reasoning without additional training or architectural\nmodifications.",
      "pdf_url": "http://arxiv.org/pdf/2508.13744v1",
      "published": "2025-08-19T11:31:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13744v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions",
      "authors": [
        "Daniel M. Jimenez-Gutierrez",
        "Yelizaveta Falkouskaya",
        "Jose L. Hernandez-Ramos",
        "Aris Anagnostopoulos",
        "Ioannis Chatzigiannakis",
        "Andrea Vitaletti"
      ],
      "abstract": "Federated Learning (FL) is an emerging distributed machine learning paradigm\nenabling multiple clients to train a global model collaboratively without\nsharing their raw data. While FL enhances data privacy by design, it remains\nvulnerable to various security and privacy threats. This survey provides a\ncomprehensive overview of more than 200 papers regarding the state-of-the-art\nattacks and defense mechanisms developed to address these challenges,\ncategorizing them into security-enhancing and privacy-preserving techniques.\nSecurity-enhancing methods aim to improve FL robustness against malicious\nbehaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same\ntime, privacy-preserving techniques focus on protecting sensitive data through\ncryptographic approaches, differential privacy, and secure aggregation. We\ncritically analyze the strengths and limitations of existing methods, highlight\nthe trade-offs between privacy, security, and model performance, and discuss\nthe implications of non-IID data distributions on the effectiveness of these\ndefenses. Furthermore, we identify open research challenges and future\ndirections, including the need for scalable, adaptive, and energy-efficient\nsolutions operating in dynamic and heterogeneous FL environments. Our survey\naims to guide researchers and practitioners in developing robust and\nprivacy-preserving FL systems, fostering advancements safeguarding\ncollaborative learning frameworks' integrity and confidentiality.",
      "pdf_url": "http://arxiv.org/pdf/2508.13730v1",
      "published": "2025-08-19T11:06:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13730v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings",
      "authors": [
        "Hanna Herasimchyk",
        "Alhassan Abdelhalim",
        "Sören Laue",
        "Michaela Regneri"
      ],
      "abstract": "Understanding what knowledge is implicitly encoded in deep learning models is\nessential for improving the interpretability of AI systems. This paper examines\ncommon methods to explain the knowledge encoded in word embeddings, which are\ncore elements of large language models (LLMs). These methods typically involve\nmapping embeddings onto collections of human-interpretable semantic features,\nknown as feature norms. Prior work assumes that accurately predicting these\nsemantic features from the word embeddings implies that the embeddings contain\nthe corresponding knowledge. We challenge this assumption by demonstrating that\nprediction accuracy alone does not reliably indicate genuine feature-based\ninterpretability.\n  We show that these methods can successfully predict even random information,\nconcluding that the results are predominantly determined by an algorithmic\nupper bound rather than meaningful semantic representation in the word\nembeddings. Consequently, comparisons between datasets based solely on\nprediction performance do not reliably indicate which dataset is better\ncaptured by the word embeddings. Our analysis illustrates that such mappings\nprimarily reflect geometric similarity within vector spaces rather than\nindicating the genuine emergence of semantic properties.",
      "pdf_url": "http://arxiv.org/pdf/2508.13729v1",
      "published": "2025-08-19T11:00:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13729v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning",
      "authors": [
        "Minh Hoang Nguyen",
        "Van Dai Do",
        "Dung Nguyen",
        "Thin Nguyen",
        "Hung Le"
      ],
      "abstract": "Large language model (LLM) agents-especially smaller, open-source\nmodels-often produce causally invalid or incoherent actions in collaborative\ntasks due to their reliance on surface-level correlations rather than grounded\ncausal reasoning. This limitation undermines their performance in terms of\ncoordination and planning in dynamic environments. We address this challenge\nwith CausalPlan, a two-phase framework that integrates explicit structural\ncausal reasoning into the LLM planning process. At the core of CausalPlan is\nthe Structural Causal Action (SCA) model, which learns a causal graph from\nagent trajectories to capture how prior actions and current environment states\ninfluence future decisions. This structure is then used to guide action\nselection by assigning causal scores to LLM-generated proposals, reweighting\nthem accordingly, or falling back to causally grounded alternatives when\nneeded. By embedding this causal knowledge directly into the decision loop,\nCausalPlan constrains planning to intervention-consistent behaviours without\nrequiring fine-tuning of the LLM itself. We evaluate CausalPlan on the\nOvercooked-AI benchmark across five multi-agent coordination tasks and four\nLLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.\nExperimental results show that CausalPlan consistently reduces invalid actions\nand improves collaboration in both AI-AI and human-AI settings, outperforming\nstrong reinforcement learning baselines. Our findings highlight the value of\ncausality-driven planning for deploying efficient, interpretable, and\ngeneralisable multi-agent LLM systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.13721v1",
      "published": "2025-08-19T10:37:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13721v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Generics and Default Reasoning in Large Language Models",
      "authors": [
        "James Ravi Kirkpatrick",
        "Rachel Katharine Sterken"
      ],
      "abstract": "This paper evaluates the capabilities of 28 large language models (LLMs) to\nreason with 20 defeasible reasoning patterns involving generic generalizations\n(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.\nGenerics are of special interest to linguists, philosophers, logicians, and\ncognitive scientists because of their complex exception-permitting behaviour\nand their centrality to default reasoning, cognition, and concept acquisition.\nWe find that while several frontier models handle many default reasoning\nproblems well, performance varies widely across models and prompting styles.\nFew-shot prompting modestly improves performance for some models, but\nchain-of-thought (CoT) prompting often leads to serious performance degradation\n(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy\nin zero-shot condition, temperature 0). Most models either struggle to\ndistinguish between defeasible and deductive inference or misinterpret generics\nas universal statements. These findings underscore both the promise and limits\nof current LLMs for default reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2508.13718v1",
      "published": "2025-08-19T10:28:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13718v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "The AI Risk Spectrum: From Dangerous Capabilities to Existential Threats",
      "authors": [
        "Markov Grey",
        "Charbel-Raphaël Segerie"
      ],
      "abstract": "As AI systems become more capable, integrated, and widespread, understanding\nthe associated risks becomes increasingly important. This paper maps the full\nspectrum of AI risks, from current harms affecting individual users to\nexistential threats that could endanger humanity's survival. We organize these\nrisks into three main causal categories. Misuse risks, which occur when people\ndeliberately use AI for harmful purposes - creating bioweapons, launching\ncyberattacks, adversarial AI attacks or deploying lethal autonomous weapons.\nMisalignment risks happen when AI systems pursue outcomes that conflict with\nhuman values, irrespective of developer intentions. This includes risks arising\nthrough specification gaming (reward hacking), scheming and power-seeking\ntendencies in pursuit of long-term strategic goals. Systemic risks, which arise\nwhen AI integrates into complex social systems in ways that gradually undermine\nhuman agency - concentrating power, accelerating political and economic\ndisempowerment, creating overdependence that leads to human enfeeblement, or\nirreversibly locking in current values curtailing future moral progress. Beyond\nthese core categories, we identify risk amplifiers - competitive pressures,\naccidents, corporate indifference, and coordination failures - that make all\nrisks more likely and severe. Throughout, we connect today's existing risks and\nempirically observable AI behaviors to plausible future outcomes, demonstrating\nhow existing trends could escalate to catastrophic outcomes. Our goal is to\nhelp readers understand the complete landscape of AI risks. Good futures are\npossible, but they don't happen by default. Navigating these challenges will\nrequire unprecedented coordination, but an extraordinary future awaits if we\ndo.",
      "pdf_url": "http://arxiv.org/pdf/2508.13700v1",
      "published": "2025-08-19T10:05:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13700v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "The DeepLog Neurosymbolic Machine",
      "authors": [
        "Vincent Derkinderen",
        "Robin Manhaeve",
        "Rik Adriaensen",
        "Lucas Van Praet",
        "Lennert De Smet",
        "Giuseppe Marra",
        "Luc De Raedt"
      ],
      "abstract": "We contribute a theoretical and operational framework for neurosymbolic AI\ncalled DeepLog. DeepLog introduces building blocks and primitives for\nneurosymbolic AI that make abstraction of commonly used representations and\ncomputational mechanisms used in neurosymbolic AI. DeepLog can represent and\nemulate a wide range of neurosymbolic systems. It consists of two key\ncomponents. The first is the DeepLog language for specifying neurosymbolic\nmodels and inference tasks. This language consists of an annotated neural\nextension of grounded first-order logic, and makes abstraction of the type of\nlogic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the\narchitecture or in the loss function. The second DeepLog component is situated\nat the computational level and uses extended algebraic circuits as\ncomputational graphs. Together these two components are to be considered as a\nneurosymbolic abstract machine, with the DeepLog language as the intermediate\nlevel of abstraction and the circuits level as the computational one. DeepLog\nis implemented in software, relies on the latest insights in implementing\nalgebraic circuits on GPUs, and is declarative in that it is easy to obtain\ndifferent neurosymbolic models by making different choices for the underlying\nalgebraic structures and logics. The generality and efficiency of the DeepLog\nneurosymbolic machine is demonstrated through an experimental comparison\nbetween 1) different fuzzy and probabilistic logics, 2) between using logic in\nthe architecture or in the loss function, and 3) between a standalone CPU-based\nimplementation of a neurosymbolic AI system and a DeepLog GPU-based one.",
      "pdf_url": "http://arxiv.org/pdf/2508.13697v1",
      "published": "2025-08-19T09:58:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13697v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models",
      "authors": [
        "Xiao-Wen Yang",
        "Jie-Jing Shao",
        "Lan-Zhe Guo",
        "Bo-Wen Zhang",
        "Zhi Zhou",
        "Lin-Han Jia",
        "Wang-Zhou Dai",
        "Yu-Feng Li"
      ],
      "abstract": "Large Language Models (LLMs) have shown promising results across various\ntasks, yet their reasoning capabilities remain a fundamental challenge.\nDeveloping AI systems with strong reasoning capabilities is regarded as a\ncrucial milestone in the pursuit of Artificial General Intelligence (AGI) and\nhas garnered considerable attention from both academia and industry. Various\ntechniques have been explored to enhance the reasoning capabilities of LLMs,\nwith neuro-symbolic approaches being a particularly promising way. This paper\ncomprehensively reviews recent developments in neuro-symbolic approaches for\nenhancing LLM reasoning. We first present a formalization of reasoning tasks\nand give a brief introduction to the neurosymbolic learning paradigm. Then, we\ndiscuss neuro-symbolic methods for improving the reasoning capabilities of LLMs\nfrom three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.\nFinally, we discuss several key challenges and promising future directions. We\nhave also released a GitHub repository including papers and resources related\nto this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.",
      "pdf_url": "http://arxiv.org/pdf/2508.13678v1",
      "published": "2025-08-19T09:27:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13678v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model",
      "authors": [
        "Yu Li",
        "Zulong Chen",
        "Wenjian Xu",
        "Hong Wen",
        "Yipeng Yu",
        "Man Lung Yiu",
        "Yuyu Yin"
      ],
      "abstract": "To maintain the company's talent pool, recruiters need to continuously search\nfor resumes from third-party websites (e.g., LinkedIn, Indeed). However,\nfetched resumes are often incomplete and inaccurate. To improve the quality of\nthird-party resumes and enrich the company's talent pool, it is essential to\nconduct duplication detection between the fetched resumes and those already in\nthe company's talent pool. Such duplication detection is challenging due to the\nsemantic complexity, structural heterogeneity, and information incompleteness\nof resume texts. To this end, we propose MHSNet, an multi-level identity\nverification framework that fine-tunes BGE-M3 using contrastive learning. With\nthe fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and\ndense representations for resumes, enabling the computation of corresponding\nmulti-level semantic similarities. Moreover, the state-aware Mixture-of-Experts\n(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental\nresults verify the effectiveness of MHSNet",
      "pdf_url": "http://arxiv.org/pdf/2508.13676v1",
      "published": "2025-08-19T09:27:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.13676v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}