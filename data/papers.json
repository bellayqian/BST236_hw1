{
  "last_updated": "2025-02-19T00:43:54.001410",
  "papers": [
    {
      "title": "Diffusion Models without Classifier-free Guidance",
      "authors": [
        "Zhicong Tang",
        "Jianmin Bao",
        "Dong Chen",
        "Baining Guo"
      ],
      "abstract": "This paper presents Model-guidance (MG), a novel objective for training\ndiffusion model that addresses and removes of the commonly used Classifier-free\nguidance (CFG). Our innovative approach transcends the standard modeling of\nsolely data distribution to incorporating the posterior probability of\nconditions. The proposed technique originates from the idea of CFG and is easy\nyet effective, making it a plug-and-play module for existing models. Our method\nsignificantly accelerates the training process, doubles the inference speed,\nand achieve exceptional quality that parallel and even surpass concurrent\ndiffusion models with CFG. Extensive experiments demonstrate the effectiveness,\nefficiency, scalability on different models and datasets. Finally, we establish\nstate-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.\nOur code is available at https://github.com/tzco/Diffusion-wo-CFG.",
      "pdf_url": "http://arxiv.org/pdf/2502.12154v1",
      "published": "2025-02-17T18:59:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12154v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "HARBOR: Exploring Persona Dynamics in Multi-Agent Competition",
      "authors": [
        "Kenan Jiang",
        "Li Xiong",
        "Fei Liu"
      ],
      "abstract": "We investigate factors contributing to LLM agents' success in competitive\nmulti-agent environments, using auctions as a testbed where agents bid to\nmaximize profit. The agents are equipped with bidding domain knowledge,\ndistinct personas that reflect item preferences, and a memory of auction\nhistory. Our work extends the classic auction scenario by creating a realistic\nenvironment where multiple agents bid on houses, weighing aspects such as size,\nlocation, and budget to secure the most desirable homes at the lowest prices.\nParticularly, we investigate three key questions: (a) How does a persona\ninfluence an agent's behavior in a competitive setting? (b) Can an agent\neffectively profile its competitors' behavior during auctions? (c) How can\npersona profiling be leveraged to create an advantage using strategies such as\ntheory of mind? Through a series of experiments, we analyze the behaviors of\nLLM agents and shed light on new findings. Our testbed, called HARBOR, offers a\nvaluable platform for deepening our understanding of multi-agent workflows in\ncompetitive environments.",
      "pdf_url": "http://arxiv.org/pdf/2502.12149v1",
      "published": "2025-02-17T18:58:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12149v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control",
      "authors": [
        "Jinyan Su",
        "Jennifer Healey",
        "Preslav Nakov",
        "Claire Cardie"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nmitigate large language model (LLM) hallucinations by incorporating external\nknowledge retrieval. However, existing RAG frameworks often apply retrieval\nindiscriminately,leading to inefficiencies-over-retrieving when unnecessary or\nfailing to retrieve iteratively when required for complex reasoning. Recent\nadaptive retrieval strategies, though adaptively navigates these retrieval\nstrategies, predict only based on query complexity and lacks user-driven\nflexibility, making them infeasible for diverse user application needs. In this\npaper, we introduce a novel user-controllable RAG framework that enables\ndynamic adjustment of the accuracy-cost trade-off. Our approach leverages two\nclassifiers: one trained to prioritize accuracy and another to prioritize\nretrieval efficiency. Via an interpretable control parameter $\\alpha$, users\ncan seamlessly navigate between minimal-cost retrieval and high-accuracy\nretrieval based on their specific requirements. We empirically demonstrate that\nour approach effectively balances accuracy, retrieval cost, and user\ncontrollability, making it a practical and adaptable solution for real-world\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2502.12145v1",
      "published": "2025-02-17T18:56:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12145v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Small Models Struggle to Learn from Strong Reasoners",
      "authors": [
        "Yuetai Li",
        "Xiang Yue",
        "Zhangchen Xu",
        "Fengqing Jiang",
        "Luyao Niu",
        "Bill Yuchen Lin",
        "Bhaskar Ramasubramanian",
        "Radha Poovendran"
      ],
      "abstract": "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
      "pdf_url": "http://arxiv.org/pdf/2502.12143v1",
      "published": "2025-02-17T18:56:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12143v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Transformer Dynamics: A neuroscientific approach to interpretability of large language models",
      "authors": [
        "Jesseba Fernando",
        "Grigori Guitchounts"
      ],
      "abstract": "As artificial intelligence models have exploded in scale and capability,\nunderstanding of their internal mechanisms remains a critical challenge.\nInspired by the success of dynamical systems approaches in neuroscience, here\nwe propose a novel framework for studying computations in deep learning\nsystems. We focus on the residual stream (RS) in transformer models,\nconceptualizing it as a dynamical system evolving across layers. We find that\nactivations of individual RS units exhibit strong continuity across layers,\ndespite the RS being a non-privileged basis. Activations in the RS accelerate\nand grow denser over layers, while individual units trace unstable periodic\norbits. In reduced-dimensional spaces, the RS follows a curved trajectory with\nattractor-like dynamics in the lower layers. These insights bridge dynamical\nsystems theory and mechanistic interpretability, establishing a foundation for\na \"neuroscience of AI\" that combines theoretical rigor with large-scale data\nanalysis to advance our understanding of modern neural networks.",
      "pdf_url": "http://arxiv.org/pdf/2502.12131v1",
      "published": "2025-02-17T18:49:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12131v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Scaling Autonomous Agents via Automatic Reward Modeling And Planning",
      "authors": [
        "Zhenfang Chen",
        "Delin Chen",
        "Rui Sun",
        "Wenjun Liu",
        "Chuang Gan"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of text-generation tasks. However, LLMs still struggle with problems\nrequiring multi-step decision-making and environmental feedback, such as online\nshopping, scientific reasoning, and mathematical problem-solving. Unlike pure\ntext data, collecting large-scale decision-making data is challenging.\nMoreover, many powerful LLMs are only accessible through APIs, which hinders\ntheir fine-tuning for agent tasks due to cost and complexity. To address LLM\nagents' limitations, we propose a framework that can automatically learn a\nreward model from the environment without human annotations. This model can be\nused to evaluate the action trajectories of LLM agents and provide heuristics\nfor task planning. Specifically, our approach involves employing one LLM-based\nagent to navigate an environment randomly, generating diverse action\ntrajectories. Subsequently, a separate LLM is leveraged to assign a task intent\nand synthesize a negative response alongside the correct response for each\ntrajectory. These triplets (task intent, positive response, and negative\nresponse) are then utilized as training data to optimize a reward model capable\nof scoring action trajectories. The effectiveness and generalizability of our\nframework are demonstrated through evaluations conducted on different agent\nbenchmarks. In conclusion, our proposed framework represents a significant\nadvancement in enhancing LLM agents' decision-making capabilities. By\nautomating the learning of reward models, we overcome the challenges of data\nscarcity and API limitations, potentially revolutionizing the application of\nLLMs in complex and interactive environments. This research paves the way for\nmore sophisticated AI agents capable of tackling a wide range of real-world\nproblems requiring multi-step decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2502.12130v1",
      "published": "2025-02-17T18:49:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12130v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities",
      "authors": [
        "Florian Sestak",
        "Artur Toshev",
        "Andreas Fürst",
        "Günter Klambauer",
        "Andreas Mayr",
        "Johannes Brandstetter"
      ],
      "abstract": "Generative models are spearheading recent progress in deep learning, showing\nstrong promise for trajectory sampling in dynamical systems as well. However,\nwhile latent space modeling paradigms have transformed image and video\ngeneration, similar approaches are more difficult for most dynamical systems.\nSuch systems -- from chemical molecule structures to collective human behavior\n-- are described by interactions of entities, making them inherently linked to\nconnectivity patterns and the traceability of entities over time. Our approach,\nLaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked\nEntities), combines the advantages of graph neural networks, i.e., the\ntraceability of entities across time-steps, with the efficiency and scalability\nof recent advances in image and video generation, where pre-trained encoder and\ndecoder are frozen to enable generative modeling in the latent space. The core\nidea of LaM-SLidE is to introduce identifier representations (IDs) to allow for\nretrieval of entity properties, e.g., entity coordinates, from latent system\nrepresentations and thus enables traceability. Experimentally, across different\ndomains, we show that LaM-SLidE performs favorably in terms of speed, accuracy,\nand generalizability. (Code is available at\nhttps://github.com/ml-jku/LaM-SLidE)",
      "pdf_url": "http://arxiv.org/pdf/2502.12128v1",
      "published": "2025-02-17T18:49:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12128v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy",
      "authors": [
        "Roman Malashin",
        "Valeria Yachnaya",
        "Alexander Mullin"
      ],
      "abstract": "We investigate the training dynamics of deep classifiers by examining how\nhierarchical relationships between classes evolve during training. Through\nextensive experiments, we argue that the learning process in classification\nproblems can be understood through the lens of label clustering. Specifically,\nwe observe that networks tend to distinguish higher-level (hypernym) categories\nin the early stages of training, and learn more specific (hyponym) categories\nlater. We introduce a novel framework to track the evolution of the feature\nmanifold during training, revealing how the hierarchy of class relations\nemerges and refines across the network layers. Our analysis demonstrates that\nthe learned representations closely align with the semantic structure of the\ndataset, providing a quantitative description of the clustering process.\nNotably, we show that in the hypernym label space, certain properties of neural\ncollapse appear earlier than in the hyponym label space, helping to bridge the\ngap between the initial and terminal phases of learning. We believe our\nfindings offer new insights into the mechanisms driving hierarchical learning\nin deep networks, paving the way for future advancements in understanding deep\nlearning dynamics.",
      "pdf_url": "http://arxiv.org/pdf/2502.12125v1",
      "published": "2025-02-17T18:47:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12125v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
      "authors": [
        "Prasanna Mayilvahanan",
        "Thaddäus Wiedemer",
        "Sayak Mallick",
        "Matthias Bethge",
        "Wieland Brendel"
      ],
      "abstract": "Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2502.12120v1",
      "published": "2025-02-17T18:45:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12120v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection",
      "authors": [
        "Jinhe Bi",
        "Yifan Wang",
        "Danqi Yan",
        "Xun Xiao",
        "Artur Hecker",
        "Volker Tresp",
        "Yunpu Ma"
      ],
      "abstract": "Visual instruction tuning refines pre-trained Multimodal Large Language\nModels (MLLMs) to enhance their real-world task performance. However, the rapid\nexpansion of visual instruction datasets introduces significant data\nredundancy, leading to excessive computational costs. Existing data selection\nmethods predominantly rely on proxy models or loss-based metrics, both of which\nimpose substantial computational overheads due to the necessity of model\ninference and backpropagation. To address this challenge, we propose PRISM, a\nnovel training-free approach for efficient multimodal data selection. Unlike\nexisting methods, PRISM eliminates the reliance on proxy models, warm-up\npretraining, and gradient-based optimization. Instead, it leverages Pearson\ncorrelation analysis to quantify the intrinsic visual encoding properties of\nMLLMs, computing a task-specific correlation score to identify high-value\ninstances. This not only enbles data-efficient selection,but maintains the\noriginal performance. Empirical evaluations across multiple MLLMs demonstrate\nthat PRISM reduces the overall time required for visual instruction tuning and\ndata selection to just 30% of conventional methods, while surpassing fully\nfine-tuned models across eight multimodal and three language understanding\nbenchmarks, achieving a 101.7% relative improvement in final performance.",
      "pdf_url": "http://arxiv.org/pdf/2502.12119v1",
      "published": "2025-02-17T18:43:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12119v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Personality Structured Interview for Large Language Model Simulation in Personality Research",
      "authors": [
        "Pengda Wang",
        "Huiqi Zou",
        "Hanjie Chen",
        "Tianjun Sun",
        "Ziang Xiao",
        "Frederick L. Oswald"
      ],
      "abstract": "Although psychometrics researchers have recently explored the use of large\nlanguage models (LLMs) as proxies for human participants, LLMs often fail to\ngenerate heterogeneous data with human-like diversity, which diminishes their\nvalue in advancing social science research. To address these challenges, we\nexplored the potential of the theory-informed Personality Structured Interview\n(PSI) as a tool for simulating human responses in personality research. In this\napproach, the simulation is grounded in nuanced real-human interview\ntranscripts that target the personality construct of interest. We have provided\na growing set of 357 structured interview transcripts from a representative\nsample, each containing an individual's response to 32 open-ended questions\ncarefully designed to gather theory-based personality evidence. Additionally,\ngrounded in psychometric research, we have summarized an evaluation framework\nto systematically validate LLM-generated psychometric data. Results from three\nexperiments demonstrate that well-designed structured interviews could improve\nhuman-like heterogeneity in LLM-simulated personality data and predict\npersonality-related behavioral outcomes (i.e., organizational citizenship\nbehaviors and counterproductive work behavior). We further discuss the role of\ntheory-informed structured interviews in LLM-based simulation and outline a\ngeneral framework for designing structured interviews to simulate human-like\ndata for psychometric research.",
      "pdf_url": "http://arxiv.org/pdf/2502.12109v1",
      "published": "2025-02-17T18:31:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12109v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Using the Path of Least Resistance to Explain Deep Networks",
      "authors": [
        "Sina Salek",
        "Joseph Enguehard"
      ],
      "abstract": "Integrated Gradients (IG), a widely used axiomatic path-based attribution\nmethod, assigns importance scores to input features by integrating model\ngradients along a straight path from a baseline to the input. While effective\nin some cases, we show that straight paths can lead to flawed attributions. In\nthis paper, we identify the cause of these misattributions and propose an\nalternative approach that treats the input space as a Riemannian manifold,\ncomputing attributions by integrating gradients along geodesics. We call this\nmethod Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we\nintroduce two techniques: a k-Nearest Neighbours-based approach for smaller\nmodels and a Stochastic Variational Inference-based method for larger ones.\nAdditionally, we propose a new axiom, Strong Completeness, extending the axioms\nsatisfied by IG. We show that this property is desirable for attribution\nmethods and that GIG is the only method that satisfies it. Through experiments\non both synthetic and real-world data, we demonstrate that GIG outperforms\nexisting explainability methods, including IG.",
      "pdf_url": "http://arxiv.org/pdf/2502.12108v1",
      "published": "2025-02-17T18:29:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12108v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Relational Norms for Human-AI Cooperation",
      "authors": [
        "Brian D. Earp",
        "Sebastian Porsdam Mann",
        "Mateo Aboy",
        "Edmond Awad",
        "Monika Betzler",
        "Marietjie Botes",
        "Rachel Calcott",
        "Mina Caraccio",
        "Nick Chater",
        "Mark Coeckelbergh",
        "Mihaela Constantinescu",
        "Hossein Dabbagh",
        "Kate Devlin",
        "Xiaojun Ding",
        "Vilius Dranseika",
        "Jim A. C. Everett",
        "Ruiping Fan",
        "Faisal Feroz",
        "Kathryn B. Francis",
        "Cindy Friedman",
        "Orsolya Friedrich",
        "Iason Gabriel",
        "Ivar Hannikainen",
        "Julie Hellmann",
        "Arasj Khodadade Jahrome",
        "Niranjan S. Janardhanan",
        "Paul Jurcys",
        "Andreas Kappes",
        "Maryam Ali Khan",
        "Gordon Kraft-Todd",
        "Maximilian Kroner Dale",
        "Simon M. Laham",
        "Benjamin Lange",
        "Muriel Leuenberger",
        "Jonathan Lewis",
        "Peng Liu",
        "David M. Lyreskog",
        "Matthijs Maas",
        "John McMillan",
        "Emilian Mihailov",
        "Timo Minssen",
        "Joshua Teperowski Monrad",
        "Kathryn Muyskens",
        "Simon Myers",
        "Sven Nyholm",
        "Alexa M. Owen",
        "Anna Puzio",
        "Christopher Register",
        "Madeline G. Reinecke",
        "Adam Safron",
        "Henry Shevlin",
        "Hayate Shimizu",
        "Peter V. Treit",
        "Cristina Voinea",
        "Karen Yan",
        "Anda Zahiu",
        "Renwen Zhang",
        "Hazem Zohny",
        "Walter Sinnott-Armstrong",
        "Ilina Singh",
        "Julian Savulescu",
        "Margaret S. Clark"
      ],
      "abstract": "How we should design and interact with social artificial intelligence depends\non the socio-relational role the AI is meant to emulate or occupy. In human\nsociety, relationships such as teacher-student, parent-child, neighbors,\nsiblings, or employer-employee are governed by specific norms that prescribe or\nproscribe cooperative functions including hierarchy, care, transaction, and\nmating. These norms shape our judgments of what is appropriate for each\npartner. For example, workplace norms may allow a boss to give orders to an\nemployee, but not vice versa, reflecting hierarchical and transactional\nexpectations. As AI agents and chatbots powered by large language models are\nincreasingly designed to serve roles analogous to human positions - such as\nassistant, mental health provider, tutor, or romantic partner - it is\nimperative to examine whether and how human relational norms should extend to\nhuman-AI interactions. Our analysis explores how differences between AI systems\nand humans, such as the absence of conscious experience and immunity to\nfatigue, may affect an AI's capacity to fulfill relationship-specific functions\nand adhere to corresponding norms. This analysis, which is a collaborative\neffort by philosophers, psychologists, relationship scientists, ethicists,\nlegal experts, and AI researchers, carries important implications for AI\nsystems design, user behavior, and regulation. While we accept that AI systems\ncan offer significant benefits such as increased availability and consistency\nin certain socio-relational roles, they also risk fostering unhealthy\ndependencies or unrealistic expectations that could spill over into human-human\nrelationships. We propose that understanding and thoughtfully shaping (or\nimplementing) suitable human-AI relational norms will be crucial for ensuring\nthat human-AI interactions are ethical, trustworthy, and favorable to human\nwell-being.",
      "pdf_url": "http://arxiv.org/pdf/2502.12102v1",
      "published": "2025-02-17T18:23:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12102v1",
      "categories": [
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "A Study on Leveraging Search and Self-Feedback for Agent Reasoning",
      "authors": [
        "Karthikeyan K",
        "Michelle Yuan",
        "Elman Mansimov",
        "Katerina Margatina",
        "Anurag Pratik",
        "Daniele Bonadiman",
        "Monica Sunkara",
        "Yi Zhang",
        "Yassine Benajiba"
      ],
      "abstract": "Recent works have demonstrated that incorporating search during inference can\nsignificantly improve reasoning capabilities of language agents. Some\napproaches may make use of the ground truth or rely on model's own generated\nfeedback. The search algorithm uses this feedback to then produce values that\nwill update its criterion for exploring and exploiting various reasoning paths.\nIn this study, we investigate how search and model's self-feedback can be\nleveraged for reasoning tasks. First, we explore differences in ground-truth\nfeedback and self-feedback during search for math reasoning. Second, we observe\nlimitations in applying search techniques to more complex tasks like\ntool-calling and design domain-specific approaches to address these gaps. Our\nexperiments reveal challenges related to generalization when solely relying on\nself-feedback during search. For search to work effectively, either access to\nthe ground-truth is needed or feedback mechanisms need to be carefully designed\nfor the specific task.",
      "pdf_url": "http://arxiv.org/pdf/2502.12094v1",
      "published": "2025-02-17T18:12:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12094v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Meta-Statistical Learning: Supervised Learning of Statistical Inference",
      "authors": [
        "Maxime Peyrard",
        "Kyunghyun Cho"
      ],
      "abstract": "This work demonstrates that the tools and principles driving the success of\nlarge language models (LLMs) can be repurposed to tackle distribution-level\ntasks, where the goal is to predict properties of the data-generating\ndistribution rather than labels for individual datapoints. These tasks\nencompass statistical inference problems such as parameter estimation,\nhypothesis testing, or mutual information estimation. Framing these tasks\nwithin traditional machine learning pipelines is challenging, as supervision is\ntypically tied to individual datapoint. We propose meta-statistical learning, a\nframework inspired by multi-instance learning that reformulates statistical\ninference tasks as supervised learning problems. In this approach, entire\ndatasets are treated as single inputs to neural networks, which predict\ndistribution-level parameters. Transformer-based architectures, without\npositional encoding, provide a natural fit due to their permutation-invariance\nproperties. By training on large-scale synthetic datasets, meta-statistical\nmodels can leverage the scalability and optimization infrastructure of\nTransformer-based LLMs. We demonstrate the framework's versatility with\napplications in hypothesis testing and mutual information estimation, showing\nstrong performance, particularly for small datasets where traditional neural\nmethods struggle.",
      "pdf_url": "http://arxiv.org/pdf/2502.12088v1",
      "published": "2025-02-17T18:04:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12088v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
      "authors": [
        "Heming Xia",
        "Yongqi Li",
        "Chak Tou Leong",
        "Wenjie Wang",
        "Wenjie Li"
      ],
      "abstract": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.",
      "pdf_url": "http://arxiv.org/pdf/2502.12067v1",
      "published": "2025-02-17T17:37:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12067v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication Facilities with Large Language Models",
      "authors": [
        "Yifan Zhang",
        "Xue Yang"
      ],
      "abstract": "Automating planning with LLMs presents transformative opportunities for\ntraditional industries, yet remains underexplored. In commercial construction,\nthe complexity of automated scheduling often requires manual intervention to\nensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to\noptimize construction schedules in complex projects like semiconductor\nfabrication. CONSTRUCTA addresses key challenges by: (1) integrating\nconstruction-specific knowledge through static RAG; (2) employing\ncontext-sampling techniques inspired by architectural expertise to provide\nrelevant input; and (3) deploying Construction DPO to align schedules with\nexpert preferences using RLHF. Experiments on proprietary data demonstrate\nperformance improvements of +42.3% in missing value prediction, +79.1% in\ndependency analysis, and +28.9% in automated planning compared to baseline\nmethods, showcasing its potential to revolutionize construction workflows and\ninspire domain-specific LLM advancements.",
      "pdf_url": "http://arxiv.org/pdf/2502.12066v1",
      "published": "2025-02-17T17:35:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12066v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "title": "AI-generated Text Detection with a GLTR-based Approach",
      "authors": [
        "Lucía Yan Wu",
        "Isabel Segura-Bedmar"
      ],
      "abstract": "The rise of LLMs (Large Language Models) has contributed to the improved\nperformance and development of cutting-edge NLP applications. However, these\ncan also pose risks when used maliciously, such as spreading fake news, harmful\ncontent, impersonating individuals, or facilitating school plagiarism, among\nothers. This is because LLMs can generate high-quality texts, which are\nchallenging to differentiate from those written by humans. GLTR, which stands\nfor Giant Language Model Test Room and was developed jointly by the MIT-IBM\nWatson AI Lab and HarvardNLP, is a visual tool designed to help detect\nmachine-generated texts based on GPT-2, that highlights the words in text\ndepending on the probability that they were machine-generated. One limitation\nof GLTR is that the results it returns can sometimes be ambiguous and lead to\nconfusion. This study aims to explore various ways to improve GLTR's\neffectiveness for detecting AI-generated texts within the context of the\nIberLef-AuTexTification 2023 shared task, in both English and Spanish\nlanguages. Experiment results show that our GLTR-based GPT-2 model overcomes\nthe state-of-the-art models on the English dataset with a macro F1-score of\n80.19%, except for the first ranking model (80.91%). However, for the Spanish\ndataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%\ncompared to the top-performing model.",
      "pdf_url": "http://arxiv.org/pdf/2502.12064v1",
      "published": "2025-02-17T17:32:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12064v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
      "authors": [
        "Xinyu Zhang",
        "Yuxuan Dong",
        "Yanrui Wu",
        "Jiaxing Huang",
        "Chengyou Jia",
        "Basura Fernando",
        "Mike Zheng Shou",
        "Lingling Zhang",
        "Jun Liu"
      ],
      "abstract": "Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:/dxzxy12138.github.io/PhysReason.",
      "pdf_url": "http://arxiv.org/pdf/2502.12054v1",
      "published": "2025-02-17T17:24:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12054v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond",
      "authors": [
        "Shreya Shukla",
        "Jose Torres",
        "Abhijit Mishra",
        "Jacek Gwizdka",
        "Shounak Roychowdhury"
      ],
      "abstract": "Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial\nIntelligence (GenAI) has opened new frontiers in brain signal decoding,\nenabling assistive communication, neural representation learning, and\nmultimodal integration. BCIs, particularly those leveraging\nElectroencephalography (EEG), provide a non-invasive means of translating\nneural activity into meaningful outputs. Recent advances in deep learning,\nincluding Generative Adversarial Networks (GANs) and Transformer-based Large\nLanguage Models (LLMs), have significantly improved EEG-based generation of\nimages, text, and speech. This paper provides a literature review of the\nstate-of-the-art in EEG-based multimodal generation, focusing on (i)\nEEG-to-image generation through GANs, Variational Autoencoders (VAEs), and\nDiffusion Models, and (ii) EEG-to-text generation leveraging Transformer based\nlanguage models and contrastive learning methods. Additionally, we discuss the\nemerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We\nhighlight key datasets, use cases, challenges, and EEG feature encoding methods\nthat underpin generative approaches. By providing a structured overview of\nEEG-based generative AI, this survey aims to equip researchers and\npractitioners with insights to advance neural decoding, enhance assistive\ntechnologies, and expand the frontiers of brain-computer interaction.",
      "pdf_url": "http://arxiv.org/pdf/2502.12048v1",
      "published": "2025-02-17T17:16:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12048v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Masked Latent Prediction and Classification for Self-Supervised Audio Representation Learning",
      "authors": [
        "Aurian Quelennec",
        "Pierre Chouteau",
        "Geoffroy Peeters",
        "Slim Essid"
      ],
      "abstract": "Recently, self-supervised learning methods based on masked latent prediction\nhave proven to encode input data into powerful representations. However, during\ntraining, the learned latent space can be further transformed to extract\nhigher-level information that could be more suited for downstream\nclassification tasks. Therefore, we propose a new method: MAsked latenT\nPrediction And Classification (MATPAC), which is trained with two pretext tasks\nsolved jointly. As in previous work, the first pretext task is a masked latent\nprediction task, ensuring a robust input representation in the latent space.\nThe second one is unsupervised classification, which utilises the latent\nrepresentations of the first pretext task to match probability distributions\nbetween a teacher and a student. We validate the MATPAC method by comparing it\nto other state-of-the-art proposals and conducting ablations studies. MATPAC\nreaches state-of-the-art self-supervised learning results on reference audio\nclassification datasets such as OpenMIC, GTZAN, ESC-50 and US8K and outperforms\ncomparable supervised methods results for musical auto-tagging on\nMagna-tag-a-tune.",
      "pdf_url": "http://arxiv.org/pdf/2502.12031v1",
      "published": "2025-02-17T17:02:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12031v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths over Knowledge Graphs",
      "authors": [
        "Qi Zhao",
        "Hongyu Yang",
        "Qi Song",
        "Xinwei Yao",
        "Xiangyang Li"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath.",
      "pdf_url": "http://arxiv.org/pdf/2502.12029v1",
      "published": "2025-02-17T17:02:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12029v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities",
      "authors": [
        "Fengqing Jiang",
        "Zhangchen Xu",
        "Yuetai Li",
        "Luyao Niu",
        "Zhen Xiang",
        "Bo Li",
        "Bill Yuchen Lin",
        "Radha Poovendran"
      ],
      "abstract": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage\nlong chain-of-thought (CoT) reasoning to generate structured intermediate\nsteps, enhancing their reasoning capabilities. However, long CoT does not\ninherently guarantee safe outputs, potentially leading to harmful consequences\nsuch as the introduction of security vulnerabilities in code or the spread of\nmisinformation. Current research on large language model (LLM) safety usually\nfocuses on short-answer responses, overlooking the long CoT style outputs of\nLRMs. To bridge this gap, we conduct a systematic study of LRM safety. First,\nwe investigate safety evaluators calibrated against human annotations. Using\nour newly developed metrics, we thoroughly assess the safety of 12\nstate-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results\nshow that LRMs are not safe compared to their reasoning advance. Further, we\nperform a fine-grained analysis of the reasoning trace and final answer. We\nfind that three decoding strategies-ZeroThink, LessThink, and MoreThink-can\nimprove model safety without additional training. However, these strategies\neither use constrained reasoning traces or incur high inference costs. To\nbetter strengthen LRM safety, we introduce SafeChain, the first-of-its-kind\nsafety training dataset in CoT style. We fine-tune two LRMs with SafeChain,\nshowing that it not only enhances model safety but also preserves performance\nacross 6 reasoning benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2502.12025v1",
      "published": "2025-02-17T16:57:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12025v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving",
      "authors": [
        "Xin Xu",
        "Yan Xu",
        "Tianhao Chen",
        "Yuchen Yan",
        "Chengwu Liu",
        "Zaoyu Chen",
        "Yufei Wang",
        "Yichun Yin",
        "Yasheng Wang",
        "Lifeng Shang",
        "Qun Liu"
      ],
      "abstract": "Existing approaches to mathematical reasoning with large language models\n(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated\nReasoning (TIR) for precise computation. While efforts have been made to\ncombine these methods, they primarily rely on post-selection or predefined\nstrategies, leaving an open question: whether LLMs can autonomously adapt their\nreasoning strategy based on their inherent capabilities. In this work, we\npropose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework\nthat enables LLMs to personalize their reasoning strategy spontaneously,\naligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware\ndata selection during supervised fine-tuning (SFT) to tailor training data to\nthe model's unique abilities. This approach equips LLMs to autonomously\ndetermine and apply the appropriate reasoning strategy at test time. We\nevaluate TATA through extensive experiments on six mathematical reasoning\nbenchmarks, using both general-purpose and math-specialized LLMs. Empirical\nresults demonstrate that TATA effectively combines the complementary strengths\nof CoT and TIR, achieving superior or comparable performance with improved\ninference efficiency compared to TIR alone. Further analysis underscores the\ncritical role of aptitude-aware data selection in enabling LLMs to make\neffective and adaptive reasoning decisions and align reasoning strategies with\nmodel capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2502.12022v1",
      "published": "2025-02-17T16:56:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12022v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
      "authors": [
        "Fengwei Teng",
        "Zhaoyang Yu",
        "Quan Shi",
        "Jiayi Zhang",
        "Chenglin Wu",
        "Yuyu Luo"
      ],
      "abstract": "Large Language Models (LLMs) achieve superior performance through\ntraining-time scaling, and test-time scaling further enhances their\ncapabilities by conducting effective reasoning during inference. However, as\nthe scale of reasoning increases, existing test-time scaling methods suffer\nfrom accumulated historical information, which not only wastes computational\nresources but also interferes with effective reasoning. To address this issue,\nwe observe that complex reasoning progress is often achieved by solving a\nsequence of independent subquestions, each being self-contained and verifiable.\nThese subquestions are essentially atomic questions, relying primarily on their\ncurrent state rather than accumulated history, similar to the memoryless\ntransitions in a Markov process. Based on this observation, we propose Atom of\nThoughts (AoT), where each state transition in the reasoning process consists\nof decomposing the current question into a dependency-based directed acyclic\ngraph and contracting its subquestions, forming a new atomic question state.\nThis iterative decomposition-contraction process continues until reaching\ndirectly solvable atomic questions, naturally realizing Markov transitions\nbetween question states. Furthermore, these atomic questions can be seamlessly\nintegrated into existing test-time scaling methods, enabling AoT to serve as a\nplug-in enhancement for improving reasoning capabilities. Experiments across\nsix benchmarks demonstrate the effectiveness of AoT both as a standalone\nframework and a plug-in enhancement. Notably, on HotpotQA, when applied to\ngpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and\nDeepSeek-R1 by 10.6%. The code will be available at\nhttps://github.com/qixucen/atom.",
      "pdf_url": "http://arxiv.org/pdf/2502.12018v1",
      "published": "2025-02-17T16:52:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12018v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Demographic Attributes Prediction from Speech Using WavLM Embeddings",
      "authors": [
        "Yuchen Yang",
        "Thomas Thebaud",
        "Najim Dehak"
      ],
      "abstract": "This paper introduces a general classifier based on WavLM features, to infer\ndemographic characteristics, such as age, gender, native language, education,\nand country, from speech. Demographic feature prediction plays a crucial role\nin applications like language learning, accessibility, and digital forensics,\nenabling more personalized and inclusive technologies. Leveraging pretrained\nmodels for embedding extraction, the proposed framework identifies key acoustic\nand linguistic fea-tures associated with demographic attributes, achieving a\nMean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy\nfor gender classification across various datasets. Our system improves upon\nexisting models by up to relative 30% in MAE and up to relative 10% in accuracy\nand F1 scores across tasks, leveraging a diverse range of datasets and large\npretrained models to ensure robustness and generalizability. This study offers\nnew insights into speaker diversity and provides a strong foundation for future\nresearch in speech-based demographic profiling.",
      "pdf_url": "http://arxiv.org/pdf/2502.12007v1",
      "published": "2025-02-17T16:43:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.12007v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Presumed Cultural Identity: How Names Shape LLM Responses",
      "authors": [
        "Siddhesh Pawar",
        "Arnav Arora",
        "Lucie-Aimée Kaffee",
        "Isabelle Augenstein"
      ],
      "abstract": "Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.",
      "pdf_url": "http://arxiv.org/pdf/2502.11995v1",
      "published": "2025-02-17T16:35:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11995v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ]
    },
    {
      "title": "Characterizing Photorealism and Artifacts in Diffusion Model-Generated Images",
      "authors": [
        "Negar Kamali",
        "Karyn Nakamura",
        "Aakriti Kumar",
        "Angelos Chatzimparmpas",
        "Jessica Hullman",
        "Matthew Groh"
      ],
      "abstract": "Diffusion model-generated images can appear indistinguishable from authentic\nphotographs, but these images often contain artifacts and implausibilities that\nreveal their AI-generated provenance. Given the challenge to public trust in\nmedia posed by photorealistic AI-generated images, we conducted a large-scale\nexperiment measuring human detection accuracy on 450 diffusion-model generated\nimages and 149 real images. Based on collecting 749,828 observations and 34,675\ncomments from 50,444 participants, we find that scene complexity of an image,\nartifact types within an image, display time of an image, and human curation of\nAI-generated images all play significant roles in how accurately people\ndistinguish real from AI-generated images. Additionally, we propose a taxonomy\ncharacterizing artifacts often appearing in images generated by diffusion\nmodels. Our empirical observations and taxonomy offer nuanced insights into the\ncapabilities and limitations of diffusion models to generate photorealistic\nimages in 2024.",
      "pdf_url": "http://arxiv.org/pdf/2502.11989v1",
      "published": "2025-02-17T16:28:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11989v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Machine Learning Should Maximize Welfare, Not (Only) Accuracy",
      "authors": [
        "Nir Rosenfeld",
        "Haifeng Xu"
      ],
      "abstract": "Decades of research in machine learning have given us powerful tools for\nmaking accurate predictions. But when used in social settings and on human\ninputs, better accuracy does not immediately translate to better social\noutcomes. This may not be surprising given that conventional learning\nframeworks are not designed to express societal preferences -- let alone\npromote them. This position paper argues that machine learning is currently\nmissing, and can gain much from incorporating, a proper notion of social\nwelfare. The field of welfare economics asks: how should we allocate limited\nresources to self-interested agents in a way that maximizes social benefit? We\nargue that this perspective applies to many modern applications of machine\nlearning in social contexts, and advocate for its adoption. Rather than\ndisposing of prediction, we aim to leverage this forte of machine learning for\npromoting social welfare. We demonstrate this idea by proposing a conceptual\nframework that gradually transitions from accuracy maximization (with awareness\nto welfare) to welfare maximization (via accurate prediction). We detail\napplications and use-cases for which our framework can be effective, identify\ntechnical challenges and practical opportunities, and highlight future avenues\nworth pursuing.",
      "pdf_url": "http://arxiv.org/pdf/2502.11981v1",
      "published": "2025-02-17T16:22:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11981v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Learning Generalizable Prompt for CLIP with Class Similarity Knowledge",
      "authors": [
        "Sehun Jung",
        "Hyang-won Lee"
      ],
      "abstract": "In vision-language models (VLMs), prompt tuning has shown its effectiveness\nin adapting models to downstream tasks. However, learned prompts struggle to\ngeneralize to unseen classes, as they tend to overfit to the classes that are\ntargeted during prompt tuning. Examining failure cases, we observed that\nlearned prompts disrupt the semantics of unseen classes, generating text\nembeddings with incorrect semantic relationships among classes. To address\nthis, we propose Similarity Alignment Regularization (SAR), which regularizes\nlearnable prompts to preserve the semantic relationships among classes captured\nby hand-crafted prompts. Specifically, we first obtain novel classes related to\nbase classes using ChatGPT-4o and utilize them as potential unseen classes\nduring prompt tuning. Then, by targeting both base and novel classes, SAR\naligns the similarity relationships among text embeddings generated by\nlearnable prompts with the similarity relationships from hand-crafted prompts.\nExtensive experiments applying SAR to existing prompt tuning methods\ndemonstrate its effectiveness in improving generalization to unseen classes.",
      "pdf_url": "http://arxiv.org/pdf/2502.11969v1",
      "published": "2025-02-17T16:18:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11969v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Theoretical Barriers in Bellman-Based Reinforcement Learning",
      "authors": [
        "Brieuc Pinon",
        "Raphaël Jungers",
        "Jean-Charles Delvenne"
      ],
      "abstract": "Reinforcement Learning algorithms designed for high-dimensional spaces often\nenforce the Bellman equation on a sampled subset of states, relying on\ngeneralization to propagate knowledge across the state space. In this paper, we\nidentify and formalize a fundamental limitation of this common approach.\nSpecifically, we construct counterexample problems with a simple structure that\nthis approach fails to exploit. Our findings reveal that such algorithms can\nneglect critical information about the problems, leading to inefficiencies.\nFurthermore, we extend this negative result to another approach from the\nliterature: Hindsight Experience Replay learning state-to-state reachability.",
      "pdf_url": "http://arxiv.org/pdf/2502.11968v1",
      "published": "2025-02-17T16:18:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11968v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A MIMO Wireless Channel Foundation Model via CIR-CSI Consistency",
      "authors": [
        "Jun Jiang",
        "Wenjun Yu",
        "Yunfan Li",
        "Yuan Gao",
        "Shugong Xu"
      ],
      "abstract": "In the field of artificial intelligence, self-supervised learning has\ndemonstrated superior generalization capabilities by leveraging large-scale\nunlabeled datasets for pretraining, which is especially critical for wireless\ncommunication models to adapt to a variety of scenarios. This paper\ninnovatively treats Channel State Information (CSI) and Channel Impulse\nResponse (CIR) as naturally aligned multi-modal data and proposes the first\nMIMO wireless channel foundation model, named CSI-CLIP. By effectively\ncapturing the joint representations of both CIR and CSI, CSI-CLIP exhibits\nremarkable adaptability across scenarios and robust feature extraction\ncapabilities. Experimental results show that in positioning task, CSI-CLIP\nreduces the mean error distance by 22%; in beam management task, it increases\naccuracy by 1% compared to traditional supervised methods, as well as in the\nchannel identification task. These improvements not only highlight the\npotential and value of CSI-CLIP in integrating sensing and communication but\nalso demonstrate its significant advantages over existing techniques. Moreover,\nviewing CSI and CIR as multi-modal pairs and contrastive learning for wireless\nchannel foundation model open up new research directions in the domain of MIMO\nwireless communications.",
      "pdf_url": "http://arxiv.org/pdf/2502.11965v1",
      "published": "2025-02-17T16:13:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11965v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning",
      "authors": [
        "Tianyi Wu",
        "Jingwei Ni",
        "Bryan Hooi",
        "Jiaheng Zhang",
        "Elliott Ash",
        "See-Kiong Ng",
        "Mrinmaya Sachan",
        "Markus Leippold"
      ],
      "abstract": "Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language\nModels (LLMs), but it may lower their truthfulness. This trade-off arises\nbecause IFT steers LLMs to generate responses with long-tail knowledge that is\nnot well covered during pre-training, leading to more informative but less\ntruthful answers when generalizing to unseen tasks. In this paper, we\nempirically demonstrate this helpfulness-truthfulness trade-off in IFT and\npropose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs\nto recognize their uncertainty and explicitly reflect it at the end of their\nresponses. Experimental results show that UNIT-tuned models maintain their\nhelpfulness while distinguishing between certain and uncertain claims, thereby\nreducing hallucinations.",
      "pdf_url": "http://arxiv.org/pdf/2502.11962v1",
      "published": "2025-02-17T16:10:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11962v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "STRIVE: Structured Reasoning for Self-Improvement in Claim Verification",
      "authors": [
        "Haisong Gong",
        "Jing Li",
        "Junfei Wu",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
      ],
      "abstract": "Claim verification is the task of determining whether a claim is supported or\nrefuted by evidence. Self-improvement methods, where reasoning chains are\ngenerated and those leading to correct results are selected for training, have\nsucceeded in tasks like mathematical problem solving. However, in claim\nverification, this approach struggles. Low-quality reasoning chains may falsely\nmatch binary truth labels, introducing faulty reasoning into the\nself-improvement process and ultimately degrading performance. To address this,\nwe propose STRIVE: Structured Reasoning for Self-Improved Verification. Our\nmethod introduces a structured reasoning design with Claim Decomposition,\nEntity Analysis, and Evidence Grounding Verification. These components improve\nreasoning quality, reduce errors, and provide additional supervision signals\nfor self-improvement. STRIVE begins with a warm-up phase, where the base model\nis fine-tuned on a small number of annotated examples to learn the structured\nreasoning design. It is then applied to generate reasoning chains for all\ntraining examples, selecting only those that are correct and structurally sound\nfor subsequent self-improvement training. We demonstrate that STRIVE achieves\nsignificant improvements over baseline models, with a 31.4% performance gain\nover the base model and 20.7% over Chain of Thought on the HOVER datasets,\nhighlighting its effectiveness.",
      "pdf_url": "http://arxiv.org/pdf/2502.11959v1",
      "published": "2025-02-17T16:07:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11959v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Massively Scaling Explicit Policy-conditioned Value Functions",
      "authors": [
        "Nico Bohlinger",
        "Jan Peters"
      ],
      "abstract": "We introduce a scaling strategy for Explicit Policy-Conditioned Value\nFunctions (EPVFs) that significantly improves performance on challenging\ncontinuous-control tasks. EPVFs learn a value function V({\\theta}) that is\nexplicitly conditioned on the policy parameters, enabling direct gradient-based\nupdates to the parameters of any policy. However, EPVFs at scale struggle with\nunrestricted parameter growth and efficient exploration in the policy parameter\nspace. To address these issues, we utilize massive parallelization with\nGPU-based simulators, big batch sizes, weight clipping and scaled peturbations.\nOur results show that EPVFs can be scaled to solve complex tasks, such as a\ncustom Ant environment, and can compete with state-of-the-art Deep\nReinforcement Learning (DRL) baselines like Proximal Policy Optimization (PPO)\nand Soft Actor-Critic (SAC). We further explore action-based policy parameter\nrepresentations from previous work and specialized neural network architectures\nto efficiently handle weight-space features, which have not been used in the\ncontext of DRL before.",
      "pdf_url": "http://arxiv.org/pdf/2502.11949v1",
      "published": "2025-02-17T16:02:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11949v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction",
      "authors": [
        "Ailin Huang",
        "Boyong Wu",
        "Bruce Wang",
        "Chao Yan",
        "Chen Hu",
        "Chengli Feng",
        "Fei Tian",
        "Feiyu Shen",
        "Jingbei Li",
        "Mingrui Chen",
        "Peng Liu",
        "Ruihang Miao",
        "Wang You",
        "Xi Chen",
        "Xuerui Yang",
        "Yechang Huang",
        "Yuxiang Zhang",
        "Zheng Gong",
        "Zixin Zhang",
        "Brian Li",
        "Changyi Wan",
        "Hanpeng Hu",
        "Ranchen Ming",
        "Song Yuan",
        "Xuelin Zhang",
        "Yu Zhou",
        "Bingxin Li",
        "Buyun Ma",
        "Kang An",
        "Wei Ji",
        "Wen Li",
        "Xuan Wen",
        "Yuankai Ma",
        "Yuanwei Liang",
        "Yun Mou",
        "Bahtiyar Ahmidi",
        "Bin Wang",
        "Bo Li",
        "Changxin Miao",
        "Chen Xu",
        "Chengting Feng",
        "Chenrun Wang",
        "Dapeng Shi",
        "Deshan Sun",
        "Dingyuan Hu",
        "Dula Sai",
        "Enle Liu",
        "Guanzhe Huang",
        "Gulin Yan",
        "Heng Wang",
        "Haonan Jia",
        "Haoyang Zhang",
        "Jiahao Gong",
        "Jianchang Wu",
        "Jiahong Liu",
        "Jianjian Sun",
        "Jiangjie Zhen",
        "Jie Feng",
        "Jie Wu",
        "Jiaoren Wu",
        "Jie Yang",
        "Jinguo Wang",
        "Jingyang Zhang",
        "Junzhe Lin",
        "Kaixiang Li",
        "Lei Xia",
        "Li Zhou",
        "Longlong Gu",
        "Mei Chen",
        "Menglin Wu",
        "Ming Li",
        "Mingxiao Li",
        "Mingyao Liang",
        "Na Wang",
        "Nie Hao",
        "Qiling Wu",
        "Qinyuan Tan",
        "Shaoliang Pang",
        "Shiliang Yang",
        "Shuli Gao",
        "Siqi Liu",
        "Sitong Liu",
        "Tiancheng Cao",
        "Tianyu Wang",
        "Wenjin Deng",
        "Wenqing He",
        "Wen Sun",
        "Xin Han",
        "Xiaomin Deng",
        "Xiaojia Liu",
        "Xu Zhao",
        "Yanan Wei",
        "Yanbo Yu",
        "Yang Cao",
        "Yangguang Li",
        "Yangzhen Ma",
        "Yanming Xu",
        "Yaqiang Shi",
        "Yilei Wang",
        "Yinmin Zhong",
        "Yu Luo",
        "Yuanwei Lu",
        "Yuhe Yin",
        "Yuting Yan",
        "Yuxiang Yang",
        "Zhe Xie",
        "Zheng Ge",
        "Zheng Sun",
        "Zhewei Huang",
        "Zhichao Chang",
        "Zidong Yang",
        "Zili Zhang",
        "Binxing Jiao",
        "Daxin Jiang",
        "Heung-Yeung Shum",
        "Jiansheng Chen",
        "Jing Li",
        "Shuchang Zhou",
        "Xiangyu Zhang",
        "Xinhao Zhang",
        "Yibo Zhu"
      ],
      "abstract": "Real-time speech interaction, serving as a fundamental interface for\nhuman-machine collaboration, holds immense potential. However, current\nopen-source models face limitations such as high costs in voice data\ncollection, weakness in dynamic control, and limited intelligence. To address\nthese challenges, this paper introduces Step-Audio, the first production-ready\nopen-source solution. Key contributions include: 1) a 130B-parameter unified\nspeech-text multi-modal model that achieves unified understanding and\ngeneration, with the Step-Audio-Chat version open-sourced; 2) a generative\nspeech data engine that establishes an affordable voice cloning framework and\nproduces the open-sourced lightweight Step-Audio-TTS-3B model through\ndistillation; 3) an instruction-driven fine control system enabling dynamic\nadjustments across dialects, emotions, singing, and RAP; 4) an enhanced\ncognitive architecture augmented with tool calling and role-playing abilities\nto manage complex tasks effectively. Based on our new StepEval-Audio-360\nevaluation benchmark, Step-Audio achieves state-of-the-art performance in human\nevaluations, especially in terms of instruction following. On open-source\nbenchmarks like LLaMA Question, shows 9.3% average performance improvement,\ndemonstrating our commitment to advancing the development of open-source\nmulti-modal language technologies. Our code and models are available at\nhttps://github.com/stepfun-ai/Step-Audio.",
      "pdf_url": "http://arxiv.org/pdf/2502.11946v1",
      "published": "2025-02-17T15:58:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11946v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Deep Spatio-Temporal Neural Network for Air Quality Reanalysis",
      "authors": [
        "Ammar Kheder",
        "Benjamin Foreback",
        "Lili Wang",
        "Zhi-Song Liu",
        "Michael Boy"
      ],
      "abstract": "Air quality prediction is key to mitigating health impacts and guiding\ndecisions, yet existing models tend to focus on temporal trends while\noverlooking spatial generalization. We propose AQ-Net, a spatiotemporal\nreanalysis model for both observed and unobserved stations in the near future.\nAQ-Net utilizes the LSTM and multi-head attention for the temporal regression.\nWe also propose a cyclic encoding technique to ensure continuous time\nrepresentation. To learn fine-grained spatial air quality estimation, we\nincorporate AQ-Net with the neural kNN to explore feature-based interpolation,\nsuch that we can fill the spatial gaps given coarse observation stations. To\ndemonstrate the efficiency of our model for spatiotemporal reanalysis, we use\ndata from 2013-2017 collected in northern China for PM2.5 analysis. Extensive\nexperiments show that AQ-Net excels in air quality reanalysis, highlighting the\npotential of hybrid spatio-temporal models to better capture environmental\ndynamics, especially in urban areas where both spatial and temporal variability\nare critical.",
      "pdf_url": "http://arxiv.org/pdf/2502.11941v1",
      "published": "2025-02-17T15:52:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11941v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "FitLight: Federated Imitation Learning for Plug-and-Play Autonomous Traffic Signal Control",
      "authors": [
        "Yutong Ye",
        "Yingbo Zhou",
        "Zhusen Liu",
        "Xiao Du",
        "Hao Zhou",
        "Xiang Lian",
        "Mingsong Chen"
      ],
      "abstract": "Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC)\nmethods have been extensively studied, their practical applications still raise\nsome serious issues such as high learning cost and poor generalizability. This\nis because the ``trial-and-error'' training style makes RL agents extremely\ndependent on the specific traffic environment, which also requires a long\nconvergence time. To address these issues, we propose a novel Federated\nImitation Learning (FIL)-based framework for multi-intersection TSC, named\nFitLight, which allows RL agents to plug-and-play for any traffic environment\nwithout additional pre-training cost. Unlike existing imitation learning\napproaches that rely on pre-training RL agents with demonstrations, FitLight\nallows real-time imitation learning and seamless transition to reinforcement\nlearning. Due to our proposed knowledge-sharing mechanism and novel hybrid\npressure-based agent design, RL agents can quickly find a best control policy\nwith only a few episodes. Moreover, for resource-constrained TSC scenarios,\nFitLight supports model pruning and heterogeneous model aggregation, such that\nRL agents can work on a micro-controller with merely 16{\\it KB} RAM and 32{\\it\nKB} ROM. Extensive experiments demonstrate that, compared to state-of-the-art\nmethods, FitLight not only provides a superior starting point but also\nconverges to a better final solution on both real-world and synthetic datasets,\neven under extreme resource limitations.",
      "pdf_url": "http://arxiv.org/pdf/2502.11937v1",
      "published": "2025-02-17T15:48:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11937v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
      "authors": [
        "Yi Fang",
        "Bowen Jin",
        "Jiacheng Shen",
        "Sirui Ding",
        "Qiaoyu Tan",
        "Jiawei Han"
      ],
      "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2502.11925v1",
      "published": "2025-02-17T15:35:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11925v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
      "authors": [
        "Jiamin Su",
        "Yibo Yan",
        "Fangteng Fu",
        "Han Zhang",
        "Jingheng Ye",
        "Xiang Liu",
        "Jiahao Huo",
        "Huiyu Zhou",
        "Xuming Hu"
      ],
      "abstract": "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2502.11916v1",
      "published": "2025-02-17T15:31:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11916v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "On the robustness of ChatGPT in teaching Korean Mathematics",
      "authors": [
        "Phuong-Nam Nguyen",
        "Quang Nguyen-The",
        "An Vu-Minh",
        "Diep-Anh Nguyen",
        "Xuan-Lam Pham"
      ],
      "abstract": "ChatGPT, an Artificial Intelligence model, has the potential to revolutionize\neducation. However, its effectiveness in solving non-English questions remains\nuncertain. This study evaluates ChatGPT's robustness using 586 Korean\nmathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering\n391 out of 586 questions. We also assess its ability to rate mathematics\nquestions based on eleven criteria and perform a topic analysis. Our findings\nshow that ChatGPT's ratings align with educational theory and test-taker\nperspectives. While ChatGPT performs well in question classification, it\nstruggles with non-English contexts, highlighting areas for improvement. Future\nresearch should address linguistic biases and enhance accuracy across diverse\nlanguages. Domain-specific optimizations and multilingual training could\nimprove ChatGPT's role in personalized education.",
      "pdf_url": "http://arxiv.org/pdf/2502.11915v1",
      "published": "2025-02-17T15:31:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11915v1",
      "categories": [
        "cs.AI",
        "math.HO",
        "I.2.7; K.3.1; G.3"
      ]
    },
    {
      "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
      "authors": [
        "Zhihang Yuan",
        "Siyuan Wang",
        "Rui Xie",
        "Hanling Zhang",
        "Tongcheng Fang",
        "Yuzhang Shang",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "abstract": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a\ntraining-free paradigm that can make use of adaptive temporal compression in\nlatent space. While existing video generative models apply fixed compression\nrates via pretrained VAE, we observe that real-world video content exhibits\nsubstantial temporal non-uniformity, with high-motion segments containing more\ninformation than static scenes. Based on this insight, DLFR-VAE dynamically\nadjusts the latent frame rate according to the content complexity.\nSpecifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent\nFrame Rate Scheduler that partitions videos into temporal chunks and adaptively\ndetermines optimal frame rates based on information-theoretic content\ncomplexity, and (2) A training-free adaptation mechanism that transforms\npretrained VAE architectures into a dynamic VAE that can process features with\nvariable frame rates. Our simple but effective DLFR-VAE can function as a\nplug-and-play module, seamlessly integrating with existing video generation\nmodels and accelerating the video generation process.",
      "pdf_url": "http://arxiv.org/pdf/2502.11897v1",
      "published": "2025-02-17T15:22:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11897v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning",
      "authors": [
        "Yanxiao Zhao",
        "Yangge Qian",
        "Jingyang Shan",
        "Xiaolin Qin"
      ],
      "abstract": "Reinforcement learning (RL) in continuous action spaces encounters persistent\nchallenges, such as inefficient exploration and convergence to suboptimal\nsolutions. To address these limitations, we propose CAMEL, a novel framework\nintegrating LLM-generated suboptimal policies into the RL training pipeline.\nCAMEL leverages dynamic action masking and an adaptive epsilon-masking\nmechanism to guide exploration during early training stages while gradually\nenabling agents to optimize policies independently. At the core of CAMEL lies\nthe integration of Python-executable suboptimal policies generated by LLMs\nbased on environment descriptions and task objectives. Although simplistic and\nhard-coded, these policies offer valuable initial guidance for RL agents. To\neffectively utilize these priors, CAMEL employs masking-aware optimization to\ndynamically constrain the action space based on LLM outputs. Additionally,\nepsilon-masking gradually reduces reliance on LLM-generated guidance, enabling\nagents to transition from constrained exploration to autonomous policy\nrefinement. Experimental validation on Gymnasium MuJoCo environments\ndemonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated\npolicies significantly improve sample efficiency, achieving performance\ncomparable to or surpassing expert masking baselines. For Walker2d-v4, where\nLLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust\nRL performance without notable degradation, highlighting the framework's\nadaptability across diverse tasks. While CAMEL shows promise in enhancing\nsample efficiency and mitigating convergence challenges, these issues remain\nopen for further research. Future work aims to generalize CAMEL to multimodal\nLLMs for broader observation-action spaces and automate policy evaluation,\nreducing human intervention and enhancing scalability in RL training pipelines.",
      "pdf_url": "http://arxiv.org/pdf/2502.11896v1",
      "published": "2025-02-17T15:22:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11896v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?",
      "authors": [
        "Jacob Nielsen",
        "Peter Schneider-Kamp",
        "Lukas Galke"
      ],
      "abstract": "Large language models (LLMs) require immense resources for training and\ninference. Quantization, a technique that reduces the precision of model\nparameters, offers a promising solution for improving LLM efficiency and\nsustainability. While post-training quantization methods typically achieve 4-8\nbits per parameter, recent research suggests that training LLMs with 1.58 bits\nper weight parameter from scratch can maintain model accuracy while greatly\nreducing memory requirements and energy consumption at inference time. Here, we\ninvestigate a training strategy for quantization-aware pre-training, where the\nmodels are first trained with 16-bit precision and then transition into\n1.58-bit quantization-aware training. Our results on 11 downstream tasks show\nthat this 16-to-1.58-bit training strategy is preferable over full 1.58-bit\ntraining and leaves models closer to those which have undergone 16-bit\ntraining. We further investigate the effects of retaining the optimizer state\nat the transition point and gradually phasing in quantization strength --\nfinding that both techniques alleviate the magnitude of loss spikes, but also\nthat these effects can be compensated through further training.",
      "pdf_url": "http://arxiv.org/pdf/2502.11895v1",
      "published": "2025-02-17T15:21:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11895v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Stonefish: Supporting Machine Learning Research in Marine Robotics",
      "authors": [
        "Michele Grimaldi",
        "Patryk Cieslak",
        "Eduardo Ochoa",
        "Vibhav Bharti",
        "Hayat Rajani",
        "Ignacio Carlucho",
        "Maria Koskinopoulou",
        "Yvan R. Petillot",
        "Nuno Gracias"
      ],
      "abstract": "Simulations are highly valuable in marine robotics, offering a cost-effective\nand controlled environment for testing in the challenging conditions of\nunderwater and surface operations. Given the high costs and logistical\ndifficulties of real-world trials, simulators capable of capturing the\noperational conditions of subsea environments have become key in developing and\nrefining algorithms for remotely-operated and autonomous underwater vehicles.\nThis paper highlights recent enhancements to the Stonefish simulator, an\nadvanced open-source platform supporting development and testing of marine\nrobotics solutions. Key updates include a suite of additional sensors, such as\nan event-based camera, a thermal camera, and an optical flow camera, as well\nas, visual light communication, support for tethered operations, improved\nthruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy.\nThese developments and an automated annotation tool significantly bolster\nStonefish's role in marine robotics research, especially in the field of\nmachine learning, where training data with a known ground truth is hard or\nimpossible to collect.",
      "pdf_url": "http://arxiv.org/pdf/2502.11887v1",
      "published": "2025-02-17T15:13:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11887v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "LIMR: Less is More for RL Scaling",
      "authors": [
        "Xuefeng Li",
        "Haoyang Zou",
        "Pengfei Liu"
      ],
      "abstract": "In this paper, we ask: what truly determines the effectiveness of RL training\ndata for enhancing language models' reasoning capabilities? While recent\nadvances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack\nof transparency about training data requirements has hindered systematic\nprogress. Starting directly from base models without distillation, we challenge\nthe assumption that scaling up RL training data inherently improves\nperformance. we demonstrate that a strategically selected subset of just 1,389\nsamples can outperform the full 8,523-sample dataset. We introduce Learning\nImpact Measurement (LIM), an automated method to evaluate and prioritize\ntraining samples based on their alignment with model learning trajectories,\nenabling efficient resource utilization and scalable implementation. Our method\nachieves comparable or even superior performance using only 1,389 samples\nversus the full 8,523 samples dataset. Notably, while recent data-efficient\napproaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it\nsignificantly underperforms at 7B-scale through supervised fine-tuning (SFT).\nIn contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and\noutperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results\nfundamentally reshape our understanding of RL scaling in LLMs, demonstrating\nthat precise sample selection, rather than data scale, may be the key to\nunlocking enhanced reasoning capabilities. For reproducible research and future\ninnovation, we are open-sourcing LIMR, including implementation of LIM,\ntraining and evaluation code, curated datasets, and trained models at\nhttps://github.com/GAIR-NLP/LIMR.",
      "pdf_url": "http://arxiv.org/pdf/2502.11886v1",
      "published": "2025-02-17T15:13:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11886v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
      "authors": [
        "Shao Zhang",
        "Xihuai Wang",
        "Wenhao Zhang",
        "Chaoran Li",
        "Junru Song",
        "Tingyu Li",
        "Lin Qiu",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Wen Yao",
        "Weinan Zhang",
        "Xinbing Wang",
        "Ying Wen"
      ],
      "abstract": "Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. To the best of our\nknowledge, DPT-Agent is the first language agent framework that achieves\nsuccessful real-time simultaneous human-AI collaboration autonomously. Code of\nDPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",
      "pdf_url": "http://arxiv.org/pdf/2502.11882v1",
      "published": "2025-02-17T15:09:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11882v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models",
      "authors": [
        "Hyunwoo Kim",
        "Melanie Sclar",
        "Tan Zhi-Xuan",
        "Lance Ying",
        "Sydney Levine",
        "Yang Liu",
        "Joshua B. Tenenbaum",
        "Yejin Choi"
      ],
      "abstract": "Existing LLM reasoning methods have shown impressive capabilities across\nvarious tasks, such as solving math and coding problems. However, applying\nthese methods to scenarios without ground-truth answers or rule-based\nverification methods - such as tracking the mental states of an agent - remains\nchallenging. Inspired by the sequential Monte Carlo algorithm, we introduce\nthought-tracing, an inference-time reasoning algorithm designed to trace the\nmental states of specific agents by generating hypotheses and weighting them\nbased on observations without relying on ground-truth solutions to questions in\ndatasets. Our algorithm is modeled after the Bayesian theory-of-mind framework,\nusing LLMs to approximate probabilistic inference over agents' evolving mental\nstates based on their perceptions and actions. We evaluate thought-tracing on\ndiverse theory-of-mind benchmarks, demonstrating significant performance\nimprovements compared to baseline LLMs. Our experiments also reveal interesting\nbehaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind,\nhighlighting the difference of social reasoning compared to other domains.",
      "pdf_url": "http://arxiv.org/pdf/2502.11881v1",
      "published": "2025-02-17T15:08:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11881v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Bitnet.cpp: Efficient Edge Inference for Ternary LLMs",
      "authors": [
        "Jinheng Wang",
        "Hansong Zhou",
        "Ting Song",
        "Shijie Cao",
        "Yan Xia",
        "Ting Cao",
        "Jianyu Wei",
        "Shuming Ma",
        "Hongyu Wang",
        "Furu Wei"
      ],
      "abstract": "The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has\nspurred interest in ternary LLMs. Despite this, research and practical\napplications focusing on efficient edge inference for ternary LLMs remain\nscarce. To bridge this gap, we introduce Bitnet.cpp, an inference system\noptimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix\nmultiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,\nBitnet.cpp incorporates a novel mpGEMM library to facilitate\nsub-2-bits-per-weight, efficient and lossless inference. The library features\ntwo core solutions: Ternary Lookup Table (TL), which addresses spatial\ninefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),\nwhich ensures lossless edge inference, both enabling high-speed inference. Our\nexperiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over\nfull-precision baselines and up to 2.32x over low-bit baselines, setting new\nbenchmarks in the field. Additionally, we expand TL to element-wise lookup\ntable (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and\nempirical evidence of its considerable potential. Bitnet.cpp is publicly\navailable at https://github.com/microsoft/BitNet/tree/paper , offering a\nsophisticated solution for the efficient and practical deployment of edge LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2502.11880v1",
      "published": "2025-02-17T15:06:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11880v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ]
    },
    {
      "title": "FedEAT: A Robustness Optimization Framework for Federated LLMs",
      "authors": [
        "Yahao Pang",
        "Xingyuan Wu",
        "Xiaojin Zhang",
        "Wei Chen",
        "Hai Jin"
      ],
      "abstract": "Significant advancements have been made by Large Language Models (LLMs) in\nthe domains of natural language understanding and automated content creation.\nHowever, they still face persistent problems, including substantial\ncomputational costs and inadequate availability of training data. The\ncombination of Federated Learning (FL) and LLMs (federated LLMs) offers a\nsolution by leveraging distributed data while protecting privacy, which\npositions it as an ideal choice for sensitive domains. However, Federated LLMs\nstill suffer from robustness challenges, including data heterogeneity,\nmalicious clients, and adversarial attacks, which greatly hinder their\napplications. We first introduce the robustness problems in federated LLMs, to\naddress these challenges, we propose FedEAT (Federated Embedding space\nAdversarial Training), a novel framework that applies adversarial training in\nthe embedding space of client LLM and employs a robust aggregation approach,\nspecifically geometric median aggregation, to enhance the robustness of\nFederated LLMs. Our experiments demonstrate that FedEAT effectively improves\nthe robustness of Federated LLMs with minimal performance loss.",
      "pdf_url": "http://arxiv.org/pdf/2502.11863v1",
      "published": "2025-02-17T14:55:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.11863v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}