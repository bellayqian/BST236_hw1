{
  "last_updated": "2025-12-19T00:55:39.151797",
  "papers": [
    {
      "title": "Spatia: Video Generation with Updatable Spatial Memory",
      "authors": [
        "Jinjing Zhao",
        "Fangyun Wei",
        "Zhening Liu",
        "Hongyang Zhang",
        "Chang Xu",
        "Yan Lu"
      ],
      "abstract": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
      "pdf_url": "https://arxiv.org/pdf/2512.15716v1",
      "published": "2025-12-17T18:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15716v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
      "authors": [
        "Vincent Huang",
        "Dami Choi",
        "Daniel D. Johnson",
        "Sarah Schwettmann",
        "Jacob Steinhardt"
      ],
      "abstract": "Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.",
      "pdf_url": "https://arxiv.org/pdf/2512.15712v1",
      "published": "2025-12-17T18:59:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15712v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Artism: AI-Driven Dual-Engine System for Art Generation and Critique",
      "authors": [
        "Shuai Liu",
        "Yiqing Tian",
        "Yang Chen",
        "Mar Canet Sola"
      ],
      "abstract": "This paper proposes a dual-engine AI architectural method designed to address the complex problem of exploring potential trajectories in the evolution of art. We present two interconnected components: AIDA (an artificial artist social network) and the Ismism Machine, a system for critical analysis. The core innovation lies in leveraging deep learning and multi-agent collaboration to enable multidimensional simulations of art historical developments and conceptual innovation patterns. The framework explores a shift from traditional unidirectional critique toward an intelligent, interactive mode of reflexive practice. We are currently applying this method in experimental studies on contemporary art concepts. This study introduces a general methodology based on AI-driven critical loops, offering new possibilities for computational analysis of art.",
      "pdf_url": "https://arxiv.org/pdf/2512.15710v1",
      "published": "2025-12-17T18:58:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15710v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
      "authors": [
        "Jonas Pai",
        "Liam Achenbach",
        "Victoriano Montesinos",
        "Benedek Forrai",
        "Oier Mees",
        "Elvis Nava"
      ],
      "abstract": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \\model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.",
      "pdf_url": "https://arxiv.org/pdf/2512.15692v1",
      "published": "2025-12-17T18:47:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15692v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "BashArena: A Control Setting for Highly Privileged AI Agents",
      "authors": [
        "Adam Kaufman",
        "James Lucassen",
        "Tyler Tracy",
        "Cody Rushing",
        "Aryan Bhatt"
      ],
      "abstract": "Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.",
      "pdf_url": "https://arxiv.org/pdf/2512.15688v1",
      "published": "2025-12-17T18:45:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15688v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
      "authors": [
        "Zhenwen Liang",
        "Sidi Lu",
        "Wenhao Yu",
        "Kishan Panaganti",
        "Yujun Zhou",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.",
      "pdf_url": "https://arxiv.org/pdf/2512.15687v1",
      "published": "2025-12-17T18:44:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15687v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers",
      "authors": [
        "Adam Karvonen",
        "James Chua",
        "Clément Dumas",
        "Kit Fraser-Taliente",
        "Subhash Kantamneni",
        "Julian Minder",
        "Euan Ong",
        "Arnab Sen Sharma",
        "Daniel Wen",
        "Owain Evans",
        "Samuel Marks"
      ],
      "abstract": "Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.",
      "pdf_url": "https://arxiv.org/pdf/2512.15674v1",
      "published": "2025-12-17T18:26:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15674v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Explaining the Reasoning of Large Language Models Using Attribution Graphs",
      "authors": [
        "Chase Walker",
        "Rickard Ewetz"
      ],
      "abstract": "Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.",
      "pdf_url": "https://arxiv.org/pdf/2512.15663v1",
      "published": "2025-12-17T18:15:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15663v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning",
      "authors": [
        "Jiaqi Xu",
        "Cuiling Lan",
        "Xuejin Chen",
        "Yan LU"
      ],
      "abstract": "Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.",
      "pdf_url": "https://arxiv.org/pdf/2512.15662v1",
      "published": "2025-12-17T18:15:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15662v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning",
      "authors": [
        "Xiaodi Li",
        "Dingcheng Li",
        "Rujun Gao",
        "Mahmoud Zamani",
        "Feng Mi",
        "Latifur Khan"
      ],
      "abstract": "Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model's ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.",
      "pdf_url": "https://arxiv.org/pdf/2512.15658v1",
      "published": "2025-12-17T18:11:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15658v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
      "authors": [
        "Hongbo Zhao",
        "Meng Wang",
        "Fei Zhu",
        "Wenzhuo Liu",
        "Bolin Ni",
        "Fanhu Zeng",
        "Gaofeng Meng",
        "Zhaoxiang Zhang"
      ],
      "abstract": "The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.",
      "pdf_url": "https://arxiv.org/pdf/2512.15649v1",
      "published": "2025-12-17T17:58:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15649v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
      "authors": [
        "Yuanhang Li",
        "Yiren Song",
        "Junzhe Bai",
        "Xinran Liang",
        "Hu Yang",
        "Libiao Jin",
        "Qi Mao"
      ],
      "abstract": "We propose \\textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.",
      "pdf_url": "https://arxiv.org/pdf/2512.15635v1",
      "published": "2025-12-17T17:47:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15635v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness",
      "authors": [
        "Darshita Rathore",
        "Vineet Kumar",
        "Chetna Bansal",
        "Anindya Moitra"
      ],
      "abstract": "Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.",
      "pdf_url": "https://arxiv.org/pdf/2512.15634v1",
      "published": "2025-12-17T17:44:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15634v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Evaluating Metrics for Safety with LLM-as-Judges",
      "authors": [
        "Kester Clegg",
        "Richard Hawkins",
        "Ibrahim Habli",
        "Tom Lawton"
      ],
      "abstract": "LLMs (Large Language Models) are increasingly used in text processing pipelines to intelligently respond to a variety of inputs and generation tasks. This raises the possibility of replacing human roles that bottleneck existing information flows, either due to insufficient staff or process complexity. However, LLMs make mistakes and some processing roles are safety critical. For example, triaging post-operative care to patients based on hospital referral letters, or updating site access schedules in nuclear facilities for work crews. If we want to introduce LLMs into critical information flows that were previously performed by humans, how can we make them safe and reliable? Rather than make performative claims about augmented generation frameworks or graph-based techniques, this paper argues that the safety argument should focus on the type of evidence we get from evaluation points in LLM processes, particularly in frameworks that employ LLM-as-Judges (LaJ) evaluators. This paper argues that although we cannot get deterministic evaluations from many natural language processing tasks, by adopting a basket of weighted metrics it may be possible to lower the risk of errors within an evaluation, use context sensitivity to define error severity and design confidence thresholds that trigger human review of critical LaJ judgments when concordance across evaluators is low.",
      "pdf_url": "https://arxiv.org/pdf/2512.15617v1",
      "published": "2025-12-17T17:24:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15617v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "How Smoothing is N-simplicial Attention?",
      "authors": [
        "Alexandre Dussolle",
        "Pietro Liò"
      ],
      "abstract": "Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.",
      "pdf_url": "https://arxiv.org/pdf/2512.15600v1",
      "published": "2025-12-17T17:10:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15600v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Decision-Theoretic Approach for Managing Misalignment",
      "authors": [
        "Daniel A. Herrmann",
        "Abinav Chari",
        "Isabelle Qian",
        "Sree Sharvesh",
        "B. A. Levinstein"
      ],
      "abstract": "When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.",
      "pdf_url": "https://arxiv.org/pdf/2512.15584v1",
      "published": "2025-12-17T16:44:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15584v1",
      "categories": [
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "Evaluating Large Language Models in Scientific Discovery",
      "authors": [
        "Zhangde Song",
        "Jieyu Lu",
        "Yuanqi Du",
        "Botao Yu",
        "Thomas M. Pruyn",
        "Yue Huang",
        "Kehan Guo",
        "Xiuzhe Luo",
        "Yuanhao Qu",
        "Yi Qu",
        "Yinkai Wang",
        "Haorui Wang",
        "Jeff Guo",
        "Jingru Gan",
        "Parshin Shojaee",
        "Di Luo",
        "Andres M Bran",
        "Gen Li",
        "Qiyuan Zhao",
        "Shao-Xiong Lennon Luo",
        "Yuxuan Zhang",
        "Xiang Zou",
        "Wanru Zhao",
        "Yifan F. Zhang",
        "Wucheng Zhang",
        "Shunan Zheng",
        "Saiyang Zhang",
        "Sartaaj Takrim Khan",
        "Mahyar Rajabi-Kochi",
        "Samantha Paradi-Maropakis",
        "Tony Baltoiu",
        "Fengyu Xie",
        "Tianyang Chen",
        "Kexin Huang",
        "Weiliang Luo",
        "Meijing Fang",
        "Xin Yang",
        "Lixue Cheng",
        "Jiajun He",
        "Soha Hassoun",
        "Xiangliang Zhang",
        "Wei Wang",
        "Chandan K. Reddy",
        "Chao Zhang",
        "Zhiling Zheng",
        "Mengdi Wang",
        "Le Cong",
        "Carla P. Gomes",
        "Chang-Yu Hsieh",
        "Aditya Nandy",
        "Philippe Schwaller",
        "Heather J. Kulik",
        "Haojun Jia",
        "Huan Sun",
        "Seyed Mohamad Moosavi",
        "Chenru Duan"
      ],
      "abstract": "Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific \"superintelligence\". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.",
      "pdf_url": "https://arxiv.org/pdf/2512.15567v1",
      "published": "2025-12-17T16:20:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15567v1",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci",
        "cs.LG",
        "physics.chem-ph"
      ]
    },
    {
      "title": "A Conditioned UNet for Music Source Separation",
      "authors": [
        "Ken O'Hanlon",
        "Basil Woods",
        "Lin Wang",
        "Mark Sandler"
      ],
      "abstract": "In this paper we propose a conditioned UNet for Music Source Separation (MSS). MSS is generally performed by multi-output neural networks, typically UNets, with each output representing a particular stem from a predefined instrument vocabulary. In contrast, conditioned MSS networks accept an audio query related to a stem of interest alongside the signal from which that stem is to be extracted. Thus, a strict vocabulary is not required and this enables more realistic tasks in MSS. The potential of conditioned approaches for such tasks has been somewhat hidden due to a lack of suitable data, an issue recently addressed with the MoisesDb dataset. A recent method, Banquet, employs this dataset with promising results seen on larger vocabularies. Banquet uses Bandsplit RNN rather than a UNet and the authors state that UNets should not be suitable for conditioned MSS. We counter this argument and propose QSCNet, a novel conditioned UNet for MSS that integrates network conditioning elements in the Sparse Compressed Network for MSS. We find QSCNet to outperform Banquet by over 1dB SNR on a couple of MSS tasks, while using less than half the number of parameters.",
      "pdf_url": "https://arxiv.org/pdf/2512.15532v1",
      "published": "2025-12-17T15:35:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15532v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "BERT and CNN integrated Neural Collaborative Filtering for Recommender Systems",
      "authors": [
        "Abdullah Al Munem",
        "Sumona Yeasmin",
        "Mohammad Rezwanul Huq"
      ],
      "abstract": "Every day, a significant number of users visit the internet for different needs. The owners of a website generate profits from the user interaction with the contents or items of the website. A robust recommendation system can increase user interaction with a website by recommending items according to the user's unique preferences. BERT and CNN-integrated neural collaborative filtering (NCF) have been proposed for the recommendation system in this experiment. The proposed model takes inputs from the user and item profile and finds the user's interest. This model can handle numeric, categorical, and image data to extract the latent features from the inputs. The model is trained and validated on a small sample of the MovieLens dataset for 25 epochs. The same dataset has been used to train and validate a simple NCF and a BERT-based NCF model and compared with the proposed model. The proposed model outperformed those two baseline models. The obtained result for the proposed model is 0.72 recall and 0.486 Hit Ratio @ 10 for 799 users on the MovieLens dataset. This experiment concludes that considering both categorical and image data can improve the performance of a recommendation system.",
      "pdf_url": "https://arxiv.org/pdf/2512.15526v1",
      "published": "2025-12-17T15:27:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15526v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection",
      "authors": [
        "Konstantinos Kalogiannis",
        "Ahmed Mohamed Hussain",
        "Hexu Li",
        "Panos Papadimitratos"
      ],
      "abstract": "Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.",
      "pdf_url": "https://arxiv.org/pdf/2512.15503v1",
      "published": "2025-12-17T14:45:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15503v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ]
    },
    {
      "title": "Soft Geometric Inductive Bias for Object Centric Dynamics",
      "authors": [
        "Hampus Linander",
        "Conor Heins",
        "Alexander Tschantz",
        "Marco Perin",
        "Christopher Buckley"
      ],
      "abstract": "Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.",
      "pdf_url": "https://arxiv.org/pdf/2512.15493v1",
      "published": "2025-12-17T14:40:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15493v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
      "authors": [
        "Wei Du",
        "Shubham Toshniwal",
        "Branislav Kisacanin",
        "Sadegh Mahdavi",
        "Ivan Moshkov",
        "George Armstrong",
        "Stephen Ge",
        "Edgar Minasyan",
        "Feng Chen",
        "Igor Gitman"
      ],
      "abstract": "High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).\n  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.\n  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.\n  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.",
      "pdf_url": "https://arxiv.org/pdf/2512.15489v1",
      "published": "2025-12-17T14:37:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15489v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?",
      "authors": [
        "Hua Yang",
        "Alejandro Velasco",
        "Thanh Le-Cong",
        "Md Nazmul Haque",
        "Bowen Xu",
        "Denys Poshyvanyk"
      ],
      "abstract": "The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.\n  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.",
      "pdf_url": "https://arxiv.org/pdf/2512.15468v1",
      "published": "2025-12-17T14:12:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15468v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "On Assessing the Relevance of Code Reviews Authored by Generative Models",
      "authors": [
        "Robert Heumüller",
        "Frank Ortmeier"
      ],
      "abstract": "The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.",
      "pdf_url": "https://arxiv.org/pdf/2512.15466v1",
      "published": "2025-12-17T14:12:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15466v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Intent-Driven UAM Rescheduling",
      "authors": [
        "Jeongseok Kim",
        "Kangjin Kim"
      ],
      "abstract": "Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.",
      "pdf_url": "https://arxiv.org/pdf/2512.15462v1",
      "published": "2025-12-17T14:04:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15462v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SC"
      ]
    },
    {
      "title": "Double Horizon Model-Based Policy Optimization",
      "authors": [
        "Akihiro Kubo",
        "Paavo Parmas",
        "Shin Ishii"
      ],
      "abstract": "Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long \"distribution rollout\" (DR) and a short \"training rollout\" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.",
      "pdf_url": "https://arxiv.org/pdf/2512.15439v1",
      "published": "2025-12-17T13:37:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15439v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat",
      "authors": [
        "Stefan Edelkamp"
      ],
      "abstract": "In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.",
      "pdf_url": "https://arxiv.org/pdf/2512.15435v1",
      "published": "2025-12-17T13:27:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15435v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments",
      "authors": [
        "Quanxi Zhou",
        "Wencan Mao",
        "Manabu Tsukada",
        "John C. S. Lui",
        "Yusheng Ji"
      ],
      "abstract": "Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.",
      "pdf_url": "https://arxiv.org/pdf/2512.15430v1",
      "published": "2025-12-17T13:26:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15430v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering",
      "authors": [
        "Liang Peng",
        "Yixuan Ye",
        "Cheng Liu",
        "Hangjun Che",
        "Fei Wang",
        "Zhiwen Yu",
        "Si Wu",
        "Hau-San Wong"
      ],
      "abstract": "Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.",
      "pdf_url": "https://arxiv.org/pdf/2512.15396v1",
      "published": "2025-12-17T12:48:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15396v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations",
      "authors": [
        "Reinhard Moratz",
        "Niklas Daute",
        "James Ondieki",
        "Markus Kattenbeck",
        "Mario Krajina",
        "Ioannis Giannopoulos"
      ],
      "abstract": "This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.",
      "pdf_url": "https://arxiv.org/pdf/2512.15388v1",
      "published": "2025-12-17T12:40:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15388v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Emotion Recognition in Signers",
      "authors": [
        "Kotaro Funakoshi",
        "Yaoxiong Zhu"
      ],
      "abstract": "Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2512.15376v1",
      "published": "2025-12-17T12:26:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15376v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
      "authors": [
        "Zehua Pei",
        "Hui-Ling Zhen",
        "Shixiong Kai",
        "Sinno Jialin Pan",
        "Yunhe Wang",
        "Mingxuan Yuan",
        "Bei Yu"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \\textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \\textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
      "pdf_url": "https://arxiv.org/pdf/2512.15374v1",
      "published": "2025-12-17T12:25:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15374v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models",
      "authors": [
        "Mikel Williams-Lekuona",
        "Georgina Cosma"
      ],
      "abstract": "Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.15372v1",
      "published": "2025-12-17T12:19:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15372v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "Adversarial versification in portuguese as a jailbreak operator in LLMs",
      "authors": [
        "Joao Queiroz"
      ],
      "abstract": "Recent evidence shows that the versification of prompts constitutes a highly effective adversarial mechanism against aligned LLMs. The study 'Adversarial poetry as a universal single-turn jailbreak mechanism in large language models' demonstrates that instructions routinely refused in prose become executable when rewritten as verse, producing up to 18 x more safety failures in benchmarks derived from MLCommons AILuminate. Manually written poems reach approximately 62% ASR, and automated versions 43%, with some models surpassing 90% success in single-turn interactions. The effect is structural: systems trained with RLHF, constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic formal variation. Versification displaces the prompt into sparsely supervised latent regions, revealing guardrails that are excessively dependent on surface patterns. This dissociation between apparent robustness and real vulnerability exposes deep limitations in current alignment regimes. The absence of evaluations in Portuguese, a language with high morphosyntactic complexity, a rich metric-prosodic tradition, and over 250 million speakers, constitutes a critical gap. Experimental protocols must parameterise scansion, metre, and prosodic variation to test vulnerabilities specific to Lusophone patterns, which are currently ignored.",
      "pdf_url": "https://arxiv.org/pdf/2512.15353v1",
      "published": "2025-12-17T11:55:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15353v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery",
      "authors": [
        "Hiroyoshi Nagahama",
        "Katsufumi Inoue",
        "Masayoshi Todorokihara",
        "Michifumi Yoshioka"
      ],
      "abstract": "Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\\% accuracy (+5.4\\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.15344v1",
      "published": "2025-12-17T11:41:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15344v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ]
    },
    {
      "title": "Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality",
      "authors": [
        "Efe Bozkir",
        "Enkelejda Kasneci"
      ],
      "abstract": "The rapid development of generative artificial intelligence (AI) and large language models (LLMs), and the availability of services that make them accessible, have led the general public to begin incorporating them into everyday life. The extended reality (XR) community has also sought to integrate LLMs, particularly in the form of conversational agents, to enhance user experience and task efficiency. When interacting with such conversational agents, users may easily disclose sensitive information due to the naturalistic flow of the conversations, and combining such conversational data with fine-grained sensor data may lead to novel privacy issues. To address these issues, a user-centric understanding of technology acceptance and concerns is essential. Therefore, to this end, we conducted a large-scale crowdsourcing study with 1036 participants, examining user decision-making processes regarding LLM-powered conversational agents in XR, across factors of XR setting type, speech interaction type, and data processing location. We found that while users generally accept these technologies, they express concerns related to security, privacy, social implications, and trust. Our results suggest that familiarity plays a crucial role, as daily generative AI use is associated with greater acceptance. In contrast, previous ownership of XR devices is linked to less acceptance, possibly due to existing familiarity with the settings. We also found that men report higher acceptance with fewer concerns than women. Regarding data type sensitivity, location data elicited the most significant concern, while body temperature and virtual object states were considered least sensitive. Overall, our study highlights the importance of practitioners effectively communicating their measures to users, who may remain distrustful. We conclude with implications and recommendations for LLM-powered XR.",
      "pdf_url": "https://arxiv.org/pdf/2512.15343v1",
      "published": "2025-12-17T11:41:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15343v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Vision-based module for accurately reading linear scales in a laboratory",
      "authors": [
        "Parvesh Saini",
        "Soumyadipta Maiti",
        "Beena Rai"
      ],
      "abstract": "Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.",
      "pdf_url": "https://arxiv.org/pdf/2512.15327v1",
      "published": "2025-12-17T11:24:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15327v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Managing Ambiguity: A Proof of Concept of Human-AI Symbiotic Sense-making based on Quantum-Inspired Cognitive Mechanism of Rogue Variable Detection",
      "authors": [
        "Agnieszka Bienkowska",
        "Jacek Malecki",
        "Alexander Mathiesen-Ohman",
        "Katarzyna Tworek"
      ],
      "abstract": "Organizations increasingly operate in environments characterized by volatility, uncertainty, complexity, and ambiguity (VUCA), where early indicators of change often emerge as weak, fragmented signals. Although artificial intelligence (AI) is widely used to support managerial decision-making, most AI-based systems remain optimized for prediction and resolution, leading to premature interpretive closure under conditions of high ambiguity. This creates a gap in management science regarding how human-AI systems can responsibly manage ambiguity before it crystallizes into error or crisis. This study addresses this gap by presenting a proof of concept (PoC) of the LAIZA human-AI augmented symbiotic intelligence system and its patented process: Systems and Methods for Quantum-Inspired Rogue Variable Modeling (QRVM), Human-in-the-Loop Decoherence, and Collective Cognitive Inference. The mechanism operationalizes ambiguity as a non-collapsed cognitive state, detects persistent interpretive breakdowns (rogue variables), and activates structured human-in-the-loop clarification when autonomous inference becomes unreliable. Empirically, the article draws on a three-month case study conducted in 2025 within the AI development, involving prolonged ambiguity surrounding employee intentions and intellectual property boundaries. The findings show that preserving interpretive plurality enabled early scenario-based preparation, including proactive patent protection, allowing decisive and disruption-free action once ambiguity collapsed. The study contributes to management theory by reframing ambiguity as a first-class construct and demonstrates the practical value of human-AI symbiosis for organizational resilience in VUCA environments.",
      "pdf_url": "https://arxiv.org/pdf/2512.15325v1",
      "published": "2025-12-17T11:23:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15325v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment",
      "authors": [
        "Antony Jerald",
        "Dattesh Shanbhag",
        "Sudhanya Chatterjee"
      ],
      "abstract": "Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.",
      "pdf_url": "https://arxiv.org/pdf/2512.15315v1",
      "published": "2025-12-17T11:05:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15315v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies",
      "authors": [
        "Charan Prakash Rathore",
        "Saumi Ray",
        "Dhruv Kumar"
      ],
      "abstract": "Extracting structured information from zeolite synthesis experimental procedures is critical for materials discovery, yet existing methods have not systematically evaluated Large Language Models (LLMs) for this domain-specific task. This work addresses a fundamental question: what is the efficacy of different prompting strategies when applying LLMs to scientific information extraction? We focus on four key subtasks: event type classification (identifying synthesis steps), trigger text identification (locating event mentions), argument role extraction (recognizing parameter types), and argument text extraction (extracting parameter values). We evaluate four prompting strategies - zero-shot, few-shot, event-specific, and reflection-based - across six state-of-the-art LLMs (Gemma-3-12b-it, GPT-5-mini, O4-mini, Claude-Haiku-3.5, DeepSeek reasoning and non-reasoning) using the ZSEE dataset of 1,530 annotated sentences. Results demonstrate strong performance on event type classification (80-90\\% F1) but modest performance on fine-grained extraction tasks, particularly argument role and argument text extraction (50-65\\% F1). GPT-5-mini exhibits extreme prompt sensitivity with 11-79\\% F1 variation. Notably, advanced prompting strategies provide minimal improvements over zero-shot approaches, revealing fundamental architectural limitations. Error analysis identifies systematic hallucination, over-generalization, and inability to capture synthesis-specific nuances. Our findings demonstrate that while LLMs achieve high-level understanding, precise extraction of experimental parameters requires domain-adapted models, providing quantitative benchmarks for scientific information extraction.",
      "pdf_url": "https://arxiv.org/pdf/2512.15312v1",
      "published": "2025-12-17T11:02:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15312v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Graph Pattern-based Association Rules Evaluated Under No-repeated-anything Semantics in the Graph Transactional Setting",
      "authors": [
        "Basil Ell"
      ],
      "abstract": "We introduce graph pattern-based association rules (GPARs) for directed labeled multigraphs such as RDF graphs. GPARs support both generative tasks, where a graph is extended, and evaluative tasks, where the plausibility of a graph is assessed. The framework goes beyond related formalisms such as graph functional dependencies, graph entity dependencies, relational association rules, graph association rules, multi-relation and path association rules, and Horn rules. Given a collection of graphs, we evaluate graph patterns under no-repeated-anything semantics, which allows the topology of a graph to be taken into account more effectively. We define a probability space and derive confidence, lift, leverage, and conviction in a probabilistic setting. We further analyze how these metrics relate to their classical itemset-based counterparts and identify conditions under which their characteristic properties are preserved.",
      "pdf_url": "https://arxiv.org/pdf/2512.15308v1",
      "published": "2025-12-17T10:52:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15308v1",
      "categories": [
        "cs.DB",
        "cs.AI"
      ]
    },
    {
      "title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I",
      "authors": [
        "Seok-Hyun Ga",
        "Chun-Yen Chang"
      ],
      "abstract": "The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that \"Perception Errors\" were dominant, highlighting a \"Perception-Cognition Gap\" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a \"Calculation-Conceptualization Discrepancy,\" successfully performing calculations while failing to apply the underlying scientific concepts, and \"Process Hallucination,\" where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing \"AI-resistant questions\" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.",
      "pdf_url": "https://arxiv.org/pdf/2512.15298v1",
      "published": "2025-12-17T10:46:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15298v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis",
      "authors": [
        "Toshihide Ubukata",
        "Enhong Mu",
        "Takuto Yamauchi",
        "Mingyue Zhang",
        "Jialong Li",
        "Kenji Tei"
      ],
      "abstract": "Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.",
      "pdf_url": "https://arxiv.org/pdf/2512.15295v1",
      "published": "2025-12-17T10:45:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15295v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions",
      "authors": [
        "Siva Sai",
        "Ishika Goyal",
        "Shubham Sharma",
        "Sri Harshita Manuri",
        "Vinay Chamola",
        "Rajkumar Buyya"
      ],
      "abstract": "The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.",
      "pdf_url": "https://arxiv.org/pdf/2512.15286v1",
      "published": "2025-12-17T10:39:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15286v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning",
      "authors": [
        "Yiliu Sun",
        "Zicheng Zhao",
        "Yang Wei",
        "Yanfang Zhang",
        "Chen Gong"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.",
      "pdf_url": "https://arxiv.org/pdf/2512.15274v1",
      "published": "2025-12-17T10:26:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15274v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
      "authors": [
        "Yuze Wu",
        "Mo Zhu",
        "Xingxing Li",
        "Yuheng Du",
        "Yuxin Fan",
        "Wenjun Li",
        "Xin Zhou",
        "Fei Gao"
      ],
      "abstract": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.",
      "pdf_url": "https://arxiv.org/pdf/2512.15258v1",
      "published": "2025-12-17T10:02:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15258v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis",
      "authors": [
        "Youssef Ghallab",
        "Omar Iraqy",
        "Mohamed Kandil",
        "Mohamed Ashraf",
        "Saadeldine Eletter",
        "Morougue Ghazal",
        "Ayman Khalafallah",
        "Nagwa El-Makky"
      ],
      "abstract": "Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.",
      "pdf_url": "https://arxiv.org/pdf/2512.15250v1",
      "published": "2025-12-17T09:49:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15250v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification",
      "authors": [
        "Yupeng Zhang",
        "Adam G. Dunn",
        "Usman Naseem",
        "Jinman Kim"
      ],
      "abstract": "Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $Δ$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $Δ$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.",
      "pdf_url": "https://arxiv.org/pdf/2512.15249v1",
      "published": "2025-12-17T09:47:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15249v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications",
      "authors": [
        "Zhengchao Chen",
        "Haoran Wang",
        "Jing Yao",
        "Pedram Ghamisi",
        "Jun Zhou",
        "Peter M. Atkinson",
        "Bing Zhang"
      ],
      "abstract": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).",
      "pdf_url": "https://arxiv.org/pdf/2512.15231v1",
      "published": "2025-12-17T09:31:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15231v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task in WMT 2024",
      "authors": [
        "Yash Bhaskar",
        "Parameswari Krishnamurthy"
      ],
      "abstract": "This paper presents the systems submitted by the Yes-MT team for the Low-Resource Indic Language Translation Shared Task at WMT 2024 (Pakray et al., 2024), focusing on translating between English and the Assamese, Mizo, Khasi, and Manipuri languages. The experiments explored various approaches, including fine-tuning pre-trained models like mT5 (Xue et al., 2020) and IndicBart (Dabre et al., 2021) in both multilingual and monolingual settings, LoRA (Hu et al., 2021) fine-tuning IndicTrans2 (Gala et al., 2023), zero-shot and few-shot prompting (Brown, 2020) with large language models (LLMs) like Llama 3 (Dubey et al., 2024) and Mixtral 8x7b (Jiang et al., 2024), LoRA supervised fine-tuning of Llama 3 (Mecklenburg et al., 2024), and training Transformer models (Vaswani, 2017) from scratch. The results were evaluated on the WMT23 Low-Resource Indic Language Translation Shared Task test data using SacreBLEU (Post, 2018) and CHRF (Popovic, 2015), highlighting the challenges of low-resource translation and the potential of LLMs for these tasks, particularly with fine-tuning.",
      "pdf_url": "https://arxiv.org/pdf/2512.15226v1",
      "published": "2025-12-17T09:24:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2512.15226v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}