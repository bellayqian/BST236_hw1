{
  "last_updated": "2025-04-24T00:50:26.846349",
  "papers": [
    {
      "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities",
      "authors": [
        "Thomas Schmied",
        "Jörg Bornschein",
        "Jordi Grau-Moya",
        "Markus Wulfmeier",
        "Razvan Pascanu"
      ],
      "abstract": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
      "pdf_url": "http://arxiv.org/pdf/2504.16078v1",
      "published": "2025-04-22T17:57:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16078v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Describe Anything: Detailed Localized Image and Video Captioning",
      "authors": [
        "Long Lian",
        "Yifan Ding",
        "Yunhao Ge",
        "Sifei Liu",
        "Hanzi Mao",
        "Boyi Li",
        "Marco Pavone",
        "Ming-Yu Liu",
        "Trevor Darrell",
        "Adam Yala",
        "Yin Cui"
      ],
      "abstract": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.",
      "pdf_url": "http://arxiv.org/pdf/2504.16072v1",
      "published": "2025-04-22T17:51:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16072v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Vision language models are unreliable at trivial spatial cognition",
      "authors": [
        "Sangeet Khemlani",
        "Tyler Tran",
        "Nathaniel Gyory",
        "Anthony M. Harrison",
        "Wallace E. Lawson",
        "Ravenna Thielstrom",
        "Hunter Thompson",
        "Taaren Singh",
        "J. Gregory Trafton"
      ],
      "abstract": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing.",
      "pdf_url": "http://arxiv.org/pdf/2504.16061v1",
      "published": "2025-04-22T17:38:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16061v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement",
      "authors": [
        "Zhifan Ye",
        "Kejing Xia",
        "Yonggan Fu",
        "Xin Dong",
        "Jihoon Hong",
        "Xiangchi Yuan",
        "Shizhe Diao",
        "Jan Kautz",
        "Pavlo Molchanov",
        "Yingyan Celine Lin"
      ],
      "abstract": "State space models (SSMs) have emerged as an efficient alternative to\nTransformer models for language modeling, offering linear computational\ncomplexity and constant memory usage as context length increases. However,\ndespite their efficiency in handling long contexts, recent studies have shown\nthat SSMs, such as Mamba models, generally underperform compared to\nTransformers in long-context understanding tasks. To address this significant\nshortfall and achieve both efficient and accurate long-context understanding,\nwe propose LongMamba, a training-free technique that significantly enhances the\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\nthat the hidden channels in Mamba can be categorized into local and global\nchannels based on their receptive field lengths, with global channels primarily\nresponsible for long-context capability. These global channels can become the\nkey bottleneck as the input context lengthens. Specifically, when input lengths\nlargely exceed the training sequence length, global channels exhibit\nlimitations in adaptively extend their receptive fields, leading to Mamba's\npoor long-context performance. The key idea of LongMamba is to mitigate the\nhidden state memory decay in these global channels by preventing the\naccumulation of unimportant tokens in their memory. This is achieved by first\nidentifying critical tokens in the global channels and then applying token\nfiltering to accumulate only those critical tokens. Through extensive\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\nsets a new standard for Mamba's long-context performance, significantly\nextending its operational range without requiring additional training. Our code\nis available at https://github.com/GATECH-EIC/LongMamba.",
      "pdf_url": "http://arxiv.org/pdf/2504.16053v1",
      "published": "2025-04-22T17:30:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16053v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis",
      "authors": [
        "Frank Li",
        "Hari Trivedi",
        "Bardia Khosravi",
        "Theo Dapamede",
        "Mohammadreza Chavoshi",
        "Abdulhameed Dere",
        "Rohan Satya Isaac",
        "Aawez Mansuri",
        "Janice Newsome",
        "Saptarshi Purkayastha",
        "Judy Gichoya"
      ],
      "abstract": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology.",
      "pdf_url": "http://arxiv.org/pdf/2504.16047v1",
      "published": "2025-04-22T17:20:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16047v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Approximate matrices of systems of max-min fuzzy relational equations",
      "authors": [
        "Ismaïl Baaj"
      ],
      "abstract": "In this article, we address the inconsistency of a system of max-min fuzzy\nrelational equations by minimally modifying the matrix governing the system in\norder to achieve consistency. Our method yields consistent systems that\napproximate the original inconsistent system in the following sense: the\nright-hand side vector of each consistent system is that of the inconsistent\nsystem, and the coefficients of the matrix governing each consistent system are\nobtained by modifying, exactly and minimally, the entries of the original\nmatrix that must be corrected to achieve consistency, while leaving all other\nentries unchanged.\n  To obtain a consistent system that closely approximates the considered\ninconsistent system, we study the distance (in terms of a norm among $L_1$,\n$L_2$ or $L_\\infty$) between the matrix of the inconsistent system and the set\nformed by the matrices of consistent systems that use the same right-hand side\nvector as the inconsistent system. We show that our method allows us to\ndirectly compute matrices of consistent systems that use the same right-hand\nside vector as the inconsistent system whose distance in terms of $L_\\infty$\nnorm to the matrix of the inconsistent system is minimal (the computational\ncosts are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit\nanalytical formula for computing this minimal $L_\\infty$ distance. Finally, we\ntranslate our results for systems of min-max fuzzy relational equations and\npresent some potential applications.",
      "pdf_url": "http://arxiv.org/pdf/2504.16042v1",
      "published": "2025-04-22T17:09:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16042v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "Muon Optimizer Accelerates Grokking",
      "authors": [
        "Amund Tveit",
        "Bjørn Remseth",
        "Arve Skogvold"
      ],
      "abstract": "This paper investigates the impact of different optimizers on the grokking\nphenomenon, where models exhibit delayed generalization. We conducted\nexperiments across seven numerical tasks (primarily modular arithmetic) using a\nmodern Transformer architecture. The experimental configuration systematically\nvaried the optimizer (Muon vs. AdamW) and the softmax activation function\n(standard softmax, stablemax, and sparsemax) to assess their combined effect on\nlearning dynamics. Our empirical evaluation reveals that the Muon optimizer,\ncharacterized by its use of spectral norm constraints and second-order\ninformation, significantly accelerates the onset of grokking compared to the\nwidely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch\nfrom 153.09 to 102.89 across all configurations, a statistically significant\ndifference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice\nplays a crucial role in facilitating the transition from memorization to\ngeneralization.",
      "pdf_url": "http://arxiv.org/pdf/2504.16041v1",
      "published": "2025-04-22T17:08:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16041v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2"
      ]
    },
    {
      "title": "LLMs meet Federated Learning for Scalable and Secure IoT Management",
      "authors": [
        "Yazan Otoum",
        "Arghavan Asad",
        "Amiya Nayak"
      ],
      "abstract": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions.",
      "pdf_url": "http://arxiv.org/pdf/2504.16032v1",
      "published": "2025-04-22T16:56:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16032v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3",
      "authors": [
        "Ahmed R. Sadik",
        "Siddhata Govind"
      ],
      "abstract": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection",
      "pdf_url": "http://arxiv.org/pdf/2504.16027v1",
      "published": "2025-04-22T16:44:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16027v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ]
    },
    {
      "title": "Trends in AI Supercomputers",
      "authors": [
        "Konstantin F. Pilz",
        "James Sanders",
        "Robi Rahman",
        "Lennart Heim"
      ],
      "abstract": "Frontier AI development relies on powerful AI supercomputers, yet analysis of\nthese systems is limited. We create a dataset of 500 AI supercomputers from\n2019 to 2025 and analyze key trends in performance, power needs, hardware cost,\nownership, and global distribution. We find that the computational performance\nof AI supercomputers has doubled every nine months, while hardware acquisition\ncost and power needs both doubled every year. The leading system in March 2025,\nxAI's Colossus, used 200,000 AI chips, had a hardware cost of \\$7B, and\nrequired 300 MW of power, as much as 250,000 households. As AI supercomputers\nevolved from tools for science to industrial machines, companies rapidly\nexpanded their share of total AI supercomputer performance, while the share of\ngovernments and academia diminished. Globally, the United States accounts for\nabout 75% of total performance in our dataset, with China in second place at\n15%. If the observed trends continue, the leading AI supercomputer in 2030 will\nachieve $2\\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a\nhardware cost of \\$200 billion, and require 9 GW of power. Our analysis\nprovides visibility into the AI supercomputer landscape, allowing policymakers\nto assess key AI trends like resource needs, ownership, and national\ncompetitiveness.",
      "pdf_url": "http://arxiv.org/pdf/2504.16026v1",
      "published": "2025-04-22T16:44:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16026v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support",
      "authors": [
        "Dinithi Dissanayake",
        "Suranga Nanayakkara"
      ],
      "abstract": "Flow theory describes an optimal cognitive state where individuals experience\ndeep focus and intrinsic motivation when a task's difficulty aligns with their\nskill level. In AI-augmented reasoning, interventions that disrupt the state of\ncognitive flow can hinder rather than enhance decision-making. This paper\nproposes a context-aware cognitive augmentation framework that adapts\ninterventions based on three key contextual factors: type, timing, and scale.\nBy leveraging multimodal behavioral cues (e.g., gaze behavior, typing\nhesitation, interaction speed), AI can dynamically adjust cognitive support to\nmaintain or restore flow. We introduce the concept of cognitive flow, an\nextension of flow theory in AI-augmented reasoning, where interventions are\npersonalized, adaptive, and minimally intrusive. By shifting from static\ninterventions to context-aware augmentation, our approach ensures that AI\nsystems support deep engagement in complex decision-making and reasoning\nwithout disrupting cognitive immersion.",
      "pdf_url": "http://arxiv.org/pdf/2504.16021v1",
      "published": "2025-04-22T16:35:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16021v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer",
      "authors": [
        "Soham Sane"
      ],
      "abstract": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\naddressing the memory overhead and hyperparameter complexity of adaptive\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\ngradient normalization followed by a smooth hyperbolic tangent transformation,\n$g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness\nparameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\nformulation; (2) a formal non-convex convergence analysis guaranteeing\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\nperformance profile. While exhibiting instability in off-policy DQN, it\nprovides enhanced training stability with competitive results in TD3 (requiring\ncareful $\\alpha$ tuning) and achieves substantially superior performance in\non-policy PPO. These results underscore the critical importance of empirical\n$\\alpha$ selection, revealing strong interactions between the optimizer's\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\nalternative optimizer for memory-constrained scenarios and shows significant\npromise for on-policy learning regimes where its stability and efficiency\nadvantages can be particularly impactful.",
      "pdf_url": "http://arxiv.org/pdf/2504.16020v2",
      "published": "2025-04-22T16:33:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16020v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ]
    },
    {
      "title": "CAPO: Cost-Aware Prompt Optimization",
      "authors": [
        "Tom Zehle",
        "Moritz Schlager",
        "Timo Heiß",
        "Matthias Feurer"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2504.16005v2",
      "published": "2025-04-22T16:14:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16005v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ]
    },
    {
      "title": "How Private is Your Attention? Bridging Privacy with In-Context Learning",
      "authors": [
        "Soham Bonnerjee",
        "Zhen Wei",
        "Yeon",
        "Anna Asch",
        "Sagnik Nandy",
        "Promit Ghosal"
      ],
      "abstract": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings.",
      "pdf_url": "http://arxiv.org/pdf/2504.16000v1",
      "published": "2025-04-22T16:05:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.16000v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning",
      "authors": [
        "Sindhuja Madabushi",
        "Ahmad Faraz Khan",
        "Haider Ali",
        "Jin-Hee Cho"
      ],
      "abstract": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL.",
      "pdf_url": "http://arxiv.org/pdf/2504.15995v1",
      "published": "2025-04-22T16:00:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15995v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models",
      "authors": [
        "Shang Wang"
      ],
      "abstract": "The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation.",
      "pdf_url": "http://arxiv.org/pdf/2504.15983v1",
      "published": "2025-04-22T15:33:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15983v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Bug Destiny Prediction in Large Open-Source Software Repositories through Sentiment Analysis and BERT Topic Modeling",
      "authors": [
        "Sophie C. Pope",
        "Andrew Barovic",
        "Armin Moin"
      ],
      "abstract": "This study explores a novel approach to predicting key bug-related outcomes,\nincluding the time to resolution, time to fix, and ultimate status of a bug,\nusing data from the Bugzilla Eclipse Project. Specifically, we leverage\nfeatures available before a bug is resolved to enhance predictive accuracy. Our\nmethodology incorporates sentiment analysis to derive both an emotionality\nscore and a sentiment classification (positive or negative). Additionally, we\nintegrate the bug's priority level and its topic, extracted using a BERTopic\nmodel, as features for a Convolutional Neural Network (CNN) and a Multilayer\nPerceptron (MLP). Our findings indicate that the combination of BERTopic and\nsentiment analysis can improve certain model performance metrics. Furthermore,\nwe observe that balancing model inputs enhances practical applicability, albeit\nat the cost of a significant reduction in accuracy in most cases. To address\nour primary objectives, predicting time-to-resolution, time-to-fix, and bug\ndestiny, we employ both binary classification and exact time value predictions,\nallowing for a comparative evaluation of their predictive effectiveness.\nResults demonstrate that sentiment analysis serves as a valuable predictor of a\nbug's eventual outcome, particularly in determining whether it will be fixed.\nHowever, its utility is less pronounced when classifying bugs into more complex\nor unconventional outcome categories.",
      "pdf_url": "http://arxiv.org/pdf/2504.15972v1",
      "published": "2025-04-22T15:18:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15972v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Universal Approximation with Softmax Attention",
      "authors": [
        "Jerry Yao-Chieh Hu",
        "Hude Liu",
        "Hong-Yu Chen",
        "Weimin Wu",
        "Han Liu"
      ],
      "abstract": "We prove that with linear transformations, both (i) two-layer self-attention\nand (ii) one-layer self-attention followed by a softmax function are universal\napproximators for continuous sequence-to-sequence functions on compact domains.\nOur main technique is a new interpolation-based method for analyzing\nattention's internal mechanism. This leads to our key insight: self-attention\nis able to approximate a generalized version of ReLU to arbitrary precision,\nand hence subsumes many known universal approximators. Building on these, we\nshow that two-layer multi-head attention alone suffices as a\nsequence-to-sequence universal approximator. In contrast, prior works rely on\nfeed-forward networks to establish universal approximation in Transformers.\nFurthermore, we extend our techniques to show that, (softmax-)attention-only\nlayers are capable of approximating various statistical models in-context. We\nbelieve these techniques hold independent interest.",
      "pdf_url": "http://arxiv.org/pdf/2504.15956v1",
      "published": "2025-04-22T14:51:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15956v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity",
      "authors": [
        "Fanny Jourdan",
        "Yannick Chevalier",
        "Cécile Favre"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.",
      "pdf_url": "http://arxiv.org/pdf/2504.15941v1",
      "published": "2025-04-22T14:35:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15941v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models",
      "authors": [
        "Saban Ozturk",
        "Melih B. Yilmaz",
        "Muti Kara",
        "M. Talat Yavuz",
        "Aykut Koç",
        "Tolga Çukur"
      ],
      "abstract": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods.",
      "pdf_url": "http://arxiv.org/pdf/2504.15929v1",
      "published": "2025-04-22T14:17:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15929v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers",
      "authors": [
        "Meng Wang",
        "Tian Lin",
        "Qingshan Hou",
        "Aidi Lin",
        "Jingcheng Wang",
        "Qingsheng Peng",
        "Truong X. Nguyen",
        "Danqi Fang",
        "Ke Zou",
        "Ting Xu",
        "Cancan Xue",
        "Ten Cheer Quek",
        "Qinkai Yu",
        "Minxin Liu",
        "Hui Zhou",
        "Zixuan Xiao",
        "Guiqin He",
        "Huiyu Liang",
        "Tingkun Shi",
        "Man Chen",
        "Linna Liu",
        "Yuanyuan Peng",
        "Lianyu Wang",
        "Qiuming Hu",
        "Junhong Chen",
        "Zhenhua Zhang",
        "Cheng Chen",
        "Yitian Zhao",
        "Dianbo Liu",
        "Jianhua Wu",
        "Xinjian Chen",
        "Changqing Zhang",
        "Triet Thanh Nguyen",
        "Yanda Meng",
        "Yalin Zheng",
        "Yih Chung Tham",
        "Carol Y. Cheung",
        "Huazhu Fu",
        "Haoyu Chen",
        "Ching-Yu Cheng"
      ],
      "abstract": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers.",
      "pdf_url": "http://arxiv.org/pdf/2504.15928v1",
      "published": "2025-04-22T14:17:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15928v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics",
      "authors": [
        "Ling Cheng",
        "Jiashu Pu",
        "Ruicheng Liang",
        "Qian Shao",
        "Hezhe Qiao",
        "Feida Zhu"
      ],
      "abstract": "Semi-supervised community detection methods are widely used for identifying\nspecific communities due to the label scarcity. Existing semi-supervised\ncommunity detection methods typically involve two learning stages learning in\nboth initial identification and subsequent adjustment, which often starts from\nan unreasonable community core candidate. Moreover, these methods encounter\nscalability issues because they depend on reinforcement learning and generative\nadversarial networks, leading to higher computational costs and restricting the\nselection of candidates. To address these limitations, we draw a parallel\nbetween crystallization kinetics and community detection to integrate the\nspontaneity of the annealing process into community detection. Specifically, we\nliken community detection to identifying a crystal subgrain (core) that expands\ninto a complete grain (community) through a process similar to annealing. Based\non this finding, we propose CLique ANNealing (CLANN), which applies kinetics\nconcepts to community detection by integrating these principles into the\noptimization process to strengthen the consistency of the community core.\nSubsequently, a learning-free Transitive Annealer was employed to refine the\nfirst-stage candidates by merging neighboring cliques and repositioning the\ncommunity core, enabling a spontaneous growth process that enhances\nscalability. Extensive experiments on \\textbf{43} different network settings\ndemonstrate that CLANN outperforms state-of-the-art methods across multiple\nreal-world datasets, showcasing its exceptional efficacy and efficiency in\ncommunity detection.",
      "pdf_url": "http://arxiv.org/pdf/2504.15927v1",
      "published": "2025-04-22T14:17:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15927v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "Achieving Distributive Justice in Federated Learning via Uncertainty Quantification",
      "authors": [
        "Alycia Carey",
        "Xintao Wu"
      ],
      "abstract": "Client-level fairness metrics for federated learning are used to ensure that\nall clients in a federation either: a) have similar final performance on their\nlocal data distributions (i.e., client parity), or b) obtain final performance\non their local data distributions relative to their contribution to the\nfederated learning process (i.e., contribution fairness). While a handful of\nworks that propose either client-parity or contribution-based fairness metrics\nground their definitions and decisions in social theories of equality -- such\nas distributive justice -- most works arbitrarily choose what notion of\nfairness to align with which makes it difficult for practitioners to choose\nwhich fairness metric aligns best with their fairness ethics. In this work, we\npropose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),\na flexible federated learning framework that can achieve multiple distributive\njustice-based client-level fairness metrics. Namely, by utilizing techniques\ninspired by fair resource allocation, in conjunction with performing aleatoric\nuncertainty-based client weighing, our UDJ-FL framework is able to achieve\negalitarian, utilitarian, Rawls' difference principle, or desert-based\nclient-level fairness. We empirically show the ability of UDJ-FL to achieve all\nfour defined distributive justice-based client-level fairness metrics in\naddition to providing fairness equivalent to (or surpassing) other popular fair\nfederated learning works. Further, we provide justification for why aleatoric\nuncertainty weighing is necessary to the construction of our UDJ-FL framework\nas well as derive theoretical guarantees for the generalization bounds of\nUDJ-FL. Our code is publicly available at\nhttps://github.com/alycia-noel/UDJ-FL.",
      "pdf_url": "http://arxiv.org/pdf/2504.15924v1",
      "published": "2025-04-22T14:07:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15924v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "68T01",
        "I.2.0"
      ]
    },
    {
      "title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions",
      "authors": [
        "Chang Zong",
        "Bin Li",
        "Shoujun Zhou",
        "Jian Wan",
        "Lei Zhang"
      ],
      "abstract": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc.",
      "pdf_url": "http://arxiv.org/pdf/2504.15918v2",
      "published": "2025-04-22T14:03:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15918v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "68T45, 68T20"
      ]
    },
    {
      "title": "Automated Bug Report Prioritization in Large Open-Source Projects",
      "authors": [
        "Riley Pierson",
        "Armin Moin"
      ],
      "abstract": "Large open-source projects receive a large number of issues (known as bugs),\nincluding software defect (i.e., bug) reports and new feature requests from\ntheir user and developer communities at a fast rate. The often limited project\nresources do not allow them to deal with all issues. Instead, they have to\nprioritize them according to the project's priorities and the issues'\nseverities. In this paper, we propose a novel approach to automated bug\nprioritization based on the natural language text of the bug reports that are\nstored in the open bug repositories of the issue-tracking systems. We conduct\ntopic modeling using a variant of LDA called TopicMiner-MTM and text\nclassification with the BERT large language model to achieve a higher\nperformance level compared to the state-of-the-art. Experimental results using\nan existing reference dataset containing 85,156 bug reports of the Eclipse\nPlatform project indicate that we outperform existing approaches in terms of\nAccuracy, Precision, Recall, and F1-measure of the bug report priority\nprediction.",
      "pdf_url": "http://arxiv.org/pdf/2504.15912v1",
      "published": "2025-04-22T13:57:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15912v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network",
      "authors": [
        "Wenjing Xiao",
        "Chenglong Shi",
        "Miaojiang Chen",
        "Zhiquan Liu",
        "Min Chen",
        "H. Herbert Song"
      ],
      "abstract": "With the exponential growth of Internet of Things (IoT) devices, edge\ncomputing (EC) is gradually playing an important role in providing\ncost-effective services. However, existing approaches struggle to perform well\nin graph-structured scenarios where user data is correlated, such as traffic\nflow prediction and social relationship recommender systems. In particular,\ngraph neural network (GNN)-based approaches lead to expensive server\ncommunication cost. To address this problem, we propose GraphEdge, an efficient\nGNN-based EC architecture. It considers the EC system of GNN tasks, where there\nare associations between users and it needs to take into account the task data\nof its neighbors when processing the tasks of a user. Specifically, the\narchitecture first perceives the user topology and represents their data\nassociations as a graph layout at each time step. Then the graph layout is\noptimized by calling our proposed hierarchical traversal graph cut algorithm\n(HiCut), which cuts the graph layout into multiple weakly associated subgraphs\nbased on the aggregation characteristics of GNN, and the communication cost\nbetween different subgraphs during GNN inference is minimized. Finally, based\non the optimized graph layout, our proposed deep reinforcement learning (DRL)\nbased graph offloading algorithm (DRLGO) is executed to obtain the optimal\noffloading strategy for the tasks of users, the offloading strategy is\nsubgraph-based, it tries to offload user tasks in a subgraph to the same edge\nserver as possible while minimizing the task processing time and energy\nconsumption of the EC system. Experimental results show the good effectiveness\nand dynamic adaptation of our proposed architecture and it also performs well\neven in dynamic scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2504.15905v1",
      "published": "2025-04-22T13:45:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15905v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations",
      "authors": [
        "Nikhil Khandalkar",
        "Pavan Yadav",
        "Krishna Shinde",
        "Lokesh B. Ramegowda",
        "Rajarshi Das"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility.",
      "pdf_url": "http://arxiv.org/pdf/2504.15903v1",
      "published": "2025-04-22T13:43:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15903v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Dynamic Early Exit in Reasoning Models",
      "authors": [
        "Chenxu Yang",
        "Qingyi Si",
        "Yongjie Duan",
        "Zheliang Zhu",
        "Chenyu Zhu",
        "Zheng Lin",
        "Li Cao",
        "Weiping Wang"
      ],
      "abstract": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%.",
      "pdf_url": "http://arxiv.org/pdf/2504.15895v1",
      "published": "2025-04-22T13:36:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15895v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making",
      "authors": [
        "Chengbo Zheng",
        "Tim Miller",
        "Alina Bialkowski",
        "H Peter Soyer",
        "Monika Janda"
      ],
      "abstract": "High stakes decision-making often requires a continuous interplay between\nevolving evidence and shifting hypotheses, a dynamic that is not well supported\nby current AI decision support systems. In this paper, we introduce a\nmixed-initiative framework for AI assisted decision making that is grounded in\nthe data-frame theory of sensemaking and the evaluative AI paradigm. Our\napproach enables both humans and AI to collaboratively construct, validate, and\nadapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer\ndiagnosis prototype that leverages a concept bottleneck model to facilitate\ninterpretable interactions and dynamic updates to diagnostic hypotheses.",
      "pdf_url": "http://arxiv.org/pdf/2504.15894v1",
      "published": "2025-04-22T13:36:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15894v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading",
      "authors": [
        "Farida Mohsen",
        "Samir Belhaouari",
        "Zubair Shah"
      ],
      "abstract": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis.",
      "pdf_url": "http://arxiv.org/pdf/2504.15883v1",
      "published": "2025-04-22T13:27:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15883v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation",
      "authors": [
        "Qizhen Wu Lei Chen",
        "Kexin Liu",
        "Jinhu Lü"
      ],
      "abstract": "In swarm robotics, confrontation scenarios, including strategic\nconfrontations, require efficient decision-making that integrates discrete\ncommands and continuous actions. Traditional task and motion planning methods\nseparate decision-making into two layers, but their unidirectional structure\nfails to capture the interdependence between these layers, limiting\nadaptability in dynamic environments. Here, we propose a novel bidirectional\napproach based on hierarchical reinforcement learning, enabling dynamic\ninteraction between the layers. This method effectively maps commands to task\nallocation and actions to path planning, while leveraging cross-training\ntechniques to enhance learning across the hierarchical framework. Furthermore,\nwe introduce a trajectory prediction model that bridges abstract task\nrepresentations with actionable planning goals. In our experiments, it achieves\nover 80\\% in confrontation win rate and under 0.01 seconds in decision time,\noutperforming existing approaches. Demonstrations through large-scale tests and\nreal-world robot experiments further emphasize the generalization capabilities\nand practical applicability of our method.",
      "pdf_url": "http://arxiv.org/pdf/2504.15876v1",
      "published": "2025-04-22T13:22:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15876v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search",
      "authors": [
        "Lotfi Abdelkrim Mecharbat",
        "Ibrahim Almakky",
        "Martin Takac",
        "Mohammad Yaqub"
      ],
      "abstract": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.",
      "pdf_url": "http://arxiv.org/pdf/2504.15865v2",
      "published": "2025-04-22T13:04:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15865v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters",
      "authors": [
        "Xiang Liu",
        "Hau Chan",
        "Minming Li",
        "Xianlong Zeng",
        "Chenchen Fu",
        "Weiwei Wu"
      ],
      "abstract": "Federated learning (FL) is a promising approach that allows requesters (\\eg,\nservers) to obtain local training models from workers (e.g., clients). Since\nworkers are typically unwilling to provide training services/models freely and\nvoluntarily, many incentive mechanisms in FL are designed to incentivize\nparticipation by offering monetary rewards from requesters. However, existing\nstudies neglect two crucial aspects of real-world FL scenarios. First, workers\ncan possess inherent incompatibility characteristics (e.g., communication\nchannels and data sources), which can lead to degradation of FL efficiency\n(e.g., low communication efficiency and poor model generalization). Second, the\nrequesters are budgeted, which limits the amount of workers they can hire for\ntheir tasks. In this paper, we investigate the scenario in FL where multiple\nbudgeted requesters seek training services from incompatible workers with\nprivate training costs. We consider two settings: the cooperative budget\nsetting where requesters cooperate to pool their budgets to improve their\noverall utility and the non-cooperative budget setting where each requester\noptimizes their utility within their own budgets. To address efficiency\ndegradation caused by worker incompatibility, we develop novel\ncompatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both\nsettings to elicit true private costs and determine workers to hire for\nrequesters and their rewards while satisfying requester budget constraints. Our\nmechanisms guarantee individual rationality, truthfulness, budget feasibility,\nand approximation performance. We conduct extensive experiments using\nreal-world datasets to show that the proposed mechanisms significantly\noutperform existing baselines.",
      "pdf_url": "http://arxiv.org/pdf/2504.15847v1",
      "published": "2025-04-22T12:42:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15847v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases",
      "authors": [
        "Modhurita Mitra",
        "Martine G. de Vos",
        "Nicola Cortinovis",
        "Dawa Ometto"
      ],
      "abstract": "There has been enormous interest in generative AI since ChatGPT was launched\nin 2022. However, there are concerns about the accuracy and consistency of the\noutputs of generative AI. We have carried out an exploratory study on the\napplication of this new technology in research data processing. We identified\ntasks for which rule-based or traditional machine learning approaches were\ndifficult to apply, and then performed these tasks using generative AI.\n  We demonstrate the feasibility of using the generative AI model Claude 3 Opus\nin three research projects involving complex data processing tasks:\n  1) Information extraction: We extract plant species names from historical\nseedlists (catalogues of seeds) published by botanical gardens.\n  2) Natural language understanding: We extract certain data points (name of\ndrug, name of health indication, relative effectiveness, cost-effectiveness,\netc.) from documents published by Health Technology Assessment organisations in\nthe EU.\n  3) Text classification: We assign industry codes to projects on the\ncrowdfunding website Kickstarter.\n  We share the lessons we learnt from these use cases: How to determine if\ngenerative AI is an appropriate tool for a given data processing task, and if\nso, how to maximise the accuracy and consistency of the results obtained.",
      "pdf_url": "http://arxiv.org/pdf/2504.15829v1",
      "published": "2025-04-22T12:21:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15829v1",
      "categories": [
        "cs.AI",
        "68T50",
        "I.2.7"
      ]
    },
    {
      "title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers",
      "authors": [
        "Xuyang Zhong",
        "Haochen Luo",
        "Chen Liu"
      ],
      "abstract": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms.",
      "pdf_url": "http://arxiv.org/pdf/2504.15827v1",
      "published": "2025-04-22T12:18:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15827v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models",
      "authors": [
        "Songyan Xie",
        "Jinghang Wen",
        "Encheng Su",
        "Qiucheng Yu"
      ],
      "abstract": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.",
      "pdf_url": "http://arxiv.org/pdf/2504.15823v1",
      "published": "2025-04-22T12:10:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15823v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Fusing Reward and Dueling Feedback in Stochastic Bandits",
      "authors": [
        "Xuchuang Wang",
        "Qirun Zeng",
        "Jinhang Zuo",
        "Xutong Liu",
        "Mohammad Hajiesmaili",
        "John C. S. Lui",
        "Adam Wierman"
      ],
      "abstract": "This paper investigates the fusion of absolute (reward) and relative\n(dueling) feedback in stochastic bandits, where both feedback types are\ngathered in each decision round. We derive a regret lower bound, demonstrating\nthat an efficient algorithm may incur only the smaller among the reward and\ndueling-based regret for each individual arm. We propose two fusion approaches:\n(1) a simple elimination fusion algorithm that leverages both feedback types to\nexplore all arms and unifies collected information by sharing a common\ncandidate arm set, and (2) a decomposition fusion algorithm that selects the\nmore effective feedback to explore the corresponding arms and randomly assigns\none feedback type for exploration and the other for exploitation in each round.\nThe elimination fusion experiences a suboptimal multiplicative term of the\nnumber of arms in regret due to the intrinsic suboptimality of dueling\nelimination. In contrast, the decomposition fusion achieves regret matching the\nlower bound up to a constant under a common assumption. Extensive experiments\nconfirm the efficacy of our algorithms and theoretical results.",
      "pdf_url": "http://arxiv.org/pdf/2504.15812v1",
      "published": "2025-04-22T11:51:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15812v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations",
      "authors": [
        "Kai Luo",
        "Juan Tang",
        "Mingchao Cai",
        "Xiaoqing Zeng",
        "Manqi Xie",
        "Ming Yan"
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\nMulti-layer Perceptrons (MLPs) due to their superior function-fitting abilities\nin data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,\nfor solving high-index differential-algebraic equations (DAEs) by integrating\nKANs with Physics-Informed Neural Networks (PINNs). This framework not only\npreserves the ability of traditional PINNs to model complex systems governed by\nphysical laws but also enhances their performance by leveraging the\nfunction-fitting strengths of KANs. Numerical experiments demonstrate that for\nDAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute\nerrors of both differential and algebraic variables by 1 to 2 orders of\nmagnitude compared to traditional PINNs. To assess the effectiveness of this\napproach, we analyze the drift-off error and find that both PINNs and DAE-KAN\noutperform classical numerical methods in controlling this phenomenon. Our\nresults highlight the potential of neural network methods, particularly\nDAE-KAN, in solving high-index DAEs with substantial computational accuracy and\ngeneralization, offering a promising solution for challenging partial\ndifferential-algebraic equations.",
      "pdf_url": "http://arxiv.org/pdf/2504.15806v2",
      "published": "2025-04-22T11:42:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15806v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback",
      "authors": [
        "Ning Wang",
        "Bingkun Yao",
        "Jie Zhou",
        "Yuchen Hu",
        "Xi Wang",
        "Nan Guan",
        "Zhe Jiang"
      ],
      "abstract": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B.",
      "pdf_url": "http://arxiv.org/pdf/2504.15804v1",
      "published": "2025-04-22T11:38:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15804v1",
      "categories": [
        "cs.AR",
        "cs.AI"
      ]
    },
    {
      "title": "A closer look at how large language models trust humans: patterns and biases",
      "authors": [
        "Valeria Lerman",
        "Yaniv Dover"
      ],
      "abstract": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI.",
      "pdf_url": "http://arxiv.org/pdf/2504.15801v1",
      "published": "2025-04-22T11:31:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15801v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Crisp complexity of fuzzy classifiers",
      "authors": [
        "Raquel Fernandez-Peralta",
        "Javier Fumanal-Idocin",
        "Javier Andreu-Perez"
      ],
      "abstract": "Rule-based systems are a very popular form of explainable AI, particularly in\nthe fuzzy community, where fuzzy rules are widely used for control and\nclassification problems. However, fuzzy rule-based classifiers struggle to\nreach bigger traction outside of fuzzy venues, because users sometimes do not\nknow about fuzzy and because fuzzy partitions are not so easy to interpret in\nsome situations. In this work, we propose a methodology to reduce fuzzy\nrule-based classifiers to crisp rule-based classifiers. We study different\npossible crisp descriptions and implement an algorithm to obtain them. Also, we\nanalyze the complexity of the resulting crisp classifiers. We believe that our\nresults can help both fuzzy and non-fuzzy practitioners understand better the\nway in which fuzzy rule bases partition the feature space and how easily one\nsystem can be translated to another and vice versa. Our complexity metric can\nalso help to choose between different fuzzy classifiers based on what the\nequivalent crisp partitions look like.",
      "pdf_url": "http://arxiv.org/pdf/2504.15791v1",
      "published": "2025-04-22T11:06:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15791v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents",
      "authors": [
        "Siyu Zhou",
        "Tianyi Zhou",
        "Yijun Yang",
        "Guodong Long",
        "Deheng Ye",
        "Jing Jiang",
        "Chengqi Zhang"
      ],
      "abstract": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
      "pdf_url": "http://arxiv.org/pdf/2504.15785v1",
      "published": "2025-04-22T10:58:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15785v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach",
      "authors": [
        "Ruizhe Li",
        "Chiwei Zhu",
        "Benfeng Xu",
        "Xiaorui Wang",
        "Zhendong Mao"
      ],
      "abstract": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%).",
      "pdf_url": "http://arxiv.org/pdf/2504.15784v1",
      "published": "2025-04-22T10:52:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15784v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving",
      "authors": [
        "Daocheng Fu",
        "Zijun Chen",
        "Renqiu Xia",
        "Qi Liu",
        "Yuan Feng",
        "Hongbin Zhou",
        "Renrui Zhang",
        "Shiyang Feng",
        "Peng Gao",
        "Junchi Yan",
        "Botian Shi",
        "Bo Zhang",
        "Yu Qiao"
      ],
      "abstract": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen",
      "pdf_url": "http://arxiv.org/pdf/2504.15780v1",
      "published": "2025-04-22T10:45:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15780v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Shannon invariants: A scalable approach to information decomposition",
      "authors": [
        "Aaron J. Gutknecht",
        "Fernando E. Rosas",
        "David A. Ehrlich",
        "Abdullah Makkeh",
        "Pedro A. M. Mediano",
        "Michael Wibral"
      ],
      "abstract": "Distributed systems, such as biological and artificial neural networks,\nprocess information via complex interactions engaging multiple subsystems,\nresulting in high-order patterns with distinct properties across scales.\nInvestigating how these systems process information remains challenging due to\ndifficulties in defining appropriate multivariate metrics and ensuring their\nscalability to large systems. To address these challenges, we introduce a novel\nframework based on what we call \"Shannon invariants\" -- quantities that capture\nessential properties of high-order information processing in a way that depends\nonly on the definition of entropy and can be efficiently calculated for large\nsystems. Our theoretical results demonstrate how Shannon invariants can be used\nto resolve long-standing ambiguities regarding the interpretation of widely\nused multivariate information-theoretic measures. Moreover, our practical\nresults reveal distinctive information-processing signatures of various deep\nlearning architectures across layers, which lead to new insights into how these\nsystems process information and how this evolves during training. Overall, our\nframework resolves fundamental limitations in analyzing high-order phenomena\nand offers broad opportunities for theoretical developments and empirical\nanalyses.",
      "pdf_url": "http://arxiv.org/pdf/2504.15779v1",
      "published": "2025-04-22T10:41:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15779v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math.IT",
        "nlin.AO",
        "physics.data-an"
      ]
    },
    {
      "title": "Clifford Group Equivariant Diffusion Models for 3D Molecular Generation",
      "authors": [
        "Cong Liu",
        "Sharvaree Vadgama",
        "David Ruhe",
        "Erik Bekkers",
        "Patrick Forrè"
      ],
      "abstract": "This paper explores leveraging the Clifford algebra's expressive power for\n$\\E(n)$-equivariant diffusion models. We utilize the geometric products between\nClifford multivectors and the rich geometric information encoded in Clifford\nsubspaces in \\emph{Clifford Diffusion Models} (CDMs). We extend the diffusion\nprocess beyond just Clifford one-vectors to incorporate all higher-grade\nmultivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us\nto apply latent diffusion across complete multivectors. This enables CDMs to\ncapture the joint distribution across different subspaces of the algebra,\nincorporating richer geometric information through higher-order features. We\nprovide empirical results for unconditional molecular generation on the QM9\ndataset, showing that CDMs provide a promising avenue for generative modeling.",
      "pdf_url": "http://arxiv.org/pdf/2504.15773v1",
      "published": "2025-04-22T10:30:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15773v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction",
      "authors": [
        "Tobias Demmler",
        "Lennart Hartung",
        "Andreas Tamke",
        "Thao Dang",
        "Alexander Hegai",
        "Karsten Haug",
        "Lars Mikelsons"
      ],
      "abstract": "In autonomous driving, accurately predicting the movements of other traffic\nparticipants is crucial, as it significantly influences a vehicle's planning\nprocesses. Modern trajectory prediction models strive to interpret complex\npatterns and dependencies from agent and map data. The Motion Transformer (MTR)\narchitecture and subsequent work define the most accurate methods in common\nbenchmarks such as the Waymo Open Motion Benchmark. The MTR model employs\npre-generated static intention points as initial goal points for trajectory\nprediction. However, the static nature of these points frequently leads to\nmisalignment with map data in specific traffic scenarios, resulting in\nunfeasible or unrealistic goal points. Our research addresses this limitation\nby integrating scene-specific dynamic intention points into the MTR model. This\nadaptation of the MTR model was trained and evaluated on the Waymo Open Motion\nDataset. Our findings demonstrate that incorporating dynamic intention points\nhas a significant positive impact on trajectory prediction accuracy, especially\nfor predictions over long time horizons. Furthermore, we analyze the impact on\nground truth trajectories which are not compliant with the map data or are\nillegal maneuvers.",
      "pdf_url": "http://arxiv.org/pdf/2504.15766v1",
      "published": "2025-04-22T10:20:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15766v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment",
      "authors": [
        "Seung Gyu Jeong",
        "Sung Woo Nam",
        "Seong Kwan Jung",
        "Seong-Eun Kim"
      ],
      "abstract": "Respiratory auscultation is crucial for early detection of pediatric\npneumonia, a condition that can quickly worsen without timely intervention. In\nareas with limited physician access, effective auscultation is challenging. We\npresent a smartphone-based system that leverages built-in microphones and\nadvanced deep learning algorithms to detect abnormal respiratory sounds\nindicative of pneumonia risk. Our end-to-end deep learning framework employs\ndomain generalization to integrate a large electronic stethoscope dataset with\na smaller smartphone-derived dataset, enabling robust feature learning for\naccurate respiratory assessments without expensive equipment. The accompanying\nmobile application guides caregivers in collecting high-quality lung sound\nsamples and provides immediate feedback on potential pneumonia risks. User\nstudies show strong classification performance and high acceptance,\ndemonstrating the system's ability to facilitate proactive interventions and\nreduce preventable childhood pneumonia deaths. By seamlessly integrating into\nubiquitous smartphones, this approach offers a promising avenue for more\nequitable and comprehensive remote pediatric care.",
      "pdf_url": "http://arxiv.org/pdf/2504.15743v1",
      "published": "2025-04-22T09:45:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15743v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Collaborative Split Federated Learning with Parallel Training and Aggregation",
      "authors": [
        "Yiannis Papageorgiou",
        "Yannis Thomas",
        "Alexios Filippakopoulos",
        "Ramin Khalili",
        "Iordanis Koutsopoulos"
      ],
      "abstract": "Federated learning (FL) operates based on model exchanges between the server\nand the clients, and it suffers from significant client-side computation and\ncommunication burden. Split federated learning (SFL) arises a promising\nsolution by splitting the model into two parts, that are trained sequentially:\nthe clients train the first part of the model (client-side model) and transmit\nit to the server that trains the second (server-side model). Existing SFL\nschemes though still exhibit long training delays and significant communication\noverhead, especially when clients of different computing capability\nparticipate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a\nnovel scheme that splits the model into three parts, namely the model parts\ntrained at the computationally weak clients, the ones trained at the\ncomputationally strong clients, and the ones at the server. Unlike existing\nworks, C-SFL enables parallel training and aggregation of model's parts at the\nclients and at the server, resulting in reduced training delays and\ncommmunication overhead while improving the model's accuracy. Experiments\nverify the multiple gains of C-SFL against the existing schemes.",
      "pdf_url": "http://arxiv.org/pdf/2504.15724v1",
      "published": "2025-04-22T09:18:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15724v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences",
      "authors": [
        "Anna Karnysheva",
        "Christian Drescher",
        "Dietrich Klakow"
      ],
      "abstract": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain.",
      "pdf_url": "http://arxiv.org/pdf/2504.15719v1",
      "published": "2025-04-22T09:08:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.15719v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}