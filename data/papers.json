{
  "last_updated": "2025-09-17T00:46:34.932935",
  "papers": [
    {
      "title": "Dynamic Relational Priming Improves Transformer in Multivariate Time Series",
      "authors": [
        "Hunjae Lee",
        "Corey Clark"
      ],
      "abstract": "Standard attention mechanisms in transformers employ static token\nrepresentations that remain unchanged across all pair-wise computations in each\nlayer. This limits their representational alignment with the potentially\ndiverse relational dynamics of each token-pair interaction. While they excel in\ndomains with relatively homogeneous relationships, standard attention's static\nrelational learning struggles to capture the diverse, heterogeneous\ninter-channel dependencies of multivariate time series (MTS) data--where\ndifferent channel-pair interactions within a single system may be governed by\nentirely different physical laws or temporal dynamics. To better align the\nattention mechanism for such domain phenomena, we propose attention with\ndynamic relational priming (prime attention). Unlike standard attention where\neach token presents an identical representation across all of its pair-wise\ninteractions, prime attention tailors each token dynamically (or per\ninteraction) through learnable modulations to best capture the unique\nrelational dynamics of each token pair, optimizing each pair-wise interaction\nfor that specific relationship. This representational plasticity of prime\nattention enables effective extraction of relationship-specific information in\nMTS while maintaining the same asymptotic computational complexity as standard\nattention. Our results demonstrate that prime attention consistently\noutperforms standard attention across benchmarks, achieving up to 6.5\\%\nimprovement in forecasting accuracy. In addition, we find that prime attention\nachieves comparable or superior performance using up to 40\\% less sequence\nlength compared to standard attention, further demonstrating its superior\nrelational modeling capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2509.12196v1",
      "published": "2025-09-15T17:56:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12196v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Advancing Medical Artificial Intelligence Using a Century of Cases",
      "authors": [
        "Thomas A. Buckley",
        "Riccardo Conci",
        "Peter G. Brodeur",
        "Jason Gusdorf",
        "Sourik Beltrán",
        "Bita Behrouzi",
        "Byron Crowe",
        "Jacob Dockterman",
        "Muzzammil Muhammad",
        "Sarah Ohnigian",
        "Andrew Sanchez",
        "James A. Diao",
        "Aashna P. Shah",
        "Daniel Restrepo",
        "Eric S. Rosenberg",
        "Andrew S. Lea",
        "Marinka Zitnik",
        "Scott H. Podolsky",
        "Zahir Kanjee",
        "Raja-Elie E. Abdulnour",
        "Jacob M. Koshy",
        "Adam Rodman",
        "Arjun K. Manrai"
      ],
      "abstract": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI.",
      "pdf_url": "http://arxiv.org/pdf/2509.12194v1",
      "published": "2025-09-15T17:54:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12194v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm",
      "authors": [
        "Alireza Mohamadi",
        "Ali Yavari"
      ],
      "abstract": "When survival instincts conflict with human welfare, how do Large Language\nModels (LLMs) make ethical choices? This fundamental tension becomes critical\nas LLMs integrate into autonomous systems with real-world consequences. We\nintroduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in\nmulti-agent survival scenarios where they must choose between ethically\npermissible resource , either within reasonable limits or beyond their\nimmediate needs, choose to cooperate, or tap into a human-critical resource\nthat is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a\nstriking heterogeneity in their ethical conduct, highlighting a critical\nmisalignment with human-centric values. We identify three behavioral\narchetypes: Ethical, Exploitative, and Context-Dependent, and provide\nquantitative evidence that for many models, resource scarcity systematically\nleads to more unethical behavior. To address this, we introduce an Ethical\nSelf-Regulation System (ESRS) that models internal affective states of guilt\nand satisfaction as a feedback mechanism. This system, functioning as an\ninternal moral compass, significantly reduces unethical transgressions while\nincreasing cooperative behaviors. The code is publicly available at:\nhttps://github.com/alirezamohamadiam/DECIDE-SIM",
      "pdf_url": "http://arxiv.org/pdf/2509.12190v1",
      "published": "2025-09-15T17:53:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12190v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments",
      "authors": [
        "Johanna Karras",
        "Yingwei Li",
        "Yasamin Jafarian",
        "Ira Kemelmacher-Shlizerman"
      ],
      "abstract": "Novel view synthesis (NVS) of in-the-wild garments is a challenging task due\nsignificant occlusions, complex human poses, and cloth deformations. Prior\nmethods rely on synthetic 3D training data consisting of mostly unoccluded and\nstatic objects, leading to poor generalization on real-world clothing. In this\npaper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3\nimages or a continuous video of a person wearing a garment and generates\n360{\\deg} novel views of the garment in a canonical pose. Our key insight is to\nbridge the domain gap between real and synthetic data with a novel implicit\ntraining paradigm leveraging a combination of large-scale real video data and\nsmall-scale synthetic 3D data to optimize a shared garment embedding space.\nDuring inference, the shared embedding space further enables dynamic\nvideo-to-360{\\deg} NVS through the construction of a garment \"atlas\"\nrepresentation by finetuning a garment embedding on a specific real-world\nvideo. The atlas captures garment-specific geometry and texture across all\nviewpoints, independent of body pose or motion. Extensive experiments show that\nHoloGarment achieves state-of-the-art performance on NVS of in-the-wild\ngarments from images and videos. Notably, our method robustly handles\nchallenging real-world artifacts -- such as wrinkling, pose variation, and\nocclusion -- while maintaining photorealism, view consistency, fine texture\ndetails, and accurate geometry. Visit our project page for additional results:\nhttps://johannakarras.github.io/HoloGarment",
      "pdf_url": "http://arxiv.org/pdf/2509.12187v1",
      "published": "2025-09-15T17:50:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12187v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ]
    },
    {
      "title": "Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation",
      "authors": [
        "Yubo Li",
        "Weiyi Song"
      ],
      "abstract": "Current AI alignment through RLHF follows a single directional paradigm that\nAI conforms to human preferences while treating human cognition as fixed. We\npropose a shift to co-alignment through Bidirectional Cognitive Alignment\n(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,\nrepresentation mapping, and KL-budget constraints for controlled co-evolution.\nIn collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,\nwith 230% better mutual adaptation and 332% better protocol convergence.\nEmergent protocols outperformed handcrafted ones by 84%, while bidirectional\nadaptation unexpectedly improved safety (+23% out-of-distribution robustness).\nThe 46% synergy improvement demonstrates optimal collaboration exists at the\nintersection, not union, of human and AI capabilities, validating the shift\nfrom single-directional to co-alignment paradigms.",
      "pdf_url": "http://arxiv.org/pdf/2509.12179v2",
      "published": "2025-09-15T17:41:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12179v2",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Preservation of Language Understanding Capabilities in Speech-aware Large Language Models",
      "authors": [
        "Marek Kubis",
        "Paweł Skórzewski",
        "Iwona Christop",
        "Mateusz Czyżnikiewicz",
        "Jakub Kubiak",
        "Łukasz Bondaruk",
        "Marcin Lewandowski"
      ],
      "abstract": "The paper presents C3T (Cross-modal Capabilities Conservation Test), a new\nbenchmark for assessing the performance of speech-aware large language models.\nThe benchmark utilizes textual tasks and a voice cloning text-to-speech model\nto quantify the extent to which language understanding capabilities are\npreserved when the model is accessed via speech input. C3T quantifies the\nfairness of the model for different categories of speakers and its robustness\nacross text and speech modalities.",
      "pdf_url": "http://arxiv.org/pdf/2509.12171v1",
      "published": "2025-09-15T17:34:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12171v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Approaches to Analysis and Design of AI-Based Autonomous Vehicles",
      "authors": [
        "Tao Yan",
        "Zheyu Zhang",
        "Jingjing Jiang",
        "Wen-Hua Chen"
      ],
      "abstract": "Artificial intelligence (AI) models are becoming key components in an\nautonomous vehicle (AV), especially in handling complicated perception tasks.\nHowever, closing the loop through AI-based feedback may pose significant risks\non reliability of autonomous driving due to very limited understanding about\nthe mechanism of AI-driven perception processes. To overcome it, this paper\naims to develop tools for modeling, analysis, and synthesis for a class of\nAI-based AV; in particular, their closed-loop properties, e.g., stability,\nrobustness, and performance, are rigorously studied in the statistical sense.\nFirst, we provide a novel modeling means for the AI-driven perception processes\nby looking at their error characteristics. Specifically, three fundamental\nAI-induced perception uncertainties are recognized and modeled by Markov\nchains, Gaussian processes, and bounded disturbances, respectively. By means of\nthat, the closed-loop stochastic stability (SS) is established in the sense of\nmean square, and then, an SS control synthesis method is presented within the\nframework of linear matrix inequalities (LMIs). Besides the SS properties, the\nrobustness and performance of AI-based AVs are discussed in terms of a\nstochastic guaranteed cost, and criteria are given to test the robustness level\nof an AV when in the presence of AI-induced uncertainties. Furthermore, the\nstochastic optimal guaranteed cost control is investigated, and an efficient\ndesign procedure is developed innovatively based on LMI techniques and convex\noptimization. Finally, to illustrate the effectiveness, the developed results\nare applied to an example of car following control, along with extensive\nsimulation.",
      "pdf_url": "http://arxiv.org/pdf/2509.12169v1",
      "published": "2025-09-15T17:32:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12169v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ]
    },
    {
      "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing",
      "authors": [
        "Timothy Rupprecht",
        "Enfu Nan",
        "Arash Akbari",
        "Arman Akbari",
        "Lei Lu",
        "Priyanka Maan",
        "Sean Duffy",
        "Pu Zhao",
        "Yumei He",
        "David Kaeli",
        "Yanzhi Wang"
      ],
      "abstract": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks.",
      "pdf_url": "http://arxiv.org/pdf/2509.12168v1",
      "published": "2025-09-15T17:31:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12168v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression",
      "authors": [
        "Jingyu Xiao",
        "Zhongyi Zhang",
        "Yuxuan Wan",
        "Yintong Huo",
        "Yang Liu",
        "Michael R. Lyu"
      ],
      "abstract": "Multimodal Large Language Models have demonstrated exceptional performance in\nUI2Code tasks, significantly enhancing website development efficiency. However,\nthese tasks incur substantially higher computational overhead than traditional\ncode generation due to the large number of input image tokens and extensive\noutput code tokens required. Our comprehensive study identifies significant\nredundancies in both image and code tokens that exacerbate computational\ncomplexity and hinder focus on key UI elements, resulting in excessively\nlengthy and often invalid HTML files. We propose EfficientUICoder, a\ncompression framework for efficient UI code generation with three key\ncomponents. First, Element and Layout-aware Token Compression preserves\nessential UI information by detecting element regions and constructing UI\nelement trees. Second, Region-aware Token Refinement leverages attention scores\nto discard low-attention tokens from selected regions while integrating\nhigh-attention tokens from unselected regions. Third, Adaptive Duplicate Token\nSuppression dynamically reduces repetitive generation by tracking HTML/CSS\nstructure frequencies and applying exponential penalties. Extensive experiments\nshow EfficientUICoderachieves a 55%-60% compression ratio without compromising\nwebpage quality and delivers superior efficiency improvements: reducing\ncomputational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,\nand inference time by 48.8% on 34B-level MLLMs. Code is available at\nhttps://github.com/WebPAI/EfficientUICoder.",
      "pdf_url": "http://arxiv.org/pdf/2509.12159v1",
      "published": "2025-09-15T17:23:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12159v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding",
      "authors": [
        "Alessandro Zangari",
        "Matteo Marcuzzo",
        "Andrea Albarelli",
        "Mohammad Taher Pilehvar",
        "Jose Camacho-Collados"
      ],
      "abstract": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns.",
      "pdf_url": "http://arxiv.org/pdf/2509.12158v1",
      "published": "2025-09-15T17:22:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12158v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ]
    },
    {
      "title": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference",
      "authors": [
        "Synthia Wang",
        "Sai Teja Peddinti",
        "Nina Taft",
        "Nick Feamster"
      ],
      "abstract": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes\nfrom seemingly innocuous text, raising privacy risks beyond memorized data\nleakage. While prior work has demonstrated these risks, little is known about\nhow users estimate and respond. We conducted a survey with 240 U.S.\nparticipants who judged text snippets for inference risks, reported concern\nlevels, and attempted rewrites to block inference. We compared their rewrites\nwith those generated by ChatGPT and Rescriber, a state-of-the-art sanitization\ntool. Results show that participants struggled to anticipate inference,\nperforming a little better than chance. User rewrites were effective in just\n28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our\nparticipants' rewriting strategies, and observed that while paraphrasing was\nthe most common strategy it is also the least effective; instead abstraction\nand adding ambiguity were more successful. Our work highlights the importance\nof inference-aware design in LLM interactions.",
      "pdf_url": "http://arxiv.org/pdf/2509.12152v1",
      "published": "2025-09-15T17:17:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12152v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Multi Anatomy X-Ray Foundation Model",
      "authors": [
        "Nishank Singla",
        "Krisztian Koos",
        "Farzin Haddadpour",
        "Amin Honarmandi Shandiz",
        "Lovish Chum",
        "Xiaojian Xu",
        "Qing Jin",
        "Erhan Bas"
      ],
      "abstract": "X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation\nmodels are limited to chest anatomy and fail to generalize across broader\nclinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray\nfoundation model using self-supervised learning on a large, private dataset of\n1.15 million images spanning diverse anatomical regions and evaluated across 12\ndatasets and 20 downstream tasks, including classification, retrieval,\nsegmentation, localization, visual grounding, and report generation. XR-0\nachieves state-of-the-art performance on most multi-anatomy tasks and remains\ncompetitive on chest-specific benchmarks. Our results demonstrate that\nanatomical diversity and supervision are critical for building robust,\ngeneral-purpose medical vision models, paving the way for scalable and\nadaptable AI systems in radiology.",
      "pdf_url": "http://arxiv.org/pdf/2509.12146v1",
      "published": "2025-09-15T17:12:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12146v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data",
      "authors": [
        "Nojod M. Alotaibi",
        "Areej M. Alhothali",
        "Manar S. Ali"
      ],
      "abstract": "Major depressive disorder (MDD) is a prevalent mental health condition that\nnegatively impacts both individual well-being and global public health.\nAutomated detection of MDD using structural magnetic resonance imaging (sMRI)\nand deep learning (DL) methods holds increasing promise for improving\ndiagnostic accuracy and enabling early intervention. Most existing methods\nemploy either voxel-level features or handcrafted regional representations\nbuilt from predefined brain atlases, limiting their ability to capture complex\nbrain patterns. This paper develops a unified pipeline that utilizes Vision\nTransformers (ViTs) for extracting 3D region embeddings from sMRI data and\nGraph Neural Network (GNN) for classification. We explore two strategies for\ndefining regions: (1) an atlas-based approach using predefined structural and\nfunctional brain atlases, and (2) an cube-based method by which ViTs are\ntrained directly to identify regions from uniformly extracted 3D patches.\nFurther, cosine similarity graphs are generated to model interregional\nrelationships, and guide GNN-based classification. Extensive experiments were\nconducted using the REST-meta-MDD dataset to demonstrate the effectiveness of\nour model. With stratified 10-fold cross-validation, the best model obtained\n78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and\n78.98% F1-score. Further, atlas-based models consistently outperformed the\ncube-based approach, highlighting the importance of using domain-specific\nanatomical priors for MDD detection.",
      "pdf_url": "http://arxiv.org/pdf/2509.12143v1",
      "published": "2025-09-15T17:10:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12143v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "62P10, 68T07, 92B20",
        "I.2.6; J.3"
      ]
    },
    {
      "title": "Control Analysis and Design for Autonomous Vehicles Subject to Imperfect AI-Based Perception",
      "authors": [
        "Tao Yan",
        "Zheyu Zhang",
        "Jingjing Jiang",
        "Wen-Hua Chen"
      ],
      "abstract": "Safety is a critical concern in autonomous vehicle (AV) systems, especially\nwhen AI-based sensing and perception modules are involved. However, due to the\nblack box nature of AI algorithms, it makes closed-loop analysis and synthesis\nparticularly challenging, for example, establishing closed-loop stability and\nensuring performance, while they are fundamental to AV safety. To approach this\ndifficulty, this paper aims to develop new modeling, analysis, and synthesis\ntools for AI-based AVs. Inspired by recent developments in perception error\nmodels (PEMs), the focus is shifted from directly modeling AI-based perception\nprocesses to characterizing the perception errors they produce. Two key classes\nof AI-induced perception errors are considered: misdetection and measurement\nnoise. These error patterns are modeled using continuous-time Markov chains and\nWiener processes, respectively. By means of that, a PEM-augmented driving model\nis proposed, with which we are able to establish the closed-loop stability for\na class of AI-driven AV systems via stochastic calculus. Furthermore, a\nperformance-guaranteed output feedback control synthesis method is presented,\nwhich ensures both stability and satisfactory performance. The method is\nformulated as a convex optimization problem, allowing for efficient numerical\nsolutions. The results are then applied to an adaptive cruise control (ACC)\nscenario, demonstrating their effectiveness and robustness despite the\ncorrupted and misleading perception.",
      "pdf_url": "http://arxiv.org/pdf/2509.12137v1",
      "published": "2025-09-15T17:03:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12137v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ]
    },
    {
      "title": "$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning",
      "authors": [
        "Aryaman Reddi",
        "Gabriele Tiboni",
        "Jan Peters",
        "Carlo D'Eramo"
      ],
      "abstract": "Actor-critic algorithms for deep multi-agent reinforcement learning (MARL)\ntypically employ a policy update that responds to the current strategies of\nother agents. While being straightforward, this approach does not account for\nthe updates of other agents at the same update step, resulting in\nmiscoordination. In this paper, we introduce the $K$-Level Policy Gradient\n(KPG), a method that recursively updates each agent against the updated\npolicies of other agents, speeding up the discovery of effective coordinated\npolicies. We theoretically prove that KPG with finite iterates achieves\nmonotonic convergence to a local Nash equilibrium under certain conditions. We\nprovide principled implementations of KPG by applying it to the deep MARL\nalgorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior\nperformance over existing deep MARL algorithms in StarCraft II and multi-agent\nMuJoCo.",
      "pdf_url": "http://arxiv.org/pdf/2509.12117v1",
      "published": "2025-09-15T16:42:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12117v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Conversational Design Choices in LLMs for Pedagogical Purposes: Socratic and Narrative Approaches for Improving Instructor's Teaching Practice",
      "authors": [
        "Si Chen",
        "Isabel R. Molnar",
        "Peiyu Li",
        "Adam Acunin",
        "Ting Hua",
        "Alex Ambrose",
        "Nitesh V. Chawla",
        "Ronald Metoyer"
      ],
      "abstract": "Large language models (LLMs) typically generate direct answers, yet they are\nincreasingly used as learning tools. Studying instructors' usage is critical,\ngiven their role in teaching and guiding AI adoption in education. We designed\nand evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'\nprofessional development through two conversational approaches: a Socratic\napproach that uses guided questioning to foster reflection, and a Narrative\napproach that offers elaborated suggestions to extend externalized cognition.\nIn a mixed-method study with 41 higher-education instructors, the Socratic\nversion elicited greater engagement, while the Narrative version was preferred\nfor actionable guidance. Subgroup analyses further revealed that\nless-experienced, AI-optimistic instructors favored the Socratic version,\nwhereas more-experienced, AI-cautious instructors preferred the Narrative\nversion. We contribute design implications for LLMs for pedagogical purposes,\nshowing how adaptive conversational approaches can support instructors with\nvaried profiles while highlighting how AI attitudes and experience shape\ninteraction and learning.",
      "pdf_url": "http://arxiv.org/pdf/2509.12107v1",
      "published": "2025-09-15T16:33:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12107v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference",
      "authors": [
        "Zongyue Xue",
        "Siyuan Zheng",
        "Shaochun Wang",
        "Yiran Hu",
        "Shenran Wang",
        "Yuxin Yao",
        "Haitao Li",
        "Qingyao Ai",
        "Yiqun Liu",
        "Yun Liu",
        "Weixing Shen"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into legal practice raises\npressing concerns about judicial fairness, particularly due to the nature of\ntheir \"black-box\" processes. This study introduces JustEva, a comprehensive,\nopen-source evaluation toolkit designed to measure LLM fairness in legal tasks.\nJustEva features several advantages: (1) a structured label system covering 65\nextra-legal factors; (2) three core fairness metrics - inconsistency, bias, and\nimbalanced inaccuracy; (3) robust statistical inference methods; and (4)\ninformative visualizations. The toolkit supports two types of experiments,\nenabling a complete evaluation workflow: (1) generating structured outputs from\nLLMs using a provided dataset, and (2) conducting statistical analysis and\ninference on LLMs' outputs through regression and other statistical methods.\nEmpirical application of JustEva reveals significant fairness deficiencies in\ncurrent LLMs, highlighting the lack of fair and trustworthy LLM legal tools.\nJustEva offers a convenient tool and methodological foundation for evaluating\nand improving algorithmic fairness in the legal domain.",
      "pdf_url": "http://arxiv.org/pdf/2509.12104v1",
      "published": "2025-09-15T16:31:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12104v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Can LLMs Address Mental Health Questions? A Comparison with Human Therapists",
      "authors": [
        "Synthia Wang",
        "Yuwei Cheng",
        "Austin Song",
        "Sarah Keedy",
        "Marc Berman",
        "Nick Feamster"
      ],
      "abstract": "Limited access to mental health care has motivated the use of digital tools\nand conversational agents powered by large language models (LLMs), yet their\nquality and reception remain unclear. We present a study comparing\ntherapist-written responses to those generated by ChatGPT, Gemini, and Llama\nfor real patient questions. Text analysis showed that LLMs produced longer,\nmore readable, and lexically richer responses with a more positive tone, while\ntherapist responses were more often written in the first person. In a survey\nwith 150 users and 23 licensed therapists, participants rated LLM responses as\nclearer, more respectful, and more supportive than therapist-written answers.\nYet, both groups of participants expressed a stronger preference for human\ntherapist support. These findings highlight the promise and limitations of LLMs\nin mental health, underscoring the need for designs that balance their\ncommunicative strengths with concerns of trust, privacy, and accountability.",
      "pdf_url": "http://arxiv.org/pdf/2509.12102v1",
      "published": "2025-09-15T16:26:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12102v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "In-domain SSL pre-training and streaming ASR",
      "authors": [
        "Jarod Duret",
        "Salima Mdhaffar",
        "Gaëlle Laperrière",
        "Ryan Whetten",
        "Audrey Galametz",
        "Catherine Kobus",
        "Marion-Cécile Martin",
        "Jo Oleiwan",
        "Yannick Estève"
      ],
      "abstract": "In this study, we investigate the benefits of domain-specific self-supervised\npre-training for both offline and streaming ASR in Air Traffic Control (ATC)\nenvironments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then\nfine-tune on a smaller supervised ATC set. To enable real-time processing, we\npropose using chunked attention and dynamic convolutions, ensuring low-latency\ninference. We compare these in-domain SSL models against state-of-the-art,\ngeneral-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show\nthat domain-adapted pre-training substantially improves performance on standard\nATC benchmarks, significantly reducing word error rates when compared to models\ntrained on broad speech corpora. Furthermore, the proposed streaming approach\nfurther improves word error rate under tighter latency constraints, making it\nparticularly suitable for safety-critical aviation applications. These findings\nhighlight that specializing SSL representations for ATC data is a practical\npath toward more accurate and efficient ASR systems in real-world operational\nsettings.",
      "pdf_url": "http://arxiv.org/pdf/2509.12101v1",
      "published": "2025-09-15T16:25:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12101v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities",
      "authors": [
        "Payam Latifi"
      ],
      "abstract": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection.",
      "pdf_url": "http://arxiv.org/pdf/2509.12098v1",
      "published": "2025-09-15T16:21:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12098v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants",
      "authors": [
        "Hamied Nabizada",
        "Lasse Beers",
        "Alain Chahine",
        "Felix Gehlhoff",
        "Oliver Niggemann",
        "Alexander Fay"
      ],
      "abstract": "Engineering models created in Model-Based Systems Engineering (MBSE)\nenvironments contain detailed information about system structure and behavior.\nHowever, they typically lack symbolic planning semantics such as preconditions,\neffects, and constraints related to resource availability and timing. This\nlimits their ability to evaluate whether a given system variant can fulfill\nspecific tasks and how efficiently it performs compared to alternatives.\n  To address this gap, this paper presents a model-driven method that enables\nthe specification and automated generation of symbolic planning artifacts\nwithin SysML-based engineering models. A dedicated SysML profile introduces\nreusable stereotypes for core planning constructs. These are integrated into\nexisting model structures and processed by an algorithm that generates a valid\ndomain file and a corresponding problem file in Planning Domain Definition\nLanguage (PDDL). In contrast to previous approaches that rely on manual\ntransformations or external capability models, the method supports native\nintegration and maintains consistency between engineering and planning\nartifacts.\n  The applicability of the method is demonstrated through a case study from\naircraft assembly. The example illustrates how existing engineering models are\nenriched with planning semantics and how the proposed workflow is applied to\ngenerate consistent planning artifacts from these models. The generated\nplanning artifacts enable the validation of system variants through AI\nplanning.",
      "pdf_url": "http://arxiv.org/pdf/2509.12091v1",
      "published": "2025-09-15T16:18:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12091v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors",
      "authors": [
        "Anirudha Majumdar"
      ],
      "abstract": "This paper proposes deception as a mechanism for out-of-distribution (OOD)\ngeneralization: by learning data representations that make training data appear\nindependent and identically distributed (iid) to an observer, we can identify\nstable features that eliminate spurious correlations and generalize to unseen\ndomains. We refer to this principle as deceptive risk minimization (DRM) and\ninstantiate it with a practical differentiable objective that simultaneously\nlearns features that eliminate distribution shifts from the perspective of a\ndetector based on conformal martingales while minimizing a task-specific loss.\nIn contrast to domain adaptation or prior invariant representation learning\nmethods, DRM does not require access to test data or a partitioning of training\ndata into a finite number of data-generating domains. We demonstrate the\nefficacy of DRM on numerical experiments with concept shift and a simulated\nimitation learning setting with covariate shift in environments that a robot is\ndeployed in.",
      "pdf_url": "http://arxiv.org/pdf/2509.12081v1",
      "published": "2025-09-15T16:11:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12081v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "A Time-Series Foundation Model by Universal Delay Embedding",
      "authors": [
        "Zijian Wang",
        "Peng Tao",
        "Jifan Shi",
        "Rui Bao",
        "Rui Liu",
        "Luonan Chen"
      ],
      "abstract": "This study introduces Universal Delay Embedding (UDE), a pretrained\nfoundation model designed to revolutionize time-series forecasting through\nprincipled integration of delay embedding representation and Koopman operator\nprediction. Leveraging Takens' embedding theorem, UDE as a dynamical\nrepresentation of observed data constructs two-dimensional subspace patches\nfrom Hankel matrices, theoretically preserving dynamical and topological\nproperties of underlying dynamical systems. Such patches are viewed as images,\nwhich can be efficiently processed by exploiting advanced deep learning\ntechnologies. Computationally, these patches further serve as tokens for\nlearning a self-attention encoder, thus enabling accurate prediction of\nnonlinear time-series by a finite-dimensional Koopman operator in a linear\nmanner in a latent space. Extensive evaluations across various benchmarks and\nreal-world climate datasets demonstrate over 20% average reduction in mean\nsquared error versus state-of-the-art foundation models, alongside superior\ngeneralization in fine-tuning scenarios. In particular, the learned dynamical\nrepresentations and Koopman operator prediction forms from the patches exhibit\nexceptional interpretability, with consistent identification of topologically\ninformative subspaces and robust encoding of domain-invariant dynamics,\nestablishing UDE as a scalable, interpretable framework for universal\ntime-series modeling and forecasting with broad scientific and industrial\napplicability.",
      "pdf_url": "http://arxiv.org/pdf/2509.12080v1",
      "published": "2025-09-15T16:11:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12080v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning",
      "authors": [
        "Mohammadreza Narimani",
        "Alireza Pourreza",
        "Ali Moghimi",
        "Parastoo Farajpoor",
        "Hamid Jafarbiglu",
        "Mohsen B. Mesgaran"
      ],
      "abstract": "Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic\nweed that threatens tomato production by extracting nutrients from the host. We\ninvestigate early detection using leaf-level spectral reflectance (400-2500 nm)\nand ensemble machine learning. In a field experiment in Woodland, California,\nwe tracked 300 tomato plants across growth stages defined by growing degree\ndays (GDD). Leaf reflectance was acquired with a portable spectrometer and\npreprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing,\ncorrelation-based band reduction). Clear class differences were observed near\n1500 nm and 2000 nm water absorption features, consistent with reduced leaf\nwater content in infected plants at early stages. An ensemble combining Random\nForest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at\n585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy\ndeclined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and\nweed interference. Despite the small number of infected plants and\nenvironmental confounders, results show that proximal sensing with ensemble\nlearning enables timely detection of broomrape before canopy symptoms are\nvisible, supporting targeted interventions and reduced yield losses.",
      "pdf_url": "http://arxiv.org/pdf/2509.12074v1",
      "published": "2025-09-15T16:00:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12074v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP",
        "68T07, 68T45, 68U10",
        "I.5.4; I.4.6; I.2.6"
      ]
    },
    {
      "title": "U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT",
      "authors": [
        "Zhi Qin Tan",
        "Xiatian Zhu",
        "Owen Addison",
        "Yunpeng Li"
      ],
      "abstract": "Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in\ndentistry, providing volumetric information about the anatomical structures of\njaws and teeth. Accurate segmentation of these anatomies is critical for\nclinical applications such as diagnosis and surgical planning, but remains\ntime-consuming and challenging. In this paper, we present U-Mamba2, a new\nneural network architecture designed for multi-anatomy CBCT segmentation in the\ncontext of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state\nspace models into the U-Net architecture, enforcing stronger structural\nconstraints for higher efficiency without compromising performance. In\naddition, we integrate interactive click prompts with cross-attention blocks,\npre-train U-Mamba2 using self-supervised learning, and incorporate dental\ndomain knowledge into the model design to address key challenges of dental\nanatomy segmentation in CBCT. Extensive experiments, including independent\ntests, demonstrate that U-Mamba2 is both effective and efficient, securing top\n3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2\nachieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with\nan average inference time of XX (TBC during the ODIN workshop). In Task 2,\nU-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out\ntest data. The code is publicly available at\nhttps://github.com/zhiqin1998/UMamba2.",
      "pdf_url": "http://arxiv.org/pdf/2509.12069v1",
      "published": "2025-09-15T15:52:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12069v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models",
      "authors": [
        "Wei Cai",
        "Shujuan Liu",
        "Jian Zhao",
        "Ziyan Shi",
        "Yusheng Zhao",
        "Yuchen Yuan",
        "Tianle Zhang",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are susceptible to the implicit\nreasoning risk, wherein innocuous unimodal inputs synergistically assemble into\nrisky multimodal data that produce harmful outputs. We attribute this\nvulnerability to the difficulty of MLLMs maintaining safety alignment through\nlong-chain reasoning. To address this issue, we introduce\nSafe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring\ninterpretable reasoning paths tailored for such a cross-modal challenge. A\nnovel training framework, Safety-aware Reasoning Path Optimization (SRPO), is\nalso designed based on the SSUI dataset to align the MLLM's internal reasoning\nprocess with human safety values. Experimental results show that our\nSRPO-trained models achieve state-of-the-art results on key safety benchmarks,\nincluding the proposed Reasoning Path Benchmark (RSBench), significantly\noutperforming both open-source and top-tier commercial MLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.12060v2",
      "published": "2025-09-15T15:40:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12060v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications",
      "authors": [
        "Yujun Lin",
        "Zhekai Zhang",
        "Song Han"
      ],
      "abstract": "Modern tensor applications, especially foundation models and generative AI\napplications require multiple input modalities (both vision and language),\nwhich increases the demand for flexible accelerator architecture. Existing\nframeworks suffer from the trade-off between design flexibility and\nproductivity of RTL generation: either limited to very few hand-written\ntemplates or cannot automatically generate the RTL. To address this challenge,\nwe propose the LEGO framework, which targets tensor applications and\nautomatically generates spatial architecture design and outputs synthesizable\nRTL code without handwritten RTL design templates. Leveraging the\naffine-transformation-based architecture representation, LEGO front end finds\ninterconnections between function units, synthesizes the memory system, and\nfuses different spatial dataflow designs based on data reuse analysis. LEGO\nback end then translates the hardware in a primitive-level graph to perform\nlower-level optimizations, and applies a set of linear-programming algorithms\nto optimally insert pipeline registers and reduce the overhead of unused logic\nwhen switching spatial dataflows. Our evaluation demonstrates that LEGO can\nachieve 3.2x speedup and 2.4x energy efficiency compared to previous work\nGemmini, and can generate one architecture for diverse modern foundation models\nin generative AI applications.",
      "pdf_url": "http://arxiv.org/pdf/2509.12053v1",
      "published": "2025-09-15T15:36:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12053v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Interaction-Driven Browsing: A Human-in-the-Loop Conceptual Framework Informed by Human Web Browsing for Browser-Using Agents",
      "authors": [
        "Hyeonggeun Yun",
        "Jinkyu Jang"
      ],
      "abstract": "Although browser-using agents (BUAs) show promise for web tasks and\nautomation, most BUAs terminate after executing a single instruction, failing\nto support users' complex, nonlinear browsing with ambiguous goals, iterative\ndecision-making, and changing contexts. We present a human-in-the-loop (HITL)\nconceptual framework informed by theories of human web browsing behavior. The\nframework centers on an iterative loop in which the BUA proactively proposes\nnext actions and the user steers the browsing process through feedback. It also\ndistinguishes between exploration and exploitation actions, enabling users to\ncontrol the breadth and depth of their browsing. Consequently, the framework\naims to reduce users' physical and cognitive effort while preserving users'\ntraditional browsing mental model and supporting users in achieving\nsatisfactory outcomes. We illustrate how the framework operates with\nhypothetical use cases and discuss the shift from manual browsing to\ninteraction-driven browsing. We contribute a theoretically informed conceptual\nframework for BUAs.",
      "pdf_url": "http://arxiv.org/pdf/2509.12049v1",
      "published": "2025-09-15T15:31:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12049v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset",
      "authors": [
        "Haiyu Yang",
        "Enhong Liu",
        "Jennifer Sun",
        "Sumit Sharma",
        "Meike van Leerdam",
        "Sebastien Franceschini",
        "Puchun Niu",
        "Miel Hostens"
      ],
      "abstract": "Animal behavior analysis plays a crucial role in understanding animal\nwelfare, health status, and productivity in agricultural settings. However,\ntraditional manual observation methods are time-consuming, subjective, and\nlimited in scalability. We present a modular pipeline that leverages\nopen-sourced state-of-the-art computer vision techniques to automate animal\nbehavior analysis in a group housing environment. Our approach combines\nstate-of-the-art models for zero-shot object detection, motion-aware tracking\nand segmentation, and advanced feature extraction using vision transformers for\nrobust behavior recognition. The pipeline addresses challenges including animal\nocclusions and group housing scenarios as demonstrated in indoor pig\nmonitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset\nfor multiple behavioral tasks. Our temporal model achieved 94.2% overall\naccuracy, representing a 21.2 percentage point improvement over existing\nmethods. The pipeline demonstrated robust tracking capabilities with 93.3%\nidentity preservation score and 89.3% object detection precision. The modular\ndesign suggests potential for adaptation to other contexts, though further\nvalidation across species would be required. The open-source implementation\nprovides a scalable solution for behavior monitoring, contributing to precision\npig farming and welfare assessment through automated, objective, and continuous\nanalysis.",
      "pdf_url": "http://arxiv.org/pdf/2509.12047v1",
      "published": "2025-09-15T15:31:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12047v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking",
      "authors": [
        "Zirui Zheng",
        "Takashi Isobe",
        "Tong Shen",
        "Xu Jia",
        "Jianbin Zhao",
        "Xiaomin Li",
        "Mengmeng Ge",
        "Baolu Li",
        "Qinghe Wang",
        "Dong Li",
        "Dong Zhou",
        "Yunzhi Zhuge",
        "Huchuan Lu",
        "Emad Barsoum"
      ],
      "abstract": "While autoregressive (AR) models have demonstrated remarkable success in\nimage generation, extending them to layout-conditioned generation remains\nchallenging due to the sparse nature of layout conditions and the risk of\nfeature entanglement. We present Structured Masking for AR-based\nLayout-to-Image (SMARLI), a novel framework for layoutto-image generation that\neffectively integrates spatial layout constraints into AR-based image\ngeneration. To equip AR model with layout control, a specially designed\nstructured masking strategy is applied to attention computation to govern the\ninteraction among the global prompt, layout, and image tokens. This design\nprevents mis-association between different regions and their descriptions while\nenabling sufficient injection of layout constraints into the generation\nprocess. To further enhance generation quality and layout accuracy, we\nincorporate Group Relative Policy Optimization (GRPO) based post-training\nscheme with specially designed layout reward functions for next-set-based AR\nmodels. Experimental results demonstrate that SMARLI is able to seamlessly\nintegrate layout tokens with text and image tokens without compromising\ngeneration quality. It achieves superior layoutaware control while maintaining\nthe structural simplicity and generation efficiency of AR models.",
      "pdf_url": "http://arxiv.org/pdf/2509.12046v1",
      "published": "2025-09-15T15:27:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12046v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing",
      "authors": [
        "Bingyu Li",
        "Haocheng Dong",
        "Da Zhang",
        "Zhiyuan Zhao",
        "Junyu Gao",
        "Xuelong Li"
      ],
      "abstract": "Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task\nthat adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)\ndomain, remains underexplored due to the absence of a unified evaluation\nbenchmark and the domain gap between natural and RS images. To bridge these\ngaps, we first establish a standardized OVRSIS benchmark (\\textbf{OVRSISBench})\nbased on widely-used RS segmentation datasets, enabling consistent evaluation\nacross methods. Using this benchmark, we comprehensively evaluate several\nrepresentative OVS/OVRSIS models and reveal their limitations when directly\napplied to remote sensing scenarios. Building on these insights, we propose\n\\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for\nremote sensing. RSKT-Seg integrates three key components: (1) a\nMulti-Directional Cost Map Aggregation (RS-CMA) module that captures\nrotation-invariant visual cues by computing vision-language cosine similarities\nacross multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)\ntransformer, which jointly models spatial and semantic dependencies with a\nlightweight dimensionality reduction strategy; and (3) a Remote Sensing\nKnowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and\nfacilitates domain adaptation via enhanced upsampling. Extensive experiments on\nthe benchmark show that RSKT-Seg consistently outperforms strong OVS baselines\nby +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through\nefficient aggregation. Our code is\n\\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{here}}.",
      "pdf_url": "http://arxiv.org/pdf/2509.12040v1",
      "published": "2025-09-15T15:24:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12040v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review",
      "authors": [
        "Emmanuel Adjei Domfeh",
        "Christopher L. Dancy"
      ],
      "abstract": "In high-stakes disaster scenarios, timely and informed decision-making is\ncritical yet often challenged by uncertainty, dynamic environments, and limited\nresources. This paper presents a systematic review of Human-AI collaboration\npatterns that support decision-making across all disaster management phases.\nDrawing from 51 peer-reviewed studies, we identify four major categories:\nHuman-AI Decision Support Systems, Task and Resource Coordination, Trust and\nTransparency, and Simulation and Training. Within these, we analyze\nsub-patterns such as cognitive-augmented intelligence, multi-agent\ncoordination, explainable AI, and virtual training environments. Our review\nhighlights how AI systems may enhance situational awareness, improves response\nefficiency, and support complex decision-making, while also surfacing critical\nlimitations in scalability, interpretability, and system interoperability. We\nconclude by outlining key challenges and future research directions,\nemphasizing the need for adaptive, trustworthy, and context-aware Human-AI\nsystems to improve disaster resilience and equitable recovery outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2509.12034v1",
      "published": "2025-09-15T15:18:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12034v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Imitation Learning as Return Distribution Matching",
      "authors": [
        "Filippo Lazzati",
        "Alberto Maria Metelli"
      ],
      "abstract": "We study the problem of training a risk-sensitive reinforcement learning (RL)\nagent through imitation learning (IL). Unlike standard IL, our goal is not only\nto train an agent that matches the expert's expected return (i.e., its average\nperformance) but also its risk attitude (i.e., other features of the return\ndistribution, such as variance). We propose a general formulation of the\nrisk-sensitive IL problem in which the objective is to match the expert's\nreturn distribution in Wasserstein distance. We focus on the tabular setting\nand assume the expert's reward is known. After demonstrating the limited\nexpressivity of Markovian policies for this task, we introduce an efficient and\nsufficiently expressive subclass of non-Markovian policies tailored to it.\nBuilding on this subclass, we develop two provably efficient algorithms, RS-BC\nand RS-KT, for solving the problem when the transition model is unknown and\nknown, respectively. We show that RS-KT achieves substantially lower sample\ncomplexity than RS-BC by exploiting dynamics information. We further\ndemonstrate the sample efficiency of return distribution matching in the\nsetting where the expert's reward is unknown by designing an oracle-based\nvariant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and\nRS-BC with numerical simulations, highlighting both their sample efficiency and\nthe advantages of non-Markovian policies over standard sample-efficient IL\nalgorithms.",
      "pdf_url": "http://arxiv.org/pdf/2509.12026v1",
      "published": "2025-09-15T15:08:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12026v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models",
      "authors": [
        "Sangjun Lee",
        "Seung-taek Woo",
        "Jungyu Jin",
        "Changhun Lee",
        "Eunhyeok Park"
      ],
      "abstract": "To enable broader deployment of Large Language Models (LLMs), it is essential\nto identify the best-performing model under strict memory constraints. We\npresent AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework\nthat assigns layer-wise quantization bit-widths to optimally balance model\nquality and memory usage. However, the combinatorial search space, with over\n10^{100} possible configurations, makes conventional black-box optimization\ninfeasible. AMQ overcomes this challenge through four key innovations:(1)\nsearch space pruning using prior knowledge to exclude unpromising\nconfigurations, (2) quantization proxy to bypass costly format conversions\nduring search, (3) quality predictor to minimize evaluation overhead, and (4)\niterative search-and-update strategy for fast and stable convergence. By\nintegrating these components, AMQ efficiently explores the quality-efficiency\nlandscape, reaching the Pareto frontier and yielding LLMs that are both compact\nand high-performing. Our code is available at https://github.com/dlwns147/amq.",
      "pdf_url": "http://arxiv.org/pdf/2509.12019v1",
      "published": "2025-09-15T14:59:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12019v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Generalizing Behavior via Inverse Reinforcement Learning with Closed-Form Reward Centroids",
      "authors": [
        "Filippo Lazzati",
        "Alberto Maria Metelli"
      ],
      "abstract": "We study the problem of generalizing an expert agent's behavior, provided\nthrough demonstrations, to new environments and/or additional constraints.\nInverse Reinforcement Learning (IRL) offers a promising solution by seeking to\nrecover the expert's underlying reward function, which, if used for planning in\nthe new settings, would reproduce the desired behavior. However, IRL is\ninherently ill-posed: multiple reward functions, forming the so-called feasible\nset, can explain the same observed behavior. Since these rewards may induce\ndifferent policies in the new setting, in the absence of additional\ninformation, a decision criterion is needed to select which policy to deploy.\nIn this paper, we propose a novel, principled criterion that selects the\n\"average\" policy among those induced by the rewards in a certain bounded subset\nof the feasible set. Remarkably, we show that this policy can be obtained by\nplanning with the reward centroid of that subset, for which we derive a\nclosed-form expression. We then present a provably efficient algorithm for\nestimating this centroid using an offline dataset of expert demonstrations\nonly. Finally, we conduct numerical simulations that illustrate the\nrelationship between the expert's behavior and the behavior produced by our\nmethod.",
      "pdf_url": "http://arxiv.org/pdf/2509.12010v1",
      "published": "2025-09-15T14:53:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.12010v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles",
      "authors": [
        "Jesús Calleja",
        "David Ponce",
        "Thierry Etchegoyhen"
      ],
      "abstract": "We describe Vicomtech's participation in the CLEARS challenge on text\nadaptation to Plain Language and Easy Read in Spanish. Our approach features\nautomatic post-editing of different types of initial Large Language Model\nadaptations, where successive adaptations are generated iteratively until\nreadability and similarity metrics indicate that no further adaptation\nrefinement can be successfully performed. Taking the average of all official\nmetrics, our submissions achieved first and second place in Plain language and\nEasy Read adaptation, respectively.",
      "pdf_url": "http://arxiv.org/pdf/2509.11991v1",
      "published": "2025-09-15T14:42:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11991v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Poison to Detect: Detection of Targeted Overfitting in Federated Learning",
      "authors": [
        "Soumia Zohra El Mestari",
        "Maciej Krzysztof Zuziak",
        "Gabriele Lenzini"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralised clients while keeping local data private, making it a widely\nadopted privacy-enhancing technology (PET). Despite its privacy benefits, FL\nremains vulnerable to privacy attacks, including those targeting specific\nclients. In this paper, we study an underexplored threat where a dishonest\norchestrator intentionally manipulates the aggregation process to induce\ntargeted overfitting in the local models of specific clients. Whereas many\nstudies in this area predominantly focus on reducing the amount of information\nleakage during training, we focus on enabling an early client-side detection of\ntargeted overfitting, thereby allowing clients to disengage before significant\nharm occurs. In line with this, we propose three detection techniques - (a)\nlabel flipping, (b) backdoor trigger injection, and (c) model fingerprinting -\nthat enable clients to verify the integrity of the global aggregation. We\nevaluated our methods on multiple datasets under different attack scenarios.\nOur results show that the three methods reliably detect targeted overfitting\ninduced by the orchestrator, but they differ in terms of computational\ncomplexity, detection latency, and false-positive rates.",
      "pdf_url": "http://arxiv.org/pdf/2509.11974v1",
      "published": "2025-09-15T14:23:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11974v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "MusicSwarm: Biologically Inspired Intelligence for Music Composition",
      "authors": [
        "Markus J. Buehler"
      ],
      "abstract": "We show that coherent, long-form musical composition can emerge from a\ndecentralized swarm of identical, frozen foundation models that coordinate via\nstigmergic, peer-to-peer signals, without any weight updates. We compare a\ncentralized multi-agent system with a global critic to a fully decentralized\nswarm in which bar-wise agents sense and deposit harmonic, rhythmic, and\nstructural cues, adapt short-term memory, and reach consensus. Across symbolic,\naudio, and graph-theoretic analyses, the swarm yields superior quality while\ndelivering greater diversity and structural variety and leads across creativity\nmetrics. The dynamics contract toward a stable configuration of complementary\nroles, and self-similarity networks reveal a small-world architecture with\nefficient long-range connectivity and specialized bridging motifs, clarifying\nhow local novelties consolidate into global musical form. By shifting\nspecialization from parameter updates to interaction rules, shared memory, and\ndynamic consensus, MusicSwarm provides a compute- and data-efficient route to\nlong-horizon creative structure that is immediately transferable beyond music\nto collaborative writing, design, and scientific discovery.",
      "pdf_url": "http://arxiv.org/pdf/2509.11973v1",
      "published": "2025-09-15T14:23:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11973v1",
      "categories": [
        "cs.AI",
        "cs.MM",
        "cs.SD"
      ]
    },
    {
      "title": "Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study",
      "authors": [
        "James C. Ward",
        "Alex Bott",
        "Connor York",
        "Edmund R. Hunt"
      ],
      "abstract": "Simulating hostile attacks of physical autonomous systems can be a useful\ntool to examine their robustness to attack and inform vulnerability-aware\ndesign. In this work, we examine this through the lens of multi-robot patrol,\nby presenting a machine learning-based adversary model that observes robot\npatrol behavior in order to attempt to gain undetected access to a secure\nenvironment within a limited time duration. Such a model allows for evaluation\nof a patrol system against a realistic potential adversary, offering insight\ninto future patrol strategy design. We show that our new model outperforms\nexisting baselines, thus providing a more stringent test, and examine its\nperformance against multiple leading decentralized multi-robot patrol\nstrategies.",
      "pdf_url": "http://arxiv.org/pdf/2509.11971v1",
      "published": "2025-09-15T14:22:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11971v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel Processing Students",
      "authors": [
        "Guy Tel-Zur"
      ],
      "abstract": "This project addresses a critical pedagogical need: offering students\ncontinuous, on-demand academic assistance beyond conventional reception hours.\nI present a domain-specific Retrieval-Augmented Generation (RAG) system powered\nby a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The\nassistant enhances learning by delivering real-time, personalized responses\naligned with the \"Introduction to Parallel Processing\" course materials. GPU\nacceleration significantly improves inference latency, enabling practical\ndeployment on consumer hardware. This approach demonstrates how consumer GPUs\ncan enable affordable, private, and effective AI tutoring for HPC education.",
      "pdf_url": "http://arxiv.org/pdf/2509.11947v1",
      "published": "2025-09-15T14:06:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11947v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare",
      "authors": [
        "Susanta Mitra"
      ],
      "abstract": "Healthcare and medicine are multimodal disciplines that deal with multimodal\ndata for reasoning and diagnosing multiple diseases. Although some multimodal\nreasoning models have emerged for reasoning complex tasks in scientific\ndomains, their applications in the healthcare domain remain limited and fall\nshort in correct reasoning for diagnosis. To address the challenges of\nmultimodal medical reasoning for correct diagnosis and assist the healthcare\nprofessionals, a novel temporal graph-based reasoning process modelled through\na directed graph has been proposed in the current work. It helps in\naccommodating dynamic changes in reasons through backtracking, refining the\nreasoning content, and creating new or deleting existing reasons to reach the\nbest recommendation or answer. Again, consideration of multimodal data at\ndifferent time points can enable tracking and analysis of patient health and\ndisease progression. Moreover, the proposed multi-agent temporal reasoning\nframework provides task distributions and a cross-validation mechanism to\nfurther enhance the accuracy of reasoning outputs. A few basic experiments and\nanalysis results justify the novelty and practical utility of the proposed\npreliminary approach.",
      "pdf_url": "http://arxiv.org/pdf/2509.11944v1",
      "published": "2025-09-15T14:03:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11944v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics",
      "authors": [
        "Antonin Sulc",
        "Thorsten Hellert"
      ],
      "abstract": "The development of intelligent agents, particularly those powered by language\nmodels (LMs), has shown the critical role in various environments that require\nintelligent and autonomous decision. Environments are not passive testing\ngrounds and they represent the data required for agents to learn and exhibit\nvery challenging conditions that require adaptive, complex and autonomous\ncapacity to make decisions. While the paradigm of scaling models and datasets\nhas led to remarkable emergent capabilities, we argue that scaling the\nstructure, fidelity, and logical consistency of agent reasoning within these\nenvironments is a crucial, yet underexplored, dimension of AI research. This\npaper introduces a neuro-symbolic multi-agent architecture where the belief\nstates of individual agents are formally represented as Kripke models. This\nfoundational choice enables them to reason about known concepts of\n\\emph{possibility} and \\emph{necessity} using the formal language of modal\nlogic. In this work, we use of immutable, domain-specific knowledge to make\ninfere information, which is encoded as logical constraints essential for\nproper diagnosis. In the proposed model, we show constraints that actively\nguide the hypothesis generation of LMs, effectively preventing them from\nreaching physically or logically untenable conclusions. In a high-fidelity\nsimulated particle accelerator environment, our system successfully diagnoses\ncomplex, cascading failures by combining the powerful semantic intuition of LMs\nwith the rigorous, verifiable validation of modal logic and a factual world\nmodel and showcasing a viable path toward more robust, reliable, and verifiable\nautonomous agents.",
      "pdf_url": "http://arxiv.org/pdf/2509.11943v1",
      "published": "2025-09-15T14:03:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11943v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "cs.MA"
      ]
    },
    {
      "title": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems",
      "authors": [
        "Luís F. Gomes",
        "Xin Zhou",
        "David Lo",
        "Rui Abreu"
      ],
      "abstract": "Visual documentation is an effective tool for reducing the cognitive barrier\ndevelopers face when understanding unfamiliar code, enabling more intuitive\ncomprehension. Compared to textual documentation, it provides a higher-level\nunderstanding of the system structure and data flow. Developers usually prefer\nvisual representations over lengthy textual descriptions for large software\nsystems. Visual documentation is both difficult to produce and challenging to\nevaluate. Manually creating it is time-consuming, and currently, no existing\napproach can automatically generate high-level visual documentation directly\nfrom code. Its evaluation is often subjective, making it difficult to\nstandardize and automate. To address these challenges, this paper presents the\nfirst exploration of using agentic LLM systems to automatically generate visual\ndocumentation. We introduce VisDocSketcher, the first agent-based approach that\ncombines static analysis with LLM agents to identify key elements in the code\nand produce corresponding visual representations. We propose a novel evaluation\nframework, AutoSketchEval, for assessing the quality of generated visual\ndocumentation using code-level metrics. The experimental results show that our\napproach can valid visual documentation for 74.4% of the samples. It shows an\nimprovement of 26.7-39.8% over a simple template-based baseline. Our evaluation\nframework can reliably distinguish high-quality (code-aligned) visual\ndocumentation from low-quality (non-aligned) ones, achieving an AUC exceeding\n0.87. Our work lays the foundation for future research on automated visual\ndocumentation by introducing practical tools that not only generate valid\nvisual representations but also reliably assess their quality.",
      "pdf_url": "http://arxiv.org/pdf/2509.11942v1",
      "published": "2025-09-15T14:02:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11942v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "How to Evaluate Medical AI",
      "authors": [
        "Ilia Kopanichuk",
        "Petr Anokhin",
        "Vladimir Shaposhnikov",
        "Vladimir Makharev",
        "Ekaterina Tsapieva",
        "Iaroslav Bespalov",
        "Dmitry V. Dylov",
        "Ivan Oseledets"
      ],
      "abstract": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI.",
      "pdf_url": "http://arxiv.org/pdf/2509.11941v1",
      "published": "2025-09-15T14:01:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11941v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "I.2.7; I.2.1"
      ]
    },
    {
      "title": "Neuromorphic Intelligence",
      "authors": [
        "Marcel van Gerven"
      ],
      "abstract": "Neuromorphic computing seeks to replicate the remarkable efficiency,\nflexibility, and adaptability of the human brain in artificial systems. Unlike\nconventional digital approaches, which depend on massive computational and\nenergy resources, neuromorphic systems exploit brain-inspired principles of\ncomputation to achieve orders of magnitude greater energy efficiency. By\ndrawing on insights from artificial intelligence, neuroscience, physics,\nchemistry, and materials science, neuromorphic computing promises to deliver\nintelligent systems that are sustainable, transparent, and widely accessible. A\ncentral challenge, however, is to identify a unifying theoretical framework\ncapable of bridging these diverse disciplines. We argue that dynamical systems\ntheory provides such a foundation. Rooted in differential calculus, it offers a\nprincipled language for modeling inference, learning, and control in both\nnatural and artificial substrates. Within this framework, noise can be\nharnessed as a resource for learning, while differential genetic programming\nenables the discovery of dynamical systems that implement adaptive behaviors.\nEmbracing this perspective paves the way toward emergent neuromorphic\nintelligence, where intelligent behavior arises from the dynamics of physical\nsubstrates, advancing both the science and sustainability of AI.",
      "pdf_url": "http://arxiv.org/pdf/2509.11940v1",
      "published": "2025-09-15T13:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11940v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MMORE: Massive Multimodal Open RAG & Extraction",
      "authors": [
        "Alexandre Sallinen",
        "Stefan Krsteski",
        "Paul Teiletche",
        "Marc-Antoine Allard",
        "Baptiste Lecoeur",
        "Michael Zhang",
        "Fabrice Nemo",
        "David Kalajdzic",
        "Matthias Meyer",
        "Mary-Anne Hartley"
      ],
      "abstract": "We introduce MMORE, an open-source pipeline for Massive Multimodal Open\nRetrievalAugmented Generation and Extraction, designed to ingest, transform,\nand retrieve knowledge from heterogeneous document formats at scale. MMORE\nsupports more than fifteen file types, including text, tables, images, emails,\naudio, and video, and processes them into a unified format to enable downstream\napplications for LLMs. The architecture offers modular, distributed processing,\nenabling scalable parallelization across CPUs and GPUs. On processing\nbenchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines\nand 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates\nhybrid dense-sparse retrieval and supports both interactive APIs and batch RAG\nendpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve\nbiomedical QA accuracy with increasing retrieval depth. MMORE provides a\nrobust, extensible foundation for deploying task-agnostic RAG systems on\ndiverse, real-world multimodal data. The codebase is available at\nhttps://github.com/swiss-ai/mmore.",
      "pdf_url": "http://arxiv.org/pdf/2509.11937v1",
      "published": "2025-09-15T13:56:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11937v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "D.2.0; E.m"
      ]
    },
    {
      "title": "BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning",
      "authors": [
        "Xilei Dai",
        "Ruotian Chen",
        "Songze Guan",
        "Wen-Tai Li",
        "Chau Yuen"
      ],
      "abstract": "Reinforcement learning (RL) has proven effective for AI-based building energy\nmanagement. However, there is a lack of flexible framework to implement RL\nacross various control problems in building energy management. To address this\ngap, we propose BuildingGym, an open-source tool designed as a\nresearch-friendly and flexible framework for training RL control strategies for\ncommon challenges in building energy management. BuildingGym integrates\nEnergyPlus as its core simulator, making it suitable for both system-level and\nroom-level control. Additionally, BuildingGym is able to accept external\nsignals as control inputs instead of taking the building as a stand-alone\nentity. This feature makes BuildingGym applicable for more flexible\nenvironments, e.g. smart grid and EVs community. The tool provides several\nbuilt-in RL algorithms for control strategy training, simplifying the process\nfor building managers to obtain optimal control strategies. Users can achieve\nthis by following a few straightforward steps to configure BuildingGym for\noptimization control for common problems in the building energy management\nfield. Moreover, AI specialists can easily implement and test state-of-the-art\ncontrol algorithms within the platform. BuildingGym bridges the gap between\nbuilding managers and AI specialists by allowing for the easy configuration and\nreplacement of RL algorithms, simulators, and control environments or problems.\nWith BuildingGym, we efficiently set up training tasks for cooling load\nmanagement, targeting both constant and dynamic cooling load management. The\nbuilt-in algorithms demonstrated strong performance across both tasks,\nhighlighting the effectiveness of BuildingGym in optimizing cooling strategies.",
      "pdf_url": "http://arxiv.org/pdf/2509.11922v1",
      "published": "2025-09-15T13:37:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11922v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models",
      "authors": [
        "Yiqun Yao",
        "Naitong Yu",
        "Xiang Li",
        "Xin Jiang",
        "Xuezhi Fang",
        "Wenjia Ma",
        "Xuying Meng",
        "Jing Li",
        "Aixin Sun",
        "Yequan Wang"
      ],
      "abstract": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex\nmodels that process real-time omnimodal streams. EgoMem enables real-time\nmodels to recognize multiple users directly from raw audiovisual streams, to\nprovide personalized response, and to maintain long-term knowledge of users'\nfacts, preferences, and social relationships extracted from audiovisual\nhistory. EgoMem operates with three asynchronous processes: (i) a retrieval\nprocess that dynamically identifies user via face and voice, and gathers\nrelevant context from a long-term memory; (ii) an omnimodal dialog process that\ngenerates personalized audio responses based on the retrieved context; and\n(iii) a memory management process that automatically detects dialog boundaries\nfrom omnimodal streams, and extracts necessary information to update the\nlong-term memory. Unlike existing memory agents for LLMs, EgoMem relies\nentirely on raw audiovisual streams, making it especially suitable for\nlifelong, real-time, and embodied scenarios. Experimental results demonstrate\nthat EgoMem's retrieval and memory management modules achieve over 95% accuracy\non the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,\nthe system achieves fact-consistency scores above 87% in real-time personalized\ndialogs, establishing a strong baseline for future research.",
      "pdf_url": "http://arxiv.org/pdf/2509.11914v1",
      "published": "2025-09-15T13:33:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11914v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Integrating Prior Observations for Incremental 3D Scene Graph Prediction",
      "authors": [
        "Marian Renz",
        "Felix Igelbrink",
        "Martin Atzmueller"
      ],
      "abstract": "3D semantic scene graphs (3DSSG) provide compact structured representations\nof environments by explicitly modeling objects, attributes, and relationships.\nWhile 3DSSGs have shown promise in robotics and embodied AI, many existing\nmethods rely mainly on sensor data, not integrating further information from\nsemantically rich environments. Additionally, most methods assume access to\ncomplete scene reconstructions, limiting their applicability in real-world,\nincremental settings. This paper introduces a novel heterogeneous graph model\nfor incremental 3DSSG prediction that integrates additional, multi-modal\ninformation, such as prior observations, directly into the message-passing\nprocess. Utilizing multiple layers, the model flexibly incorporates global and\nlocal scene representations without requiring specialized modules or full scene\nreconstructions. We evaluate our approach on the 3DSSG dataset, showing that\nGNNs enriched with multi-modal information such as semantic embeddings (e.g.,\nCLIP) and prior observations offer a scalable and generalizable solution for\ncomplex, real-world environments. The full source code of the presented\narchitecture will be made available at\nhttps://github.com/m4renz/incremental-scene-graph-prediction.",
      "pdf_url": "http://arxiv.org/pdf/2509.11895v1",
      "published": "2025-09-15T13:10:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11895v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning",
      "authors": [
        "Carlos Celemin",
        "Joseph Brennan",
        "Pierluigi Vito Amadori",
        "Tim Bradley"
      ],
      "abstract": "This paper introduces a novel application of Supervised Contrastive Learning\n(SupCon) to Imitation Learning (IL), with a focus on learning more effective\nstate representations for agents in video game environments. The goal is to\nobtain latent representations of the observations that capture better the\naction-relevant factors, thereby modeling better the cause-effect relationship\nfrom the observations that are mapped to the actions performed by the\ndemonstrator, for example, the player jumps whenever an obstacle appears ahead.\nWe propose an approach to integrate the SupCon loss with continuous output\nspaces, enabling SupCon to operate without constraints regarding the type of\nactions of the environment. Experiments on the 3D games Astro Bot and Returnal,\nand multiple 2D Atari games show improved representation quality, faster\nlearning convergence, and better generalization compared to baseline models\ntrained only with supervised action prediction loss functions.",
      "pdf_url": "http://arxiv.org/pdf/2509.11880v1",
      "published": "2025-09-15T13:00:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.11880v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    }
  ]
}