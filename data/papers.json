{
  "last_updated": "2025-11-11T00:52:57.195660",
  "papers": [
    {
      "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning",
      "authors": [
        "Junwen Pan",
        "Qizhe Zhang",
        "Rui Zhang",
        "Ming Lu",
        "Xin Wan",
        "Yuan Zhang",
        "Chang Liu",
        "Qi She"
      ],
      "abstract": "Temporal search aims to identify a minimal set of relevant frames from tens\nof thousands based on a given query, serving as a foundation for accurate\nlong-form video understanding. Existing works attempt to progressively narrow\nthe search space. However, these approaches typically rely on a hand-crafted\nsearch process, lacking end-to-end optimization for learning optimal search\nstrategies. In this paper, we propose TimeSearch-R, which reformulates temporal\nsearch as interleaved text-video thinking, seamlessly integrating searching\nvideo clips into the reasoning process through reinforcement learning (RL).\nHowever, applying RL training methods, such as Group Relative Policy\nOptimization (GRPO), to video reasoning can result in unsupervised intermediate\nsearch decisions. This leads to insufficient exploration of the video content\nand inconsistent logical reasoning. To address these issues, we introduce GRPO\nwith Completeness Self-Verification (GRPO-CSV), which gathers searched video\nframes from the interleaved reasoning process and utilizes the same policy\nmodel to verify the adequacy of searched frames, thereby improving the\ncompleteness of video reasoning. Additionally, we construct datasets\nspecifically designed for the SFT cold-start and RL training of GRPO-CSV,\nfiltering out samples with weak temporal dependencies to enhance task\ndifficulty and improve temporal search capabilities. Extensive experiments\ndemonstrate that TimeSearch-R achieves significant improvements on temporal\nsearch benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as\nlong-form video understanding benchmarks like VideoMME and MLVU. Notably,\nTimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%\nimprovement over the base model Qwen2.5-VL and 2.0% over the advanced video\nreasoning model Video-R1. Our code is available at\nhttps://github.com/Time-Search/TimeSearch-R.",
      "pdf_url": "http://arxiv.org/pdf/2511.05489v1",
      "published": "2025-11-07T18:58:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05489v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction",
      "authors": [
        "Abigail Lin"
      ],
      "abstract": "Predicting the effect of amino acid mutations on enzyme thermodynamic\nstability (DDG) is fundamental to protein engineering and drug design. While\nrecent deep learning approaches have shown promise, they often process sequence\nand structure information independently, failing to capture the intricate\ncoupling between local structural geometry and global sequential patterns. We\npresent DGTN (Diffused Graph-Transformer Network), a novel architecture that\nco-learns graph neural network (GNN) weights for structural priors and\ntransformer attention through a diffusion mechanism. Our key innovation is a\nbidirectional diffusion process where: (1) GNN-derived structural embeddings\nguide transformer attention via learnable diffusion kernels, and (2)\ntransformer representations refine GNN message passing through\nattention-modulated graph updates. We provide rigorous mathematical analysis\nshowing this co-learning scheme achieves provably better approximation bounds\nthan independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves\nstate-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with\n6.2% improvement over best baselines. Ablation studies confirm the diffusion\nmechanism contributes 4.8 points to correlation. Our theoretical analysis\nproves the diffused attention converges to optimal structure-sequence coupling,\nwith convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work\nestablishes a principled framework for integrating heterogeneous protein\nrepresentations through learnable diffusion.",
      "pdf_url": "http://arxiv.org/pdf/2511.05483v1",
      "published": "2025-11-07T18:52:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05483v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "On Flow Matching KL Divergence",
      "authors": [
        "Maojiang Su",
        "Jerry Yao-Chieh Hu",
        "Sophia Pi",
        "Han Liu"
      ],
      "abstract": "We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler\n(KL) divergence of the flow-matching distribution approximation. In particular,\nif the $L_2$ flow-matching loss is bounded by $\\epsilon^2 > 0$, then the KL\ndivergence between the true data distribution and the estimated distribution is\nbounded by $A_1 \\epsilon + A_2 \\epsilon^2$. Here, the constants $A_1$ and $A_2$\ndepend only on the regularities of the data and velocity fields. Consequently,\nthis bound implies statistical convergence rates of Flow Matching Transformers\nunder the Total Variation (TV) distance. We show that, flow matching achieves\nnearly minimax-optimal efficiency in estimating smooth distributions. Our\nresults make the statistical efficiency of flow matching comparable to that of\ndiffusion models under the TV distance. Numerical studies on synthetic and\nlearned velocities corroborate our theory.",
      "pdf_url": "http://arxiv.org/pdf/2511.05480v1",
      "published": "2025-11-07T18:47:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05480v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ]
    },
    {
      "title": "AI Literacy Assessment Revisited: A Task-Oriented Approach Aligned with Real-world Occupations",
      "authors": [
        "Christopher Bogart",
        "Aparna Warrier",
        "Arav Agarwal",
        "Ross Higashi",
        "Yufan Zhang",
        "Jesse Flot",
        "Jaromir Savelka",
        "Heather Burte",
        "Majd Sakr"
      ],
      "abstract": "As artificial intelligence (AI) systems become ubiquitous in professional\ncontexts, there is an urgent need to equip workers, often with backgrounds\noutside of STEM, with the skills to use these tools effectively as well as\nresponsibly, that is, to be AI literate. However, prevailing definitions and\ntherefore assessments of AI literacy often emphasize foundational technical\nknowledge, such as programming, mathematics, and statistics, over practical\nknowledge such as interpreting model outputs, selecting tools, or identifying\nethical concerns. This leaves a noticeable gap in assessing someone's AI\nliteracy for real-world job use. We propose a work-task-oriented assessment\nmodel for AI literacy which is grounded in the competencies required for\neffective use of AI tools in professional settings. We describe the development\nof a novel AI literacy assessment instrument, and accompanying formative\nassessments, in the context of a US Navy robotics training program. The program\nincluded training in robotics and AI literacy, as well as a competition with\npractical tasks and a multiple choice scenario task meant to simulate use of AI\nin a job setting. We found that, as a measure of applied AI literacy, the\ncompetition's scenario task outperformed the tests we adopted from past\nresearch or developed ourselves. We argue that when training people for\nAI-related work, educators should consider evaluating them with instruments\nthat emphasize highly contextualized practical skills rather than abstract\ntechnical knowledge, especially when preparing workers without technical\nbackgrounds for AI-integrated roles.",
      "pdf_url": "http://arxiv.org/pdf/2511.05475v1",
      "published": "2025-11-07T18:38:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05475v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models",
      "authors": [
        "Jingxuan Xu",
        "Ken Deng",
        "Weihao Li",
        "Songwei Yu",
        "Huaixi Tang",
        "Haoyang Huang",
        "Zhiyi Lai",
        "Zizheng Zhan",
        "Yanan Wu",
        "Chenchen Zhang",
        "Kepeng Lei",
        "Yifan Yao",
        "Xinping Lei",
        "Wenqiang Zhu",
        "Zongxian Feng",
        "Han Li",
        "Junqi Xiong",
        "Dailin Li",
        "Zuchen Gao",
        "Kun Wu",
        "Wen Xiang",
        "Ziqi Zhan",
        "Yuanxing Zhang",
        "Wuxuan Gong",
        "Ziyuan Gao",
        "Guanxiang Wang",
        "Yirong Xue",
        "Xiaojiang Zhang",
        "Jinghui Wang",
        "Huiming Wang",
        "Wenhao Zhuang",
        "Zhaoxiang Zhang",
        "Yuqun Zhang",
        "Haotian Zhang",
        "Bin Chen",
        "Jiaheng Liu"
      ],
      "abstract": "Evaluating large language models (LLMs) for software engineering has been\nlimited by narrow task coverage, language bias, and insufficient alignment with\nreal-world developer workflows. Existing benchmarks often focus on algorithmic\nproblems or Python-centric bug fixing, leaving critical dimensions of software\nengineering underexplored. To address these gaps, we introduce SWE-Compass1, a\ncomprehensive benchmark that unifies heterogeneous code-related evaluations\ninto a structured and production-aligned framework. SWE-Compass spans 8 task\ntypes, 8 programming scenarios, and 10 programming languages, with 2000\nhigh-quality instances curated from authentic GitHub pull requests and refined\nthrough systematic filtering and validation. We benchmark ten state-of-the-art\nLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear\nhierarchy of difficulty across task types, languages, and scenarios. Moreover,\nby aligning evaluation with real-world developer practices, SWE-Compass\nprovides a rigorous and reproducible foundation for diagnosing and advancing\nagentic coding capabilities in large language models.",
      "pdf_url": "http://arxiv.org/pdf/2511.05459v1",
      "published": "2025-11-07T18:01:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05459v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Self-adaptive weighting and sampling for physics-informed neural networks",
      "authors": [
        "Wenqian Chen",
        "Amanda Howard",
        "Panos Stinis"
      ],
      "abstract": "Physics-informed deep learning has emerged as a promising framework for\nsolving partial differential equations (PDEs). Nevertheless, training these\nmodels on complex problems remains challenging, often leading to limited\naccuracy and efficiency. In this work, we introduce a hybrid adaptive sampling\nand weighting method to enhance the performance of physics-informed neural\nnetworks (PINNs). The adaptive sampling component identifies training points in\nregions where the solution exhibits rapid variation, while the adaptive\nweighting component balances the convergence rate across training points.\nNumerical experiments show that applying only adaptive sampling or only\nadaptive weighting is insufficient to consistently achieve accurate\npredictions, particularly when training points are scarce. Since each method\nemphasizes different aspects of the solution, their effectiveness is problem\ndependent. By combining both strategies, the proposed framework consistently\nimproves prediction accuracy and training efficiency, offering a more robust\napproach for solving PDEs with PINNs.",
      "pdf_url": "http://arxiv.org/pdf/2511.05452v1",
      "published": "2025-11-07T17:48:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05452v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ]
    },
    {
      "title": "APP: Accelerated Path Patching with Task-Specific Pruning",
      "authors": [
        "Frauke Andersen",
        "William Rudman",
        "Ruochen Zhang",
        "Carsten Eickhoff"
      ],
      "abstract": "Circuit discovery is a key step in many mechanistic interpretability\npipelines. Current methods, such as Path Patching, are computationally\nexpensive and have limited in-depth circuit analysis for smaller models. In\nthis study, we propose Accelerated Path Patching (APP), a hybrid approach\nleveraging our novel contrastive attention head pruning method to drastically\nreduce the search space of circuit discovery methods. Our Contrastive-FLAP\npruning algorithm uses techniques from causal mediation analysis to assign\nhigher pruning scores to task-specific attention heads, leading to higher\nperforming sparse models compared to traditional pruning techniques. Although\nContrastive-FLAP is successful at preserving task-specific heads that existing\npruning algorithms remove at low sparsity ratios, the circuits found by\nContrastive-FLAP alone are too large to satisfy the minimality constraint\nrequired in circuit analysis. APP first applies Contrastive-FLAP to reduce the\nsearch space on required for circuit discovery algorithms by, on average, 56\\%.\nNext, APP, applies traditional Path Patching on the remaining attention heads,\nleading to a speed up of 59.63\\%-93.27\\% compared to Path Patching applied to\nthe dense model. Despite the substantial computational saving that APP\nprovides, circuits obtained from APP exhibit substantial overlap and similar\nperformance to previously established Path Patching circuits",
      "pdf_url": "http://arxiv.org/pdf/2511.05442v1",
      "published": "2025-11-07T17:20:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05442v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68Uxx",
        "I.2.7; I.2.6; I.2.m"
      ]
    },
    {
      "title": "\"I Like That You Have to Poke Around\": Instructors on How Experiential Approaches to AI Literacy Spark Inquiry and Critical Thinking",
      "authors": [
        "Aparna Maya Warrier",
        "Arav Agarwal",
        "Jaromir Savelka",
        "Christopher Bogart",
        "Heather Burte"
      ],
      "abstract": "As artificial intelligence (AI) increasingly shapes decision-making across\ndomains, there is a growing need to support AI literacy among learners beyond\ncomputer science. However, many current approaches rely on programming-heavy\ntools or abstract lecture-based content, limiting accessibility for non-STEM\naudiences. This paper presents findings from a study of AI User, a modular,\nweb-based curriculum that teaches core AI concepts through interactive, no-code\nprojects grounded in real-world scenarios. The curriculum includes eight\nprojects; this study focuses on instructor feedback on Projects 5-8, which\naddress applied topics such as natural language processing, computer vision,\ndecision support, and responsible AI. Fifteen community college instructors\nparticipated in structured focus groups, completing the projects as learners\nand providing feedback through individual reflection and group discussion.\nUsing thematic analysis, we examined how instructors evaluated the design,\ninstructional value, and classroom applicability of these experiential\nactivities. Findings highlight instructors' appreciation for exploratory tasks,\nrole-based simulations, and real-world relevance, while also surfacing design\ntrade-offs around cognitive load, guidance, and adaptability for diverse\nlearners. This work extends prior research on AI literacy by centering\ninstructor perspectives on teaching complex AI topics without code. It offers\nactionable insights for designing inclusive, experiential AI learning resources\nthat scale across disciplines and learner backgrounds.",
      "pdf_url": "http://arxiv.org/pdf/2511.05430v1",
      "published": "2025-11-07T17:05:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05430v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids",
      "authors": [
        "Emad Efatinasab",
        "Nahal Azadi",
        "Davide Dalle Pezze",
        "Gian Antonio Susto",
        "Chuadhry Mujeeb Ahmed",
        "Mirco Rampazzo"
      ],
      "abstract": "As smart grids evolve to meet growing energy demands and modern operational\nchallenges, the ability to accurately predict faults becomes increasingly\ncritical. However, existing AI-based fault prediction models struggle to ensure\nreliability in evolving environments where they are required to adapt to new\nfault types and operational zones. In this paper, we propose a continual\nlearning (CL) framework in the smart grid context to evolve the model together\nwith the environment. We design four realistic evaluation scenarios grounded in\nclass-incremental and domain-incremental learning to emulate evolving grid\nconditions. We further introduce Prototype-based Dark Experience Replay\n(ProDER), a unified replay-based approach that integrates prototype-based\nfeature regularization, logit distillation, and a prototype-guided replay\nmemory. ProDER achieves the best performance among tested CL techniques, with\nonly a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone\nprediction. These results demonstrate the practicality of CL for scalable,\nreal-world fault prediction in smart grids.",
      "pdf_url": "http://arxiv.org/pdf/2511.05420v1",
      "published": "2025-11-07T16:51:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05420v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments",
      "authors": [
        "Laura Alejandra Encinar Gonzalez",
        "John Folkesson",
        "Rudolph Triebel",
        "Riccardo Giubilato"
      ],
      "abstract": "Robust loop closure detection is a critical component of Simultaneous\nLocalization and Mapping (SLAM) algorithms in GNSS-denied environments, such as\nin the context of planetary exploration. In these settings, visual place\nrecognition often fails due to aliasing and weak textures, while LiDAR-based\nmethods suffer from sparsity and ambiguity. This paper presents MPRF, a\nmultimodal pipeline that leverages transformer-based foundation models for both\nvision and LiDAR modalities to achieve robust loop closure in severely\nunstructured environments. Unlike prior work limited to retrieval, MPRF\nintegrates a two-stage visual retrieval strategy with explicit 6-DoF pose\nestimation, combining DINOv2 features with SALAD aggregation for efficient\ncandidate screening and SONATA-based LiDAR descriptors for geometric\nverification. Experiments on the S3LI dataset and S3LI Vulcano dataset show\nthat MPRF outperforms state-of-the-art retrieval methods in precision while\nenhancing pose estimation robustness in low-texture regions. By providing\ninterpretable correspondences suitable for SLAM back-ends, MPRF achieves a\nfavorable trade-off between accuracy, efficiency, and reliability,\ndemonstrating the potential of foundation models to unify place recognition and\npose estimation. Code and models will be released at github.com/DLR-RM/MPRF.",
      "pdf_url": "http://arxiv.org/pdf/2511.05404v1",
      "published": "2025-11-07T16:30:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05404v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.9; I.2.10"
      ]
    },
    {
      "title": "Robust Neural Audio Fingerprinting using Music Foundation Models",
      "authors": [
        "Shubhr Singh",
        "Kiran Bhat",
        "Xavier Riley",
        "Benjamin Resnick",
        "John Thickstun",
        "Walter De Brouwer"
      ],
      "abstract": "The proliferation of distorted, compressed, and manipulated music on modern\nmedia platforms like TikTok motivates the development of more robust audio\nfingerprinting techniques to identify the sources of musical recordings. In\nthis paper, we develop and evaluate new neural audio fingerprinting techniques\nwith the aim of improving their robustness. We make two contributions to neural\nfingerprinting methodology: (1) we use a pretrained music foundation model as\nthe backbone of the neural architecture and (2) we expand the use of data\naugmentation to train fingerprinting models under a wide variety of audio\nmanipulations, including time streching, pitch modulation, compression, and\nfiltering. We systematically evaluate our methods in comparison to two\nstate-of-the-art neural fingerprinting models: NAFP and GraFPrint. Results show\nthat fingerprints extracted with music foundation models (e.g., MuQ, MERT)\nconsistently outperform models trained from scratch or pretrained on\nnon-musical audio. Segment-level evaluation further reveals their capability to\naccurately localize fingerprint matches, an important practical feature for\ncatalog management.",
      "pdf_url": "http://arxiv.org/pdf/2511.05399v1",
      "published": "2025-11-07T16:25:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05399v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction",
      "authors": [
        "Yiting He",
        "Zhishuai Liu",
        "Weixin Wang",
        "Pan Xu"
      ],
      "abstract": "Off-dynamics reinforcement learning (RL), where training and deployment\ntransition dynamics are different, can be formulated as learning in a robust\nMarkov decision process (RMDP) where uncertainties in transition dynamics are\nimposed. Existing literature mostly assumes access to generative models\nallowing arbitrary state-action queries or pre-collected datasets with a good\nstate coverage of the deployment environment, bypassing the challenge of\nexploration. In this work, we study a more realistic and challenging setting\nwhere the agent is limited to online interaction with the training environment.\nTo capture the intrinsic difficulty of exploration in online RMDPs, we\nintroduce the supremal visitation ratio, a novel quantity that measures the\nmismatch between the training dynamics and the deployment dynamics. We show\nthat if this ratio is unbounded, online learning becomes exponentially hard. We\npropose the first computationally efficient algorithm that achieves sublinear\nregret in online RMDPs with $f$-divergence based transition uncertainties. We\nalso establish matching regret lower bounds, demonstrating that our algorithm\nachieves optimal dependence on both the supremal visitation ratio and the\nnumber of interaction episodes. Finally, we validate our theoretical results\nthrough comprehensive numerical experiments.",
      "pdf_url": "http://arxiv.org/pdf/2511.05396v1",
      "published": "2025-11-07T16:24:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05396v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ]
    },
    {
      "title": "AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly",
      "authors": [
        "Alexander Htet Kyaw",
        "Haotian Ma",
        "Sasa Zivkovic",
        "Jenny Sabin"
      ],
      "abstract": "We present an AI-assisted Augmented Reality assembly workflow that uses deep\nlearning-based object recognition to identify different assembly components and\ndisplay step-by-step instructions. For each assembly step, the system displays\na bounding box around the corresponding components in the physical space, and\nwhere the component should be placed. By connecting assembly instructions with\nthe real-time location of relevant components, the system eliminates the need\nfor manual searching, sorting, or labeling of different components before each\nassembly. To demonstrate the feasibility of using object recognition for\nAR-assisted assembly, we highlight a case study involving the assembly of LEGO\nsculptures.",
      "pdf_url": "http://arxiv.org/pdf/2511.05394v1",
      "published": "2025-11-07T16:20:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05394v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "H.5.2; H.5.1; I.4.8; I.2.6"
      ]
    },
    {
      "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework",
      "authors": [
        "Chao Zhang",
        "Yuhao Wang",
        "Derong Xu",
        "Haoxin Zhang",
        "Yuanjie Lyu",
        "Yuhao Chen",
        "Shuochen Liu",
        "Tong Xu",
        "Xiangyu Zhao",
        "Yan Gao",
        "Yao Hu",
        "Enhong Chen"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment\nLarge Language Models' (LLMs) reliability. For flexibility, agentic RAG employs\nautonomous, multi-round retrieval and reasoning to resolve queries. Although\nrecent agentic RAG has improved via reinforcement learning, they often incur\nsubstantial token overhead from search and reasoning processes. This trade-off\nprioritizes accuracy over efficiency. To address this issue, this work proposes\nTeaRAG, a token-efficient agentic RAG framework capable of compressing both\nretrieval content and reasoning steps. 1) First, the retrieved content is\ncompressed by augmenting chunk-based semantic retrieval with a graph retrieval\nusing concise triplets. A knowledge association graph is then built from\nsemantic similarity and co-occurrence. Finally, Personalized PageRank is\nleveraged to highlight key knowledge within this graph, reducing the number of\ntokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative\nProcess-aware Direct Preference Optimization (IP-DPO) is proposed.\nSpecifically, our reward function evaluates the knowledge sufficiency by a\nknowledge matching mechanism, while penalizing excessive reasoning steps. This\ndesign can produce high-quality preference-pair datasets, supporting iterative\nDPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the\naverage Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on\nLlama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/TeaRAG.",
      "pdf_url": "http://arxiv.org/pdf/2511.05385v1",
      "published": "2025-11-07T16:08:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05385v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Reasoning Is All You Need for Urban Planning AI",
      "authors": [
        "Sijie Yang",
        "Jiatong Li",
        "Filip Biljecki"
      ],
      "abstract": "AI has proven highly successful at urban planning analysis -- learning\npatterns from data to predict future conditions. The next frontier is\nAI-assisted decision-making: agents that recommend sites, allocate resources,\nand evaluate trade-offs while reasoning transparently about constraints and\nstakeholder values. Recent breakthroughs in reasoning AI -- CoT prompting,\nReAct, and multi-agent collaboration frameworks -- now make this vision\nachievable.\n  This position paper presents the Agentic Urban Planning AI Framework for\nreasoning-capable planning agents that integrates three cognitive layers\n(Perception, Foundation, Reasoning) with six logic components (Analysis,\nGeneration, Verification, Evaluation, Collaboration, Decision) through a\nmulti-agents collaboration framework. We demonstrate why planning decisions\nrequire explicit reasoning capabilities that are value-based (applying\nnormative principles), rule-grounded (guaranteeing constraint satisfaction),\nand explainable (generating transparent justifications) -- requirements that\nstatistical learning alone cannot fulfill. We compare reasoning agents with\nstatistical learning, present a comprehensive architecture with benchmark\nevaluation metrics, and outline critical research challenges. This framework\nshows how AI agents can augment human planners by systematically exploring\nsolution spaces, verifying regulatory compliance, and deliberating over\ntrade-offs transparently -- not replacing human judgment but amplifying it with\ncomputational reasoning capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2511.05375v1",
      "published": "2025-11-07T15:59:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05375v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AI Literacy for Community Colleges: Instructors' Perspectives on Scenario-Based and Interactive Approaches to Teaching AI",
      "authors": [
        "Aparna Maya Warrier",
        "Arav Agarwal",
        "Jaromir Savelka",
        "Christopher A Bogart",
        "Heather Burte"
      ],
      "abstract": "This research category full paper investigates how community college\ninstructors evaluate interactive, no-code AI literacy resources designed for\nnon-STEM learners. As artificial intelligence becomes increasingly integrated\ninto everyday technologies, AI literacy - the ability to evaluate AI systems,\ncommunicate with them, and understand their broader impacts - has emerged as a\ncritical skill across disciplines. Yet effective, scalable approaches for\nteaching these concepts in higher education remain limited, particularly for\nstudents outside STEM fields.\n  To address this gap, we developed AI User, an interactive online curriculum\nthat introduces core AI concepts through scenario - based activities set in\nreal - world contexts. This study presents findings from four focus groups with\ninstructors who engaged with AI User materials and participated in structured\nfeedback activities. Thematic analysis revealed that instructors valued\nexploratory tasks that simulated real - world AI use cases and fostered\nexperimentation, while also identifying challenges related to scaffolding,\naccessibility, and multi-modal support. A ranking task for instructional\nsupport materials showed a strong preference for interactive demonstrations\nover traditional educational materials like conceptual guides or lecture\nslides.\n  These findings offer insights into instructor perspectives on making AI\nconcepts more accessible and relevant for broad learner audiences. They also\ninform the design of AI literacy tools that align with diverse teaching\ncontexts and support critical engagement with AI in higher education.",
      "pdf_url": "http://arxiv.org/pdf/2511.05363v1",
      "published": "2025-11-07T15:51:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05363v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "A multimodal multiplex of the mental lexicon for multilingual individuals",
      "authors": [
        "Maria Huynh",
        "Wilder C. Rodrigues"
      ],
      "abstract": "Historically, bilingualism was often perceived as an additional cognitive\nload that could hinder linguistic and intellectual development. However, over\nthe last three decades, this view has changed considerably. Numerous studies\nhave aimed to model and understand the architecture of the bilingual word\nrecognition system Dijkstra and van Heuven (2002), investigating how parallel\nactivation operates in the brain and how one language influences another Kroll\net al. (2015). Increasingly, evidence suggests that multilinguals, individuals\nwho speak three or more languages, can perform better than monolinguals in\nvarious linguistic and cognitive tasks, such as learning an additional language\nAbu-Rabia and Sanitsky (2010). This research proposal focuses on the study of\nthe mental lexicon and how it may be structured in individuals who speak\nmultiple languages. Building on the work of Stella et al. (2018), who\ninvestigated explosive learning in humans using a multiplex model of the mental\nlexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by\nDijkstra and van Heuven (2002), the present study applies the same multilayer\nnetwork principles introduced by Kivela et al. (2014). Our experimental design\nextends previous research by incorporating multimodality into the multiplex\nmodel, introducing an additional layer that connects visual inputs to their\ncorresponding lexical representations across the multilingual layers of the\nmental lexicon. In this research, we aim to explore how a heritage language\ninfluences the acquisition of another language. Specifically, we ask: Does the\npresence of visual input in a translation task influence participants'\nproficiency and accuracy compared to text-only conditions?",
      "pdf_url": "http://arxiv.org/pdf/2511.05361v1",
      "published": "2025-11-07T15:51:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05361v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Perceptually Aligning Representations of Music via Noise-Augmented Autoencoders",
      "authors": [
        "Mathias Rose Bjare",
        "Giorgia Cantisani",
        "Marco Pasini",
        "Stefan Lattner",
        "Gerhard Widmer"
      ],
      "abstract": "We argue that training autoencoders to reconstruct inputs from noised\nversions of their encodings, when combined with perceptual losses, yields\nencodings that are structured according to a perceptual hierarchy. We\ndemonstrate the emergence of this hierarchical structure by showing that, after\ntraining an audio autoencoder in this manner, perceptually salient information\nis captured in coarser representation structures than with conventional\ntraining. Furthermore, we show that such perceptual hierarchies improve latent\ndiffusion decoding in the context of estimating surprisal in music pitches and\npredicting EEG-brain responses to music listening. Pretrained weights are\navailable on github.com/CPJKU/pa-audioic.",
      "pdf_url": "http://arxiv.org/pdf/2511.05350v1",
      "published": "2025-11-07T15:44:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05350v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions",
      "authors": [
        "Klára Bendová",
        "Tomáš Knap",
        "Jan Černý",
        "Vojtěch Pour",
        "Jaromir Savelka",
        "Ivana Kvapilíková",
        "Jakub Drápal"
      ],
      "abstract": "Criminal justice administrative data contain only a limited amount of\ninformation about the committed offense. However, there is an unused source of\nextensive information in continental European courts' decisions: descriptions\nof criminal behaviors in verdicts by which offenders are found guilty. In this\npaper, we study the feasibility of extracting these descriptions from publicly\navailable court decisions from Slovakia. We use two different approaches for\nretrieval: regular expressions and large language models (LLMs). Our baseline\nwas a simple method employing regular expressions to identify typical words\noccurring before and after the description. The advanced regular expression\napproach further focused on \"sparing\" and its normalization (insertion of\nspaces between individual letters), typical for delineating the description.\nThe LLM approach involved prompting the Gemini Flash 2.0 model to extract the\ndescriptions using predefined instructions. Although the baseline identified\ndescriptions in only 40.5% of verdicts, both methods significantly outperformed\nit, achieving 97% with advanced regular expressions and 98.75% with LLMs, and\n99.5% when combined. Evaluation by law students showed that both advanced\nmethods matched human annotations in about 90% of cases, compared to just 34.5%\nfor the baseline. LLMs fully matched human-labeled descriptions in 91.75% of\ninstances, and a combination of advanced regular expressions with LLMs reached\n92%.",
      "pdf_url": "http://arxiv.org/pdf/2511.05320v1",
      "published": "2025-11-07T15:17:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05320v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance",
      "authors": [
        "Valeriu Dimidov",
        "Faisal Hawlader",
        "Sasan Jafarnejad",
        "Raphaël Frank"
      ],
      "abstract": "Economic constraints, limited availability of datasets for reproducibility\nand shortages of specialized expertise have long been recognized as key\nchallenges to the adoption and advancement of predictive maintenance (PdM) in\nthe automotive sector. Recent progress in large language models (LLMs) presents\nan opportunity to overcome these barriers and speed up the transition of PdM\nfrom research to industrial practice. Under these conditions, we explore the\npotential of LLM-based agents to support PdM cleaning pipelines. Specifically,\nwe focus on maintenance logs, a critical data source for training\nwell-performing machine learning (ML) models, but one often affected by errors\nsuch as typos, missing fields, near-duplicate entries, and incorrect dates. We\nevaluate LLM agents on cleaning tasks involving six distinct types of noise.\nOur findings show that LLMs are effective at handling generic cleaning tasks\nand offer a promising foundation for future industrial applications. While\ndomain-specific errors remain challenging, these results highlight the\npotential for further improvements through specialized training and enhanced\nagentic capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2511.05311v1",
      "published": "2025-11-07T15:12:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05311v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "cs.SE"
      ]
    },
    {
      "title": "Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation",
      "authors": [
        "Matteo Bastico",
        "David Ryckelynck",
        "Laurent Corté",
        "Yannick Tillier",
        "Etienne Decencière"
      ],
      "abstract": "As 3D point clouds become a cornerstone of modern technology, the need for\nsophisticated generative models and reliable evaluation metrics has grown\nexponentially. In this work, we first expose that some commonly used metrics\nfor evaluating generated point clouds, particularly those based on Chamfer\nDistance (CD), lack robustness against defects and fail to capture geometric\nfidelity and local shape consistency when used as quality indicators. We\nfurther show that introducing samples alignment prior to distance calculation\nand replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet\nessential steps to ensure the consistency and robustness of point cloud\ngenerative model evaluation metrics. While existing metrics primarily focus on\ndirectly comparing 3D Euclidean coordinates, we present a novel metric, named\nSurface Normal Concordance (SNC), which approximates surface similarity by\ncomparing estimated point normals. This new metric, when combined with\ntraditional ones, provides a more comprehensive evaluation of the quality of\ngenerated samples. Finally, leveraging recent advancements in transformer-based\nmodels for point cloud analysis, such as serialized patch attention , we\npropose a new architecture for generating high-fidelity 3D structures, the\nDiffusion Point Transformer. We perform extensive experiments and comparisons\non the ShapeNet dataset, showing that our model outperforms previous solutions,\nparticularly in terms of quality of generated point clouds, achieving new\nstate-of-the-art. Code available at\nhttps://github.com/matteo-bastico/DiffusionPointTransformer.",
      "pdf_url": "http://arxiv.org/pdf/2511.05308v1",
      "published": "2025-11-07T15:07:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05308v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding",
      "authors": [
        "Zhenyu Yang",
        "Kairui Zhang",
        "Yuhang Hu",
        "Bing Wang",
        "Shengsheng Qian",
        "Bin Wen",
        "Fan Yang",
        "Tingting Gao",
        "Weiming Dong",
        "Changsheng Xu"
      ],
      "abstract": "Despite significant progress in Video Large Language Models (Video-LLMs) for\noffline video understanding, existing online Video-LLMs typically struggle to\nsimultaneously process continuous frame-by-frame inputs and determine optimal\nresponse timing, often compromising real-time responsiveness and narrative\ncoherence. To address these limitations, we introduce LiveStar, a pioneering\nlive streaming assistant that achieves always-on proactive responses through\nadaptive streaming decoding. Specifically, LiveStar incorporates: (1) a\ntraining strategy enabling incremental video-language alignment for\nvariable-length video streams, preserving temporal consistency across\ndynamically evolving frame sequences; (2) a response-silence decoding framework\nthat determines optimal proactive response timing via a single forward pass\nverification; (3) memory-aware acceleration via peak-end memory compression for\nonline inference on 10+ minute videos, combined with streaming key-value cache\nto achieve 1.53x faster inference. We also construct an OmniStar dataset, a\ncomprehensive dataset for training and benchmarking that encompasses 15 diverse\nreal-world scenarios and 5 evaluation tasks for online video understanding.\nExtensive experiments across three benchmarks demonstrate LiveStar's\nstate-of-the-art performance, achieving an average 19.5% improvement in\nsemantic correctness with 18.1% reduced timing difference compared to existing\nonline Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.\nOur model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
      "pdf_url": "http://arxiv.org/pdf/2511.05299v1",
      "published": "2025-11-07T15:00:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05299v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DeepEyesV2: Toward Agentic Multimodal Model",
      "authors": [
        "Jack Hong",
        "Chenxiao Zhao",
        "ChengLin Zhu",
        "Weiheng Lu",
        "Guohai Xu",
        "Xing Yu"
      ],
      "abstract": "Agentic multimodal models should not only comprehend text and images, but\nalso actively invoke external tools, such as code execution environments and\nweb search, and integrate these operations into reasoning. In this work, we\nintroduce DeepEyesV2 and explore how to build an agentic multimodal model from\nthe perspectives of data construction, training methods, and model evaluation.\nWe observe that direct reinforcement learning alone fails to induce robust\ntool-use behavior. This phenomenon motivates a two-stage training pipeline: a\ncold-start stage to establish tool-use patterns, and reinforcement learning\nstage to further refine tool invocation. We curate a diverse, moderately\nchallenging training dataset, specifically including examples where tool use is\nbeneficial. We further introduce RealX-Bench, a comprehensive benchmark\ndesigned to evaluate real-world multimodal reasoning, which inherently requires\nthe integration of multiple capabilities, including perception, search, and\nreasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative\nbenchmarks, demonstrating its effectiveness across real-world understanding,\nmathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2\nexhibits task-adaptive tool invocation, tending to use image operations for\nperception tasks and numerical computations for reasoning tasks. Reinforcement\nlearning further enables complex tool combinations and allows model to\nselectively invoke tools based on context. We hope our study can provide\nguidance for community in developing agentic multimodal models.",
      "pdf_url": "http://arxiv.org/pdf/2511.05271v1",
      "published": "2025-11-07T14:31:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05271v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems",
      "authors": [
        "Ishan Kavathekar",
        "Hemang Jain",
        "Ameya Rathod",
        "Ponnurangam Kumaraguru",
        "Tanuja Ganu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities as\nautonomous agents through tool use, planning, and decision-making abilities,\nleading to their widespread adoption across diverse tasks. As task complexity\ngrows, multi-agent LLM systems are increasingly used to solve problems\ncollaboratively. However, safety and security of these systems remains largely\nunder-explored. Existing benchmarks and datasets predominantly focus on\nsingle-agent settings, failing to capture the unique vulnerabilities of\nmulti-agent dynamics and co-ordination. To address this gap, we introduce\n$\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent\n$\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the\nrobustness and safety of multi-agent LLM systems. TAMAS includes five distinct\nscenarios comprising 300 adversarial instances across six attack types and 211\ntools, along with 100 harmless tasks. We assess system performance across ten\nbackbone LLMs and three agent interaction configurations from Autogen and\nCrewAI frameworks, highlighting critical challenges and failure modes in\ncurrent multi-agent deployments. Furthermore, we introduce Effective Robustness\nScore (ERS) to assess the tradeoff between safety and task effectiveness of\nthese frameworks. Our findings show that multi-agent systems are highly\nvulnerable to adversarial attacks, underscoring the urgent need for stronger\ndefenses. TAMAS provides a foundation for systematically studying and improving\nthe safety of multi-agent LLM systems.",
      "pdf_url": "http://arxiv.org/pdf/2511.05269v1",
      "published": "2025-11-07T14:30:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05269v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage",
      "authors": [
        "Gabriel Serrão Seabra",
        "Nikolaj T. Mücke",
        "Vinicius Luiz Santos Silva",
        "Alexandre A. Emerick",
        "Denis Voskov",
        "Femke Vossepoel"
      ],
      "abstract": "Accurate characterization of subsurface heterogeneity is important for the\nsafe and effective implementation of geological carbon storage (GCS) projects.\nThis paper explores how machine learning methods can enhance data assimilation\nfor GCS with a framework that integrates score-based diffusion models with\nmachine learning-enhanced localization in channelized reservoirs during CO$_2$\ninjection. We employ a machine learning-enhanced localization framework that\nuses large ensembles ($N_s = 5000$) with permeabilities generated by the\ndiffusion model and states computed by simple ML algorithms to improve\ncovariance estimation for the Ensemble Smoother with Multiple Data Assimilation\n(ESMDA). We apply ML algorithms to a prior ensemble of channelized permeability\nfields, generated with the geostatistical model FLUVSIM. Our approach is\napplied on a CO$_2$ injection scenario simulated using the Delft Advanced\nResearch Terra Simulator (DARTS). Our ML-based localization maintains\nsignificantly more ensemble variance than when localization is not applied,\nwhile achieving comparable data-matching quality. This framework has practical\nimplications for GCS projects, helping improve the reliability of uncertainty\nquantification for risk assessment.",
      "pdf_url": "http://arxiv.org/pdf/2511.05266v1",
      "published": "2025-11-07T14:28:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05266v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones",
      "authors": [
        "Taihelong Zeng",
        "Yun Lin",
        "Yuhe Shi",
        "Yan Li",
        "Zhiqing Wei",
        "Xuanru Ji"
      ],
      "abstract": "The emergence of truck-drone collaborative systems in last-mile logistics has\npositioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal\nextension of classical routing optimization, where synchronized vehicle\ncoordination promises substantial operational efficiency and reduced\nenvironmental impact, yet introduces NP-hard combinatorial complexity beyond\nthe reach of conventional optimization paradigms. Deep reinforcement learning\noffers a theoretically grounded framework to address TSP-D's inherent\nchallenges through self-supervised policy learning and adaptive\ndecision-making. This study proposes a hierarchical Actor-Critic deep\nreinforcement learning framework for solving the TSP-D problem. The\narchitecture consists of two primary components: a Transformer-inspired encoder\nand an efficient Minimal Gated Unit decoder. The encoder incorporates a novel,\noptimized k-nearest neighbors sparse attention mechanism specifically for\nfocusing on relevant spatial relationships, further enhanced by the integration\nof global node features. The Minimal Gated Unit decoder processes these encoded\nrepresentations to efficiently generate solution sequences. The entire\nframework operates within an asynchronous advantage actor-critic paradigm.\nExperimental results show that, on benchmark TSP-D instances of various scales\n(N=10 to 100), the proposed model can obtain competitive or even superior\nsolutions in shorter average computation times compared to high-performance\nheuristic algorithms and existing reinforcement learning methods. Moreover,\ncompared to advanced reinforcement learning algorithm benchmarks, the proposed\nframework significantly reduces the total training time required while\nachieving superior final performance, highlighting its notable advantage in\ntraining efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2511.05265v1",
      "published": "2025-11-07T14:26:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05265v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU",
      "authors": [
        "Qi Sun",
        "Dingju Zhou",
        "Lina Zhang"
      ],
      "abstract": "The analysis of character appearance frequency is essential for understanding\nnarrative structure, character prominence, and story progression in anime. In\nthis work, we introduce OregairuChar, a benchmark dataset designed for\nappearance frequency analysis in the anime series My Teen Romantic Comedy\nSNAFU. The dataset comprises 1600 manually selected frames from the third\nseason, annotated with 2860 bounding boxes across 11 main characters.\nOregairuChar captures diverse visual challenges, including occlusion, pose\nvariation, and inter-character similarity, providing a realistic basis for\nappearance-based studies. To enable quantitative research, we benchmark several\nobject detection models on the dataset and leverage their predictions for\nfine-grained, episode-level analysis of character presence over time. This\napproach reveals patterns of character prominence and their evolution within\nthe narrative. By emphasizing appearance frequency, OregairuChar serves as a\nvaluable resource for exploring computational narrative dynamics and\ncharacter-centric storytelling in stylized media.",
      "pdf_url": "http://arxiv.org/pdf/2511.05263v1",
      "published": "2025-11-07T14:25:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05263v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Gate-Based Quantum Genetic Algorithm for Real-Valued Global Optimization",
      "authors": [
        "Leandro C. Souza",
        "Laurent E. Dardenne",
        "Renato Portugal"
      ],
      "abstract": "We propose a gate-based Quantum Genetic Algorithm (QGA) for real-valued\nglobal optimization. In this model, individuals are represented by quantum\ncircuits whose measurement outcomes are decoded into real-valued vectors\nthrough binary discretization. Evolutionary operators act directly on circuit\nstructures, allowing mutation and crossover to explore the space of gate-based\nencodings. Both fixed-depth and variable-depth variants are introduced,\nenabling either uniform circuit complexity or adaptive structural evolution.\nFitness is evaluated through quantum sampling, using the mean decoded output of\nmeasurement outcomes as the argument of the objective function. To isolate the\nimpact of quantum resources, we compare gate sets with and without the Hadamard\ngate, showing that superposition consistently improves convergence and\nrobustness across benchmark functions such as the Rastrigin function.\nFurthermore, we demonstrate that introducing pairwise inter-individual\nentanglement in the population accelerates early convergence, revealing that\nquantum correlations among individuals provide an additional optimization\nadvantage. Together, these results show that both superposition and\nentanglement enhance the search dynamics of evolutionary quantum algorithms,\nestablishing gate-based QGAs as a promising framework for quantum-enhanced\nglobal optimization.",
      "pdf_url": "http://arxiv.org/pdf/2511.05254v1",
      "published": "2025-11-07T14:14:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05254v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks",
      "authors": [
        "Mohamed Sanim Akremi",
        "Rim Slama",
        "Hedi Tabia"
      ],
      "abstract": "Online continuous motion recognition is a hot topic of research since it is\nmore practical in real life application cases. Recently, Skeleton-based\napproaches have become increasingly popular, demonstrating the power of using\nsuch 3D temporal data. However, most of these works have focused on\nsegment-based recognition and are not suitable for the online scenarios. In\nthis paper, we propose an online recognition system for skeleton sequence\nstreaming composed from two main components: a detector and a classifier, which\nuse a Semi-Positive Definite (SPD) matrix representation and a Siamese network.\nThe powerful statistical representations for the skeletal data given by the SPD\nmatrices and the learning of their semantic similarity by the Siamese network\nenable the detector to predict time intervals of the motions throughout an\nunsegmented sequence. In addition, they ensure the classifier capability to\nrecognize the motion in each predicted interval. The proposed detector is\nflexible and able to identify the kinetic state continuously. We conduct\nextensive experiments on both hand gesture and body action recognition\nbenchmarks to prove the accuracy of our online recognition system which in most\ncases outperforms state-of-the-art performances.",
      "pdf_url": "http://arxiv.org/pdf/2511.05250v1",
      "published": "2025-11-07T14:09:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05250v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos",
      "authors": [
        "Mengqi Guo",
        "Bo Xu",
        "Yanyan Li",
        "Gim Hee Lee"
      ],
      "abstract": "Novel view synthesis from monocular videos of dynamic scenes with unknown\ncamera poses remains a fundamental challenge in computer vision and graphics.\nWhile recent advances in 3D representations such as Neural Radiance Fields\n(NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static\nscenes, they struggle with dynamic content and typically rely on pre-computed\ncamera poses. We present 4D3R, a pose-free dynamic neural rendering framework\nthat decouples static and dynamic components through a two-stage approach. Our\nmethod first leverages 3D foundational models for initial pose and geometry\nestimation, followed by motion-aware refinement. 4D3R introduces two key\ntechnical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that\ncombines transformer-based learned priors with SAM2 for robust dynamic object\nsegmentation, enabling more accurate camera pose refinement; and (2) an\nefficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses\ncontrol points with a deformation field MLP and linear blend skinning to model\ndynamic motion, significantly reducing computational cost while maintaining\nhigh-quality reconstruction. Extensive experiments on real-world dynamic\ndatasets demonstrate that our approach achieves up to 1.8dB PSNR improvement\nover state-of-the-art methods, particularly in challenging scenarios with large\ndynamic objects, while reducing computational requirements by 5x compared to\nprevious dynamic scene representations.",
      "pdf_url": "http://arxiv.org/pdf/2511.05229v1",
      "published": "2025-11-07T13:25:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05229v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Autonomous generation of different courses of action in mechanized combat operations",
      "authors": [
        "Johan Schubert",
        "Patrik Hansen",
        "Pontus Hörling",
        "Ronnie Johansson"
      ],
      "abstract": "In this paper, we propose a methodology designed to support decision-making\nduring the execution phase of military ground combat operations, with a focus\non one's actions. This methodology generates and evaluates recommendations for\nvarious courses of action for a mechanized battalion, commencing with an\ninitial set assessed by their anticipated outcomes. It systematically produces\nthousands of individual action alternatives, followed by evaluations aimed at\nidentifying alternative courses of action with superior outcomes. These\nalternatives are appraised in light of the opponent's status and actions,\nconsidering unit composition, force ratios, types of offense and defense, and\nanticipated advance rates. Field manuals evaluate battle outcomes and\nadvancement rates. The processes of generation and evaluation work\nconcurrently, yielding a variety of alternative courses of action. This\napproach facilitates the management of new course generation based on\npreviously evaluated actions. As the combat unfolds and conditions evolve,\nrevised courses of action are formulated for the decision-maker within a\nsequential decision-making framework.",
      "pdf_url": "http://arxiv.org/pdf/2511.05182v1",
      "published": "2025-11-07T12:02:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05182v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "H.4.2; I.2.3; I.2.6; I.2.8; J.7"
      ]
    },
    {
      "title": "No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models",
      "authors": [
        "Ragini Gupta",
        "Naman Raina",
        "Bo Chen",
        "Li Chen",
        "Claudiu Danilov",
        "Josh Eckhardt",
        "Keyshla Bernard",
        "Klara Nahrstedt"
      ],
      "abstract": "Modern IoT deployments for environmental sensing produce high volume\nspatiotemporal data to support downstream tasks such as forecasting, typically\npowered by machine learning models. While existing filtering and strategic\ndeployment techniques optimize collected data volume at the edge, they overlook\nhow variations in sampling frequencies and spatial coverage affect downstream\nmodel performance. In many forecasting models, incorporating data from\nadditional sensors denoise predictions by providing broader spatial contexts.\nThis interplay between sampling frequency, spatial coverage and different\nforecasting model architectures remain underexplored. This work presents a\nsystematic study of forecasting models - classical models (VAR), neural\nnetworks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs),\nand time series foundation models (TSFMs: Chronos Moirai, TimesFM) under\nvarying spatial sensor nodes density and sampling intervals using real-world\ntemperature data in a wireless sensor network. Our results show that STGNNs are\neffective when sensor deployments are sparse and sampling rate is moderate,\nleveraging spatial correlations via encoded graph structure to compensate for\nlimited coverage. In contrast, TSFMs perform competitively at high frequencies\nbut degrade when spatial coverage from neighboring sensors is reduced.\nCrucially, the multivariate TSFM Moirai outperforms all models by natively\nlearning cross-sensor dependencies. These findings offer actionable insights\nfor building efficient forecasting pipelines in spatio-temporal systems. All\ncode for model configurations, training, dataset, and logs are open-sourced for\nreproducibility:\nhttps://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models",
      "pdf_url": "http://arxiv.org/pdf/2511.05179v1",
      "published": "2025-11-07T11:50:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05179v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models",
      "authors": [
        "Davide Marincione",
        "Donato Crisostomi",
        "Roberto Dessi",
        "Emanuele Rodolà",
        "Emanuele Rossi"
      ],
      "abstract": "Foundation models capable of generalizing across species and tasks represent\na promising new frontier in bioacoustics, with NatureLM being one of the most\nprominent examples. While its domain-specific fine-tuning yields strong\nperformance on bioacoustic benchmarks, we observe that it also introduces\ntrade-offs in instruction-following flexibility. For instance, NatureLM\nachieves high accuracy when prompted for either the common or scientific name\nindividually, but its accuracy drops significantly when both are requested in a\nsingle prompt. We address this by applying a simple model merging strategy that\ninterpolates NatureLM with its base language model, recovering\ninstruction-following capabilities with minimal loss of domain expertise.\nFinally, we show that the merged model exhibits markedly stronger zero-shot\ngeneralization, achieving over a 200% relative improvement and setting a new\nstate-of-the-art in closed-set zero-shot classification of unseen species.",
      "pdf_url": "http://arxiv.org/pdf/2511.05171v1",
      "published": "2025-11-07T11:40:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05171v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "title": "Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model",
      "authors": [
        "Ahmad Hatahet",
        "Christoph Knieke",
        "Andreas Rausch"
      ],
      "abstract": "Software Architecture Descriptions (SADs) are essential for managing the\ninherent complexity of modern software systems. They enable high-level\narchitectural reasoning, guide design decisions, and facilitate effective\ncommunication among diverse stakeholders. However, in practice, SADs are often\nmissing, outdated, or poorly aligned with the system's actual implementation.\nConsequently, developers are compelled to derive architectural insights\ndirectly from source code-a time-intensive process that increases cognitive\nload, slows new developer onboarding, and contributes to the gradual\ndegradation of clarity over the system's lifetime. To address these issues, we\npropose a semi-automated generation of SADs from source code by integrating\nreverse engineering (RE) techniques with a Large Language Model (LLM). Our\napproach recovers both static and behavioral architectural views by extracting\na comprehensive component diagram, filtering architecturally significant\nelements (core components) via prompt engineering, and generating state machine\ndiagrams to model component behavior based on underlying code logic with\nfew-shots prompting. This resulting views representation offer a scalable and\nmaintainable alternative to traditional manual architectural documentation.\nThis methodology, demonstrated using C++ examples, highlights the potent\ncapability of LLMs to: 1) abstract the component diagram, thereby reducing the\nreliance on human expert involvement, and 2) accurately represent complex\nsoftware behaviors, especially when enriched with domain-specific knowledge\nthrough few-shot prompting. These findings suggest a viable path toward\nsignificantly reducing manual effort while enhancing system understanding and\nlong-term maintainability.",
      "pdf_url": "http://arxiv.org/pdf/2511.05165v1",
      "published": "2025-11-07T11:35:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05165v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks",
      "authors": [
        "Azhar Hussain Mozumder",
        "M. John Basha",
        "Chayapathi A. R"
      ],
      "abstract": "With more and more existing networks being transformed to Software-Defined\nNetworking (SDN), they need to be more secure and demand smarter ways of\ntraffic control. This work, SmartSecChain-SDN, is a platform that combines\nmachine learning based intrusion detection, blockchain-based storage of logs,\nand application-awareness-based priority in SDN networks. To detect network\nintrusions in a real-time, precision and low-false positives setup, the\nframework utilizes the application of advanced machine learning algorithms,\nnamely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is\nbased on the Hyperledger Fabric, which is a permissioned blockchain technology,\nto provide secure, scalable, and privacy-preserving storage and, thus,\nguarantee that the Intrusion Detection System (IDS) records cannot be altered\nand can be analyzed comprehensively. The system also has Quality of Service\n(QoS) rules and traffic shaping based on applications, which enables\nprioritization of critical services, such as VoIP, video conferencing, and\nbusiness applications, as well as de-prioritization of non-essential traffic,\nsuch as downloads and updates. Mininet can simulate real-time SDN scenarios\nbecause it is used to prototype whole architectures. It is also compatible with\ncontrollers OpenDaylight and Ryu. It has tested the framework using the InSDN\ndataset and proved that it can identify different kinds of cyberattacks and\nhandle bandwidth allocation efficiently under circumstances of resource\nconstraints. SmartSecChain-SDN comprehensively addresses SDN system protection,\nsecuring and enhancing. The proposed study offers an innovative, extensible way\nto improve cybersecurity, regulatory compliance, and the administration of\nnext-generation programmable networks.",
      "pdf_url": "http://arxiv.org/pdf/2511.05156v1",
      "published": "2025-11-07T11:22:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05156v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NI",
        "C.2.3"
      ]
    },
    {
      "title": "From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection",
      "authors": [
        "Jingsong Liu",
        "Han Li",
        "Nassir Navab",
        "Peter J. Schüffler"
      ],
      "abstract": "AI-based biomarkers can infer molecular features directly from hematoxylin &\neosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global\npatch-level embeddings and overlook cell-level morphology. We present a PFM\nmodel, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale\nself-supervised pretraining with cell-centric post-tuning and attention pooling\nto fuse local and global tokens. Across four tasks involving four biomarkers\nand eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2%\naverage improvement over prior PFMs, advancing interpretable and robust\nAI-based biomarker detection in digital pathology.",
      "pdf_url": "http://arxiv.org/pdf/2511.05150v1",
      "published": "2025-11-07T11:05:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05150v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DL101 Neural Network Outputs and Loss Functions",
      "authors": [
        "Fernando Berzal"
      ],
      "abstract": "The loss function used to train a neural network is strongly connected to its\noutput layer from a statistical point of view. This technical report analyzes\ncommon activation functions for a neural network output layer, like linear,\nsigmoid, ReLU, and softmax, detailing their mathematical properties and their\nappropriate use cases. A strong statistical justification exists for the\nselection of the suitable loss function for training a deep learning model.\nThis report connects common loss functions such as Mean Squared Error (MSE),\nMean Absolute Error (MAE), and various Cross-Entropy losses to the statistical\nprinciple of Maximum Likelihood Estimation (MLE). Choosing a specific loss\nfunction is equivalent to assuming a specific probability distribution for the\nmodel output, highlighting the link between these functions and the Generalized\nLinear Models (GLMs) that underlie network output layers. Additional scenarios\nof practical interest are also considered, such as alternative output\nencodings, constrained outputs, and distributions with heavy tails.",
      "pdf_url": "http://arxiv.org/pdf/2511.05131v1",
      "published": "2025-11-07T10:20:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05131v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Deep learning models are vulnerable, but adversarial examples are even more vulnerable",
      "authors": [
        "Jun Li",
        "Yanwei Xu",
        "Keran Li",
        "Xiaoli Zhang"
      ],
      "abstract": "Understanding intrinsic differences between adversarial examples and clean\nsamples is key to enhancing DNN robustness and detection against adversarial\nattacks. This study first empirically finds that image-based adversarial\nexamples are notably sensitive to occlusion. Controlled experiments on CIFAR-10\nused nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples,\npaired with original samples for evaluation. We introduce Sliding Mask\nConfidence Entropy (SMCE) to quantify model confidence fluctuation under\nocclusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy\nField Maps and statistical distributions show adversarial examples have\nsignificantly higher confidence volatility under occlusion than originals.\nBased on this, we propose Sliding Window Mask-based Adversarial Example\nDetection (SWM-AED), which avoids catastrophic overfitting of conventional\nadversarial training. Evaluations across classifiers and attacks on CIFAR-10\ndemonstrate robust performance, with accuracy over 62% in most cases and up to\n96.5%.",
      "pdf_url": "http://arxiv.org/pdf/2511.05073v1",
      "published": "2025-11-07T08:43:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05073v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation",
      "authors": [
        "Mingyu Sung",
        "Hyeonmin Choe",
        "Il-Min Kim",
        "Sangseok Yun",
        "Jae Mo Kang"
      ],
      "abstract": "Monocular depth estimation (MDE), inferring pixel-level depths in single RGB\nimages from a monocular camera, plays a crucial and pivotal role in a variety\nof AI applications demanding a three-dimensional (3D) topographical scene. In\nthe real-world scenarios, MDE models often need to be deployed in environments\nwith different conditions from those for training. Test-time (domain)\nadaptation (TTA) is one of the compelling and practical approaches to address\nthe issue. Although there have been notable advancements in TTA for MDE,\nparticularly in a self-supervised manner, existing methods are still\nineffective and problematic when applied to diverse and dynamic environments.\nTo break through this challenge, we propose a novel and high-performing TTA\nframework for MDE, named PITTA. Our approach incorporates two key innovative\nstrategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware\nimage masking. Specifically, PITTA enables highly effective TTA on a pretrained\nMDE network in a pose-agnostic manner without resorting to any camera pose\ninformation. Besides, our instance-aware masking strategy extracts\ninstance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.)\nfrom a segmentation mask produced by a pretrained panoptic segmentation\nnetwork, by removing static objects including background components. To further\nboost performance, we also present a simple yet effective edge extraction\nmethodology for the input image (i.e., a single monocular image) and depth map.\nExtensive experimental evaluations on DrivingStereo and Waymo datasets with\nvarying environmental conditions demonstrate that our proposed framework,\nPITTA, surpasses the existing state-of-the-art techniques with remarkable\nperformance improvements in MDE during TTA.",
      "pdf_url": "http://arxiv.org/pdf/2511.05055v1",
      "published": "2025-11-07T07:55:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05055v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs",
      "authors": [
        "Wakuto Matsumi",
        "Riaz-Ul-Haque Mian"
      ],
      "abstract": "Machine learning based on neural networks has advanced rapidly, but the high\nenergy consumption required for training and inference remains a major\nchallenge. Hyperdimensional Computing (HDC) offers a lightweight,\nbrain-inspired alternative that enables high parallelism but often suffers from\nlower accuracy on complex visual tasks. To overcome this, hybrid accelerators\ncombining HDC and Convolutional Neural Networks (CNNs) have been proposed,\nthough their adoption is limited by poor generalizability and programmability.\nThe rise of open-source RISC-V architectures has created new opportunities for\ndomain-specific GPU design. Unlike traditional proprietary GPUs, emerging\nRISC-V-based GPUs provide flexible, programmable platforms suitable for custom\ncomputation models such as HDC. In this study, we design and implement custom\nGPU instructions optimized for HDC operations, enabling efficient processing\nfor hybrid HDC-CNN workloads. Experimental results using four types of custom\nHDC instructions show a performance improvement of up to 56.2 times in\nmicrobenchmark tests, demonstrating the potential of RISC-V GPUs for\nenergy-efficient, high-performance computing.",
      "pdf_url": "http://arxiv.org/pdf/2511.05053v1",
      "published": "2025-11-07T07:50:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05053v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian",
      "authors": [
        "Mykyta Syromiatnikov",
        "Victoria Ruvinskaya"
      ],
      "abstract": "Evaluating the real capabilities of large language models in low-resource\nlanguages still represents a challenge, as many existing benchmarks focus on\nwidespread tasks translated from English or evaluate only simple language\nunderstanding. This paper introduces UA-Code-Bench, a new open-source benchmark\nestablished for a thorough evaluation of language models' code generation and\ncompetitive programming problem-solving abilities in Ukrainian. The benchmark\ncomprises 500 problems from the Eolymp platform, evenly distributed across five\ncomplexity levels from very easy to very hard. A diverse set of 13 leading\nproprietary and open-source models, generating Python solutions based on a\none-shot prompt, was evaluated via the dedicated Eolymp environment against\nhidden tests, ensuring code correctness. The obtained results reveal that even\ntop-performing models, such as OpenAI o3 and GPT-5, solve only half of the\nproblems, highlighting the challenge of code generation in low-resource natural\nlanguage. Furthermore, this research presents a comprehensive analysis of\nperformance across various difficulty levels, as well as an assessment of\nsolution uniqueness and computational efficiency, measured by both elapsed time\nand memory consumption of the generated solutions. In conclusion, this work\ndemonstrates the value of competitive programming benchmarks in evaluating\nlarge language models, especially in underrepresented languages. It also paves\nthe way for future research on multilingual code generation and\nreasoning-enhanced models. The benchmark, data parsing, preparation, code\ngeneration, and evaluation scripts are available at\nhttps://huggingface.co/datasets/NLPForUA/ua-code-bench.",
      "pdf_url": "http://arxiv.org/pdf/2511.05040v1",
      "published": "2025-11-07T07:24:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05040v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "PECL: A Heterogeneous Parallel Multi-Domain Network for Radar-Based Human Activity Recognition",
      "authors": [
        "Jiuqi Yan",
        "Chendong Xu",
        "Dongyu Liu"
      ],
      "abstract": "Radar systems are increasingly favored for medical applications because they\nprovide non-intrusive monitoring with high privacy and robustness to lighting\nconditions. However, existing research typically relies on single-domain radar\nsignals and overlooks the temporal dependencies inherent in human activity,\nwhich complicates the classification of similar actions. To address this issue,\nwe designed the Parallel-EfficientNet-CBAM-LSTM (PECL) network to process data\nin three complementary domains: Range-Time, Doppler-Time, and Range-Doppler.\nPECL combines a channel-spatial attention module and temporal units to capture\nmore features and dynamic dependencies during action sequences, improving both\naccuracy and robustness. The experimental results show that PECL achieves an\naccuracy of 96.16% on the same dataset, outperforming existing methods by at\nleast 4.78%. PECL also performs best in distinguishing between easily confused\nactions. Despite its strong performance, PECL maintains moderate model\ncomplexity, with 23.42M parameters and 1324.82M FLOPs. Its parameter-efficient\ndesign further reduces computational cost.",
      "pdf_url": "http://arxiv.org/pdf/2511.05039v1",
      "published": "2025-11-07T07:22:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05039v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation",
      "authors": [
        "Jing Jin",
        "Xu Liu",
        "Te Gao",
        "Zhihong Shi",
        "Yixiong Liang",
        "Ruiqing Zheng",
        "Hulin Kuang",
        "Min Zeng",
        "Shichao Kan"
      ],
      "abstract": "Whole Slide Image (WSI) representation is critical for cancer subtyping,\ncancer recognition and mutation prediction.Training an end-to-end WSI\nrepresentation model poses significant challenges, as a standard gigapixel\nslide can contain tens of thousands of image tiles, making it difficult to\ncompute gradients of all tiles in a single mini-batch due to current GPU\nlimitations. To address this challenge, we propose a method of dynamic residual\nencoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI\nrepresentation. Our approach utilizes a memory bank to store the features of\ntiles across all WSIs in the dataset. During training, a mini-batch usually\ncontains multiple WSIs. For each WSI in the batch, a subset of tiles is\nrandomly sampled and their features are computed using a tile encoder. Then,\nadditional tile features from the same WSI are selected from the memory bank.\nThe representation of each individual WSI is generated using a residual\nencoding technique that incorporates both the sampled features and those\nretrieved from the memory bank. Finally, the slide-level contrastive loss is\ncomputed based on the representations and histopathology reports ofthe WSIs\nwithin the mini-batch. Experiments conducted over cancer subtyping, cancer\nrecognition, and mutation prediction tasks proved the effectiveness of the\nproposed DRE-SLCL method.",
      "pdf_url": "http://arxiv.org/pdf/2511.05034v1",
      "published": "2025-11-07T07:17:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05034v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4.9; I.2.10"
      ]
    },
    {
      "title": "OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data",
      "authors": [
        "Dongjin Park",
        "Hasung Yeo",
        "Joon-Woo Lee"
      ],
      "abstract": "Federated fine-tuning (FFT) adapts foundation models to decentralized data\nbut remains fragile under heterogeneous client distributions due to local\ndrift, i.e., client-level update divergences that induce systematic bias and\namplified variance in the global model. Existing aggregation and\npersonalization methods largely correct drift post hoc, which proves brittle\nunder extreme non-IID conditions. We introduce OvA-LP, a minimalist framework\nthat is, to our knowledge, the first explicitly designed to suppress drift at\nits source within the PEFT-based FFT paradigm. OvA-LP combines linear probing\non a frozen encoder with a one-vs-all head and a simple two-stage procedure,\npreserving pretrained feature geometry and decoupling logits to prevent the\nmechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over\nshard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of\nits IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1%\n(PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains\nresilience under both symmetric and asymmetric label noise. In addition,\nprecomputing encoder features makes per-round cost nearly independent of\nencoder size. Together, these results demonstrate that OvA-LP provides a\nprincipled and efficient basis for robust FFT under heterogeneity.",
      "pdf_url": "http://arxiv.org/pdf/2511.05028v1",
      "published": "2025-11-07T07:00:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05028v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "8bit-GPT: Exploring Human-AI Interaction on Obsolete Macintosh Operating Systems",
      "authors": [
        "Hala Sheta"
      ],
      "abstract": "The proliferation of assistive chatbots offering efficient, personalized\ncommunication has driven widespread over-reliance on them for decision-making,\ninformation-seeking and everyday tasks. This dependence was found to have\nadverse consequences on information retention as well as lead to superficial\nemotional attachment. As such, this work introduces 8bit-GPT; a language model\nsimulated on a legacy Macintosh Operating System, to evoke reflection on the\nnature of Human-AI interaction and the consequences of anthropomorphic\nrhetoric. Drawing on reflective design principles such as slow-technology and\ncounterfunctionality, this work aims to foreground the presence of chatbots as\na tool by defamiliarizing the interface and prioritizing inefficient\ninteraction, creating a friction between the familiar and not.",
      "pdf_url": "http://arxiv.org/pdf/2511.05025v1",
      "published": "2025-11-07T06:56:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05025v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies",
      "authors": [
        "Prasoon Varshney",
        "Makesh Narsimhan Sreedhar",
        "Liwei Jiang",
        "Traian Rebedea",
        "Christopher Parisien"
      ],
      "abstract": "Large language models (LLMs) are typically aligned to a universal set of\nsafety and usage principles intended for broad public acceptability. Yet,\nreal-world applications of LLMs often take place within organizational\necosystems shaped by distinctive corporate policies, regulatory requirements,\nuse cases, brand guidelines, and ethical commitments. This reality highlights\nthe need for rigorous and comprehensive evaluation of LLMs with pluralistic\nalignment goals, an alignment paradigm that emphasizes adaptability to diverse\nuser values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE\n(PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs'\ncapacity to adhere to pluralistic alignment specifications in multi-turn,\ninteractive conversations. PBSUITE consists of (1) a diverse dataset of 300\nrealistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic\nevaluation framework for stress-testing model compliance with custom behavioral\nspecifications under adversarial conditions. Using PBSUITE, We find that\nleading open- and closed-source LLMs maintain robust adherence to behavioral\npolicies in single-turn settings (less than 4% failure rates), but their\ncompliance weakens substantially in multi-turn adversarial interactions (up to\n84% failure rates). These findings highlight that existing model alignment and\nsafety moderation methods fall short in coherently enforcing pluralistic\nbehavioral policies in real-world LLM interactions. Our work contributes both\nthe dataset and analytical framework to support future research toward robust\nand context-aware pluralistic alignment techniques.",
      "pdf_url": "http://arxiv.org/pdf/2511.05018v1",
      "published": "2025-11-07T06:43:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05018v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multi-agent Coordination via Flow Matching",
      "authors": [
        "Dongsu Lee",
        "Daehee Lee",
        "Amy Zhang"
      ],
      "abstract": "This work presents MAC-Flow, a simple yet expressive framework for\nmulti-agent coordination. We argue that requirements of effective coordination\nare twofold: (i) a rich representation of the diverse joint behaviors present\nin offline data and (ii) the ability to act efficiently in real time. However,\nprior approaches often sacrifice one for the other, i.e., denoising\ndiffusion-based solutions capture complex coordination but are computationally\nslow, while Gaussian policy-based solutions are fast but brittle in handling\nmulti-agent interaction. MAC-Flow addresses this trade-off by first learning a\nflow-based representation of joint behaviors, and then distilling it into\ndecentralized one-step policies that preserve coordination while enabling fast\nexecution. Across four different benchmarks, including $12$ environments and\n$34$ datasets, MAC-Flow alleviates the trade-off between performance and\ncomputational cost, specifically achieving about $\\boldsymbol{\\times14.5}$\nfaster inference compared to diffusion-based MARL methods, while maintaining\ngood performance. At the same time, its inference speed is similar to that of\nprior Gaussian policy-based offline multi-agent reinforcement learning (MARL)\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2511.05005v1",
      "published": "2025-11-07T06:24:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05005v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval",
      "authors": [
        "Hyunkyu Kim",
        "Yeeun Yoo",
        "Youngjun Kwak"
      ],
      "abstract": "As financial applications of large language models (LLMs) gain attention,\naccurate Information Retrieval (IR) remains crucial for reliable AI services.\nHowever, existing benchmarks fail to capture the complex and domain-specific\ninformation needs of real-world banking scenarios. Building domain-specific IR\nbenchmarks is costly and constrained by legal restrictions on using real\ncustomer data. To address these challenges, we propose a systematic methodology\nfor constructing domain-specific IR benchmarks through LLM-based query\ngeneration. As a concrete implementation of this methodology, our pipeline\ncombines single and multi-document query generation with an enhanced and\nreasoning-augmented answerability assessment method, achieving stronger\nalignment with human judgments than prior approaches. Using this methodology,\nwe construct KoBankIR, comprising 815 queries derived from 204 official banking\ndocuments. Our experiments show that existing retrieval models struggle with\nthe complex multi-document queries in KoBankIR, demonstrating the value of our\nsystematic approach for domain-specific benchmark construction and underscoring\nthe need for improved retrieval techniques in financial domains.",
      "pdf_url": "http://arxiv.org/pdf/2511.05000v1",
      "published": "2025-11-07T06:06:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05000v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records",
      "authors": [
        "Daniel S. Lee",
        "Mayra S. Haedo-Cruz",
        "Chen Jiang",
        "Oshin Miranda",
        "LiRong Wang"
      ],
      "abstract": "Transformer-based deep learning models have shown promise for disease risk\nprediction using electronic health records(EHRs), but modeling temporal\ndependencies remains a key challenge due to irregular visit intervals and lack\nof uniform structure. We propose a Bi-Positional Embedding Transformer Encoder\nor BiPETE for single-disease prediction, which integrates rotary positional\nembeddings to encode relative visit timing and sinusoidal embeddings to\npreserve visit order. Without relying on large-scale pretraining, BiPETE is\ntrained on EHR data from two mental health cohorts-depressive disorder and\npost-traumatic stress disorder (PTSD)-to predict the risk of alcohol and\nsubstance use disorders (ASUD). BiPETE outperforms baseline models, improving\nthe area under the precision-recall curve (AUPRC) by 34% and 50% in the\ndepression and PTSD cohorts, respectively. An ablation study further confirms\nthe effectiveness of the dual positional encoding strategy. We apply the\nIntegrated Gradients method to interpret model predictions, identifying key\nclinical features associated with ASUD risk and protection, such as abnormal\ninflammatory, hematologic, and metabolic markers, as well as specific\nmedications and comorbidities. Overall, these key clinical features identified\nby the attribution methods contribute to a deeper understanding of the risk\nassessment process and offer valuable clues for mitigating potential risks. In\nsummary, our study presents a practical and interpretable framework for disease\nrisk prediction using EHR data, which can achieve strong performance.",
      "pdf_url": "http://arxiv.org/pdf/2511.04998v1",
      "published": "2025-11-07T06:01:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04998v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ]
    },
    {
      "title": "Enhancing Public Speaking Skills in Engineering Students Through AI",
      "authors": [
        "Amol Harsh",
        "Brainerd Prince",
        "Siddharth Siddharth",
        "Deepan Raj Prabakar Muthirayan",
        "Kabir S Bhalla",
        "Esraaj Sarkar Gupta",
        "Siddharth Sahu"
      ],
      "abstract": "This research-to-practice full paper was inspired by the persistent challenge\nin effective communication among engineering students. Public speaking is a\nnecessary skill for future engineers as they have to communicate technical\nknowledge with diverse stakeholders. While universities offer courses or\nworkshops, they are unable to offer sustained and personalized training to\nstudents. Providing comprehensive feedback on both verbal and non-verbal\naspects of public speaking is time-intensive, making consistent and\nindividualized assessment impractical. This study integrates research on verbal\nand non-verbal cues in public speaking to develop an AI-driven assessment model\nfor engineering students. Our approach combines speech analysis, computer\nvision, and sentiment detection into a multi-modal AI system that provides\nassessment and feedback. The model evaluates (1) verbal communication (pitch,\nloudness, pacing, intonation), (2) non-verbal communication (facial\nexpressions, gestures, posture), and (3) expressive coherence, a novel\nintegration ensuring alignment between speech and body language. Unlike\nprevious systems that assess these aspects separately, our model fuses multiple\nmodalities to deliver personalized, scalable feedback. Preliminary testing\ndemonstrated that our AI-generated feedback was moderately aligned with expert\nevaluations. Among the state-of-the-art AI models evaluated, all of which were\nLarge Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro\nemerged as the best-performing, showing the strongest agreement with human\nannotators. By eliminating reliance on human evaluators, this AI-driven public\nspeaking trainer enables repeated practice, helping students naturally align\ntheir speech with body language and emotion, crucial for impactful and\nprofessional communication.",
      "pdf_url": "http://arxiv.org/pdf/2511.04995v1",
      "published": "2025-11-07T05:44:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04995v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ]
    }
  ]
}