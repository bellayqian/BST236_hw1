{
  "last_updated": "2025-02-18T00:43:24.605447",
  "papers": [
    {
      "title": "Region-Adaptive Sampling for Diffusion Transformers",
      "authors": [
        "Ziming Liu",
        "Yifan Yang",
        "Chengruidong Zhang",
        "Yiqi Zhang",
        "Lili Qiu",
        "Yang You",
        "Yuqing Yang"
      ],
      "abstract": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
      "pdf_url": "http://arxiv.org/pdf/2502.10389v1",
      "published": "2025-02-14T18:59:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10389v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Simplifying DINO via Coding Rate Regularization",
      "authors": [
        "Ziyang Wu",
        "Jingyuan Zhang",
        "Druv Pai",
        "XuDong Wang",
        "Chandan Singh",
        "Jianwei Yang",
        "Jianfeng Gao",
        "Yi Ma"
      ],
      "abstract": "DINO and DINOv2 are two model families being widely used to learn\nrepresentations from unlabeled imagery data at large scales. Their learned\nrepresentations often enable state-of-the-art performance for downstream tasks,\nsuch as image classification and segmentation. However, they employ many\nempirically motivated design choices and their training pipelines are highly\ncomplex and unstable -- many hyperparameters need to be carefully tuned to\nensure that the representations do not collapse -- which poses considerable\ndifficulty to improving them or adapting them to new domains. In this work, we\nposit that we can remove most such-motivated idiosyncrasies in the pre-training\npipelines, and only need to add an explicit coding rate term in the loss\nfunction to avoid collapse of the representations. As a result, we obtain\nhighly simplified variants of the DINO and DINOv2 which we call SimDINO and\nSimDINOv2, respectively. Remarkably, these simplified models are more robust to\ndifferent design choices, such as network architecture and hyperparameters, and\nthey learn even higher-quality representations, measured by performance on\ndownstream tasks, offering a Pareto improvement over the corresponding DINO and\nDINOv2 models. This work highlights the potential of using simplifying design\nprinciples to improve the empirical practice of deep learning.",
      "pdf_url": "http://arxiv.org/pdf/2502.10385v1",
      "published": "2025-02-14T18:58:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10385v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Representation and Interpretation in Artificial and Natural Computing",
      "authors": [
        "Luis A. Pineda"
      ],
      "abstract": "Artificial computing machinery transforms representations through an\nobjective process, to be interpreted subjectively by humans, so the machine and\nthe interpreter are different entities, but in the putative natural computing\nboth processes are performed by the same agent. The method or process that\ntransforms a representation is called here \\emph{the mode of computing}. The\nmode used by digital computers is the algorithmic one, but there are others,\nsuch as quantum computers and diverse forms of non-conventional computing, and\nthere is an open-ended set of representational formats and modes that could be\nused in artificial and natural computing. A mode based on a notion of computing\ndifferent from Turing's may perform feats beyond what the Turing Machine does\nbut the modes would not be of the same kind and could not be compared. For a\nmode of computing to be more powerful than the algorithmic one, it ought to\ncompute functions lacking an effective algorithm, and Church Thesis would not\nhold. Here, a thought experiment including a computational demon using a\nhypothetical mode for such an effect is presented. If there is natural\ncomputing, there is a mode of natural computing whose properties may be causal\nto the phenomenological experience. Discovering it would come with solving the\nhard problem of consciousness; but if it turns out that such a mode does not\nexist, there is no such thing as natural computing, and the mind is not a\ncomputational process.",
      "pdf_url": "http://arxiv.org/pdf/2502.10383v1",
      "published": "2025-02-14T18:57:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10383v1",
      "categories": [
        "cs.AI",
        "F.0"
      ]
    },
    {
      "title": "OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models",
      "authors": [
        "William Chen",
        "Jinchuan Tian",
        "Yifan Peng",
        "Brian Yan",
        "Chao-Han Huck Yang",
        "Shinji Watanabe"
      ],
      "abstract": "Neural scaling laws offer valuable insights for designing robust sequence\nprocessing architectures. While these laws have been extensively characterized\nin other modalities, their behavior in speech remains comparatively\nunderexplored. In this work, we introduce OWLS, an open-access, reproducible\nsuite of multilingual speech recognition and translation models spanning 0.25B\nto 18B parameters, with the 18B version being the largest speech model, to the\nbest of our knowledge. OWLS leverages up to 360K hours of public speech data\nacross 150 languages, enabling a systematic investigation into how data, model,\nand compute scaling each influence performance in multilingual speech tasks. We\nuse OWLS to derive neural scaling laws, showing how final performance can be\nreliably predicted when scaling. One of our key findings is that scaling\nenhances performance on low-resource languages/dialects, helping to mitigate\nbias and improve the accessibility of speech technologies. Finally, we show how\nOWLS can be used to power new research directions by discovering emergent\nabilities in large-scale speech models. Model checkpoints will be released on\nhttps://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d\nfor future studies.",
      "pdf_url": "http://arxiv.org/pdf/2502.10373v1",
      "published": "2025-02-14T18:51:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10373v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds",
      "authors": [
        "Huayi Wang",
        "Zirui Wang",
        "Junli Ren",
        "Qingwei Ben",
        "Tao Huang",
        "Weinan Zhang",
        "Jiangmiao Pang"
      ],
      "abstract": "Traversing risky terrains with sparse footholds poses a significant challenge\nfor humanoid robots, requiring precise foot placements and stable locomotion.\nExisting approaches designed for quadrupedal robots often fail to generalize to\nhumanoid robots due to differences in foot geometry and unstable morphology,\nwhile learning-based approaches for humanoid locomotion still face great\nchallenges on complex terrains due to sparse foothold reward signals and\ninefficient learning processes. To address these challenges, we introduce\nBeamDojo, a reinforcement learning (RL) framework designed for enabling agile\nhumanoid locomotion on sparse footholds. BeamDojo begins by introducing a\nsampling-based foothold reward tailored for polygonal feet, along with a double\ncritic to balancing the learning process between dense locomotion rewards and\nsparse foothold rewards. To encourage sufficient trail-and-error exploration,\nBeamDojo incorporates a two-stage RL approach: the first stage relaxes the\nterrain dynamics by training the humanoid on flat terrain while providing it\nwith task terrain perceptive observations, and the second stage fine-tunes the\npolicy on the actual task terrain. Moreover, we implement a onboard LiDAR-based\nelevation map to enable real-world deployment. Extensive simulation and\nreal-world experiments demonstrate that BeamDojo achieves efficient learning in\nsimulation and enables agile locomotion with precise foot placement on sparse\nfootholds in the real world, maintaining a high success rate even under\nsignificant external disturbances.",
      "pdf_url": "http://arxiv.org/pdf/2502.10363v1",
      "published": "2025-02-14T18:42:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10363v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "STAR: Spectral Truncation and Rescale for Model Merging",
      "authors": [
        "Yu-Ang Lee",
        "Ching-Yun Ko",
        "Tejaswini Pedapati",
        "I-Hsin Chung",
        "Mi-Yen Yeh",
        "Pin-Yu Chen"
      ],
      "abstract": "Model merging is an efficient way of obtaining a multi-task model from\nseveral pretrained models without further fine-tuning, and it has gained\nattention in various domains, including natural language processing (NLP).\nDespite the efficiency, a key challenge in model merging is the seemingly\ninevitable decrease in task performance as the number of models increases. In\nthis paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd\n$\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by\ntruncating small components in the respective spectral spaces, which is\nfollowed by an automatic parameter rescaling scheme to retain the nuclear norm\nof the original matrix. STAR requires no additional inference on original\ntraining data and is robust to hyperparamater choice. We demonstrate the\neffectiveness of STAR through extensive model merging cases on diverse NLP\ntasks. Specifically, STAR works robustly across varying model sizes, and can\noutperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is\npublicly available at https://github.com/IBM/STAR.",
      "pdf_url": "http://arxiv.org/pdf/2502.10339v1",
      "published": "2025-02-14T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10339v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering",
      "authors": [
        "Nick Ferguson",
        "Liane Guillou",
        "Alan Bundy",
        "Kwabena Nuamah"
      ],
      "abstract": "Large Language Models (LLMs) excel in natural language tasks but still face\nchallenges in Question Answering (QA) tasks requiring complex, multi-step\nreasoning. We outline the types of reasoning required in some of these tasks,\nand reframe them in terms of meta-level reasoning (akin to high-level strategic\nreasoning or planning) and object-level reasoning (embodied in lower-level\ntasks such as mathematical reasoning). Franklin, a novel dataset with\nrequirements of meta- and object-level reasoning, is introduced and used along\nwith three other datasets to evaluate four LLMs at question answering tasks\nrequiring multiple steps of reasoning. Results from human annotation studies\nsuggest LLMs demonstrate meta-level reasoning with high frequency, but struggle\nwith object-level reasoning tasks in some of the datasets used. Additionally,\nevidence suggests that LLMs find the object-level reasoning required for the\nquestions in the Franklin dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning requirements.",
      "pdf_url": "http://arxiv.org/pdf/2502.10338v1",
      "published": "2025-02-14T17:55:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10338v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Process Reward Models for LLM Agents: Practical Framework and Directions",
      "authors": [
        "Sanjiban Choudhury"
      ],
      "abstract": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable\nframework for training LLM agents to continually improve through interactions.\nAgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo\nrollouts to compute reward targets and optimize policies. It requires minimal\nmodifications to existing RLHF pipelines, making it easy to integrate at scale.\nBeyond AgentPRM, we propose InversePRM, which learns process rewards directly\nfrom demonstrations without explicit outcome supervision. We also explore key\nchallenges and opportunities, including exploration, process reward shaping,\nand model-predictive reasoning. We evaluate on ALFWorld benchmark, show that\nsmall 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o\nbaselines, and analyze test-time scaling, reward hacking, and more. Our code is\navailable at: https://github.com/sanjibanc/agent_prm.",
      "pdf_url": "http://arxiv.org/pdf/2502.10325v1",
      "published": "2025-02-14T17:34:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10325v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ExplainReduce: Summarising local explanations via proxies",
      "authors": [
        "Lauri Seppäläinen",
        "Mudong Guo",
        "Kai Puolamäki"
      ],
      "abstract": "Most commonly used non-linear machine learning methods are closed-box models,\nuninterpretable to humans. The field of explainable artificial intelligence\n(XAI) aims to develop tools to examine the inner workings of these closed\nboxes. An often-used model-agnostic approach to XAI involves using simple\nmodels as local approximations to produce so-called local explanations;\nexamples of this approach include LIME, SHAP, and SLISEMAP. This paper shows\nhow a large set of local explanations can be reduced to a small \"proxy set\" of\nsimple models, which can act as a generative global explanation. This reduction\nprocedure, ExplainReduce, can be formulated as an optimisation problem and\napproximated efficiently using greedy heuristics.",
      "pdf_url": "http://arxiv.org/pdf/2502.10311v1",
      "published": "2025-02-14T17:14:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10311v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "I.2.4"
      ]
    },
    {
      "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "authors": [
        "Ermis Soumalias",
        "Yanchen Jiang",
        "Kehang Zhu",
        "Michael Curry",
        "Sven Seuken",
        "David C. Parkes"
      ],
      "abstract": "We study the potential of large language models (LLMs) as proxies for humans\nto simplify preference elicitation (PE) in combinatorial assignment. While\ntraditional PE methods rely on iterative queries to capture preferences, LLMs\noffer a one-shot alternative with reduced human effort. We propose a framework\nfor LLM proxies that can work in tandem with SOTA ML-powered preference\nelicitation schemes. Our framework handles the novel challenges introduced by\nLLMs, such as response variability and increased computational costs. We\nexperimentally evaluate the efficiency of LLM proxies against human queries in\nthe well-studied course allocation domain, and we investigate the model\ncapabilities required for success. We find that our approach improves\nallocative efficiency by up to 20%, and these results are robust across\ndifferent LLMs and to differences in quality and accuracy of reporting.",
      "pdf_url": "http://arxiv.org/pdf/2502.10308v1",
      "published": "2025-02-14T17:12:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10308v1",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ]
    },
    {
      "title": "Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations",
      "authors": [
        "Abdelrhman Shaheen",
        "Anas Badr",
        "Ali Abohendy",
        "Hatem Alsaadawy",
        "Nadine Alsayad"
      ],
      "abstract": "Reinforcement Learning (RL) has been widely used in many applications,\nparticularly in gaming, which serves as an excellent training ground for AI\nmodels. Google DeepMind has pioneered innovations in this field, employing\nreinforcement learning algorithms, including model-based, model-free, and deep\nQ-network approaches, to create advanced AI models such as AlphaGo, AlphaGo\nZero, and MuZero. AlphaGo, the initial model, integrates supervised learning\nand reinforcement learning to master the game of Go, surpassing professional\nhuman players. AlphaGo Zero refines this approach by eliminating reliance on\nhuman gameplay data, instead utilizing self-play for enhanced learning\nefficiency. MuZero further extends these advancements by learning the\nunderlying dynamics of game environments without explicit knowledge of the\nrules, achieving adaptability across various games, including complex Atari\ngames. This paper reviews the significance of reinforcement learning\napplications in Atari and strategy-based games, analyzing these three models,\ntheir key innovations, training processes, challenges encountered, and\nimprovements made. Additionally, we discuss advancements in the field of\ngaming, including MiniZero and multi-agent models, highlighting future\ndirections and emerging AI models from Google DeepMind.",
      "pdf_url": "http://arxiv.org/pdf/2502.10303v1",
      "published": "2025-02-14T17:06:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10303v1",
      "categories": [
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "A Hybrid Cross-Stage Coordination Pre-ranking Model for Online Recommendation Systems",
      "authors": [
        "Binglei Zhao",
        "Houying Qi",
        "Guang Xu",
        "Mian Ma",
        "Xiwei Zhao",
        "Feng Mei",
        "Sulong Xu",
        "Jinghe Hu"
      ],
      "abstract": "Large-scale recommendation systems often adopt cascading architecture\nconsisting of retrieval, pre-ranking, ranking, and re-ranking stages. With\nstrict latency requirements, pre-ranking utilizes lightweight models to perform\na preliminary selection from massive retrieved candidates. However, recent\nworks focus solely on improving consistency with ranking, relying exclusively\non downstream stages. Since downstream input is derived from the pre-ranking\noutput, they will exacerbate the sample selection bias (SSB) issue and Matthew\neffect, leading to sub-optimal results. To address the limitation, we propose a\nnovel Hybrid Cross-Stage Coordination Pre-ranking model (HCCP) to integrate\ninformation from upstream (retrieval) and downstream (ranking, re-ranking)\nstages. Specifically, cross-stage coordination refers to the pre-ranking's\nadaptability to the entire stream and the role of serving as a more effective\nbridge between upstream and downstream. HCCP consists of Hybrid Sample\nConstruction and Hybrid Objective Optimization. Hybrid sample construction\ncaptures multi-level unexposed data from the entire stream and rearranges them\nto become the optimal guiding \"ground truth\" for pre-ranking learning. Hybrid\nobjective optimization contains the joint optimization of consistency and\nlong-tail precision through our proposed Margin InfoNCE loss. It is\nspecifically designed to learn from such hybrid unexposed samples, improving\nthe overall performance and mitigating the SSB issue. The appendix describes a\nproof of the efficacy of the proposed loss in selecting potential positives.\nExtensive offline and online experiments indicate that HCCP outperforms SOTA\nmethods by improving cross-stage coordination. It contributes up to 14.9% UCVR\nand 1.3% UCTR in the JD E-commerce recommendation system. Concerning code\nprivacy, we provide a pseudocode for reference.",
      "pdf_url": "http://arxiv.org/pdf/2502.10284v1",
      "published": "2025-02-14T16:42:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10284v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Probing Perceptual Constancy in Large Vision Language Models",
      "authors": [
        "Haoran Sun",
        "Suyang Yu",
        "Yijiang Li",
        "Qingying Gao",
        "Haiyun Lyu",
        "Hokin Deng",
        "Dezhi Luo"
      ],
      "abstract": "Perceptual constancy is the ability to maintain stable perceptions of objects\ndespite changes in sensory input, such as variations in distance, angle, or\nlighting. This ability is crucial for recognizing visual information in a\ndynamic world, making it essential for Vision-Language Models (VLMs). However,\nwhether VLMs are currently and theoretically capable of mastering this ability\nremains underexplored. In this study, we evaluated 33 VLMs using 253\nexperiments across three domains: color, size, and shape constancy. The\nexperiments included single-image and video adaptations of classic cognitive\ntasks, along with novel tasks in in-the-wild conditions, to evaluate the\nmodels' recognition of object properties under varying conditions. We found\nsignificant variability in VLM performance, with models performance in shape\nconstancy clearly dissociated from that of color and size constancy.",
      "pdf_url": "http://arxiv.org/pdf/2502.10273v1",
      "published": "2025-02-14T16:31:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10273v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Are Large Language Models the future crowd workers of Linguistics?",
      "authors": [
        "Iris Ferrazzo"
      ],
      "abstract": "Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities.",
      "pdf_url": "http://arxiv.org/pdf/2502.10266v1",
      "published": "2025-02-14T16:23:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10266v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers",
      "authors": [
        "Aivin V. Solatorio",
        "Rafael Macalaba",
        "James Liounis"
      ],
      "abstract": "Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2502.10263v1",
      "published": "2025-02-14T16:16:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10263v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.DB",
        "cs.LG"
      ]
    },
    {
      "title": "Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices",
      "authors": [
        "Mohamed Aboelenien Ahmed",
        "Kilian Pfeiffer",
        "Ramin Khalili",
        "Heba Khdr",
        "Jörg Henkel"
      ],
      "abstract": "Federated fine-tuning offers a promising approach for tuning Large Language\nModels (LLMs) on edge devices while preserving data privacy. However,\nfine-tuning these models on edge devices remains challenging due to high\nmemory, communication, and computational demands. Zero-order optimization with\ntask alignment provides a potential solution, enabling fine-tuning with\ninference-level memory requirements but requires a longer convergence time. In\nthis paper, we propose Federated Split-Perturbation Zero-order Optimization\n(FedSPZO) that divides the network into two blocks, applying a different number\nof perturbations per block in a computationally effective way, achieving faster\nconvergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation\noverhead compared to zero-order state of the art techniques in federated\nlearning.",
      "pdf_url": "http://arxiv.org/pdf/2502.10239v1",
      "published": "2025-02-14T15:49:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10239v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise Control",
      "authors": [
        "Thomas Jiralerspong",
        "Berton Earnshaw",
        "Jason Hartford",
        "Yoshua Bengio",
        "Luca Scimeca"
      ],
      "abstract": "Diffusion Probabilistic Models (DPMs) are powerful generative models that\nhave achieved unparalleled success in a number of generative tasks. In this\nwork, we aim to build inductive biases into the training and sampling of\ndiffusion models to better accommodate the target distribution of the data to\nmodel. For topologically structured data, we devise a frequency-based noising\noperator to purposefully manipulate, and set, these inductive biases. We first\nshow that appropriate manipulations of the noising forward process can lead\nDPMs to focus on particular aspects of the distribution to learn. We show that\ndifferent datasets necessitate different inductive biases, and that appropriate\nfrequency-based noise control induces increased generative performance compared\nto standard diffusion. Finally, we demonstrate the possibility of ignoring\ninformation at particular frequencies while learning. We show this in an image\ncorruption and recovery task, where we train a DPM to recover the original\ntarget distribution after severe noise corruption.",
      "pdf_url": "http://arxiv.org/pdf/2502.10236v1",
      "published": "2025-02-14T15:46:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10236v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Multiagent Path Search Algorithm for Large-Scale Coalition Structure Generation",
      "authors": [
        "Redha Taguelmimt",
        "Samir Aknine",
        "Djamila Boukredera",
        "Narayan Changder",
        "Tuomas Sandholm"
      ],
      "abstract": "Coalition structure generation (CSG), i.e. the problem of optimally\npartitioning a set of agents into coalitions to maximize social welfare, is a\nfundamental computational problem in multiagent systems. This problem is\nimportant for many applications where small run times are necessary, including\ntransportation and disaster response. In this paper, we develop SALDAE, a\nmultiagent path finding algorithm for CSG that operates on a graph of coalition\nstructures. Our algorithm utilizes a variety of heuristics and strategies to\nperform the search and guide it. It is an anytime algorithm that can handle\nlarge problems with hundreds and thousands of agents. We show empirically on\nnine standard value distributions, including disaster response and electric\nvehicle allocation benchmarks, that our algorithm enables a rapid finding of\nhigh-quality solutions and compares favorably with other state-of-the-art\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2502.10226v1",
      "published": "2025-02-14T15:21:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10226v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "93A16, 68T01",
        "I.2; F.2"
      ]
    },
    {
      "title": "Forget the Data and Fine-Tuning! Just Fold the Network to Compress",
      "authors": [
        "Dong Wang",
        "Haris Šikić",
        "Lothar Thiele",
        "Olga Saukh"
      ],
      "abstract": "We introduce model folding, a novel data-free model compression technique\nthat merges structurally similar neurons across layers, significantly reducing\nthe model size without the need for fine-tuning or access to training data.\nUnlike existing methods, model folding preserves data statistics during\ncompression by leveraging k-means clustering, and using novel data-free\ntechniques to prevent variance collapse or explosion. Our theoretical framework\nand experiments across standard benchmarks, including ResNet18 and LLaMA-7B,\ndemonstrate that model folding achieves comparable performance to data-driven\ncompression techniques and outperforms recently proposed data-free methods,\nespecially at high sparsity levels. This approach is particularly effective for\ncompressing large-scale models, making it suitable for deployment in\nresource-constrained environments.",
      "pdf_url": "http://arxiv.org/pdf/2502.10216v1",
      "published": "2025-02-14T15:10:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10216v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Do Large Language Models Reason Causally Like Us? Even Better?",
      "authors": [
        "Hanna M. Dettki",
        "Brenden M. Lake",
        "Charley M. Wu",
        "Bob Rehder"
      ],
      "abstract": "Causal reasoning is a core component of intelligence. Large language models\n(LLMs) have shown impressive capabilities in generating human-like text,\nraising questions about whether their responses reflect true understanding or\nstatistical patterns. We compared causal reasoning in humans and four LLMs\nusing tasks based on collider graphs, rating the likelihood of a query variable\noccurring given evidence from other variables. We find that LLMs reason\ncausally along a spectrum from human-like to normative inference, with\nalignment shifting based on model, context, and task. Overall, GPT-4o and\nClaude showed the most normative behavior, including \"explaining away\", whereas\nGemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected\nindependence of causes - Claude the least - they exhibited strong associative\nreasoning and predictive inference when assessing the likelihood of the effect\ngiven its causes. These findings underscore the need to assess AI biases as\nthey increasingly assist human decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2502.10215v1",
      "published": "2025-02-14T15:09:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10215v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Prediction hubs are context-informed frequent tokens in LLMs",
      "authors": [
        "Beatrix M. G. Nielsen",
        "Iuri Macocco",
        "Marco Baroni"
      ],
      "abstract": "Hubness, the tendency for few points to be among the nearest neighbours of a\ndisproportionate number of other points, commonly arises when applying standard\ndistance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first show, theoretically, that the only representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appeareance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction. On\nthe other hand, when other distance computations involving LLM representations\nare performed, we do not have the same theoretical guarantees, and, indeed, we\nsee nuisance hubs appear. In summary, our work highlights, on the one hand, how\nhubness, while omnipresent in high-dimensional spaces, is not always a negative\nproperty that needs to be mitigated, and, on the other hand, it shows that\nvarious widely-used LLMs have developed a guessing strategy that consists in\nconstantly assigning a high probability to frequent tokens.",
      "pdf_url": "http://arxiv.org/pdf/2502.10201v1",
      "published": "2025-02-14T14:52:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10201v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Dynamic Reinforcement Learning for Actors",
      "authors": [
        "Katsunari Shibata"
      ],
      "abstract": "Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly\ncontrols system dynamics, instead of the actor (action-generating neural\nnetwork) outputs at each moment, bringing about a major qualitative shift in\nreinforcement learning (RL) from static to dynamic. The actor is initially\ndesigned to generate chaotic dynamics through the loop with its environment,\nenabling the agent to perform flexible and deterministic exploration. Dynamic\nRL controls global system dynamics using a local index called \"sensitivity,\"\nwhich indicates how much the input neighborhood contracts or expands into the\ncorresponding output neighborhood through each neuron's processing. While\nsensitivity adjustment learning (SAL) prevents excessive convergence of the\ndynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them --\nto converge more to improve reproducibility around better state transitions\nwith positive TD error and to diverge more to enhance exploration around worse\ntransitions with negative TD error. Dynamic RL was applied only to the actor in\nan Actor-Critic RL architecture while applying it to the critic remains a\nchallenge. It was tested on two dynamic tasks and functioned effectively\nwithout external exploration noise or backward computation through time.\nMoreover, it exhibited excellent adaptability to new environments, although\nsome problems remain. Drawing parallels between 'exploration' and 'thinking,'\nthe author hypothesizes that \"exploration grows into thinking through learning\"\nand believes this RL could be a key technique for the emergence of thinking,\nincluding inspiration that cannot be reconstructed from massive existing text\ndata. Finally, despite being presumptuous, the author presents the argument\nthat this research should not proceed due to its potentially fatal risks,\naiming to encourage discussion.",
      "pdf_url": "http://arxiv.org/pdf/2502.10200v1",
      "published": "2025-02-14T14:50:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10200v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "MathConstruct: Challenging LLM Reasoning with Constructive Proofs",
      "authors": [
        "Mislav Balunović",
        "Jasper Dekoninck",
        "Nikola Jovanović",
        "Ivo Petrov",
        "Martin Vechev"
      ],
      "abstract": "While Large Language Models (LLMs) demonstrate impressive performance in\nmathematics, existing math benchmarks come with significant limitations. Many\nfocus on problems with fixed ground-truth answers, and are often saturated due\nto problem simplicity or the viability of guessing or memorization. Crucially,\nthey capture only a narrow subset of relevant math problems. To address this\nresearch gap, we introduce \\mc, a new benchmark of 126 challenging problems\nsourced from various math competitions, which targets constructive proofs, a\nwidely encountered problem type requiring the construction of mathematical\nobjects with specific properties. These proofs are particularly suitable for\nLLM evaluation, as solution correctness can be easily verified. Our automated\nverifiers also enable MathConstruct to generate problem variations, used to\nevaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct\nproblems, highlighting its complexity and importance for LLM evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2502.10197v1",
      "published": "2025-02-14T14:44:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10197v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Exploring the Camera Bias of Person Re-identification",
      "authors": [
        "Myungseo Song",
        "Jin-Woo Park",
        "Jong-Seok Lee"
      ],
      "abstract": "We empirically investigate the camera bias of person re-identification (ReID)\nmodels. Previously, camera-aware methods have been proposed to address this\nissue, but they are largely confined to training domains of the models. We\nmeasure the camera bias of ReID models on unseen domains and reveal that camera\nbias becomes more pronounced under data distribution shifts. As a debiasing\nmethod for unseen domain data, we revisit feature normalization on embedding\nvectors. While the normalization has been used as a straightforward solution,\nits underlying causes and broader applicability remain unexplored. We analyze\nwhy this simple method is effective at reducing bias and show that it can be\napplied to detailed bias factors such as low-level image properties and body\nangle. Furthermore, we validate its generalizability across various models and\nbenchmarks, highlighting its potential as a simple yet effective test-time\npostprocessing method for ReID. In addition, we explore the inherent risk of\ncamera bias in unsupervised learning of ReID models. The unsupervised models\nremain highly biased towards camera labels even for seen domain data,\nindicating substantial room for improvement. Based on observations of the\nnegative impact of camera-biased pseudo labels on training, we suggest simple\ntraining strategies to mitigate the bias. By applying these strategies to\nexisting unsupervised learning algorithms, we show that significant performance\nimprovements can be achieved with minor modifications.",
      "pdf_url": "http://arxiv.org/pdf/2502.10195v1",
      "published": "2025-02-14T14:39:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10195v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Merging public elementary schools to reduce racial/ethnic segregation",
      "authors": [
        "Madison Landry",
        "Nabeel Gillani"
      ],
      "abstract": "Diverse schools can help address implicit biases and increase empathy, mutual\nrespect, and reflective thought by fostering connections between students from\ndifferent racial/ethnic, socioeconomic, and other backgrounds. Unfortunately,\ndemographic segregation remains rampant in US public schools, despite over 70\nyears since the passing of federal legislation formally outlawing segregation\nby race. However, changing how students are assigned to schools can help foster\nmore integrated learning environments. In this paper, we explore \"school\nmergers\" as one such under-explored, yet promising, student assignment policy\nchange. School mergers involve merging the school attendance boundaries, or\ncatchment areas, of schools and subsequently changing the grades each school\noffers. We develop an algorithm to simulate elementary school mergers across\n200 large school districts serving 4.5 million elementary school students and\nfind that pairing or tripling schools in this way could reduce racial/ethnic\nsegregation by a median relative 20% -- and as much as nearly 60% in some\ndistricts -- while increasing driving times to schools by an average of a few\nminutes each way. Districts with many interfaces between\nracially/ethnically-disparate neighborhoods tend to be prime candidates for\nmergers. We also compare the expected results of school mergers to other\ntypical integration policies, like redistricting, and find that different\npolicies may be more or less suitable in different places. Finally, we make our\nresults available through a public dashboard for policymakers and community\nmembers to explore further (https://mergers.schooldiversity.org). Together, our\nstudy offers new findings and tools to support integration policy-making across\nUS public school districts.",
      "pdf_url": "http://arxiv.org/pdf/2502.10193v1",
      "published": "2025-02-14T14:36:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10193v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "From Markov to Laplace: How Mamba In-Context Learns Markov Chains",
      "authors": [
        "Marco Bondaschi",
        "Nived Rajaraman",
        "Xiuying Wei",
        "Kannan Ramchandran",
        "Razvan Pascanu",
        "Caglar Gulcehre",
        "Michael Gastpar",
        "Ashok Vardhan Makkuva"
      ],
      "abstract": "While transformer-based language models have driven the AI revolution thus\nfar, their computational complexity has spurred growing interest in viable\nalternatives, such as structured state space sequence models (SSMs) and\nSelective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown\nremarkable inference speed ups over transformers while achieving comparable or\nsuperior performance on complex language modeling tasks. However, despite these\narchitectural innovations and empirical successes, the fundamental learning\ncapabilities of Mamba remain poorly understood. In this paper, we address this\ngap by studying in-context learning (ICL) on Markov chains and uncovering a\nsurprising phenomenon: unlike transformers, even a single-layer Mamba\nefficiently learns the in-context Laplacian smoothing estimator, which is both\nBayes and minimax optimal, for all Markovian orders. To explain this, we\ntheoretically characterize the representation capacity of Mamba and reveal the\nfundamental role of convolution in enabling it to represent the optimal\nLaplacian smoothing. These theoretical insights align strongly with empirical\nresults and, to the best of our knowledge, represent the first formal\nconnection between Mamba and optimal statistical estimators. Finally, we\noutline promising research directions inspired by these findings.",
      "pdf_url": "http://arxiv.org/pdf/2502.10178v1",
      "published": "2025-02-14T14:13:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10178v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning",
      "authors": [
        "Mingcong Lei",
        "Yiming Zhao",
        "Ge Wang",
        "Zhixin Mai",
        "Shuguang Cui",
        "Yatong Han",
        "Jinke Ren"
      ],
      "abstract": "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
      "pdf_url": "http://arxiv.org/pdf/2502.10177v1",
      "published": "2025-02-14T14:12:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10177v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Technical Risks of (Lethal) Autonomous Weapons Systems",
      "authors": [
        "Heramb Podar",
        "Alycia Colijn"
      ],
      "abstract": "The autonomy and adaptability of (Lethal) Autonomous Weapons Systems, (L)AWS\nin short, promise unprecedented operational capabilities, but they also\nintroduce profound risks that challenge the principles of control,\naccountability, and stability in international security. This report outlines\nthe key technological risks associated with (L)AWS deployment, emphasizing\ntheir unpredictability, lack of transparency, and operational unreliability,\nwhich can lead to severe unintended consequences.\n  Key Takeaways:\n  1. Proposed advantages of (L)AWS can only be achieved through objectification\nand classification, but a range of systematic risks limit the reliability and\npredictability of classifying algorithms.\n  2. These systematic risks include the black-box nature of AI decision-making,\nsusceptibility to reward hacking, goal misgeneralization and potential for\nemergent behaviors that escape human control.\n  3. (L)AWS could act in ways that are not just unexpected but also\nuncontrollable, undermining mission objectives and potentially escalating\nconflicts.\n  4. Even rigorously tested systems may behave unpredictably and harmfully in\nreal-world conditions, jeopardizing both strategic stability and humanitarian\nprinciples.",
      "pdf_url": "http://arxiv.org/pdf/2502.10174v1",
      "published": "2025-02-14T14:09:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10174v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Revisiting Generalization Power of a DNN in Terms of Symbolic Interactions",
      "authors": [
        "Lei Cheng",
        "Junpeng Zhang",
        "Qihan Ren",
        "Quanshi Zhang"
      ],
      "abstract": "This paper aims to analyze the generalization power of deep neural networks\n(DNNs) from the perspective of interactions. Unlike previous analysis of a\nDNN's generalization power in a highdimensional feature space, we find that the\ngeneralization power of a DNN can be explained as the generalization power of\nthe interactions. We found that the generalizable interactions follow a\ndecay-shaped distribution, while non-generalizable interactions follow a\nspindle-shaped distribution. Furthermore, our theory can effectively\ndisentangle these two types of interactions from a DNN. We have verified that\nour theory can well match real interactions in a DNN in experiments.",
      "pdf_url": "http://arxiv.org/pdf/2502.10162v1",
      "published": "2025-02-14T13:46:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10162v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "SessionRec: Next Session Prediction Paradigm For Generative Sequential Recommendation",
      "authors": [
        "Lei Huang",
        "Hao Guo",
        "Linzhi Peng",
        "Long Zhang",
        "Xiaoteng Wang",
        "Daoyuan Wang",
        "Shichao Wang",
        "Jinpeng Wang",
        "Lei Wang",
        "Sheng Chen"
      ],
      "abstract": "We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for\ngenerative sequential recommendation, addressing the fundamental misalignment\nbetween conventional next-item prediction paradigm (NIPP) and real-world\nrecommendation scenarios. Unlike NIPP's item-level autoregressive generation\nthat contradicts actual session-based user interactions, our framework\nintroduces a session-aware representation learning through hierarchical\nsequence aggregation (intra/inter-session), reducing attention computation\ncomplexity while enabling implicit modeling of massive negative interactions,\nand a session-based prediction objective that better captures users' diverse\ninterests through multi-item recommendation in next sessions. Moreover, we\nfound that incorporating a rank loss for items within the session under the\nnext session prediction paradigm can significantly improve the ranking\neffectiveness of generative sequence recommendation models. We also verified\nthat SessionRec exhibits clear power-law scaling laws similar to those observed\nin LLMs. Extensive experiments conducted on public datasets and online A/B test\nin Meituan App demonstrate the effectiveness of SessionRec. The proposed\nparadigm establishes new foundations for developing industrial-scale generative\nrecommendation systems through its model-agnostic architecture and\ncomputational efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2502.10157v1",
      "published": "2025-02-14T13:36:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10157v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries",
      "authors": [
        "Serkan Sulun",
        "Paula Viana",
        "Matthew E. P. Davies"
      ],
      "abstract": "We introduce EMSYNC, a video-based symbolic music generation model that\naligns music with a video's emotional content and temporal boundaries. It\nfollows a two-stage framework, where a pretrained video emotion classifier\nextracts emotional features, and a conditional music generator produces MIDI\nsequences guided by both emotional and temporal cues. We introduce boundary\noffsets, a novel temporal conditioning mechanism that enables the model to\nanticipate and align musical chords with scene cuts. Unlike existing models,\nour approach retains event-based encoding, ensuring fine-grained timing control\nand expressive musical nuances. We also propose a mapping scheme to bridge the\nvideo emotion classifier, which produces discrete emotion categories, with the\nemotion-conditioned MIDI generator, which operates on continuous-valued\nvalence-arousal inputs. In subjective listening tests, EMSYNC outperforms\nstate-of-the-art models across all subjective metrics, for music theory-aware\nparticipants as well as the general listeners.",
      "pdf_url": "http://arxiv.org/pdf/2502.10154v1",
      "published": "2025-02-14T13:32:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10154v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS",
        "eess.IV"
      ]
    },
    {
      "title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis",
      "authors": [
        "Zhiyuan Li",
        "Wenshuai Zhao",
        "Joni Pajarinen"
      ],
      "abstract": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS achieves up to 30\\% higher win rates\nthan state-of-the-art MARL algorithms in symmetric scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2502.10148v1",
      "published": "2025-02-14T13:23:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10148v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Learning Relational Tabular Data without Shared Features",
      "authors": [
        "Zhaomin Wu",
        "Shida Wang",
        "Ziyang Wang",
        "Bingsheng He"
      ],
      "abstract": "Learning relational tabular data has gained significant attention recently,\nbut most studies focus on single tables, overlooking the potential of\ncross-table learning. Cross-table learning, especially in scenarios where\ntables lack shared features and pre-aligned data, offers vast opportunities but\nalso introduces substantial challenges. The alignment space is immense, and\ndetermining accurate alignments between tables is highly complex. We propose\nLatent Entity Alignment Learning (Leal), a novel framework enabling effective\ncross-table training without requiring shared features or pre-aligned data.\nLeal operates on the principle that properly aligned data yield lower loss than\nmisaligned data, a concept embodied in its soft alignment mechanism. This\nmechanism is coupled with a differentiable cluster sampler module, ensuring\nefficient scaling to large relational tables. Furthermore, we provide a\ntheoretical proof of the cluster sampler's approximation capacity. Extensive\nexperiments on five real-world and five synthetic datasets show that Leal\nachieves up to a 26.8% improvement in predictive performance compared to\nstate-of-the-art methods, demonstrating its effectiveness and scalability.",
      "pdf_url": "http://arxiv.org/pdf/2502.10125v1",
      "published": "2025-02-14T12:51:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10125v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Image Embedding Sampling Method for Diverse Captioning",
      "authors": [
        "Sania Waheed",
        "Na Min An"
      ],
      "abstract": "Image Captioning for state-of-the-art VLMs has significantly improved over\ntime; however, this comes at the cost of increased computational complexity,\nmaking them less accessible for resource-constrained applications such as\nmobile devices and assistive technologies. Alternatively, smaller VLMs\nprioritize high-level scene descriptions, overlooking finer details that\ncontribute to a richer understanding of an image. In this paper, we introduce a\ntraining-free framework that enhances caption diversity and informativeness by\nexplicitly attending to distinct image regions using a comparably small VLM,\nBLIP, as the backbone. Our approach leverages structured segmentation to\nproduce hierarchical representations that capture both global and localized\nsemantics. Without requiring additional model training, we demonstrate that our\nmethod allows smaller VLMs to achieve performance comparable to larger models\nin terms of image-caption alignment, semantic integrity, and diversity. We\nevaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,\nachieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset\nrespectively, while maintaining strong image-caption relevancy and semantic\nintegrity with the human-annotated captions.",
      "pdf_url": "http://arxiv.org/pdf/2502.10118v1",
      "published": "2025-02-14T12:33:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10118v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Causal Information Prioritization for Efficient Reinforcement Learning",
      "authors": [
        "Hongye Cao",
        "Fan Feng",
        "Tianpei Yang",
        "Jing Huo",
        "Yang Gao"
      ],
      "abstract": "Current Reinforcement Learning (RL) methods often suffer from\nsample-inefficiency, resulting from blind exploration strategies that neglect\ncausal relationships among states, actions, and rewards. Although recent causal\napproaches aim to address this problem, they lack grounded modeling of\nreward-guided causal understanding of states and actions for goal-orientation,\nthus impairing learning efficiency. To tackle this issue, we propose a novel\nmethod named Causal Information Prioritization (CIP) that improves sample\nefficiency by leveraging factored MDPs to infer causal relationships between\ndifferent dimensions of states and actions with respect to rewards, enabling\nthe prioritization of causal information. Specifically, CIP identifies and\nleverages causal relationships between states and rewards to execute\ncounterfactual data augmentation to prioritize high-impact state features under\nthe causal understanding of the environments. Moreover, CIP integrates a\ncausality-aware empowerment learning objective, which significantly enhances\nthe agent's execution of reward-guided actions for more efficient exploration\nin complex environments. To fully assess the effectiveness of CIP, we conduct\nextensive experiments across 39 tasks in 5 diverse continuous control\nenvironments, encompassing both locomotion and manipulation skills learning\nwith pixel-based and sparse reward settings. Experimental results demonstrate\nthat CIP consistently outperforms existing RL methods across a wide range of\nscenarios.",
      "pdf_url": "http://arxiv.org/pdf/2502.10097v1",
      "published": "2025-02-14T11:44:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10097v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A novel approach to data generation in generative model",
      "authors": [
        "JaeHong Kim",
        "Jaewon Shim"
      ],
      "abstract": "Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling.",
      "pdf_url": "http://arxiv.org/pdf/2502.10092v1",
      "published": "2025-02-14T11:27:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10092v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "00A30 (Primary), 68T99 (Secondary)",
        "I.2.3; F.4.1"
      ]
    },
    {
      "title": "Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models",
      "authors": [
        "Chenrui Tie",
        "Shengxiang Sun",
        "Jinxuan Zhu",
        "Yiwei Liu",
        "Jingxiang Guo",
        "Yue Hu",
        "Haonan Chen",
        "Junting Chen",
        "Ruihai Wu",
        "Lin Shao"
      ],
      "abstract": "Humans possess an extraordinary ability to understand and execute complex\nmanipulation tasks by interpreting abstract instruction manuals. For robots,\nhowever, this capability remains a substantial challenge, as they cannot\ninterpret abstract instructions and translate them into executable actions. In\nthis paper, we present Manual2Skill, a novel framework that enables robots to\nperform complex assembly tasks guided by high-level manual instructions. Our\napproach leverages a Vision-Language Model (VLM) to extract structured\ninformation from instructional images and then uses this information to\nconstruct hierarchical assembly graphs. These graphs represent parts,\nsubassemblies, and the relationships between them. To facilitate task\nexecution, a pose estimation model predicts the relative 6D poses of components\nat each assembly step. At the same time, a motion planning module generates\nactionable sequences for real-world robotic implementation. We demonstrate the\neffectiveness of Manual2Skill by successfully assembling several real-world\nIKEA furniture items. This application highlights its ability to manage\nlong-horizon manipulation tasks with both efficiency and precision,\nsignificantly enhancing the practicality of robot learning from instruction\nmanuals. This work marks a step forward in advancing robotic systems capable of\nunderstanding and executing complex manipulation tasks in a manner akin to\nhuman capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2502.10090v1",
      "published": "2025-02-14T11:25:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10090v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS ACAM for Energy-Efficient Inference",
      "authors": [
        "Kieran Woodward",
        "Eiman Kanjo",
        "Georgios Papandroulidakis",
        "Shady Agwa",
        "Themis Prodromakis"
      ],
      "abstract": "In recent years, the development of smart edge computing systems to process\ninformation locally is on the rise. Many near-sensor machine learning (ML)\napproaches have been implemented to introduce accurate and energy efficient\ntemplate matching operations in resource-constrained edge sensing systems, such\nas wearables. To introduce novel solutions that can be viable for extreme edge\ncases, hybrid solutions combining conventional and emerging technologies have\nstarted to be proposed. Deep Neural Networks (DNN) optimised for edge\napplication alongside new approaches of computing (both device and architecture\n-wise) could be a strong candidate in implementing edge ML solutions that aim\nat competitive accuracy classification while using a fraction of the power of\nconventional ML solutions. In this work, we are proposing a hybrid\nsoftware-hardware edge classifier aimed at the extreme edge near-sensor\nsystems. The classifier consists of two parts: (i) an optimised digital tinyML\nnetwork, working as a front-end feature extractor, and (ii) a back-end\nRRAM-CMOS analogue content addressable memory (ACAM), working as a final stage\ntemplate matching system. The combined hybrid system exhibits a competitive\ntrade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$\nand $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with\n78.06$\\mu$J for the original teacher model, representing a 792-fold reduction,\nmaking it a viable solution for extreme edge applications.",
      "pdf_url": "http://arxiv.org/pdf/2502.10089v1",
      "published": "2025-02-14T11:21:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10089v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ]
    },
    {
      "title": "Towards Empowerment Gain through Causal Structure Learning in Model-Based RL",
      "authors": [
        "Hongye Cao",
        "Fan Feng",
        "Meng Fang",
        "Shaokang Dong",
        "Tianpei Yang",
        "Jing Huo",
        "Yang Gao"
      ],
      "abstract": "In Model-Based Reinforcement Learning (MBRL), incorporating causal structures\ninto dynamics models provides agents with a structured understanding of the\nenvironments, enabling efficient decision. Empowerment as an intrinsic\nmotivation enhances the ability of agents to actively control their\nenvironments by maximizing the mutual information between future states and\nactions. We posit that empowerment coupled with causal understanding can\nimprove controllability, while enhanced empowerment gain can further facilitate\ncausal reasoning in MBRL. To improve learning efficiency and controllability,\nwe propose a novel framework, Empowerment through Causal Learning (ECL), where\nan agent with the awareness of causal dynamics models achieves\nempowerment-driven exploration and optimizes its causal structure for task\nlearning. Specifically, ECL operates by first training a causal dynamics model\nof the environment based on collected data. We then maximize empowerment under\nthe causal structure for exploration, simultaneously using data gathered\nthrough exploration to update causal dynamics model to be more controllable\nthan dense dynamics model without causal structure. In downstream task\nlearning, an intrinsic curiosity reward is included to balance the causality,\nmitigating overfitting. Importantly, ECL is method-agnostic and is capable of\nintegrating various causal discovery methods. We evaluate ECL combined with 3\ncausal discovery methods across 6 environments including pixel-based tasks,\ndemonstrating its superior performance compared to other causal MBRL methods,\nin terms of causal discovery, sample efficiency, and asymptotic performance.",
      "pdf_url": "http://arxiv.org/pdf/2502.10077v1",
      "published": "2025-02-14T10:59:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10077v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Strassen Multisystolic Array Hardware Architectures",
      "authors": [
        "Trevor E. Pogue",
        "Nicola Nicolici"
      ],
      "abstract": "While Strassen's matrix multiplication algorithm reduces the complexity of\nnaive matrix multiplication, general-purpose hardware is not suitable for\nachieving the algorithm's promised theoretical speedups. This leaves the\nquestion of if it could be better exploited in custom hardware architectures\ndesigned specifically for executing the algorithm. However, there is limited\nprior work on this and it is not immediately clear how to derive such\narchitectures or if they can ultimately lead to real improvements. We bridge\nthis gap, presenting and evaluating new systolic array architectures that\nefficiently translate the theoretical complexity reductions of Strassen's\nalgorithm directly into hardware resource savings. Furthermore, the\narchitectures are multisystolic array designs that can multiply smaller\nmatrices with higher utilization than single-systolic array designs. The\nproposed designs implemented on FPGA reduce DSP requirements by a factor of\n$1.14^r$ for $r$ implemented Strassen recursion levels, and otherwise require\noverall similar soft logic resources when instantiated to support matrix sizes\ndown to 32x32 and 24x24 at 1-2 levels of Strassen recursion, respectively. We\nevaluate the proposed designs both in isolation and in an end-to-end machine\nlearning accelerator compared to baseline designs and prior works, achieving\nstate-of-the-art performance.",
      "pdf_url": "http://arxiv.org/pdf/2502.10063v1",
      "published": "2025-02-14T10:40:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10063v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.PF"
      ]
    },
    {
      "title": "Adaptive Bi-Level Multi-Robot Task Allocation and Learning under Uncertainty with Temporal Logic Constraints",
      "authors": [
        "Xiaoshan Lin",
        "Roberto Tron"
      ],
      "abstract": "This work addresses the problem of multi-robot coordination under unknown\nrobot transition models, ensuring that tasks specified by Time Window Temporal\nLogic are satisfied with user-defined probability thresholds. We present a\nbi-level framework that integrates (i) high-level task allocation, where tasks\nare assigned based on the robots' estimated task completion probabilities and\nexpected rewards, and (ii) low-level distributed policy learning and execution,\nwhere robots independently optimize auxiliary rewards while fulfilling their\nassigned tasks. To handle uncertainty in robot dynamics, our approach leverages\nreal-time task execution data to iteratively refine expected task completion\nprobabilities and rewards, enabling adaptive task allocation without explicit\nrobot transition models. We theoretically validate the proposed algorithm,\ndemonstrating that the task assignments meet the desired probability thresholds\nwith high confidence. Finally, we demonstrate the effectiveness of our\nframework through comprehensive simulations.",
      "pdf_url": "http://arxiv.org/pdf/2502.10062v1",
      "published": "2025-02-14T10:39:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10062v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.FL"
      ]
    },
    {
      "title": "A Survey on LLM-powered Agents for Recommender Systems",
      "authors": [
        "Qiyao Peng",
        "Hongtao Liu",
        "Hua Huang",
        "Qing Yang",
        "Minglai Shao"
      ],
      "abstract": "Recommender systems are essential components of many online platforms, yet\ntraditional approaches still struggle with understanding complex user\npreferences and providing explainable recommendations. The emergence of Large\nLanguage Model (LLM)-powered agents offers a promising approach by enabling\nnatural language interactions and interpretable reasoning, potentially\ntransforming research in recommender systems. This survey provides a systematic\nreview of the emerging applications of LLM-powered agents in recommender\nsystems. We identify and analyze three key paradigms in current research: (1)\nRecommender-oriented approaches, which leverage intelligent agents to enhance\nthe fundamental recommendation mechanisms; (2) Interaction-oriented approaches,\nwhich facilitate dynamic user engagement through natural dialogue and\ninterpretable suggestions; and (3) Simulation-oriented approaches, which employ\nmulti-agent frameworks to model complex user-item interactions and system\ndynamics. Beyond paradigm categorization, we analyze the architectural\nfoundations of LLM-powered recommendation agents, examining their essential\ncomponents: profile construction, memory management, strategic planning, and\naction execution. Our investigation extends to a comprehensive analysis of\nbenchmark datasets and evaluation frameworks in this domain. This systematic\nexamination not only illuminates the current state of LLM-powered agent\nrecommender systems but also charts critical challenges and promising research\ndirections in this transformative field.",
      "pdf_url": "http://arxiv.org/pdf/2502.10050v1",
      "published": "2025-02-14T09:57:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10050v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Janus: Collaborative Vision Transformer Under Dynamic Network Environment",
      "authors": [
        "Linyi Jiang",
        "Silvery D. Fu",
        "Yifei Zhu",
        "Bo Li"
      ],
      "abstract": "Vision Transformers (ViTs) have outperformed traditional Convolutional Neural\nNetwork architectures and achieved state-of-the-art results in various computer\nvision tasks. Since ViTs are computationally expensive, the models either have\nto be pruned to run on resource-limited edge devices only or have to be\nexecuted on remote cloud servers after receiving the raw data transmitted over\nfluctuating networks. The resulting degraded performance or high latency all\nhinder their widespread applications. In this paper, we present Janus, the\nfirst framework for low-latency cloud-device collaborative Vision Transformer\ninference over dynamic networks. Janus overcomes the intrinsic model\nlimitations of ViTs and realizes collaboratively executing ViT models on both\ncloud and edge devices, achieving low latency, high accuracy, and low\ncommunication overhead. Specifically, Janus judiciously combines token pruning\ntechniques with a carefully designed fine-to-coarse model splitting policy and\nnon-static mixed pruning policy. It attains a balance between accuracy and\nlatency by dynamically selecting the optimal pruning level and split point.\nExperimental results across various tasks demonstrate that Janus enhances\nthroughput by up to 5.15 times and reduces latency violation ratios by up to\n98.7% when compared with baseline approaches under various network\nenvironments.",
      "pdf_url": "http://arxiv.org/pdf/2502.10047v1",
      "published": "2025-02-14T09:49:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10047v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "Unsupervised Entity Alignment Based on Personalized Discriminative Rooted Tree",
      "authors": [
        "Yaming Yang",
        "Zhe Wang",
        "Ziyu Guan",
        "Wei Zhao",
        "Xinyan Huang",
        "Xiaofei He"
      ],
      "abstract": "Entity Alignment (EA) is to link potential equivalent entities across\ndifferent knowledge graphs (KGs). Most existing EA methods are supervised as\nthey require the supervision of seed alignments, i.e., manually specified\naligned entity pairs. Very recently, several EA studies have made some attempts\nto get rid of seed alignments. Despite achieving preliminary progress, they\nstill suffer two limitations: (1) The entity embeddings produced by their\nGNN-like encoders lack personalization since some of the aggregation subpaths\nare shared between different entities. (2) They cannot fully alleviate the\ndistribution distortion issue between candidate KGs due to the absence of the\nsupervised signal. In this work, we propose a novel unsupervised entity\nalignment approach called UNEA to address the above two issues. First, we\nparametrically sample a tree neighborhood rooted at each entity, and\naccordingly develop a tree attention aggregation mechanism to extract a\npersonalized embedding for each entity. Second, we introduce an auxiliary task\nof maximizing the mutual information between the input and the output of the KG\nencoder, to regularize the model and prevent the distribution distortion.\nExtensive experiments show that our UNEA achieves a new state-of-the-art for\nthe unsupervised EA task, and can even outperform many existing supervised EA\nbaselines.",
      "pdf_url": "http://arxiv.org/pdf/2502.10044v1",
      "published": "2025-02-14T09:45:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10044v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI Representation Learning",
      "authors": [
        "Jiawei Cheng",
        "Jingyuan Wang",
        "Yichuan Zhang",
        "Jiahao Ji",
        "Yuanshao Zhu",
        "Zhibo Zhang",
        "Xiangyu Zhao"
      ],
      "abstract": "POI representation learning plays a crucial role in handling tasks related to\nuser mobility data. Recent studies have shown that enriching POI\nrepresentations with multimodal information can significantly enhance their\ntask performance. Previously, the textual information incorporated into POI\nrepresentations typically involved only POI categories or check-in content,\nleading to relatively weak textual features in existing methods. In contrast,\nlarge language models (LLMs) trained on extensive text data have been found to\npossess rich textual knowledge. However leveraging such knowledge to enhance\nPOI representation learning presents two key challenges: first, how to extract\nPOI-related knowledge from LLMs effectively, and second, how to integrate the\nextracted information to enhance POI representations. To address these\nchallenges, we propose POI-Enhancer, a portable framework that leverages LLMs\nto improve POI representations produced by classic POI learning models. We\nfirst design three specialized prompts to extract semantic information from\nLLMs efficiently. Then, the Dual Feature Alignment module enhances the quality\nof the extracted information, while the Semantic Feature Fusion module\npreserves its integrity. The Cross Attention Fusion module then fully\nadaptively integrates such high-quality information into POI representations\nand Multi-View Contrastive Learning further injects human-understandable\nsemantic information into these representations. Extensive experiments on three\nreal-world datasets demonstrate the effectiveness of our framework, showing\nsignificant improvements across all baseline representations.",
      "pdf_url": "http://arxiv.org/pdf/2502.10038v1",
      "published": "2025-02-14T09:34:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10038v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Dream to Drive: Model-Based Vehicle Control Using Analytic World Models",
      "authors": [
        "Asen Nachkov",
        "Danda Pani Paudel",
        "Jan-Nico Zaech",
        "Davide Scaramuzza",
        "Luc Van Gool"
      ],
      "abstract": "Differentiable simulators have recently shown great promise for training\nautonomous vehicle controllers. Being able to backpropagate through them, they\ncan be placed into an end-to-end training loop where their known dynamics turn\ninto useful priors for the policy to learn, removing the typical black box\nassumption of the environment. So far, these systems have only been used to\ntrain policies. However, this is not the end of the story in terms of what they\ncan offer. Here, for the first time, we use them to train world models.\nSpecifically, we present three new task setups that allow us to learn next\nstate predictors, optimal planners, and optimal inverse states. Unlike analytic\npolicy gradients (APG), which requires the gradient of the next simulator state\nwith respect to the current actions, our proposed setups rely on the gradient\nof the next state with respect to the current state. We call this approach\nAnalytic World Models (AWMs) and showcase its applications, including how to\nuse it for planning in the Waymax simulator. Apart from pushing the limits of\nwhat is possible with such simulators, we offer an improved training recipe\nthat increases performance on the large-scale Waymo Open Motion dataset by up\nto 12% compared to baselines at essentially no additional cost.",
      "pdf_url": "http://arxiv.org/pdf/2502.10012v1",
      "published": "2025-02-14T08:46:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.10012v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Decision Information Meets Large Language Models: The Future of Explainable Operations Research",
      "authors": [
        "Yansen Zhang",
        "Qingcan Kang",
        "Wing Yin Yu",
        "Hailei Gong",
        "Xiaojin Fu",
        "Xiongwei Han",
        "Tao Zhong",
        "Chen Ma"
      ],
      "abstract": "Operations Research (OR) is vital for decision-making in many industries.\nWhile recent OR methods have seen significant improvements in automation and\nefficiency through integrating Large Language Models (LLMs), they still\nstruggle to produce meaningful explanations. This lack of clarity raises\nconcerns about transparency and trustworthiness in OR applications. To address\nthese challenges, we propose a comprehensive framework, Explainable Operations\nResearch (EOR), emphasizing actionable and understandable explanations\naccompanying optimization. The core of EOR is the concept of Decision\nInformation, which emerges from what-if analysis and focuses on evaluating the\nimpact of complex constraints (or parameters) changes on decision-making.\nSpecifically, we utilize bipartite graphs to quantify the changes in the OR\nmodel and adopt LLMs to improve the explanation capabilities. Additionally, we\nintroduce the first industrial benchmark to rigorously evaluate the\neffectiveness of explanations and analyses in OR, establishing a new standard\nfor transparency and clarity in the field.",
      "pdf_url": "http://arxiv.org/pdf/2502.09994v1",
      "published": "2025-02-14T08:25:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.09994v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability",
      "authors": [
        "Xiaoya Lu",
        "Dongrui Liu",
        "Yi Yu",
        "Luxin Xu",
        "Jing Shao"
      ],
      "abstract": "Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.",
      "pdf_url": "http://arxiv.org/pdf/2502.09990v1",
      "published": "2025-02-14T08:22:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.09990v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
      "authors": [
        "Kuan Li",
        "Liwen Zhang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Shuai Wang",
        "Minhao Cheng"
      ],
      "abstract": "Effectively incorporating external knowledge into Large Language Models\n(LLMs) is crucial for enhancing their capabilities and addressing real-world\nneeds. Retrieval-Augmented Generation (RAG) offers an effective method for\nachieving this by retrieving the most relevant fragments into LLMs. However,\nthe advancements in context window size for LLMs offer an alternative approach,\nraising the question of whether RAG remains necessary for effectively handling\nexternal knowledge. Several existing studies provide inconclusive comparisons\nbetween RAG and long-context (LC) LLMs, largely due to limitations in the\nbenchmark designs. In this paper, we present LaRA, a novel benchmark\nspecifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses\n2,326 test cases across four practical QA task categories and three types of\nnaturally occurring long texts. Through systematic evaluation of seven\nopen-source and four proprietary LLMs, we find that the optimal choice between\nRAG and LC depends on a complex interplay of factors, including the model's\nparameter size, long-text capabilities, context length, task type, and the\ncharacteristics of the retrieved chunks. Our findings provide actionable\nguidelines for practitioners to effectively leverage both RAG and LC approaches\nin developing and deploying LLM applications. Our code and dataset is provided\nat:\n\\href{https://github.com/likuanppd/LaRA}{\\textbf{https://github.com/likuanppd/LaRA}}.",
      "pdf_url": "http://arxiv.org/pdf/2502.09977v1",
      "published": "2025-02-14T08:04:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.09977v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Has My System Prompt Been Used? Large Language Model Prompt Membership Inference",
      "authors": [
        "Roman Levin",
        "Valeriia Cherepanova",
        "Abhimanyu Hans",
        "Avi Schwarzschild",
        "Tom Goldstein"
      ],
      "abstract": "Prompt engineering has emerged as a powerful technique for optimizing large\nlanguage models (LLMs) for specific applications, enabling faster prototyping\nand improved performance, and giving rise to the interest of the community in\nprotecting proprietary system prompts. In this work, we explore a novel\nperspective on prompt privacy through the lens of membership inference. We\ndevelop Prompt Detective, a statistical method to reliably determine whether a\ngiven system prompt was used by a third-party language model. Our approach\nrelies on a statistical test comparing the distributions of two groups of model\noutputs corresponding to different system prompts. Through extensive\nexperiments with a variety of language models, we demonstrate the effectiveness\nof Prompt Detective for prompt membership inference. Our work reveals that even\nminor changes in system prompts manifest in distinct response distributions,\nenabling us to verify prompt usage with statistical significance.",
      "pdf_url": "http://arxiv.org/pdf/2502.09974v1",
      "published": "2025-02-14T08:00:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.09974v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ]
    }
  ]
}