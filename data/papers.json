{
  "last_updated": "2025-04-10T00:48:05.404877",
  "papers": [
    {
      "title": "GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning through Bayesian Optimization",
      "authors": [
        "Bojana RankoviÄ‡",
        "Philippe Schwaller"
      ],
      "abstract": "Large Language Models (LLMs) can encode complex relationships in their latent\nspaces, yet harnessing them for optimization under uncertainty remains\nchallenging. We address this gap with a novel architecture that reframes LLM\nfinetuning as Gaussian process (GP) marginal likelihood optimization via deep\nkernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs\nto preserve the benefits of both - LLMs to provide a rich and flexible input\nspace for Bayesian optimization and - GPs to model this space with predictive\nuncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction\noptimization, our method nearly doubles the discovery rate of high-performing\nreactions compared to static LLM embeddings (from 24% to 43% coverage of the\ntop 5% reactions in just 50 optimization iterations). We also observe a 14%\nimprovement over domain-specific representations without requiring specialized\nfeatures. Extensive empirical evaluation across 19 benchmarks - ranging from\ngeneral chemistry to reaction and molecular property optimization -\ndemonstrates our method's robustness, generality, and consistent improvements\nacross: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder),\n(3) pretraining domains (chemistry-related or general-purpose) and (4)\nhyperparameter settings (tuned once on a single dataset). Finally, we explain\nthese improvements: joint LLM-GP optimization through marginal likelihood\nimplicitly performs contrastive learning, aligning representations to produce\n(1) better-structured embedding spaces, (2) improved uncertainty calibration,\nand (3) more efficient sampling - without requiring any external loss. This\nwork provides both practical advances in sample-efficient optimization and\ninsights into what makes effective Bayesian optimization.",
      "pdf_url": "http://arxiv.org/pdf/2504.06265v1",
      "published": "2025-04-08T17:59:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06265v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability",
      "authors": [
        "Nayantara Mudur",
        "Hao Cui",
        "Subhashini Venugopalan",
        "Paul Raccuglia",
        "Michael P. Brenner",
        "Peter Norgaard"
      ],
      "abstract": "Building precise simulations of the real world and invoking numerical solvers\nto answer quantitative problems is an essential requirement in engineering and\nscience. We present FEABench, a benchmark to evaluate the ability of large\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\nmathematics and engineering problems using finite element analysis (FEA). We\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\nto solve these problems end-to-end by reasoning over natural language problem\ndescriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to\ncompute the answers. We additionally design a language model agent equipped\nwith the ability to interact with the software through its Application\nProgramming Interface (API), examine its outputs and use tools to improve its\nsolutions over multiple iterations. Our best performing strategy generates\nexecutable API calls 88% of the time. LLMs that can successfully interact with\nand operate FEA software to solve problems such as those in our benchmark would\npush the frontiers of automation in engineering. Acquiring this capability\nwould augment LLMs' reasoning skills with the precision of numerical solvers\nand advance the development of autonomous systems that can tackle complex\nproblems in the real world. The code is available at\nhttps://github.com/google/feabench",
      "pdf_url": "http://arxiv.org/pdf/2504.06260v1",
      "published": "2025-04-08T17:59:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06260v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.NA",
        "math.NA"
      ]
    },
    {
      "title": "Decentralized Federated Domain Generalization with Style Sharing: A Formal Modeling and Convergence Analysis",
      "authors": [
        "Shahryar Zehtabi",
        "Dong-Jun Han",
        "Seyyedali Hosseinalipour",
        "Christopher G. Brinton"
      ],
      "abstract": "Much of the federated learning (FL) literature focuses on settings where\nlocal dataset statistics remain the same between training and testing time.\nRecent advances in domain generalization (DG) aim to use data from source\n(training) domains to train a model that generalizes well to data from unseen\ntarget (testing) domains. In this paper, we are motivated by two major gaps in\nexisting work on FL and DG: (1) the lack of formal mathematical analysis of DG\nobjectives and training processes; and (2) DG research in FL being limited to\nthe conventional star-topology architecture. Addressing the second gap, we\ndevelop $\\textit{Decentralized Federated Domain Generalization with Style\nSharing}$ ($\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to\nallow devices in a peer-to-peer network to achieve DG based on sharing style\ninformation inferred from their datasets. Additionally, we fill the first gap\nby providing the first systematic approach to mathematically analyzing\nstyle-based DG training optimization. We cast existing centralized DG\nalgorithms within our framework, and employ their formalisms to model\n$\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which\na sub-linear convergence rate of $\\texttt{StyleDDG}$ can be obtained. Through\nexperiments on two popular DG datasets, we demonstrate that $\\texttt{StyleDDG}$\ncan obtain significant improvements in accuracy across target domains with\nminimal added communication overhead compared to decentralized gradient methods\nthat do not employ style sharing.",
      "pdf_url": "http://arxiv.org/pdf/2504.06235v1",
      "published": "2025-04-08T17:32:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06235v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models",
      "authors": [
        "Chejian Xu",
        "Wei Ping",
        "Peng Xu",
        "Zihan Liu",
        "Boxin Wang",
        "Mohammad Shoeybi",
        "Bo Li",
        "Bryan Catanzaro"
      ],
      "abstract": "Long-context capabilities are essential for a wide range of applications,\nincluding document and video understanding, in-context learning, and\ninference-time scaling, all of which require models to process and reason over\nlong sequences of text and multimodal data. In this work, we introduce a\nefficient training recipe for building ultra-long context LLMs from aligned\ninstruct model, pushing the boundaries of context lengths from 128K to 1M, 2M,\nand 4M tokens. Our approach leverages efficient continued pretraining\nstrategies to extend the context window and employs effective instruction\ntuning to maintain the instruction-following and reasoning abilities. Our\nUltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves\nstate-of-the-art performance across a diverse set of long-context benchmarks.\nImportantly, models trained with our approach maintain competitive performance\non standard benchmarks, demonstrating balanced improvements for both long and\nshort context tasks. We further provide an in-depth analysis of key design\nchoices, highlighting the impacts of scaling strategies and data composition.\nOur findings establish a robust framework for efficiently scaling context\nlengths while preserving general model capabilities. We release all model\nweights at: https://ultralong.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2504.06214v1",
      "published": "2025-04-08T16:58:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06214v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "An experimental survey and Perspective View on Meta-Learning for Automated Algorithms Selection and Parametrization",
      "authors": [
        "Moncef Garouani"
      ],
      "abstract": "Considerable progress has been made in the recent literature studies to\ntackle the Algorithms Selection and Parametrization (ASP) problem, which is\ndiversified in multiple meta-learning setups. Yet there is a lack of surveys\nand comparative evaluations that critically analyze, summarize and assess the\nperformance of existing methods. In this paper, we provide an overview of the\nstate of the art in this continuously evolving field. The survey sheds light on\nthe motivational reasons for pursuing classifiers selection through\nmeta-learning. In this regard, Automated Machine Learning (AutoML) is usually\ntreated as an ASP problem under the umbrella of the democratization of machine\nlearning. Accordingly, AutoML makes machine learning techniques accessible to\ndomain scientists who are interested in applying advanced analytics but lack\nthe required expertise. It can ease the task of manually selecting ML\nalgorithms and tuning related hyperparameters. We comprehensively discuss the\ndifferent phases of classifiers selection based on a generic framework that is\nformed as an outcome of reviewing prior works. Subsequently, we propose a\nbenchmark knowledge base of 4 millions previously learned models and present\nextensive comparative evaluations of the prominent methods for classifiers\nselection based on 08 classification algorithms and 400 benchmark datasets. The\ncomparative study quantitatively assesses the performance of algorithms\nselection methods along while emphasizing the strengths and limitations of\nexisting studies.",
      "pdf_url": "http://arxiv.org/pdf/2504.06207v1",
      "published": "2025-04-08T16:51:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06207v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
      "authors": [
        "Eric Wang",
        "Samuel Schmidgall",
        "Paul F. Jaeger",
        "Fan Zhang",
        "Rory Pilgrim",
        "Yossi Matias",
        "Joelle Barral",
        "David Fleet",
        "Shekoofeh Azizi"
      ],
      "abstract": "Therapeutic development is a costly and high-risk endeavor that is often\nplagued by high failure rates. To address this, we introduce TxGemma, a suite\nof efficient, generalist large language models (LLMs) capable of therapeutic\nproperty prediction as well as interactive reasoning and explainability. Unlike\ntask-specific models, TxGemma synthesizes information from diverse sources,\nenabling broad application across the therapeutic development pipeline. The\nsuite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a\ncomprehensive dataset of small molecules, proteins, nucleic acids, diseases,\nand cell lines. Across 66 therapeutic development tasks, TxGemma achieved\nsuperior or comparable performance to the state-of-the-art generalist model on\n64 (superior on 45), and against state-of-the-art specialist models on 50\n(superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks,\nsuch as clinical trial adverse event prediction, requires less training data\nthan fine-tuning base LLMs, making TxGemma suitable for data-limited\napplications. Beyond these predictive capabilities, TxGemma features\nconversational models that bridge the gap between general LLMs and specialized\nproperty predictors. These allow scientists to interact in natural language,\nprovide mechanistic reasoning for predictions based on molecular structure, and\nengage in scientific discussions. Building on this, we further introduce\nAgentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that\nreasons, acts, manages diverse workflows, and acquires external domain\nknowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last\nExam benchmark (Chemistry & Biology) with 52.3% relative improvement over\no3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels\nwith improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over\no3-mini (high).",
      "pdf_url": "http://arxiv.org/pdf/2504.06196v1",
      "published": "2025-04-08T16:39:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06196v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Heuristic Methods are Good Teachers to Distill MLPs for Graph Link Prediction",
      "authors": [
        "Zongyue Qin",
        "Shichang Zhang",
        "Mingxuan Ju",
        "Tong Zhao",
        "Neil Shah",
        "Yizhou Sun"
      ],
      "abstract": "Link prediction is a crucial graph-learning task with applications including\ncitation prediction and product recommendation. Distilling Graph Neural\nNetworks (GNNs) teachers into Multi-Layer Perceptrons (MLPs) students has\nemerged as an effective approach to achieve strong performance and reducing\ncomputational cost by removing graph dependency. However, existing distillation\nmethods only use standard GNNs and overlook alternative teachers such as\nspecialized model for link prediction (GNN4LP) and heuristic methods (e.g.,\ncommon neighbors). This paper first explores the impact of different teachers\nin GNN-to-MLP distillation. Surprisingly, we find that stronger teachers do not\nalways produce stronger students: MLPs distilled from GNN4LP can underperform\nthose distilled from simpler GNNs, while weaker heuristic methods can teach\nMLPs to near-GNN performance with drastically reduced training costs. Building\non these insights, we propose Ensemble Heuristic-Distilled MLPs (EHDM), which\neliminates graph dependencies while effectively integrating complementary\nsignals via a gating mechanism. Experiments on ten datasets show an average\n7.93% improvement over previous GNN-to-MLP approaches with 1.95-3.32 times less\ntraining time, indicating EHDM is an efficient and effective link prediction\nmethod.",
      "pdf_url": "http://arxiv.org/pdf/2504.06193v1",
      "published": "2025-04-08T16:35:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06193v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SkillFlow: Efficient Skill and Code Transfer Through Communication in Adapting AI Agents",
      "authors": [
        "Pagkratios Tagkopoulos",
        "Fangzhou Li",
        "Ilias Tagkopoulos"
      ],
      "abstract": "AI agents are autonomous systems that can execute specific tasks based on\npredefined programming. Here, we present SkillFlow, a modular,\ntechnology-agnostic framework that allows agents to expand their functionality\nin an ad-hoc fashion by acquiring new skills from their environment or other\nagents. We present a theoretical model that examines under which conditions\nthis framework would be beneficial, and we then explore SkillFlow's ability to\naccelerate task completion and lead to lower cumulative costs in a real-world\napplication, namely scheduling agents for calendar events. We demonstrate that\nwithin a few iterations, SkillFlow leads to considerable (24.8%, p-value =\n$6.4\\times10^{-3}$) gains in time and cost, especially when the communication\ncost is high. Finally, we draw analogies from well-studied biological systems\nand compare this framework to that of lateral gene transfer, a significant\nprocess of adaptation and evolution in novel environments.",
      "pdf_url": "http://arxiv.org/pdf/2504.06188v1",
      "published": "2025-04-08T16:33:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06188v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ]
    },
    {
      "title": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and Real-World Wound Care",
      "authors": [
        "Vanessa Borst",
        "Timo Dittus",
        "Tassilo Dege",
        "Astrid Schmieder",
        "Samuel Kounev"
      ],
      "abstract": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For fair comparison, we\nstandardize training, data augmentation, and evaluation, conducting\ncross-validationto minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates, and evaluate this, along with mask\nquality, for the best models based on physician assessments. Overall, the\ntransformer-based TransNeXt showed the highest levels of generalizability.\nDespite variations in inference times, all models processed at least one image\nper second on the CPU, which is deemed adequate for the intended application.\nInterpretability analysis typically revealed prominent activations in wound\nregions, emphasizing focus on clinically relevant features. Expert evaluation\nshowed high mask approval for all analyzed models, with VWFormer and ConvNeXtS\nbackbone performing the best. Size retrieval accuracy was similar across\nmodels, and predictions closely matched expert annotations. Finally, we\ndemonstrate how our AI-driven wound size estimation framework, WoundAmbit, can\nbe integrated into a custom telehealth system. Our code will be made available\non GitHub upon publication.",
      "pdf_url": "http://arxiv.org/pdf/2504.06185v1",
      "published": "2025-04-08T16:25:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06185v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
      "authors": [
        "Ian Groves",
        "Andrew Campbell",
        "James Fernandes",
        "Diego Rodriguez",
        "Paul Murray",
        "Massimiliano Vasile",
        "Victoria Nockles"
      ],
      "abstract": "Foundation Models, pre-trained on large unlabelled datasets before\ntask-specific fine-tuning, are increasingly being applied to specialised\ndomains. Recent examples include ClimaX for climate and Clay for satellite\nEarth observation, but a Foundation Model for Space Object Behavioural Analysis\nhas not yet been developed. As orbital populations grow, automated methods for\ncharacterising space object behaviour are crucial for space safety. We present\na Space Safety and Sustainability Foundation Model focusing on space object\nbehavioural analysis using light curves (LCs). We implemented a\nPerceiver-Variational Autoencoder (VAE) architecture, pre-trained with\nself-supervised reconstruction and masked reconstruction on 227,000 LCs from\nthe MMT-9 observatory. The VAE enables anomaly detection, motion prediction,\nand LC generation. We fine-tuned the model for anomaly detection & motion\nprediction using two independent LC simulators (CASSANDRA and GRIAL\nrespectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink\nplatforms. Our pre-trained model achieved a reconstruction error of 0.01%,\nidentifying potentially anomalous light curves through reconstruction\ndifficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90\nand 0.95 ROC AUC scores respectively in both anomaly detection and motion mode\nprediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly\npredictions on real data revealed distinct patterns including characteristic\nobject profiles and satellite glinting. Here, we demonstrate how\nself-supervised learning can simultaneously enable anomaly detection, motion\nprediction, and synthetic data generation from rich representations learned in\npre-training. Our work therefore supports space safety and sustainability\nthrough automated monitoring and simulation capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2504.06176v1",
      "published": "2025-04-08T16:19:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06176v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.space-ph"
      ]
    },
    {
      "title": "Multi-Modality Sensing in mmWave Beamforming for Connected Vehicles Using Deep Learning",
      "authors": [
        "Muhammad Baqer Mollah",
        "Honggang Wang",
        "Mohammad Ataul Karim",
        "Hua Fang"
      ],
      "abstract": "Beamforming techniques are considered as essential parts to compensate severe\npath losses in millimeter-wave (mmWave) communications. In particular, these\ntechniques adopt large antenna arrays and formulate narrow beams to obtain\nsatisfactory received powers. However, performing accurate beam alignment over\nnarrow beams for efficient link configuration by traditional standard defined\nbeam selection approaches, which mainly rely on channel state information and\nbeam sweeping through exhaustive searching, imposes computational and\ncommunications overheads. And, such resulting overheads limit their potential\nuse in vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V)\ncommunications involving highly dynamic scenarios. In comparison, utilizing\nout-of-band contextual information, such as sensing data obtained from sensor\ndevices, provides a better alternative to reduce overheads. This paper presents\na deep learning-based solution for utilizing the multi-modality sensing data\nfor predicting the optimal beams having sufficient mmWave received powers so\nthat the best V2I and V2V line-of-sight links can be ensured proactively. The\nproposed solution has been tested on real-world measured mmWave sensing and\ncommunication data, and the results show that it can achieve up to 98.19%\naccuracies while predicting top-13 beams. Correspondingly, when compared to\nexisting been sweeping approach, the beam sweeping searching space and time\noverheads are greatly shortened roughly by 79.67% and 91.89%, respectively\nwhich confirm a promising solution for beamforming in mmWave enabled\ncommunications.",
      "pdf_url": "http://arxiv.org/pdf/2504.06173v1",
      "published": "2025-04-08T16:18:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06173v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "Real-Time Pitch/F0 Detection Using Spectrogram Images and Convolutional Neural Networks",
      "authors": [
        "Xufang Zhao",
        "Omer Tsimhoni"
      ],
      "abstract": "This paper presents a novel approach to detect F0 through Convolutional\nNeural Networks and image processing techniques to directly estimate pitch from\nspectrogram images. Our new approach demonstrates a very good detection\naccuracy; a total of 92% of predicted pitch contours have strong or moderate\ncorrelations to the true pitch contours. Furthermore, the experimental\ncomparison between our new approach and other state-of-the-art CNN methods\nreveals that our approach can enhance the detection rate by approximately 5%\nacross various Signal-to-Noise Ratio conditions.",
      "pdf_url": "http://arxiv.org/pdf/2504.06165v1",
      "published": "2025-04-08T16:01:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06165v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
      "authors": [
        "Rijul Magu",
        "Arka Dutta",
        "Sean Kim",
        "Ashiqur R. KhudaBukhsh",
        "Munmun De Choudhury"
      ],
      "abstract": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation.",
      "pdf_url": "http://arxiv.org/pdf/2504.06160v2",
      "published": "2025-04-08T15:56:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06160v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.SI",
        "J.4; K.4.1; K.4.2"
      ]
    },
    {
      "title": "ARLO: A Tailorable Approach for Transforming Natural Language Software Requirements into Architecture using LLMs",
      "authors": [
        "Tooraj Helmi"
      ],
      "abstract": "Software requirements expressed in natural language (NL) frequently suffer\nfrom verbosity, ambiguity, and inconsistency. This creates a range of\nchallenges, including selecting an appropriate architecture for a system and\nassessing different architectural alternatives. Relying on human expertise to\naccomplish the task of mapping NL requirements to architecture is\ntime-consuming and error-prone. This paper proposes ARLO, an approach that\nautomates this task by leveraging (1) a set of NL requirements for a system,\n(2) an existing standard that specifies architecturally relevant software\nquality attributes, and (3) a readily available Large Language Model (LLM).\nSpecifically, ARLO determines the subset of NL requirements for a given system\nthat is architecturally relevant and maps that subset to a tailorable matrix of\narchitectural choices. ARLO applies integer linear programming on the\narchitectural-choice matrix to determine the optimal architecture for the\ncurrent requirements. We demonstrate ARLO's efficacy using a set of real-world\nexamples. We highlight ARLO's ability (1) to trace the selected architectural\nchoices to the requirements and (2) to isolate NL requirements that exert a\nparticular influence on a system's architecture. This allows the\nidentification, comparative assessment, and exploration of alternative\narchitectural choices based on the requirements and constraints expressed\ntherein.",
      "pdf_url": "http://arxiv.org/pdf/2504.06143v1",
      "published": "2025-04-08T15:38:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06143v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "A Multimedia Analytics Model for the Foundation Model Era",
      "authors": [
        "Marcel Worring",
        "Jan ZahÃ¡lka",
        "Stef van den Elzen",
        "Maximilian Fischer",
        "Daniel Keim"
      ],
      "abstract": "The rapid advances in Foundation Models and agentic Artificial Intelligence\nare transforming multimedia analytics by enabling richer, more sophisticated\ninteractions between humans and analytical systems. Existing conceptual models\nfor visual and multimedia analytics, however, do not adequately capture the\ncomplexity introduced by these powerful AI paradigms. To bridge this gap, we\npropose a comprehensive multimedia analytics model specifically designed for\nthe foundation model era. Building upon established frameworks from visual\nanalytics, multimedia analytics, knowledge generation, analytic task\ndefinition, mixed-initiative guidance, and human-in-the-loop reinforcement\nlearning, our model emphasizes integrated human-AI teaming based on visual\nanalytics agents from both technical and conceptual perspectives. Central to\nthe model is a seamless, yet explicitly separable, interaction channel between\nexpert users and semi-autonomous analytical processes, ensuring continuous\nalignment between user intent and AI behavior. The model addresses practical\nchallenges in sensitive domains such as intelligence analysis, investigative\njournalism, and other fields handling complex, high-stakes data. We illustrate\nthrough detailed case studies how our model facilitates deeper understanding\nand targeted improvement of multimedia analytics solutions. By explicitly\ncapturing how expert users can optimally interact with and guide AI-powered\nmultimedia analytics systems, our conceptual framework sets a clear direction\nfor system design, comparison, and future research.",
      "pdf_url": "http://arxiv.org/pdf/2504.06138v1",
      "published": "2025-04-08T15:35:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06138v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "QGen Studio: An Adaptive Question-Answer Generation, Training and Evaluation Platform",
      "authors": [
        "Movina Moses",
        "Mohab Elkaref",
        "James Barry",
        "Shinnosuke Tanaka",
        "Vishnudev Kuruvanthodi",
        "Nathan Herr",
        "Campbell D Watson",
        "Geeth De Mel"
      ],
      "abstract": "We present QGen Studio: an adaptive question-answer generation, training, and\nevaluation platform. QGen Studio enables users to leverage large language\nmodels (LLMs) to create custom question-answer datasets and fine-tune models on\nthis synthetic data. It features a dataset viewer and model explorer to\nstreamline this process. The dataset viewer provides key metrics and visualizes\nthe context from which the QA pairs are generated, offering insights into data\nquality. The model explorer supports model comparison, allowing users to\ncontrast the performance of their trained LLMs against other models, supporting\nperformance benchmarking and refinement. QGen Studio delivers an interactive,\nend-to-end solution for generating QA datasets and training scalable,\ndomain-adaptable models. The studio will be open-sourced soon, allowing users\nto deploy it locally.",
      "pdf_url": "http://arxiv.org/pdf/2504.06136v1",
      "published": "2025-04-08T15:32:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06136v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Decentralizing AI Memory: SHIMI, a Semantic Hierarchical Memory Index for Scalable Agent Reasoning",
      "authors": [
        "Tooraj Helmi"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) and vector-based search have become\nfoundational tools for memory in AI systems, yet they struggle with\nabstraction, scalability, and semantic precision - especially in decentralized\nenvironments. We present SHIMI (Semantic Hierarchical Memory Index), a unified\narchitecture that models knowledge as a dynamically structured hierarchy of\nconcepts, enabling agents to retrieve information based on meaning rather than\nsurface similarity. SHIMI organizes memory into layered semantic nodes and\nsupports top-down traversal from abstract intent to specific entities, offering\nmore precise and explainable retrieval. Critically, SHIMI is natively designed\nfor decentralized ecosystems, where agents maintain local memory trees and\nsynchronize them asynchronously across networks. We introduce a lightweight\nsync protocol that leverages Merkle-DAG summaries, Bloom filters, and\nCRDT-style conflict resolution to enable partial synchronization with minimal\noverhead. Through benchmark experiments and use cases involving decentralized\nagent collaboration, we demonstrate SHIMI's advantages in retrieval accuracy,\nsemantic fidelity, and scalability - positioning it as a core infrastructure\nlayer for decentralized cognitive systems.",
      "pdf_url": "http://arxiv.org/pdf/2504.06135v1",
      "published": "2025-04-08T15:31:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06135v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
      "authors": [
        "Jingyuan Zhang",
        "Qi Wang",
        "Xingguang Ji",
        "Yahui Liu",
        "Yang Yue",
        "Fuzheng Zhang",
        "Di Zhang",
        "Guorui Zhou",
        "Kun Gai"
      ],
      "abstract": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details.",
      "pdf_url": "http://arxiv.org/pdf/2504.06122v2",
      "published": "2025-04-08T15:15:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06122v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation",
      "authors": [
        "Abinav Kalyanasundaram",
        "Karthikeyan Chandra Sekaran",
        "Philipp Stauber",
        "Michael Lange",
        "Wolfgang Utschick",
        "Michael Botsch"
      ],
      "abstract": "Precise vehicle state estimation is crucial for safe and reliable autonomous\ndriving. The number of measurable states and their precision offered by the\nonboard vehicle sensor system are often constrained by cost. For instance,\nmeasuring critical quantities such as the Vehicle Sideslip Angle (VSA) poses\nsignificant commercial challenges using current optical sensors. This paper\naddresses these limitations by focusing on the development of high-performance\nvirtual sensors to enhance vehicle state estimation for active safety. The\nproposed Uncertainty-Aware Hybrid Learning (UAHL) architecture integrates a\nmachine learning model with vehicle motion models to estimate VSA directly from\nonboard sensor data. A key aspect of the UAHL architecture is its focus on\nuncertainty quantification for individual model estimates and hybrid fusion.\nThese mechanisms enable the dynamic weighting of uncertainty-aware predictions\nfrom machine learning and vehicle motion models to produce accurate and\nreliable hybrid VSA estimates. This work also presents a novel dataset named\nReal-world Vehicle State Estimation Dataset (ReV-StED), comprising synchronized\nmeasurements from advanced vehicle dynamic sensors. The experimental results\ndemonstrate the superior performance of the proposed method for VSA estimation,\nhighlighting UAHL as a promising architecture for advancing virtual sensors and\nenhancing active safety in autonomous vehicles.",
      "pdf_url": "http://arxiv.org/pdf/2504.06105v1",
      "published": "2025-04-08T14:49:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06105v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Towards Varroa destructor mite detection using a narrow spectra illumination",
      "authors": [
        "Samuel Bielik",
        "Simon Bilik"
      ],
      "abstract": "This paper focuses on the development and modification of a beehive\nmonitoring device and Varroa destructor detection on the bees with the help of\nhyperspectral imagery while utilizing a U-net, semantic segmentation\narchitecture, and conventional computer vision methods. The main objectives\nwere to collect a dataset of bees and mites, and propose the computer vision\nmodel which can achieve the detection between bees and mites.",
      "pdf_url": "http://arxiv.org/pdf/2504.06099v1",
      "published": "2025-04-08T14:41:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06099v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Real-Time LaCAM",
      "authors": [
        "Runzhe Liang",
        "Rishi Veerapaneni",
        "Daniel Harabor",
        "Jiaoyang Li",
        "Maxim Likhachev"
      ],
      "abstract": "The vast majority of Multi-Agent Path Finding (MAPF) methods with\ncompleteness guarantees require planning full horizon paths. However, planning\nfull horizon paths can take too long and be impractical in real-world\napplications. Instead, real-time planning and execution, which only allows the\nplanner a finite amount of time before executing and replanning, is more\npractical for real world multi-agent systems. Several methods utilize real-time\nplanning schemes but none are provably complete, which leads to livelock or\ndeadlock. Our main contribution is to show the first Real-Time MAPF method with\nprovable completeness guarantees. We do this by leveraging LaCAM (Okumura 2023)\nin an incremental fashion. Our results show how we can iteratively plan for\ncongested environments with a cutoff time of milliseconds while still\nmaintaining the same success rate as full horizon LaCAM. We also show how it\ncan be used with a single-step learned MAPF policy. The proposed Real-Time\nLaCAM also provides us with a general mechanism for using iterative constraints\nfor completeness in future real-time MAPF algorithms.",
      "pdf_url": "http://arxiv.org/pdf/2504.06091v1",
      "published": "2025-04-08T14:31:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06091v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "MCAT: Visual Query-Based Localization of Standard Anatomical Clips in Fetal Ultrasound Videos Using Multi-Tier Class-Aware Token Transformer",
      "authors": [
        "Divyanshu Mishra",
        "Pramit Saha",
        "He Zhao",
        "Netzahualcoyotl Hernandez-Cruz",
        "Olga Patey",
        "Aris Papageorghiou",
        "J. Alison Noble"
      ],
      "abstract": "Accurate standard plane acquisition in fetal ultrasound (US) videos is\ncrucial for fetal growth assessment, anomaly detection, and adherence to\nclinical guidelines. However, manually selecting standard frames is\ntime-consuming and prone to intra- and inter-sonographer variability. Existing\nmethods primarily rely on image-based approaches that capture standard frames\nand then classify the input frames across different anatomies. This ignores the\ndynamic nature of video acquisition and its interpretation. To address these\nchallenges, we introduce Multi-Tier Class-Aware Token Transformer (MCAT), a\nvisual query-based video clip localization (VQ-VCL) method, to assist\nsonographers by enabling them to capture a quick US sweep. By then providing a\nvisual query of the anatomy they wish to analyze, MCAT returns the video clip\ncontaining the standard frames for that anatomy, facilitating thorough\nscreening for potential anomalies. We evaluate MCAT on two ultrasound video\ndatasets and a natural image VQ-VCL dataset based on Ego4D. Our model\noutperforms state-of-the-art methods by 10% and 13% mIoU on the ultrasound\ndatasets and by 5.35% mIoU on the Ego4D dataset, using 96% fewer tokens. MCAT's\nefficiency and accuracy have significant potential implications for public\nhealth, especially in low- and middle-income countries (LMICs), where it may\nenhance prenatal care by streamlining standard plane acquisition, simplifying\nUS-based screening, diagnosis and allowing sonographers to examine more\npatients.",
      "pdf_url": "http://arxiv.org/pdf/2504.06088v1",
      "published": "2025-04-08T14:29:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06088v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Confidence Regularized Masked Language Modeling using Text Length",
      "authors": [
        "Seunghyun Ji",
        "Soowon Lee"
      ],
      "abstract": "Masked language modeling is a widely used method for learning language\nrepresentations, where the model predicts a randomly masked word in each input.\nHowever, this approach typically considers only a single correct answer during\ntraining, ignoring the variety of plausible alternatives that humans might\nchoose. This issue becomes more pronounced when the input text is short, as the\npossible word distribution tends to have higher entropy, potentially causing\nthe model to become overconfident in its predictions. To mitigate this, we\npropose a novel confidence regularizer that adaptively adjusts the\nregularization strength based on the input length. Experiments on the GLUE and\nSQuAD benchmarks show that our method improves both accuracy and expected\ncalibration error",
      "pdf_url": "http://arxiv.org/pdf/2504.06037v2",
      "published": "2025-04-08T13:37:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06037v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Information-Theoretic Reward Decomposition for Generalizable RLHF",
      "authors": [
        "Liyuan Mao",
        "Haoran Xu",
        "Amy Zhang",
        "Weinan Zhang",
        "Chenjia Bai"
      ],
      "abstract": "A generalizable reward model is crucial in Reinforcement Learning from Human\nFeedback (RLHF) as it enables correctly evaluating unseen prompt-response\npairs. However, existing reward models lack this ability, as they are typically\ntrained by increasing the reward gap between chosen and rejected responses,\nwhile overlooking the prompts that the responses are conditioned on.\nConsequently, when the trained reward model is evaluated on prompt-response\npairs that lie outside the data distribution, neglecting the effect of prompts\nmay result in poor generalization of the reward model. To address this issue,\nwe decompose the reward value into two independent components: prompt-free\nreward and prompt-related reward. Prompt-free reward represents the evaluation\nthat is determined only by responses, while the prompt-related reward reflects\nthe reward that derives from both the prompt and the response. We extract these\ntwo components from an information-theoretic perspective, which requires no\nextra models. Subsequently, we propose a new reward learning algorithm by\nprioritizing data samples based on their prompt-free reward values. Through toy\nexamples, we demonstrate that the extracted prompt-free and prompt-related\nrewards effectively characterize two parts of the reward model. Further,\nstandard evaluations show that our method improves both the alignment\nperformance and the generalization capability of the reward model.",
      "pdf_url": "http://arxiv.org/pdf/2504.06020v1",
      "published": "2025-04-08T13:26:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06020v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "The Hall of AI Fears and Hopes: Comparing the Views of AI Influencers and those of Members of the U.S. Public Through an Interactive Platform",
      "authors": [
        "Gustavo Moreira",
        "Edyta Paulina Bogucka",
        "Marios Constantinides",
        "Daniele Quercia"
      ],
      "abstract": "AI development is shaped by academics and industry leaders - let us call them\n``influencers'' - but it is unclear how their views align with those of the\npublic. To address this gap, we developed an interactive platform that served\nas a data collection tool for exploring public views on AI, including their\nfears, hopes, and overall sense of hopefulness. We made the platform available\nto 330 participants representative of the U.S. population in terms of age, sex,\nethnicity, and political leaning, and compared their views with those of 100 AI\ninfluencers identified by Time magazine. The public fears AI getting out of\ncontrol, while influencers emphasize regulation, seemingly to deflect attention\nfrom their alleged focus on monetizing AI's potential. Interestingly, the views\nof AI influencers from underrepresented groups such as women and people of\ncolor often differ from the views of underrepresented groups in the public.",
      "pdf_url": "http://arxiv.org/pdf/2504.06016v1",
      "published": "2025-04-08T13:21:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06016v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2; K.4.1; K.4.2; K.4.3"
      ]
    },
    {
      "title": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?",
      "authors": [
        "Roman Kochnev",
        "Arash Torabi Goodarzi",
        "Zofia Antonina Bentyn",
        "Dmitry Ignatov",
        "Radu Timofte"
      ],
      "abstract": "Optimal hyperparameter selection is critical for maximizing neural network\nperformance, especially as models grow in complexity. This work investigates\nthe viability of using large language models (LLMs) for hyperparameter\noptimization by employing a fine-tuned version of Code Llama. Through\nparameter-efficient fine-tuning using LoRA, we adapt the LLM to generate\naccurate and efficient hyperparameter recommendations tailored to diverse\nneural network architectures. Unlike traditional methods such as Optuna, which\nrely on exhaustive trials, the proposed approach achieves competitive or\nsuperior results in terms of Root Mean Square Error (RMSE) while significantly\nreducing computational overhead. Our approach highlights that LLM-based\noptimization not only matches state-of-the-art methods like Tree-structured\nParzen Estimators but also accelerates the tuning process. This positions LLMs\nas a promising alternative to conventional optimization techniques,\nparticularly for rapid experimentation. Furthermore, the ability to generate\nhyperparameters in a single inference step makes this method particularly\nwell-suited for resource-constrained environments such as edge devices and\nmobile applications, where computational efficiency is paramount. The results\nconfirm that LLMs, beyond their efficiency, offer substantial time savings and\ncomparable stability, underscoring their value in advancing machine learning\nworkflows. All generated hyperparameters are included in the LEMUR Neural\nNetwork (NN) Dataset, which is publicly available and serves as an open-source\nbenchmark for hyperparameter optimization research.",
      "pdf_url": "http://arxiv.org/pdf/2504.06006v1",
      "published": "2025-04-08T13:15:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.06006v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday Knowledge",
      "authors": [
        "Firoj Alam",
        "Md Arid Hasan",
        "Sahinur Rahman Laskar",
        "Mucahid Kutlu",
        "Shammur Absar Chowdhury"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose a\nframework, NativQA, that can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages, ranging from extremely\nlow-resource to high-resource languages, which resulted over 300K Question\nAnswer (QA) pairs. The developed resources can be used for LLM benchmarking and\nfurther fine-tuning. The framework has been made publicly available for the\ncommunity (https://gitlab.com/nativqa/nativqa-framework).",
      "pdf_url": "http://arxiv.org/pdf/2504.05995v1",
      "published": "2025-04-08T13:01:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05995v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "F.2.2; I.2.7"
      ]
    },
    {
      "title": "Temporal Alignment-Free Video Matching for Few-shot Action Recognition",
      "authors": [
        "SuBeen Lee",
        "WonJun Moon",
        "Hyun Seok Seong",
        "Jae-Pil Heo"
      ],
      "abstract": "Few-Shot Action Recognition (FSAR) aims to train a model with only a few\nlabeled video instances. A key challenge in FSAR is handling divergent\nnarrative trajectories for precise video matching. While the frame- and\ntuple-level alignment approaches have been promising, their methods heavily\nrely on pre-defined and length-dependent alignment units (e.g., frames or\ntuples), which limits flexibility for actions of varying lengths and speeds. In\nthis work, we introduce a novel TEmporal Alignment-free Matching (TEAM)\napproach, which eliminates the need for temporal units in action representation\nand brute-force alignment during matching. Specifically, TEAM represents each\nvideo with a fixed set of pattern tokens that capture globally discriminative\nclues within the video instance regardless of action length or speed, ensuring\nits flexibility. Furthermore, TEAM is inherently efficient, using token-wise\ncomparisons to measure similarity between videos, unlike existing methods that\nrely on pairwise comparisons for temporal alignment. Additionally, we propose\nan adaptation process that identifies and removes common information across\nclasses, establishing clear boundaries even between novel categories. Extensive\nexperiments demonstrate the effectiveness of TEAM. Codes are available at\ngithub.com/leesb7426/TEAM.",
      "pdf_url": "http://arxiv.org/pdf/2504.05956v1",
      "published": "2025-04-08T12:11:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05956v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Representing Normative Regulations in OWL DL for Automated Compliance Checking Supported by Text Annotation",
      "authors": [
        "Ildar Baimuratov",
        "Denis Turygin"
      ],
      "abstract": "Compliance checking is the process of determining whether a regulated entity\nadheres to these regulations. Currently, compliance checking is predominantly\nmanual, requiring significant time and highly skilled experts, while still\nbeing prone to errors caused by the human factor. Various approaches have been\nexplored to automate compliance checking, however, representing regulations in\nOWL DL language which enables compliance checking through OWL reasoning has not\nbeen adopted. In this work, we propose an annotation schema and an algorithm\nthat transforms text annotations into machine-interpretable OWL DL code. The\nproposed approach is validated through a proof-of-concept implementation\napplied to examples from the building construction domain.",
      "pdf_url": "http://arxiv.org/pdf/2504.05951v1",
      "published": "2025-04-08T12:05:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05951v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AEGIS: Human Attention-based Explainable Guidance for Intelligent Vehicle Systems",
      "authors": [
        "Zhuoli Zhuang",
        "Cheng-You Lu",
        "Yu-Cheng Fred Chang",
        "Yu-Kai Wang",
        "Thomas Do",
        "Chin-Teng Lin"
      ],
      "abstract": "Improving decision-making capabilities in Autonomous Intelligent Vehicles\n(AIVs) has been a heated topic in recent years. Despite advancements, training\nmachines to capture regions of interest for comprehensive scene understanding,\nlike human perception and reasoning, remains a significant challenge. This\nstudy introduces a novel framework, Human Attention-based Explainable Guidance\nfor Intelligent Vehicle Systems (AEGIS). AEGIS utilizes human attention,\nconverted from eye-tracking, to guide reinforcement learning (RL) models to\nidentify critical regions of interest for decision-making. AEGIS uses a\npre-trained human attention model to guide RL models to identify critical\nregions of interest for decision-making. By collecting 1.2 million frames from\n20 participants across six scenarios, AEGIS pre-trains a model to predict human\nattention patterns.",
      "pdf_url": "http://arxiv.org/pdf/2504.05950v1",
      "published": "2025-04-08T12:04:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05950v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics",
      "authors": [
        "Kuntian Zhang",
        "Simin Yu",
        "Yaoshu Wang",
        "Makoto Onizuka",
        "Chuan Xiao"
      ],
      "abstract": "In this paper, we propose CKGAN, a novel generative adversarial network (GAN)\nvariant based on an integral probability metrics framework with characteristic\nkernel (CKIPM). CKIPM, as a distance between two probability distributions, is\ndesigned to optimize the lowerbound of the maximum mean discrepancy (MMD) in a\nreproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN\nmitigates the notorious problem of mode collapse by mapping the generated\nimages back to random noise. To save the effort of selecting the kernel\nfunction manually, we propose a soft selection method to automatically learn a\ncharacteristic kernel function. The experimental evaluation conducted on a set\nof synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that\nCKGAN generally outperforms other MMD-based GANs. The results also show that at\nthe cost of moderately more training time, the automatically selected kernel\nfunction delivers very close performance to the best of manually fine-tuned one\non real image benchmarks and is able to improve the performances of other\nMMD-based GANs.",
      "pdf_url": "http://arxiv.org/pdf/2504.05945v1",
      "published": "2025-04-08T11:58:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05945v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Uncovering Fairness through Data Complexity as an Early Indicator",
      "authors": [
        "Juliett SuÃ¡rez Ferreira",
        "Marija Slavkovik",
        "Jorge Casillas"
      ],
      "abstract": "Fairness constitutes a concern within machine learning (ML) applications.\nCurrently, there is no study on how disparities in classification complexity\nbetween privileged and unprivileged groups could influence the fairness of\nsolutions, which serves as a preliminary indicator of potential unfairness. In\nthis work, we investigate this gap, specifically, we focus on synthetic\ndatasets designed to capture a variety of biases ranging from historical bias\nto measurement and representational bias to evaluate how various complexity\nmetrics differences correlate with group fairness metrics. We then apply\nassociation rule mining to identify patterns that link disproportionate\ncomplexity differences between groups with fairness-related outcomes, offering\ndata-centric indicators to guide bias mitigation. Our findings are also\nvalidated by their application in real-world problems, providing evidence that\nquantifying group-wise classification complexity can uncover early indicators\nof potential fairness challenges. This investigation helps practitioners to\nproactively address bias in classification tasks.",
      "pdf_url": "http://arxiv.org/pdf/2504.05923v1",
      "published": "2025-04-08T11:28:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05923v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS"
      ]
    },
    {
      "title": "PRIMEDrive-CoT: A Precognitive Chain-of-Thought Framework for Uncertainty-Aware Object Interaction in Driving Scene Scenario",
      "authors": [
        "Sriram Mandalika",
        "Lalitha V",
        "Athira Nambiar"
      ],
      "abstract": "Driving scene understanding is a critical real-world problem that involves\ninterpreting and associating various elements of a driving environment, such as\nvehicles, pedestrians, and traffic signals. Despite advancements in autonomous\ndriving, traditional pipelines rely on deterministic models that fail to\ncapture the probabilistic nature and inherent uncertainty of real-world\ndriving. To address this, we propose PRIMEDrive-CoT, a novel uncertainty-aware\nmodel for object interaction and Chain-of-Thought (CoT) reasoning in driving\nscenarios. In particular, our approach combines LiDAR-based 3D object detection\nwith multi-view RGB references to ensure interpretable and reliable scene\nunderstanding. Uncertainty and risk assessment, along with object interactions,\nare modelled using Bayesian Graph Neural Networks (BGNNs) for probabilistic\nreasoning under ambiguous conditions. Interpretable decisions are facilitated\nthrough CoT reasoning, leveraging object dynamics and contextual cues, while\nGrad-CAM visualizations highlight attention regions. Extensive evaluations on\nthe DriveCoT dataset demonstrate that PRIMEDrive-CoT outperforms\nstate-of-the-art CoT and risk-aware models.",
      "pdf_url": "http://arxiv.org/pdf/2504.05908v1",
      "published": "2025-04-08T11:06:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05908v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Turin3D: Evaluating Adaptation Strategies under Label Scarcity in Urban LiDAR Segmentation with Semi-Supervised Techniques",
      "authors": [
        "Luca Barco",
        "Giacomo Blanco",
        "Gaetano Chiriaco",
        "Alessia Intini",
        "Luigi La Riccia",
        "Vittorio Scolamiero",
        "Piero Boccardo",
        "Paolo Garza",
        "Fabrizio Dominici"
      ],
      "abstract": "3D semantic segmentation plays a critical role in urban modelling, enabling\ndetailed understanding and mapping of city environments. In this paper, we\nintroduce Turin3D: a new aerial LiDAR dataset for point cloud semantic\nsegmentation covering an area of around 1.43 km2 in the city centre of Turin\nwith almost 70M points. We describe the data collection process and compare\nTurin3D with others previously proposed in the literature. We did not fully\nannotate the dataset due to the complexity and time-consuming nature of the\nprocess; however, a manual annotation process was performed on the validation\nand test sets, to enable a reliable evaluation of the proposed techniques. We\nfirst benchmark the performances of several point cloud semantic segmentation\nmodels, trained on the existing datasets, when tested on Turin3D, and then\nimprove their performances by applying a semi-supervised learning technique\nleveraging the unlabelled training set. The dataset will be publicly available\nto support research in outdoor point cloud segmentation, with particular\nrelevance for self-supervised and semi-supervised learning approaches given the\nabsence of ground truth annotations for the training set.",
      "pdf_url": "http://arxiv.org/pdf/2504.05882v1",
      "published": "2025-04-08T10:17:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05882v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Systematic Parameter Decision in Approximate Model Counting",
      "authors": [
        "Jinping Lei",
        "Toru Takisaka",
        "Junqiang Peng",
        "Mingyu Xiao"
      ],
      "abstract": "This paper proposes a novel approach to determining the internal parameters\nof the hashing-based approximate model counting algorithm $\\mathsf{ApproxMC}$.\nIn this problem, the chosen parameter values must ensure that\n$\\mathsf{ApproxMC}$ is Probably Approximately Correct (PAC), while also making\nit as efficient as possible. The existing approach to this problem relies on\nheuristics; in this paper, we solve this problem by formulating it as an\noptimization problem that arises from generalizing $\\mathsf{ApproxMC}$'s\ncorrectness proof to arbitrary parameter values.\n  Our approach separates the concerns of algorithm soundness and optimality,\nallowing us to address the former without the need for repetitive case-by-case\nargumentation, while establishing a clear framework for the latter.\nFurthermore, after reduction, the resulting optimization problem takes on an\nexceptionally simple form, enabling the use of a basic search algorithm and\nproviding insight into how parameter values affect algorithm performance.\nExperimental results demonstrate that our optimized parameters improve the\nruntime performance of the latest $\\mathsf{ApproxMC}$ by a factor of 1.6 to\n2.4, depending on the error tolerance.",
      "pdf_url": "http://arxiv.org/pdf/2504.05874v1",
      "published": "2025-04-08T09:58:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05874v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Agent Guide: A Simple Agent Behavioral Watermarking Framework",
      "authors": [
        "Kaibo Huang",
        "Zhongliang Yang",
        "Linna Zhou"
      ],
      "abstract": "The increasing deployment of intelligent agents in digital ecosystems, such\nas social media platforms, has raised significant concerns about traceability\nand accountability, particularly in cybersecurity and digital content\nprotection. Traditional large language model (LLM) watermarking techniques,\nwhich rely on token-level manipulations, are ill-suited for agents due to the\nchallenges of behavior tokenization and information loss during\nbehavior-to-action translation. To address these issues, we propose Agent\nGuide, a novel behavioral watermarking framework that embeds watermarks by\nguiding the agent's high-level decisions (behavior) through probability biases,\nwhile preserving the naturalness of specific executions (action). Our approach\ndecouples agent behavior into two levels, behavior (e.g., choosing to bookmark)\nand action (e.g., bookmarking with specific tags), and applies watermark-guided\nbiases to the behavior probability distribution. We employ a z-statistic-based\nstatistical analysis to detect the watermark, ensuring reliable extraction over\nmultiple rounds. Experiments in a social media scenario with diverse agent\nprofiles demonstrate that Agent Guide achieves effective watermark detection\nwith a low false positive rate. Our framework provides a practical and robust\nsolution for agent watermarking, with applications in identifying malicious\nagents and protecting proprietary agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2504.05871v1",
      "published": "2025-04-08T09:54:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05871v1",
      "categories": [
        "cs.AI",
        "K.6.5"
      ]
    },
    {
      "title": "Are Generative AI Agents Effective Personalized Financial Advisors?",
      "authors": [
        "Takehiro Takayanagi",
        "Kiyoshi Izumi",
        "Javier Sanz-Cruzado",
        "Richard McCreadie",
        "Iadh Ounis"
      ],
      "abstract": "Large language model-based agents are becoming increasingly popular as a\nlow-cost mechanism to provide personalized, conversational advice, and have\ndemonstrated impressive capabilities in relatively simple scenarios, such as\nmovie recommendations. But how do these agents perform in complex high-stakes\ndomains, where domain expertise is essential and mistakes carry substantial\nrisk? This paper investigates the effectiveness of LLM-advisors in the finance\ndomain, focusing on three distinct challenges: (1) eliciting user preferences\nwhen users themselves may be unsure of their needs, (2) providing personalized\nguidance for diverse investment preferences, and (3) leveraging advisor\npersonality to build relationships and foster trust. Via a lab-based user study\nwith 64 participants, we show that LLM-advisors often match human advisor\nperformance when eliciting preferences, although they can struggle to resolve\nconflicting user needs. When providing personalized advice, the LLM was able to\npositively influence user behavior, but demonstrated clear failure modes. Our\nresults show that accurate preference elicitation is key, otherwise, the\nLLM-advisor has little impact, or can even direct the investor toward\nunsuitable assets. More worryingly, users appear insensitive to the quality of\nadvice being given, or worse these can have an inverse relationship. Indeed,\nusers reported a preference for and increased satisfaction as well as emotional\ntrust with LLMs adopting an extroverted persona, even though those agents\nprovided worse advice.",
      "pdf_url": "http://arxiv.org/pdf/2504.05862v1",
      "published": "2025-04-08T09:41:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05862v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.IR",
        "q-fin.CP"
      ]
    },
    {
      "title": "Towards an AI-Driven Video-Based American Sign Language Dictionary: Exploring Design and Usage Experience with Learners",
      "authors": [
        "Saad Hassan",
        "Matyas Bohacek",
        "Chaelin Kim",
        "Denise Crochet"
      ],
      "abstract": "Searching for unfamiliar American Sign Language (ASL) signs is challenging\nfor learners because, unlike spoken languages, they cannot type a text-based\nquery to look up an unfamiliar sign. Advances in isolated sign recognition have\nenabled the creation of video-based dictionaries, allowing users to submit a\nvideo and receive a list of the closest matching signs. Previous HCI research\nusing Wizard-of-Oz prototypes has explored interface designs for ASL\ndictionaries. Building on these studies, we incorporate their design\nrecommendations and leverage state-of-the-art sign-recognition technology to\ndevelop an automated video-based dictionary. We also present findings from an\nobservational study with twelve novice ASL learners who used this dictionary\nduring video-comprehension and question-answering tasks. Our results address\nhuman-AI interaction challenges not covered in previous WoZ research, including\nrecording and resubmitting signs, unpredictable outputs, system latency, and\nprivacy concerns. These insights offer guidance for designing and deploying\nvideo-based ASL dictionary systems.",
      "pdf_url": "http://arxiv.org/pdf/2504.05857v1",
      "published": "2025-04-08T09:35:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05857v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Coreference Resolution with Pretrained Language Models: Bridging the Gap Between Syntax and Semantics",
      "authors": [
        "Xingzu Liu",
        "Songhang deng",
        "Mingbang Wang",
        "Zhang Dong",
        "Le Dai",
        "Jiyuan Li",
        "Ruilin Nong"
      ],
      "abstract": "Large language models have made significant advancements in various natural\nlanguage processing tasks, including coreference resolution. However,\ntraditional methods often fall short in effectively distinguishing referential\nrelationships due to a lack of integration between syntactic and semantic\ninformation. This study introduces an innovative framework aimed at enhancing\ncoreference resolution by utilizing pretrained language models. Our approach\ncombines syntax parsing with semantic role labeling to accurately capture finer\ndistinctions in referential relationships. By employing state-of-the-art\npretrained models to gather contextual embeddings and applying an attention\nmechanism for fine-tuning, we improve the performance of coreference tasks.\nExperimental results across diverse datasets show that our method surpasses\nconventional coreference resolution systems, achieving notable accuracy in\ndisambiguating references. This development not only improves coreference\nresolution outcomes but also positively impacts other natural language\nprocessing tasks that depend on precise referential understanding.",
      "pdf_url": "http://arxiv.org/pdf/2504.05855v1",
      "published": "2025-04-08T09:33:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05855v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Physics-aware generative models for turbulent fluid flows through energy-consistent stochastic interpolants",
      "authors": [
        "Nikolaj T. MÃ¼cke",
        "Benjamin Sanderse"
      ],
      "abstract": "Generative models have demonstrated remarkable success in domains such as\ntext, image, and video synthesis. In this work, we explore the application of\ngenerative models to fluid dynamics, specifically for turbulence simulation,\nwhere classical numerical solvers are computationally expensive. We propose a\nnovel stochastic generative model based on stochastic interpolants, which\nenables probabilistic forecasting while incorporating physical constraints such\nas energy stability and divergence-freeness. Unlike conventional stochastic\ngenerative models, which are often agnostic to underlying physical laws, our\napproach embeds energy consistency by making the parameters of the stochastic\ninterpolant learnable coefficients. We evaluate our method on a benchmark\nturbulence problem - Kolmogorov flow - demonstrating superior accuracy and\nstability over state-of-the-art alternatives such as autoregressive conditional\ndiffusion models (ACDMs) and PDE-Refiner. Furthermore, we achieve stable\nresults for significantly longer roll-outs than standard stochastic\ninterpolants. Our results highlight the potential of physics-aware generative\nmodels in accelerating and enhancing turbulence simulations while preserving\nfundamental conservation properties.",
      "pdf_url": "http://arxiv.org/pdf/2504.05852v1",
      "published": "2025-04-08T09:29:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05852v1",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ]
    },
    {
      "title": "PathGPT: Leveraging Large Language Models for Personalized Route Generation",
      "authors": [
        "Steeve Cuthbert Marcelyn",
        "Yucen Gao",
        "Yuzhe Zhang",
        "Xiaofeng Gao",
        "Guihai Chen"
      ],
      "abstract": "The proliferation of GPS enabled devices has led to the accumulation of a\nsubstantial corpus of historical trajectory data. By leveraging these data for\ntraining machine learning models,researchers have devised novel data-driven\nmethodologies that address the personalized route recommendation (PRR) problem.\nIn contrast to conventional algorithms such as Dijkstra shortest path\nalgorithm,these novel algorithms possess the capacity to discern and learn\npatterns within the data,thereby facilitating the generation of more\npersonalized paths. However,once these models have been trained,their\napplication is constrained to the generation of routes that align with their\ntraining patterns. This limitation renders them less adaptable to novel\nscenarios and the deployment of multiple machine learning models might be\nnecessary to address new possible scenarios,which can be costly as each model\nmust be trained separately. Inspired by recent advances in the field of Large\nLanguage Models (LLMs),we leveraged their natural language understanding\ncapabilities to develop a unified model to solve the PRR problem while being\nseamlessly adaptable to new scenarios without additional training. To\naccomplish this,we combined the extensive knowledge LLMs acquired during\ntraining with further access to external hand-crafted context\ninformation,similar to RAG (Retrieved Augmented Generation) systems,to enhance\ntheir ability to generate paths according to user-defined requirements.\nExtensive experiments on different datasets show a considerable uplift in LLM\nperformance on the PRR problem.",
      "pdf_url": "http://arxiv.org/pdf/2504.05846v1",
      "published": "2025-04-08T09:25:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05846v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments",
      "authors": [
        "Dolton Fernandes",
        "Pramod Kaushik",
        "Harsh Shukla",
        "Bapi Raju Surampudi"
      ],
      "abstract": "Traditional Reinforcement Learning (RL) algorithms assume the distribution of\nthe data to be uniform or mostly uniform. However, this is not the case with\nmost real-world applications like autonomous driving or in nature where animals\nroam. Some experiences are encountered frequently, and most of the remaining\nexperiences occur rarely; the resulting distribution is called Zipfian. Taking\ninspiration from the theory of complementary learning systems, an architecture\nfor learning from Zipfian distributions is proposed where important long tail\ntrajectories are discovered in an unsupervised manner. The proposal comprises\nan episodic memory buffer containing a prioritised memory module to ensure\nimportant rare trajectories are kept longer to address the Zipfian problem,\nwhich needs credit assignment to happen in a sample efficient manner. The\nexperiences are then reinstated from episodic memory and given weighted\nimportance forming the trajectory to be executed. Notably, the proposed\narchitecture is modular, can be incorporated in any RL architecture and yields\nimproved performance in multiple Zipfian tasks over traditional architectures.\nOur method outperforms IMPALA by a significant margin on all three tasks and\nall three evaluation metrics (Zipfian, Uniform, and Rare Accuracy) and also\ngives improvements on most Atari environments that are considered challenging",
      "pdf_url": "http://arxiv.org/pdf/2504.05840v1",
      "published": "2025-04-08T09:21:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05840v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking",
      "authors": [
        "Junxi Chen",
        "Junhao Dong",
        "Xiaohua Xie"
      ],
      "abstract": "Recently, the Image Prompt Adapter (IP-Adapter) has been increasingly\nintegrated into text-to-image diffusion models (T2I-DMs) to improve\ncontrollability. However, in this paper, we reveal that T2I-DMs equipped with\nthe IP-Adapter (T2I-IP-DMs) enable a new jailbreak attack named the hijacking\nattack. We demonstrate that, by uploading imperceptible image-space adversarial\nexamples (AEs), the adversary can hijack massive benign users to jailbreak an\nImage Generation Service (IGS) driven by T2I-IP-DMs and mislead the public to\ndiscredit the service provider. Worse still, the IP-Adapter's dependency on\nopen-source image encoders reduces the knowledge required to craft AEs.\nExtensive experiments verify the technical feasibility of the hijacking attack.\nIn light of the revealed threat, we investigate several existing defenses and\nexplore combining the IP-Adapter with adversarially trained models to overcome\nexisting defenses' limitations. Our code is available at\nhttps://github.com/fhdnskfbeuv/attackIPA.",
      "pdf_url": "http://arxiv.org/pdf/2504.05838v1",
      "published": "2025-04-08T09:20:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05838v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Human Activity Recognition using RGB-Event based Sensors: A Multi-modal Heat Conduction Model and A Benchmark Dataset",
      "authors": [
        "Shiao Wang",
        "Xiao Wang",
        "Bo Jiang",
        "Lin Zhu",
        "Guoqi Li",
        "Yaowei Wang",
        "Yonghong Tian",
        "Jin Tang"
      ],
      "abstract": "Human Activity Recognition (HAR) primarily relied on traditional RGB cameras\nto achieve high-performance activity recognition. However, the challenging\nfactors in real-world scenarios, such as insufficient lighting and rapid\nmovements, inevitably degrade the performance of RGB cameras. To address these\nchallenges, biologically inspired event cameras offer a promising solution to\novercome the limitations of traditional RGB cameras. In this work, we rethink\nhuman activity recognition by combining the RGB and event cameras. The first\ncontribution is the proposed large-scale multi-modal RGB-Event human activity\nrecognition benchmark dataset, termed HARDVS 2.0, which bridges the dataset\ngaps. It contains 300 categories of everyday real-world actions with a total of\n107,646 paired videos covering various challenging scenarios. Inspired by the\nphysics-informed heat conduction model, we propose a novel multi-modal heat\nconduction operation framework for effective activity recognition, termed\nMMHCO-HAR. More in detail, given the RGB frames and event streams, we first\nextract the feature embeddings using a stem network. Then, multi-modal Heat\nConduction blocks are designed to fuse the dual features, the key module of\nwhich is the multi-modal Heat Conduction Operation layer. We integrate RGB and\nevent embeddings through a multi-modal DCT-IDCT layer while adaptively\nincorporating the thermal conductivity coefficient via FVEs into this module.\nAfter that, we propose an adaptive fusion module based on a policy routing\nstrategy for high-performance classification. Comprehensive experiments\ndemonstrate that our method consistently performs well, validating its\neffectiveness and robustness. The source code and benchmark dataset will be\nreleased on https://github.com/Event-AHU/HARDVS/tree/HARDVSv2",
      "pdf_url": "http://arxiv.org/pdf/2504.05830v1",
      "published": "2025-04-08T09:14:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05830v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models",
      "authors": [
        "Jiahao Chen",
        "Yu Pan",
        "Yi Du",
        "Chunkai Wu",
        "Lin Wang"
      ],
      "abstract": "Recently, the diffusion model has gained significant attention as one of the\nmost successful image generation models, which can generate high-quality images\nby iteratively sampling noise. However, recent studies have shown that\ndiffusion models are vulnerable to backdoor attacks, allowing attackers to\nenter input data containing triggers to activate the backdoor and generate\ntheir desired output. Existing backdoor attack methods primarily focused on\ntarget noise-to-image and text-to-image tasks, with limited work on backdoor\nattacks in image-to-image tasks. Furthermore, traditional backdoor attacks\noften rely on a single, conspicuous trigger to generate a fixed target image,\nlacking concealability and flexibility. To address these limitations, we\npropose a novel backdoor attack method called \"Parasite\" for image-to-image\ntasks in diffusion models, which not only is the first to leverage\nsteganography for triggers hiding, but also allows attackers to embed the\ntarget content as a backdoor trigger to achieve a more flexible attack.\n\"Parasite\" as a novel attack method effectively bypasses existing detection\nframeworks to execute backdoor attacks. In our experiments, \"Parasite\" achieved\na 0 percent backdoor detection rate against the mainstream defense frameworks.\nIn addition, in the ablation study, we discuss the influence of different\nhiding coefficients on the attack results. You can find our code at\nhttps://anonymous.4open.science/r/Parasite-1715/.",
      "pdf_url": "http://arxiv.org/pdf/2504.05815v1",
      "published": "2025-04-08T08:53:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05815v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Meta-Continual Learning of Neural Fields",
      "authors": [
        "Seungyoon Woo",
        "Junhyeog Yun",
        "Gunhee Kim"
      ],
      "abstract": "Neural Fields (NF) have gained prominence as a versatile framework for\ncomplex data representation. This work unveils a new problem setting termed\n\\emph{Meta-Continual Learning of Neural Fields} (MCL-NF) and introduces a novel\nstrategy that employs a modular architecture combined with optimization-based\nmeta-learning. Focused on overcoming the limitations of existing methods for\ncontinual learning of neural fields, such as catastrophic forgetting and slow\nconvergence, our strategy achieves high-quality reconstruction with\nsignificantly improved learning speed. We further introduce Fisher Information\nMaximization loss for neural radiance fields (FIM-NeRF), which maximizes\ninformation gains at the sample level to enhance learning generalization, with\nproved convergence guarantee and generalization bound. We perform extensive\nevaluations across image, audio, video reconstruction, and view synthesis tasks\non six diverse datasets, demonstrating our method's superiority in\nreconstruction quality and speed over existing MCL and CL-NF approaches.\nNotably, our approach attains rapid adaptation of neural fields for city-scale\nNeRF rendering with reduced parameter requirement.",
      "pdf_url": "http://arxiv.org/pdf/2504.05806v1",
      "published": "2025-04-08T08:38:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05806v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "From Superficial to Deep: Integrating External Knowledge for Follow-up Question Generation Using Knowledge Graph and LLM",
      "authors": [
        "Jianyu Liu",
        "Yi Huang",
        "Sheng Bi",
        "Junlan Feng",
        "Guilin Qi"
      ],
      "abstract": "In a conversational system, dynamically generating follow-up questions based\non context can help users explore information and provide a better user\nexperience. Humans are usually able to ask questions that involve some general\nlife knowledge and demonstrate higher order cognitive skills. However, the\nquestions generated by existing methods are often limited to shallow contextual\nquestions that are uninspiring and have a large gap to the human level. In this\npaper, we propose a three-stage external knowledge-enhanced follow-up question\ngeneration method, which generates questions by identifying contextual topics,\nconstructing a knowledge graph (KG) online, and finally combining these with a\nlarge language model to generate the final question. The model generates\ninformation-rich and exploratory follow-up questions by introducing external\ncommon sense knowledge and performing a knowledge fusion operation. Experiments\nshow that compared to baseline models, our method generates questions that are\nmore informative and closer to human questioning levels while maintaining\ncontextual relevance.",
      "pdf_url": "http://arxiv.org/pdf/2504.05801v1",
      "published": "2025-04-08T08:31:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05801v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM",
      "authors": [
        "Jirong Zha",
        "Yuxuan Fan",
        "Xiao Yang",
        "Chen Gao",
        "Xinlei Chen"
      ],
      "abstract": "3D spatial understanding is essential in real-world applications such as\nrobotics, autonomous vehicles, virtual reality, and medical imaging. Recently,\nLarge Language Models (LLMs), having demonstrated remarkable success across\nvarious domains, have been leveraged to enhance 3D understanding tasks, showing\npotential to surpass traditional computer vision methods. In this survey, we\npresent a comprehensive review of methods integrating LLMs with 3D spatial\nunderstanding. We propose a taxonomy that categorizes existing methods into\nthree branches: image-based methods deriving 3D understanding from 2D visual\ndata, point cloud-based methods working directly with 3D representations, and\nhybrid modality-based methods combining multiple data streams. We\nsystematically review representative methods along these categories, covering\ndata representations, architectural modifications, and training strategies that\nbridge textual and 3D modalities. Finally, we discuss current limitations,\nincluding dataset scarcity and computational challenges, while highlighting\npromising research directions in spatial perception, multi-modal fusion, and\nreal-world applications.",
      "pdf_url": "http://arxiv.org/pdf/2504.05786v1",
      "published": "2025-04-08T08:11:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05786v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Video Flow as Time Series: Discovering Temporal Consistency and Variability for VideoQA",
      "authors": [
        "Zijie Song",
        "Zhenzhen Hu",
        "Yixiao Ma",
        "Jia Li",
        "Richang Hong"
      ],
      "abstract": "Video Question Answering (VideoQA) is a complex video-language task that\ndemands a sophisticated understanding of both visual content and temporal\ndynamics. Traditional Transformer-style architectures, while effective in\nintegrating multimodal data, often simplify temporal dynamics through\npositional encoding and fail to capture non-linear interactions within video\nsequences. In this paper, we introduce the Temporal Trio Transformer (T3T), a\nnovel architecture that models time consistency and time variability. The T3T\nintegrates three key components: Temporal Smoothing (TS), Temporal Difference\n(TD), and Temporal Fusion (TF). The TS module employs Brownian Bridge for\ncapturing smooth, continuous temporal transitions, while the TD module\nidentifies and encodes significant temporal variations and abrupt changes\nwithin the video content. Subsequently, the TF module synthesizes these\ntemporal features with textual cues, facilitating a deeper contextual\nunderstanding and response accuracy. The efficacy of the T3T is demonstrated\nthrough extensive testing on multiple VideoQA benchmark datasets. Our results\nunderscore the importance of a nuanced approach to temporal modeling in\nimproving the accuracy and depth of video-based question answering.",
      "pdf_url": "http://arxiv.org/pdf/2504.05783v1",
      "published": "2025-04-08T08:08:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05783v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models",
      "authors": [
        "Pengfei Zhou",
        "Fanrui Zhang",
        "Xiaopeng Peng",
        "Zhaopan Xu",
        "Jiaxin Ai",
        "Yansheng Qiu",
        "Chuanhao Li",
        "Zhen Li",
        "Ming Li",
        "Yukang Feng",
        "Jianwen Sun",
        "Haoquan Zhang",
        "Zizhen Li",
        "Xiaofeng Mao",
        "Wangbo Zhao",
        "Kai Wang",
        "Xiaojun Chang",
        "Wenqi Shao",
        "Yang You",
        "Kaipeng Zhang"
      ],
      "abstract": "Multimodal reasoning, which integrates language and visual cues into problem\nsolving and decision making, is a fundamental aspect of human intelligence and\na crucial step toward artificial general intelligence. However, the evaluation\nof multimodal reasoning capabilities in Multimodal Large Language Models\n(MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained\nby limited data size, narrow domain coverage, and unstructured knowledge\ndistribution. To close these gaps, we introduce MDK12-Bench, a\nmulti-disciplinary benchmark assessing the reasoning capabilities of MLLMs via\nreal-world K-12 examinations. Spanning six disciplines (math, physics,\nchemistry, biology, geography, and information science), our benchmark\ncomprises 140K reasoning instances across diverse difficulty levels from\nprimary school to 12th grade. It features 6,827 instance-level knowledge point\nannotations based on a well-organized knowledge structure, detailed answer\nexplanations, difficulty labels and cross-year partitions, providing a robust\nplatform for comprehensive evaluation. Additionally, we present a novel dynamic\nevaluation framework to mitigate data contamination issues by bootstrapping\nquestion forms, question types, and image styles during evaluation. Extensive\nexperiment on MDK12-Bench reveals the significant limitation of current MLLMs\nin multimodal reasoning. The findings on our benchmark provide insights into\nthe development of the next-generation models. Our data and codes are available\nat https://github.com/LanceZPF/MDK12.",
      "pdf_url": "http://arxiv.org/pdf/2504.05782v1",
      "published": "2025-04-08T08:06:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05782v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}