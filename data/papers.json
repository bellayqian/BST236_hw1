{
  "last_updated": "2025-03-22T00:45:35.927472",
  "papers": [
    {
      "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance",
      "authors": [
        "Quanhao Li",
        "Zhen Xing",
        "Rui Wang",
        "Hui Zhang",
        "Qi Dai",
        "Zuxuan Wu"
      ],
      "abstract": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.",
      "pdf_url": "http://arxiv.org/pdf/2503.16421v1",
      "published": "2025-03-20T17:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16421v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "Survey on Evaluation of LLM-based Agents",
      "authors": [
        "Asaf Yehudai",
        "Lilach Eden",
        "Alan Li",
        "Guy Uziel",
        "Yilun Zhao",
        "Roy Bar-Haim",
        "Arman Cohan",
        "Michal Shmueli-Scheuer"
      ],
      "abstract": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
      "pdf_url": "http://arxiv.org/pdf/2503.16416v1",
      "published": "2025-03-20T17:59:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16416v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "DreamTexture: Shape from Virtual Texture with Analysis by Augmentation",
      "authors": [
        "Ananta R. Bhattarai",
        "Xingzhe He",
        "Alla Sheffer",
        "Helge Rhodin"
      ],
      "abstract": "DreamFusion established a new paradigm for unsupervised 3D reconstruction\nfrom virtual views by combining advances in generative models and\ndifferentiable rendering. However, the underlying multi-view rendering, along\nwith supervision from large-scale generative models, is computationally\nexpensive and under-constrained. We propose DreamTexture, a novel\nShape-from-Virtual-Texture approach that leverages monocular depth cues to\nreconstruct 3D objects. Our method textures an input image by aligning a\nvirtual texture with the real depth cues in the input, exploiting the inherent\nunderstanding of monocular geometry encoded in modern diffusion models. We then\nreconstruct depth from the virtual texture deformation with a new conformal map\noptimization, which alleviates memory-intensive volumetric representations. Our\nexperiments reveal that generative models possess an understanding of monocular\nshape cues, which can be extracted by augmenting and aligning texture cues -- a\nnovel monocular reconstruction paradigm that we call Analysis by Augmentation.",
      "pdf_url": "http://arxiv.org/pdf/2503.16412v1",
      "published": "2025-03-20T17:59:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16412v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints",
      "authors": [
        "Yiran Qin",
        "Li Kang",
        "Xiufeng Song",
        "Zhenfei Yin",
        "Xiaohong Liu",
        "Xihui Liu",
        "Ruimao Zhang",
        "Lei Bai"
      ],
      "abstract": "Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2503.16408v1",
      "published": "2025-03-20T17:58:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16408v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
      "authors": [
        "Yifan Sun",
        "Han Wang",
        "Dongbai Li",
        "Gang Wang",
        "Huan Zhang"
      ],
      "abstract": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment.",
      "pdf_url": "http://arxiv.org/pdf/2503.16402v1",
      "published": "2025-03-20T17:55:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16402v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World",
      "authors": [
        "Chen Chen",
        "Zhirui Wang",
        "Taowei Sheng",
        "Yi Jiang",
        "Yundu Li",
        "Peirui Cheng",
        "Luning Zhang",
        "Kaiqiang Chen",
        "Yanfeng Hu",
        "Xue Yang",
        "Xian Sun"
      ],
      "abstract": "Existing vision-based 3D occupancy prediction methods are inherently limited\nin accuracy due to their exclusive reliance on street-view imagery, neglecting\nthe potential benefits of incorporating satellite views. We propose SA-Occ, the\nfirst Satellite-Assisted 3D occupancy prediction model, which leverages GPS &\nIMU to integrate historical yet readily available satellite imagery into\nreal-time applications, effectively mitigating limitations of ego-vehicle\nperceptions, involving occlusions and degraded performance in distant regions.\nTo address the core challenges of cross-view perception, we propose: 1)\nDynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions\ncaused by the temporal asynchrony between satellite and street views; 2)\n3D-Proj Guidance, a module that enhances 3D feature extraction from inherently\n2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the\nsampling density between street and satellite views. Evaluated on\nOcc3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among\nsingle-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring\nonly 6.93 ms of additional latency per frame. Our code and newly curated\ndataset are available at https://github.com/chenchen235/SA-Occ.",
      "pdf_url": "http://arxiv.org/pdf/2503.16399v1",
      "published": "2025-03-20T17:54:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16399v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?",
      "authors": [
        "Akhil Perincherry",
        "Jacob Krantz",
        "Stefan Lee"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an\nunseen environment using natural language instructions. In this work, we study\nif visual representations of sub-goals implied by the instructions can serve as\nnavigational cues and lead to increased navigation performance. To synthesize\nthese visual representations or imaginations, we leverage a text-to-image\ndiffusion model on landmark references contained in segmented instructions.\nThese imaginations are provided to VLN agents as an added modality to act as\nlandmark cues and an auxiliary loss is added to explicitly encourage relating\nthese with their corresponding referring expressions. Our findings reveal an\nincrease in success rate (SR) of around 1 point and up to 0.5 points in success\nscaled by inverse path length (SPL) across agents. These results suggest that\nthe proposed approach reinforces visual understanding compared to relying on\nlanguage instructions alone. Code and data for our work can be found at\nhttps://www.akhilperincherry.com/VLN-Imagine-website/.",
      "pdf_url": "http://arxiv.org/pdf/2503.16394v1",
      "published": "2025-03-20T17:53:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16394v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ]
    },
    {
      "title": "Graph of Effort: Quantifying Risk of AI Usage for Vulnerability Assessment",
      "authors": [
        "Anket Mehra",
        "Andreas Aßmuth",
        "Malte Prieß"
      ],
      "abstract": "With AI-based software becoming widely available, the risk of exploiting its\ncapabilities, such as high automation and complex pattern recognition, could\nsignificantly increase. An AI used offensively to attack non-AI assets is\nreferred to as offensive AI.\n  Current research explores how offensive AI can be utilized and how its usage\ncan be classified. Additionally, methods for threat modeling are being\ndeveloped for AI-based assets within organizations. However, there are gaps\nthat need to be addressed. Firstly, there is a need to quantify the factors\ncontributing to the AI threat. Secondly, there is a requirement to create\nthreat models that analyze the risk of being attacked by AI for vulnerability\nassessment across all assets of an organization. This is particularly crucial\nand challenging in cloud environments, where sophisticated infrastructure and\naccess control landscapes are prevalent. The ability to quantify and further\nanalyze the threat posed by offensive AI enables analysts to rank\nvulnerabilities and prioritize the implementation of proactive countermeasures.\n  To address these gaps, this paper introduces the Graph of Effort, an\nintuitive, flexible, and effective threat modeling method for analyzing the\neffort required to use offensive AI for vulnerability exploitation by an\nadversary. While the threat model is functional and provides valuable support,\nits design choices need further empirical validation in future work.",
      "pdf_url": "http://arxiv.org/pdf/2503.16392v1",
      "published": "2025-03-20T17:52:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16392v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "Attentional Triple-Encoder Network in Spatiospectral Domains for Medical Image Segmentation",
      "authors": [
        "Kristin Qi",
        "Xinhan Di"
      ],
      "abstract": "Retinal Optical Coherence Tomography (OCT) segmentation is essential for\ndiagnosing pathology. Traditional methods focus on either spatial or spectral\ndomains, overlooking their combined dependencies. We propose a triple-encoder\nnetwork that integrates CNNs for spatial features, Fast Fourier Convolution\n(FFC) for spectral features, and attention mechanisms to capture global\nrelationships across both domains. Attention fusion modules integrate\nconvolution and cross-attention to further enhance features. Our method\nachieves an average Dice score improvement from 0.855 to 0.864, outperforming\nprior work.",
      "pdf_url": "http://arxiv.org/pdf/2503.16389v1",
      "published": "2025-03-20T17:49:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16389v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Deconstructing Long Chain-of-Thought: A Structured Reasoning Optimization Framework for Long CoT Distillation",
      "authors": [
        "Yijia Luo",
        "Yulin Song",
        "Xingyao Zhang",
        "Jiaheng Liu",
        "Weixun Wang",
        "GengRu Chen",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2503.16385v1",
      "published": "2025-03-20T17:46:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16385v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Reinforcement Learning-based Heuristics to Guide Domain-Independent Dynamic Programming",
      "authors": [
        "Minori Narita",
        "Ryo Kuroiwa",
        "J. Christopher Beck"
      ],
      "abstract": "Domain-Independent Dynamic Programming (DIDP) is a state-space search\nparadigm based on dynamic programming for combinatorial optimization. In its\ncurrent implementation, DIDP guides the search using user-defined dual bounds.\nReinforcement learning (RL) is increasingly being applied to combinatorial\noptimization problems and shares several key structures with DP, being\nrepresented by the Bellman equation and state-based transition systems. We\npropose using reinforcement learning to obtain a heuristic function to guide\nthe search in DIDP. We develop two RL-based guidance approaches: value-based\nguidance using Deep Q-Networks and policy-based guidance using Proximal Policy\nOptimization. Our experiments indicate that RL-based guidance significantly\noutperforms standard DIDP and problem-specific greedy heuristics with the same\nnumber of node expansions. Further, despite longer node evaluation times, RL\nguidance achieves better run-time performance than standard DIDP on three of\nfour benchmark domains.",
      "pdf_url": "http://arxiv.org/pdf/2503.16371v1",
      "published": "2025-03-20T17:33:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16371v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse",
      "authors": [
        "Muyao Li",
        "Zihao Wang",
        "Kaichen He",
        "Xiaojian Ma",
        "Yitao Liang"
      ],
      "abstract": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.",
      "pdf_url": "http://arxiv.org/pdf/2503.16365v1",
      "published": "2025-03-20T17:21:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16365v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Neural Networks: According to the Principles of Grassmann Algebra",
      "authors": [
        "Z. Zarezadeh",
        "N. Zarezadeh"
      ],
      "abstract": "In this paper, we explore the algebra of quantum idempotents and the\nquantization of fermions which gives rise to a Hilbert space equal to the\nGrassmann algebra associated with the Lie algebra. Since idempotents carry\nrepresentations of the algebra under consideration, they form algebraic\nvarieties and smooth manifolds in the natural topology. In addition to the\nmotivation of linking up mathematical physics with machine learning, it is also\nshown that by using idempotents and invariant subspace of the corresponding\nalgebras, these representations encode and perhaps provide a probabilistic\ninterpretation of reasoning and relational paths in geometrical terms.",
      "pdf_url": "http://arxiv.org/pdf/2503.16364v1",
      "published": "2025-03-20T17:21:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16364v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
      "authors": [
        "Yunzhi Yao",
        "Jizhan Fang",
        "Jia-Chen Gu",
        "Ningyu Zhang",
        "Shumin Deng",
        "Huajun Chen",
        "Nanyun Peng"
      ],
      "abstract": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
      "pdf_url": "http://arxiv.org/pdf/2503.16356v1",
      "published": "2025-03-20T17:14:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16356v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Palatable Conceptions of Disembodied Being: Terra Incognita in the Space of Possible Minds",
      "authors": [
        "Murray Shanahan"
      ],
      "abstract": "Is it possible to articulate a conception of consciousness that is compatible\nwith the exotic characteristics of contemporary, disembodied AI systems, and\nthat can stand up to philosophical scrutiny? How would subjective time and\nselfhood show up for an entity that conformed to such a conception? Trying to\nanswer these questions, even metaphorically, stretches the language of\nconsciousness to breaking point. Ultimately, the attempt yields something like\nemptiness, in the Buddhist sense, and helps to undermine our dualistic\ninclinations towards subjectivity and selfhood.",
      "pdf_url": "http://arxiv.org/pdf/2503.16348v1",
      "published": "2025-03-20T17:05:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16348v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "HiQ-Lip: The First Quantum-Classical Hierarchical Method for Global Lipschitz Constant Estimation of ReLU Networks",
      "authors": [
        "Haoqi He",
        "Yan Xiao"
      ],
      "abstract": "Estimating the global Lipschitz constant of neural networks is crucial for\nunderstanding and improving their robustness and generalization capabilities.\nHowever, precise calculations are NP-hard, and current semidefinite programming\n(SDP) methods face challenges such as high memory usage and slow processing\nspeeds. In this paper, we propose \\textbf{HiQ-Lip}, a hybrid quantum-classical\nhierarchical method that leverages Coherent Ising Machines (CIMs) to estimate\nthe global Lipschitz constant. We tackle the estimation by converting it into a\nQuadratic Unconstrained Binary Optimization (QUBO) problem and implement a\nmultilevel graph coarsening and refinement strategy to adapt to the constraints\nof contemporary quantum hardware. Our experimental evaluations on fully\nconnected neural networks demonstrate that HiQ-Lip not only provides estimates\ncomparable to state-of-the-art methods but also significantly accelerates the\ncomputation process. In specific tests involving two-layer neural networks with\n256 hidden neurons, HiQ-Lip doubles the solving speed and offers more accurate\nupper bounds than the existing best method, LiPopt. These findings highlight\nthe promising utility of small-scale quantum devices in advancing the\nestimation of neural network robustness.",
      "pdf_url": "http://arxiv.org/pdf/2503.16342v1",
      "published": "2025-03-20T16:58:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16342v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ]
    },
    {
      "title": "Enhancing Software Quality Assurance with an Adaptive Differential Evolution based Quantum Variational Autoencoder-Transformer Model",
      "authors": [
        "Seshu Babu Barma",
        "Mohanakrishnan Hariharan",
        "Satish Arvapalli"
      ],
      "abstract": "An AI-powered quality engineering platform uses artificial intelligence to\nboost software quality assessments through automated defect prediction and\noptimized performance alongside improved feature extraction. Existing models\nresult in difficulties addressing noisy data types together with imbalances,\npattern recognition complexities, ineffective feature extraction, and\ngeneralization weaknesses. To overcome those existing challenges in this\nresearch, we develop a new model Adaptive Differential Evolution based Quantum\nVariational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum\nVariational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent\nfeatures and maintain sequential dependencies together with contextual\nrelationships, resulting in superior defect prediction accuracy. Adaptive\nDifferential Evolution (ADE) Optimization utilizes an adaptive parameter tuning\nmethod that enhances model convergence and predictive performance. ADE-QVAET\nintegrates advanced AI techniques to create a robust solution for scalable and\naccurate software defect prediction that represents a top-level AI-driven\ntechnology for quality engineering applications. The proposed ADE-QVAET model\nattains high accuracy, precision, recall, and f1-score during the training\npercentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%.",
      "pdf_url": "http://arxiv.org/pdf/2503.16335v1",
      "published": "2025-03-20T16:55:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16335v1",
      "categories": [
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "Knowledge-guided machine learning model with soil moisture for corn yield prediction under drought conditions",
      "authors": [
        "Xiaoyu Wang",
        "Yijia Xu",
        "Jingyi Huang",
        "Zhengwei Yang",
        "Zhou Zhang"
      ],
      "abstract": "Remote sensing (RS) techniques, by enabling non-contact acquisition of\nextensive ground observations, have become a valuable tool for corn yield\nprediction. Traditional process-based (PB) models are limited by fixed input\nfeatures and struggle to incorporate large volumes of RS data. In contrast,\nmachine learning (ML) models are often criticized for being ``black boxes''\nwith limited interpretability. To address these limitations, we used\nKnowledge-Guided Machine Learning (KGML), which combined the strengths of both\napproaches and fully used RS data. However, previous KGML methods overlooked\nthe crucial role of soil moisture in plant growth. To bridge this gap, we\nproposed the Knowledge-Guided Machine Learning with Soil Moisture (KGML-SM)\nframework, using soil moisture as an intermediate variable to emphasize its key\nrole in plant development. Additionally, based on the prior knowledge that the\nmodel may overestimate under drought conditions, we designed a drought-aware\nloss function that penalizes predicted yield in drought-affected areas. Our\nexperiments showed that the KGML-SM model outperformed other ML models.\nFinally, we explored the relationships between drought, soil moisture, and corn\nyield prediction, assessing the importance of various features and analyzing\nhow soil moisture impacts corn yield predictions across different regions and\ntime periods.",
      "pdf_url": "http://arxiv.org/pdf/2503.16328v1",
      "published": "2025-03-20T16:52:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16328v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence",
      "authors": [
        "Long Yuan",
        "Fengran Mo",
        "Kaiyu Huang",
        "Wenjie Wang",
        "Wangyuxuan Zhai",
        "Xiaoyu Zhu",
        "You Li",
        "Jinan Xu",
        "Jian-Yun Nie"
      ],
      "abstract": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication.",
      "pdf_url": "http://arxiv.org/pdf/2503.16326v1",
      "published": "2025-03-20T16:45:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16326v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Structured-Noise Masked Modeling for Video, Audio and Beyond",
      "authors": [
        "Aritra Bhowmik",
        "Fida Mohammad Thoker",
        "Carlos Hinojosa",
        "Bernard Ghanem",
        "Cees G. M. Snoek"
      ],
      "abstract": "Masked modeling has emerged as a powerful self-supervised learning framework,\nbut existing methods largely rely on random masking, disregarding the\nstructural properties of different modalities. In this work, we introduce\nstructured noise-based masking, a simple yet effective approach that naturally\naligns with the spatial, temporal, and spectral characteristics of video and\naudio data. By filtering white noise into distinct color noise distributions,\nwe generate structured masks that preserve modality-specific patterns without\nrequiring handcrafted heuristics or access to the data. Our approach improves\nthe performance of masked video and audio modeling frameworks without any\ncomputational overhead. Extensive experiments demonstrate that structured noise\nmasking achieves consistent improvement over random masking for standard and\nadvanced masked modeling methods, highlighting the importance of modality-aware\nmasking strategies for representation learning.",
      "pdf_url": "http://arxiv.org/pdf/2503.16311v1",
      "published": "2025-03-20T16:34:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16311v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "title": "Speeding up design and making to reduce time-to-project and time-to-market: an AI-Enhanced approach in engineering education",
      "authors": [
        "Giovanni Adorni",
        "Daniele Grosso"
      ],
      "abstract": "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2503.16307v1",
      "published": "2025-03-20T16:32:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16307v1",
      "categories": [
        "cs.AI",
        "I.2; K.3"
      ]
    },
    {
      "title": "Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1",
      "authors": [
        "Peiran Gu",
        "Fuhao Duan",
        "Wenhao Li",
        "Bochen Xu",
        "Ying Cai",
        "Teng Yao",
        "Chenxun Zhuo",
        "Tianming Liu",
        "Bao Ge"
      ],
      "abstract": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications.",
      "pdf_url": "http://arxiv.org/pdf/2503.16304v1",
      "published": "2025-03-20T16:25:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16304v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
      "authors": [
        "Zeqiang Lai",
        "Yunfei Zhao",
        "Zibo Zhao",
        "Haolin Liu",
        "Fuyun Wang",
        "Huiwen Shi",
        "Xianghui Yang",
        "Qinxiang Lin",
        "Jinwei Huang",
        "Yuhong Liu",
        "Jie Jiang",
        "Chunchao Guo",
        "Xiangyu Yue"
      ],
      "abstract": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
      "pdf_url": "http://arxiv.org/pdf/2503.16302v1",
      "published": "2025-03-20T16:23:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16302v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "Diffusion-augmented Graph Contrastive Learning for Collaborative Filter",
      "authors": [
        "Fan Huang",
        "Wei Wang"
      ],
      "abstract": "Graph-based collaborative filtering has been established as a prominent\napproach in recommendation systems, leveraging the inherent graph topology of\nuser-item interactions to model high-order connectivity patterns and enhance\nrecommendation performance. Recent advances in Graph Contrastive Learning (GCL)\nhave demonstrated promising potential to alleviate data sparsity issues by\nimproving representation learning through contrastive view generation and\nmutual information maximization. However, existing approaches lack effective\ndata augmentation strategies. Structural augmentation risks distorting\nfundamental graph topology, while feature-level perturbation techniques\npredominantly employ uniform noise scales that fail to account for\nnode-specific characteristics. To solve these challenges, we propose\nDiffusion-augmented Contrastive Learning (DGCL), an innovative framework that\nintegrates diffusion models with contrastive learning for enhanced\ncollaborative filtering. Our approach employs a diffusion process that learns\nnode-specific Gaussian distributions of representations, thereby generating\nsemantically consistent yet diversified contrastive views through reverse\ndiffusion sampling. DGCL facilitates adaptive data augmentation based on\nreconstructed representations, considering both semantic coherence and\nnode-specific features. In addition, it explores unrepresented regions of the\nlatent sparse feature space, thereby enriching the diversity of contrastive\nviews. Extensive experimental results demonstrate the effectiveness of DGCL on\nthree public datasets.",
      "pdf_url": "http://arxiv.org/pdf/2503.16290v1",
      "published": "2025-03-20T16:15:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16290v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "AI Agents in Cryptoland: Practical Attacks and No Silver Bullet",
      "authors": [
        "Atharv Singh Patlan",
        "Peiyao Sheng",
        "S. Ashwin Hebbar",
        "Prateek Mittal",
        "Pramod Viswanath"
      ],
      "abstract": "The integration of AI agents with Web3 ecosystems harnesses their\ncomplementary potential for autonomy and openness, yet also introduces\nunderexplored security risks, as these agents dynamically interact with\nfinancial protocols and immutable smart contracts. This paper investigates the\nvulnerabilities of AI agents within blockchain-based financial ecosystems when\nexposed to adversarial threats in real-world scenarios. We introduce the\nconcept of context manipulation -- a comprehensive attack vector that exploits\nunprotected context surfaces, including input channels, memory modules, and\nexternal data feeds. Through empirical analysis of ElizaOS, a decentralized AI\nagent framework for automated Web3 operations, we demonstrate how adversaries\ncan manipulate context by injecting malicious instructions into prompts or\nhistorical interaction records, leading to unintended asset transfers and\nprotocol violations which could be financially devastating. Our findings\nindicate that prompt-based defenses are insufficient, as malicious inputs can\ncorrupt an agent's stored context, creating cascading vulnerabilities across\ninteractions and platforms. This research highlights the urgent need to develop\nAI agents that are both secure and fiduciarily responsible.",
      "pdf_url": "http://arxiv.org/pdf/2503.16248v1",
      "published": "2025-03-20T15:44:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16248v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "I.2.7"
      ]
    },
    {
      "title": "Flight Testing an Optionally Piloted Aircraft: a Case Study on Trust Dynamics in Human-Autonomy Teaming",
      "authors": [
        "Jeremy C. -H. Wang",
        "Ming Hou",
        "David Dunwoody",
        "Marko Ilievski",
        "Justin Tomasi",
        "Edward Chao",
        "Carl Pigeon"
      ],
      "abstract": "This paper examines how trust is formed, maintained, or diminished over time\nin the context of human-autonomy teaming with an optionally piloted aircraft.\nWhereas traditional factor-based trust models offer a static representation of\nhuman confidence in technology, here we discuss how variations in the\nunderlying factors lead to variations in trust, trust thresholds, and human\nbehaviours. Over 200 hours of flight test data collected over a multi-year test\ncampaign from 2021 to 2023 were reviewed. The\ndispositional-situational-learned, process-performance-purpose, and IMPACTS\nhomeostasis trust models are applied to illuminate trust trends during nominal\nautonomous flight operations. The results offer promising directions for future\nstudies on trust dynamics and design-for-trust in human-autonomy teaming.",
      "pdf_url": "http://arxiv.org/pdf/2503.16227v1",
      "published": "2025-03-20T15:22:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16227v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion",
      "authors": [
        "Qizhi Pei",
        "Lijun Wu",
        "Zhuoshi Pan",
        "Yu Li",
        "Honglin Lin",
        "Chenlin Ming",
        "Xin Gao",
        "Conghui He",
        "Rui Yan"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
      "pdf_url": "http://arxiv.org/pdf/2503.16212v1",
      "published": "2025-03-20T15:00:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16212v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Logic Explanation of AI Classifiers by Categorical Explaining Functors",
      "authors": [
        "Stefano Fioravanti",
        "Francesco Giannini",
        "Paolo Frazzetto",
        "Fabio Zanasi",
        "Pietro Barbiero"
      ],
      "abstract": "The most common methods in explainable artificial intelligence are post-hoc\ntechniques which identify the most relevant features used by pretrained opaque\nmodels. Some of the most advanced post hoc methods can generate explanations\nthat account for the mutual interactions of input features in the form of logic\nrules. However, these methods frequently fail to guarantee the consistency of\nthe extracted explanations with the model's underlying reasoning. To bridge\nthis gap, we propose a theoretically grounded approach to ensure coherence and\nfidelity of the extracted explanations, moving beyond the limitations of\ncurrent heuristic-based approaches. To this end, drawing from category theory,\nwe introduce an explaining functor which structurally preserves logical\nentailment between the explanation and the opaque model's reasoning. As a proof\nof concept, we validate the proposed theoretical constructions on a synthetic\nbenchmark verifying how the proposed approach significantly mitigates the\ngeneration of contradictory or unfaithful explanations.",
      "pdf_url": "http://arxiv.org/pdf/2503.16203v1",
      "published": "2025-03-20T14:50:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16203v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Large Language Models for Water Distribution Systems Modeling and Decision-Making",
      "authors": [
        "Yinon Goldshtein",
        "Gal Perelman",
        "Assaf Schuster",
        "Avi Ostfeld"
      ],
      "abstract": "The design, operations, and management of water distribution systems (WDS)\ninvolve complex mathematical models. These models are continually improving due\nto computational advancements, leading to better decision-making and more\nefficient WDS management. However, the significant time and effort required for\nmodeling, programming, and analyzing results remain substantial challenges.\nAnother issue is the professional burden, which confines the interaction with\nmodels, databases, and other sophisticated tools to a small group of experts,\nthereby causing non-technical stakeholders to depend on these experts or make\ndecisions without modeling support. Furthermore, explaining model results is\nchallenging even for experts, as it is often unclear which conditions cause the\nmodel to reach a certain state or recommend a specific policy. The recent\nadvancements in Large Language Models (LLMs) open doors for a new stage in\nhuman-model interaction. This study proposes a framework of plain language\ninteractions with hydraulic and water quality models based on LLM-EPANET\narchitecture. This framework is tested with increasing levels of complexity of\nqueries to study the ability of LLMs to interact with WDS models, run complex\nsimulations, and report simulation results. The performance of the proposed\nframework is evaluated across several categories of queries and hyper-parameter\nconfigurations, demonstrating its potential to enhance decision-making\nprocesses in WDS management.",
      "pdf_url": "http://arxiv.org/pdf/2503.16191v1",
      "published": "2025-03-20T14:39:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16191v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation",
      "authors": [
        "Andrea Maracani",
        "Savas Ozkan",
        "Sijun Cho",
        "Hyowon Kim",
        "Eunchung Noh",
        "Jeongwon Min",
        "Cho Jung Min",
        "Dookun Park",
        "Mete Ozay"
      ],
      "abstract": "Scaling architectures have been proven effective for improving Scene Text\nRecognition (STR), but the individual contribution of vision encoder and text\ndecoder scaling remain under-explored. In this work, we present an in-depth\nempirical analysis and demonstrate that, contrary to previous observations,\nscaling the decoder yields significant performance gains, always exceeding\nthose achieved by encoder scaling alone. We also identify label noise as a key\nchallenge in STR, particularly in real-world data, which can limit the\neffectiveness of STR models. To address this, we propose Cloze\nSelf-Distillation (CSD), a method that mitigates label noise by distilling a\nstudent model from context-aware soft predictions and pseudolabels generated by\na teacher model. Additionally, we enhance the decoder architecture by\nintroducing differential cross-attention for STR. Our methodology achieves\nstate-of-the-art performance on 10 out of 11 benchmarks using only real data,\nwhile significantly reducing the parameter size and computational costs.",
      "pdf_url": "http://arxiv.org/pdf/2503.16184v1",
      "published": "2025-03-20T14:35:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16184v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation",
      "authors": [
        "Alex-Razvan Ispas",
        "Charles-Elie Simon",
        "Fabien Caspani",
        "Vincent Guigue"
      ],
      "abstract": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment.",
      "pdf_url": "http://arxiv.org/pdf/2503.16161v1",
      "published": "2025-03-20T13:58:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16161v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "62-08",
        "I.2.7"
      ]
    },
    {
      "title": "Neural Combinatorial Optimization for Real-World Routing",
      "authors": [
        "Jiwoo Son",
        "Zhikai Zhao",
        "Federico Berto",
        "Chuanbo Hua",
        "Changhyun Kwon",
        "Jinkyoo Park"
      ],
      "abstract": "Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in\nseveral real-world logistics scenarios that pose significant challenges for\noptimization. Neural Combinatorial Optimization (NCO) has emerged as a\npromising alternative to classical approaches, as it can learn fast heuristics\nto solve VRPs. However, most research works in NCO for VRPs focus on simplified\nsettings, which do not account for asymmetric distances and travel durations\nthat cannot be derived by simple Euclidean distances and unrealistic data\ndistributions, hindering real-world deployment. This work introduces RRNCO\n(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world\nVRPs in the critical aspects of both data and modeling. First, we introduce a\nnew, openly available dataset with real-world data containing a diverse dataset\nof locations, distances, and duration matrices from 100 cities, considering\nrealistic settings with actual routing distances and durations obtained from\nOpen Source Routing Machine (OSRM). Second, we propose a novel approach that\nefficiently processes both node and edge features through contextual gating,\nenabling the construction of more informed node embedding, and we finally\nincorporate an Adaptation Attention Free Module (AAFM) with neural adaptive\nbias mechanisms that effectively integrates not only distance matrices but also\nangular relationships between nodes, allowing our model to capture rich\nstructural information. RRNCO achieves state-of-the-art results in real-world\nVRPs among NCO methods. We make our dataset and code publicly available at\nhttps://github.com/ai4co/real-routing-nco.",
      "pdf_url": "http://arxiv.org/pdf/2503.16159v1",
      "published": "2025-03-20T13:57:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16159v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Unify and Triumph: Polyglot, Diverse, and Self-Consistent Generation of Unit Tests with LLMs",
      "authors": [
        "Djamel Eddine Khelladi",
        "Charly Reux",
        "Mathieu Acher"
      ],
      "abstract": "Large language model (LLM)-based test generation has gained attention in\nsoftware engineering, yet most studies evaluate LLMs' ability to generate unit\ntests in a single attempt for a given language, missing the opportunity to\nleverage LLM diversity for more robust testing. This paper introduces PolyTest,\na novel approach that enhances test generation by exploiting polyglot and\ntemperature-controlled diversity. PolyTest systematically leverages these\nproperties in two complementary ways: (1) Cross-lingual test generation, where\ntests are generated in multiple languages at zero temperature and then unified;\n(2) Diverse test sampling, where multiple test sets are generated within the\nsame language at a higher temperature before unification. A key insight is that\nLLMs can generate diverse yet contradicting tests -- same input, different\nexpected outputs -- across languages and generations. PolyTest mitigates\ninconsistencies by unifying test sets, fostering self-consistency and improving\noverall test quality. Unlike single-language or single-attempt approaches,\nPolyTest enhances testing without requiring on-the-fly execution, making it\nparticularly beneficial for weaker-performing languages. We evaluate PolyTest\non Llama3-70B, GPT-4o, and GPT-3.5 using EvalPlus, generating tests in five\nlanguages (Java, C, Python, JavaScript, and a CSV-based format) at temperature\n0 and sampling multiple sets at temperature 1. We observe that LLMs frequently\ngenerate contradicting tests across settings, and that PolyTest significantly\nimproves test quality across all considered metrics -- number of tests, passing\nrate, statement/branch coverage (up to +9.01%), and mutation score (up to\n+11.23%). Finally, PolyTest outperforms Pynguin in test generation, passing\nrate, and mutation score.",
      "pdf_url": "http://arxiv.org/pdf/2503.16144v1",
      "published": "2025-03-20T13:47:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16144v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video Streaming",
      "authors": [
        "Liming Liu",
        "Jiangkai Wu",
        "Haoyang Wang",
        "Peiheng Wang",
        "Xinggong Zhang",
        "Zongming Guo"
      ],
      "abstract": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
      "pdf_url": "http://arxiv.org/pdf/2503.16112v1",
      "published": "2025-03-20T13:00:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16112v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "AIMI: Leveraging Future Knowledge and Personalization in Sparse Event Forecasting for Treatment Adherence",
      "authors": [
        "Abdullah Mamun",
        "Diane J. Cook",
        "Hassan Ghasemzadeh"
      ],
      "abstract": "Adherence to prescribed treatments is crucial for individuals with chronic\nconditions to avoid costly or adverse health outcomes. For certain patient\ngroups, intensive lifestyle interventions are vital for enhancing medication\nadherence. Accurate forecasting of treatment adherence can open pathways to\ndeveloping an on-demand intervention tool, enabling timely and personalized\nsupport. With the increasing popularity of smartphones and wearables, it is now\neasier than ever to develop and deploy smart activity monitoring systems.\nHowever, effective forecasting systems for treatment adherence based on\nwearable sensors are still not widely available. We close this gap by proposing\nAdherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI\nis a knowledge-guided adherence forecasting system that leverages smartphone\nsensors and previous medication history to estimate the likelihood of\nforgetting to take a prescribed medication. A user study was conducted with 27\nparticipants who took daily medications to manage their cardiovascular\ndiseases. We designed and developed CNN and LSTM-based forecasting models with\nvarious combinations of input features and found that LSTM models can forecast\nmedication adherence with an accuracy of 0.932 and an F-1 score of 0.936.\nMoreover, through a series of ablation studies involving convolutional and\nrecurrent neural network architectures, we demonstrate that leveraging known\nknowledge about future and personalized training enhances the accuracy of\nmedication adherence forecasting. Code available:\nhttps://github.com/ab9mamun/AIMI.",
      "pdf_url": "http://arxiv.org/pdf/2503.16091v1",
      "published": "2025-03-20T12:32:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16091v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Allostatic Control of Persistent States in Spiking Neural Networks for perception and computation",
      "authors": [
        "Aung Htet",
        "Alejandro Rodriguez Jimenez",
        "Sarah Hamburg",
        "Alessandro Di Nuovo"
      ],
      "abstract": "We introduce a novel model for updating perceptual beliefs about the\nenvironment by extending the concept of Allostasis to the control of internal\nrepresentations. Allostasis is a fundamental regulatory mechanism observed in\nanimal physiology that orchestrates responses to maintain a dynamic equilibrium\nin bodily needs and internal states. In this paper, we focus on an application\nin numerical cognition, where a bump of activity in an attractor network is\nused as a spatial numerical representation. While existing neural networks can\nmaintain persistent states, to date, there is no unified framework for\ndynamically controlling spatial changes in neuronal activity in response to\nenvironmental changes. To address this, we couple a well known allostatic\nmicrocircuit, the Hammel model, with a ring attractor, resulting in a Spiking\nNeural Network architecture that can modulate the location of the bump as a\nfunction of some reference input. This localized activity in turn is used as a\nperceptual belief in a simulated subitization task a quick enumeration process\nwithout counting. We provide a general procedure to fine-tune the model and\ndemonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare it\nwith biological data. Finally, we analyze the dynamics of the network to\nunderstand the selectivity and specificity of different neurons to distinct\ncategories present in the input. The results of this paper, particularly the\nmechanism for moving persistent states, are not limited to numerical cognition\nbut can be applied to a wide range of tasks involving similar representations.",
      "pdf_url": "http://arxiv.org/pdf/2503.16085v1",
      "published": "2025-03-20T12:28:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16085v1",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ]
    },
    {
      "title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge",
      "authors": [
        "Marek Wodzinski",
        "Henning Müller"
      ],
      "abstract": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively.",
      "pdf_url": "http://arxiv.org/pdf/2503.16075v1",
      "published": "2025-03-20T12:12:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16075v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection",
      "authors": [
        "Sergey Berezin",
        "Reza Farahbakhsh",
        "Noel Crespi"
      ],
      "abstract": "The fundamental problem of toxicity detection lies in the fact that the term\n\"toxicity\" is ill-defined. Such uncertainty causes researchers to rely on\nsubjective and vague data during model training, which leads to non-robust and\ninaccurate results, following the 'garbage in - garbage out' paradigm. This\nstudy introduces a novel, objective, and context-aware framework for toxicity\ndetection, leveraging stress levels as a key determinant of toxicity. We\npropose new definition, metric and training approach as a parts of our\nframework and demonstrate it's effectiveness using a dataset we collected.",
      "pdf_url": "http://arxiv.org/pdf/2503.16072v1",
      "published": "2025-03-20T12:09:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16072v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
      "authors": [
        "Jiale Wei",
        "Shuchi Wu",
        "Ruochen Liu",
        "Xiang Ying",
        "Jingbo Shang",
        "Fangbo Tao"
      ],
      "abstract": "Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types.",
      "pdf_url": "http://arxiv.org/pdf/2503.16071v1",
      "published": "2025-03-20T12:04:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16071v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval",
      "authors": [
        "Qiang Zou",
        "Shuli Cheng",
        "Jiayi Chen"
      ],
      "abstract": "Cross-modal hashing is a promising approach for efficient data retrieval and\nstorage optimization. However, contemporary methods exhibit significant\nlimitations in semantic preservation, contextual integrity, and information\nredundancy, which constrains retrieval efficacy. We present PromptHash, an\ninnovative framework leveraging affinity prompt-aware collaborative learning\nfor adaptive cross-modal hashing. We propose an end-to-end framework for\naffinity-prompted collaborative hashing, with the following fundamental\ntechnical contributions: (i) a text affinity prompt learning mechanism that\npreserves contextual information while maintaining parameter efficiency, (ii)\nan adaptive gated selection fusion architecture that synthesizes State Space\nModel with Transformer network for precise cross-modal feature integration, and\n(iii) a prompt affinity alignment strategy that bridges modal heterogeneity\nthrough hierarchical contrastive learning. To the best of our knowledge, this\nstudy presents the first investigation into affinity prompt awareness within\ncollaborative cross-modal adaptive hash learning, establishing a paradigm for\nenhanced semantic consistency across modalities. Through comprehensive\nevaluation on three benchmark multi-label datasets, PromptHash demonstrates\nsubstantial performance improvements over existing approaches. Notably, on the\nNUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in\nimage-to-text and text-to-image retrieval tasks, respectively. The code is\npublicly available at https://github.com/ShiShuMo/PromptHash.",
      "pdf_url": "http://arxiv.org/pdf/2503.16064v1",
      "published": "2025-03-20T11:56:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16064v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.MM"
      ]
    },
    {
      "title": "Two-stage Incomplete Utterance Rewriting on Editing Operation",
      "authors": [
        "Zhiyu Cao",
        "Peifeng Li",
        "Qiaoming Zhu",
        "Yaxin Fan"
      ],
      "abstract": "Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly.",
      "pdf_url": "http://arxiv.org/pdf/2503.16063v1",
      "published": "2025-03-20T11:56:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16063v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts",
      "authors": [
        "Yike Yuan",
        "Ziyu Wang",
        "Zihao Huang",
        "Defa Zhu",
        "Xun Zhou",
        "Jingyi Yu",
        "Qiyang Min"
      ],
      "abstract": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.",
      "pdf_url": "http://arxiv.org/pdf/2503.16057v1",
      "published": "2025-03-20T11:45:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16057v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic",
      "authors": [
        "Bisola Faith Kayode",
        "Akinyemi Sadeeq Akintola",
        "Oluwole Fagbohun",
        "Egonna Anaesiuba-Bristol",
        "Onyekachukwu Ojumah",
        "Oluwagbade Odimayo",
        "Toyese Oloyede",
        "Aniema Inyang",
        "Teslim Kazeem",
        "Habeeb Alli",
        "Udodirim Ibem Offia",
        "Prisca Chinazor Amajuoyi"
      ],
      "abstract": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications.",
      "pdf_url": "http://arxiv.org/pdf/2503.16047v1",
      "published": "2025-03-20T11:31:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16047v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Open Science and Artificial Intelligence for supporting the sustainability of the SRC Network: The espSRC case",
      "authors": [
        "J. Garrido",
        "S. Sánchez-Expósito",
        "A. Ruiz-Falcó",
        "J. Ruedas",
        "M. Á. Mendoza",
        "V. Vázquez",
        "M. Parra",
        "J. Sánchez",
        "I. Labadie",
        "L. Darriba",
        "J. Moldón",
        "M. Rodriguez-Álvarez",
        "J. Díaz",
        "L. Verdes-Montenegro"
      ],
      "abstract": "The SKA Observatory (SKAO), a landmark project in radio astronomy, seeks to\naddress fundamental questions in astronomy. To process its immense data output,\napproximately 700 PB/year, a global network of SKA Regional Centres (SR-CNet)\nwill provide the infrastructure, tools, computational power needed for\nscientific analysis and scientific support. The Spanish SRC (espSRC) focuses on\nensuring the sustainability of this network by reducing its environmental\nimpact, integrating green practices into data platforms, and developing Open\nScience technologies to enable reproducible research. This paper discusses and\nsummarizes part of the research and development activities that the team is\nconducting to reduce the SRC energy consumption at the espSRC and SRCNet. The\npaper also discusses fundamental research on trusted repositories to support\nOpen Science practices.",
      "pdf_url": "http://arxiv.org/pdf/2503.16045v1",
      "published": "2025-03-20T11:29:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16045v1",
      "categories": [
        "astro-ph.IM",
        "cs.AI"
      ]
    },
    {
      "title": "Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation",
      "authors": [
        "Zhiyu Cao",
        "Peifeng Li",
        "Yaxin Fan",
        "Qiaoming Zhu"
      ],
      "abstract": "Although existing fashionable generation methods on Incomplete Utterance\nRewriting (IUR) can generate coherent utterances, they often result in the\ninclusion of irrelevant and redundant tokens in rewritten utterances due to\ntheir inability to focus on critical tokens in dialogue context. Furthermore,\nthe limited size of the training datasets also contributes to the insufficient\ntraining of the IUR model. To address the first issue, we propose a multi-task\nlearning framework EO-IUR (Editing Operation-guided Incomplete Utterance\nRewriting) that introduces the editing operation labels generated by sequence\nlabeling module to guide generation model to focus on critical tokens.\nFurthermore, we introduce a token-level heterogeneous graph to represent\ndialogues. To address the second issue, we propose a two-dimensional utterance\naugmentation strategy, namely editing operation-based incomplete utterance\naugmentation and LLM-based historical utterance augmentation. The experimental\nresults on three datasets demonstrate that our EO-IUR outperforms previous\nstate-of-the-art (SOTA) baselines in both open-domain and task-oriented\ndialogue. The code will be available at https://github.com/Dewset/EO-IUR.",
      "pdf_url": "http://arxiv.org/pdf/2503.16043v1",
      "published": "2025-03-20T11:26:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16043v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis and Automated Report Generation",
      "authors": [
        "Bisola Faith Kayode",
        "Akinyemi Sadeeq Akintola",
        "Oluwole Fagbohun",
        "Egonna Anaesiuba-Bristol",
        "Onyekachukwu Ojumah",
        "Oluwagbade Odimayo",
        "Toyese Oloyede",
        "Aniema Inyang",
        "Teslim Kazeem",
        "Habeeb Alli",
        "Udodirim Ibem Offia",
        "Prisca Chinazor Amajuoyi"
      ],
      "abstract": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes",
      "pdf_url": "http://arxiv.org/pdf/2503.16041v1",
      "published": "2025-03-20T11:19:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16041v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models",
      "authors": [
        "Zhihang Liu",
        "Chen-Wei Xie",
        "Pandeng Li",
        "Liming Zhao",
        "Longxiang Tang",
        "Yun Zheng",
        "Chuanbin Liu",
        "Hongtao Xie"
      ],
      "abstract": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom.",
      "pdf_url": "http://arxiv.org/pdf/2503.16036v1",
      "published": "2025-03-20T11:09:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16036v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Single Image Iterative Subject-driven Generation and Editing",
      "authors": [
        "Yair Shpitzer",
        "Gal Chechik",
        "Idan Schwartz"
      ],
      "abstract": "Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.",
      "pdf_url": "http://arxiv.org/pdf/2503.16025v1",
      "published": "2025-03-20T10:45:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16025v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement",
      "authors": [
        "Ruihan Yang",
        "Fanghua Ye",
        "Jian Li",
        "Siyu Yuan",
        "Yikai Zhang",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Deqing Yang"
      ],
      "abstract": "Large language models (LLMs) have recently transformed from text-based\nassistants to autonomous agents capable of planning, reasoning, and iteratively\nimproving their actions. While numerical reward signals and verifiers can\neffectively rank candidate actions, they often provide limited contextual\nguidance. In contrast, natural language feedback better aligns with the\ngenerative capabilities of LLMs, providing richer and more actionable\nsuggestions. However, parsing and implementing this feedback effectively can be\nchallenging for LLM-based agents. In this work, we introduce Critique-Guided\nImprovement (CGI), a novel two-player framework, comprising an actor model that\nexplores an environment and a critic model that generates detailed nature\nlanguage feedback. By training the critic to produce fine-grained assessments\nand actionable revisions, and the actor to utilize these critiques, our\napproach promotes more robust exploration of alternative strategies while\navoiding local optima. Experiments in three interactive environments show that\nCGI outperforms existing baselines by a substantial margin. Notably, even a\nsmall critic model surpasses GPT-4 in feedback quality. The resulting actor\nachieves state-of-the-art performance, demonstrating the power of explicit\niterative guidance to enhance decision-making in LLM-based agents.",
      "pdf_url": "http://arxiv.org/pdf/2503.16024v1",
      "published": "2025-03-20T10:42:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16024v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Autonomous AI imitators increase diversity in homogeneous information ecosystems",
      "authors": [
        "Emil Bakkensen Johansen",
        "Oliver Baumann"
      ],
      "abstract": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's potential\nimpact on the diversity and democratic value of information ecosystems. Here,\nwe introduce a large-scale simulation framework to examine AI-based imitation\nin news, a context critically influential for public discourse. By\nsystematically testing two distinct imitation strategies across a range of\ninformation environments varying in initial diversity, we demonstrate that\nAI-generated articles do not uniformly homogenize content. Instead, AI's\ninfluence is strongly context-dependent: AI-generated articles can introduce\nvaluable diversity in originally homogeneous news environments, while\npotentially diminishing diversity in contexts that initially display high\nheterogeneity. These results illustrate that the baseline diversity of an\ninformation space critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens information diversity. Instead, when\ninformation is initially homogeneous, AI-driven imitation can expand\nperspectives, styles, and topics. This is especially important in news\ncontexts, where information diversity fosters richer public debate by exposing\ncitizens to alternative viewpoints, challenging biases, and preventing\nnarrative monopolies, which is essential for a resilient democracy.",
      "pdf_url": "http://arxiv.org/pdf/2503.16021v1",
      "published": "2025-03-20T10:37:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.16021v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "J.4"
      ]
    }
  ]
}