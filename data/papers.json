{
  "last_updated": "2026-02-10T01:20:47.250012",
  "papers": [
    {
      "title": "Learning a Generative Meta-Model of LLM Activations",
      "authors": [
        "Grace Luo",
        "Jiahai Feng",
        "Trevor Darrell",
        "Alec Radford",
        "Jacob Steinhardt"
      ],
      "abstract": "Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating \"meta-models\" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.",
      "pdf_url": "https://arxiv.org/pdf/2602.06964v1",
      "published": "2026-02-06T18:59:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06964v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
      "authors": [
        "Yuchen Yan",
        "Liang Jiang",
        "Jin Jiang",
        "Shuaicheng Li",
        "Zujie Wen",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Jian Shao",
        "Yueting Zhuang",
        "Yongliang Shen"
      ],
      "abstract": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
      "pdf_url": "https://arxiv.org/pdf/2602.06960v1",
      "published": "2026-02-06T18:59:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06960v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
      "authors": [
        "Shenyuan Gao",
        "William Liang",
        "Kaiyuan Zheng",
        "Ayaan Malik",
        "Seonghyeon Ye",
        "Sihyun Yu",
        "Wei-Cheng Tseng",
        "Yuzhu Dong",
        "Kaichun Mo",
        "Chen-Hsuan Lin",
        "Qianli Ma",
        "Seungjun Nah",
        "Loic Magne",
        "Jiannan Xiang",
        "Yuqi Xie",
        "Ruijie Zheng",
        "Dantong Niu",
        "You Liang Tan",
        "K. R. Zentner",
        "George Kurian",
        "Suneel Indupuru",
        "Pooya Jannaty",
        "Jinwei Gu",
        "Jun Zhang",
        "Jitendra Malik",
        "Pieter Abbeel",
        "Ming-Yu Liu",
        "Yuke Zhu",
        "Joel Jang",
        "Linxi \"Jim\" Fan"
      ],
      "abstract": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.",
      "pdf_url": "https://arxiv.org/pdf/2602.06949v1",
      "published": "2026-02-06T18:49:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06949v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Agentic Uncertainty Reveals Agentic Overconfidence",
      "authors": [
        "Jean Kaddour",
        "Srijan Patel",
        "Gbètondji Dovonon",
        "Leo Richter",
        "Pasquale Minervini",
        "Matt J. Kusner"
      ],
      "abstract": "Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.",
      "pdf_url": "https://arxiv.org/pdf/2602.06948v1",
      "published": "2026-02-06T18:49:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06948v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay",
      "authors": [
        "Duygu Altinok"
      ],
      "abstract": "Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a \"subwords manifest\", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this \"subwords manifest\" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.",
      "pdf_url": "https://arxiv.org/pdf/2602.06942v1",
      "published": "2026-02-06T18:41:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06942v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Endogenous Resistance to Activation Steering in Language Models",
      "authors": [
        "Alex McKenzie",
        "Keenan Pepper",
        "Stijn Servaes",
        "Martin Leitgab",
        "Murat Cubuktepe",
        "Mike Vaiana",
        "Diogo de Lucena",
        "Judd Rosenblatt",
        "Michael S. A. Graziano"
      ],
      "abstract": "Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.",
      "pdf_url": "https://arxiv.org/pdf/2602.06941v1",
      "published": "2026-02-06T18:41:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06941v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics",
      "authors": [
        "Zuyuan Zhang",
        "Sizhe Tang",
        "Tian Lan"
      ],
      "abstract": "Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often focus on practical algorithm designs and offer limited theoretical treatment to address key questions, such as what dynamics are indeed capturable by the Bellman framework and how to inspire new algorithm classes with optimal approximations. In this paper, we present a novel topological viewpoint on temporal-difference (TD) based RL. We show that TD errors can be viewed as 1-cochain in the topological space of state transitions, while Markov dynamics are then interpreted as topological integrability. This novel view enables us to obtain a Hodge-type decomposition of TD errors into an integrable component and a topological residual, through a Bellman-de Rham projection. We further propose HodgeFlow Policy Search (HFPS) by fitting a potential network to minimize the non-integrable projection residual in RL, achieving stability/sensitivity guarantees. In numerical evaluations, HFPS is shown to significantly improve RL performance under non-Markovian.",
      "pdf_url": "https://arxiv.org/pdf/2602.06939v1",
      "published": "2026-02-06T18:35:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06939v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI",
      "authors": [
        "Ehud Shapiro"
      ],
      "abstract": "Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \\emph{readers} and \\emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers and/or writers, enabling the concise expression of rich multidirectional communication modalities.\n  GLP was designed as a language for grassroots platforms -- distributed systems with multiple instances that can operate independently of each other and of any global resource, and can coalesce into ever larger instances -- with its target architecture being smartphones communicating peer-to-peer. The operational semantics of Concurrent (single-agent) GLP and of multiagent GLP (maGLP) were defined via transition systems/multiagent transition systems, respectively.\n  Here, we describe the mathematics developed to facilitate the workstation- and smartphone-based implementations of GLP by AI in Dart. We developed dGLP -- implementation-ready deterministic operational semantics for single-agent GLP -- and proved it correct with respect to the Concurrent GLP operational semantics; dGLP was used by AI as a formal spec, from which it developed a workstation-based implementation of GLP. We developed madGLP -- an implementation-ready multiagent operational semantics for maGLP -- and proved it correct with respect to the maGLP operational semantics; madGLP is deterministic at the agent level (not at the system level due to communication asynchrony), and is being used by AI as a formal spec from which it develops a smartphone-based implementation of maGLP.",
      "pdf_url": "https://arxiv.org/pdf/2602.06934v1",
      "published": "2026-02-06T18:30:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06934v1",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.DC",
        "cs.LO",
        "cs.MA"
      ]
    },
    {
      "title": "From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers",
      "authors": [
        "Ziming Liu",
        "Sophia Sanborn",
        "Surya Ganguli",
        "Andreas Tolias"
      ],
      "abstract": "Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on \"world models\" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous \"AI Physicist\" approaches have successfully recovered such laws, they typically rely on strong, domain-specific priors that effectively \"bake in\" the physics. Conversely, Vafa et al. recently showed that generic Transformers fail to acquire these world models, achieving high predictive accuracy without capturing the underlying physical laws. We bridge this gap by systematically introducing three minimal inductive biases. We show that ensuring spatial smoothness (by formulating prediction as continuous regression) and stability (by training with noisy contexts to mitigate error accumulation) enables generic Transformers to surpass prior failures and learn a coherent Keplerian world model, successfully fitting ellipses to planetary trajectories. However, true physical insight requires a third bias: temporal locality. By restricting the attention window to the immediate past -- imposing the simple assumption that future states depend only on the local state rather than a complex history -- we force the model to abandon curve-fitting and discover Newtonian force representations. Our results demonstrate that simple architectural choices determine whether an AI becomes a curve-fitter or a physicist, marking a critical step toward automated scientific discovery.",
      "pdf_url": "https://arxiv.org/pdf/2602.06923v1",
      "published": "2026-02-06T18:17:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06923v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.class-ph"
      ]
    },
    {
      "title": "Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs",
      "authors": [
        "Samir Abdaljalil",
        "Parichit Sharma",
        "Erchin Serpedin",
        "Hasan Kurban"
      ],
      "abstract": "Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.",
      "pdf_url": "https://arxiv.org/pdf/2602.06920v1",
      "published": "2026-02-06T18:16:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06920v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "PANC: Prior-Aware Normalized Cut for Object Segmentation",
      "authors": [
        "Juan Gutiérrez",
        "Victor Gutiérrez-Garcia",
        "José Luis Blanco-Murillo"
      ],
      "abstract": "Fully unsupervised segmentation pipelines naively seek the most salient object, should this be present. As a result, most of the methods reported in the literature deliver non-deterministic partitions that are sensitive to initialization, seed order, and threshold heuristics.\n  We propose PANC, a weakly supervised spectral segmentation framework that uses a minimal set of annotated visual tokens to produce stable, controllable, and reproducible object masks. From the TokenCut approach, we augment the token-token affinity graph with a handful of priors coupled to anchor nodes. By manipulating the graph topology, we bias the spectral eigenspace toward partitions that are consistent with the annotations. Our approach preserves the global grouping enforced by dense self-supervised visual features, trading annotated tokens for significant gains in reproducibility, user control, and segmentation quality.\n  Using 5 to 30 annotations per dataset, our training-free method achieves state-of-the-art performance among weakly and unsupervised approaches on standard benchmarks (e.g., DUTS-TE, ECSSD, MS COCO). Contrarily, it excels in domains where dense labels are costly or intra-class differences are subtle. We report strong and reliable results on homogeneous, fine-grained, and texture-limited domains, achieving 96.8% (+14.43% over SotA), 78.0% (+0.2%), and 78.8% (+0.37%) average mean intersection-over-union (mIoU) on CrackForest (CFD), CUB-200-2011, and HAM10000 datasets, respectively. For multi-object benchmarks, the framework showcases explicit, user-controllable semantic segmentation.",
      "pdf_url": "https://arxiv.org/pdf/2602.06912v1",
      "published": "2026-02-06T18:07:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06912v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering",
      "authors": [
        "Saad Hossain",
        "Tom Tseng",
        "Punya Syon Pandey",
        "Samanvay Vajpayee",
        "Matthew Kowal",
        "Nayeema Nonta",
        "Samuel Simko",
        "Stephen Casper",
        "Zhijing Jin",
        "Kellin Pelrine",
        "Sirisha Rambhatla"
      ],
      "abstract": "As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench",
      "pdf_url": "https://arxiv.org/pdf/2602.06911v1",
      "published": "2026-02-06T18:04:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06911v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Supercharging Simulation-Based Inference for Bayesian Optimal Experimental Design",
      "authors": [
        "Samuel Klein",
        "Willie Neiswanger",
        "Daniel Ratner",
        "Michael Kagan",
        "Sean Gasiorowski"
      ],
      "abstract": "Bayesian optimal experimental design (BOED) seeks to maximize the expected information gain (EIG) of experiments. This requires a likelihood estimate, which in many settings is intractable. Simulation-based inference (SBI) provides powerful tools for this regime. However, existing work explicitly connecting SBI and BOED is restricted to a single contrastive EIG bound. We show that the EIG admits multiple formulations which can directly leverage modern SBI density estimators, encompassing neural posterior, likelihood, and ratio estimation. Building on this perspective, we define a novel EIG estimator using neural likelihood estimation. Further, we identify optimization as a key bottleneck of gradient based EIG maximization and show that a simple multi-start parallel gradient ascent procedure can substantially improve reliability and performance. With these innovations, our SBI-based BOED methods are able to match or outperform by up to $22\\%$ existing state-of-the-art approaches across standard BOED benchmarks.",
      "pdf_url": "https://arxiv.org/pdf/2602.06900v1",
      "published": "2026-02-06T17:50:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06900v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "cs.NE",
        "stat.ML"
      ]
    },
    {
      "title": "NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices",
      "authors": [
        "Ruchika Chavhan",
        "Malcolm Chadwick",
        "Alberto Gil Couto Pimentel Ramos",
        "Luca Morreale",
        "Mehdi Noroozi",
        "Abhinav Mehrotra"
      ],
      "abstract": "While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.",
      "pdf_url": "https://arxiv.org/pdf/2602.06879v1",
      "published": "2026-02-06T17:05:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06879v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
      "authors": [
        "Jiangping Huang",
        "Wenguang Ye",
        "Weisong Sun",
        "Jian Zhang",
        "Mingyue Zhang",
        "Yang Liu"
      ],
      "abstract": "Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.",
      "pdf_url": "https://arxiv.org/pdf/2602.06875v1",
      "published": "2026-02-06T16:59:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06875v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Git for Sketches: An Intelligent Tracking System for Capturing Design Evolution",
      "authors": [
        "Sankar B",
        "Amogh A S",
        "Sandhya Baranwal",
        "Dibakar Sen"
      ],
      "abstract": "During product conceptualization, capturing the non-linear history and cognitive intent is crucial. Traditional sketching tools often lose this context. We introduce DIMES (Design Idea Management and Evolution capture System), a web-based environment featuring sGIT (SketchGit), a custom visual version control architecture, and Generative AI. sGIT includes AEGIS, a module using hybrid Deep Learning and Machine Learning models to classify six stroke types. The system maps Git primitives to design actions, enabling implicit branching and multi-modal commits (stroke data + voice intent). In a comparative study, experts using DIMES demonstrated a 160% increase in breadth of concept exploration. Generative AI modules generated narrative summaries that enhanced knowledge transfer; novices achieved higher replication fidelity (Neural Transparency-based Cosine Similarity: 0.97 vs. 0.73) compared to manual summaries. AI-generated renderings also received higher user acceptance (Purchase Likelihood: 4.2 vs 3.1). This work demonstrates that intelligent version control bridges creative action and cognitive documentation, offering a new paradigm for design education.",
      "pdf_url": "https://arxiv.org/pdf/2602.06047v1",
      "published": "2026-02-06T16:52:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06047v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts",
      "authors": [
        "Xinyu Zhao",
        "Qingyun Sun",
        "Jiayi Luo",
        "Xingcheng Fu",
        "Jianxin Li"
      ],
      "abstract": "Graph Anomaly Detection (GAD) aims to identify irregular patterns in graph data, and recent works have explored zero-shot generalist GAD to enable generalization to unseen graph datasets. However, existing zero-shot GAD methods largely ignore intrinsic geometric differences across diverse anomaly patterns, substantially limiting their cross-domain generalization. In this work, we reveal that anomaly detectability is highly dependent on the underlying geometric properties and that embedding graphs from different domains into a single static curvature space can distort the structural signatures of anomalies. To address the challenge that a single curvature space cannot capture geometry-dependent graph anomaly patterns, we propose GAD-MoRE, a novel framework for zero-shot Generalizable Graph Anomaly Detection with a Mixture of Riemannian Experts architecture. Specifically, to ensure that each anomaly pattern is modeled in the Riemannian space where it is most detectable, GAD-MoRE employs a set of specialized Riemannian expert networks, each operating in a distinct curvature space. To align raw node features with curvature-specific anomaly characteristics, we introduce an anomaly-aware multi-curvature feature alignment module that projects inputs into parallel Riemannian spaces, enabling the capture of diverse geometric characteristics. Finally, to facilitate better generalization beyond seen patterns, we design a memory-based dynamic router that adaptively assigns each input to the most compatible expert based on historical reconstruction performance on similar anomalies. Extensive experiments in the zero-shot setting demonstrate that GAD-MoRE significantly outperforms state-of-the-art generalist GAD baselines, and even surpasses strong competitors that are few-shot fine-tuned with labeled data from the target domain.",
      "pdf_url": "https://arxiv.org/pdf/2602.06859v1",
      "published": "2026-02-06T16:46:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06859v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
      "authors": [
        "Alisia Lupidi",
        "Bhavul Gauri",
        "Thomas Simon Foster",
        "Bassel Al Omari",
        "Despoina Magka",
        "Alberto Pepe",
        "Alexis Audran-Reiss",
        "Muna Aghamelu",
        "Nicolas Baldwin",
        "Lucia Cipolina-Kun",
        "Jean-Christophe Gagnon-Audet",
        "Chee Hau Leow",
        "Sandra Lefdal",
        "Hossam Mossalam",
        "Abhinav Moudgil",
        "Saba Nazir",
        "Emanuel Tewolde",
        "Isabel Urrego",
        "Jordi Armengol Estape",
        "Amar Budhiraja",
        "Gaurav Chaurasia",
        "Abhishek Charnalia",
        "Derek Dunfield",
        "Karen Hambardzumyan",
        "Daniel Izcovich",
        "Martin Josifoski",
        "Ishita Mediratta",
        "Kelvin Niu",
        "Parth Pathak",
        "Michael Shvartsman",
        "Edan Toledo",
        "Anton Protopopov",
        "Roberta Raileanu",
        "Alexander Miller",
        "Tatiana Shavrina",
        "Jakob Foerster",
        "Yoram Bachrach"
      ],
      "abstract": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
      "pdf_url": "https://arxiv.org/pdf/2602.06855v1",
      "published": "2026-02-06T16:45:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06855v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models",
      "authors": [
        "Jonathan Pan"
      ],
      "abstract": "Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical framework designed to characterize factual recall circuits. We implement a modular pipeline that first localizes critical layers using classical causal tracing, then maps specific attention head activations into an exponentially large quantum Hilbert space. Using open-weight models (Meta Llama-3.2-1B and Alibaba Qwen2.5-1.5B-Instruct), we perform a two-stage analysis that reveals a fundamental architectural divergence. While Qwen's layer 7 circuit functions as a classic Recall Hub, we discover that Llama's layer 9 acts as an Interference Suppression circuit, where ablating the identified heads paradoxically improves factual recall. Our results demonstrate that quantum kernels can distinguish between these constructive (recall) and reductive (suppression) mechanisms, offering a high-resolution tool for analyzing the fine-grained topology of attention.",
      "pdf_url": "https://arxiv.org/pdf/2602.06852v1",
      "published": "2026-02-06T16:40:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06852v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping",
      "authors": [
        "Chao Zhou",
        "Tianyi Wei",
        "Yiling Chen",
        "Wenbo Zhou",
        "Nenghai Yu"
      ],
      "abstract": "While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\\times$ inference speedup and a 5.1$\\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.",
      "pdf_url": "https://arxiv.org/pdf/2602.06850v1",
      "published": "2026-02-06T16:39:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06850v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "The Representational Geometry of Number",
      "authors": [
        "Zhimin Hu",
        "Lanhao Niu",
        "Sashank Varma"
      ],
      "abstract": "A central question in cognitive science is whether conceptual representations converge onto a shared manifold to support generalization, or diverge into orthogonal subspaces to minimize task interference. While prior work has discovered evidence for both, a mechanistic account of how these properties coexist and transform across tasks remains elusive. We propose that representational sharing lies not in the concepts themselves, but in the geometric relations between them. Using number concepts as a testbed and language models as high-dimensional computational substrates, we show that number representations preserve a stable relational structure across tasks. Task-specific representations are embedded in distinct subspaces, with low-level features like magnitude and parity encoded along separable linear directions. Crucially, we find that these subspaces are largely transformable into one another via linear mappings, indicating that representations share relational structure despite being located in distinct subspaces. Together, these results provide a mechanistic lens of how language models balance the shared structure of number representation with functional flexibility. It suggests that understanding arises when task-specific transformations are applied to a shared underlying relational structure of conceptual representations.",
      "pdf_url": "https://arxiv.org/pdf/2602.06843v1",
      "published": "2026-02-06T16:35:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06843v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From Features to Actions: Explainability in Traditional and Agentic AI Systems",
      "authors": [
        "Sindhuja Chaduvula",
        "Jessee Ho",
        "Kina Kim",
        "Aravind Narayanan",
        "Mahshid Alinoori",
        "Muskan Garg",
        "Dhanesh Ramachandram",
        "Shaina Raza"
      ],
      "abstract": "Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\\times$ more prevalent in failed runs and reduces success probability by 49\\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.\n  Resources:\n  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework",
      "pdf_url": "https://arxiv.org/pdf/2602.06841v1",
      "published": "2026-02-06T16:34:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06841v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization",
      "authors": [
        "Jin Wang",
        "Hui Ma",
        "Fei Xing",
        "Ming Yan"
      ],
      "abstract": "Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.",
      "pdf_url": "https://arxiv.org/pdf/2602.06838v1",
      "published": "2026-02-06T16:27:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06838v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LLM Active Alignment: A Nash Equilibrium Perspective",
      "authors": [
        "Tonghan Wang",
        "Yuqi Pan",
        "Xinyi Yang",
        "Yanchen Jiang",
        "Milind Tambe",
        "David C. Parkes"
      ],
      "abstract": "We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.",
      "pdf_url": "https://arxiv.org/pdf/2602.06836v1",
      "published": "2026-02-06T16:26:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06836v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models",
      "authors": [
        "Yuming Li",
        "Qingyu Li",
        "Chengyu Bai",
        "Xiangyang Luo",
        "Zeyue Xue",
        "Wenyu Qin",
        "Meng Wang",
        "Yikai Wang",
        "Shanghang Zhang"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations in sample learning value as well as the dynamic nature of critical exploration moments.\n  To address this issue, we conduct a detailed analysis of the internal attention dynamics during GRPO training and uncover a key insight: attention entropy can serve as a powerful dual-signal proxy. First, across different samples, the relative change in attention entropy (ΔEntropy), which reflects the divergence between the current policy and the base policy, acts as a robust indicator of sample learning value. Second, during the denoising process, the peaks of absolute attention entropy (Entropy(t)), which quantify attention dispersion, effectively identify critical timesteps where high-value exploration occurs.\n  Building on this observation, we propose Adaptive Entropy-Guided Policy Optimization (AEGPO), a novel dual-signal, dual-level adaptive optimization strategy. At the global level, AEGPO uses ΔEntropy to dynamically allocate rollout budgets, prioritizing prompts with higher learning value. At the local level, it exploits the peaks of Entropy(t) to guide exploration selectively at critical high-dispersion timesteps rather than uniformly across all denoising steps.\n  By focusing computation on the most informative samples and the most critical moments, AEGPO enables more efficient and effective policy optimization. Experiments on text-to-image generation tasks demonstrate that AEGPO significantly accelerates convergence and achieves superior alignment performance compared to standard GRPO variants.",
      "pdf_url": "https://arxiv.org/pdf/2602.06825v1",
      "published": "2026-02-06T16:09:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06825v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "AI-Generated Music Detection in Broadcast Monitoring",
      "authors": [
        "David Lopez-Ayala",
        "Asier Cabello",
        "Pablo Zinemanas",
        "Emilio Molina",
        "Martin Rocamora"
      ],
      "abstract": "AI music generators have advanced to the point where their outputs are often indistinguishable from human compositions. While detection methods have emerged, they are typically designed and validated in music streaming contexts with clean, full-length tracks. Broadcast audio, however, poses a different challenge: music appears as short excerpts, often masked by dominant speech, conditions under which existing detectors fail. In this work, we introduce AI-OpenBMAT, the first dataset tailored to broadcast-style AI-music detection. It contains 3,294 one-minute audio excerpts (54.9 hours) that follow the duration patterns and loudness relations of real television audio, combining human-made production music with stylistically matched continuations generated with Suno v3.5. We benchmark a CNN baseline and state-of-the-art SpectTTTra models to assess SNR and duration robustness, and evaluate on a full broadcast scenario. Across all settings, models that excel in streaming scenarios suffer substantial degradation, with F1-scores dropping below 60% when music is in the background or has a short duration. These results highlight speech masking and short music length as critical open challenges for AI music detection, and position AI-OpenBMAT as a benchmark for developing detectors capable of meeting industrial broadcast requirements.",
      "pdf_url": "https://arxiv.org/pdf/2602.06823v1",
      "published": "2026-02-06T16:08:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06823v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ]
    },
    {
      "title": "POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models",
      "authors": [
        "Yi Chen",
        "Wonjin Shin",
        "Shuhong Liu",
        "Tho Mai",
        "Jeongmo Lee",
        "Chuanbo Hua",
        "Kun Wang",
        "Jun Liu",
        "Joo-Young Kim"
      ],
      "abstract": "Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.",
      "pdf_url": "https://arxiv.org/pdf/2602.06822v1",
      "published": "2026-02-06T16:07:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06822v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training",
      "authors": [
        "Dunwei Tu",
        "Hongyan Hao",
        "Hansi Yang",
        "Yihao Chen",
        "Yi-Kai Zhang",
        "Zhikang Xia",
        "Yu Yang",
        "Yueqing Sun",
        "Xingchen Liu",
        "Furao Shen",
        "Qi Gu",
        "Hui Su",
        "Xunliang Cai"
      ],
      "abstract": "Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.",
      "pdf_url": "https://arxiv.org/pdf/2602.06820v1",
      "published": "2026-02-06T16:05:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06820v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Bridging 6G IoT and AI: LLM-Based Efficient Approach for Physical Layer's Optimization Tasks",
      "authors": [
        "Ahsan Mehmood",
        "Naveed Ul Hassan",
        "Ghassan M. Kraidy"
      ],
      "abstract": "This paper investigates the role of large language models (LLMs) in sixth-generation (6G) Internet of Things (IoT) networks and proposes a prompt-engineering-based real-time feedback and verification (PE-RTFV) framework that perform physical-layer's optimization tasks through an iteratively process. By leveraging the naturally available closed-loop feedback inherent in wireless communication systems, PE-RTFV enables real-time physical-layer optimization without requiring model retraining. The proposed framework employs an optimization LLM (O-LLM) to generate task-specific structured prompts, which are provided to an agent LLM (A-LLM) to produce task-specific solutions. Utilizing real-time system feedback, the O-LLM iteratively refines the prompts to guide the A-LLM toward improved solutions in a gradient-descent-like optimization process. We test PE-RTFV approach on wireless-powered IoT testbed case study on user-goal-driven constellation design through semantically solving rate-energy (RE)-region optimization problem which demonstrates that PE-RTFV achieves near-genetic-algorithm performance within only a few iterations, validating its effectiveness for complex physical-layer optimization tasks in resource-constrained IoT networks.",
      "pdf_url": "https://arxiv.org/pdf/2602.06819v1",
      "published": "2026-02-06T16:05:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06819v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Wild Guesses and Mild Guesses in Active Concept Learning",
      "authors": [
        "Anirudh Chari",
        "Neil Pattanaik"
      ],
      "abstract": "Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting \"safe\" queries, leading to faster convergence on simple rules. Our results suggest that \"confirmation bias\" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.",
      "pdf_url": "https://arxiv.org/pdf/2602.06818v1",
      "published": "2026-02-06T16:04:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06818v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SuReNav: Superpixel Graph-based Constraint Relaxation for Navigation in Over-constrained Environments",
      "authors": [
        "Keonyoung Koh",
        "Moonkyeong Jung",
        "Samuel Seungsup Lee",
        "Daehyung Park"
      ],
      "abstract": "We address the over-constrained planning problem in semi-static environments. The planning objective is to find a best-effort solution that avoids all hard constraint regions while minimally traversing the least risky areas. Conventional methods often rely on pre-defined area costs, limiting generalizations. Further, the spatial continuity of navigation spaces makes it difficult to identify regions that are passable without overestimation. To overcome these challenges, we propose SuReNav, a superpixel graph-based constraint relaxation and navigation method that imitates human-like safe and efficient navigation. Our framework consists of three components: 1) superpixel graph map generation with regional constraints, 2) regional-constraint relaxation using graph neural network trained on human demonstrations for safe and efficient navigation, and 3) interleaving relaxation, planning, and execution for complete navigation. We evaluate our method against state-of-the-art baselines on 2D semantic maps and 3D maps from OpenStreetMap, achieving the highest human-likeness score of complete navigation while maintaining a balanced trade-off between efficiency and safety. We finally demonstrate its scalability and generalization performance in real-world urban navigation with a quadruped robot, Spot.",
      "pdf_url": "https://arxiv.org/pdf/2602.06807v1",
      "published": "2026-02-06T15:55:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06807v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "On the Identifiability of Steering Vectors in Large Language Models",
      "authors": [
        "Sohan Venkatesh",
        "Ashish Mahendran Kurapath"
      ],
      "abstract": "Activation steering methods, such as persona vectors, are widely used to control large language model behavior and increasingly interpreted as revealing meaningful internal representations. This interpretation implicitly assumes steering directions are identifiable and uniquely recoverable from input-output behavior. We formalize steering as an intervention on internal representations and prove that, under realistic modeling and data conditions, steering vectors are fundamentally non-identifiable due to large equivalence classes of behaviorally indistinguishable interventions. Empirically, we validate this across multiple models and semantic traits, showing orthogonal perturbations achieve near-equivalent efficacy with negligible effect sizes. However, identifiability is recoverable under structural assumptions including statistical independence, sparsity constraints, multi-environment validation or cross-layer consistency. These findings reveal fundamental interpretability limits and clarify structural assumptions required for reliable safety-critical control.",
      "pdf_url": "https://arxiv.org/pdf/2602.06801v1",
      "published": "2026-02-06T15:53:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06801v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling",
      "authors": [
        "Kate Sanders",
        "Nathaniel Weir",
        "Sapana Chaudhary",
        "Kaj Bostrom",
        "Huzefa Rangwala"
      ],
      "abstract": "An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or \"rubrics\", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.",
      "pdf_url": "https://arxiv.org/pdf/2602.06795v1",
      "published": "2026-02-06T15:51:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06795v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs",
      "authors": [
        "Yassine Chagna",
        "Antal Goldschmidt"
      ],
      "abstract": "This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.",
      "pdf_url": "https://arxiv.org/pdf/2602.06777v1",
      "published": "2026-02-06T15:31:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06777v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Understanding What State Space Models Learn About Code",
      "authors": [
        "Jiali Wu",
        "Abhinav Anand",
        "Shweta Verma",
        "Mira Mezini"
      ],
      "abstract": "State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a black box. We present the first systematic analysis of what SSM-based code models actually learn and perform the first comparative analysis of SSM and Transformer-based code models. Our analysis reveals that SSMs outperform Transformers at capturing code syntax and semantics in pretraining but forgets certain syntactic and semantic relations during fine-tuning on task, especially when the task emphasizes short-range dependencies. To diagnose this, we introduce SSM-Interpret, a frequency-domain framework that exposes a spectral shift toward short-range dependencies during fine-tuning. Guided by these findings, we propose architectural modifications that significantly improve the performance of SSM-based code model, validating that our analysis directly enables better models.",
      "pdf_url": "https://arxiv.org/pdf/2602.06774v1",
      "published": "2026-02-06T15:29:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06774v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models",
      "authors": [
        "Fengpeng Li",
        "Kemou Li",
        "Qizhou Wang",
        "Bo Han",
        "Jiantao Zhou"
      ],
      "abstract": "Concept erasure helps stop diffusion models (DMs) from generating harmful content; but current methods face robustness retention trade off. Robustness means the model fine-tuned by concept erasure methods resists reactivation of erased concepts, even under semantically related prompts. Retention means unrelated concepts are preserved so the model's overall utility stays intact. Both are critical for concept erasure in practice, yet addressing them simultaneously is challenging, as existing works typically improve one factor while sacrificing the other. Prior work typically strengthens one while degrading the other, e.g., mapping a single erased prompt to a fixed safe target leaves class level remnants exploitable by prompt attacks, whereas retention-oriented schemes underperform against adaptive adversaries. This paper introduces Adversarial Erasure with Gradient Informed Synergy (AEGIS), a retention-data-free framework that advances both robustness and retention.",
      "pdf_url": "https://arxiv.org/pdf/2602.06771v1",
      "published": "2026-02-06T15:27:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06771v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "A Unified Framework for LLM Watermarks",
      "authors": [
        "Thibaud Gloaguen",
        "Robin Staab",
        "Nikola Jovanović",
        "Martin Vechev"
      ],
      "abstract": "LLM watermarks allow tracing AI-generated texts by inserting a detectable signal into their generated content. Recent works have proposed a wide range of watermarking algorithms, each with distinct designs, usually built using a bottom-up approach. Crucially, there is no general and principled formulation for LLM watermarking.\n  In this work, we show that most existing and widely used watermarking schemes can in fact be derived from a principled constrained optimization problem. Our formulation unifies existing watermarking methods and explicitly reveals the constraints that each method optimizes. In particular, it highlights an understudied quality-diversity-power trade-off. At the same time, our framework also provides a principled approach for designing novel watermarking schemes tailored to specific requirements. For instance, it allows us to directly use perplexity as a proxy for quality, and derive new schemes that are optimal with respect to this constraint. Our experimental evaluation validates our framework: watermarking schemes derived from a given constraint consistently maximize detection power with respect to that constraint.",
      "pdf_url": "https://arxiv.org/pdf/2602.06754v1",
      "published": "2026-02-06T15:00:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06754v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Gold Exploration using Representations from a Multispectral Autoencoder",
      "authors": [
        "Argyro Tsandalidou",
        "Konstantinos Dogeas",
        "Eleftheria Tetoula Tsonga",
        "Elisavet Parselia",
        "Georgios Tsimiklis",
        "George Arvanitakis"
      ],
      "abstract": "Satellite imagery is employed for large-scale prospectivity mapping due to the high cost and typically limited availability of on-site mineral exploration data. In this work, we present a proof-of-concept framework that leverages generative representations learned from multispectral Sentinel-2 imagery to identify gold-bearing regions from space. An autoencoder foundation model, called Isometric, which is pretrained on the large-scale FalconSpace-S2 v1.0 dataset, produces information-dense spectral-spatial representations that serve as inputs to a lightweight XGBoost classifier. We compare this representation-based approach with a raw spectral input baseline using a dataset of 63 Sentinel-2 images from known gold and non-gold locations. The proposed method improves patch-level accuracy from 0.51 to 0.68 and image-level accuracy from 0.55 to 0.73, demonstrating that generative embeddings capture transferable mineralogical patterns even with limited labeled data. These results highlight the potential of foundation-model representations to make mineral exploration more efficient, scalable, and globally applicable.",
      "pdf_url": "https://arxiv.org/pdf/2602.06748v1",
      "published": "2026-02-06T14:47:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06748v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions",
      "authors": [
        "Alessandro Abate",
        "Giuseppe De Giacomo",
        "Mathias Jackermeier",
        "Jan Kretínský",
        "Maximilian Prokop",
        "Christoph Weinhuber"
      ],
      "abstract": "We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.",
      "pdf_url": "https://arxiv.org/pdf/2602.06746v1",
      "published": "2026-02-06T14:46:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06746v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Optimal Abstractions for Verifying Properties of Kolmogorov-Arnold Networks (KANs)",
      "authors": [
        "Noah Schwartz",
        "Chandra Kanth Nagesh",
        "Sriram Sankaranarayanan",
        "Ramneet Kaur",
        "Tuhin Sahai",
        "Susmit Jha"
      ],
      "abstract": "We present a novel approach for verifying properties of Kolmogorov-Arnold Networks (KANs), a class of neural networks characterized by nonlinear, univariate activation functions typically implemented as piecewise polynomial splines or Gaussian processes. Our method creates mathematical ``abstractions'' by replacing each KAN unit with a piecewise affine (PWA) function, providing both local and global error estimates between the original network and its approximation. These abstractions enable property verification by encoding the problem as a Mixed Integer Linear Program (MILP), determining whether outputs satisfy specified properties when inputs belong to a given set. A critical challenge lies in balancing the number of pieces in the PWA approximation: too many pieces add binary variables that make verification computationally intractable, while too few pieces create excessive error margins that yield uninformative bounds. Our key contribution is a systematic framework that exploits KAN structure to find optimal abstractions. By combining dynamic programming at the unit level with a knapsack optimization across the network, we minimize the total number of pieces while guaranteeing specified error bounds. This approach determines the optimal approximation strategy for each unit while maintaining overall accuracy requirements. Empirical evaluation across multiple KAN benchmarks demonstrates that the upfront analysis costs of our method are justified by superior verification results.",
      "pdf_url": "https://arxiv.org/pdf/2602.06737v1",
      "published": "2026-02-06T14:33:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06737v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "Pairwise is Not Enough: Hypergraph Neural Networks for Multi-Agent Pathfinding",
      "authors": [
        "Rishabh Jain",
        "Keisuke Okumura",
        "Michael Amir",
        "Pietro Lio",
        "Amanda Prorok"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) is a representative multi-agent coordination problem, where multiple agents are required to navigate to their respective goals without collisions. Solving MAPF optimally is known to be NP-hard, leading to the adoption of learning-based approaches to alleviate the online computational burden. Prevailing approaches, such as Graph Neural Networks (GNNs), are typically constrained to pairwise message passing between agents. However, this limitation leads to suboptimal behaviours and critical issues, such as attention dilution, particularly in dense environments where group (i.e. beyond just two agents) coordination is most critical. Despite the importance of such higher-order interactions, existing approaches have not been able to fully explore them. To address this representational bottleneck, we introduce HMAGAT (Hypergraph Multi-Agent Attention Network), a novel architecture that leverages attentional mechanisms over directed hypergraphs to explicitly capture group dynamics. Empirically, HMAGAT establishes a new state-of-the-art among learning-based MAPF solvers: e.g., despite having just 1M parameters and being trained on 100$\\times$ less data, it outperforms the current SoTA 85M parameter model. Through detailed analysis of HMAGAT's attention values, we demonstrate how hypergraph representations mitigate the attention dilution inherent in GNNs and capture complex interactions where pairwise methods fail. Our results illustrate that appropriate inductive biases are often more critical than the training data size or sheer parameter count for multi-agent problems.",
      "pdf_url": "https://arxiv.org/pdf/2602.06733v1",
      "published": "2026-02-06T14:28:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06733v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models",
      "authors": [
        "Zuyao Xu",
        "Yuqi Qiu",
        "Lu Sun",
        "FaSheng Miao",
        "Fubin Wu",
        "Xinyi Wang",
        "Xiang Li",
        "Haozhe Lu",
        "ZhengZe Zhang",
        "Yuxin Hu",
        "Jialu Li",
        "Jin Luo",
        "Feng Zhang",
        "Rui Luo",
        "Xinran Liu",
        "Yingxian Li",
        "Jiaji Liu"
      ],
      "abstract": "Citations provide the basis for trusting scientific claims; when they are invalid or fabricated, this trust collapses. With the advent of Large Language Models (LLMs), this risk has intensified: LLMs are increasingly used for academic writing, yet their tendency to fabricate citations (``ghost citations'') poses a systemic threat to citation validity.\n  To quantify this threat and inform mitigation, we develop CiteVerifier, an open-source framework for large-scale citation verification, and conduct the first comprehensive study of citation validity in the LLM era through three experiments built on it. We benchmark 13 state-of-the-art LLMs on citation generation across 40 research domains, finding that all models hallucinate citations at rates from 14.23\\% to 94.93\\%, with significant variation across research domains. Moreover, we analyze 2.2 million citations from 56,381 papers published at top-tier AI/ML and Security venues (2020--2025), confirming that 1.07\\% of papers contain invalid or fabricated citations (604 papers), with an 80.9\\% increase in 2025 alone. Furthermore, we survey 97 researchers and analyze 94 valid responses after removing 3 conflicting samples, revealing a critical ``verification gap'': 41.5\\% of researchers copy-paste BibTeX without checking and 44.4\\% choose no-action responses when encountering suspicious references; meanwhile, 76.7\\% of reviewers do not thoroughly check references and 80.0\\% never suspect fake citations. Our findings reveal an accelerating crisis where unreliable AI tools, combined with inadequate human verification by researchers and insufficient peer review scrutiny, enable fabricated citations to contaminate the scientific record. We propose interventions for researchers, venues, and tool developers to protect citation integrity.",
      "pdf_url": "https://arxiv.org/pdf/2602.06718v1",
      "published": "2026-02-06T14:08:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06718v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare",
      "authors": [
        "Daniil Plyusov",
        "Alexey Gorbatovski",
        "Boris Shaposhnikov",
        "Viacheslav Sinii",
        "Alexey Malakhov",
        "Daniil Gavrilov"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\\rightarrow$ 70.3 (GRPO), 69.3 $\\rightarrow$ 72.5 (DAPO), and 73.2 $\\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.",
      "pdf_url": "https://arxiv.org/pdf/2602.06717v1",
      "published": "2026-02-06T14:07:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06717v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Autoregressive Models for Knowledge Graph Generation",
      "authors": [
        "Thiviyan Thanapalasingam",
        "Antonis Vozikis",
        "Peter Bloem",
        "Paul Groth"
      ],
      "abstract": "Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.",
      "pdf_url": "https://arxiv.org/pdf/2602.06707v1",
      "published": "2026-02-06T13:50:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06707v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers",
      "authors": [
        "Shentong Mo",
        "Lanqing Li"
      ],
      "abstract": "Generative models for de novo protein backbone design have achieved remarkable success in creating novel protein structures. However, these diffusion-based approaches remain computationally intensive and slower than desired for large-scale structural exploration. While recent efforts like Proteina have introduced flow-matching to improve sampling efficiency, the potential of tokenization for structural compression and acceleration remains largely unexplored in the protein domain. In this work, we present SaDiT, a novel framework that accelerates protein backbone generation by integrating SaProt Tokenization with a Diffusion Transformer (DiT) architecture. SaDiT leverages a discrete latent space to represent protein geometry, significantly reducing the complexity of the generation process while maintaining theoretical SE(3) equivalence. To further enhance efficiency, we introduce an IPA Token Cache mechanism that optimizes the Invariant Point Attention (IPA) layers by reusing computed token states during iterative sampling. Experimental results demonstrate that SaDiT outperforms state-of-the-art models, including RFDiffusion and Proteina, in both computational speed and structural viability. We evaluate our model across unconditional backbone generation and fold-class conditional generation tasks, where SaDiT shows superior ability to capture complex topological features with high designability.",
      "pdf_url": "https://arxiv.org/pdf/2602.06706v1",
      "published": "2026-02-06T13:50:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06706v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data",
      "authors": [
        "Lucie Termignon",
        "Simonas Zilinskas",
        "Hadrien Pélissier",
        "Aurélien Barrot",
        "Nicolas Chesnais",
        "Elie Gavoty"
      ],
      "abstract": "Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.",
      "pdf_url": "https://arxiv.org/pdf/2602.06669v1",
      "published": "2026-02-06T12:53:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06669v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Not All Layers Need Tuning: Selective Layer Restoration Recovers Diversity",
      "authors": [
        "Bowen Zhang",
        "Meiyi Wang",
        "Harold Soh"
      ],
      "abstract": "Post-training improves instruction-following and helpfulness of large language models (LLMs) but often reduces generation diversity, which leads to repetitive outputs in open-ended settings, a phenomenon known as mode collapse. Motivated by evidence that LLM layers play distinct functional roles, we hypothesize that mode collapse can be localized to specific layers and that restoring a carefully chosen range of layers to their pre-trained weights can recover diversity while maintaining high output quality. To validate this hypothesis and decide which layers to restore, we design a proxy task -- Constrained Random Character(CRC) -- with an explicit validity set and a natural diversity objective. Results on CRC reveal a clear diversity-validity trade-off across restoration ranges and identify configurations that increase diversity with minimal quality loss. Based on these findings, we propose Selective Layer Restoration (SLR), a training-free method that restores selected layers in a post-trained model to their pre-trained weights, yielding a hybrid model with the same architecture and parameter count, incurring no additional inference cost. Across three different tasks (creative writing, open-ended question answering, and multi-step reasoning) and three different model families (Llama, Qwen, and Gemma), we find SLR can consistently and substantially improve output diversity while maintaining high output quality.",
      "pdf_url": "https://arxiv.org/pdf/2602.06665v1",
      "published": "2026-02-06T12:49:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06665v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan",
      "authors": [
        "Boyu Chen",
        "Tai Guo",
        "Weiyu Cui",
        "Yuqing Li",
        "Xingxing Wang",
        "Chuan Shi",
        "Cheng Yang"
      ],
      "abstract": "Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint optimization of intra-tower and inter-tower tasks. However, we observe that joint optimization often leads to certain modalities dominating the training process, while other modalities are neglected. In addition, inconsistent training speeds across modalities can easily result in the one-epoch problem. To address these challenges, we propose a staged pretraining strategy, which guides the model to focus on specialized tasks at each stage, enabling it to effectively attend to and utilize multimodal features, and allowing flexible control over the training process at each stage to avoid the one-epoch problem. Furthermore, to better utilize the semantic IDs that compress high-dimensional multimodal embeddings, we design both generative and discriminative tasks to help the model understand the associations between SIDs, queries, and item features, thereby improving overall performance. Extensive experiments on large-scale real-world Meituan data demonstrate that our method achieves improvements of 3.80%, 2.64%, and 2.17% on R@5, R@10, and R@20, and 5.10%, 4.22%, and 2.09% on N@5, N@10, and N@20 compared to mainstream baselines. Online A/B testing on the Meituan platform shows that our approach achieves a 1.12% increase in revenue and a 1.02% increase in click-through rate, validating the effectiveness and superiority of our method in practical applications.",
      "pdf_url": "https://arxiv.org/pdf/2602.06654v1",
      "published": "2026-02-06T12:29:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06654v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "RAPID: Reconfigurable, Adaptive Platform for Iterative Design",
      "authors": [
        "Zi Yin",
        "Fanhong Li",
        "Shurui Zheng",
        "Jia Liu"
      ],
      "abstract": "Developing robotic manipulation policies is iterative and hypothesis-driven: researchers test tactile sensing, gripper geometries, and sensor placements through real-world data collection and training. Yet even minor end-effector changes often require mechanical refitting and system re-integration, slowing iteration. We present RAPID, a full-stack reconfigurable platform designed to reduce this friction. RAPID is built around a tool-free, modular hardware architecture that unifies handheld data collection and robot deployment, and a matching software stack that maintains real-time awareness of the underlying hardware configuration through a driver-level Physical Mask derived from USB events. This modular hardware architecture reduces reconfiguration to seconds and makes systematic multi-modal ablation studies practical, allowing researchers to sweep diverse gripper and sensing configurations without repeated system bring-up. The Physical Mask exposes modality presence as an explicit runtime signal, enabling auto-configuration and graceful degradation under sensor hot-plug events, so policies can continue executing when sensors are physically added or removed. System-centric experiments show that RAPID reduces the setup time for multi-modal configurations by two orders of magnitude compared to traditional workflows and preserves policy execution under runtime sensor hot-unplug events. The hardware designs, drivers, and software stack are open-sourced at https://rapid-kit.github.io/ .",
      "pdf_url": "https://arxiv.org/pdf/2602.06653v1",
      "published": "2026-02-06T12:28:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06653v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Same Answer, Different Representations: Hidden instability in VLMs",
      "authors": [
        "Farooq Ahmad Wani",
        "Alessandro Suglia",
        "Rohit Saxena",
        "Aryo Pradipta Gema",
        "Wai-Chung Kwan",
        "Fazl Barez",
        "Maria Sofia Bucarelli",
        "Fabrizio Silvestri",
        "Pasquale Minervini"
      ],
      "abstract": "The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.",
      "pdf_url": "https://arxiv.org/pdf/2602.06652v1",
      "published": "2026-02-06T12:24:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2602.06652v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    }
  ]
}