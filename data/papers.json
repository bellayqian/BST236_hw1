{
  "last_updated": "2025-08-05T00:59:06.878050",
  "papers": [
    {
      "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models",
      "authors": [
        "Xushuo Tang",
        "Yi Ding",
        "Zhengyi Yang",
        "Yin Chen",
        "Yongrui Gu",
        "Wenke Yang",
        "Mingchen Ju",
        "Xin Cao",
        "Yongfei Liu",
        "Wenjie Zhang"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.",
      "pdf_url": "http://arxiv.org/pdf/2508.00788v1",
      "published": "2025-08-01T17:11:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00788v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics",
      "authors": [
        "Tom Or",
        "Omri Azencot"
      ],
      "abstract": "Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2508.00784v1",
      "published": "2025-08-01T17:07:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00784v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation",
      "authors": [
        "Kien T. Pham",
        "Yingqing He",
        "Yazhou Xing",
        "Qifeng Chen",
        "Long Chen"
      ],
      "abstract": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.",
      "pdf_url": "http://arxiv.org/pdf/2508.00782v1",
      "published": "2025-08-01T17:05:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00782v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation",
      "authors": [
        "Irene Iele",
        "Francesco Di Feola",
        "Valerio Guarrasi",
        "Paolo Soda"
      ],
      "abstract": "Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/cosbidev/Sample-Aware_TTA.",
      "pdf_url": "http://arxiv.org/pdf/2508.00766v1",
      "published": "2025-08-01T16:41:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00766v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations",
      "authors": [
        "Qiyao Xue",
        "Yuchen Dou",
        "Ryan Shi",
        "Xiang Lorraine Li",
        "Wei Gao"
      ],
      "abstract": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.",
      "pdf_url": "http://arxiv.org/pdf/2508.00760v1",
      "published": "2025-08-01T16:34:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00760v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection",
      "authors": [
        "Yaxin Ma",
        "Benjamin Colburn",
        "Jose C. Principe"
      ],
      "abstract": "Bayesian neural networks and deep ensemble methods have been proposed for\nuncertainty quantification; however, they are computationally intensive and\nrequire large storage. By utilizing a single deterministic model, we can solve\nthe above issue. We propose an effective method based on feature space density\nto quantify uncertainty for distributional shifts and out-of-distribution (OOD)\ndetection. Specifically, we leverage the information potential field derived\nfrom kernel density estimation to approximate the feature space density of the\ntraining set. By comparing this density with the feature space representation\nof test samples, we can effectively determine whether a distributional shift\nhas occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons\nand Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The\nresults demonstrate that our method outperforms baseline models.",
      "pdf_url": "http://arxiv.org/pdf/2508.00754v1",
      "published": "2025-08-01T16:31:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00754v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking",
      "authors": [
        "Qing Zhang",
        "Alex Deng",
        "Michelle Du",
        "Huiji Gao",
        "Liwei He",
        "Sanjeev Katariya"
      ],
      "abstract": "Evaluation plays a crucial role in the development of ranking algorithms on\nsearch and recommender systems. It enables online platforms to create\nuser-friendly features that drive commercial success in a steady and effective\nmanner. The online environment is particularly conducive to applying causal\ninference techniques, such as randomized controlled experiments (known as A/B\ntest), which are often more challenging to implement in fields like medicine\nand public policy. However, businesses face unique challenges when it comes to\neffective A/B test. Specifically, achieving sufficient statistical power for\nconversion-based metrics can be time-consuming, especially for significant\npurchases like booking accommodations. While offline evaluations are quicker\nand more cost-effective, they often lack accuracy and are inadequate for\nselecting candidates for A/B test. To address these challenges, we developed\ninterleaving and counterfactual evaluation methods to facilitate rapid online\nassessments for identifying the most promising candidates for A/B tests. Our\napproach not only increased the sensitivity of experiments by a factor of up to\n100 (depending on the approach and metrics) compared to traditional A/B testing\nbut also streamlined the experimental process. The practical insights gained\nfrom usage in production can also benefit organizations with similar interests.",
      "pdf_url": "http://arxiv.org/pdf/2508.00751v1",
      "published": "2025-08-01T16:28:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00751v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "H.3; G.3"
      ]
    },
    {
      "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos",
      "authors": [
        "Laura Pedrouzo-Rodriguez",
        "Pedro Delgado-DeRobles",
        "Luis F. Gomez",
        "Ruben Tolosana",
        "Ruben Vera-Rodriguez",
        "Aythami Morales",
        "Julian Fierrez"
      ],
      "abstract": "Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's\navatar-preserving their appearance and voice-making it nearly impossible to\ndetect its fraudulent usage by sight or sound alone. In this paper, we explore\nthe challenge of biometric verification in such avatar-mediated scenarios. Our\nmain question is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.00748v1",
      "published": "2025-08-01T16:23:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00748v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.MM"
      ]
    },
    {
      "title": "Agentic large language models improve retrieval-based radiology question answering",
      "authors": [
        "Sebastian Wind",
        "Jeta Sopa",
        "Daniel Truhn",
        "Mahshad Lotfinia",
        "Tri-Thien Nguyen",
        "Keno Bressem",
        "Lisa Adams",
        "Mirabela Rusu",
        "Harald Köstler",
        "Gerhard Wellein",
        "Andreas Maier",
        "Soroosh Tayebi Arasteh"
      ],
      "abstract": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.",
      "pdf_url": "http://arxiv.org/pdf/2508.00743v1",
      "published": "2025-08-01T16:18:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00743v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data",
      "authors": [
        "Sohaib Imran",
        "Rob Lamb",
        "Peter M. Atkinson"
      ],
      "abstract": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.",
      "pdf_url": "http://arxiv.org/pdf/2508.00741v1",
      "published": "2025-08-01T16:12:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00741v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "How LLMs are Shaping the Future of Virtual Reality",
      "authors": [
        "Süeda Özkaya",
        "Santiago Berrezueta-Guzman",
        "Stefan Wagner"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.",
      "pdf_url": "http://arxiv.org/pdf/2508.00737v1",
      "published": "2025-08-01T16:08:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00737v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems",
      "authors": [
        "Liuyun Xu",
        "Seymour M. J. Spence"
      ],
      "abstract": "Existing variance reduction techniques used in stochastic simulations for\nrare event analysis still require a substantial number of model evaluations to\nestimate small failure probabilities. In the context of complex, nonlinear\nfinite element modeling environments, this can become computationally\nchallenging-particularly for systems subjected to stochastic excitation. To\naddress this challenge, a multi-fidelity stratified sampling scheme with\nadaptive machine learning metamodels is introduced for efficiently propagating\nuncertainties and estimating small failure probabilities. In this approach, a\nhigh-fidelity dataset generated through stratified sampling is used to train a\ndeep learning-based metamodel, which then serves as a cost-effective and highly\ncorrelated low-fidelity model. An adaptive training scheme is proposed to\nbalance the trade-off between approximation quality and computational demand\nassociated with the development of the low-fidelity model. By integrating the\nlow-fidelity outputs with additional high-fidelity results, an unbiased\nestimate of the strata-wise failure probabilities is obtained using a\nmulti-fidelity Monte Carlo framework. The overall probability of failure is\nthen computed using the total probability theorem. Application to a full-scale\nhigh-rise steel building subjected to stochastic wind excitation demonstrates\nthat the proposed scheme can accurately estimate exceedance probability curves\nfor nonlinear responses of interest, while achieving significant computational\nsavings compared to single-fidelity variance reduction approaches.",
      "pdf_url": "http://arxiv.org/pdf/2508.00734v1",
      "published": "2025-08-01T16:04:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00734v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA",
      "authors": [
        "Yingxu Wang",
        "Shiqi Fan",
        "Mengzhu Wang",
        "Siwei Liu"
      ],
      "abstract": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.00719v1",
      "published": "2025-08-01T15:38:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00719v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning",
      "authors": [
        "Yingxu Wang",
        "Mengzhu Wang",
        "Zhichao Huang",
        "Suyu Liu"
      ],
      "abstract": "Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled\nsource graphs to unlabeled target graphs by learning domain-invariant\nrepresentations, which is essential in applications such as molecular property\nprediction and social network analysis. However, most existing GDA methods rely\non the assumption of clean source labels, which rarely holds in real-world\nscenarios where annotation noise is pervasive. This label noise severely\nimpairs feature alignment and degrades adaptation performance under domain\nshifts. To address this challenge, we propose Nested Graph Pseudo-Label\nRefinement (NeGPR), a novel framework tailored for graph-level domain\nadaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,\nsemantic and topology branches, by enforcing neighborhood consistency in the\nfeature space, thereby reducing the influence of noisy supervision. To bridge\ndomain gaps, NeGPR employs a nested refinement mechanism in which one branch\nselects high-confidence target samples to guide the adaptation of the other,\nenabling progressive cross-domain learning. Furthermore, since pseudo-labels\nmay still contain noise and the pre-trained branches are already overfitted to\nthe noisy labels in the source domain, NeGPR incorporates a noise-aware\nregularization strategy. This regularization is theoretically proven to\nmitigate the adverse effects of pseudo-label noise, even under the presence of\nsource overfitting, thus enhancing the robustness of the adaptation process.\nExtensive experiments on benchmark datasets demonstrate that NeGPR consistently\noutperforms state-of-the-art methods under severe label noise, achieving gains\nof up to 12.7% in accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.00716v1",
      "published": "2025-08-01T15:32:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00716v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "JSON-Bag: A generic game trajectory representation",
      "authors": [
        "Dien Nguyen",
        "Diego Perez-Liebana",
        "Simon Lucas"
      ],
      "abstract": "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically\nrepresent game trajectories by tokenizing their JSON descriptions and apply\nJensen-Shannon distance (JSD) as distance metric for them. Using a\nprototype-based nearest-neighbor search (P-NNS), we evaluate the validity of\nJSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders},\n\\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop},\n\\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory\nclassification tasks: classifying the playing agents, game parameters, or game\nseeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the\nmajority of tasks. Evaluating on N-shot classification suggests using JSON-Bag\nprototype to represent game trajectory classes is also sample efficient.\nAdditionally, we demonstrate JSON-Bag ability for automatic feature extraction\nby treating tokens as individual features to be used in Random Forest to solve\nthe tasks above, which significantly improves accuracy on underperforming\ntasks. Finally, we show that, across all six games, the JSD between JSON-Bag\nprototypes of agent classes highly correlates with the distances between\nagents' policies.",
      "pdf_url": "http://arxiv.org/pdf/2508.00712v1",
      "published": "2025-08-01T15:26:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00712v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System",
      "authors": [
        "Shubham Kumar Nigam",
        "Balaramamahanthi Deepak Patnaik",
        "Shivam Mishra",
        "Ajay Varghese Thomas",
        "Noel Shallum",
        "Kripabandhu Ghosh",
        "Arnab Bhattacharya"
      ],
      "abstract": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.",
      "pdf_url": "http://arxiv.org/pdf/2508.00709v1",
      "published": "2025-08-01T15:23:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00709v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Efficient Solution and Learning of Robust Factored MDPs",
      "authors": [
        "Yannik Schnitzer",
        "Alessandro Abate",
        "David Parker"
      ],
      "abstract": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling\nepistemic uncertainty about transition dynamics. Learning r-MDPs from\ninteractions with an unknown environment enables the synthesis of robust\npolicies with provable (PAC) guarantees on performance, but this can require a\nlarge number of sample interactions. We propose novel methods for solving and\nlearning r-MDPs based on factored state-space representations that leverage the\nindependence between model uncertainty across system components. Although\npolicy synthesis for factored r-MDPs leads to hard, non-convex optimisation\nproblems, we show how to reformulate these into tractable linear programs.\nBuilding on these, we also propose methods to learn factored model\nrepresentations directly. Our experimental results show that exploiting\nfactored structure can yield dimensional gains in sample efficiency, producing\nmore effective robust policies with tighter performance guarantees than\nstate-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.00707v1",
      "published": "2025-08-01T15:23:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00707v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "D3: Training-Free AI-Generated Video Detection Using Second-Order Features",
      "authors": [
        "Chende Zheng",
        "Ruiqi suo",
        "Chenhao Lin",
        "Zhengyu Zhao",
        "Le Yang",
        "Shuai Liu",
        "Minghui Yang",
        "Cong Wang",
        "Chao Shen"
      ],
      "abstract": "The evolution of video generation techniques, such as Sora, has made it\nincreasingly easy to produce high-fidelity AI-generated videos, raising public\nconcern over the dissemination of synthetic content. However, existing\ndetection methodologies remain limited by their insufficient exploration of\ntemporal artifacts in synthetic videos. To bridge this gap, we establish a\ntheoretical framework through second-order dynamical analysis under Newtonian\nmechanics, subsequently extending the Second-order Central Difference features\ntailored for temporal artifact detection. Building on this theoretical\nfoundation, we reveal a fundamental divergence in second-order feature\ndistributions between real and AI-generated videos. Concretely, we propose\nDetection by Difference of Differences (D3), a novel training-free detection\nmethod that leverages the above second-order temporal discrepancies. We\nvalidate the superiority of our D3 on 4 open-source datasets (Gen-Video,\nVideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,\nD3 outperforms the previous best method by 10.39% (absolute) mean Average\nPrecision. Additional experiments on time cost and post-processing operations\ndemonstrate D3's exceptional computational efficiency and strong robust\nperformance. Our code is available at https://github.com/Zig-HS/D3.",
      "pdf_url": "http://arxiv.org/pdf/2508.00701v1",
      "published": "2025-08-01T15:17:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00701v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation",
      "authors": [
        "Yiming Wu",
        "Huan Wang",
        "Zhenghao Chen",
        "Jianxin Pang",
        "Dong Xu"
      ],
      "abstract": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.",
      "pdf_url": "http://arxiv.org/pdf/2508.00697v1",
      "published": "2025-08-01T15:14:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00697v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries",
      "authors": [
        "Shubham Kumar Nigam",
        "Tanmay Dubey",
        "Noel Shallum",
        "Arnab Bhattacharya"
      ],
      "abstract": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.",
      "pdf_url": "http://arxiv.org/pdf/2508.00679v1",
      "published": "2025-08-01T14:49:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00679v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations",
      "authors": [
        "Banan Alkhateeb",
        "Ellis Solaiman"
      ],
      "abstract": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.",
      "pdf_url": "http://arxiv.org/pdf/2508.00674v1",
      "published": "2025-08-01T14:47:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00674v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications",
      "authors": [
        "Wenxuan Wang",
        "Zizhan Ma",
        "Meidan Ding",
        "Shiyi Zheng",
        "Shengyuan Liu",
        "Jie Liu",
        "Jiaming Ji",
        "Wenting Chen",
        "Xiang Li",
        "Linlin Shen",
        "Yixuan Yuan"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.",
      "pdf_url": "http://arxiv.org/pdf/2508.00669v1",
      "published": "2025-08-01T14:41:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00669v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Advancing Quantum Information Science Pre-College Education: The Case for Learning Sciences Collaboration",
      "authors": [
        "Raquel Coelho",
        "Roy Pea",
        "Christian Schunn",
        "Jinglei Cheng",
        "Junyu Liu"
      ],
      "abstract": "As quantum information science advances and the need for pre-college\nengagement grows, a critical question remains: How can young learners be\nprepared to participate in a field so radically different from what they have\nencountered before? This paper argues that meeting this challenge will require\nstrong interdisciplinary collaboration with the Learning Sciences (LS), a field\ndedicated to understanding how people learn and designing theory-guided\nenvironments to support learning. Drawing on lessons from previous STEM\neducation efforts, we discuss two key contributions of the learning sciences to\nquantum information science (QIS) education. The first is design-based\nresearch, the signature methodology of learning sciences, which can inform the\ndevelopment, refinement, and scaling of effective QIS learning experiences. The\nsecond is a framework for reshaping how learners reason about, learn and\nparticipate in QIS practices through shifts in knowledge representations that\nprovide new forms of engagement and associated learning. We call for a two-way\npartnership between quantum information science and the learning sciences, one\nthat not only supports learning in quantum concepts and practices but also\nimproves our understanding of how to teach and support learning in highly\ncomplex domains. We also consider potential questions involved in bridging\nthese disciplinary communities and argue that the theoretical and practical\nbenefits justify the effort.",
      "pdf_url": "http://arxiv.org/pdf/2508.00668v1",
      "published": "2025-08-01T14:41:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00668v1",
      "categories": [
        "physics.ed-ph",
        "cs.AI",
        "cs.CY",
        "quant-ph"
      ]
    },
    {
      "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI",
      "authors": [
        "Maryam Mosleh",
        "Marie Devlin",
        "Ellis Solaiman"
      ],
      "abstract": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.",
      "pdf_url": "http://arxiv.org/pdf/2508.00665v1",
      "published": "2025-08-01T14:36:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00665v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies",
      "authors": [
        "Chakattrai Sookkongwaree",
        "Tattep Lakmuang",
        "Chainarong Amornbunchornvej"
      ],
      "abstract": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2508.00658v1",
      "published": "2025-08-01T14:22:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00658v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "econ.EM",
        "stat.ME"
      ]
    },
    {
      "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings",
      "authors": [
        "Alexia Jolicoeur-Martineau"
      ],
      "abstract": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.",
      "pdf_url": "http://arxiv.org/pdf/2508.00632v1",
      "published": "2025-08-01T13:45:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00632v1",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.MM"
      ]
    },
    {
      "title": "Backdoor Attacks on Deep Learning Face Detection",
      "authors": [
        "Quentin Le Roux",
        "Yannick Teglia",
        "Teddy Furon",
        "Philippe Loubet-Moundi"
      ],
      "abstract": "Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities.",
      "pdf_url": "http://arxiv.org/pdf/2508.00620v1",
      "published": "2025-08-01T13:29:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00620v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data",
      "authors": [
        "Mukesh Kumar Sahu",
        "Pinki Roy"
      ],
      "abstract": "Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment.",
      "pdf_url": "http://arxiv.org/pdf/2508.00615v1",
      "published": "2025-08-01T13:25:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00615v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?",
      "authors": [
        "Lennart Meincke",
        "Ethan Mollick",
        "Lilach Mollick",
        "Dan Shapiro"
      ],
      "abstract": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.",
      "pdf_url": "http://arxiv.org/pdf/2508.00614v1",
      "published": "2025-08-01T13:23:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00614v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Composable OS Kernel Architectures for Autonomous Intelligence",
      "authors": [
        "Rajpreet Singh",
        "Vidhi Kothari"
      ],
      "abstract": "As intelligent systems permeate edge devices, cloud infrastructure, and\nembedded real-time environments, this research proposes a new OS kernel\narchitecture for intelligent systems, transforming kernels from static resource\nmanagers to adaptive, AI-integrated platforms. Key contributions include: (1)\ntreating Loadable Kernel Modules (LKMs) as AI-oriented computation units for\nfast sensory and cognitive processing in kernel space; (2) expanding the Linux\nkernel into an AI-native environment with built-in deep learning inference,\nfloating-point acceleration, and real-time adaptive scheduling for efficient ML\nworkloads; and (3) introducing a Neurosymbolic kernel design leveraging\nCategory Theory and Homotopy Type Theory to unify symbolic reasoning and\ndifferentiable logic within OS internals. Together, these approaches enable\noperating systems to proactively anticipate and adapt to the cognitive needs of\nautonomous intelligent applications.",
      "pdf_url": "http://arxiv.org/pdf/2508.00604v1",
      "published": "2025-08-01T13:07:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00604v1",
      "categories": [
        "cs.OS",
        "cs.AI"
      ]
    },
    {
      "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks",
      "authors": [
        "Francesco Panebianco",
        "Stefano Bonfanti",
        "Francesco Trovò",
        "Michele Carminati"
      ],
      "abstract": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard.",
      "pdf_url": "http://arxiv.org/pdf/2508.00602v1",
      "published": "2025-08-01T13:04:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00602v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Wukong Framework for Not Safe For Work Detection in Text-to-Image systems",
      "authors": [
        "Mingrui Liu",
        "Sixiao Zhang",
        "Cheng Long"
      ],
      "abstract": "Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)\ntechnology enabling diverse and creative image synthesis. However, some outputs\nmay contain Not Safe For Work (NSFW) content (e.g., violence), violating\ncommunity guidelines. Detecting NSFW content efficiently and accurately, known\nas external safeguarding, is essential. Existing external safeguards fall into\ntwo types: text filters, which analyze user prompts but overlook T2I\nmodel-specific variations and are prone to adversarial attacks; and image\nfilters, which analyze final generated images but are computationally costly\nand introduce latency. Diffusion models, the foundation of modern T2I systems\nlike Stable Diffusion, generate images through iterative denoising using a\nU-Net architecture with ResNet and Transformer blocks. We observe that: (1)\nearly denoising steps define the semantic layout of the image, and (2)\ncross-attention layers in U-Net are crucial for aligning text and image\nregions. Based on these insights, we propose Wukong, a transformer-based NSFW\ndetection framework that leverages intermediate outputs from early denoising\nsteps and reuses U-Net's pre-trained cross-attention parameters. Wukong\noperates within the diffusion process, enabling early detection without waiting\nfor full image generation. We also introduce a new dataset containing prompts,\nseeds, and image-specific NSFW labels, and evaluate Wukong on this and two\npublic benchmarks. Results show that Wukong significantly outperforms\ntext-based safeguards and achieves comparable accuracy of image filters, while\noffering much greater efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2508.00591v1",
      "published": "2025-08-01T12:45:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00591v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation",
      "authors": [
        "Ruiqing Ding",
        "Qianfang Sun",
        "Yongkang Leng",
        "Hui Yin",
        "Xiaojian Li"
      ],
      "abstract": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.",
      "pdf_url": "http://arxiv.org/pdf/2508.00581v1",
      "published": "2025-08-01T12:24:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00581v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery",
      "authors": [
        "Raul Castilla-Arquillo",
        "Carlos Perez-del-Pulgar",
        "Levin Gerdes",
        "Alfonso Garcia-Cerezo",
        "Miguel A. Olivares-Mendez"
      ],
      "abstract": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.",
      "pdf_url": "http://arxiv.org/pdf/2508.00580v1",
      "published": "2025-08-01T12:23:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00580v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models",
      "authors": [
        "Zhanliang Wang",
        "Kai Wang"
      ],
      "abstract": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.",
      "pdf_url": "http://arxiv.org/pdf/2508.00576v1",
      "published": "2025-08-01T12:19:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00576v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Analysing Temporal Reasoning in Description Logics Using Formal Grammars",
      "authors": [
        "Camille Bourgaux",
        "Anton Gnatenko",
        "Michaël Thomazo"
      ],
      "abstract": "We establish a correspondence between (fragments of)\n$\\mathcal{TEL}^\\bigcirc$, a temporal extension of the $\\mathcal{EL}$\ndescription logic with the LTL operator $\\bigcirc^k$, and some specific kinds\nof formal grammars, in particular, conjunctive grammars (context-free grammars\nequipped with the operation of intersection). This connection implies that\n$\\mathcal{TEL}^\\bigcirc$ does not possess the property of ultimate periodicity\nof models, and further leads to undecidability of query answering in\n$\\mathcal{TEL}^\\bigcirc$, closing a question left open since the introduction\nof $\\mathcal{TEL}^\\bigcirc$. Moreover, it also allows to establish decidability\nof query answering for some new interesting fragments of\n$\\mathcal{TEL}^\\bigcirc$, and to reuse for this purpose existing tools and\nalgorithms for conjunctive grammars.",
      "pdf_url": "http://arxiv.org/pdf/2508.00575v1",
      "published": "2025-08-01T12:17:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00575v1",
      "categories": [
        "cs.LO",
        "cs.AI"
      ]
    },
    {
      "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought",
      "authors": [
        "Jianwei Wang",
        "Ziming Wu",
        "Fuming Lai",
        "Shaobing Lian",
        "Ziqian Zeng"
      ],
      "abstract": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.",
      "pdf_url": "http://arxiv.org/pdf/2508.00574v1",
      "published": "2025-08-01T12:17:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00574v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Activation-Guided Local Editing for Jailbreaking Attacks",
      "authors": [
        "Jiecong Wang",
        "Haoran Li",
        "Hao Peng",
        "Ziqian Zeng",
        "Zihao Wang",
        "Haohua Du",
        "Zhengtao Yu"
      ],
      "abstract": "Jailbreaking is an essential adversarial technique for red-teaming these\nmodels to uncover and patch security flaws. However, existing jailbreak methods\nface significant drawbacks. Token-level jailbreak attacks often produce\nincoherent or unreadable inputs and exhibit poor transferability, while\nprompt-level attacks lack scalability and rely heavily on manual effort and\nhuman ingenuity. We propose a concise and effective two-stage framework that\ncombines the advantages of these approaches. The first stage performs a\nscenario-based generation of context and rephrases the original malicious query\nto obscure its harmful intent. The second stage then utilizes information from\nthe model's hidden states to guide fine-grained edits, effectively steering the\nmodel's internal representation of the input from a malicious toward a benign\none. Extensive experiments demonstrate that this method achieves\nstate-of-the-art Attack Success Rate, with gains of up to 37.74% over the\nstrongest baseline, and exhibits excellent transferability to black-box models.\nOur analysis further demonstrates that AGILE maintains substantial\neffectiveness against prominent defense mechanisms, highlighting the\nlimitations of current safeguards and providing valuable insights for future\ndefense development. Our code is available at\nhttps://github.com/yunsaijc/AGILE.",
      "pdf_url": "http://arxiv.org/pdf/2508.00555v1",
      "published": "2025-08-01T11:52:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00555v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval",
      "authors": [
        "Wenchao Gu",
        "Zongyi Lyu",
        "Yanlin Wang",
        "Hongyu Zhang",
        "Cuiyun Gao",
        "Michael R. Lyu"
      ],
      "abstract": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%.",
      "pdf_url": "http://arxiv.org/pdf/2508.00546v1",
      "published": "2025-08-01T11:39:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00546v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Foundations of Interpretable Models",
      "authors": [
        "Pietro Barbiero",
        "Mateo Espinosa Zarlenga",
        "Alberto Termine",
        "Mateja Jamnik",
        "Giuseppe Marra"
      ],
      "abstract": "We argue that existing definitions of interpretability are not actionable in\nthat they fail to inform users about general, sound, and robust interpretable\nmodel design. This makes current interpretability research fundamentally\nill-posed. To address this issue, we propose a definition of interpretability\nthat is general, simple, and subsumes existing informal notions within the\ninterpretable AI community. We show that our definition is actionable, as it\ndirectly reveals the foundational properties, underlying assumptions,\nprinciples, data structures, and architectural features necessary for designing\ninterpretable models. Building on this, we propose a general blueprint for\ndesigning interpretable models and introduce the first open-sourced library\nwith native support for interpretable data structures and processes.",
      "pdf_url": "http://arxiv.org/pdf/2508.00545v1",
      "published": "2025-08-01T11:36:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00545v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ]
    },
    {
      "title": "Towards a Measure Theory of Semantic Information",
      "authors": [
        "George M. Coghill"
      ],
      "abstract": "A classic account of the quantification of semantic information is that of\nBar-Hiller and Carnap. Their account proposes an inverse relation between the\ninformativeness of a statement and its probability. However, their approach\nassigns the maximum informativeness to a contradiction: which Floridi refers to\nas the Bar-Hillel-Carnap paradox. He developed a novel theory founded on a\ndistance metric and parabolic relation, designed to remove this paradox.\nUnfortunately is approach does not succeed in that aim.\n  In this paper I critique Floridi's theory of strongly semantic information on\nits own terms and show where it succeeds and fails. I then present a new\napproach based on the unit circle (a relation that has been the basis of\ntheories from basic trigonometry to quantum theory). This is used, by analogy\nwith von Neumann's quantum probability to construct a measure space for\ninformativeness that meets all the requirements stipulated by Floridi and\nremoves the paradox. In addition, while contradictions and tautologies have\nzero informativeness, it is found that messages which are contradictory to each\nother are equally informative. The utility of this is explained by means of an\nexample.",
      "pdf_url": "http://arxiv.org/pdf/2508.00525v1",
      "published": "2025-08-01T11:03:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00525v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "math.IT"
      ]
    },
    {
      "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking",
      "authors": [
        "Haoyu Wang",
        "Chris M. Poskitt",
        "Jun Sun",
        "Jiali Wei"
      ],
      "abstract": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.",
      "pdf_url": "http://arxiv.org/pdf/2508.00500v1",
      "published": "2025-08-01T10:24:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00500v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI",
      "authors": [
        "Mohammed Kamran",
        "Maria Bernathova",
        "Raoul Varga",
        "Christian Singer",
        "Zsuzsanna Bago-Horvath",
        "Thomas Helbich",
        "Georg Langs",
        "Philipp Seeböck"
      ],
      "abstract": "Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced\nMRI (DCE-MRI) is critical for early cancer detection, especially in high-risk\npatients. While recent deep learning methods have advanced lesion segmentation,\nthey primarily target large lesions and neglect valuable longitudinal and\nclinical information routinely used by radiologists. In real-world screening,\ndetecting subtle or emerging lesions requires radiologists to compare across\ntimepoints and consider previous radiology assessments, such as the BI-RADS\nscore. We propose LesiOnTime, a novel 3D segmentation approach that mimics\nclinical diagnostic workflows by jointly leveraging longitudinal imaging and\nBIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)\nblock that dynamically integrates information from previous and current scans;\nand (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent\nspace alignment for scans with similar radiological assessments, thus embedding\ndomain knowledge into the training process. Evaluated on a curated in-house\nlongitudinal dataset of high-risk patients with DCE-MRI, our approach\noutperforms state-of-the-art single-timepoint and longitudinal baselines by 5%\nin terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute\ncomplementary performance gains. These results highlight the importance of\nincorporating temporal and clinical context for reliable early lesion\nsegmentation in real-world breast cancer screening. Our code is publicly\navailable at https://github.com/cirmuw/LesiOnTime",
      "pdf_url": "http://arxiv.org/pdf/2508.00496v1",
      "published": "2025-08-01T10:19:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00496v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning",
      "authors": [
        "Carlo Alessi",
        "Federico Vasile",
        "Federico Ceola",
        "Giulia Pasquale",
        "Nicolò Boccardo",
        "Lorenzo Natale"
      ],
      "abstract": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation",
      "pdf_url": "http://arxiv.org/pdf/2508.00491v1",
      "published": "2025-08-01T10:09:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00491v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization",
      "authors": [
        "Yuning Jiang",
        "Nay Oo",
        "Qiaoran Meng",
        "Lu Lin",
        "Dusit Niyato",
        "Zehui Xiong",
        "Hoon Wei Lim",
        "Biplab Sikdar"
      ],
      "abstract": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use.",
      "pdf_url": "http://arxiv.org/pdf/2508.00478v1",
      "published": "2025-08-01T09:53:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00478v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "91A10, 91A43, 68T01, 94A60",
        "C.2.0; I.2.6; K.6.5"
      ]
    },
    {
      "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs",
      "authors": [
        "Andrea Asperti",
        "Alberto Naibo",
        "Claudio Sacerdoti Coen"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.",
      "pdf_url": "http://arxiv.org/pdf/2508.00459v1",
      "published": "2025-08-01T09:31:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00459v1",
      "categories": [
        "cs.AI",
        "68T07, 68T20",
        "I.2.6; I.2.7; I.2.3"
      ]
    },
    {
      "title": "M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation",
      "authors": [
        "Chuan He",
        "Yongchao Liu",
        "Qiang Li",
        "Wenliang Zhong",
        "Chuntao Hong",
        "Xinwei Yao"
      ],
      "abstract": "Cold-start item recommendation is a significant challenge in recommendation\nsystems, particularly when new items are introduced without any historical\ninteraction data. While existing methods leverage multi-modal content to\nalleviate the cold-start issue, they often neglect the inherent multi-view\nstructure of modalities, the distinction between shared and modality-specific\nfeatures. In this paper, we propose Multi-Modal Multi-View Variational\nAutoEncoder (M^2VAE), a generative model that addresses the challenges of\nmodeling common and unique views in attribute and multi-modal features, as well\nas user preferences over single-typed item features. Specifically, we generate\ntype-specific latent variables for item IDs, categorical attributes, and image\nfeatures, and use Product-of-Experts (PoE) to derive a common representation. A\ndisentangled contrastive loss decouples the common view from unique views while\npreserving feature informativeness. To model user inclinations, we employ a\npreference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.\nWe further incorporate co-occurrence signals via contrastive learning,\neliminating the need for pretraining. Extensive experiments on real-world\ndatasets validate the effectiveness of our approach.",
      "pdf_url": "http://arxiv.org/pdf/2508.00452v1",
      "published": "2025-08-01T09:16:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00452v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation",
      "authors": [
        "Hongxiang Lin",
        "Hao Guo",
        "Zeshun Li",
        "Erpeng Xue",
        "Yongqian He",
        "Xiangyu Hou",
        "Zhaoyu Hu",
        "Lei Wang",
        "Sheng Chen"
      ],
      "abstract": "Traditional recommendation systems tend to trap users in strong feedback\nloops by excessively pushing content aligned with their historical preferences,\nthereby limiting exploration opportunities and causing content fatigue.\nAlthough large language models (LLMs) demonstrate potential with their diverse\ncontent generation capabilities, existing LLM-enhanced dual-model frameworks\nface two major limitations: first, they overlook long-term preferences driven\nby group identity, leading to biased interest modeling; second, they suffer\nfrom static optimization flaws, as a one-time alignment process fails to\nleverage incremental user data for closed-loop optimization. To address these\nchallenges, we propose the Co-Evolutionary Alignment (CoEA) method. For\ninterest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)\nmodule, jointly modeling long-term group identity and short-term individual\ninterests through parallel processing of behavioral sequences. For static\noptimization limitations, we design a Periodic Collaborative Optimization (PCO)\nmechanism. This mechanism regularly conducts preference verification on\nincremental data using the Relevance LLM, then guides the Novelty LLM to\nperform fine-tuning based on the verification results, and subsequently feeds\nback the output of the incrementally fine-tuned Novelty LLM to the Relevance\nLLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.\nExtensive online and offline experiments verify the effectiveness of the CoEA\nmodel in exploratory recommendation.",
      "pdf_url": "http://arxiv.org/pdf/2508.00450v1",
      "published": "2025-08-01T09:10:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00450v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation",
      "authors": [
        "Jiale Zhou",
        "Wenhan Wang",
        "Shikun Li",
        "Xiaolei Qu",
        "Xin Guo",
        "Yizhong Liu",
        "Wenzhong Tang",
        "Xun Lin",
        "Yefeng Zheng"
      ],
      "abstract": "Tubular structure segmentation (TSS) is important for various applications,\nsuch as hemodynamic analysis and route navigation. Despite significant progress\nin TSS, domain shifts remain a major challenge, leading to performance\ndegradation in unseen target domains. Unlike other segmentation tasks, TSS is\nmore sensitive to domain shifts, as changes in topological structures can\ncompromise segmentation integrity, and variations in local features\ndistinguishing foreground from background (e.g., texture and contrast) may\nfurther disrupt topological continuity. To address these challenges, we propose\nTopology-enhanced Test-Time Adaptation (TopoTTA), the first test-time\nadaptation framework designed specifically for TSS. TopoTTA consists of two\nstages: Stage 1 adapts models to cross-domain topological discrepancies using\nthe proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance\ntopological representation without altering pre-trained parameters; Stage 2\nimproves topological continuity by a novel Topology Hard sample Generation\n(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels\nin the generated pseudo-break regions. Extensive experiments across four\nscenarios and ten datasets demonstrate TopoTTA's effectiveness in handling\ntopological distribution shifts, achieving an average improvement of 31.81% in\nclDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2508.00442v1",
      "published": "2025-08-01T08:59:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00442v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Reducing the gap between general purpose data and aerial images in concentrated solar power plants",
      "authors": [
        "M. A. Pérez-Cutiño",
        "J. Valverde",
        "J. Capitán",
        "J. M. Díaz-Báñez"
      ],
      "abstract": "In the context of Concentrated Solar Power (CSP) plants, aerial images\ncaptured by drones present a unique set of challenges. Unlike urban or natural\nlandscapes commonly found in existing datasets, solar fields contain highly\nreflective surfaces, and domain-specific elements that are uncommon in\ntraditional computer vision benchmarks. As a result, machine learning models\ntrained on generic datasets struggle to generalize to this setting without\nextensive retraining and large volumes of annotated data. However, collecting\nand labeling such data is costly and time-consuming, making it impractical for\nrapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of\nAerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By\ngenerating synthetic data that closely mimic real-world conditions, our\nobjective is to facilitate pretraining of models before deployment,\nsignificantly reducing the need for extensive manual labeling. Our main\ncontributions are threefold: (1) we introduce AerialCSP, a high-quality\nsynthetic dataset for aerial inspection of CSP plants, providing annotated data\nfor object detection and image segmentation; (2) we benchmark multiple models\non AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we\ndemonstrate that pretraining on AerialCSP significantly improves real-world\nfault detection, particularly for rare and small defects, reducing the need for\nextensive manual labeling. AerialCSP is made publicly available at\nhttps://mpcutino.github.io/aerialcsp/.",
      "pdf_url": "http://arxiv.org/pdf/2508.00440v1",
      "published": "2025-08-01T08:57:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00440v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    }
  ]
}