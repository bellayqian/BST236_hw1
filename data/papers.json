{
  "last_updated": "2025-05-15T00:50:54.899992",
  "papers": [
    {
      "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
      "authors": [
        "Shanda Li",
        "Tanya Marwah",
        "Junhong Shen",
        "Weiwei Sun",
        "Andrej Risteski",
        "Yiming Yang",
        "Ameet Talwalkar"
      ],
      "abstract": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE.",
      "pdf_url": "http://arxiv.org/pdf/2505.08783v1",
      "published": "2025-05-13T17:58:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08783v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NA",
        "math.NA"
      ]
    },
    {
      "title": "ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus",
      "authors": [
        "Etienne Guichard",
        "Felix Reimers",
        "Mia Kvalsund",
        "Mikkel Lepperød",
        "Stefano Nichele"
      ],
      "abstract": "The Abstraction and Reasoning Corpus (ARC), later renamed ARC-AGI, poses a\nfundamental challenge in artificial general intelligence (AGI), requiring\nsolutions that exhibit robust abstraction and reasoning capabilities across\ndiverse tasks, while only few (with median count of three) correct examples are\npresented. While ARC-AGI remains very challenging for artificial intelligence\nsystems, it is rather easy for humans. This paper introduces ARC-NCA, a\ndevelopmental approach leveraging standard Neural Cellular Automata (NCA) and\nNCA enhanced with hidden memories (EngramNCA) to tackle the ARC-AGI benchmark.\nNCAs are employed for their inherent ability to simulate complex dynamics and\nemergent patterns, mimicking developmental processes observed in biological\nsystems. Developmental solutions may offer a promising avenue for enhancing\nAI's problem-solving capabilities beyond mere training data extrapolation.\nARC-NCA demonstrates how integrating developmental principles into\ncomputational models can foster adaptive reasoning and abstraction. We show\nthat our ARC-NCA proof-of-concept results may be comparable to, and sometimes\nsurpass, that of ChatGPT 4.5, at a fraction of the cost.",
      "pdf_url": "http://arxiv.org/pdf/2505.08778v1",
      "published": "2025-05-13T17:55:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08778v1",
      "categories": [
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology",
      "authors": [
        "Yatai Ji",
        "Zhengqiu Zhu",
        "Yong Zhao",
        "Beidan Liu",
        "Chen Gao",
        "Yihao Zhao",
        "Sihang Qiu",
        "Yue Hu",
        "Quanjun Yin",
        "Yong Li"
      ],
      "abstract": "Aerial Visual Object Search (AVOS) tasks in urban environments require\nUnmanned Aerial Vehicles (UAVs) to autonomously search for and identify target\nobjects using visual and textual cues without external guidance. Existing\napproaches struggle in complex urban environments due to redundant semantic\nprocessing, similar object distinction, and the exploration-exploitation\ndilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS,\nthe first benchmark dataset for autonomous search of common urban objects. This\ndataset comprises 2,420 tasks across six object categories with varying\ndifficulty levels, enabling comprehensive evaluation of UAV agents' search\ncapabilities. To solve the AVOS tasks, we also propose PRPSearcher\n(Perception-Reasoning-Planning Searcher), a novel agentic method powered by\nmulti-modal large language models (MLLMs) that mimics human three-tier\ncognition. Specifically, PRPSearcher constructs three specialized maps: an\nobject-centric dynamic semantic map enhancing spatial perception, a 3D\ncognitive map based on semantic attraction values for target reasoning, and a\n3D uncertainty map for balanced exploration-exploitation search. Also, our\napproach incorporates a denoising mechanism to mitigate interference from\nsimilar objects and utilizes an Inspiration Promote Thought (IPT) prompting\nmechanism for adaptive action planning. Experimental results on CityAVOS\ndemonstrate that PRPSearcher surpasses existing baselines in both success rate\nand search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and\n-46.40% NE). While promising, the performance gap compared to humans highlights\nthe need for better semantic reasoning and spatial exploration capabilities in\nAVOS tasks. This work establishes a foundation for future advances in embodied\ntarget search. Dataset and source code are available at\nhttps://anonymous.4open.science/r/CityAVOS-3DF8.",
      "pdf_url": "http://arxiv.org/pdf/2505.08765v2",
      "published": "2025-05-13T17:34:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08765v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion",
      "authors": [
        "Huiyan Qi",
        "Bin Zhu",
        "Chong-Wah Ngo",
        "Jingjing Chen",
        "Ee-Peng Lim"
      ],
      "abstract": "Nutrition estimation is an important component of promoting healthy eating\nand mitigating diet-related health risks. Despite advances in tasks such as\nfood classification and ingredient recognition, progress in nutrition\nestimation is limited due to the lack of datasets with nutritional annotations.\nTo address this issue, we introduce FastFood, a dataset with 84,446 images\nacross 908 fast food categories, featuring ingredient and nutritional\nannotations. In addition, we propose a new model-agnostic Visual-Ingredient\nFeature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating\nvisual and ingredient features. Ingredient robustness is improved through\nsynonym replacement and resampling strategies during training. The\ningredient-aware visual feature fusion module combines ingredient features and\nvisual representation to achieve accurate nutritional prediction. During\ntesting, ingredient predictions are refined using large multimodal models by\ndata augmentation and majority voting. Our experiments on both FastFood and\nNutrition5k datasets validate the effectiveness of our proposed method built in\ndifferent backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the\nimportance of ingredient information in nutrition estimation.\nhttps://huiyanqi.github.io/fastfood-nutrition-estimation/.",
      "pdf_url": "http://arxiv.org/pdf/2505.08747v1",
      "published": "2025-05-13T17:01:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08747v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models",
      "authors": [
        "Xiaoyang Chen",
        "Xinan Dai",
        "Yu Du",
        "Qian Feng",
        "Naixu Guo",
        "Tingshuo Gu",
        "Yuting Gao",
        "Yingyi Gao",
        "Xudong Han",
        "Xiang Jiang",
        "Yilin Jin",
        "Hongyi Lin",
        "Shisheng Lin",
        "Xiangnan Li",
        "Yuante Li",
        "Yixing Li",
        "Zhentao Lai",
        "Zilu Ma",
        "Yingrong Peng",
        "Jiacheng Qian",
        "Hao-Yu Sun",
        "Jianbo Sun",
        "Zirui Wang",
        "Siwei Wu",
        "Zian Wang",
        "Bin Xu",
        "Jianghao Xu",
        "Yiyang Yu",
        "Zichuan Yang",
        "Hongji Zha",
        "Ruichong Zhang"
      ],
      "abstract": "To advance the mathematical proficiency of large language models (LLMs), the\nDeepMath team has launched an open-source initiative aimed at developing an\nopen mathematical LLM and systematically evaluating its mathematical\ncreativity. This paper represents the initial contribution of this initiative.\nWhile recent developments in mathematical LLMs have predominantly emphasized\nreasoning skills, as evidenced by benchmarks on elementary to\nundergraduate-level mathematical tasks, the creative capabilities of these\nmodels have received comparatively little attention, and evaluation datasets\nremain scarce. To address this gap, we propose an evaluation criteria for\nmathematical creativity and introduce DeepMath-Creative, a novel, high-quality\nbenchmark comprising constructive problems across algebra, geometry, analysis,\nand other domains. We conduct a systematic evaluation of mainstream LLMs'\ncreative problem-solving abilities using this dataset. Experimental results\nshow that even under lenient scoring criteria -- emphasizing core solution\ncomponents and disregarding minor inaccuracies, such as small logical gaps,\nincomplete justifications, or redundant explanations -- the best-performing\nmodel, O3 Mini, achieves merely 70% accuracy, primarily on basic\nundergraduate-level constructive tasks. Performance declines sharply on more\ncomplex problems, with models failing to provide substantive strategies for\nopen problems. These findings suggest that, although current LLMs display a\ndegree of constructive proficiency on familiar and lower-difficulty problems,\nsuch performance is likely attributable to the recombination of memorized\npatterns rather than authentic creative insight or novel synthesis.",
      "pdf_url": "http://arxiv.org/pdf/2505.08744v1",
      "published": "2025-05-13T16:58:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08744v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Securing RAG: A Risk Assessment and Mitigation Framework",
      "authors": [
        "Lukas Ammann",
        "Sara Ott",
        "Christoph R. Landolt",
        "Marco P. Lehmann"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.08728v1",
      "published": "2025-05-13T16:39:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08728v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Memorization-Compression Cycles Improve Generalization",
      "authors": [
        "Fangyuan Yu"
      ],
      "abstract": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation.",
      "pdf_url": "http://arxiv.org/pdf/2505.08727v1",
      "published": "2025-05-13T16:37:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08727v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts",
      "authors": [
        "Yang Su",
        "Na Yan",
        "Yansha Deng",
        "Robert Schober"
      ],
      "abstract": "Large language models (LLMs) hosted on cloud servers alleviate the\ncomputational and storage burdens on local devices but raise privacy concerns\ndue to sensitive data transmission and require substantial communication\nbandwidth, which is challenging in constrained environments. In contrast, small\nlanguage models (SLMs) running locally enhance privacy but suffer from limited\nperformance on complex tasks. To balance computational cost, performance, and\nprivacy protection under bandwidth constraints, we propose a privacy-aware\nwireless collaborative mixture of experts (PWC-MoE) framework. Specifically,\nPWC-MoE employs a sparse privacy-aware gating network to dynamically route\nsensitive tokens to privacy experts located on local clients, while\nnon-sensitive tokens are routed to non-privacy experts located at the remote\nbase station. To achieve computational efficiency, the gating network ensures\nthat each token is dynamically routed to and processed by only one expert. To\nenhance scalability and prevent overloading of specific experts, we introduce a\ngroup-wise load-balancing mechanism for the gating network that evenly\ndistributes sensitive tokens among privacy experts and non-sensitive tokens\namong non-privacy experts. To adapt to bandwidth constraints while preserving\nmodel performance, we propose a bandwidth-adaptive and importance-aware token\noffloading scheme. This scheme incorporates an importance predictor to evaluate\nthe importance scores of non-sensitive tokens, prioritizing the most important\ntokens for transmission to the base station based on their predicted importance\nand the available bandwidth. Experiments demonstrate that the PWC-MoE framework\neffectively preserves privacy and maintains high performance even in\nbandwidth-constrained environments, offering a practical solution for deploying\nLLMs in privacy-sensitive and bandwidth-limited scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2505.08719v1",
      "published": "2025-05-13T16:27:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08719v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Big Data and the Computational Social Science of Entrepreneurship and Innovation",
      "authors": [
        "Ningzi Li",
        "Shiyang Lai",
        "James Evans"
      ],
      "abstract": "As large-scale social data explode and machine-learning methods evolve,\nscholars of entrepreneurship and innovation face new research opportunities but\nalso unique challenges. This chapter discusses the difficulties of leveraging\nlarge-scale data to identify technological and commercial novelty, document new\nventure origins, and forecast competition between new technologies and\ncommercial forms. It suggests how scholars can take advantage of new text,\nnetwork, image, audio, and video data in two distinct ways that advance\ninnovation and entrepreneurship research. First, machine-learning models,\ncombined with large-scale data, enable the construction of precision\nmeasurements that function as system-level observatories of innovation and\nentrepreneurship across human societies. Second, new artificial intelligence\nmodels fueled by big data generate 'digital doubles' of technology and\nbusiness, forming laboratories for virtual experimentation about innovation and\nentrepreneurship processes and policies. The chapter argues for the advancement\nof theory development and testing in entrepreneurship and innovation by\ncoupling big data with big models.",
      "pdf_url": "http://arxiv.org/pdf/2505.08706v1",
      "published": "2025-05-13T16:13:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08706v1",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY",
        "cs.SI",
        "q-fin.EC",
        "stat.AP"
      ]
    },
    {
      "title": "Controllable Image Colorization with Instance-aware Texts and Masks",
      "authors": [
        "Yanru An",
        "Ling Gui",
        "Qiang Hu",
        "Chunlei Cai",
        "Tianxiao Ye",
        "Xiaoyun Zhang",
        "Yanfeng Wang"
      ],
      "abstract": "Recently, the application of deep learning in image colorization has received\nwidespread attention. The maturation of diffusion models has further advanced\nthe development of image colorization models. However, current mainstream image\ncolorization models still face issues such as color bleeding and color binding\nerrors, and cannot colorize images at the instance level. In this paper, we\npropose a diffusion-based colorization method MT-Color to achieve precise\ninstance-aware colorization with use-provided guidance. To tackle color\nbleeding issue, we design a pixel-level mask attention mechanism that\nintegrates latent features and conditional gray image features through\ncross-attention. We use segmentation masks to construct cross-attention masks,\npreventing pixel information from exchanging between different instances. We\nalso introduce an instance mask and text guidance module that extracts instance\nmasks and text representations of each instance, which are then fused with\nlatent features through self-attention, utilizing instance masks to form\nself-attention masks to prevent instance texts from guiding the colorization of\nother areas, thus mitigating color binding errors. Furthermore, we apply a\nmulti-instance sampling strategy, which involves sampling each instance region\nseparately and then fusing the results. Additionally, we have created a\nspecialized dataset for instance-level colorization tasks, GPT-color, by\nleveraging large visual language models on existing image datasets. Qualitative\nand quantitative experiments show that our model and dataset outperform\nprevious methods and datasets.",
      "pdf_url": "http://arxiv.org/pdf/2505.08705v1",
      "published": "2025-05-13T16:13:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08705v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs",
      "authors": [
        "K M Sajjadul Islam",
        "Ayesha Siddika Nipu",
        "Jiawei Wu",
        "Praveen Madiraju"
      ],
      "abstract": "Electronic Health Records (EHRs) are digital records of patient information,\noften containing unstructured clinical text. Named Entity Recognition (NER) is\nessential in EHRs for extracting key medical entities like problems, tests, and\ntreatments to support downstream clinical applications. This paper explores\nprompt-based medical entity recognition using large language models (LLMs),\nspecifically GPT-4o and DeepSeek-R1, guided by various prompt engineering\ntechniques, including zero-shot, few-shot, and an ensemble approach. Among all\nstrategies, GPT-4o with prompt ensemble achieved the highest classification\nperformance with an F1-score of 0.95 and recall of 0.98, outperforming\nDeepSeek-R1 on the task. The ensemble method improved reliability by\naggregating outputs through embedding-based similarity and majority voting.",
      "pdf_url": "http://arxiv.org/pdf/2505.08704v1",
      "published": "2025-05-13T16:11:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08704v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A Survey of Deep Learning for Complex Speech Spectrograms",
      "authors": [
        "Yuying Xie",
        "Zheng-Hua Tan"
      ],
      "abstract": "Recent advancements in deep learning have significantly impacted the field of\nspeech signal processing, particularly in the analysis and manipulation of\ncomplex spectrograms. This survey provides a comprehensive overview of the\nstate-of-the-art techniques leveraging deep neural networks for processing\ncomplex spectrograms, which encapsulate both magnitude and phase information.\nWe begin by introducing complex spectrograms and their associated features for\nvarious speech processing tasks. Next, we explore the key components and\narchitectures of complex-valued neural networks, which are specifically\ndesigned to handle complex-valued data and have been applied for complex\nspectrogram processing. We then discuss various training strategies and loss\nfunctions tailored for training neural networks to process and model complex\nspectrograms. The survey further examines key applications, including phase\nretrieval, speech enhancement, and speech separation, where deep learning has\nachieved significant progress by leveraging complex spectrograms or their\nderived feature representations. Additionally, we examine the intersection of\ncomplex spectrograms with generative models. This survey aims to serve as a\nvaluable resource for researchers and practitioners in the field of speech\nsignal processing and complex-valued neural networks.",
      "pdf_url": "http://arxiv.org/pdf/2505.08694v1",
      "published": "2025-05-13T15:53:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08694v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "VizCV: AI-assisted visualization of researchers' publications tracks",
      "authors": [
        "Vladimír Lazárik",
        "Marco Agus",
        "Barbora Kozlíková",
        "Pere-Pau Vázquez"
      ],
      "abstract": "Analyzing how the publication records of scientists and research groups have\nevolved over the years is crucial for assessing their expertise since it can\nsupport the management of academic environments by assisting with career\nplanning and evaluation. We introduce VizCV, a novel web-based end-to-end\nvisual analytics framework that enables the interactive exploration of\nresearchers' scientific trajectories. It incorporates AI-assisted analysis and\nsupports automated reporting of career evolution. Our system aims to model\ncareer progression through three key dimensions: a) research topic evolution to\ndetect and visualize shifts in scholarly focus over time, b) publication record\nand the corresponding impact, c) collaboration dynamics depicting the growth\nand transformation of a researcher's co-authorship network. AI-driven insights\nprovide automated explanations of career transitions, detecting significant\nshifts in research direction, impact surges, or collaboration expansions. The\nsystem also supports comparative analysis between researchers, allowing users\nto compare topic trajectories and impact growth. Our interactive, multi-tab and\nmultiview system allows for the exploratory analysis of career milestones under\ndifferent perspectives, such as the most impactful articles, emerging research\nthemes, or obtaining a detailed analysis of the contribution of the researcher\nin a subfield. The key contributions include AI/ML techniques for: a) topic\nanalysis, b) dimensionality reduction for visualizing patterns and trends, c)\nthe interactive creation of textual descriptions of facets of data through\nconfigurable prompt generation and large language models, that include key\nindicators, to help understanding the career development of individuals or\ngroups.",
      "pdf_url": "http://arxiv.org/pdf/2505.08691v1",
      "published": "2025-05-13T15:47:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08691v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks",
      "authors": [
        "Hangwei Zhang",
        "Zhimu Huang",
        "Yan Wang"
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving\npartial differential equations (PDEs). Yet their original formulation is\ncomputationally and memory intensive, motivating the introduction of Chebyshev\nType-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed\nthe vanilla KANs architecture, our rigorous theoretical analysis reveals that\nthey still suffer from rank collapse, ultimately limiting their expressive\ncapacity. To overcome these limitations, we enhance Chebyshev1KANs by\nintegrating wavelet-activated MLPs with learnable parameters and an internal\nattention mechanism. We prove that this design preserves a full-rank Jacobian\nand is capable of approximating solutions to PDEs of arbitrary order.\nFurthermore, to alleviate the loss instability and imbalance introduced by the\nChebyshev polynomial basis, we externally incorporate a Residual Gradient\nAttention (RGA) mechanism that dynamically re-weights individual loss terms\naccording to their gradient norms and residual magnitudes. By jointly\nleveraging internal and external attention, we present AC-PKAN, a novel\narchitecture that constitutes an enhancement to weakly supervised\nPhysics-Informed Neural Networks (PINNs) and extends the expressive power of\nKANs. Experimental results from nine benchmark tasks across three domains show\nthat AC-PKAN consistently outperforms or matches state-of-the-art models such\nas PINNsFormer, establishing it as a highly effective tool for solving complex\nreal-world engineering problems in zero-data or data-sparse regimes. The code\nwill be made publicly available upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2505.08687v1",
      "published": "2025-05-13T15:46:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08687v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Mamba-based Network for Semi-supervised Singing Melody Extraction Using Confidence Binary Regularization",
      "authors": [
        "Xiaoliang He",
        "Kangjie Dong",
        "Jingkai Cao",
        "Shuai Yu",
        "Wei Li",
        "Yi Yu"
      ],
      "abstract": "Singing melody extraction (SME) is a key task in the field of music\ninformation retrieval. However, existing methods are facing several\nlimitations: firstly, prior models use transformers to capture the contextual\ndependencies, which requires quadratic computation resulting in low efficiency\nin the inference stage. Secondly, prior works typically rely on\nfrequencysupervised methods to estimate the fundamental frequency (f0), which\nignores that the musical performance is actually based on notes. Thirdly,\ntransformers typically require large amounts of labeled data to achieve optimal\nperformances, but the SME task lacks of sufficient annotated data. To address\nthese issues, in this paper, we propose a mamba-based network, called\nSpectMamba, for semi-supervised singing melody extraction using confidence\nbinary regularization. In particular, we begin by introducing vision mamba to\nachieve computational linear complexity. Then, we propose a novel note-f0\ndecoder that allows the model to better mimic the musical performance. Further,\nto alleviate the scarcity of the labeled data, we introduce a confidence binary\nregularization (CBR) module to leverage the unlabeled data by maximizing the\nprobability of the correct classes. The proposed method is evaluated on several\npublic datasets and the conducted experiments demonstrate the effectiveness of\nour proposed method.",
      "pdf_url": "http://arxiv.org/pdf/2505.08681v1",
      "published": "2025-05-13T15:43:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08681v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "A Study of Data-driven Methods for Inventory Optimization",
      "authors": [
        "Lee Yeung Ping",
        "Patrick Wong",
        "Tan Cheng Han"
      ],
      "abstract": "This paper shows a comprehensive analysis of three algorithms (Time Series,\nRandom Forest (RF) and Deep Reinforcement Learning) into three inventory models\n(the Lost Sales, Dual-Sourcing and Multi-Echelon Inventory Model). These\nmethodologies are applied in the supermarket context. The main purpose is to\nanalyse efficient methods for the data-driven. Their possibility, potential and\ncurrent challenges are taken into consideration in this report. By comparing\nthe results in each model, the effectiveness of each algorithm is evaluated\nbased on several key performance indicators, including forecast accuracy,\nadaptability to market changes, and overall impact on inventory costs and\ncustomer satisfaction levels. The data visualization tools and statistical\nmetrics are the indicators for the comparisons and show some obvious trends and\npatterns that can guide decision-making in inventory management. These tools\nenable managers to not only track the performance of different algorithms in\nreal-time but also to drill down into specific data points to understand the\nunderlying causes of inventory fluctuations. This level of detail is crucial\nfor pinpointing inefficiencies and areas for improvement within the supply\nchain.",
      "pdf_url": "http://arxiv.org/pdf/2505.08673v1",
      "published": "2025-05-13T15:35:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08673v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A Social Robot with Inner Speech for Dietary Guidance",
      "authors": [
        "Valerio Belcamino",
        "Alessandro Carfì",
        "Valeria Seidita",
        "Fulvio Mastrogiovanni",
        "Antonio Chella"
      ],
      "abstract": "We explore the use of inner speech as a mechanism to enhance transparency and\ntrust in social robots for dietary advice. In humans, inner speech structures\nthought processes and decision-making; in robotics, it improves explainability\nby making reasoning explicit. This is crucial in healthcare scenarios, where\ntrust in robotic assistants depends on both accurate recommendations and\nhuman-like dialogue, which make interactions more natural and engaging.\nBuilding on this, we developed a social robot that provides dietary advice, and\nwe provided the architecture with inner speech capabilities to validate user\ninput, refine reasoning, and generate clear justifications. The system\nintegrates large language models for natural language understanding and a\nknowledge graph for structured dietary information. By making decisions more\ntransparent, our approach strengthens trust and improves human-robot\ninteraction in healthcare. We validated this by measuring the computational\nefficiency of our architecture and conducting a small user study, which\nassessed the reliability of inner speech in explaining the robot's behavior.",
      "pdf_url": "http://arxiv.org/pdf/2505.08664v1",
      "published": "2025-05-13T15:26:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08664v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A Comparative Study of Human Activity Recognition: Motion, Tactile, and multi-modal Approaches",
      "authors": [
        "Valerio Belcamino",
        "Nhat Minh Dinh Le",
        "Quan Khanh Luu",
        "Alessandro Carfì",
        "Van Anh Ho",
        "Fulvio Mastrogiovanni"
      ],
      "abstract": "Human activity recognition (HAR) is essential for effective Human-Robot\nCollaboration (HRC), enabling robots to interpret and respond to human actions.\nThis study evaluates the ability of a vision-based tactile sensor to classify\n15 activities, comparing its performance to an IMU-based data glove.\nAdditionally, we propose a multi-modal framework combining tactile and motion\ndata to leverage their complementary strengths. We examined three approaches:\nmotion-based classification (MBC) using IMU data, tactile-based classification\n(TBC) with single or dual video streams, and multi-modal classification (MMC)\nintegrating both. Offline validation on segmented datasets assessed each\nconfiguration's accuracy under controlled conditions, while online validation\non continuous action sequences tested online performance. Results showed the\nmulti-modal approach consistently outperformed single-modality methods,\nhighlighting the potential of integrating tactile and motion sensing to enhance\nHAR systems for collaborative robotics.",
      "pdf_url": "http://arxiv.org/pdf/2505.08657v1",
      "published": "2025-05-13T15:20:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08657v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation",
      "authors": [
        "Dvir Cohen",
        "Lin Burg",
        "Sviatoslav Pykhnivskyi",
        "Hagit Gur",
        "Stanislav Kovynov",
        "Olga Atzmon",
        "Gilad Barkan"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is a cornerstone of modern question\nanswering (QA) systems, enabling grounded answers based on external knowledge.\nAlthough recent progress has been driven by open-domain datasets, enterprise QA\nsystems need datasets that mirror the concrete, domain-specific issues users\nraise in day-to-day support scenarios. Critically, evaluating end-to-end RAG\nsystems requires benchmarks comprising not only question--answer pairs but also\nthe specific knowledge base (KB) snapshot from which answers were derived. To\naddress this need, we introduce WixQA, a benchmark suite featuring QA datasets\nprecisely grounded in the released KB corpus, enabling holistic evaluation of\nretrieval and generation components. WixQA includes three distinct QA datasets\nderived from Wix.com customer support interactions and grounded in a snapshot\nof the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user\nqueries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200\nexpert-validated QA pairs distilled from user dialogues; and (iii)\nWixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically\nderived from each article in the knowledge base. We release the KB snapshot\nalongside the datasets under MIT license and provide comprehensive baseline\nresults, forming a unique benchmark for evaluating enterprise RAG systems in\nrealistic enterprise environments.",
      "pdf_url": "http://arxiv.org/pdf/2505.08643v1",
      "published": "2025-05-13T15:02:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08643v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
      "authors": [
        "Darshan Deshpande",
        "Varun Gangal",
        "Hersh Mehta",
        "Jitin Krishnan",
        "Anand Kannappan",
        "Rebecca Qian"
      ],
      "abstract": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.",
      "pdf_url": "http://arxiv.org/pdf/2505.08638v1",
      "published": "2025-05-13T14:55:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08638v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Integrating Natural Language Processing and Exercise Monitoring for Early Diagnosis of Metabolic Syndrome: A Deep Learning Approach",
      "authors": [
        "Yichen Zhao",
        "Yuhua Wang",
        "Xi Cheng",
        "Junhao Fang",
        "Yang Yang"
      ],
      "abstract": "Metabolic syndrome (MetS) is a medication condition characterized by\nabdominal obesity, insulin resistance, hypertension and hyperlipidemia. It\nincreases the risk of majority of chronic diseases, including type 2 diabetes\nmellitus, and affects about one quarter of the global population. Therefore,\nearly detection and timely intervention for MetS are crucial. Standard\ndiagnosis for MetS components requires blood tests conducted within medical\ninstitutions. However, it is frequently underestimated, leading to unmet need\nfor care for MetS population. This study aims to use the least physiological\ndata and free texts about exercises related activities, which are obtained\neasily in daily life, to diagnosis MetS. We collected the data from 40\nvolunteers in a nursing home and used data augmentation to reduce the\nimbalance. We propose a deep learning framework for classifying MetS that\nintegrates natural language processing (NLP) and exercise monitoring. The\nresults showed that the best model reported a high positive result (AUROC=0.806\nand REC=76.3%) through 3-fold cross-validation. Feature importance analysis\nrevealed that text and minimum heart rate on a daily basis contribute the most\nin the classification of MetS. This study demonstrates the potential\napplication of data that are easily measurable in daily life for the early\ndiagnosis of MetS, which could contribute to reducing the cost of screening and\nmanagement for MetS population.",
      "pdf_url": "http://arxiv.org/pdf/2505.08628v1",
      "published": "2025-05-13T14:48:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08628v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models",
      "authors": [
        "Donghoon Kim",
        "Minji Bae",
        "Kyuhong Shim",
        "Byonghyo Shim"
      ],
      "abstract": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models.",
      "pdf_url": "http://arxiv.org/pdf/2505.08622v1",
      "published": "2025-05-13T14:40:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08622v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "Resource-Efficient Language Models: Quantization for Fast and Accessible Inference",
      "authors": [
        "Tollef Emil Jørgensen"
      ],
      "abstract": "Large language models have significantly advanced natural language\nprocessing, yet their heavy resource demands pose severe challenges regarding\nhardware accessibility and energy consumption. This paper presents a focused\nand high-level review of post-training quantization (PTQ) techniques designed\nto optimize the inference efficiency of LLMs by the end-user, including details\non various quantization schemes, granularities, and trade-offs. The aim is to\nprovide a balanced overview between the theory and applications of\npost-training quantization.",
      "pdf_url": "http://arxiv.org/pdf/2505.08620v1",
      "published": "2025-05-13T14:39:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08620v1",
      "categories": [
        "cs.AI",
        "68T07",
        "I.2.0"
      ]
    },
    {
      "title": "MINIMALIST: switched-capacitor circuits for efficient in-memory computation of gated recurrent units",
      "authors": [
        "Sebastian Billaudelle",
        "Laura Kriener",
        "Filippo Moro",
        "Tristan Torchet",
        "Melika Payvand"
      ],
      "abstract": "Recurrent neural networks (RNNs) have been a long-standing candidate for\nprocessing of temporal sequence data, especially in memory-constrained systems\nthat one may find in embedded edge computing environments. Recent advances in\ntraining paradigms have now inspired new generations of efficient RNNs. We\nintroduce a streamlined and hardware-compatible architecture based on minimal\ngated recurrent units (GRUs), and an accompanying efficient mixed-signal\nhardware implementation of the model. The proposed design leverages\nswitched-capacitor circuits not only for in-memory computation (IMC), but also\nfor the gated state updates. The mixed-signal cores rely solely on commodity\ncircuits consisting of metal capacitors, transmission gates, and a clocked\ncomparator, thus greatly facilitating scaling and transfer to other technology\nnodes.\n  We benchmark the performance of our architecture on time series data,\nintroducing all constraints required for a direct mapping to the hardware\nsystem. The direct compatibility is verified in mixed-signal simulations,\nreproducing data recorded from the software-only network model.",
      "pdf_url": "http://arxiv.org/pdf/2505.08599v1",
      "published": "2025-05-13T14:13:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08599v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment",
      "authors": [
        "Barak Pinkovich",
        "Boaz Matalon",
        "Ehud Rivlin",
        "Hector Rotstein"
      ],
      "abstract": "This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI)\ndataset comprising 2525 images taken by a drone flying over dense urban\nenvironments. MESSI is unique in two main features. First, it contains images\nfrom various altitudes, allowing us to investigate the effect of depth on\nsemantic segmentation. Second, it includes images taken from several different\nurban regions (at different altitudes). This is important since the variety\ncovers the visual richness captured by a drone's 3D flight, performing\nhorizontal and vertical maneuvers. MESSI contains images annotated with\nlocation, orientation, and the camera's intrinsic parameters and can be used to\ntrain a deep neural network for semantic segmentation or other applications of\ninterest (e.g., localization, navigation, and tracking). This paper describes\nthe dataset and provides annotation details. It also explains how semantic\nsegmentation was performed using several neural network models and shows\nseveral relevant statistics. MESSI will be published in the public domain to\nserve as an evaluation benchmark for semantic segmentation using images\ncaptured by a drone or similar vehicle flying over a dense urban environment.",
      "pdf_url": "http://arxiv.org/pdf/2505.08589v1",
      "published": "2025-05-13T14:01:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08589v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Small but Significant: On the Promise of Small Language Models for Accessible AIED",
      "authors": [
        "Yumou Wei",
        "Paulo Carvalho",
        "John Stamper"
      ],
      "abstract": "GPT has become nearly synonymous with large language models (LLMs), an\nincreasingly popular term in AIED proceedings. A simple keyword-based search\nreveals that 61% of the 76 long and short papers presented at AIED 2024\ndescribe novel solutions using LLMs to address some of the long-standing\nchallenges in education, and 43% specifically mention GPT. Although LLMs\npioneered by GPT create exciting opportunities to strengthen the impact of AI\non education, we argue that the field's predominant focus on GPT and other\nresource-intensive LLMs (with more than 10B parameters) risks neglecting the\npotential impact that small language models (SLMs) can make in providing\nresource-constrained institutions with equitable and affordable access to\nhigh-quality AI tools. Supported by positive results on knowledge component\n(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as\nPhi-2 can produce an effective solution without elaborate prompting strategies.\nHence, we call for more attention to developing SLM-based AIED approaches.",
      "pdf_url": "http://arxiv.org/pdf/2505.08588v1",
      "published": "2025-05-13T13:58:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08588v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ]
    },
    {
      "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
      "authors": [
        "Haroon Wahab",
        "Hassan Ugail",
        "Irfan Mehmood"
      ],
      "abstract": "Recent proliferation of generative AI tools for visual content\ncreation-particularly in the context of visual artworks-has raised serious\nconcerns about copyright infringement and forgery. The large-scale datasets\nused to train these models often contain a mixture of copyrighted and\nnon-copyrighted artworks. Given the tendency of generative models to memorize\ntraining patterns, they are susceptible to varying degrees of copyright\nviolation. Building on the recently proposed DeepfakeArt Challenge benchmark,\nthis work introduces DFA-CON, a contrastive learning framework designed to\ndetect copyright-infringing or forged AI-generated art. DFA-CON learns a\ndiscriminative representation space, posing affinity among original artworks\nand their forged counterparts within a contrastive learning framework. The\nmodel is trained across multiple attack types, including inpainting, style\ntransfer, adversarial perturbation, and cutmix. Evaluation results demonstrate\nrobust detection performance across most attack types, outperforming recent\npretrained foundation models. Code and model checkpoints will be released\npublicly upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2505.08552v1",
      "published": "2025-05-13T13:23:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08552v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation",
      "authors": [
        "Yifu Yuan",
        "Haiqin Cui",
        "Yibin Chen",
        "Zibin Dong",
        "Fei Ni",
        "Longxin Kou",
        "Jinyi Liu",
        "Pengyi Li",
        "Yan Zheng",
        "Jianye Hao"
      ],
      "abstract": "Achieving generalization in robotic manipulation remains a critical\nchallenge, particularly for unseen scenarios and novel tasks. Current\nVision-Language-Action (VLA) models, while building on top of general\nVision-Language Models (VLMs), still fall short of achieving robust zero-shot\nperformance due to the scarcity and heterogeneity prevalent in embodied\ndatasets. To address these limitations, we propose FSD (From Seeing to Doing),\na novel vision-language model that generates intermediate representations\nthrough spatial relationship reasoning, providing fine-grained guidance for\nrobotic manipulation. Our approach combines a hierarchical data pipeline for\ntraining with a self-consistency mechanism that aligns spatial coordinates with\nvisual signals. Through extensive experiments, we comprehensively validated\nFSD's capabilities in both \"seeing\" and \"doing,\" achieving outstanding\nperformance across 8 benchmarks for general spatial reasoning and embodied\nreference abilities, as well as on our proposed more challenging benchmark\nVABench. We also verified zero-shot capabilities in robot manipulation,\ndemonstrating significant performance improvements over baseline methods in\nboth SimplerEnv and real robot settings. Experimental results show that FSD\nachieves 54.1% success rate in SimplerEnv and 72% success rate across 8\nreal-world tasks, outperforming the strongest baseline by 30%.",
      "pdf_url": "http://arxiv.org/pdf/2505.08548v1",
      "published": "2025-05-13T13:20:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08548v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Guiding LLM-based Smart Contract Generation with Finite State Machine",
      "authors": [
        "Hao Luo",
        "Yuhao Lin",
        "Xiao Yan",
        "Xintong Hu",
        "Yuxiang Wang",
        "Qiming Zeng",
        "Hao Wang",
        "Jiawei Jiang"
      ],
      "abstract": "Smart contract is a kind of self-executing code based on blockchain\ntechnology with a wide range of application scenarios, but the traditional\ngeneration method relies on manual coding and expert auditing, which has a high\nthreshold and low efficiency. Although Large Language Models (LLMs) show great\npotential in programming tasks, they still face challenges in smart contract\ngeneration w.r.t. effectiveness and security. To solve these problems, we\npropose FSM-SCG, a smart contract generation framework based on finite state\nmachine (FSM) and LLMs, which significantly improves the quality of the\ngenerated code by abstracting user requirements to generate FSM, guiding LLMs\nto generate smart contracts, and iteratively optimizing the code with the\nfeedback of compilation and security checks. The experimental results show that\nFSM-SCG significantly improves the quality of smart contract generation.\nCompared to the best baseline, FSM-SCG improves the compilation success rate of\ngenerated smart contract code by at most 48%, and reduces the average\nvulnerability risk score by approximately 68%.",
      "pdf_url": "http://arxiv.org/pdf/2505.08542v1",
      "published": "2025-05-13T13:13:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08542v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News",
      "authors": [
        "Yuhan Liu",
        "Yuxuan Liu",
        "Xiaoqing Zhang",
        "Xiuying Chen",
        "Rui Yan"
      ],
      "abstract": "In today's digital environment, the rapid propagation of fake news via social\nnetworks poses significant social challenges. Most existing detection methods\neither employ traditional classification models, which suffer from low\ninterpretability and limited generalization capabilities, or craft specific\nprompts for large language models (LLMs) to produce explanations and results\ndirectly, failing to leverage LLMs' reasoning abilities fully. Inspired by the\nsaying that \"truth becomes clearer through debate,\" our study introduces a\nnovel multi-agent system with LLMs named TruEDebate (TED) to enhance the\ninterpretability and effectiveness of fake news detection. TED employs a\nrigorous debate process inspired by formal debate settings. Central to our\napproach are two innovative components: the DebateFlow Agents and the\nInsightFlow Agents. The DebateFlow Agents organize agents into two teams, where\none supports and the other challenges the truth of the news. These agents\nengage in opening statements, cross-examination, rebuttal, and closing\nstatements, simulating a rigorous debate process akin to human discourse\nanalysis, allowing for a thorough evaluation of news content. Concurrently, the\nInsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent\nand the Analysis Agent. The Synthesis Agent summarizes the debates and provides\nan overarching viewpoint, ensuring a coherent and comprehensive evaluation. The\nAnalysis Agent, which includes a role-aware encoder and a debate graph,\nintegrates role embeddings and models the interactions between debate roles and\narguments using an attention mechanism, providing the final judgment.",
      "pdf_url": "http://arxiv.org/pdf/2505.08532v1",
      "published": "2025-05-13T13:03:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08532v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "ExEBench: Benchmarking Foundation Models on Extreme Earth Events",
      "authors": [
        "Shan Zhao",
        "Zhitong Xiong",
        "Jie Zhao",
        "Xiao Xiang Zhu"
      ],
      "abstract": "Our planet is facing increasingly frequent extreme events, which pose major\nrisks to human lives and ecosystems. Recent advances in machine learning (ML),\nespecially with foundation models (FMs) trained on extensive datasets, excel in\nextracting features and show promise in disaster management. Nevertheless,\nthese models often inherit biases from training data, challenging their\nperformance over extreme values. To explore the reliability of FM in the\ncontext of extreme events, we introduce \\textbf{ExE}Bench (\\textbf{Ex}treme\n\\textbf{E}arth Benchmark), a collection of seven extreme event categories\nacross floods, wildfires, storms, tropical cyclones, extreme precipitation,\nheatwaves, and cold waves. The dataset features global coverage, varying data\nvolumes, and diverse data sources with different spatial, temporal, and\nspectral characteristics. To broaden the real-world impact of FMs, we include\nmultiple challenging ML tasks that are closely aligned with operational needs\nin extreme events detection, monitoring, and forecasting. ExEBench aims to (1)\nassess FM generalizability across diverse, high-impact tasks and domains, (2)\npromote the development of novel ML methods that benefit disaster management,\nand (3) offer a platform for analyzing the interactions and cascading effects\nof extreme events to advance our understanding of Earth system, especially\nunder the climate change expected in the decades to come. The dataset and code\nare public https://github.com/zhaoshan2/EarthExtreme-Bench.",
      "pdf_url": "http://arxiv.org/pdf/2505.08529v1",
      "published": "2025-05-13T13:02:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08529v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning",
      "authors": [
        "Minsu Kim",
        "Seong-Hyeon Hwang",
        "Steven Euijong Whang"
      ],
      "abstract": "In the context of continual learning, acquiring new knowledge while\nmaintaining previous knowledge presents a significant challenge. Existing\nmethods often use experience replay techniques that store a small portion of\nprevious task data for training. In experience replay approaches, data\naugmentation has emerged as a promising strategy to further improve the model\nperformance by mixing limited previous task data with sufficient current task\ndata. However, we theoretically and empirically analyze that training with\nmixed samples from random sample pairs may harm the knowledge of previous tasks\nand cause greater catastrophic forgetting. We then propose GradMix, a robust\ndata augmentation method specifically designed for mitigating catastrophic\nforgetting in class-incremental learning. GradMix performs gradient-based\nselective mixup using a class-based criterion that mixes only samples from\nhelpful class pairs and not from detrimental class pairs for reducing\ncatastrophic forgetting. Our experiments on various real datasets show that\nGradMix outperforms data augmentation baselines in accuracy by minimizing the\nforgetting of previous knowledge.",
      "pdf_url": "http://arxiv.org/pdf/2505.08528v1",
      "published": "2025-05-13T13:01:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08528v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "On the Complexity and Properties of Preferential Propositional Dependence Logic",
      "authors": [
        "Kai Sauerwald",
        "Arne Meier",
        "Juha Kontinen"
      ],
      "abstract": "This paper considers the complexity and properties of KLM-style preferential\nreasoning in the setting of propositional logic with team semantics and\ndependence atoms, also known as propositional dependence logic. Preferential\nteam-based reasoning is shown to be cumulative, yet violates System~P. We give\nintuitive conditions that fully characterise those cases where preferential\npropositional dependence logic satisfies System~P. We show that these\ncharacterisations do, surprisingly, not carry over to preferential team-based\npropositional logic. Furthermore, we show how classical entailment and\ndependence logic entailment can be expressed in terms of non-trivial\npreferential models. Finally, we present the complexity of preferential\nteam-based reasoning for two natural representations. This includes novel\ncomplexity results for classical (non-team-based) preferential reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2505.08522v1",
      "published": "2025-05-13T12:54:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08522v1",
      "categories": [
        "cs.AI",
        "cs.LO",
        "03B70, 03B62",
        "I.2.3; F.4.1"
      ]
    },
    {
      "title": "Learning Advanced Self-Attention for Linear Transformers in the Singular Value Domain",
      "authors": [
        "Hyowon Wi",
        "Jeongwhan Choi",
        "Noseong Park"
      ],
      "abstract": "Transformers have demonstrated remarkable performance across diverse domains.\nThe key component of Transformers is self-attention, which learns the\nrelationship between any two tokens in the input sequence. Recent studies have\nrevealed that the self-attention can be understood as a normalized adjacency\nmatrix of a graph. Notably, from the perspective of graph signal processing\n(GSP), the self-attention can be equivalently defined as a simple graph filter,\napplying GSP using the value vector as the signal. However, the self-attention\nis a graph filter defined with only the first order of the polynomial matrix,\nand acts as a low-pass filter preventing the effective leverage of various\nfrequency information. Consequently, existing self-attention mechanisms are\ndesigned in a rather simplified manner. Therefore, we propose a novel method,\ncalled \\underline{\\textbf{A}}ttentive \\underline{\\textbf{G}}raph\n\\underline{\\textbf{F}}ilter (AGF), interpreting the self-attention as learning\nthe graph filter in the singular value domain from the perspective of graph\nsignal processing for directed graphs with the linear complexity w.r.t. the\ninput length $n$, i.e., $\\mathcal{O}(nd^2)$. In our experiments, we demonstrate\nthat AGF achieves state-of-the-art performance on various tasks, including Long\nRange Arena benchmark and time series classification.",
      "pdf_url": "http://arxiv.org/pdf/2505.08516v1",
      "published": "2025-05-13T12:48:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08516v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation System to Streamline Patient-to-Trial Matching",
      "authors": [
        "Majd Abdallah",
        "Sigve Nakken",
        "Mariska Bierkens",
        "Johanna Galvis",
        "Alexis Groppi",
        "Slim Karkar",
        "Lana Meiqari",
        "Maria Alexandra Rujano",
        "Steve Canham",
        "Rodrigo Dienstmann",
        "Remond Fijneman",
        "Eivind Hovig",
        "Gerrit Meijer",
        "Macha Nikolski"
      ],
      "abstract": "Patient recruitment remains a major bottleneck in clinical trials, calling\nfor scalable and automated solutions. We present TrialMatchAI, an AI-powered\nrecommendation system that automates patient-to-trial matching by processing\nheterogeneous clinical data, including structured records and unstructured\nphysician notes. Built on fine-tuned, open-source large language models (LLMs)\nwithin a retrieval-augmented generation framework, TrialMatchAI ensures\ntransparency and reproducibility and maintains a lightweight deployment\nfootprint suitable for clinical environments. The system normalizes biomedical\nentities, retrieves relevant trials using a hybrid search strategy combining\nlexical and semantic similarity, re-ranks results, and performs criterion-level\neligibility assessments using medical Chain-of-Thought reasoning. This pipeline\ndelivers explainable outputs with traceable decision rationales. In real-world\nvalidation, 92 percent of oncology patients had at least one relevant trial\nretrieved within the top 20 recommendations. Evaluation across synthetic and\nreal clinical datasets confirmed state-of-the-art performance, with expert\nassessment validating over 90 percent accuracy in criterion-level eligibility\nclassification, particularly excelling in biomarker-driven matches. Designed\nfor modularity and privacy, TrialMatchAI supports Phenopackets-standardized\ndata, enables secure local deployment, and allows seamless replacement of LLM\ncomponents as more advanced models emerge. By enhancing efficiency and\ninterpretability and offering lightweight, open-source deployment, TrialMatchAI\nprovides a scalable solution for AI-driven clinical trial matching in precision\nmedicine.",
      "pdf_url": "http://arxiv.org/pdf/2505.08508v1",
      "published": "2025-05-13T12:39:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08508v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ]
    },
    {
      "title": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models",
      "authors": [
        "Takumi Shibata",
        "Yuichi Miyamura"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled zero-shot\nautomated essay scoring (AES), providing a promising way to reduce the cost and\neffort of essay scoring in comparison with manual grading. However, most\nexisting zero-shot approaches rely on LLMs to directly generate absolute\nscores, which often diverge from human evaluations owing to model biases and\ninconsistent scoring. To address these limitations, we propose LLM-based\nComparative Essay Scoring (LCES), a method that formulates AES as a pairwise\ncomparison task. Specifically, we instruct LLMs to judge which of two essays is\nbetter, collect many such comparisons, and convert them into continuous scores.\nConsidering that the number of possible comparisons grows quadratically with\nthe number of essays, we improve scalability by employing RankNet to\nefficiently transform LLM preferences into scalar scores. Experiments using AES\nbenchmark datasets show that LCES outperforms conventional zero-shot methods in\naccuracy while maintaining computational efficiency. Moreover, LCES is robust\nacross different LLM backbones, highlighting its applicability to real-world\nzero-shot AES.",
      "pdf_url": "http://arxiv.org/pdf/2505.08498v1",
      "published": "2025-05-13T12:26:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08498v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Achieving Scalable Robot Autonomy via neurosymbolic planning using lightweight local LLM",
      "authors": [
        "Nicholas Attolino",
        "Alessio Capitanelli",
        "Fulvio Mastrogiovanni"
      ],
      "abstract": "PDDL-based symbolic task planning remains pivotal for robot autonomy yet\nstruggles with dynamic human-robot collaboration due to scalability,\nre-planning demands, and delayed plan availability. Although a few\nneurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to\naddress these challenges, reliance on closed-source, remote models with limited\ncontext introduced critical constraints: third-party dependency, inconsistent\nresponse times, restricted plan length and complexity, and multi-domain\nscalability issues. We present Gideon, a novel framework that enables the\ntransition to modern, smaller, local LLMs with extended context length. Gideon\nintegrates a novel problem generator to systematically generate large-scale\ndatasets of realistic domain-problem-plan tuples for any domain, and adapts\nneurosymbolic planning for local LLMs, enabling on-device execution and\nextended context for multi-domain support. Preliminary experiments in\nsingle-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k\nsamples, demonstrate a valid plan percentage of 66.1% (32k model) and show that\nthe figure can be further scaled through additional data. Multi-domain tests on\n16k samples yield an even higher 70.6% planning validity rate, proving\nextensibility across domains and signaling that data variety can have a\npositive effect on learning efficiency. Although long-horizon planning and\nreduced model size make Gideon training much less efficient than baseline\nmodels based on larger LLMs, the results are still significant considering that\nthe trained model is about 120x smaller than baseline and that significant\nadvantages can be achieved in inference efficiency, scalability, and\nmulti-domain adaptability, all critical factors in human-robot collaboration.\nTraining inefficiency can be mitigated by Gideon's streamlined data generation\npipeline.",
      "pdf_url": "http://arxiv.org/pdf/2505.08492v1",
      "published": "2025-05-13T12:22:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08492v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "I.2.6; I.2.8; I.2.9"
      ]
    },
    {
      "title": "An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling",
      "authors": [
        "Chetra Mang",
        "Axel TahmasebiMoradi",
        "David Danan",
        "Mouadh Yagoubi"
      ],
      "abstract": "Physical models classically involved Partial Differential equations (PDE) and\ndepending of their underlying complexity and the level of accuracy required,\nand known to be computationally expensive to numerically solve them. Thus, an\nidea would be to create a surrogate model relying on data generated by such\nsolver. However, training such a model on an imbalanced data have been shown to\nbe a very difficult task. Indeed, if the distribution of input leads to a poor\nresponse manifold representation, the model may not learn well and\nconsequently, it may not predict the outcome with acceptable accuracy. In this\nwork, we present an Adaptive Sampling Algorithm for Data Generation (ASADG)\ninvolving a physical model. As the initial input data may not accurately\nrepresent the response manifold in higher dimension, this algorithm iteratively\nadds input data into it. At each step the barycenter of each simplicial\ncomplex, that the manifold is discretized into, is added as new input data, if\na certain threshold is satisfied. We demonstrate the efficiency of the data\nsampling algorithm in comparison with LHS method for generating more\nrepresentative input data. To do so, we focus on the construction of a harmonic\ntransport problem metamodel by generating data through a classical solver. By\nusing such algorithm, it is possible to generate the same number of input data\nas LHS while providing a better representation of the response manifold.",
      "pdf_url": "http://arxiv.org/pdf/2505.08487v1",
      "published": "2025-05-13T12:17:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08487v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "BAT: Benchmark for Auto-bidding Task",
      "authors": [
        "Alexandra Khirianova",
        "Ekaterina Solodneva",
        "Andrey Pudovikov",
        "Sergey Osokin",
        "Egor Samosvat",
        "Yuriy Dorn",
        "Alexander Ledovsky",
        "Yana Zenkova"
      ],
      "abstract": "The optimization of bidding strategies for online advertising slot auctions\npresents a critical challenge across numerous digital marketplaces. A\nsignificant obstacle to the development, evaluation, and refinement of\nreal-time autobidding algorithms is the scarcity of comprehensive datasets and\nstandardized benchmarks.\n  To address this deficiency, we present an auction benchmark encompassing the\ntwo most prevalent auction formats. We implement a series of robust baselines\non a novel dataset, addressing the most salient Real-Time Bidding (RTB) problem\ndomains: budget pacing uniformity and Cost Per Click (CPC) constraint\noptimization. This benchmark provides a user-friendly and intuitive framework\nfor researchers and practitioners to develop and refine innovative autobidding\nalgorithms, thereby facilitating advancements in the field of programmatic\nadvertising. The implementation and additional resources can be accessed at the\nfollowing repository (https://github.com/avito-tech/bat-autobidding-benchmark,\nhttps://doi.org/10.5281/zenodo.14794182).",
      "pdf_url": "http://arxiv.org/pdf/2505.08485v1",
      "published": "2025-05-13T12:12:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08485v1",
      "categories": [
        "cs.AI",
        "stat.ML",
        "91B26"
      ]
    },
    {
      "title": "Distributed Quantum Neural Networks on Distributed Photonic Quantum Computing",
      "authors": [
        "Kuan-Cheng Chen",
        "Chen-Yu Liu",
        "Yu Shang",
        "Felix Burt",
        "Kin K. Leung"
      ],
      "abstract": "We introduce a distributed quantum-classical framework that synergizes\nphotonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping\nto achieve parameter-efficient training of classical neural networks. By\nleveraging universal linear-optical decompositions of $M$-mode interferometers\nand photon-counting measurement statistics, our architecture generates neural\nparameters through a hybrid quantum-classical workflow: photonic QNNs with\n$M(M+1)/2$ trainable parameters produce high-dimensional probability\ndistributions that are mapped to classical network weights via an MPS model\nwith bond dimension $\\chi$. Empirical validation on MNIST classification\ndemonstrates that photonic QT achieves an accuracy of $95.50\\% \\pm 0.84\\%$\nusing 3,292 parameters ($\\chi = 10$), compared to $96.89\\% \\pm 0.31\\%$ for\nclassical baselines with 6,690 parameters. Moreover, a ten-fold compression\nratio is achieved at $\\chi = 4$, with a relative accuracy loss of less than\n$3\\%$. The framework outperforms classical compression techniques (weight\nsharing/pruning) by 6--12\\% absolute accuracy while eliminating quantum\nhardware requirements during inference through classical deployment of\ncompressed parameters. Simulations incorporating realistic photonic noise\ndemonstrate the framework's robustness to near-term hardware imperfections.\nAblation studies confirm quantum necessity: replacing photonic QNNs with random\ninputs collapses accuracy to chance level ($10.0\\% \\pm 0.5\\%$). Photonic\nquantum computing's room-temperature operation, inherent scalability through\nspatial-mode multiplexing, and HPC-integrated architecture establish a\npractical pathway for distributed quantum machine learning, combining the\nexpressivity of photonic Hilbert spaces with the deployability of classical\nneural networks.",
      "pdf_url": "http://arxiv.org/pdf/2505.08474v1",
      "published": "2025-05-13T11:58:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08474v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models",
      "authors": [
        "Fujun Zhang",
        "XiangDong Su"
      ],
      "abstract": "Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm\nin applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs\nstill struggle with the discrepancies between the representation obtained from\nthe PLMs' encoder and the optimal input to the PLMs' decoder. This paper\ntackles this challenge by learning to calibrate the representation of PLMs in\nthe latent space. In the proposed representation calibration method (RepCali),\nwe integrate a specific calibration block to the latent space after the encoder\nand use the calibrated output as the decoder input. The merits of the proposed\nRepCali include its universality to all PLMs with encoder-decoder\narchitectures, its plug-and-play nature, and ease of implementation. Extensive\nexperiments on 25 PLM-based models across 8 tasks (including both English and\nChinese datasets) demonstrate that the proposed RepCali offers desirable\nenhancements to PLMs (including LLMs) and significantly improves the\nperformance of downstream tasks. Comparison experiments across 4 benchmark\ntasks indicate that RepCali is superior to the representative fine-tuning\nbaselines.",
      "pdf_url": "http://arxiv.org/pdf/2505.08463v1",
      "published": "2025-05-13T11:47:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08463v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Strategy-Augmented Planning for Large Language Models via Opponent Exploitation",
      "authors": [
        "Shuai Xu",
        "Sijia Cui",
        "Yanna Wang",
        "Bo Xu",
        "Qi Wang"
      ],
      "abstract": "Efficiently modeling and exploiting opponents is a long-standing challenge in\nadversarial domains. Large Language Models (LLMs) trained on extensive textual\ndata have recently demonstrated outstanding performance in general tasks,\nintroducing new research directions for opponent modeling. Some studies\nprimarily focus on directly using LLMs to generate decisions based on the\nelaborate prompt context that incorporates opponent descriptions, while these\napproaches are limited to scenarios where LLMs possess adequate domain\nexpertise. To address that, we introduce a two-stage Strategy-Augmented\nPlanning (SAP) framework that significantly enhances the opponent exploitation\ncapabilities of LLM-based agents by utilizing a critical component, the\nStrategy Evaluation Network (SEN). Specifically, in the offline stage, we\nconstruct an explicit strategy space and subsequently collect strategy-outcome\npair data for training the SEN network. During the online phase, SAP\ndynamically recognizes the opponent's strategies and greedily exploits them by\nsearching best response strategy on the well-trained SEN, finally translating\nstrategy to a course of actions by carefully designed prompts. Experimental\nresults show that SAP exhibits robust generalization capabilities, allowing it\nto perform effectively not only against previously encountered opponent\nstrategies but also against novel, unseen strategies. In the MicroRTS\nenvironment, SAP achieves a 85.35\\% performance improvement over baseline\nmethods and matches the competitiveness of reinforcement learning approaches\nagainst state-of-the-art (SOTA) rule-based AI.",
      "pdf_url": "http://arxiv.org/pdf/2505.08459v1",
      "published": "2025-05-13T11:41:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08459v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem",
      "authors": [
        "Lotfi Kobrosly",
        "Marc-Emmanuel Coupvent des Graviers",
        "Christophe Guettier",
        "Tristan Cazenave"
      ],
      "abstract": "The Flexible Job-Shop Scheduling Problem (FJSSP) is an NP-hard combinatorial\noptimization problem, with several application domains, especially for\nmanufacturing purposes. The objective is to\n  efficiently schedule multiple operations on dissimilar machines. These\noperations are gathered into jobs, and operations pertaining to the same job\nneed to be scheduled sequentially. Different methods have been previously\ntested to solve this problem, such as Constraint Solving, Tabu Search, Genetic\nAlgorithms, or Monte Carlo Tree Search (MCTS). We propose a novel algorithm\nderived from the Generalized Nested Rollout Policy Adaptation, developed to\nsolve the FJSSP. We report encouraging experimental results, as our algorithm\nperforms better than other MCTS-based approaches, even if makespans obtained on\nlarge instances are still far from known upper bounds.",
      "pdf_url": "http://arxiv.org/pdf/2505.08451v1",
      "published": "2025-05-13T11:27:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08451v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Agent-as-a-Service based on Agent Network",
      "authors": [
        "Yuhan Zhu",
        "Haojie Liu",
        "Jian Wang",
        "Bing Li",
        "Zikang Yin",
        "Yefei Liao"
      ],
      "abstract": "The rise of large model-based AI agents has spurred interest in Multi-Agent\nSystems (MAS) for their capabilities in decision-making, collaboration, and\nadaptability. While the Model Context Protocol (MCP) addresses tool invocation\nand data exchange challenges via a unified protocol, it lacks support for\norganizing agent-level collaboration. To bridge this gap, we propose\nAgent-as-a-Service based on Agent Network (AaaS-AN), a service-oriented\nparadigm grounded in the Role-Goal-Process-Service (RGPS) standard. AaaS-AN\nunifies the entire agent lifecycle, including construction, integration,\ninteroperability, and networked collaboration, through two core components: (1)\na dynamic Agent Network, which models agents and agent groups as vertexes that\nself-organize within the network based on task and role dependencies; (2)\nservice-oriented agents, incorporating service discovery, registration, and\ninteroperability protocols. These are orchestrated by a Service Scheduler,\nwhich leverages an Execution Graph to enable distributed coordination, context\ntracking, and runtime task management. We validate AaaS-AN on mathematical\nreasoning and application-level code generation tasks, which outperforms\nstate-of-the-art baselines. Notably, we constructed a MAS based on AaaS-AN\ncontaining agent groups, Robotic Process Automation (RPA) workflows, and MCP\nservers over 100 agent services. We also release a dataset containing 10,000\nlong-horizon multi-agent workflows to facilitate future research on long-chain\ncollaboration in MAS.",
      "pdf_url": "http://arxiv.org/pdf/2505.08446v1",
      "published": "2025-05-13T11:15:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08446v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency",
      "authors": [
        "Adel Ammar",
        "Anis Koubaa",
        "Omer Nacar",
        "Wadii Boulila"
      ],
      "abstract": "Large language models achieve high task performance yet often hallucinate or\nrely on outdated knowledge. Retrieval-augmented generation (RAG) addresses\nthese gaps by coupling generation with external search. We analyse how\nhyperparameters influence speed and quality in RAG systems, covering Chroma and\nFaiss vector stores, chunking policies, cross-encoder re-ranking, and\ntemperature, and we evaluate six metrics: faithfulness, answer correctness,\nanswer relevancy, context precision, context recall, and answer similarity.\nChroma processes queries 13% faster, whereas Faiss yields higher retrieval\nprecision, revealing a clear speed-accuracy trade-off. Naive fixed-length\nchunking with small windows and minimal overlap outperforms semantic\nsegmentation while remaining the quickest option. Re-ranking provides modest\ngains in retrieval quality yet increases runtime by roughly a factor of 5, so\nits usefulness depends on latency constraints. These results help practitioners\nbalance computational cost and accuracy when tuning RAG systems for\ntransparent, up-to-date responses. Finally, we re-evaluate the top\nconfigurations with a corrective RAG workflow and show that their advantages\npersist when the model can iteratively request additional evidence. We obtain a\nnear-perfect context precision (99%), which demonstrates that RAG systems can\nachieve extremely high retrieval accuracy with the right combination of\nhyperparameters, with significant implications for applications where retrieval\nquality directly impacts downstream task performance, such as clinical decision\nsupport in healthcare.",
      "pdf_url": "http://arxiv.org/pdf/2505.08445v1",
      "published": "2025-05-13T11:13:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08445v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering",
      "authors": [
        "Chuanzhi Xu",
        "Haoxian Zhou",
        "Langyi Chen",
        "Haodong Chen",
        "Ying Zhou",
        "Vera Chung",
        "Qiang Qu"
      ],
      "abstract": "Event cameras have emerged as promising sensors for 3D reconstruction due to\ntheir ability to capture per-pixel brightness changes asynchronously. Unlike\nconventional frame-based cameras, they produce sparse and temporally rich data\nstreams, which enable more accurate 3D reconstruction and open up the\npossibility of performing reconstruction in extreme environments such as\nhigh-speed motion, low light, or high dynamic range scenes. In this survey, we\nprovide the first comprehensive review focused exclusively on 3D reconstruction\nusing event cameras. The survey categorises existing works into three major\ntypes based on input modality - stereo, monocular, and multimodal systems, and\nfurther classifies them by reconstruction approach, including geometry-based,\ndeep learning-based, and recent neural rendering techniques such as Neural\nRadiance Fields and 3D Gaussian Splatting. Methods with a similar research\nfocus were organised chronologically into the most subdivided groups. We also\nsummarise public datasets relevant to event-based 3D reconstruction. Finally,\nwe highlight current research limitations in data availability, evaluation,\nrepresentation, and dynamic scene handling, and outline promising future\nresearch directions. This survey aims to serve as a comprehensive reference and\na roadmap for future developments in event-driven 3D reconstruction.",
      "pdf_url": "http://arxiv.org/pdf/2505.08438v1",
      "published": "2025-05-13T11:04:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08438v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Hakim: Farsi Text Embedding Model",
      "authors": [
        "Mehran Sarmadi",
        "Morteza Alikhani",
        "Erfan Zinvandi",
        "Zahra Pourbahman"
      ],
      "abstract": "Recent advancements in text embedding have significantly improved natural\nlanguage understanding across many languages, yet Persian remains notably\nunderrepresented in large-scale embedding research. In this paper, we present\nHakim, a novel state-of-the-art Persian text embedding model that achieves a\n8.5% performance improvement over existing approaches on the FaMTEB benchmark,\noutperforming all previously developed Persian language models. As part of this\nwork, we introduce three new datasets - Corpesia, Pairsia-sup, and\nPairsia-unsup - to support supervised and unsupervised training scenarios.\nAdditionally, Hakim is designed for applications in chatbots and\nretrieval-augmented generation (RAG) systems, particularly addressing retrieval\ntasks that require incorporating message history within these systems. We also\npropose a new baseline model built on the BERT architecture. Our language model\nconsistently achieves higher accuracy across various Persian NLP tasks, while\nthe RetroMAE-based model proves particularly effective for textual information\nretrieval applications. Together, these contributions establish a new\nfoundation for advancing Persian language understanding.",
      "pdf_url": "http://arxiv.org/pdf/2505.08435v2",
      "published": "2025-05-13T10:57:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08435v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Explaining Autonomous Vehicles with Intention-aware Policy Graphs",
      "authors": [
        "Sara Montese",
        "Victor Gimenez-Abalos",
        "Atia Cortés",
        "Ulises Cortés",
        "Sergio Alvarez-Napagao"
      ],
      "abstract": "The potential to improve road safety, reduce human driving error, and promote\nenvironmental sustainability have enabled the field of autonomous driving to\nprogress rapidly over recent decades. The performance of autonomous vehicles\nhas significantly improved thanks to advancements in Artificial Intelligence,\nparticularly Deep Learning. Nevertheless, the opacity of their decision-making,\nrooted in the use of accurate yet complex AI models, has created barriers to\ntheir societal trust and regulatory acceptance, raising the need for\nexplainability. We propose a post-hoc, model-agnostic solution to provide\nteleological explanations for the behaviour of an autonomous vehicle in urban\nenvironments. Building on Intention-aware Policy Graphs, our approach enables\nthe extraction of interpretable and reliable explanations of vehicle behaviour\nin the nuScenes dataset from global and local perspectives. We demonstrate the\npotential of these explanations to assess whether the vehicle operates within\nacceptable legal boundaries and to identify possible vulnerabilities in\nautonomous driving datasets and models.",
      "pdf_url": "http://arxiv.org/pdf/2505.08404v1",
      "published": "2025-05-13T09:58:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08404v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ConDiSim: Conditional Diffusion Models for Simulation Based Inference",
      "authors": [
        "Mayank Nautiyal",
        "Andreas Hellander",
        "Prashant Singh"
      ],
      "abstract": "We present a conditional diffusion model - ConDiSim, for simulation-based\ninference of complex systems with intractable likelihoods. ConDiSim leverages\ndenoising diffusion probabilistic models to approximate posterior\ndistributions, consisting of a forward process that adds Gaussian noise to\nparameters, and a reverse process learning to denoise, conditioned on observed\ndata. This approach effectively captures complex dependencies and\nmulti-modalities within posteriors. ConDiSim is evaluated across ten benchmark\nproblems and two real-world test problems, where it demonstrates effective\nposterior approximation accuracy while maintaining computational efficiency and\nstability in model training. ConDiSim offers a robust and extensible framework\nfor simulation-based inference, particularly suitable for parameter inference\nworkflows requiring fast inference methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.08403v1",
      "published": "2025-05-13T09:58:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08403v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping",
      "authors": [
        "Ren Zhuang",
        "Ben Wang",
        "Shuifa Sun"
      ],
      "abstract": "Large Language Models leverage Chain-of-Thought (CoT) prompting for complex\ntasks, but their reasoning traces are often excessively verbose and\ninefficient, leading to significant computational costs and latency. Current\nCoT compression techniques typically rely on generic importance metrics and\nstatic compression rates, which may inadvertently remove functionally critical\ntokens or fail to adapt to varying reasoning complexity. To overcome these\nlimitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic\nCoT compression via supervised fine-tuning. This approach introduces two\nsynergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric\naccurately identifying functionally relevant tokens by measuring the gradient\ninfluence of their intermediate representations on the final answer loss, and\n(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the\ncompression rate based on runtime model uncertainty while ensuring local\ncoherence through an adaptive N-token constraint. To our knowledge, this is the\nfirst work unifying a goal-oriented, gradient-based importance metric with\ndynamic, uncertainty-aware skipping for CoT compression. Trained on compressed\nMATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization\nacross diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It\nachieves substantial efficiency gains - reducing CoT token counts by over 45%\non average and delivering 1.6-2.0 times inference speedups - while maintaining\nhigh reasoning accuracy. Notably, it significantly outperforms existing\nbaselines by preserving accuracy even at high effective compression rates,\nadvancing the state of the art in the CoT reasoning efficiency-accuracy\ntrade-off.",
      "pdf_url": "http://arxiv.org/pdf/2505.08392v1",
      "published": "2025-05-13T09:39:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.08392v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}