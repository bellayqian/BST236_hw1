{
  "last_updated": "2025-06-11T00:53:46.480788",
  "papers": [
    {
      "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets",
      "authors": [
        "Anh-Quan Cao",
        "Ivan Lopes",
        "Raoul de Charette"
      ],
      "abstract": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2506.08013v1",
      "published": "2025-06-09T17:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.08013v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Vision Transformers Don't Need Trained Registers",
      "authors": [
        "Nick Jiang",
        "Amil Dravid",
        "Alexei Efros",
        "Yossi Gandelsman"
      ],
      "abstract": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.",
      "pdf_url": "http://arxiv.org/pdf/2506.08010v1",
      "published": "2025-06-09T17:59:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.08010v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior",
      "authors": [
        "Penghao Wu",
        "Shengnan Ma",
        "Bo Wang",
        "Jiaheng Yu",
        "Lewei Lu",
        "Ziwei Liu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.",
      "pdf_url": "http://arxiv.org/pdf/2506.08012v1",
      "published": "2025-06-09T17:59:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.08012v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
      "authors": [
        "Xun Huang",
        "Zhengqi Li",
        "Guande He",
        "Mingyuan Zhou",
        "Eli Shechtman"
      ],
      "abstract": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2506.08009v1",
      "published": "2025-06-09T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.08009v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Hidden in plain sight: VLMs overlook their visual representations",
      "authors": [
        "Stephanie Fu",
        "Tyler Bonnen",
        "Devin Guillory",
        "Trevor Darrell"
      ],
      "abstract": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.",
      "pdf_url": "http://arxiv.org/pdf/2506.08008v1",
      "published": "2025-06-09T17:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.08008v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Dynamic View Synthesis as an Inverse Problem",
      "authors": [
        "Hidir Yesiltepe",
        "Pinar Yanardag"
      ],
      "abstract": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.",
      "pdf_url": "http://arxiv.org/pdf/2506.08004v1",
      "published": "2025-06-09T17:59:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.08004v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Audio-Sync Video Generation with Multi-Stream Temporal Control",
      "authors": [
        "Shuchen Weng",
        "Haojie Zheng",
        "Zheng Chang",
        "Si Li",
        "Boxin Shi",
        "Xinlong Wang"
      ],
      "abstract": "Audio is inherently temporal and closely synchronized with the visual world,\nmaking it a naturally aligned and expressive control signal for controllable\nvideo generation (e.g., movies). Beyond control, directly translating audio\ninto video is essential for understanding and visualizing rich audio narratives\n(e.g., Podcasts or historical recordings). However, existing approaches fall\nshort in generating high-quality videos with precise audio-visual\nsynchronization, especially across diverse and complex audio types. In this\nwork, we introduce MTV, a versatile framework for audio-sync video generation.\nMTV explicitly separates audios into speech, effects, and music tracks,\nenabling disentangled control over lip motion, event timing, and visual mood,\nrespectively -- resulting in fine-grained and semantically aligned video\ngeneration. To support the framework, we additionally present DEMIX, a dataset\ncomprising high-quality cinematic videos and demixed audio tracks. DEMIX is\nstructured into five overlapped subsets, enabling scalable multi-stage training\nfor diverse generation scenarios. Extensive experiments demonstrate that MTV\nachieves state-of-the-art performance across six standard metrics spanning\nvideo quality, text-video consistency, and audio-video alignment. Project page:\nhttps://hjzheng.net/projects/MTV/.",
      "pdf_url": "http://arxiv.org/pdf/2506.08003v1",
      "published": "2025-06-09T17:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.08003v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
      "authors": [
        "Zeju Qiu",
        "Simon Buchholz",
        "Tim Z. Xiao",
        "Maximilian Dax",
        "Bernhard Schölkopf",
        "Weiyang Liu"
      ],
      "abstract": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2506.08001v1",
      "published": "2025-06-09T17:59:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.08001v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment",
      "authors": [
        "Victor Barres",
        "Honghua Dong",
        "Soham Ray",
        "Xujie Si",
        "Karthik Narasimhan"
      ],
      "abstract": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.",
      "pdf_url": "http://arxiv.org/pdf/2506.07982v1",
      "published": "2025-06-09T17:52:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07982v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
      "authors": [
        "Junhong Shen",
        "Hao Bai",
        "Lunjun Zhang",
        "Yifei Zhou",
        "Amrith Setlur",
        "Shengbang Tong",
        "Diego Caples",
        "Nan Jiang",
        "Tong Zhang",
        "Ameet Talwalkar",
        "Aviral Kumar"
      ],
      "abstract": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.",
      "pdf_url": "http://arxiv.org/pdf/2506.07976v2",
      "published": "2025-06-09T17:50:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07976v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization",
      "authors": [
        "Hongzheng Chen",
        "Yingheng Wang",
        "Yaohui Cai",
        "Hins Hu",
        "Jiajie Li",
        "Shirley Huang",
        "Chenhui Deng",
        "Rongjian Liang",
        "Shufeng Kong",
        "Haoxing Ren",
        "Samitha Samaranayake",
        "Carla P. Gomes",
        "Zhiru Zhang"
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated significant advancements\nin reasoning and agent-based problem-solving, current evaluation methodologies\nfail to adequately assess their capabilities: existing benchmarks either rely\non closed-ended questions prone to saturation and memorization, or subjective\ncomparisons that lack consistency and rigor. In this work, we introduce\nHeuriGym, an agentic framework designed for evaluating heuristic algorithms\ngenerated by LLMs for combinatorial optimization problems, characterized by\nclearly defined objectives and expansive solution spaces. HeuriGym empowers\nLLMs to propose heuristics, receive evaluative feedback via code execution, and\niteratively refine their solutions. We evaluate nine state-of-the-art models on\nnine problems across domains such as computer systems, logistics, and biology,\nexposing persistent limitations in tool use, planning, and adaptive reasoning.\nTo quantify performance, we propose the Quality-Yield Index (QYI), a metric\nthat captures both solution pass rate and quality. Even top models like\nGPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below\nthe expert baseline of 1. Our open-source benchmark aims to guide the\ndevelopment of LLMs toward more effective and realistic problem-solving in\nscientific and engineering domains.",
      "pdf_url": "http://arxiv.org/pdf/2506.07972v1",
      "published": "2025-06-09T17:46:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07972v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design",
      "authors": [
        "Wenxin Tang",
        "Jingyu Xiao",
        "Wenxuan Jiang",
        "Xi Xiao",
        "Yuhang Wang",
        "Xuxin Tang",
        "Qing Li",
        "Yuehe Ma",
        "Junliang Liu",
        "Shisong Tang",
        "Michael R. Lyu"
      ],
      "abstract": "Manual slide creation is labor-intensive and requires expert prior knowledge.\nExisting natural language-based LLM generation methods struggle to capture the\nvisual and structural nuances of slide designs. To address this, we formalize\nthe Reference Image to Slide Generation task and propose Slide2Code, the first\nbenchmark with difficulty-tiered samples based on a novel Slide Complexity\nMetric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework\nfor generating editable slides from reference images. SlideCoder integrates a\nColor Gradient-based Segmentation algorithm and a Hierarchical\nRetrieval-Augmented Generation method to decompose complex tasks and enhance\ncode generation. We also release SlideMaster, a 7B open-source model fine-tuned\nwith improved reverse-engineered data. Experiments show that SlideCoder\noutperforms state-of-the-art baselines by up to 40.5 points, demonstrating\nstrong performance across layout fidelity, execution accuracy, and visual\nconsistency. Our code is available at\nhttps://github.com/vinsontang1/SlideCoder.",
      "pdf_url": "http://arxiv.org/pdf/2506.07964v1",
      "published": "2025-06-09T17:39:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07964v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Reinforcing Multimodal Understanding and Generation with Dual Self-rewards",
      "authors": [
        "Jixiang Hong",
        "Yiran Zhang",
        "Guanzhong Wang",
        "Yi Liu",
        "Ji-Rong Wen",
        "Rui Yan"
      ],
      "abstract": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate image-text alignment, prone to\ngenerating text responses contradicting the visual input or failing to follow\nthe text-to-image prompts. Current solutions require external supervision\n(e.g., human feedback or reward models) and only address unidirectional\ntasks-either understanding or generation. In this work, based on the\nobservation that understanding and generation are inverse dual tasks, we\nintroduce a self-supervised dual reward mechanism to reinforce the\nunderstanding and generation capabilities of LMMs. Specifically, we sample\nmultiple outputs for a given input in one task domain, then reverse the\ninput-output pairs to compute the dual likelihood of the model as self-rewards\nfor optimization. Extensive experimental results on visual understanding and\ngeneration benchmarks demonstrate that our method can effectively enhance the\nperformance of the model without any external supervision, especially achieving\nremarkable improvements in text-to-image tasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.07963v1",
      "published": "2025-06-09T17:38:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07963v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "Correlated Errors in Large Language Models",
      "authors": [
        "Elliot Kim",
        "Avi Garg",
        "Kenny Peng",
        "Nikhil Garg"
      ],
      "abstract": "Diversity in training data, architecture, and providers is assumed to\nmitigate homogeneity in LLMs. However, we lack empirical evidence on whether\ndifferent LLMs differ meaningfully. We conduct a large-scale empirical\nevaluation on over 350 LLMs overall, using two popular leaderboards and a\nresume-screening task. We find substantial correlation in model errors -- on\none leaderboard dataset, models agree 60% of the time when both models err. We\nidentify factors driving model correlation, including shared architectures and\nproviders. Crucially, however, larger and more accurate models have highly\ncorrelated errors, even with distinct architectures and providers. Finally, we\nshow the effects of correlation in two downstream tasks: LLM-as-judge\nevaluation and hiring -- the latter reflecting theoretical predictions\nregarding algorithmic monoculture.",
      "pdf_url": "http://arxiv.org/pdf/2506.07962v1",
      "published": "2025-06-09T17:37:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07962v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ]
    },
    {
      "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models",
      "authors": [
        "Peiyan Li",
        "Yixiang Chen",
        "Hongtao Wu",
        "Xiao Ma",
        "Xiangnan Wu",
        "Yan Huang",
        "Liang Wang",
        "Tao Kong",
        "Tieniu Tan"
      ],
      "abstract": "Recently, leveraging pre-trained vision-language models (VLMs) for building\nvision-language-action (VLA) models has emerged as a promising approach to\neffective robot manipulation learning. However, only few methods incorporate 3D\nsignals into VLMs for action prediction, and they do not fully leverage the\nspatial structure inherent in 3D data, leading to low sample efficiency. In\nthis paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D\ninputs to multiple 2D images, ensuring input alignment with the VLM backbone,\nand (2) utilizes 2D heatmaps for action prediction, unifying the input and\noutput spaces within a consistent 2D image space. In addition, we propose a\nscalable pre-training method that equips the VLM backbone with the capability\nto predict 2D heatmaps before downstream policy learning. Extensive experiments\nshow the proposed method is able to learn 3D manipulation efficiently and\neffectively. BridgeVLA outperforms state-of-the-art baseline methods across\nthree simulation benchmarks. In RLBench, it improves the average success rate\nfrom 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better\nperformance in challenging generalization settings, boosting the average\nsuccess rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing\nbaseline methods in terms of average success rate. In real-robot experiments,\nBridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It\ngeneralizes robustly in multiple out-of-distribution settings, including visual\ndisturbances and unseen instructions. Remarkably, it is able to achieve a\nsuccess rate of 96.8% on 10+ tasks with only 3 trajectories per task,\nhighlighting its extraordinary sample efficiency. Project\nWebsite:https://bridgevla.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2506.07961v1",
      "published": "2025-06-09T17:36:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07961v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols",
      "authors": [
        "Arnav Sheth",
        "Ivaxi Sheth",
        "Mario Fritz"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have shown promising\ncapabilities in generating code for general-purpose programming languages. In\ncontrast, their applicability for hardware description languages, particularly\nfor generating synthesizable and functionally correct designs, remains\nsignificantly underexplored. HDLs such as SystemVerilog are logic-oriented and\ndemand strict adherence to timing semantics, concurrency, and synthesizability\nconstraints. Moreover, HDL-based design flows encompass a broad set of tasks\nbeyond structural code generation, including testbench development,\nassertion-based verification, timing closure, and protocol-level integration\nfor on-chip communication. The objective of our paper is to analyze the\ncapabilities of state-of-the-art LLMs in generating SystemVerilog\nimplementations of standard communication protocols, a core component of\nembedded and System-on-Chip (SoC) architectures. This paper introduces the\nfirst benchmark suite targeting four widely used protocols: SPI, I2C, UART, and\nAXI. We define code generation tasks that capture varying levels of design\nabstraction and prompt specificity. The generated designs are assessed for\nsyntactic correctness, synthesizability, and functional fidelity via waveform\nsimulation and test benches.",
      "pdf_url": "http://arxiv.org/pdf/2506.07945v1",
      "published": "2025-06-09T17:10:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07945v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations",
      "authors": [
        "Yizhen Li",
        "Dell Zhang",
        "Xuelong Li",
        "Yiqing Shen"
      ],
      "abstract": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.",
      "pdf_url": "http://arxiv.org/pdf/2506.07943v1",
      "published": "2025-06-09T17:05:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07943v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation",
      "authors": [
        "Christopher Subia-Waud"
      ],
      "abstract": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.",
      "pdf_url": "http://arxiv.org/pdf/2506.07940v1",
      "published": "2025-06-09T17:00:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07940v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models",
      "authors": [
        "Chengyue Huang",
        "Yuchen Zhu",
        "Sichen Zhu",
        "Jingyun Xiao",
        "Moises Andrade",
        "Shivang Chopra",
        "Zsolt Kira"
      ],
      "abstract": "Vision-language models (VLMs) are widely assumed to exhibit in-context\nlearning (ICL), a property similar to that of their language-only counterparts.\nWhile recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies\nshow they often rely on shallow heuristics -- such as copying or majority\nvoting -- rather than true task understanding. We revisit this assumption by\nevaluating VLMs under distribution shifts, where support examples come from a\ndataset different from the query. Surprisingly, performance often degrades with\nmore demonstrations, and models tend to copy answers rather than learn from\nthem. To investigate further, we propose a new MM-ICL with Reasoning pipeline\nthat augments each demonstration with a generated rationale alongside the\nanswer. We conduct extensive and comprehensive experiments on both perception-\nand reasoning-required datasets with open-source VLMs ranging from 3B to 72B\nand proprietary models such as Gemini 2.0. We conduct controlled studies\nvarying shot count, retrieval method, rationale quality, and distribution. Our\nresults show limited performance sensitivity across these factors, suggesting\nthat current VLMs do not effectively utilize demonstration-level information as\nintended in MM-ICL.",
      "pdf_url": "http://arxiv.org/pdf/2506.07936v1",
      "published": "2025-06-09T16:55:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07936v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Diffusion of Responsibility in Collective Decision Making",
      "authors": [
        "Pavel Naumov",
        "Jia Tao"
      ],
      "abstract": "The term \"diffusion of responsibility'' refers to situations in which\nmultiple agents share responsibility for an outcome, obscuring individual\naccountability. This paper examines this frequently undesirable phenomenon in\nthe context of collective decision-making mechanisms.\n  The work shows that if a decision is made by two agents, then the only way to\navoid diffusion of responsibility is for one agent to act as a \"dictator'',\nmaking the decision unilaterally. In scenarios with more than two agents, any\ndiffusion-free mechanism is an \"elected dictatorship'' where the agents elect a\nsingle agent to make a unilateral decision.\n  The technical results are obtained by defining a bisimulation of\ndecision-making mechanisms, proving that bisimulation preserves\nresponsibility-related properties, and establishing the results for a smallest\nbisimular mechanism.",
      "pdf_url": "http://arxiv.org/pdf/2506.07935v1",
      "published": "2025-06-09T16:54:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07935v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "Solving Inequality Proofs with Large Language Models",
      "authors": [
        "Jiayi Sheng",
        "Luna Lyu",
        "Jikai Jin",
        "Tony Xia",
        "Alex Gu",
        "James Zou",
        "Pan Lu"
      ],
      "abstract": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2506.07927v1",
      "published": "2025-06-09T16:43:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07927v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Uncovering the Functional Roles of Nonlinearity in Memory",
      "authors": [
        "Manuel Brenner",
        "Georgia Koppe"
      ],
      "abstract": "Memory and long-range temporal processing are core requirements for sequence\nmodeling tasks across natural language processing, time-series forecasting,\nspeech recognition, and control. While nonlinear recurrence has long been\nviewed as essential for enabling such mechanisms, recent work suggests that\nlinear dynamics may often suffice. In this study, we go beyond performance\ncomparisons to systematically dissect the functional role of nonlinearity in\nrecurrent networks--identifying both when it is computationally necessary, and\nwhat mechanisms it enables. We use Almost Linear Recurrent Neural Networks\n(AL-RNNs), which allow fine-grained control over nonlinearity, as both a\nflexible modeling tool and a probe into the internal mechanisms of memory.\nAcross a range of classic sequence modeling tasks and a real-world stimulus\nselection task, we find that minimal nonlinearity is not only sufficient but\noften optimal, yielding models that are simpler, more robust, and more\ninterpretable than their fully nonlinear or linear counterparts. Our results\nprovide a principled framework for selectively introducing nonlinearity,\nbridging dynamical systems theory with the functional demands of long-range\nmemory and structured computation in recurrent neural networks, with\nimplications for both artificial and biological neural systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.07919v1",
      "published": "2025-06-09T16:32:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07919v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "nlin.CD",
        "physics.comp-ph"
      ]
    },
    {
      "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement",
      "authors": [
        "Dimitris Panagopoulos",
        "Adolfo Perrusquia",
        "Weisi Guo"
      ],
      "abstract": "In dynamic environments, the rapid obsolescence of pre-existing environmental\nknowledge creates a gap between an agent's internal model and the evolving\nreality of its operational context. This disparity between prior and updated\nenvironmental valuations fundamentally limits the effectiveness of autonomous\ndecision-making. To bridge this gap, the contextual bias of human domain\nstakeholders, who naturally accumulate insights through direct, real-time\nobservation, becomes indispensable. However, translating their nuanced, and\ncontext-rich input into actionable intelligence for autonomous systems remains\nan open challenge. To address this, we propose LUCIFER (Language Understanding\nand Context-Infused Framework for Exploration and Behavior Refinement), a\ndomain-agnostic framework that integrates a hierarchical decision-making\narchitecture with reinforcement learning (RL) and large language models (LLMs)\ninto a unified system. This architecture mirrors how humans decompose complex\ntasks, enabling a high-level planner to coordinate specialised sub-agents, each\nfocused on distinct objectives and temporally interdependent actions. Unlike\ntraditional applications where LLMs are limited to single role, LUCIFER\nintegrates them in two synergistic roles: as context extractors, structuring\nverbal stakeholder input into domain-aware representations that influence\ndecision-making through an attention space mechanism aligning LLM-derived\ninsights with the agent's learning process, and as zero-shot exploration\nfacilitators guiding the agent's action selection process during exploration.\nWe benchmark various LLMs in both roles and demonstrate that LUCIFER improves\nexploration efficiency and decision quality, outperforming flat,\ngoal-conditioned policies. Our findings show the potential of context-driven\ndecision-making, where autonomous systems leverage human contextual knowledge\nfor operational success.",
      "pdf_url": "http://arxiv.org/pdf/2506.07915v1",
      "published": "2025-06-09T16:30:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07915v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces",
      "authors": [
        "Kevin Rojas",
        "Yuchen Zhu",
        "Sichen Zhu",
        "Felix X. -F. Ye",
        "Molei Tao"
      ],
      "abstract": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.",
      "pdf_url": "http://arxiv.org/pdf/2506.07903v1",
      "published": "2025-06-09T16:20:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07903v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
      "authors": [
        "MiniCPM Team",
        "Chaojun Xiao",
        "Yuxuan Li",
        "Xu Han",
        "Yuzhuo Bai",
        "Jie Cai",
        "Haotian Chen",
        "Wentong Chen",
        "Xin Cong",
        "Ganqu Cui",
        "Ning Ding",
        "Shengdan Fan",
        "Yewei Fang",
        "Zixuan Fu",
        "Wenyu Guan",
        "Yitong Guan",
        "Junshao Guo",
        "Yufeng Han",
        "Bingxiang He",
        "Yuxiang Huang",
        "Cunliang Kong",
        "Qiuzuo Li",
        "Siyuan Li",
        "Wenhao Li",
        "Yanghao Li",
        "Yishan Li",
        "Zhen Li",
        "Dan Liu",
        "Biyuan Lin",
        "Yankai Lin",
        "Xiang Long",
        "Quanyu Lu",
        "Yaxi Lu",
        "Peiyan Luo",
        "Hongya Lyu",
        "Litu Ou",
        "Yinxu Pan",
        "Zekai Qu",
        "Qundong Shi",
        "Zijun Song",
        "Jiayuan Su",
        "Zhou Su",
        "Ao Sun",
        "Xianghui Sun",
        "Peijun Tang",
        "Fangzheng Wang",
        "Feng Wang",
        "Shuo Wang",
        "Yudong Wang",
        "Yesai Wu",
        "Zhenyu Xiao",
        "Jie Xie",
        "Zihao Xie",
        "Yukun Yan",
        "Jiarui Yuan",
        "Kaihuo Zhang",
        "Lei Zhang",
        "Linyue Zhang",
        "Xueren Zhang",
        "Yudi Zhang",
        "Hengyu Zhao",
        "Weilin Zhao",
        "Weilun Zhao",
        "Yuanqian Zhao",
        "Zhi Zheng",
        "Ge Zhou",
        "Jie Zhou",
        "Wei Zhou",
        "Zihan Zhou",
        "Zixuan Zhou",
        "Zhiyuan Liu",
        "Guoyang Zeng",
        "Chao Jia",
        "Dahai Li",
        "Maosong Sun"
      ],
      "abstract": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.",
      "pdf_url": "http://arxiv.org/pdf/2506.07900v1",
      "published": "2025-06-09T16:16:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07900v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution",
      "authors": [
        "Shuja Khalid",
        "Mohamed Ibrahim",
        "Yang Liu"
      ],
      "abstract": "We present a novel approach for enhancing the resolution and geometric\nfidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.\nCurrent 3DGS methods are fundamentally limited by their input resolution,\nproducing reconstructions that cannot extrapolate finer details than are\npresent in the training views. Our work breaks this limitation through a\nlightweight generative model that predicts and refines additional 3D Gaussians\nwhere needed most. The key innovation is our Hessian-assisted sampling\nstrategy, which intelligently identifies regions that are likely to benefit\nfrom densification, ensuring computational efficiency. Unlike computationally\nintensive GANs or diffusion approaches, our method operates in real-time\n(0.015s per inference on a single consumer-grade GPU), making it practical for\ninteractive applications. Comprehensive experiments demonstrate significant\nimprovements in both geometric accuracy and rendering quality compared to\nstate-of-the-art methods, establishing a new paradigm for resolution-free 3D\nscene enhancement.",
      "pdf_url": "http://arxiv.org/pdf/2506.07897v1",
      "published": "2025-06-09T16:13:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07897v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark",
      "authors": [
        "Shoko Oka"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have revitalized\nphilosophical debates surrounding artificial intelligence. Two of the most\nfundamental challenges - namely, the Frame Problem and the Symbol Grounding\nProblem - have historically been viewed as unsolvable within traditional\nsymbolic AI systems. This study investigates whether modern LLMs possess the\ncognitive capacities required to address these problems. To do so, I designed\ntwo benchmark tasks reflecting the philosophical core of each problem,\nadministered them under zero-shot conditions to 13 prominent LLMs (both closed\nand open-source), and assessed the quality of the models' outputs across five\ntrials each. Responses were scored along multiple criteria, including\ncontextual reasoning, semantic coherence, and information filtering. The\nresults demonstrate that while open-source models showed variability in\nperformance due to differences in model size, quantization, and instruction\ntuning, several closed models consistently achieved high scores. These findings\nsuggest that select modern LLMs may be acquiring capacities sufficient to\nproduce meaningful and stable responses to these long-standing theoretical\nchallenges.",
      "pdf_url": "http://arxiv.org/pdf/2506.07896v1",
      "published": "2025-06-09T16:12:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07896v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Diffusion Counterfactual Generation with Semantic Abduction",
      "authors": [
        "Rajat Rasal",
        "Avinash Kori",
        "Fabio De Sousa Ribeiro",
        "Tian Xia",
        "Ben Glocker"
      ],
      "abstract": "Counterfactual image generation presents significant challenges, including\npreserving identity, maintaining perceptual quality, and ensuring faithfulness\nto an underlying causal model. While existing auto-encoding frameworks admit\nsemantic latent spaces which can be manipulated for causal control, they\nstruggle with scalability and fidelity. Advancements in diffusion models\npresent opportunities for improving counterfactual image editing, having\ndemonstrated state-of-the-art visual quality, human-aligned perception and\nrepresentation learning capabilities. Here, we present a suite of\ndiffusion-based causal mechanisms, introducing the notions of spatial, semantic\nand dynamic abduction. We propose a general framework that integrates semantic\nrepresentations into diffusion models through the lens of Pearlian causality to\nedit images via a counterfactual reasoning process. To our knowledge, this is\nthe first work to consider high-level semantic identity preservation for\ndiffusion counterfactuals and to demonstrate how semantic control enables\nprincipled trade-offs between faithful causal control and identity\npreservation.",
      "pdf_url": "http://arxiv.org/pdf/2506.07883v1",
      "published": "2025-06-09T15:54:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07883v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ]
    },
    {
      "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity",
      "authors": [
        "Jinxi Li",
        "Ziyang Song",
        "Siyuan Zhou",
        "Bo Yang"
      ],
      "abstract": "In this paper, we aim to model 3D scene geometry, appearance, and the\nunderlying physics purely from multi-view videos. By applying various governing\nPDEs as PINN losses or incorporating physics simulation into neural networks,\nexisting works often fail to learn complex physical motions at boundaries or\nrequire object priors such as masks or types. In this paper, we propose\nFreeGave to learn the physics of complex dynamic 3D scenes without needing any\nobject priors. The key to our approach is to introduce a physics code followed\nby a carefully designed divergence-free module for estimating a per-Gaussian\nvelocity field, without relying on the inefficient PINN losses. Extensive\nexperiments on three public datasets and a newly collected challenging\nreal-world dataset demonstrate the superior performance of our method for\nfuture frame extrapolation and motion segmentation. Most notably, our\ninvestigation into the learned physics codes reveals that they truly learn\nmeaningful 3D physical motion patterns in the absence of any human labels in\ntraining.",
      "pdf_url": "http://arxiv.org/pdf/2506.07865v1",
      "published": "2025-06-09T15:31:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07865v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes",
      "authors": [
        "Mirko Paolo Barbato",
        "Giorgia Rigamonti",
        "Davide Marelli",
        "Paolo Napoletano"
      ],
      "abstract": "Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous\nmonitoring to prevent severe hypo- and hyperglycemic events. While continuous\nglucose monitoring has improved blood glucose management, deploying predictive\nmodels on wearable devices remains challenging due to computational and memory\nconstraints. To address this, we propose a novel Lightweight Sequential\nTransformer model designed for blood glucose prediction in T1D. By integrating\nthe strengths of Transformers' attention mechanisms and the sequential\nprocessing of recurrent neural networks, our architecture captures long-term\ndependencies while maintaining computational efficiency. The model is optimized\nfor deployment on resource-constrained edge devices and incorporates a balanced\nloss function to handle the inherent data imbalance in hypo- and hyperglycemic\nevents. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,\ndemonstrate that the proposed model outperforms state-of-the-art methods in\npredicting glucose levels and detecting adverse events. This work fills the gap\nbetween high-performance modeling and practical deployment, providing a\nreliable and efficient T1D management solution.",
      "pdf_url": "http://arxiv.org/pdf/2506.07864v1",
      "published": "2025-06-09T15:27:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07864v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective",
      "authors": [
        "Firas Laakom",
        "Haobo Chen",
        "Jürgen Schmidhuber",
        "Yuheng Bu"
      ],
      "abstract": "Despite substantial progress in promoting fairness in high-stake applications\nusing machine learning models, existing methods often modify the training\nprocess, such as through regularizers or other interventions, but lack formal\nguarantees that fairness achieved during training will generalize to unseen\ndata. Although overfitting with respect to prediction performance has been\nextensively studied, overfitting in terms of fairness loss has received far\nless attention. This paper proposes a theoretical framework for analyzing\nfairness generalization error through an information-theoretic lens. Our novel\nbounding technique is based on Efron-Stein inequality, which allows us to\nderive tight information-theoretic fairness generalization bounds with both\nMutual Information (MI) and Conditional Mutual Information (CMI). Our empirical\nresults validate the tightness and practical relevance of these bounds across\ndiverse fairness-aware learning algorithms. Our framework offers valuable\ninsights to guide the design of algorithms improving fairness generalization.",
      "pdf_url": "http://arxiv.org/pdf/2506.07861v1",
      "published": "2025-06-09T15:24:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07861v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds",
      "authors": [
        "Zihui Zhang",
        "Weisheng Dai",
        "Hongtao Wen",
        "Bo Yang"
      ],
      "abstract": "We study the problem of unsupervised 3D semantic segmentation on raw point\nclouds without needing human labels in training. Existing methods usually\nformulate this problem into learning per-point local features followed by a\nsimple grouping strategy, lacking the ability to discover additional and\npossibly richer semantic priors beyond local features. In this paper, we\nintroduce LogoSP to learn 3D semantics from both local and global point\nfeatures. The key to our approach is to discover 3D semantic information by\ngrouping superpoints according to their global patterns in the frequency\ndomain, thus generating highly accurate semantic pseudo-labels for training a\nsegmentation network. Extensive experiments on two indoor and an outdoor\ndatasets show that our LogoSP surpasses all existing unsupervised methods by\nlarge margins, achieving the state-of-the-art performance for unsupervised 3D\nsemantic segmentation. Notably, our investigation into the learned global\npatterns reveals that they truly represent meaningful 3D semantics in the\nabsence of human labels during training.",
      "pdf_url": "http://arxiv.org/pdf/2506.07857v1",
      "published": "2025-06-09T15:21:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07857v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Residual Reweighted Conformal Prediction for Graph Neural Networks",
      "authors": [
        "Zheng Zhang",
        "Jie Bao",
        "Zhixin Zhou",
        "Nicolo Colombo",
        "Lixin Cheng",
        "Rui Luo"
      ],
      "abstract": "Graph Neural Networks (GNNs) excel at modeling relational data but face\nsignificant challenges in high-stakes domains due to unquantified uncertainty.\nConformal prediction (CP) offers statistical coverage guarantees, but existing\nmethods often produce overly conservative prediction intervals that fail to\naccount for graph heteroscedasticity and structural biases. While residual\nreweighting CP variants address some of these limitations, they neglect graph\ntopology, cluster-specific uncertainties, and risk data leakage by reusing\ntraining sets. To address these issues, we propose Residual Reweighted GNN\n(RR-GNN), a framework designed to generate minimal prediction sets with\nprovable marginal coverage guarantees.\n  RR-GNN introduces three major innovations to enhance prediction performance.\nFirst, it employs Graph-Structured Mondrian CP to partition nodes or edges into\ncommunities based on topological features, ensuring cluster-conditional\ncoverage that reflects heterogeneity. Second, it uses Residual-Adaptive\nNonconformity Scores by training a secondary GNN on a held-out calibration set\nto estimate task-specific residuals, dynamically adjusting prediction intervals\naccording to node or edge uncertainty. Third, it adopts a Cross-Training\nProtocol, which alternates the optimization of the primary GNN and the residual\npredictor to prevent information leakage while maintaining graph dependencies.\nWe validate RR-GNN on 15 real-world graphs across diverse tasks, including node\nclassification, regression, and edge weight prediction. Compared to CP\nbaselines, RR-GNN achieves improved efficiency over state-of-the-art methods,\nwith no loss of coverage.",
      "pdf_url": "http://arxiv.org/pdf/2506.07854v1",
      "published": "2025-06-09T15:19:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07854v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms",
      "authors": [
        "Hudson de Martim"
      ],
      "abstract": "Effectively representing legal norms for automated processing is a critical\nchallenge, particularly in tracking the diachronic evolution of their\nhierarchical components (e.g., articles, paragraphs). While foundational\nframeworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal\ndocuments at a macro level, they lack native mechanisms for granular,\ncomponent-level versioning. This limitation hinders the deterministic\npoint-in-time reconstruction of legal texts, a fundamental capability for\nreliable Legal Tech and AI applications. This paper proposes a structured,\ntemporal model that extends the FRBRoo framework to address this gap. It\nintroduces specialized subclasses of Expressio - Temporal Version (TV) and\nLanguage Version (LV - to represent the state of a legal norm and its\nlinguistic variations at specific points in time. The model applies this same\nparadigm hierarchically, introducing Component Work (CW), Component Temporal\nVersion (CTV), and Component Language Version (CLV) to track the lifecycle of\nindividual articles, paragraphs, and clauses. Using the Brazilian Federal\nConstitution as a case study, the paper demonstrates how each amendment creates\nnew Component Temporal Versions for affected provisions, while unaffected\ncomponents retain their existing versions. This fine-grained, time-aware\narchitecture enables the precise, deterministic retrieval and reconstruction of\nany part of a legal text as it existed on a specific date. The model provides a\nrobust foundation for developing advanced legal information systems, knowledge\ngraphs, and AI tools capable of accurate historical analysis and impact\nassessment, overcoming the limitations of current generative models.",
      "pdf_url": "http://arxiv.org/pdf/2506.07853v1",
      "published": "2025-06-09T15:18:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07853v1",
      "categories": [
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement",
      "authors": [
        "Teng Hu",
        "Zhentao Yu",
        "Zhengguang Zhou",
        "Jiangning Zhang",
        "Yuan Zhou",
        "Qinglin Lu",
        "Ran Yi"
      ],
      "abstract": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.",
      "pdf_url": "http://arxiv.org/pdf/2506.07848v1",
      "published": "2025-06-09T15:11:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07848v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Diffusion models under low-noise regime",
      "authors": [
        "Elizabeth Pavlova",
        "Xue-Xin Wei"
      ],
      "abstract": "Recent work on diffusion models proposed that they operate in two regimes:\nmemorization, in which models reproduce their training data, and\ngeneralization, in which they generate novel samples. While this has been\ntested in high-noise settings, the behavior of diffusion models as effective\ndenoisers when the corruption level is small remains unclear. To address this\ngap, we systematically investigated the behavior of diffusion models under\nlow-noise diffusion dynamics, with implications for model robustness and\ninterpretability. Using (i) CelebA subsets of varying sample sizes and (ii)\nanalytic Gaussian mixture benchmarks, we reveal that models trained on disjoint\ndata diverge near the data manifold even when their high-noise outputs\nconverge. We quantify how training set size, data geometry, and model objective\nchoice shape denoising trajectories and affect score accuracy, providing\ninsights into how these models actually learn representations of data\ndistributions. This work starts to address gaps in our understanding of\ngenerative model reliability in practical applications where small\nperturbations are common.",
      "pdf_url": "http://arxiv.org/pdf/2506.07841v1",
      "published": "2025-06-09T15:07:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07841v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains",
      "authors": [
        "Shijie Wang",
        "Yilun Zhang",
        "Zeyu Lai",
        "Dexing Kong"
      ],
      "abstract": "Multimodal large language models (MLLMs) have shown great potential in\ngeneral domains but perform poorly in some specific domains due to a lack of\ndomain-specific data, such as image-text data or vedio-text data. In some\nspecific domains, there is abundant graphic and textual data scattered around,\nbut lacks standardized arrangement. In the field of medical ultrasound, there\nare ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic\ndiagnostic reports, and so on. However, these ultrasonic materials are often\nsaved in the forms of PDF, images, etc., and cannot be directly used for the\ntraining of MLLMs. This paper proposes a novel image-text reasoning supervised\nfine-tuning data generation pipeline to create specific domain quadruplets\n(image, question, thinking trace, and answer) from domain-specific materials. A\nmedical ultrasound domain dataset ReMUD is established, containing over 45,000\nreasoning and non-reasoning supervised fine-tuning Question Answering (QA) and\nVisual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on\nQwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound\nfield. To facilitate research, the ReMUD dataset, data generation codebase, and\nReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,\naddressing the data shortage issue in specific domain MLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2506.07837v1",
      "published": "2025-06-09T15:01:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07837v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Are Trees Really Green? A Detection Approach of IoT Malware Attacks",
      "authors": [
        "Silvia Lucia Sanna",
        "Diego Soi",
        "Davide Maiorca",
        "Giorgio Giacinto"
      ],
      "abstract": "Nowadays, the Internet of Things (IoT) is widely employed, and its usage is\ngrowing exponentially because it facilitates remote monitoring, predictive\nmaintenance, and data-driven decision making, especially in the healthcare and\nindustrial sectors. However, IoT devices remain vulnerable due to their\nresource constraints and difficulty in applying security patches. Consequently,\nvarious cybersecurity attacks are reported daily, such as Denial of Service,\nparticularly in IoT-driven solutions. Most attack detection methodologies are\nbased on Machine Learning (ML) techniques, which can detect attack patterns.\nHowever, the focus is more on identification rather than considering the impact\nof ML algorithms on computational resources. This paper proposes a green\nmethodology to identify IoT malware networking attacks based on flow\nprivacy-preserving statistical features. In particular, the hyperparameters of\nthree tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are\noptimized based on energy consumption and test-time performance in terms of\nMatthew's Correlation Coefficient. Our results show that models maintain high\nperformance and detection accuracy while consistently reducing power usage in\nterms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion\nDetection Systems are suitable for IoT and other resource-constrained devices.",
      "pdf_url": "http://arxiv.org/pdf/2506.07836v1",
      "published": "2025-06-09T15:01:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07836v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "Improving large language models with concept-aware fine-tuning",
      "authors": [
        "Michael K. Chen",
        "Xikun Zhang",
        "Jiaxing Huang",
        "Dacheng Tao"
      ],
      "abstract": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm",
      "pdf_url": "http://arxiv.org/pdf/2506.07833v1",
      "published": "2025-06-09T14:55:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07833v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information",
      "authors": [
        "Jan Corazza",
        "Hadi Partovi Aria",
        "Hyohun Kim",
        "Daniel Neider",
        "Zhe Xu"
      ],
      "abstract": "Reinforcement learning (RL) algorithms can find an optimal policy for a\nsingle agent to accomplish a particular task. However, many real-world problems\nrequire multiple agents to collaborate in order to achieve a common goal. For\nexample, a robot executing a task in a warehouse may require the assistance of\na drone to retrieve items from high shelves. In Decentralized Multi-Agent RL\n(DMARL), agents learn independently and then combine their policies at\nexecution time, but often must satisfy constraints on compatibility of local\npolicies to ensure that they can achieve the global task when combined. In this\npaper, we study how providing high-level symbolic knowledge to agents can help\naddress unique challenges of this setting, such as privacy constraints,\ncommunication limitations, and performance concerns. In particular, we extend\nthe formal tools used to check the compatibility of local policies with the\nteam task, making decentralized training with theoretical guarantees usable in\nmore scenarios. Furthermore, we empirically demonstrate that symbolic knowledge\nabout the temporal evolution of events in the environment can significantly\nexpedite the learning process in DMARL.",
      "pdf_url": "http://arxiv.org/pdf/2506.07829v1",
      "published": "2025-06-09T14:53:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07829v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs",
      "authors": [
        "Yao Yan"
      ],
      "abstract": "Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility.",
      "pdf_url": "http://arxiv.org/pdf/2506.07824v1",
      "published": "2025-06-09T14:48:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07824v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation",
      "authors": [
        "Xintong Duan",
        "Yutong He",
        "Fahim Tajwar",
        "Ruslan Salakhutdinov",
        "J. Zico Kolter",
        "Jeff Schneider"
      ],
      "abstract": "Although diffusion models have achieved strong results in decision-making\ntasks, their slow inference speed remains a key limitation. While the\nconsistency model offers a potential solution, its applications to\ndecision-making often struggle with suboptimal demonstrations or rely on\ncomplex concurrent training of multiple networks. In this work, we propose a\nnovel approach to consistency distillation for offline reinforcement learning\nthat directly incorporates reward optimization into the distillation process.\nOur method enables single-step generation while maintaining higher performance\nand simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and\nlong horizon planning demonstrate that our approach can achieve an 8.7%\nimprovement over previous state-of-the-art while offering up to 142x speedup\nover diffusion counterparts in inference time.",
      "pdf_url": "http://arxiv.org/pdf/2506.07822v1",
      "published": "2025-06-09T14:48:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07822v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation",
      "authors": [
        "Jiaxiang Chen",
        "Zhuo Wang",
        "Mingxi Zou",
        "Qifan Wang",
        "Zenglin Xu"
      ],
      "abstract": "Human reasoning is flexible, adaptive, and grounded in prior\nexperience-qualities that large language models (LLMs) still struggle to\nemulate. Existing methods either explore diverse reasoning paths at inference\ntime or search for optimal workflows through expensive operations, but both\nfall short in leveraging multiple reusable strategies in a structured,\nefficient manner. We propose Guideline Forest, a framework that enhances LLMs\nreasoning by inducing structured reasoning strategies-called guidelines-from\nverified examples and executing them via step-wise aggregation. Unlike\ntest-time search or single-path distillation, our method draws on verified\nreasoning experiences by inducing reusable guidelines and expanding each into\ndiverse variants. Much like human reasoning, these variants reflect alternative\nthought patterns, are executed in parallel, refined via self-correction, and\naggregated step by step-enabling the model to adaptively resolve uncertainty\nand synthesize robust solutions.We evaluate Guideline Forest on four\nbenchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and\nprogrammatic reasoning. Guideline Forest consistently outperforms strong\nbaselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further\nhighlight the effectiveness of multi-path reasoning and stepwise aggregation,\nunderscoring the Guideline Forest's adaptability and generalization potential.",
      "pdf_url": "http://arxiv.org/pdf/2506.07820v2",
      "published": "2025-06-09T14:46:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07820v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution",
      "authors": [
        "Junseo Bang",
        "Joonhee Lee",
        "Kyeonghyun Lee",
        "Haechang Lee",
        "Dong Un Kang",
        "Se Young Chun"
      ],
      "abstract": "Arbitrary-scale image super-resolution aims to upsample images to any desired\nresolution, offering greater flexibility than traditional fixed-scale\nsuper-resolution. Recent approaches in this domain utilize regression-based or\ngenerative models, but many of them are a single-stage upsampling process,\nwhich may be challenging to learn across a wide, continuous distribution of\nscaling factors. Progressive upsampling strategies have shown promise in\nmitigating this issue, yet their integration with diffusion models for flexible\nupscaling remains underexplored. Here, we present CasArbi, a novel\nself-cascaded diffusion framework for arbitrary-scale image super-resolution.\nCasArbi meets the varying scaling demands by breaking them down into smaller\nsequential factors and progressively enhancing the image resolution at each\nstep with seamless transitions for arbitrary scales. Our novel\ncoordinate-guided residual diffusion model allows for the learning of\ncontinuous image representations while enabling efficient diffusion sampling.\nExtensive experiments demonstrate that our CasArbi outperforms prior arts in\nboth perceptual and distortion performance metrics across diverse\narbitrary-scale super-resolution benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2506.07813v1",
      "published": "2025-06-09T14:43:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07813v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Proposal to Extend the Common Model of Cognition with Metacognition",
      "authors": [
        "John Laird",
        "Christian Lebiere",
        "Paul Rosenbloom",
        "Andrea Stocco",
        "Robert Wray"
      ],
      "abstract": "The Common Model of Cognition (CMC) provides an abstract characterization of\nthe structure and processing required by a cognitive architecture for\nhuman-like minds. We propose a unified approach to integrating metacognition\nwithin the CMC. We propose that metacognition involves reasoning over explicit\nrepresentations of an agent's cognitive capabilities and processes in working\nmemory. Our proposal exploits the existing cognitive capabilities of the CMC,\nmaking minimal extensions in the structure and information available within\nworking memory. We provide examples of metacognition within our proposal.",
      "pdf_url": "http://arxiv.org/pdf/2506.07807v1",
      "published": "2025-06-09T14:35:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07807v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability",
      "authors": [
        "Jie Bao",
        "Chuangyin Dang",
        "Rui Luo",
        "Hanwei Zhang",
        "Zhixin Zhou"
      ],
      "abstract": "As deep learning models are increasingly deployed in high-risk applications,\nrobust defenses against adversarial attacks and reliable performance guarantees\nbecome paramount. Moreover, accuracy alone does not provide sufficient\nassurance or reliable uncertainty estimates for these models. This study\nadvances adversarial training by leveraging principles from Conformal\nPrediction. Specifically, we develop an adversarial attack method, termed OPSA\n(OPtimal Size Attack), designed to reduce the efficiency of conformal\nprediction at any significance level by maximizing model uncertainty without\nrequiring coverage guarantees. Correspondingly, we introduce OPSA-AT\n(Adversarial Training), a defense strategy that integrates OPSA within a novel\nconformal training paradigm. Experimental evaluations demonstrate that our OPSA\nattack method induces greater uncertainty compared to baseline approaches for\nvarious defenses. Conversely, our OPSA-AT defensive model significantly\nenhances robustness not only against OPSA but also other adversarial attacks,\nand maintains reliable prediction. Our findings highlight the effectiveness of\nthis integrated approach for developing trustworthy and resilient deep learning\nmodels for safety-critical domains. Our code is available at\nhttps://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.",
      "pdf_url": "http://arxiv.org/pdf/2506.07804v1",
      "published": "2025-06-09T14:33:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07804v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification",
      "authors": [
        "Iustin Sirbu",
        "Robert-Adrian Popovici",
        "Cornelia Caragea",
        "Stefan Trausan-Matu",
        "Traian Rebedea"
      ],
      "abstract": "We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm\ncombining the paradigms of co-training and consistency regularization with\npseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label\nweighting module designed for three key purposes: selecting and filtering\npseudo-labels based on head agreement and model confidence, and weighting them\naccording to the perceived classification difficulty. This novel module\nenhances and unifies three existing techniques -- heads agreement from\nMultihead Co-training, self-adaptive thresholds from FreeMatch, and Average\nPseudo-Margins from MarginMatch -- resulting in a holistic approach that\nimproves robustness and performance in SSL settings. Experimental results on\nbenchmark datasets highlight the superior performance of MultiMatch, achieving\nstate-of-the-art results on 9 out of 10 setups from 5 natural language\nprocessing datasets and ranking first according to the Friedman test among 19\nmethods. Furthermore, MultiMatch demonstrates exceptional robustness in highly\nimbalanced settings, outperforming the second-best approach by 3.26% -- and\ndata imbalance is a key factor for many text classification tasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.07801v1",
      "published": "2025-06-09T14:27:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07801v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ]
    },
    {
      "title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger",
      "authors": [
        "Qi Yang",
        "Chenghao Zhang",
        "Lubin Fan",
        "Kun Ding",
        "Jieping Ye",
        "Shiming Xiang"
      ],
      "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have\nsignificantly improved performance in Visual Question Answering (VQA) tasks\nthrough multimodal Retrieval-Augmented Generation (RAG). However, existing\nmethods still face challenges, such as the scarcity of knowledge with reasoning\nexamples and erratic responses from retrieved knowledge. To address these\nissues, in this study, we propose a multimodal RAG framework, termed RCTS,\nwhich enhances LVLMs by constructing a Reasoning Context-enriched knowledge\nbase and a Tree Search re-ranking method. Specifically, we introduce a\nself-consistent evaluation mechanism to enrich the knowledge base with\nintrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with\nHeuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This\nensures that LVLMs can leverage high-quality contextual reasoning for better\nand more consistent responses. Extensive experiments demonstrate that our\nframework achieves state-of-the-art performance on multiple VQA datasets,\nsignificantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.\nIt highlights the effectiveness of our knowledge base and re-ranking method in\nimproving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.",
      "pdf_url": "http://arxiv.org/pdf/2506.07785v1",
      "published": "2025-06-09T14:00:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07785v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models",
      "authors": [
        "Diego Forniés-Tabuenca",
        "Alejandro Uribe",
        "Urtzi Otamendi",
        "Arkaitz Artetxe",
        "Juan Carlos Rivera",
        "Oier Lopez de Lacalle"
      ],
      "abstract": "Multi-objective optimization is fundamental in complex decision-making tasks.\nTraditional algorithms, while effective, often demand extensive\nproblem-specific modeling and struggle to adapt to nonlinear structures. Recent\nadvances in Large Language Models (LLMs) offer enhanced explainability,\nadaptability, and reasoning. This work proposes Reflective Evolution of\nMulti-objective Heuristics (REMoH), a novel framework integrating NSGA-II with\nLLM-based heuristic generation. A key innovation is a reflection mechanism that\nuses clustering and search-space reflection to guide the creation of diverse,\nhigh-quality heuristics, improving convergence and maintaining solution\ndiversity. The approach is evaluated on the Flexible Job Shop Scheduling\nProblem (FJSSP) in-depth benchmarking against state-of-the-art methods using\nthree instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate\nthat REMoH achieves competitive results compared to state-of-the-art approaches\nwith reduced modeling effort and enhanced adaptability. These findings\nunderscore the potential of LLMs to augment traditional optimization, offering\ngreater flexibility, interpretability, and robustness in multi-objective\nscenarios.",
      "pdf_url": "http://arxiv.org/pdf/2506.07759v1",
      "published": "2025-06-09T13:38:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07759v1",
      "categories": [
        "cs.AI",
        "cs.NE",
        "I.2.7; I.2.8; F.2.2"
      ]
    },
    {
      "title": "Agent Semantics, Semantic Spacetime, and Graphical Reasoning",
      "authors": [
        "Mark Burgess"
      ],
      "abstract": "Some formal aspects of the Semantic Spacetime graph model are presented, with\nreference to its use for directed knowledge representations and process\nmodelling. A finite $\\gamma(3,4)$ representation is defined to form a closed\nset of operations that can scale to any degree of semantic complexity. The\nSemantic Spacetime postulates bring predictability with minimal constraints to\npathways in graphs. The ubiquitous appearance of absorbing states in any\npartial graph means that a graph process leaks information. The issue is\nclosely associated with the issue of division by zero, which signals a loss of\nclosure and the need for manual injection of remedial information. The Semantic\nSpacetime model (and its Promise Theory) origins help to clarify how such\nabsorbing states are associated with boundary information where intentionality\ncan enter.",
      "pdf_url": "http://arxiv.org/pdf/2506.07756v1",
      "published": "2025-06-09T13:37:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.07756v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "I.2.11; F.4.1; I.2.4; G.2.2"
      ]
    }
  ]
}