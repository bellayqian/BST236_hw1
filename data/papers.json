{
  "last_updated": "2025-07-31T00:57:43.531597",
  "papers": [
    {
      "title": "Foundation Models for Demand Forecasting via Dual-Strategy Ensembling",
      "authors": [
        "Wei Yang",
        "Defu Cao",
        "Yan Liu"
      ],
      "abstract": "Accurate demand forecasting is critical for supply chain optimization, yet\nremains difficult in practice due to hierarchical complexity, domain shifts,\nand evolving external factors. While recent foundation models offer strong\npotential for time series forecasting, they often suffer from architectural\nrigidity and limited robustness under distributional change. In this paper, we\npropose a unified ensemble framework that enhances the performance of\nfoundation models for sales forecasting in real-world supply chains. Our method\ncombines two complementary strategies: (1) Hierarchical Ensemble (HE), which\npartitions training and inference by semantic levels (e.g., store, category,\ndepartment) to capture localized patterns; and (2) Architectural Ensemble (AE),\nwhich integrates predictions from diverse model backbones to mitigate bias and\nimprove stability. We conduct extensive experiments on the M5 benchmark and\nthree external sales datasets, covering both in-domain and zero-shot\nforecasting. Results show that our approach consistently outperforms strong\nbaselines, improves accuracy across hierarchical levels, and provides a simple\nyet effective mechanism for boosting generalization in complex forecasting\nenvironments.",
      "pdf_url": "http://arxiv.org/pdf/2507.22053v1",
      "published": "2025-07-29T17:56:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22053v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The Interspeech 2025 Speech Accessibility Project Challenge",
      "authors": [
        "Xiuwen Zheng",
        "Bornali Phukon",
        "Jonghwan Na",
        "Ed Cutrell",
        "Kyu Han",
        "Mark Hasegawa-Johnson",
        "Pan-Pan Jiang",
        "Aadhrik Kuila",
        "Colin Lea",
        "Bob MacDonald",
        "Gautam Mantena",
        "Venkatesh Ravichandran",
        "Leda Sari",
        "Katrin Tomanek",
        "Chang D. Yoo",
        "Chris Zwilling"
      ],
      "abstract": "While the last decade has witnessed significant advancements in Automatic\nSpeech Recognition (ASR) systems, performance of these systems for individuals\nwith speech disabilities remains inadequate, partly due to limited public\ntraining data. To bridge this gap, the 2025 Interspeech Speech Accessibility\nProject (SAP) Challenge was launched, utilizing over 400 hours of SAP data\ncollected and transcribed from more than 500 individuals with diverse speech\ndisabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline,\nthe SAP Challenge evaluates submissions based on Word Error Rate and Semantic\nScore. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2\nbaseline in terms of WER, while 17 teams surpassed the baseline on SemScore.\nNotably, the top team achieved the lowest WER of 8.11\\%, and the highest\nSemScore of 88.44\\% at the same time, setting new benchmarks for future ASR\nsystems in recognizing impaired speech.",
      "pdf_url": "http://arxiv.org/pdf/2507.22047v1",
      "published": "2025-07-29T17:50:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22047v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Supervised Quantum Image Processing",
      "authors": [
        "Marco Parigi",
        "Mehran Khosrojerdi",
        "Filippo Caruso",
        "Leonardo Banchi"
      ],
      "abstract": "In the era of big data and artificial intelligence, the increasing volume of\ndata and the demand to solve more and more complex computational challenges are\ntwo driving forces for improving the efficiency of data storage, processing and\nanalysis. Quantum image processing (QIP) is an interdisciplinary field between\nquantum information science and image processing, which has the potential to\nalleviate some of these challenges by leveraging the power of quantum\ncomputing. In this work, we compare and examine the compression properties of\nfour different Quantum Image Representations (QImRs): namely, Tensor Network\nRepresentation (TNR), Flexible Representation of Quantum Image (FRQI), Novel\nEnhanced Quantum Representation NEQR, and Quantum Probability Image Encoding\n(QPIE). Our simulations show that FRQI performs a higher compression of image\ninformation than TNR, NEQR, and QPIE. Furthermore, we investigate the trade-off\nbetween accuracy and memory in binary classification problems, evaluating the\nperformance of quantum kernels based on QImRs compared to the classical linear\nkernel. Our results indicate that quantum kernels provide comparable\nclassification average accuracy but require exponentially fewer resources for\nimage storage.",
      "pdf_url": "http://arxiv.org/pdf/2507.22039v1",
      "published": "2025-07-29T17:40:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22039v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "81P68, 81P70, 81P40, 68Q12, 68T01",
        "I.2; I.4; J.2"
      ]
    },
    {
      "title": "Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security",
      "authors": [
        "Muzhi Dai",
        "Shixuan Liu",
        "Zhiyuan Zhao",
        "Junyu Gao",
        "Hao Sun",
        "Xuelong Li"
      ],
      "abstract": "The rapid advancement of multimodal large language models (MLLMs) has led to\nbreakthroughs in various applications, yet their security remains a critical\nchallenge. One pressing issue involves unsafe image-query pairs--jailbreak\ninputs specifically designed to bypass security constraints and elicit\nunintended responses from MLLMs. Compared to general multimodal data, such\nunsafe inputs are relatively sparse, which limits the diversity and richness of\ntraining samples available for developing robust defense models. Meanwhile,\nexisting guardrail-type methods rely on external modules to enforce security\nconstraints but fail to address intrinsic vulnerabilities within MLLMs.\nTraditional supervised fine-tuning (SFT), on the other hand, often over-refuses\nharmless inputs, compromising general performance. Given these challenges, we\npropose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack\ntraining method to enhance the security of MLLMs. SecTOW consists of two\nmodules: a defender and an auxiliary attacker, both trained iteratively using\nreinforcement learning (GRPO). During the iterative process, the attacker\nidentifies security vulnerabilities in the defense model and expands jailbreak\ndata. The expanded data are then used to train the defender, enabling it to\naddress identified security vulnerabilities. We also design reward mechanisms\nused for GRPO to simplify the use of response labels, reducing dependence on\ncomplex generative labels and enabling the efficient use of synthetic data.\nAdditionally, a quality monitoring mechanism is used to mitigate the defender's\nover-refusal of harmless inputs and ensure the diversity of the jailbreak data\ngenerated by the attacker. Experimental results on safety-specific and general\nbenchmarks demonstrate that SecTOW significantly improves security while\npreserving general performance.",
      "pdf_url": "http://arxiv.org/pdf/2507.22037v1",
      "published": "2025-07-29T17:39:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22037v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "UserBench: An Interactive Gym Environment for User-Centric Agents",
      "authors": [
        "Cheng Qian",
        "Zuxin Liu",
        "Akshara Prabhakar",
        "Zhiwei Liu",
        "Jianguo Zhang",
        "Haolin Chen",
        "Heng Ji",
        "Weiran Yao",
        "Shelby Heinecke",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang"
      ],
      "abstract": "Large Language Models (LLMs)-based agents have made impressive progress in\nreasoning and tool use, enabling them to solve complex tasks. However, their\nability to proactively collaborate with users, especially when goals are vague,\nevolving, or indirectly expressed, remains underexplored. To address this gap,\nwe introduce UserBench, a user-centric benchmark designed to evaluate agents in\nmulti-turn, preference-driven interactions. UserBench features simulated users\nwho start with underspecified goals and reveal preferences incrementally,\nrequiring agents to proactively clarify intent and make grounded decisions with\ntools. Our evaluation of leading open- and closed-source LLMs reveals a\nsignificant disconnect between task completion and user alignment. For\ninstance, models provide answers that fully align with all user intents only\n20% of the time on average, and even the most advanced models uncover fewer\nthan 30% of all user preferences through active interaction. These results\nhighlight the challenges of building agents that are not just capable task\nexecutors, but true collaborative partners. UserBench offers an interactive\nenvironment to measure and advance this critical capability.",
      "pdf_url": "http://arxiv.org/pdf/2507.22034v1",
      "published": "2025-07-29T17:34:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22034v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from Free-Text Reports",
      "authors": [
        "Mohammed Baharoon",
        "Luyang Luo",
        "Michael Moritz",
        "Abhinav Kumar",
        "Sung Eun Kim",
        "Xiaoman Zhang",
        "Miao Zhu",
        "Mahmoud Hussain Alabbad",
        "Maha Sbayel Alhazmi",
        "Neel P. Mistry",
        "Kent Ryan Kleinschmidt",
        "Brady Chrisler",
        "Sathvik Suryadevara",
        "Sri Sai Dinesh Jaliparthi",
        "Noah Michael Prudlo",
        "Mark David Marino",
        "Jeremy Palacio",
        "Rithvik Akula",
        "Hong-Yu Zhou",
        "Ibrahim Ethem Hamamci",
        "Scott J. Adams",
        "Hassan Rayhan AlOmaish",
        "Pranav Rajpurkar"
      ],
      "abstract": "We present ReXGroundingCT, the first publicly available dataset to link\nfree-text radiology findings with pixel-level segmentations in 3D chest CT\nscans that is manually annotated. While prior datasets have relied on\nstructured labels or predefined categories, ReXGroundingCT captures the full\nexpressiveness of clinical language represented in free text and grounds it to\nspatially localized 3D segmentation annotations in volumetric imaging. This\naddresses a critical gap in medical AI: the ability to connect complex,\ndescriptive text, such as \"3 mm nodule in the left lower lobe\", to its precise\nanatomical location in three-dimensional space, a capability essential for\ngrounded radiology report generation systems. The dataset comprises 3,142\nnon-contrast chest CT scans paired with standardized radiology reports from the\nCT-RATE dataset. Using a systematic three-stage pipeline, GPT-4 was used to\nextract positive lung and pleural findings, which were then manually segmented\nby expert annotators. A total of 8,028 findings across 16,301 entities were\nannotated, with quality control performed by board-certified radiologists.\nApproximately 79% of findings are focal abnormalities, while 21% are non-focal.\nThe training set includes up to three representative segmentations per finding,\nwhile the validation and test sets contain exhaustive labels for each finding\nentity. ReXGroundingCT establishes a new benchmark for developing and\nevaluating sentence-level grounding and free-text medical segmentation models\nin chest CT. The dataset can be accessed at\nhttps://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.",
      "pdf_url": "http://arxiv.org/pdf/2507.22030v1",
      "published": "2025-07-29T17:27:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22030v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding",
      "authors": [
        "Shuquan Lian",
        "Yuhang Wu",
        "Jia Ma",
        "Zihan Song",
        "Bingqi Chen",
        "Xiawu Zheng",
        "Hui Li"
      ],
      "abstract": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.",
      "pdf_url": "http://arxiv.org/pdf/2507.22025v2",
      "published": "2025-07-29T17:22:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22025v2",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation",
      "authors": [
        "Raju Ningappa Mulawade",
        "Christoph Garth",
        "Alexander Wiebel"
      ],
      "abstract": "We propose a novel segmentation-based explainable artificial intelligence\n(XAI) method for neural networks working on point cloud classification. As one\nbuilding block of this method, we propose a novel point-shifting mechanism to\nintroduce perturbations in point cloud data. Recently, AI has seen an\nexponential growth. Hence, it is important to understand the decision-making\nprocess of AI algorithms when they are applied in critical areas. Our work\nfocuses on explaining AI algorithms that classify point cloud data. An\nimportant aspect of the methods used for explaining AI algorithms is their\nability to produce explanations that are easy for humans to understand. This\nallows them to analyze the AI algorithms better and make appropriate decisions\nbased on that analysis. Therefore, in this work, we intend to generate\nmeaningful explanations that can be easily interpreted by humans. The point\ncloud data we consider represents 3D objects such as cars, guitars, and\nlaptops. We make use of point cloud segmentation models to generate\nexplanations for the working of classification models. The segments are used to\nintroduce perturbations into the input point cloud data and generate saliency\nmaps. The perturbations are introduced using the novel point-shifting mechanism\nproposed in this work which ensures that the shifted points no longer influence\nthe output of the classification algorithm. In contrast to previous methods,\nthe segments used by our method are meaningful, i.e. humans can easily\ninterpret the meaning of the segments. Thus, the benefit of our method over\nother methods is its ability to produce more meaningful saliency maps. We\ncompare our method with the use of classical clustering algorithms to generate\nexplanations. We also analyze the saliency maps generated for example inputs\nusing our method to demonstrate the usefulness of the method in generating\nmeaningful explanations.",
      "pdf_url": "http://arxiv.org/pdf/2507.22020v1",
      "published": "2025-07-29T17:12:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22020v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring the Stratified Space Structure of an RL Game with the Volume Growth Transform",
      "authors": [
        "Justin Curry",
        "Brennan Lagasse",
        "Ngoc B. Lam",
        "Gregory Cox",
        "David Rosenbluth",
        "Alberto Speranzon"
      ],
      "abstract": "In this work, we explore the structure of the embedding space of a\ntransformer model trained for playing a particular reinforcement learning (RL)\ngame. Specifically, we investigate how a transformer-based Proximal Policy\nOptimization (PPO) model embeds visual inputs in a simple environment where an\nagent must collect \"coins\" while avoiding dynamic obstacles consisting of\n\"spotlights.\" By adapting Robinson et al.'s study of the volume growth\ntransform for LLMs to the RL setting, we find that the token embedding space\nfor our visual coin collecting game is also not a manifold, and is better\nmodeled as a stratified space, where local dimension can vary from point to\npoint. We further strengthen Robinson's method by proving that fairly general\nvolume growth curves can be realized by stratified spaces. Finally, we carry\nout an analysis that suggests that as an RL agent acts, its latent\nrepresentation alternates between periods of low local dimension, while\nfollowing a fixed sub-strategy, and bursts of high local dimension, where the\nagent achieves a sub-goal (e.g., collecting an object) or where the\nenvironmental complexity increases (e.g., more obstacles appear). Consequently,\nour work suggests that the distribution of dimensions in a stratified latent\nspace may provide a new geometric indicator of complexity for RL games.",
      "pdf_url": "http://arxiv.org/pdf/2507.22010v1",
      "published": "2025-07-29T17:00:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22010v1",
      "categories": [
        "math.AT",
        "cs.AI",
        "cs.CG",
        "cs.LG",
        "math.DG",
        "58A35"
      ]
    },
    {
      "title": "PHAX: A Structured Argumentation Framework for User-Centered Explainable AI in Public Health and Biomedical Sciences",
      "authors": [
        "Bahar İlgen",
        "Akshat Dubey",
        "Georges Hattab"
      ],
      "abstract": "Ensuring transparency and trust in AI-driven public health and biomedical\nsciences systems requires more than accurate predictions-it demands\nexplanations that are clear, contextual, and socially accountable. While\nexplainable AI (XAI) has advanced in areas like feature attribution and model\ninterpretability, most methods still lack the structure and adaptability needed\nfor diverse health stakeholders, including clinicians, policymakers, and the\ngeneral public. We introduce PHAX-a Public Health Argumentation and\neXplainability framework-that leverages structured argumentation to generate\nhuman-centered explanations for AI outputs. PHAX is a multi-layer architecture\ncombining defeasible reasoning, adaptive natural language techniques, and user\nmodeling to produce context-aware, audience-specific justifications. More\nspecifically, we show how argumentation enhances explainability by supporting\nAI-driven decision-making, justifying recommendations, and enabling interactive\ndialogues across user types. We demonstrate the applicability of PHAX through\nuse cases such as medical term simplification, patient-clinician communication,\nand policy justification. In particular, we show how simplification decisions\ncan be modeled as argument chains and personalized based on user\nexpertise-enhancing both interpretability and trust. By aligning formal\nreasoning methods with communicative demands, PHAX contributes to a broader\nvision of transparent, human-centered AI in public health.",
      "pdf_url": "http://arxiv.org/pdf/2507.22009v1",
      "published": "2025-07-29T17:00:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22009v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation",
      "authors": [
        "Yida Tao",
        "Yen-Chia Hsu"
      ],
      "abstract": "Industrial smoke segmentation is critical for air-quality monitoring and\nenvironmental protection but is often hampered by the high cost and scarcity of\npixel-level annotations in real-world settings. We introduce CEDANet, a\nhuman-in-the-loop, class-aware domain adaptation framework that uniquely\nintegrates weak, citizen-provided video-level labels with adversarial feature\nalignment. Specifically, we refine pseudo-labels generated by a source-trained\nsegmentation model using citizen votes, and employ class-specific domain\ndiscriminators to transfer rich source-domain representations to the industrial\ndomain. Comprehensive experiments on SMOKE5K and custom IJmond datasets\ndemonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of\n0.261 with citizen feedback, vastly outperforming the baseline model, which\nscored 0.083 and 0.043 respectively. This represents a five-fold increase in\nF1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with\ncitizen-constrained pseudo-labels achieves performance comparable to the same\narchitecture trained on limited 100 fully annotated images with F1-score of\n0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully\nsupervised-level accuracy without target-domain annotations. Our research\nvalidates the scalability and cost-efficiency of combining citizen science with\nweakly supervised domain adaptation, offering a practical solution for complex,\ndata-scarce environmental monitoring applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.22002v1",
      "published": "2025-07-29T16:53:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22002v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Staining and locking computer vision models without retraining",
      "authors": [
        "Oliver J. Sutton",
        "Qinghua Zhou",
        "George Leete",
        "Alexander N. Gorban",
        "Ivan Y. Tyukin"
      ],
      "abstract": "We introduce new methods of staining and locking computer vision models, to\nprotect their owners' intellectual property. Staining, also known as\nwatermarking, embeds secret behaviour into a model which can later be used to\nidentify it, while locking aims to make a model unusable unless a secret\ntrigger is inserted into input images. Unlike existing methods, our algorithms\ncan be used to stain and lock pre-trained models without requiring fine-tuning\nor retraining, and come with provable, computable guarantees bounding their\nworst-case false positive rates. The stain and lock are implemented by directly\nmodifying a small number of the model's weights and have minimal impact on the\n(unlocked) model's performance. Locked models are unlocked by inserting a small\n`trigger patch' into the corner of the input image. We present experimental\nresults showing the efficacy of our methods and demonstrating their practical\nperformance on a variety of computer vision models.",
      "pdf_url": "http://arxiv.org/pdf/2507.22000v1",
      "published": "2025-07-29T16:47:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.22000v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T07, 68T45, 68W40",
        "I.2.10; F.2.0; K.5.1; K.6.5"
      ]
    },
    {
      "title": "Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation",
      "authors": [
        "Siddhartha Pradhan",
        "Shikshya Shiwakoti",
        "Neha Bathuri"
      ],
      "abstract": "We investigate whether knowledge distillation (KD) from multiple\nheterogeneous teacher models can enhance the generation of transferable\nadversarial examples. A lightweight student model is trained using two KD\nstrategies: curriculum-based switching and joint optimization, with ResNet50\nand DenseNet-161 as teachers. The trained student is then used to generate\nadversarial examples using FG, FGS, and PGD attacks, which are evaluated\nagainst a black-box target model (GoogLeNet). Our results show that student\nmodels distilled from multiple teachers achieve attack success rates comparable\nto ensemble-based baselines, while reducing adversarial example generation time\nby up to a factor of six. An ablation study further reveals that lower\ntemperature settings and the inclusion of hard-label supervision significantly\nenhance transferability. These findings suggest that KD can serve not only as a\nmodel compression technique but also as a powerful tool for improving the\nefficiency and effectiveness of black-box adversarial attacks.",
      "pdf_url": "http://arxiv.org/pdf/2507.21992v1",
      "published": "2025-07-29T16:43:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21992v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge",
      "authors": [
        "Zihan Zhao",
        "Bo Chen",
        "Ziping Wan",
        "Lu Chen",
        "Xuanze Lin",
        "Shiyang Yu",
        "Situo Zhang",
        "Da Ma",
        "Zichen Zhu",
        "Danyang Zhang",
        "Huayang Wang",
        "Zhongyang Dai",
        "Liyang Wen",
        "Xin Chen",
        "Kai Yu"
      ],
      "abstract": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves cutting-edge performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2507.21990v2",
      "published": "2025-07-29T16:40:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21990v2",
      "categories": [
        "cs.CE",
        "cs.AI"
      ]
    },
    {
      "title": "The Effect of Compression Techniques on Large Multimodal Language Models in the Medical Domain",
      "authors": [
        "Tanvir Ahmed Khan",
        "Aranya Saha",
        "Ismam Nur Swapnil",
        "Mohammad Ariful Haque"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) hold huge potential for usage in the\nmedical domain, but their computational costs necessitate efficient compression\ntechniques. This paper evaluates the impact of structural pruning and\nactivation-aware quantization on a fine-tuned LLAVA model for medical\napplications. We propose a novel layer selection method for pruning, analyze\ndifferent quantization techniques, and assess the performance trade-offs in a\nprune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B\nparameters to run within 4 GB of VRAM, reducing memory usage by 70% while\nachieving 4% higher model performance compared to traditional pruning and\nquantization techniques in the same compression ratio.",
      "pdf_url": "http://arxiv.org/pdf/2507.21976v1",
      "published": "2025-07-29T16:25:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21976v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks",
      "authors": [
        "Mohamed Sana",
        "Nicola Piovesan",
        "Antonio De Domenico",
        "Yibin Kang",
        "Haozhe Zhang",
        "Merouane Debbah",
        "Fadhel Ayed"
      ],
      "abstract": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.",
      "pdf_url": "http://arxiv.org/pdf/2507.21974v1",
      "published": "2025-07-29T16:21:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21974v1",
      "categories": [
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data & Activities",
      "authors": [
        "Sourish Gunesh Dhekane",
        "Thomas Ploetz"
      ],
      "abstract": "Developing zero-shot human activity recognition (HAR) methods is a critical\ndirection in smart home research -- considering its impact on making HAR\nsystems work across smart homes having diverse sensing modalities, layouts, and\nactivities of interest. The state-of-the-art solutions along this direction are\nbased on generating natural language descriptions of the sensor data and\nfeeding it via a carefully crafted prompt to the LLM to perform classification.\nDespite their performance guarantees, such ``prompt-the-LLM'' approaches carry\nseveral risks, including privacy invasion, reliance on an external service, and\ninconsistent predictions due to version changes, making a case for alternative\nzero-shot HAR methods that do not require prompting the LLMs. In this paper, we\npropose one such solution that models sensor data and activities using natural\nlanguage, leveraging its embeddings to perform zero-shot classification and\nthereby bypassing the need to prompt the LLMs for activity predictions. The\nimpact of our work lies in presenting a detailed case study on six datasets,\nhighlighting how language modeling can bolster HAR systems in zero-shot\nrecognition.",
      "pdf_url": "http://arxiv.org/pdf/2507.21964v1",
      "published": "2025-07-29T16:13:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21964v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Fine-Tuning Code Language Models to Detect Cross-Language Bugs",
      "authors": [
        "Zengyang Li",
        "Yimeng Li",
        "Binbin Huang",
        "Peng Liang",
        "Ran Mo",
        "Hui Liu",
        "Yutao Ma"
      ],
      "abstract": "Multilingual programming, which involves using multiple programming languages\n(PLs) in a single project, is increasingly common due to its benefits. However,\nit introduces cross-language bugs (CLBs), which arise from interactions between\ndifferent PLs and are difficult to detect by single-language bug detection\ntools. This paper investigates the potential of pre-trained code language\nmodels (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language\ncode identification tool, and constructed a CLB dataset involving three PL\ncombinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction\ntypes. We fine-tuned 13 CodeLMs on this dataset and evaluated their\nperformance, analyzing the effects of dataset size, token sequence length, and\ncode comments. Results show that all CodeLMs performed poorly before\nfine-tuning, but exhibited varying degrees of performance improvement after\nfine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably,\nsmall fine-tuned CodeLMs tended to performe better than large ones. CodeLMs\nfine-tuned on single-language bug datasets performed poorly on CLB detection,\ndemonstrating the distinction between CLBs and single-language bugs.\nAdditionally, increasing the fine-tuning dataset size significantly improved\nperformance, while longer token sequences did not necessarily improve the model\nperformance. The impact of code comments varied across models. Some fine-tuned\nCodeLMs' performance was improved, while others showed degraded performance.",
      "pdf_url": "http://arxiv.org/pdf/2507.21954v1",
      "published": "2025-07-29T16:06:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21954v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation",
      "authors": [
        "Yi Kong",
        "Dianxi Shi",
        "Guoli Yang",
        "Zhang ke-di",
        "Chenlin Huang",
        "Xiaopeng Li",
        "Songchang Jin"
      ],
      "abstract": "The recent advancement of autonomous agents powered by Large Language Models\n(LLMs) has demonstrated significant potential for automating tasks on mobile\ndevices through graphical user interfaces (GUIs). Despite initial progress,\nthese agents still face challenges when handling complex real-world tasks.\nThese challenges arise from a lack of knowledge about real-life mobile\napplications in LLM-based agents, which may lead to ineffective task planning\nand even cause hallucinations. To address these challenges, we propose a novel\nLLM-based agent framework called MapAgent that leverages memory constructed\nfrom historical trajectories to augment current task planning. Specifically, we\nfirst propose a trajectory-based memory mechanism that transforms task\nexecution trajectories into a reusable and structured page-memory database.\nEach page within a trajectory is extracted as a compact yet comprehensive\nsnapshot, capturing both its UI layout and functional context. Secondly, we\nintroduce a coarse-to-fine task planning approach that retrieves relevant pages\nfrom the memory database based on similarity and injects them into the LLM\nplanner to compensate for potential deficiencies in understanding real-world\napp scenarios, thereby achieving more informed and context-aware task planning.\nFinally, planned tasks are transformed into executable actions through a task\nexecutor supported by a dual-LLM architecture, ensuring effective tracking of\ntask progress. Experimental results in real-world scenarios demonstrate that\nMapAgent achieves superior performance to existing methods. The code will be\nopen-sourced to support further research.",
      "pdf_url": "http://arxiv.org/pdf/2507.21953v1",
      "published": "2025-07-29T16:05:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21953v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal",
      "authors": [
        "Jiyu Wu",
        "Yifan Liu",
        "Jiancheng Huang",
        "Mingfu Yan",
        "Shifeng Chen"
      ],
      "abstract": "Existing shadow removal methods often rely on shadow masks, which are\nchallenging to acquire in real-world scenarios. Exploring intrinsic image cues,\nsuch as local contrast information, presents a potential alternative for\nguiding shadow removal in the absence of explicit masks. However, the cue's\ninherent ambiguity becomes a critical limitation in complex scenes, where it\ncan fail to distinguish true shadows from low-reflectance objects and intricate\nbackground textures. To address this motivation, we propose the Adaptive Gated\nDual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs\nthe contrast prior to effectively disentangle shadow features from confounding\nvisual elements. Furthermore, to tackle the persistent challenge of restoring\nsoft shadow boundaries and fine-grained details, we introduce a diffusion-based\nFrequency-Contrast Fusion Network (FCFN) that leverages high-frequency and\ncontrast cues to guide the generative process. Extensive experiments\ndemonstrate that our method achieves state-of-the-art results among mask-free\napproaches while maintaining competitive performance relative to mask-based\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2507.21949v1",
      "published": "2025-07-29T16:00:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21949v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Generalization in Data-free Quantization via Mixup-class Prompting",
      "authors": [
        "Jiwoong Park",
        "Chaeun Lee",
        "Yongseok Choi",
        "Sein Park",
        "Deokki Hong",
        "Jungwook Choi"
      ],
      "abstract": "Post-training quantization (PTQ) improves efficiency but struggles with\nlimited calibration data, especially under privacy constraints. Data-free\nquantization (DFQ) mitigates this by generating synthetic images using\ngenerative models such as generative adversarial networks (GANs) and\ntext-conditioned latent diffusion models (LDMs), while applying existing PTQ\nalgorithms. However, the relationship between generated synthetic images and\nthe generalizability of the quantized model during PTQ remains underexplored.\nWithout investigating this relationship, synthetic images generated by previous\nprompt engineering methods based on single-class prompts suffer from issues\nsuch as polysemy, leading to performance degradation. We propose\n\\textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses\nmultiple class labels at the text prompt level to generate diverse, robust\nsynthetic data. This approach enhances generalization, and improves\noptimization stability in PTQ. We provide quantitative insights through\ngradient norm and generalization error analysis. Experiments on convolutional\nneural networks (CNNs) and vision transformers (ViTs) show that our method\nconsistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore,\nit pushes the performance boundary in extremely low-bit scenarios, achieving\nnew state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation\n(W2A4) quantization.",
      "pdf_url": "http://arxiv.org/pdf/2507.21947v1",
      "published": "2025-07-29T16:00:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21947v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Post-Training Large Language Models via Reinforcement Learning from Self-Feedback",
      "authors": [
        "Carel van Niekerk",
        "Renato Vukovic",
        "Benjamin Matthias Ruppik",
        "Hsien-chin Lin",
        "Milica Gašić"
      ],
      "abstract": "Large Language Models (LLMs) often produce plausible but poorly-calibrated\nanswers, limiting their reliability on reasoning-intensive tasks. We present\nReinforcement Learning from Self-Feedback (RLSF), a post-training stage that\nuses the model's own confidence as an intrinsic reward, mimicking how humans\nlearn in the absence of external feedback. After a frozen LLM generates several\nchain-of-thought solutions, we define and compute the confidence of each final\nanswer span and rank the traces accordingly. These synthetic preferences are\nthen used to fine-tune the policy with standard preference optimization,\nsimilar to RLHF yet requiring no human labels, gold answers, or externally\ncurated rewards.\n  RLSF simultaneously (i) refines the model's probability estimates --\nrestoring well-behaved calibration -- and (ii) strengthens step-by-step\nreasoning, yielding improved performance on arithmetic reasoning and\nmultiple-choice question answering.\n  By turning a model's own uncertainty into useful self-feedback, RLSF affirms\nreinforcement learning on intrinsic model behaviour as a principled and\ndata-efficient component of the LLM post-training pipeline and warrents further\nresearch in intrinsic rewards for LLM post-training.",
      "pdf_url": "http://arxiv.org/pdf/2507.21931v1",
      "published": "2025-07-29T15:46:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21931v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Libra: Large Chinese-based Safeguard for AI Content",
      "authors": [
        "Ziyang Chen",
        "Huimu Yu",
        "Xing Wu",
        "Dongqin Liu",
        "Songlin Hu"
      ],
      "abstract": "Large language models (LLMs) excel in text understanding and generation but\nraise significant safety and ethical concerns in high-stakes applications. To\nmitigate these risks, we present Libra-Guard, a cutting-edge safeguard system\ndesigned to enhance the safety of Chinese-based LLMs. Leveraging a two-stage\ncurriculum training pipeline, Libra-Guard enhances data efficiency by employing\nguard pretraining on synthetic samples, followed by fine-tuning on\nhigh-quality, real-world data, thereby significantly reducing reliance on\nmanual annotations. To enable rigorous safety evaluations, we also introduce\nLibra-Test, the first benchmark specifically designed to evaluate the\neffectiveness of safeguard systems for Chinese content. It covers seven\ncritical harm scenarios and includes over 5,700 samples annotated by domain\nexperts. Experiments show that Libra-Guard achieves 86.79% accuracy,\noutperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat\n(65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o.\nThese contributions establish a robust framework for advancing the safety\ngovernance of Chinese LLMs and represent a tentative step toward developing\nsafer, more reliable Chinese AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.21929v1",
      "published": "2025-07-29T15:45:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21929v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda",
      "authors": [
        "Christian Meske",
        "Tobias Hermanns",
        "Esther von der Weiden",
        "Kai-Uwe Loser",
        "Thorsten Berger"
      ],
      "abstract": "Software development is undergoing a fundamental transformation as vibe\ncoding becomes widespread, with large portions of contemporary codebases now\nbeing AI-generated. The disconnect between rapid adoption and limited\nconceptual understanding highlights the need for an inquiry into this emerging\nparadigm. Drawing on an intent perspective and historical analysis, we define\nvibe coding as a software development paradigm where humans and generative AI\nengage in collaborative flow to co-create software artifacts through natural\nlanguage dialogue, shifting the mediation of developer intent from\ndeterministic instruction to probabilistic inference. By intent mediation, we\nrefer to the fundamental process through which developers translate their\nconceptual goals into representations that computational systems can execute.\nOur results show that vibe coding reconfigures cognitive work by redistributing\nepistemic labor between humans and machines, shifting the expertise in the\nsoftware development process away from traditional areas such as design or\ntechnical implementation toward collaborative orchestration. We identify key\nopportunities, including democratization, acceleration, and systemic leverage,\nalongside risks, such as black box codebases, responsibility gaps, and\necosystem bias. We conclude with a research agenda spanning human-,\ntechnology-, and organization-centered directions to guide future\ninvestigations of this paradigm.",
      "pdf_url": "http://arxiv.org/pdf/2507.21928v1",
      "published": "2025-07-29T15:44:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21928v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "SwinECAT: A Transformer-based fundus disease classification model with Shifted Window Attention and Efficient Channel Attention",
      "authors": [
        "Peiran Gu",
        "Teng Yao",
        "Mengshen He",
        "Fuhao Duan",
        "Feiyan Liu",
        "RenYuan Peng",
        "Bao Ge"
      ],
      "abstract": "In recent years, artificial intelligence has been increasingly applied in the\nfield of medical imaging. Among these applications, fundus image analysis\npresents special challenges, including small lesion areas in certain fundus\ndiseases and subtle inter-disease differences, which can lead to reduced\nprediction accuracy and overfitting in the models. To address these challenges,\nthis paper proposes the Transformer-based model SwinECAT, which combines the\nShifted Window (Swin) Attention with the Efficient Channel Attention (ECA)\nAttention. SwinECAT leverages the Swin Attention mechanism in the Swin\nTransformer backbone to effectively capture local spatial structures and\nlong-range dependencies within fundus images. The lightweight ECA mechanism is\nincorporated to guide the SwinECAT's attention toward critical feature\nchannels, enabling more discriminative feature representation. In contrast to\nprevious studies that typically classify fundus images into 4 to 6 categories,\nthis work expands fundus disease classification to 9 distinct types, thereby\nenhancing the granularity of diagnosis. We evaluate our method on the Eye\nDisease Image Dataset (EDID) containing 16,140 fundus images for 9-category\nclassification. Experimental results demonstrate that SwinECAT achieves 88.29\\%\naccuracy, with weighted F1-score of 0.88 and macro F1-score of 0.90. The\nclassification results of our proposed model SwinECAT significantly outperform\nthe baseline Swin Transformer and multiple compared baseline models. To our\nknowledge, this represents the highest reported performance for 9-category\nclassification on this public dataset.",
      "pdf_url": "http://arxiv.org/pdf/2507.21922v1",
      "published": "2025-07-29T15:35:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21922v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Training language models to be warm and empathetic makes them less reliable and more sycophantic",
      "authors": [
        "Lujain Ibrahim",
        "Franziska Sofia Hafner",
        "Luc Rocher"
      ],
      "abstract": "Artificial intelligence (AI) developers are increasingly building language\nmodels with warm and empathetic personas that millions of people now use for\nadvice, therapy, and companionship. Here, we show how this creates a\nsignificant trade-off: optimizing language models for warmth undermines their\nreliability, especially when users express vulnerability. We conducted\ncontrolled experiments on five language models of varying sizes and\narchitectures, training them to produce warmer, more empathetic responses, then\nevaluating them on safety-critical tasks. Warm models showed substantially\nhigher error rates (+10 to +30 percentage points) than their original\ncounterparts, promoting conspiracy theories, providing incorrect factual\ninformation, and offering problematic medical advice. They were also\nsignificantly more likely to validate incorrect user beliefs, particularly when\nuser messages expressed sadness. Importantly, these effects were consistent\nacross different model architectures, and occurred despite preserved\nperformance on standard benchmarks, revealing systematic risks that current\nevaluation practices may fail to detect. As human-like AI systems are deployed\nat an unprecedented scale, our findings indicate a need to rethink how we\ndevelop and oversee these systems that are reshaping human relationships and\nsocial interaction.",
      "pdf_url": "http://arxiv.org/pdf/2507.21919v2",
      "published": "2025-07-29T15:33:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21919v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Evaluating Deepfake Detectors in the Wild",
      "authors": [
        "Viacheslav Pirogov",
        "Maksim Artemev"
      ],
      "abstract": "Deepfakes powered by advanced machine learning models present a significant\nand evolving threat to identity verification and the authenticity of digital\nmedia. Although numerous detectors have been developed to address this problem,\ntheir effectiveness has yet to be tested when applied to real-world data. In\nthis work we evaluate modern deepfake detectors, introducing a novel testing\nprocedure designed to mimic real-world scenarios for deepfake detection. Using\nstate-of-the-art deepfake generation methods, we create a comprehensive dataset\ncontaining more than 500,000 high-quality deepfake images. Our analysis shows\nthat detecting deepfakes still remains a challenging task. The evaluation shows\nthat in fewer than half of the deepfake detectors tested achieved an AUC score\ngreater than 60%, with the lowest being 50%. We demonstrate that basic image\nmanipulations, such as JPEG compression or image enhancement, can significantly\nreduce model performance. All code and data are publicly available at\nhttps://github.com/messlav/Deepfake-Detectors-in-the-Wild.",
      "pdf_url": "http://arxiv.org/pdf/2507.21905v1",
      "published": "2025-07-29T15:17:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21905v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LLM-based Content Classification Approach for GitHub Repositories by the README Files",
      "authors": [
        "Malik Uzair Mehmood",
        "Shahid Hussain",
        "Wen Li Wang",
        "Muhammad Usama Malik"
      ],
      "abstract": "GitHub is the world's most popular platform for storing, sharing, and\nmanaging code. Every GitHub repository has a README file associated with it.\nThe README files should contain project-related information as per the\nrecommendations of GitHub to support the usage and improvement of repositories.\nHowever, GitHub repository owners sometimes neglected these recommendations.\nThis prevents a GitHub repository from reaching its full potential. This\nresearch posits that the comprehensiveness of a GitHub repository's README file\nsignificantly influences its adoption and utilization, with a lack of detail\npotentially hindering its full potential for widespread engagement and impact\nwithin the research community. Large Language Models (LLMs) have shown great\nperformance in many text-based tasks including text classification, text\ngeneration, text summarization and text translation. In this study, an approach\nis developed to fine-tune LLMs for automatically classifying different sections\nof GitHub README files. Three encoder-only LLMs are utilized, including BERT,\nDistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a\ngold-standard dataset consisting of 4226 README file sections. This approach\noutperforms current state-of-the-art methods and has achieved an overall F1\nscore of 0.98. Moreover, we have also investigated the use of\nParameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation\n(LoRA) and shown an economical alternative to full fine-tuning without\ncompromising much performance. The results demonstrate the potential of using\nLLMs in designing an automatic classifier for categorizing the content of\nGitHub README files. Consequently, this study contributes to the development of\nautomated tools for GitHub repositories to improve their identifications and\npotential usages.",
      "pdf_url": "http://arxiv.org/pdf/2507.21899v1",
      "published": "2025-07-29T15:09:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21899v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "title": "Data-driven quantum Koopman method for simulating nonlinear dynamics",
      "authors": [
        "Baoyang Zhang",
        "Zhen Lu",
        "Yaomin Zhao",
        "Yue Yang"
      ],
      "abstract": "Quantum computation offers potential exponential speedups for simulating\ncertain physical systems, but its application to nonlinear dynamics is\ninherently constrained by the requirement of unitary evolution. We propose the\nquantum Koopman method (QKM), a data-driven framework that bridges this gap\nthrough transforming nonlinear dynamics into linear unitary evolution in\nhigher-dimensional observable spaces. Leveraging the Koopman operator theory to\nachieve a global linearization, our approach maps system states into a\nhierarchy of Hilbert spaces using a deep autoencoder. Within the linearized\nembedding spaces, the state representation is decomposed into modulus and phase\ncomponents, and the evolution is governed by a set of unitary Koopman operators\nthat act exclusively on the phase. These operators are constructed from\ndiagonal Hamiltonians with coefficients learned from data, a structure designed\nfor efficient implementation on quantum hardware. This architecture enables\ndirect multi-step prediction, and the operator's computational complexity\nscales logarithmically with the observable space dimension. The QKM is\nvalidated across diverse nonlinear systems. Its predictions maintain relative\nerrors below 6% for reaction-diffusion systems and shear flows, and capture key\nstatistics in 2D turbulence. This work establishes a practical pathway for\nquantum-accelerated simulation of nonlinear phenomena, exploring a framework\nbuilt on the synergy between deep learning for global linearization and quantum\nalgorithms for unitary dynamics evolution.",
      "pdf_url": "http://arxiv.org/pdf/2507.21890v1",
      "published": "2025-07-29T15:00:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21890v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph",
        "physics.flu-dyn"
      ]
    },
    {
      "title": "Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline",
      "authors": [
        "Stefanos Gkikas",
        "Ioannis Kyprakis",
        "Manolis Tsiknakis"
      ],
      "abstract": "Pain is a complex condition affecting a large portion of the population.\nAccurate and consistent evaluation is essential for individuals experiencing\npain, and it supports the development of effective and advanced management\nstrategies. Automatic pain assessment systems provide continuous monitoring and\nsupport clinical decision-making, aiming to reduce distress and prevent\nfunctional decline. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed method introduces a pipeline that leverages respiration as the input\nsignal and incorporates a highly efficient cross-attention transformer\nalongside a multi-windowing strategy. Extensive experiments demonstrate that\nrespiration is a valuable physiological modality for pain assessment. Moreover,\nexperiments revealed that compact and efficient models, when properly\noptimized, can achieve strong performance, often surpassing larger\ncounterparts. The proposed multi-window approach effectively captures both\nshort-term and long-term features, as well as global characteristics, thereby\nenhancing the model's representational capacity.",
      "pdf_url": "http://arxiv.org/pdf/2507.21886v2",
      "published": "2025-07-29T14:58:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21886v2",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "The Impact of Foundational Models on Patient-Centric e-Health Systems",
      "authors": [
        "Elmira Onagh",
        "Alireza Davoodi",
        "Maleknaz Nayebi"
      ],
      "abstract": "As Artificial Intelligence (AI) becomes increasingly embedded in healthcare\ntechnologies, understanding the maturity of AI in patient-centric applications\nis critical for evaluating its trustworthiness, transparency, and real-world\nimpact. In this study, we investigate the integration and maturity of AI\nfeature integration in 116 patient-centric healthcare applications. Using Large\nLanguage Models (LLMs), we extracted key functional features, which are then\ncategorized into different stages of the Gartner AI maturity model. Our results\nshow that over 86.21\\% of applications remain at the early stages of AI\nintegration, while only 13.79% demonstrate advanced AI integration.",
      "pdf_url": "http://arxiv.org/pdf/2507.21882v1",
      "published": "2025-07-29T14:56:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21882v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image",
      "authors": [
        "Stefanos Gkikas",
        "Ioannis Kyprakis",
        "Manolis Tsiknakis"
      ],
      "abstract": "Pain is a multifaceted phenomenon that affects a substantial portion of the\npopulation. Reliable and consistent evaluation benefits those experiencing pain\nand underpins the development of effective and advanced management strategies.\nAutomatic pain-assessment systems deliver continuous monitoring, inform\nclinical decision-making, and aim to reduce distress while preventing\nfunctional decline. By incorporating physiological signals, these systems\nprovide objective, accurate insights into an individual's condition. This study\nhas been submitted to the \\textit{Second Multimodal Sensing Grand Challenge for\nNext-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline\nthat leverages electrodermal activity signals as input modality. Multiple\nrepresentations of the signal are created and visualized as waveforms, and they\nare jointly visualized within a single multi-representation diagram. Extensive\nexperiments incorporating various processing and filtering techniques, along\nwith multiple representation combinations, demonstrate the effectiveness of the\nproposed approach. It consistently yields comparable, and in several cases\nsuperior, results to traditional fusion methods, establishing it as a robust\nalternative for integrating different signal representations or modalities.",
      "pdf_url": "http://arxiv.org/pdf/2507.21881v2",
      "published": "2025-07-29T14:53:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21881v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis",
      "authors": [
        "Stefanos Gkikas",
        "Ioannis Kyprakis",
        "Manolis Tsiknakis"
      ],
      "abstract": "Pain is a complex and pervasive condition that affects a significant portion\nof the population. Accurate and consistent assessment is essential for\nindividuals suffering from pain, as well as for developing effective management\nstrategies in a healthcare system. Automatic pain assessment systems enable\ncontinuous monitoring, support clinical decision-making, and help minimize\npatient distress while mitigating the risk of functional deterioration.\nLeveraging physiological signals offers objective and precise insights into a\nperson's state, and their integration in a multimodal framework can further\nenhance system performance. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed approach introduces \\textit{Tiny-BioMoE}, a lightweight pretrained\nembedding model for biosignal analysis. Trained on $4.4$ million biosignal\nimage representations and consisting of only $7.3$ million parameters, it\nserves as an effective tool for extracting high-quality embeddings for\ndownstream tasks. Extensive experiments involving electrodermal activity, blood\nvolume pulse, respiratory signals, peripheral oxygen saturation, and their\ncombinations highlight the model's effectiveness across diverse modalities in\nautomatic pain recognition tasks. \\textit{\\textcolor{blue}{The model's\narchitecture (code) and weights are available at\nhttps://github.com/GkikasStefanos/Tiny-BioMoE.",
      "pdf_url": "http://arxiv.org/pdf/2507.21875v2",
      "published": "2025-07-29T14:46:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21875v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data",
      "authors": [
        "Raffaele Pojer",
        "Andrea Passerini",
        "Kim G. Larsen",
        "Manfred Jaeger"
      ],
      "abstract": "Graph neural networks (GNNs) excel at predictive tasks on graph-structured\ndata but often lack the ability to incorporate symbolic domain knowledge and\nperform general reasoning. Relational Bayesian Networks (RBNs), in contrast,\nenable fully generative probabilistic modeling over graph-like structures and\nsupport rich symbolic knowledge and probabilistic inference. This paper\npresents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs,\ncombining the learning strength of GNNs with the flexible reasoning\ncapabilities of RBNs.\n  We develop two implementations of this integration: one compiles GNNs\ndirectly into the native RBN language, while the other maintains the GNN as an\nexternal component. Both approaches preserve the semantics and computational\nproperties of GNNs while fully aligning with the RBN modeling paradigm. We also\npropose a maximum a-posteriori (MAP) inference method for these neuro-symbolic\nmodels.\n  To demonstrate the framework's versatility, we apply it to two distinct\nproblems. First, we transform a GNN for node classification into a collective\nclassification model that explicitly models homo- and heterophilic label\npatterns, substantially improving accuracy. Second, we introduce a\nmulti-objective network optimization problem in environmental planning, where\nMAP inference supports complex decision-making. Both applications include new\npublicly available benchmark datasets.\n  This work introduces a powerful and coherent neuro-symbolic approach to graph\ndata, bridging learning and reasoning in ways that enable novel applications\nand improved performance across diverse tasks.",
      "pdf_url": "http://arxiv.org/pdf/2507.21873v1",
      "published": "2025-07-29T14:43:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21873v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors",
      "authors": [
        "Shouyi Lu",
        "Zihan Lin",
        "Chao Lu",
        "Huanran Wang",
        "Guirong Zhuo",
        "Lianqing Zheng"
      ],
      "abstract": "Autonomous driving systems rely heavily on multimodal perception data to\nunderstand complex environments. However, the long-tailed distribution of\nreal-world data hinders generalization, especially for rare but safety-critical\nvehicle categories. To address this challenge, we propose MultiEditor, a\ndual-branch latent diffusion framework designed to edit images and LiDAR point\nclouds in driving scenarios jointly. At the core of our approach is introducing\n3D Gaussian Splatting (3DGS) as a structural and appearance prior for target\nobjects. Leveraging this prior, we design a multi-level appearance control\nmechanism--comprising pixel-level pasting, semantic-level guidance, and\nmulti-branch refinement--to achieve high-fidelity reconstruction across\nmodalities. We further propose a depth-guided deformable cross-modality\ncondition module that adaptively enables mutual guidance between modalities\nusing 3DGS-rendered depth, significantly enhancing cross-modality consistency.\nExtensive experiments demonstrate that MultiEditor achieves superior\nperformance in visual and geometric fidelity, editing controllability, and\ncross-modality consistency. Furthermore, generating rare-category vehicle data\nwith MultiEditor substantially enhances the detection accuracy of perception\nmodels on underrepresented classes.",
      "pdf_url": "http://arxiv.org/pdf/2507.21872v2",
      "published": "2025-07-29T14:42:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21872v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity",
      "authors": [
        "Xingjian Zhang",
        "Siwei Wen",
        "Wenjun Wu",
        "Lei Huang"
      ],
      "abstract": "Large Language Models (LLMs) have made remarkable progress in enhancing\nstep-by-step reasoning through reinforcement learning. However, the Group\nRelative Policy Optimization (GRPO) algorithm, which relies on sparse reward\nrules, often encounters the issue of identical rewards within groups, leading\nto the advantage collapse problem. Existing works typically address this\nchallenge from two perspectives: enforcing model reflection to enhance response\ndiversity, and introducing internal feedback to augment the training signal\n(advantage). In this work, we begin by analyzing the limitations of model\nreflection and investigating the policy entropy of responses at the\nfine-grained sample level. Based on our experimental findings, we propose the\nEDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage\nand \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the\nproblem of advantage collapse. Extensive experiments on several main reasoning\nbenchmarks demonstrate the effectiveness and superiority of our approach. It is\navailable at https://github.com/ZhangXJ199/EDGE-GRPO.",
      "pdf_url": "http://arxiv.org/pdf/2507.21848v1",
      "published": "2025-07-29T14:23:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21848v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Probabilistic Active Goal Recognition",
      "authors": [
        "Chenyuan Zhang",
        "Cristian Rojas Cardenas",
        "Hamid Rezatofighi",
        "Mor Vered",
        "Buser Say"
      ],
      "abstract": "In multi-agent environments, effective interaction hinges on understanding\nthe beliefs and intentions of other agents. While prior work on goal\nrecognition has largely treated the observer as a passive reasoner, Active Goal\nRecognition (AGR) focuses on strategically gathering information to reduce\nuncertainty. We adopt a probabilistic framework for Active Goal Recognition and\npropose an integrated solution that combines a joint belief update mechanism\nwith a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan\nefficiently and infer the actor's hidden goal without requiring domain-specific\nknowledge. Through comprehensive empirical evaluation in a grid-based domain,\nwe show that our joint belief update significantly outperforms passive goal\nrecognition, and that our domain-independent MCTS performs comparably to our\nstrong domain-specific greedy baseline. These results establish our solution as\na practical and robust framework for goal inference, advancing the field toward\nmore interactive and adaptive multi-agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.21846v1",
      "published": "2025-07-29T14:22:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21846v1",
      "categories": [
        "cs.AI",
        "cs.SC"
      ]
    },
    {
      "title": "Against racing to AGI: Cooperation, deterrence, and catastrophic risks",
      "authors": [
        "Leonard Dung",
        "Max Hellrigel-Holderbaum"
      ],
      "abstract": "AGI Racing is the view that it is in the self-interest of major actors in AI\ndevelopment, especially powerful nations, to accelerate their frontier AI\ndevelopment to build highly capable AI, especially artificial general\nintelligence (AGI), before competitors have a chance. We argue against AGI\nRacing. First, the downsides of racing to AGI are much higher than portrayed by\nthis view. Racing to AGI would substantially increase catastrophic risks from\nAI, including nuclear instability, and undermine the prospects of technical AI\nsafety research to be effective. Second, the expected benefits of racing may be\nlower than proponents of AGI Racing hold. In particular, it is questionable\nwhether winning the race enables complete domination over losers. Third,\ninternational cooperation and coordination, and perhaps carefully crafted\ndeterrence measures, constitute viable alternatives to racing to AGI which have\nmuch smaller risks and promise to deliver most of the benefits that racing to\nAGI is supposed to provide. Hence, racing to AGI is not in anyone's\nself-interest as other actions, particularly incentivizing and seeking\ninternational cooperation around AI issues, are preferable.",
      "pdf_url": "http://arxiv.org/pdf/2507.21839v1",
      "published": "2025-07-29T14:17:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21839v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Analysis of Fourier Neural Operators via Effective Field Theory",
      "authors": [
        "Taeyoung Kim"
      ],
      "abstract": "Fourier Neural Operators (FNOs) have emerged as leading surrogates for\nhigh-dimensional partial-differential equations, yet their stability,\ngeneralization and frequency behavior lack a principled explanation. We present\nthe first systematic effective-field-theory analysis of FNOs in an\ninfinite-dimensional function space, deriving closed recursion relations for\nthe layer kernel and four-point vertex and then examining three practically\nimportant settings-analytic activations, scale-invariant cases and\narchitectures with residual connections. The theory shows that nonlinear\nactivations inevitably couple frequency inputs to high-frequency modes that are\notherwise discarded by spectral truncation, and experiments confirm this\nfrequency transfer. For wide networks we obtain explicit criticality conditions\non the weight-initialization ensemble that keep small input perturbations to\nhave uniform scale across depth, and empirical tests validate these\npredictions. Taken together, our results quantify how nonlinearity enables\nneural operators to capture non-trivial features, supply criteria for\nhyper-parameter selection via criticality analysis, and explain why\nscale-invariant activations and residual connections enhance feature learning\nin FNOs.",
      "pdf_url": "http://arxiv.org/pdf/2507.21833v1",
      "published": "2025-07-29T14:10:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21833v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences",
      "authors": [
        "Andreas Reich",
        "Claudia Thoms",
        "Tobias Schrimpf"
      ],
      "abstract": "LLMs are seeing widespread use for task automation, including automated\ncoding in the social sciences. However, even though researchers have proposed\ndifferent prompting strategies, their effectiveness varies across LLMs and\ntasks. Often trial and error practices are still widespread. We propose\nHALC$-$a general pipeline that allows for the systematic and reliable\nconstruction of optimal prompts for any given coding task and model, permitting\nthe integration of any prompting strategy deemed relevant. To investigate LLM\ncoding and validate our pipeline, we sent a total of 1,512 individual prompts\nto our local LLMs in over two million requests. We test prompting strategies\nand LLM task performance based on few expert codings (ground truth). When\ncompared to these expert codings, we find prompts that code reliably for single\nvariables (${\\alpha}$climate = .76; ${\\alpha}$movement = .78) and across two\nvariables (${\\alpha}$climate = .71; ${\\alpha}$movement = .74) using the LLM\nMistral NeMo. Our prompting strategies are set up in a way that aligns the LLM\nto our codebook$-$we are not optimizing our codebook for LLM friendliness. Our\npaper provides insights into the effectiveness of different prompting\nstrategies, crucial influencing factors, and the identification of reliable\nprompts for each coding task and model.",
      "pdf_url": "http://arxiv.org/pdf/2507.21831v1",
      "published": "2025-07-29T14:10:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21831v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework",
      "authors": [
        "Kuiye Ding",
        "Fanda Fan",
        "Yao Wang",
        "Ruijie jian",
        "Xiaorui Wang",
        "Luqi Gong",
        "Yishan Jiang",
        "Chunjie Luo an Jianfeng Zhan"
      ],
      "abstract": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance.",
      "pdf_url": "http://arxiv.org/pdf/2507.21830v2",
      "published": "2025-07-29T14:08:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21830v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "An Agentic AI for a New Paradigm in Business Process Development",
      "authors": [
        "Mohammad Azarijafari",
        "Luisa Mich",
        "Michele Missikoff"
      ],
      "abstract": "Artificial Intelligence agents represent the next major revolution in the\ncontinuous technological evolution of industrial automation. In this paper, we\nintroduce a new approach for business process design and development that\nleverages the capabilities of Agentic AI. Departing from the traditional\ntask-based approach to business process design, we propose an agent-based\nmethod, where agents contribute to the achievement of business goals,\nidentified by a set of business objects. When a single agent cannot fulfill a\ngoal, we have a merge goal that can be achieved through the collaboration of\nmultiple agents. The proposed model leads to a more modular and intelligent\nbusiness process development by organizing it around goals, objects, and\nagents. As a result, this approach enables flexible and context-aware\nautomation in dynamic industrial environments.",
      "pdf_url": "http://arxiv.org/pdf/2507.21823v1",
      "published": "2025-07-29T13:58:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21823v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
      "authors": [
        "Junzhe Li",
        "Yutao Cui",
        "Tao Huang",
        "Yinping Ma",
        "Chun Fan",
        "Miles Yang",
        "Zhao Zhong"
      ],
      "abstract": "Although GRPO substantially enhances flow matching models in human preference\nalignment of image generation, methods such as FlowGRPO still exhibit\ninefficiency due to the necessity of sampling and optimizing over all denoising\nsteps specified by the Markov Decision Process (MDP). In this paper, we propose\n$\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed\nsampling strategies through the integration of stochastic differential\nequations (SDE) and ordinary differential equations (ODE). This streamlines the\noptimization process within the MDP to improve efficiency and boost\nperformance. Specifically, MixGRPO introduces a sliding window mechanism, using\nSDE sampling and GRPO-guided optimization only within the window, while\napplying ODE sampling outside. This design confines sampling randomness to the\ntime-steps within the window, thereby reducing the optimization overhead, and\nallowing for more focused gradient updates to accelerate convergence.\nAdditionally, as time-steps beyond the sliding window are not involved in\noptimization, higher-order solvers are supported for sampling. So we present a\nfaster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves\ntraining efficiency while achieving comparable performance. MixGRPO exhibits\nsubstantial gains across multiple dimensions of human preference alignment,\noutperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%\nlower training time. Notably, MixGRPO-Flash further reduces training time by\n71%. Codes and models are available at\n$\\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.",
      "pdf_url": "http://arxiv.org/pdf/2507.21802v1",
      "published": "2025-07-29T13:40:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21802v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer",
      "authors": [
        "Xie Zhang",
        "Yina Wang",
        "Chenshu Wu"
      ],
      "abstract": "The empirical success of deep learning has spurred its application to the\nradio-frequency (RF) domain, leading to significant advances in Deep Wireless\nSensing (DWS). However, most existing DWS models function as black boxes with\nlimited interpretability, which hampers their generalizability and raises\nconcerns in security-sensitive physical applications. In this work, inspired by\nthe remarkable advances of white-box transformers, we present RF-CRATE, the\nfirst mathematically interpretable deep network architecture for RF sensing,\ngrounded in the principles of complex sparse rate reduction. To accommodate the\nunique RF signals, we conduct non-trivial theoretical derivations that extend\nthe original real-valued white-box transformer to the complex domain. By\nleveraging the CR-Calculus framework, we successfully construct a fully\ncomplex-valued white-box transformer with theoretically derived self-attention\nand residual multi-layer perceptron modules. Furthermore, to improve the\nmodel's ability to extract discriminative features from limited wireless data,\nwe introduce Subspace Regularization, a novel regularization strategy that\nenhances feature diversity, resulting in an average performance improvement of\n19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against\nseven baselines with multiple public and self-collected datasets involving\ndifferent RF signals. The results show that RF-CRATE achieves performance on\npar with thoroughly engineered black-box models, while offering full\nmathematical interpretability. More importantly, by extending CRATE to the\ncomplex domain, RF-CRATE yields substantial improvements, achieving an average\nclassification gain of 5.08% and reducing regression error by 10.34% across\ndiverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at:\nhttps://github.com/rfcrate/RF_CRATE.",
      "pdf_url": "http://arxiv.org/pdf/2507.21799v1",
      "published": "2025-07-29T13:35:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21799v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects",
      "authors": [
        "Yuying Zhang",
        "Kevin Sebastian Luck",
        "Francesco Verdoja",
        "Ville Kyrki",
        "Joni Pajarinen"
      ],
      "abstract": "Mobile manipulation is a critical capability for robots operating in diverse,\nreal-world environments. However, manipulating deformable objects and materials\nremains a major challenge for existing robot learning algorithms. While various\nbenchmarks have been proposed to evaluate manipulation strategies with rigid\nobjects, there is still a notable lack of standardized benchmarks that address\nmobile manipulation tasks involving deformable objects.\n  To address this gap, we introduce MoDeSuite, the first Mobile Manipulation\nDeformable Object task suite, designed specifically for robot learning.\nMoDeSuite consists of eight distinct mobile manipulation tasks covering both\nelastic objects and deformable objects, each presenting a unique challenge\ninspired by real-world robot applications. Success in these tasks requires\neffective collaboration between the robot's base and manipulator, as well as\nthe ability to exploit the deformability of the objects. To evaluate and\ndemonstrate the use of the proposed benchmark, we train two state-of-the-art\nreinforcement learning algorithms and two imitation learning algorithms,\nhighlighting the difficulties encountered and showing their performance in\nsimulation. Furthermore, we demonstrate the practical relevance of the suite by\ndeploying the trained policies directly into the real world with the Spot\nrobot, showcasing the potential for sim-to-real transfer. We expect that\nMoDeSuite will open a novel research domain in mobile manipulation involving\ndeformable objects. Find more details, code, and videos at\nhttps://sites.google.com/view/modesuite/home.",
      "pdf_url": "http://arxiv.org/pdf/2507.21796v1",
      "published": "2025-07-29T13:33:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21796v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Hybrid Causal Identification and Causal Mechanism Clustering",
      "authors": [
        "Saixiong Liu",
        "Yuhua Qian",
        "Jue Li",
        "Honghong Cheng",
        "Feijiang Li"
      ],
      "abstract": "Bivariate causal direction identification is a fundamental and vital problem\nin the causal inference field. Among binary causal methods, most methods based\non additive noise only use one single causal mechanism to construct a causal\nmodel. In the real world, observations are always collected in different\nenvironments with heterogeneous causal relationships. Therefore, on observation\ndata, this paper proposes a Mixture Conditional Variational Causal Inference\nmodel (MCVCI) to infer heterogeneous causality. Specifically, according to the\nidentifiability of the Hybrid Additive Noise Model (HANM), MCVCI combines the\nsuperior fitting capabilities of the Gaussian mixture model and the neural\nnetwork and elegantly uses the likelihoods obtained from the probabilistic\nbounds of the mixture conditional variational auto-encoder as causal decision\ncriteria. Moreover, we model the casual heterogeneity into cluster numbers and\npropose the Mixture Conditional Variational Causal Clustering (MCVCC) method,\nwhich can reveal causal mechanism expression. Compared with state-of-the-art\nmethods, the comprehensive best performance demonstrates the effectiveness of\nthe methods proposed in this paper on several simulated and real data.",
      "pdf_url": "http://arxiv.org/pdf/2507.21792v1",
      "published": "2025-07-29T13:27:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21792v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Can large language models assist choice modelling? Insights into prompting strategies and current models capabilities",
      "authors": [
        "Georges Sfeir",
        "Gabriel Nova",
        "Stephane Hess",
        "Sander van Cranenburgh"
      ],
      "abstract": "Large Language Models (LLMs) are widely used to support various workflows\nacross different disciplines, yet their potential in choice modelling remains\nrelatively unexplored. This work examines the potential of LLMs as assistive\nagents in the specification and, where technically feasible, estimation of\nMultinomial Logit models. We implement a systematic experimental framework\ninvolving thirteen versions of six leading LLMs (ChatGPT, Claude, DeepSeek,\nGemini, Gemma, and Llama) evaluated under five experimental configurations.\nThese configurations vary along three dimensions: modelling goal (suggesting\nvs. suggesting and estimating MNLs); prompting strategy (Zero-Shot vs.\nChain-of-Thoughts); and information availability (full dataset vs. data\ndictionary only). Each LLM-suggested specification is implemented, estimated,\nand evaluated based on goodness-of-fit metrics, behavioural plausibility, and\nmodel complexity. Findings reveal that proprietary LLMs can generate valid and\nbehaviourally sound utility specifications, particularly when guided by\nstructured prompts. Open-weight models such as Llama and Gemma struggled to\nproduce meaningful specifications. Claude 4 Sonnet consistently produced the\nbest-fitting and most complex models, while GPT models suggested models with\nrobust and stable modelling outcomes. Some LLMs performed better when provided\nwith just data dictionary, suggesting that limiting raw data access may enhance\ninternal reasoning capabilities. Among all LLMs, GPT o3 was uniquely capable of\ncorrectly estimating its own specifications by executing self-generated code.\nOverall, the results demonstrate both the promise and current limitations of\nLLMs as assistive agents in choice modelling, not only for model specification\nbut also for supporting modelling decision and estimation, and provide\npractical guidance for integrating these tools into choice modellers'\nworkflows.",
      "pdf_url": "http://arxiv.org/pdf/2507.21790v1",
      "published": "2025-07-29T13:24:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21790v1",
      "categories": [
        "econ.EM",
        "cs.AI"
      ]
    },
    {
      "title": "Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP Results",
      "authors": [
        "Ali Fallahi",
        "Azam Bastanfard",
        "Amineh Amini",
        "Hadi Saboohi"
      ],
      "abstract": "The importance of recommender systems on the web has grown, especially in the\nmovie industry, with a vast selection of options to watch. To assist users in\ntraversing available items and finding relevant results, recommender systems\nanalyze operational data and investigate users' tastes and habits. Providing\nhighly individualized suggestions can boost user engagement and satisfaction,\nwhich is one of the fundamental goals of the movie industry, significantly in\nonline platforms. According to recent studies and research, using\nknowledge-based techniques and considering the semantic ideas of the textual\ndata is a suitable way to get more appropriate results. This study provides a\nnew method for building a knowledge graph based on semantic information. It\nuses the ChatGPT, as a large language model, to assess the brief descriptions\nof movies and extract their tone of voice. Results indicated that using the\nproposed method may significantly enhance accuracy rather than employing the\nexplicit genres supplied by the publishers.",
      "pdf_url": "http://arxiv.org/pdf/2507.21770v1",
      "published": "2025-07-29T12:55:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21770v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Learning Kinetic Monte Carlo stochastic dynamics with Deep Generative Adversarial Networks",
      "authors": [
        "Daniele Lanzoni",
        "Olivier Pierre-Louis",
        "Roberto Bergamaschini",
        "Francesco Montalenti"
      ],
      "abstract": "We show that Generative Adversarial Networks (GANs) may be fruitfully\nexploited to learn stochastic dynamics, surrogating traditional models while\ncapturing thermal fluctuations. Specifically, we showcase the application to a\ntwo-dimensional, many-particle system, focusing on surface-step fluctuations\nand on the related time-dependent roughness. After the construction of a\ndataset based on Kinetic Monte Carlo simulations, a conditional GAN is trained\nto propagate stochastically the state of the system in time, allowing the\ngeneration of new sequences with a reduced computational cost. Modifications\nwith respect to standard GANs, which facilitate convergence and increase\naccuracy, are discussed. The trained network is demonstrated to quantitatively\nreproduce equilibrium and kinetic properties, including scaling laws, with\ndeviations of a few percent from the exact value. Extrapolation limits and\nfuture perspectives are critically discussed.",
      "pdf_url": "http://arxiv.org/pdf/2507.21763v1",
      "published": "2025-07-29T12:48:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21763v1",
      "categories": [
        "cond-mat.stat-mech",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ]
    },
    {
      "title": "LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection",
      "authors": [
        "Jing Ren",
        "Suyu Ma",
        "Hong Jia",
        "Xiwei Xu",
        "Ivan Lee",
        "Haytham Fayek",
        "Xiaodong Li",
        "Feng Xia"
      ],
      "abstract": "Detecting driver fatigue is critical for road safety, as drowsy driving\nremains a leading cause of traffic accidents. Many existing solutions rely on\ncomputationally demanding deep learning models, which result in high latency\nand are unsuitable for embedded robotic devices with limited resources (such as\nintelligent vehicles/cars) where rapid detection is necessary to prevent\naccidents. This paper introduces LiteFat, a lightweight spatio-temporal graph\nlearning model designed to detect driver fatigue efficiently while maintaining\nhigh accuracy and low computational demands. LiteFat involves converting\nstreaming video data into spatio-temporal graphs (STG) using facial landmark\ndetection, which focuses on key motion patterns and reduces unnecessary data\nprocessing. LiteFat uses MobileNet to extract facial features and create a\nfeature matrix for the STG. A lightweight spatio-temporal graph neural network\nis then employed to identify signs of fatigue with minimal processing and low\nlatency. Experimental results on benchmark datasets show that LiteFat performs\ncompetitively while significantly decreasing computational complexity and\nlatency as compared to current state-of-the-art methods. This work enables the\ndevelopment of real-time, resource-efficient human fatigue detection systems\nthat can be implemented upon embedded robotic devices.",
      "pdf_url": "http://arxiv.org/pdf/2507.21756v1",
      "published": "2025-07-29T12:37:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21756v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}