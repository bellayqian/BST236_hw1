{
  "last_updated": "2025-10-17T00:48:46.556750",
  "papers": [
    {
      "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
      "authors": [
        "Xinchen Zhang",
        "Xiaoying Zhang",
        "Youbin Wu",
        "Yanbin Cao",
        "Renrui Zhang",
        "Ruihang Chu",
        "Ling Yang",
        "Yujiu Yang"
      ],
      "abstract": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.",
      "pdf_url": "http://arxiv.org/pdf/2510.13804v1",
      "published": "2025-10-15T17:59:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13804v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs",
      "authors": [
        "Yi Zhang",
        "Bolin Ni",
        "Xin-Sheng Chen",
        "Heng-Rui Zhang",
        "Yongming Rao",
        "Houwen Peng",
        "Qinglin Lu",
        "Han Hu",
        "Meng-Hao Guo",
        "Shi-Min Hu"
      ],
      "abstract": "Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.",
      "pdf_url": "http://arxiv.org/pdf/2510.13795v1",
      "published": "2025-10-15T17:52:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13795v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach",
      "authors": [
        "Ziqing Lu",
        "Lifeng Lai",
        "Weiyu Xu"
      ],
      "abstract": "Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged\nin many security-related applications, such as autonomous driving, financial\ndecisions, and drone/robot algorithms. In order to improve the\nrobustness/defense of RL systems against adversaries, studying various\nadversarial attacks on RL systems is very important. Most previous work\nconsidered deterministic adversarial attack strategies in MDP, which the\nrecipient (victim) agent can defeat by reversing the deterministic attacks. In\nthis paper, we propose a provably ``invincible'' or ``uncounterable'' type of\nadversarial attack on RL. The attackers apply a rate-distortion\ninformation-theoretic approach to randomly change agents' observations of the\ntransition kernel (or other properties) so that the agent gains zero or very\nlimited information about the ground-truth kernel (or other properties) during\nthe training. We derive an information-theoretic lower bound on the recipient\nagent's reward regret and show the impact of rate-distortion attacks on\nstate-of-the-art model-based and model-free algorithms. We also extend this\nnotion of an information-theoretic approach to other types of adversarial\nattack, such as state observation attacks.",
      "pdf_url": "http://arxiv.org/pdf/2510.13792v1",
      "published": "2025-10-15T17:48:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13792v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The Art of Scaling Reinforcement Learning Compute for LLMs",
      "authors": [
        "Devvrit Khatri",
        "Lovish Madaan",
        "Rishabh Tiwari",
        "Rachit Bansal",
        "Sai Surya Duvvuri",
        "Manzil Zaheer",
        "Inderjit S. Dhillon",
        "David Brandfonbrener",
        "Rishabh Agarwal"
      ],
      "abstract": "Reinforcement learning (RL) has become central to training large language\nmodels (LLMs), yet the field lacks predictive scaling methodologies comparable\nto those established for pre-training. Despite rapidly rising compute budgets,\nthere is no principled understanding of how to evaluate algorithmic\nimprovements for scaling RL compute. We present the first large-scale\nsystematic study, amounting to more than 400,000 GPU-hours, that defines a\nprincipled framework for analyzing and predicting RL scaling in LLMs. We fit\nsigmoidal compute-performance curves for RL training and ablate a wide range of\ncommon design choices to analyze their effects on asymptotic performance and\ncompute efficiency. We observe: (1) Not all recipes yield similar asymptotic\nperformance, (2) Details such as loss aggregation, normalization, curriculum,\nand off-policy algorithm primarily modulate compute efficiency without\nmaterially shifting the asymptote, and (3) Stable, scalable recipes follow\npredictable scaling trajectories, enabling extrapolation from smaller-scale\nruns. Combining these insights, we propose a best-practice recipe, ScaleRL, and\ndemonstrate its effectiveness by successfully scaling and predicting validation\nperformance on a single RL run scaled up to 100,000 GPU-hours. Our work\nprovides both a scientific framework for analyzing scaling in RL and a\npractical recipe that brings RL training closer to the predictability long\nachieved in pre-training.",
      "pdf_url": "http://arxiv.org/pdf/2510.13786v1",
      "published": "2025-10-15T17:43:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13786v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy",
      "authors": [
        "Xinyi Chen",
        "Yilun Chen",
        "Yanwei Fu",
        "Ning Gao",
        "Jiaya Jia",
        "Weiyang Jin",
        "Hao Li",
        "Yao Mu",
        "Jiangmiao Pang",
        "Yu Qiao",
        "Yang Tian",
        "Bin Wang",
        "Bolun Wang",
        "Fangjing Wang",
        "Hanqing Wang",
        "Tai Wang",
        "Ziqin Wang",
        "Xueyuan Wei",
        "Chao Wu",
        "Shuai Yang",
        "Jinhui Ye",
        "Junqiu Yu",
        "Jia Zeng",
        "Jingjing Zhang",
        "Jinyu Zhang",
        "Shi Zhang",
        "Feng Zheng",
        "Bowen Zhou",
        "Yangkun Zhu"
      ],
      "abstract": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
      "pdf_url": "http://arxiv.org/pdf/2510.13778v1",
      "published": "2025-10-15T17:30:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13778v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Scaling Vision Transformers for Functional MRI with Flat Maps",
      "authors": [
        "Connor Lane",
        "Daniel Z. Kaplan",
        "Tanishq Mathew Abraham",
        "Paul S. Scotti"
      ],
      "abstract": "A key question for adapting modern deep learning architectures to functional\nMRI (fMRI) is how to represent the data for model input. To bridge the modality\ngap between fMRI and natural images, we transform the 4D volumetric fMRI data\ninto videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K\nhours of fMRI flat map videos from the Human Connectome Project using the\nspatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI\nmodeling performance improves with dataset size according to a strict power\nscaling law. Downstream classification benchmarks show that our model learns\nrich representations supporting both fine-grained state decoding across\nsubjects, as well as subject-specific trait decoding across changes in brain\nstate. This work is part of an ongoing open science project to build foundation\nmodels for fMRI data. Our code and datasets are available at\nhttps://github.com/MedARC-AI/fmri-fm.",
      "pdf_url": "http://arxiv.org/pdf/2510.13768v1",
      "published": "2025-10-15T17:15:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13768v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.NC"
      ]
    },
    {
      "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
      "authors": [
        "Junhong Shen",
        "Mu Cai",
        "Bo Hu",
        "Ameet Talwalkar",
        "David A Ross",
        "Cordelia Schmid",
        "Alireza Fathi"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for\nstructured visuals like charts and diagrams, as pixel-based perception lacks a\nmechanism for verification. To address this, we propose to leverage derendering\n-- the process of reverse-engineering visuals into executable code -- as a new\nmodality for verifiable visual reasoning. Specifically, we propose RECODE, an\nagentic framework that first generates multiple candidate programs to reproduce\nthe input image. It then uses a critic to select the most faithful\nreconstruction and iteratively refines the code. This process not only\ntransforms an ambiguous perceptual task into a verifiable, symbolic problem,\nbut also enables precise calculations and logical inferences later on. On\nvarious visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,\nRECODE significantly outperforms methods that do not leverage code or only use\ncode for drawing auxiliary lines or cropping. Our work demonstrates that\ngrounding visual perception in executable code provides a new path toward more\naccurate and verifiable multimodal reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2510.13756v1",
      "published": "2025-10-15T17:05:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13756v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math",
      "authors": [
        "Shrey Pandit",
        "Austin Xu",
        "Xuan-Phi Nguyen",
        "Yifei Ming",
        "Caiming Xiong",
        "Shafiq Joty"
      ],
      "abstract": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
      "pdf_url": "http://arxiv.org/pdf/2510.13744v1",
      "published": "2025-10-15T16:50:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13744v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs",
      "authors": [
        "Mustafa Munir",
        "Alex Zhang",
        "Radu Marculescu"
      ],
      "abstract": "Vision graph neural networks (ViG) have demonstrated promise in vision tasks\nas a competitive alternative to conventional convolutional neural nets (CNN)\nand transformers (ViTs); however, common graph construction methods, such as\nk-nearest neighbor (KNN), can be expensive on larger images. While methods such\nas Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step\nscale can lead to over-squashing and missing multiple connections to gain the\nsame information that could be gained from a long-range link. Through this\nobservation, we propose a new graph construction method, Logarithmic Scalable\nGraph Construction (LSGC) to enhance performance by limiting the number of\nlong-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model\nthat utilizes LSGC. Furthermore, inspired by the successes of multi-scale and\nhigh-resolution architectures, we introduce and apply a high-resolution branch\nand fuse features between our high-resolution and low-resolution branches for a\nmulti-scale high-resolution Vision GNN network. Extensive experiments show that\nLogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,\nGMACs, and parameters on image classification and semantic segmentation tasks.\nOur smallest model, Ti-LogViG, achieves an average top-1 accuracy on\nImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average\naccuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%\nreduction in GMACs. Our work shows that leveraging long-range links in graph\nconstruction for ViGs through our proposed LSGC can exceed the performance of\ncurrent state-of-the-art ViGs. Code is available at\nhttps://github.com/mmunir127/LogViG-Official.",
      "pdf_url": "http://arxiv.org/pdf/2510.13740v1",
      "published": "2025-10-15T16:47:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13740v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails",
      "authors": [
        "Ravi Pandya",
        "Madison Bland",
        "Duy P. Nguyen",
        "Changliu Liu",
        "Jaime Fernández Fisac",
        "Andrea Bajcsy"
      ],
      "abstract": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails.",
      "pdf_url": "http://arxiv.org/pdf/2510.13727v1",
      "published": "2025-10-15T16:30:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13727v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access",
      "authors": [
        "Aditya Tanikanti",
        "Benoit Côté",
        "Yanfei Guo",
        "Le Chen",
        "Nickolaus Saint",
        "Ryan Chard",
        "Ken Raffenetti",
        "Rajeev Thakur",
        "Thomas Uram",
        "Ian Foster",
        "Michael E. Papka",
        "Venkatram Vishwanath"
      ],
      "abstract": "We present the Federated Inference Resource Scheduling Toolkit (FIRST), a\nframework enabling Inference-as-a-Service across distributed High-Performance\nComputing (HPC) clusters. FIRST provides cloud-like access to diverse AI\nmodels, like Large Language Models (LLMs), on existing HPC infrastructure.\nLeveraging Globus Auth and Globus Compute, the system allows researchers to run\nparallel inference workloads via an OpenAI-compliant API on private, secure\nenvironments. This cluster-agnostic API allows requests to be distributed\nacross federated clusters, targeting numerous hosted models. FIRST supports\nmultiple inference backends (e.g., vLLM), auto-scales resources, maintains\n\"hot\" nodes for low-latency execution, and offers both high-throughput batch\nand interactive modes. The framework addresses the growing demand for private,\nsecure, and scalable AI inference in scientific workflows, allowing researchers\nto generate billions of tokens daily on-premises without relying on commercial\ncloud infrastructure.",
      "pdf_url": "http://arxiv.org/pdf/2510.13724v1",
      "published": "2025-10-15T16:28:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13724v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching",
      "authors": [
        "Run Luo",
        "Xiaobo Xia",
        "Lu Wang",
        "Longze Chen",
        "Renke Shan",
        "Jing Luo",
        "Min Yang",
        "Tat-Seng Chua"
      ],
      "abstract": "Next-generation multimodal foundation models capable of any-to-any\ncross-modal generation and multi-turn interaction will serve as core components\nof artificial general intelligence systems, playing a pivotal role in\nhuman-machine interaction. However, most existing multimodal models remain\nconstrained by autoregressive architectures, whose inherent limitations prevent\na balanced integration of understanding and generation capabilities. Although\nhybrid and decoupling strategies have been explored to address these tasks\nwithin unified frameworks separately, their redundant, non-integrated designs\nlimit their applicability to broader scenarios, such as cross-modal retrieval.\nIn this work, we introduce NExT-OMNI, an open-source omnimodal foundation model\nthat achieves unified modeling through discrete flow paradigms. By leveraging\nmetric-induced probability paths and kinetic optimal velocities, NExT-OMNI\nnatively supports any-to-any understanding and generation with enhanced\nresponse efficiency, while enabling broader application scenarios through\nconcise unified representations rather than task-decoupled designs. Trained on\nlarge-scale interleaved text, image, video, and audio data, NExT-OMNI delivers\ncompetitive performance on multimodal generation and understanding benchmarks,\nwhile outperforming prior unified models in multi-turn multimodal interaction\nand cross-modal retrieval, highlighting its architectural advantages as a\nnext-generation multimodal foundation model. To advance further research, we\nrelease training details, data protocols, and open-source both the code and\nmodel checkpoints.",
      "pdf_url": "http://arxiv.org/pdf/2510.13721v2",
      "published": "2025-10-15T16:25:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13721v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ]
    },
    {
      "title": "Dedelayed: Deleting remote inference delay via on-device correction",
      "authors": [
        "Dan Jacobellis",
        "Mateen Ulhaq",
        "Fabien Racapé",
        "Hyomin Choi",
        "Neeraja J. Yadwadkar"
      ],
      "abstract": "Remote inference allows lightweight devices to leverage powerful cloud\nmodels. However, communication network latency makes predictions stale and\nunsuitable for real-time tasks. To address this, we introduce Dedelayed, a\ndelay-corrective method that mitigates arbitrary remote inference delays,\nallowing the local device to produce low-latency outputs in real time. Our\nmethod employs a lightweight local model that processes the current frame and\nfuses in features that a heavyweight remote model computes from past frames. On\nvideo from the BDD100K driving dataset, Dedelayed improves semantic\nsegmentation accuracy over the stronger of the local-only and remote-only\nbaselines across all realistic communication network delays beyond 33 ms.\nWithout incurring additional delay, it improves accuracy by 6.4 mIoU compared\nto fully local inference and 9.8 mIoU compared to remote inference, for a\nround-trip delay of 100 ms. The advantage grows under longer delays and\nhigher-motion scenes, as delay-mitigated split inference sustains accuracy more\neffectively, providing clear advantages for real-time tasks that must remain\naligned with the current world state.",
      "pdf_url": "http://arxiv.org/pdf/2510.13714v1",
      "published": "2025-10-15T16:13:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13714v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Training LLM Agents to Empower Humans",
      "authors": [
        "Evan Ellis",
        "Vivek Myers",
        "Jens Tuyls",
        "Sergey Levine",
        "Anca Dragan",
        "Benjamin Eysenbach"
      ],
      "abstract": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.",
      "pdf_url": "http://arxiv.org/pdf/2510.13709v2",
      "published": "2025-10-15T16:09:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13709v2",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents",
      "authors": [
        "Johan Obando-Ceron",
        "Walter Mayor",
        "Samuel Lavoie",
        "Scott Fujimoto",
        "Aaron Courville",
        "Pablo Samuel Castro"
      ],
      "abstract": "Recent works have proposed accelerating the wall-clock training time of\nactor-critic methods via the use of large-scale environment parallelization;\nunfortunately, these can sometimes still require large number of environment\ninteractions to achieve a desired level of performance. Noting that\nwell-structured representations can improve the generalization and sample\nefficiency of deep reinforcement learning (RL) agents, we propose the use of\nsimplicial embeddings: lightweight representation layers that constrain\nembeddings to simplicial structures. This geometric inductive bias results in\nsparse and discrete features that stabilize critic bootstrapping and strengthen\npolicy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial\nembeddings consistently improve sample efficiency and final performance across\na variety of continuous- and discrete-control environments, without any loss in\nruntime speed.",
      "pdf_url": "http://arxiv.org/pdf/2510.13704v1",
      "published": "2025-10-15T16:01:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13704v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion",
      "authors": [
        "Minjung Shin",
        "Hyunin Cho",
        "Sooyeon Go",
        "Jin-Hwa Kim",
        "Youngjung Uh"
      ],
      "abstract": "Multi-view generation with camera pose control and prompt-based customization\nare both essential elements for achieving controllable generative models.\nHowever, existing multi-view generation models do not support customization\nwith geometric consistency, whereas customization models lack explicit\nviewpoint control, making them challenging to unify. Motivated by these gaps,\nwe introduce a novel task, multi-view customization, which aims to jointly\nachieve multi-view camera pose control and customization. Due to the scarcity\nof training data in customization, existing multi-view generation models, which\ninherently rely on large-scale datasets, struggle to generalize to diverse\nprompts. To address this, we propose MVCustom, a novel diffusion-based\nframework explicitly designed to achieve both multi-view consistency and\ncustomization fidelity. In the training stage, MVCustom learns the subject's\nidentity and geometry using a feature-field representation, incorporating the\ntext-to-video diffusion backbone enhanced with dense spatio-temporal attention,\nwhich leverages temporal coherence for multi-view consistency. In the inference\nstage, we introduce two novel techniques: depth-aware feature rendering\nexplicitly enforces geometric consistency, and consistent-aware latent\ncompletion ensures accurate perspective alignment of the customized subject and\nsurrounding backgrounds. Extensive experiments demonstrate that MVCustom is the\nonly framework that simultaneously achieves faithful multi-view generation and\ncustomization.",
      "pdf_url": "http://arxiv.org/pdf/2510.13702v1",
      "published": "2025-10-15T16:00:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13702v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Modal Logic for Temporal and Jurisdictional Classifier Models",
      "authors": [
        "Cecilia Di Florio",
        "Huimin Dong",
        "Antonino Rotolo"
      ],
      "abstract": "Logic-based models can be used to build verification tools for machine\nlearning classifiers employed in the legal field. ML classifiers predict the\noutcomes of new cases based on previous ones, thereby performing a form of\ncase-based reasoning (CBR). In this paper, we introduce a modal logic of\nclassifiers designed to formally capture legal CBR. We incorporate principles\nfor resolving conflicts between precedents, by introducing into the logic the\ntemporal dimension of cases and the hierarchy of courts within the legal\nsystem.",
      "pdf_url": "http://arxiv.org/pdf/2510.13691v1",
      "published": "2025-10-15T15:50:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13691v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas",
      "authors": [
        "Zian Li",
        "Muhan Zhang"
      ],
      "abstract": "Masked autoregressive models (MAR) have recently emerged as a powerful\nparadigm for image and video generation, combining the flexibility of masked\nmodeling with the potential of continuous tokenizer. However, video MAR models\nsuffer from two major limitations: the slow-start problem, caused by the lack\nof a structured global prior at early sampling stages, and error accumulation\nacross the autoregression in both spatial and temporal dimensions. In this\nwork, we propose CanvasMAR, a novel video MAR model that mitigates these issues\nby introducing a canvas mechanism--a blurred, global prediction of the next\nframe, used as the starting point for masked generation. The canvas provides\nglobal structure early in sampling, enabling faster and more coherent frame\nsynthesis. Furthermore, we introduce compositional classifier-free guidance\nthat jointly enlarges spatial (canvas) and temporal conditioning, and employ\nnoise-based canvas augmentation to enhance robustness. Experiments on the BAIR\nand Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality\nvideos with fewer autoregressive steps. Our approach achieves remarkable\nperformance among autoregressive models on Kinetics-600 dataset and rivals\ndiffusion-based methods.",
      "pdf_url": "http://arxiv.org/pdf/2510.13669v1",
      "published": "2025-10-15T15:29:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13669v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Axial Neural Networks for Dimension-Free Foundation Models",
      "authors": [
        "Hyunsu Kim",
        "Jonggeon Park",
        "Joan Bruna",
        "Hongseok Yang",
        "Juho Lee"
      ],
      "abstract": "The advent of foundation models in AI has significantly advanced\ngeneral-purpose learning, enabling remarkable capabilities in zero-shot\ninference and in-context learning. However, training such models on physics\ndata, including solutions to partial differential equations (PDEs), poses a\nunique challenge due to varying dimensionalities across different systems.\nTraditional approaches either fix a maximum dimension or employ separate\nencoders for different dimensionalities, resulting in inefficiencies. To\naddress this, we propose a dimension-agnostic neural network architecture, the\nAxial Neural Network (XNN), inspired by parameter-sharing structures such as\nDeep Sets and Graph Neural Networks. XNN generalizes across varying tensor\ndimensions while maintaining computational efficiency. We convert existing PDE\nfoundation models into axial neural networks and evaluate their performance\nacross three training scenarios: training from scratch, pretraining on multiple\nPDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform\ncompetitively with original models and exhibit superior generalization to\nunseen dimensions, highlighting the importance of multidimensional pretraining\nfor foundation models.",
      "pdf_url": "http://arxiv.org/pdf/2510.13665v1",
      "published": "2025-10-15T15:25:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13665v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Time Series Foundation Models: Benchmarking Challenges and Requirements",
      "authors": [
        "Marcel Meyer",
        "Sascha Kaltenpoth",
        "Kevin Zalipski",
        "Oliver Müller"
      ],
      "abstract": "Time Series Foundation Models (TSFMs) represent a new paradigm for time\nseries forecasting, offering zero-shot forecasting capabilities without the\nneed for domain-specific pre-training or fine-tuning. However, as with Large\nLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive\ntraining sets, it becomes more and more challenging to ensure the integrity of\nbenchmarking data. Our investigation of existing TSFM evaluation highlights\nmultiple challenges, ranging from the representativeness of the benchmark\ndatasets, over the lack of spatiotemporal evaluation, to risks of information\nleakage due to overlapping and obscure datasets, and the memorization of global\npatterns caused by external shocks like economic crises or pandemics. Our\nfindings reveal widespread confusion regarding data partitions, risking\ninflated performance estimates and incorrect transfer of global knowledge to\nlocal time series. We argue for the development of robust evaluation\nmethodologies to prevent pitfalls already observed in LLM and classical time\nseries benchmarking, and call upon the research community to design new,\nprincipled approaches, such as evaluations on truly out-of-sample future data,\nto safeguard the integrity of TSFM assessment.",
      "pdf_url": "http://arxiv.org/pdf/2510.13654v1",
      "published": "2025-10-15T15:15:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13654v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Closing the Gap Between Text and Speech Understanding in LLMs",
      "authors": [
        "Santiago Cuervo",
        "Skyler Seto",
        "Maureen de Seyssel",
        "Richard He Bai",
        "Zijin Gu",
        "Tatiana Likhomanenko",
        "Navdeep Jaitly",
        "Zakaria Aldeneh"
      ],
      "abstract": "Large Language Models (LLMs) can be adapted to extend their text capabilities\nto speech inputs. However, these speech-adapted LLMs consistently underperform\ntheir text-based counterparts--and even cascaded pipelines--on language\nunderstanding tasks. We term this shortfall the text-speech understanding gap:\nthe performance drop observed when a speech-adapted LLM processes spoken inputs\nrelative to when the original text-based LLM processes the equivalent text.\nRecent approaches to narrowing this gap either rely on large-scale speech\nsynthesis of text corpora, which is costly and heavily dependent on synthetic\ndata, or on large-scale proprietary speech datasets, which are not\nreproducible. As a result, there remains a need for more data-efficient\nalternatives for closing the text-speech understanding gap. In this work, we\nanalyze the gap as driven by two factors: (i) forgetting of text capabilities\nduring adaptation, and (ii) cross-modal misalignment between speech and text.\nBased on this analysis, we introduce SALAD--Sample-efficient Alignment with\nLearning through Active selection and cross-modal Distillation--which combines\ncross-modal distillation with targeted synthetic data to improve alignment\nwhile mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves\ncompetitive performance with a strong open-weight model across broad-domain\nbenchmarks in knowledge, language understanding, and reasoning, while training\non over an order of magnitude less speech data from public corpora.",
      "pdf_url": "http://arxiv.org/pdf/2510.13632v1",
      "published": "2025-10-15T14:57:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13632v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses",
      "authors": [
        "Stefan Lenz",
        "Lakisha Ortiz Rosario",
        "Georg Vollmar",
        "Arsenij Ustjanzew",
        "Fatma Alickovic",
        "Thomas Kindler",
        "Torsten Panholzer"
      ],
      "abstract": "Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential\nfor structured cancer documentation in Germany. Smaller open-weight LLMs are\nappealing for privacy-preserving automation but often struggle with coding\naccuracy in German-language contexts. This study investigates whether\ninstruction-based fine-tuning on public datasets improves the coding accuracy\nof open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded\ndiagnoses from the local tumor documentation system as test data. In a\nsystematic data quality assessment, the upper limit for ICD-10 coding\nperformance was estimated at 60-79% for exact and 81-94% for partial\n(three-character codes only) derivation. As training data, over 500,000\nquestion-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS\ncatalogues. Eight open-weight models from the Qwen, Llama, and Mistral families\n(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to\n41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3\ntopography coding also improved but started and remained considerably lower\nwith an exact accuracy of 22-40% and a partial accuracy of 56-67% after\nfine-tuning. Malformed code outputs dropped to 0% for all models.\nTumor-diagnosis recognition reached 99%. Accuracy correlated positively with\nmodel size, but gaps between small and large models narrowed after fine-tuning.\nThe reasoning mode in Qwen3 generally yielded a lower performance than\nfine-tuning and was over 100 times slower. Our findings highlight the potential\nof leveraging public catalogues to build instruction datasets that improve LLMs\nin medical documentation tasks. The complete training dataset and the\nbest-performing checkpoints of the fine-tuned models are available from\nhttps://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.",
      "pdf_url": "http://arxiv.org/pdf/2510.13624v1",
      "published": "2025-10-15T14:51:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13624v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The Role of Computing Resources in Publishing Foundation Model Research",
      "authors": [
        "Yuexing Hao",
        "Yue Huang",
        "Haoran Zhang",
        "Chenyang Zhao",
        "Zhenwen Liang",
        "Paul Pu Liang",
        "Yue Zhao",
        "Lichao Sun",
        "Saleh Kalantari",
        "Xiangliang Zhang",
        "Marzyeh Ghassemi"
      ],
      "abstract": "Cutting-edge research in Artificial Intelligence (AI) requires considerable\nresources, including Graphics Processing Units (GPUs), data, and human\nresources. In this paper, we evaluate of the relationship between these\nresources and the scientific advancement of foundation models (FM). We reviewed\n6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors\nto the impact of computing resources on scientific output. We find that\nincreased computing is correlated with national funding allocations and\ncitations, but our findings don't observe the strong correlations with research\nenvironment (academic or industrial), domain, or study methodology. We advise\nthat individuals and institutions focus on creating shared and affordable\ncomputing opportunities to lower the entry barrier for under-resourced\nresearchers. These steps can help expand participation in FM research, foster\ndiversity of ideas and contributors, and sustain innovation and progress in AI.\nThe data will be available at: https://mit-calc.csail.mit.edu/",
      "pdf_url": "http://arxiv.org/pdf/2510.13621v1",
      "published": "2025-10-15T14:50:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13621v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Message Passing on the Edge: Towards Scalable and Expressive GNNs",
      "authors": [
        "Pablo Barceló",
        "Fabian Jogl",
        "Alexander Kozachinskiy",
        "Matthias Lanzinger",
        "Stefan Neumann",
        "Cristóbal Rojas"
      ],
      "abstract": "We propose EB-1WL, an edge-based color-refinement test, and a corresponding\nGNN architecture, EB-GNN. Our architecture is inspired by a classic triangle\ncounting algorithm by Chiba and Nishizeki, and explicitly uses triangles during\nmessage passing. We achieve the following results: (1)~EB-1WL is significantly\nmore expressive than 1-WL. Further, we provide a complete logical\ncharacterization of EB-1WL based on first-order logic, and matching\ndistinguishability results based on homomorphism counting. (2)~In an important\ndistinction from previous proposals for more expressive GNN architectures,\nEB-1WL and EB-GNN require near-linear time and memory on practical graph\nlearning tasks. (3)~Empirically, we show that EB-GNN is a highly-efficient\ngeneral-purpose architecture: It substantially outperforms simple MPNNs, and\nremains competitive with task-specialized GNNs while being significantly more\ncomputationally efficient.",
      "pdf_url": "http://arxiv.org/pdf/2510.13615v1",
      "published": "2025-10-15T14:45:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13615v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "NOSA: Native and Offloadable Sparse Attention",
      "authors": [
        "Yuxiang Huang",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu"
      ],
      "abstract": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
      "pdf_url": "http://arxiv.org/pdf/2510.13602v1",
      "published": "2025-10-15T14:33:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13602v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Subject Roles in the EU AI Act: Mapping and Regulatory Implications",
      "authors": [
        "Nicola Fabiano"
      ],
      "abstract": "The European Union's Artificial Intelligence Act (Regulation (EU) 2024/1689)\nestablishes the world's first comprehensive regulatory framework for AI systems\nthrough a sophisticated ecosystem of interconnected subjects defined in Article\n3. This paper provides a structured examination of the six main categories of\nactors - providers, deployers, authorized representatives, importers,\ndistributors, and product manufacturers - collectively referred to as\n\"operators\" within the regulation. Through examination of these Article 3\ndefinitions and their elaboration across the regulation's 113 articles, 180\nrecitals, and 13 annexes, we map the complete governance structure and analyze\nhow the AI Act regulates these subjects. Our analysis reveals critical\ntransformation mechanisms whereby subjects can assume different roles under\nspecific conditions, particularly through Article 25 provisions ensuring\naccountability follows control. We identify how obligations cascade through the\nsupply chain via mandatory information flows and cooperation requirements,\ncreating a distributed yet coordinated governance system. The findings\ndemonstrate how the regulation balances innovation with the protection of\nfundamental rights through risk-based obligations that scale with the\ncapabilities and deployment contexts of AI systems, providing essential\nguidance for stakeholders implementing the AI Act's requirements.",
      "pdf_url": "http://arxiv.org/pdf/2510.13591v1",
      "published": "2025-10-15T14:21:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13591v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs",
      "authors": [
        "Pasin Buakhaw",
        "Kun Kerdthaisong",
        "Phuree Phenhiran",
        "Pitikorn Khlaisamniang",
        "Supasate Vorathammathorn",
        "Piyalitt Ittichaiwong",
        "Nutchanon Yongsatianchot"
      ],
      "abstract": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
      "pdf_url": "http://arxiv.org/pdf/2510.13586v1",
      "published": "2025-10-15T14:17:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13586v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies",
      "authors": [
        "Peng Di",
        "Faqiang Chen",
        "Xiao Bai",
        "Hongjun Yang",
        "Qingfeng Li",
        "Ganglin Wei",
        "Jian Mou",
        "Feng Shi",
        "Keting Chen",
        "Peng Tang",
        "Zhitao Shen",
        "Zheng Li",
        "Wenhui Shi",
        "Junwei Guo",
        "Hang Yu"
      ],
      "abstract": "The escalating complexity of modern software imposes an unsustainable\noperational burden on Site Reliability Engineering (SRE) teams, demanding\nAI-driven automation that can emulate expert diagnostic reasoning. Existing\nsolutions, from traditional AI methods to general-purpose multi-agent systems,\nfall short: they either lack deep causal reasoning or are not tailored for the\nspecialized, investigative workflows unique to SRE. To address this gap, we\npresent OpenDerisk, a specialized, open-source multi-agent framework\narchitected for SRE. OpenDerisk integrates a diagnostic-native collaboration\nmodel, a pluggable reasoning engine, a knowledge engine, and a standardized\nprotocol (MCP) to enable specialist agents to collectively solve complex,\nmulti-domain problems. Our comprehensive evaluation demonstrates that\nOpenDerisk significantly outperforms state-of-the-art baselines in both\naccuracy and efficiency. This effectiveness is validated by its large-scale\nproduction deployment at Ant Group, where it serves over 3,000 daily users\nacross diverse scenarios, confirming its industrial-grade scalability and\npractical impact. OpenDerisk is open source and available at\nhttps://github.com/derisk-ai/OpenDerisk/",
      "pdf_url": "http://arxiv.org/pdf/2510.13561v2",
      "published": "2025-10-15T13:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13561v2",
      "categories": [
        "cs.SE",
        "cs.AI",
        "68N30"
      ]
    },
    {
      "title": "Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents",
      "authors": [
        "David Freire-Obregón",
        "José Salas-Cáceres",
        "Javier Lorenzo-Navarro",
        "Oliverio J. Santana",
        "Daniel Hernández-Sosa",
        "Modesto Castrillón-Santana"
      ],
      "abstract": "Facial expression recognition (FER) must remain robust under both cultural\nvariation and perceptually degraded visual conditions, yet most existing\nevaluations assume homogeneous data and high-quality imagery. We introduce an\nagent-based, streaming benchmark that reveals how cross-cultural composition\nand progressive blurring interact to shape face recognition robustness. Each\nagent operates in a frozen CLIP feature space with a lightweight residual\nadapter trained online at sigma=0 and fixed during testing. Agents move and\ninteract on a 5x5 lattice, while the environment provides inputs with\nsigma-scheduled Gaussian blur. We examine monocultural populations\n(Western-only, Asian-only) and mixed environments with balanced (5/5) and\nimbalanced (8/2, 2/8) compositions, as well as different spatial contact\nstructures. Results show clear asymmetric degradation curves between cultural\ngroups: JAFFE (Asian) populations maintain higher performance at low blur but\nexhibit sharper drops at intermediate stages, whereas KDEF (Western)\npopulations degrade more uniformly. Mixed populations exhibit intermediate\npatterns, with balanced mixtures mitigating early degradation, but imbalanced\nsettings amplify majority-group weaknesses under high blur. These findings\nquantify how cultural composition and interaction structure influence the\nrobustness of FER as perceptual conditions deteriorate.",
      "pdf_url": "http://arxiv.org/pdf/2510.13557v1",
      "published": "2025-10-15T13:53:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13557v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Tandem Training for Language Models",
      "authors": [
        "Robert West",
        "Ashton Anderson",
        "Ece Kamar",
        "Eric Horvitz"
      ],
      "abstract": "As language models continue to rapidly improve, we can expect their actions\nand reasoning to become difficult or impossible for weaker agents and humans to\nfollow, undermining interpretability and oversight. With an eye on long-term\nfutures, we pursue methods that encourage models to produce solutions that\nremain intelligible to weaker collaborators. We formalize intelligibility as\nhandoff robustness: a strong model's solution is intelligible to a weaker model\nif randomly handing off control to the weaker model along the solution path\ndoes not cause failure. Building on this criterion, we introduce tandem\ntraining for language models, a reinforcement learning (RL) paradigm in which\nrollout tokens are intermittently and randomly sampled from a frozen weak model\nrather than the strong model being trained. Because rollouts succeed only when\nthe strong model's actions and reasoning process can be continued by the weak\nmodel -- when the two can co-construct a successful solution -- optimizing\nstandard RL objectives with tandem training implicitly incentivizes both\ncorrectness and intelligibility. In the GSM8K math reasoning task, tandem\ntraining reliably teaches models to abandon jargon and adapt their language to\nweaker partners while keeping task accuracy high. Our results demonstrate a\npromising route to building AI systems that remain auditable by weaker agents,\nwith implications for human--AI collaboration and multi-agent communication.",
      "pdf_url": "http://arxiv.org/pdf/2510.13551v1",
      "published": "2025-10-15T13:48:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13551v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers",
      "authors": [
        "Avihay Cohen"
      ],
      "abstract": "Large Language Model (LLM) based agents integrated into web browsers (often\ncalled agentic AI browsers) offer powerful automation of web tasks. However,\nthey are vulnerable to indirect prompt injection attacks, where malicious\ninstructions hidden in a webpage deceive the agent into unwanted actions. These\nattacks can bypass traditional web security boundaries, as the AI agent\noperates with the user privileges across sites. In this paper, we present a\nnovel fuzzing framework that runs entirely in the browser and is guided by an\nLLM to automatically discover such prompt injection vulnerabilities in real\ntime.",
      "pdf_url": "http://arxiv.org/pdf/2510.13543v1",
      "published": "2025-10-15T13:39:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13543v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "K-Merge: Online Continual Merging of Adapters for On-device Large Language Models",
      "authors": [
        "Donald Shenaj",
        "Ondrej Bohdal",
        "Taha Ceritli",
        "Mete Ozay",
        "Pietro Zanuttigh",
        "Umberto Michieli"
      ],
      "abstract": "On-device deployment of Large Language Models (LLMs) frequently leverages\nLow-Rank Adapters (LoRAs) to support diverse downstream tasks under tight\nresource constraints. To address the limited storage capacity of mobile\ndevices, recent works have explored model merging techniques to fuse multiple\nLoRAs into a single one. In practice, however, LoRAs are often delivered\nincrementally, as users request support for new tasks (e.g., novel problem\ntypes or languages). This scenario introduces a new challenge: on-device online\ncontinual merging, where the objective is to incorporate new LoRAs while\npreserving the performance on previously supported tasks. In this paper, we\npropose a data-free and computationally efficient strategy for selecting and\nmerging LoRAs when a new one becomes available, assuming the device can store\nonly a limited number of adapters. Extensive experiments across real-world\ntasks demonstrate the superiority of our approach compared to alternative\nstrategies while adhering to the storage budget and compute limitations of\non-device settings.",
      "pdf_url": "http://arxiv.org/pdf/2510.13537v1",
      "published": "2025-10-15T13:32:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13537v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain",
      "authors": [
        "William Flanagan",
        "Mukunda Das",
        "Rajitha Ramanyake",
        "Swaunja Maslekar",
        "Meghana Manipuri",
        "Joong Ho Choi",
        "Shruti Nair",
        "Shambhavi Bhusan",
        "Sanjana Dulam",
        "Mouni Pendharkar",
        "Nidhi Singh",
        "Vashisth Doshi",
        "Sachi Shah Paresh"
      ],
      "abstract": "As Generative Artificial Intelligence is adopted across the financial\nservices industry, a significant barrier to adoption and usage is measuring\nmodel performance. Historical machine learning metrics can oftentimes fail to\ngeneralize to GenAI workloads and are often supplemented using Subject Matter\nExpert (SME) Evaluation. Even in this combination, many projects fail to\naccount for various unique risks present in choosing specific metrics.\nAdditionally, many widespread benchmarks created by foundational research labs\nand educational institutions fail to generalize to industrial use. This paper\nexplains these challenges and provides a Risk Assessment Framework to allow for\nbetter application of SME and machine learning Metrics",
      "pdf_url": "http://arxiv.org/pdf/2510.13524v1",
      "published": "2025-10-15T13:17:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13524v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Narrow Operator Models of Stellarator Equilibria in Fourier Zernike Basis",
      "authors": [
        "Timo Thun",
        "Rory Conlin",
        "Dario Panici",
        "Daniel Böckenhoff"
      ],
      "abstract": "Numerical computation of the ideal Magnetohydrodynamic (MHD) equilibrium\nmagnetic field is at the base of stellarator optimisation and provides the\nstarting point for solving more sophisticated Partial Differential Equations\n(PDEs) like transport or turbulence models. Conventional approaches solve for a\nsingle stationary point of the ideal MHD equations, which is fully defined by\nthree invariants and the numerical scheme employed by the solver. We present\nthe first numerical approach that can solve for a continuous distribution of\nequilibria with fixed boundary and rotational transform, varying only the\npressure invariant. This approach minimises the force residual by optimising\nparameters of multilayer perceptrons (MLP) that map from a scalar pressure\nmultiplier to the Fourier Zernike basis as implemented in the modern\nstellarator equilibrium solver DESC.",
      "pdf_url": "http://arxiv.org/pdf/2510.13521v1",
      "published": "2025-10-15T13:13:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13521v1",
      "categories": [
        "physics.plasm-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
      "authors": [
        "Tiancheng Gu",
        "Kaicheng Yang",
        "Kaichen Zhang",
        "Xiang An",
        "Ziyong Feng",
        "Yueyi Zhang",
        "Weidong Cai",
        "Jiankang Deng",
        "Lidong Bing"
      ],
      "abstract": "Universal multimodal embedding models are foundational to various tasks.\nExisting approaches typically employ in-batch negative mining by measuring the\nsimilarity of query-candidate pairs. However, these methods often struggle to\ncapture subtle semantic differences among candidates and lack diversity in\nnegative samples. Moreover, the embeddings exhibit limited discriminative\nability in distinguishing false and hard negatives. In this paper, we leverage\nthe advanced understanding capabilities of MLLMs to enhance representation\nlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.\nOur approach first constructs a potential hard negative set through global\nretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes\nMLLMs to assess the semantic alignment of query-candidate pairs and generate\nsoft semantic matching scores. These scores serve as a foundation for hard\nnegative mining, mitigating the impact of false negatives and enabling the\nidentification of diverse, high-quality hard negatives. Furthermore, the\nsemantic matching scores are used as soft labels to mitigate the rigid\none-to-one mapping constraint. By aligning the similarity matrix with the soft\nsemantic matching score matrix, the model learns semantic distinctions among\ncandidates, significantly enhancing its discriminative capacity. To further\nimprove performance, we propose UniME-V2-Reranker, a reranking model trained on\nour mined hard negatives through a joint pairwise and listwise optimization\napproach. We conduct comprehensive experiments on the MMEB benchmark and\nmultiple retrieval tasks, demonstrating that our method achieves\nstate-of-the-art performance on average across all tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.13515v1",
      "published": "2025-10-15T13:07:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13515v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Offline and Online KL-Regularized RLHF under Differential Privacy",
      "authors": [
        "Yulian Wu",
        "Rushil Thareja",
        "Praneeth Vepakomma",
        "Francesco Orabona"
      ],
      "abstract": "In this paper, we study the offline and online settings of reinforcement\nlearning from human feedback (RLHF) with KL-regularization -- a widely used\nobjective function in large language model alignment -- under the $\\epsilon$\nlocal differential privacy ($\\epsilon$-LDP) model on the label of the human\npreference. In the offline setting, we design an algorithm based on the\nprinciple of pessimism and derive a new suboptimality gap of\n$\\tilde{O}(1/[(e^\\epsilon-1)^2 n])$ on the KL-regularized objective under\nsingle-policy concentrability. We also prove its optimality by providing a\nmatching lower bound where $n$ is the sample size.\n  In the online setting, we are the first one to theoretically investigate the\nproblem of KL-regularized RLHF with LDP. We design an optimism-based algorithm\nand derive a logarithmic regret bound of $O(d_{\\mathcal{F}}\\log\n(N_{\\mathcal{F}}\\cdot T) /(e^\\epsilon-1)^2 )$, where $T$ is the total time\nstep, $N_{\\mathcal{F}}$ is cardinality of the reward function space\n$\\mathcal{F}$ and $d_{\\mathcal{F}}$ is a variant of eluder dimension for RLHF.\nAs a by-product of our analysis, our results also imply the first analysis for\nonline KL-regularized RLHF without privacy. We implement our algorithm in the\noffline setting to verify our theoretical results and release our open source\ncode at: https://github.com/rushil-thareja/PPKL-RLHF-Official.",
      "pdf_url": "http://arxiv.org/pdf/2510.13512v1",
      "published": "2025-10-15T13:04:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13512v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Confidence as a Reward: Transforming LLMs into Reward Models",
      "authors": [
        "He Du",
        "Bowen Li",
        "Chengxing Xie",
        "Chang Gao",
        "Kai Chen",
        "Dacheng Tao"
      ],
      "abstract": "Reward models can significantly enhance the reasoning capabilities of large\nlanguage models (LLMs), but they typically require extensive curated data and\ncostly training. To mitigate these challenges, training-free approaches such as\nLLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate\nresponses, achieving promising results. Recent works have also indicated that\nmodel confidence can serve effectively as a reward metric, distinguishing\nbetween chain-of-thought (CoT) and non-CoT paths. However, the concept of using\nconfidence as a reward has not been comprehensively studied. In this work, we\nsystematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful\ntraining-free method that utilizes token-level confidence in the model's final\nanswers as a proxy for reward, especially suitable for close-ended tasks.\nThrough extensive experiments on mathematical reasoning tasks, we demonstrate\nthat CRew outperforms existing training-free reward approaches on the MATH500\nand RewardMATH benchmarks, and even surpasses most trained reward models. We\nfurther identify a strong correlation between CRew scores and the actual\nreasoning performance of the model. Additionally, we find that CRew can\neffectively filter high-quality training data. Building upon these insights, we\npropose CRew-DPO, a training strategy that constructs preference data from\nconfidence scores combined with correctness signals. Finetuning with CRew-DPO\nfurther enhances the model's judging capabilities and consistently outperforms\nexisting self-training methods.",
      "pdf_url": "http://arxiv.org/pdf/2510.13501v1",
      "published": "2025-10-15T12:51:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13501v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts",
      "authors": [
        "Shujun Xia",
        "Haokun Lin",
        "Yichen Wu",
        "Yinan Zhou",
        "Zixuan Li",
        "Zhongwei Wan",
        "Xingrun Xing",
        "Yefeng Zheng",
        "Xiang Li",
        "Caifeng Shan",
        "Zhenan Sun",
        "Quanzheng Li"
      ],
      "abstract": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, \\hk{an enhanced\nbenchmark with broader coverage of medical subjects, designed to evaluate both\nsingle and batch edits under strict locality constraints}. We then propose\nMedREK, a retrieval-based editing framework that integrates a shared query-key\nmodule for precise matching with an attention-based prompt encoder for\ninformative guidance. Experimental results on various medical benchmarks\ndemonstrate that our MedREK achieves superior performance across different core\nmetrics and provides the first validated solution for batch-editing in medical\nLLMs. Our code and dataset are available at\nhttps://github.com/mylittleriver/MedREK.",
      "pdf_url": "http://arxiv.org/pdf/2510.13500v1",
      "published": "2025-10-15T12:50:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13500v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding",
      "authors": [
        "Xiaozhe Li",
        "TianYi Lyu",
        "Siyi Yang",
        "Yuxi Gong",
        "Yizhao Yang",
        "Jinxuan Huang",
        "Ligao Zhang",
        "Zhuoyi Huang",
        "Qingwen Liu"
      ],
      "abstract": "Understanding human intent is a complex, high-level task for large language\nmodels (LLMs), requiring analytical reasoning, contextual interpretation,\ndynamic information aggregation, and decision-making under uncertainty.\nReal-world public discussions, such as consumer product discussions, are rarely\nlinear or involve a single user. Instead, they are characterized by interwoven\nand often conflicting perspectives, divergent concerns, goals, emotional\ntendencies, as well as implicit assumptions and background knowledge about\nusage scenarios. To accurately understand such explicit public intent, an LLM\nmust go beyond parsing individual sentences; it must integrate multi-source\nsignals, reason over inconsistencies, and adapt to evolving discourse, similar\nto how experts in fields like politics, economics, or finance approach complex,\nuncertain environments. Despite the importance of this capability, no\nlarge-scale benchmark currently exists for evaluating LLMs on real-world human\nintent understanding, primarily due to the challenges of collecting real-world\npublic discussion data and constructing a robust evaluation pipeline. To bridge\nthis gap, we introduce \\bench, the first dynamic, live evaluation benchmark\nspecifically designed for intent understanding, particularly in the consumer\ndomain. \\bench is the largest and most diverse benchmark of its kind,\nsupporting real-time updates while preventing data contamination through an\nautomated curation pipeline.",
      "pdf_url": "http://arxiv.org/pdf/2510.13499v1",
      "published": "2025-10-15T12:49:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13499v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation",
      "authors": [
        "Zexin Wang",
        "Lin Shi",
        "Haoyu Wu",
        "Junru Luo",
        "Xiangzeng Kong",
        "Jun Qi"
      ],
      "abstract": "Epilepsy is a prevalent neurological disorder marked by sudden, brief\nepisodes of excessive neuronal activity caused by abnormal electrical\ndischarges, which may lead to some mental disorders. Most existing deep\nlearning methods for epilepsy detection rely solely on unimodal EEG signals,\nneglecting the potential benefits of multimodal information. To address this,\nwe propose a novel multimodal model, DistilCLIP-EEG, based on the CLIP\nframework, which integrates both EEG signals and text descriptions to capture\ncomprehensive features of epileptic seizures. The model involves an EEG encoder\nbased on the Conformer architecture as a text encoder, the proposed Learnable\nBERT (BERT-LP) as prompt learning within the encoders. Both operate in a shared\nlatent space for effective cross-modal representation learning. To enhance\nefficiency and adaptability, we introduce a knowledge distillation method where\nthe trained DistilCLIP-EEG serves as a teacher to guide a more compact student\nmodel to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MIT\ndatasets, both the teacher and student models achieved accuracy rates exceeding\n97%. Across all datasets, the F1-scores were consistently above 0.94,\ndemonstrating the robustness and reliability of the proposed framework.\nMoreover, the student model's parameter count and model size are approximately\n58.1% of those of the teacher model, significantly reducing model complexity\nand storage requirements while maintaining high performance. These results\nhighlight the potential of our proposed model for EEG-based epilepsy detection\nand establish a solid foundation for deploying lightweight models in\nresource-constrained settings.",
      "pdf_url": "http://arxiv.org/pdf/2510.13497v1",
      "published": "2025-10-15T12:49:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13497v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA",
      "authors": [
        "Tommaso Bonomo",
        "Luca Gioffré",
        "Roberto Navigli"
      ],
      "abstract": "Question Answering (QA) on narrative text poses a unique challenge to current\nsystems, requiring a deep understanding of long, complex documents. However,\nthe reliability of NarrativeQA, the most widely used benchmark in this domain,\nis hindered by noisy documents and flawed QA pairs. In this work, we introduce\nLiteraryQA, a high-quality subset of NarrativeQA focused on literary works.\nUsing a human- and LLM-validated pipeline, we identify and correct low-quality\nQA samples while removing extraneous text from source documents. We then carry\nout a meta-evaluation of automatic metrics to clarify how systems should be\nevaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics\nhave a low system-level correlation to human judgment, while LLM-as-a-Judge\nevaluations, even with small open-weight models, can strongly agree with the\nranking identified by humans. Finally, we benchmark a set of long-context LLMs\non LiteraryQA. We release our code and data at\nhttps://github.com/SapienzaNLP/LiteraryQA.",
      "pdf_url": "http://arxiv.org/pdf/2510.13494v1",
      "published": "2025-10-15T12:43:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13494v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Mobile Coverage Analysis using Crowdsourced Data",
      "authors": [
        "Timothy Wong",
        "Tom Freeman",
        "Joseph Feehily"
      ],
      "abstract": "Effective assessment of mobile network coverage and the precise\nidentification of service weak spots are paramount for network operators\nstriving to enhance user Quality of Experience (QoE). This paper presents a\nnovel framework for mobile coverage and weak spot analysis utilising\ncrowdsourced QoE data. The core of our methodology involves coverage analysis\nat the individual cell (antenna) level, subsequently aggregated to the site\nlevel, using empirical geolocation data. A key contribution of this research is\nthe application of One-Class Support Vector Machine (OC-SVM) algorithm for\ncalculating mobile network coverage. This approach models the decision\nhyperplane as the effective coverage contour, facilitating robust calculation\nof coverage areas for individual cells and entire sites. The same methodology\nis extended to analyse crowdsourced service loss reports, thereby identifying\nand quantifying geographically localised weak spots. Our findings demonstrate\nthe efficacy of this novel framework in accurately mapping mobile coverage and,\ncrucially, in highlighting granular areas of signal deficiency, particularly\nwithin complex urban environments.",
      "pdf_url": "http://arxiv.org/pdf/2510.13459v1",
      "published": "2025-10-15T12:00:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13459v1",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.NI",
        "stat.AP"
      ]
    },
    {
      "title": "Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers",
      "authors": [
        "Nico Pelleriti",
        "Christoph Spiegel",
        "Shiwei Liu",
        "David Martínez-Rubio",
        "Max Zimmer",
        "Sebastian Pokutta"
      ],
      "abstract": "Certifying nonnegativity of polynomials is a well-known NP-hard problem with\ndirect applications spanning non-convex optimization, control, robotics, and\nbeyond. A sufficient condition for nonnegativity is the Sum of Squares (SOS)\nproperty, i.e., it can be written as a sum of squares of other polynomials. In\npractice, however, certifying the SOS criterion remains computationally\nexpensive and often involves solving a Semidefinite Program (SDP), whose\ndimensionality grows quadratically in the size of the monomial basis of the SOS\nexpression; hence, various methods to reduce the size of the monomial basis\nhave been proposed. In this work, we introduce the first learning-augmented\nalgorithm to certify the SOS criterion. To this end, we train a Transformer\nmodel that predicts an almost-minimal monomial basis for a given polynomial,\nthereby drastically reducing the size of the corresponding SDP. Our overall\nmethodology comprises three key components: efficient training dataset\ngeneration of over 100 million SOS polynomials, design and training of the\ncorresponding Transformer architecture, and a systematic fallback mechanism to\nensure correct termination, which we analyze theoretically. We validate our\napproach on over 200 benchmark datasets, achieving speedups of over $100\\times$\ncompared to state-of-the-art solvers and enabling the solution of instances\nwhere competing approaches fail. Our findings provide novel insights towards\ntransforming the practical scalability of SOS programming.",
      "pdf_url": "http://arxiv.org/pdf/2510.13444v1",
      "published": "2025-10-15T11:42:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13444v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Rectify and Align GPS Points to Parking Spots via Rank-1 Constraint",
      "authors": [
        "Jiaxing Deng",
        "Junbiao Pang",
        "Zhicheng Wang",
        "Haitao Yu"
      ],
      "abstract": "Parking spots are essential components, providing vital mobile resources for\nresidents in a city. Accurate Global Positioning System (GPS) points of parking\nspots are the core data for subsequent applications,e.g., parking management,\nparking policy, and urban development. However, high-rise buildings tend to\ncause GPS points to drift from the actual locations of parking spots; besides,\nthe standard lower-cost GPS equipment itself has a certain location error.\nTherefore, it is a non-trivial task to correct a few wrong GPS points from a\nlarge number of parking spots in an unsupervised approach. In this paper,\nmotivated by the physical constraints of parking spots (i.e., parking spots are\nparallel to the sides of roads), we propose an unsupervised low-rank method to\neffectively rectify errors in GPS points and further align them to the parking\nspots in a unified framework. The proposed unconventional rectification and\nalignment method is simple and yet effective for any type of GPS point errors.\nExtensive experiments demonstrate the superiority of the proposed method to\nsolve a practical problem. The data set and the code are publicly accessible\nat:https://github.com/pangjunbiao/ITS-Parking-spots-Dataset.",
      "pdf_url": "http://arxiv.org/pdf/2510.13439v1",
      "published": "2025-10-15T11:36:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13439v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse",
      "authors": [
        "Liesbeth Allein",
        "Nataly Pineda-Castañeda",
        "Andrea Rocci",
        "Marie-Francine Moens"
      ],
      "abstract": "How does a cause lead to an effect, and which intermediate causal steps\nexplain their connection? This work scrutinizes the mechanistic causal\nreasoning capabilities of large language models (LLMs) to answer these\nquestions through the task of implicit causal chain discovery. In a diagnostic\nevaluation framework, we instruct nine LLMs to generate all possible\nintermediate causal steps linking given cause-effect pairs in causal chain\nstructures. These pairs are drawn from recent resources in argumentation\nstudies featuring polarized discussion on climate change. Our analysis reveals\nthat LLMs vary in the number and granularity of causal steps they produce.\nAlthough they are generally self-consistent and confident about the\nintermediate causal connections in the generated chains, their judgments are\nmainly driven by associative pattern matching rather than genuine causal\nreasoning. Nonetheless, human evaluations confirmed the logical coherence and\nintegrity of the generated chains. Our baseline causal chain discovery\napproach, insights from our diagnostic evaluation, and benchmark dataset with\ncausal chains lay a solid foundation for advancing future work in implicit,\nmechanistic causal reasoning in argumentation settings.",
      "pdf_url": "http://arxiv.org/pdf/2510.13417v1",
      "published": "2025-10-15T11:15:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13417v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Semantic Communication Enabled Holographic Video Processing and Transmission",
      "authors": [
        "Jingkai Ying",
        "Zhiyuan Qi",
        "Yulong Feng",
        "Zhijin Qin",
        "Zhu Han",
        "Rahim Tafazolli",
        "Yonina C. Eldar"
      ],
      "abstract": "Holographic video communication is considered a paradigm shift in visual\ncommunications, becoming increasingly popular for its ability to offer\nimmersive experiences. This article provides an overview of holographic video\ncommunication and outlines the requirements of a holographic video\ncommunication system. Particularly, following a brief review of semantic com-\nmunication, an architecture for a semantic-enabled holographic video\ncommunication system is presented. Key technologies, including semantic\nsampling, joint semantic-channel coding, and semantic-aware transmission, are\ndesigned based on the proposed architecture. Two related use cases are\npresented to demonstrate the performance gain of the proposed methods. Finally,\npotential research topics are discussed to pave the way for the realization of\nsemantic-enabled holographic video communications.",
      "pdf_url": "http://arxiv.org/pdf/2510.13408v1",
      "published": "2025-10-15T11:06:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13408v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.IT",
        "cs.MM",
        "eess.SP",
        "math.IT"
      ]
    },
    {
      "title": "From Minimal Existence to Human Definition: The CES-IMU-HSG Theoretical Framework",
      "authors": [
        "Kei Itoh"
      ],
      "abstract": "This study presents an inter-universal mathematical-logical framework\nconstructed upon the minimal axiom Cogito, ergo sum (CES), integrating the\nIntermediate Meta-Universe (IMU) and the Hierarchical State Grid (HSG). The CES\ndefines existence as a reflexive correspondence --'to be' and 'to be\nsayable'--and positions any formal system, including ZFC or HoTT, as an\nattachable extension atop this minimal structure. The IMU functions as a\nregistry of axiomatic dependencies that connect heterogeneous theories,\nemploying the Institution-theoretic framework to ensure coherent\ninter-theoretical linkages. The HSG concretizes these ideas through categorical\nconstruction, defined by three orthogonal axes: the state-depth axis, the\nmapping-hierarchy axis, and the temporal axis incorporating the principle of\n'no future reference.' Through these, the identity of 'definition = state' is\nformally established as a categorical property. Extending this structure to\nbiological systems, the neural system is implemented as a 0-3D complex of\nneuron-function fields on the HSG, while its categorical extensions via\nfiberization over the material base enable the parallel integration of multiple\nphysiological universes-neural, endocrine, learning, genetic, and input/output\nsystems-into a coherent adjoint ensemble. Within this framework, human behavior\nand cognition emerge as temporal compositions of inter-universal algorithms\nconstrained by the material base. Finally, by contrasting human cognition,\nwhich relies on external CES, with machine existence, this study introduces the\nconcept of internal CES, wherein a machine grounds its own logic upon the\nfactuality of its operation. This internal self-axiomatization establishes a\ncontinuous bridge between philosophical ontology and engineering\nimplementation, providing a new foundation for the autonomous and self-defining\nexistence of artificial intelligence.",
      "pdf_url": "http://arxiv.org/pdf/2510.13400v1",
      "published": "2025-10-15T10:56:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13400v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization",
      "authors": [
        "Yunxiao Zhao",
        "Zhiqiang Wang",
        "Xingtong Yu",
        "Xiaoli Li",
        "Jiye Liang",
        "Ru Li"
      ],
      "abstract": "Rationalization, a data-centric framework, aims to build self-explanatory\nmodels to explain the prediction outcome by generating a subset of\nhuman-intelligible pieces of the input data. It involves a cooperative game\nmodel where a generator generates the most human-intelligible parts of the\ninput (i.e., rationales), followed by a predictor that makes predictions based\non these generated rationales. Conventional rationalization methods typically\nimpose constraints via regularization terms to calibrate or penalize undesired\ngeneration. However, these methods are suffering from a problem called mode\ncollapse, in which the predictor produces correct predictions yet the generator\nconsistently outputs rationales with collapsed patterns. Moreover, existing\nstudies are typically designed separately for specific collapsed patterns,\nlacking a unified consideration. In this paper, we systematically revisit\ncooperative rationalization from a novel game-theoretic perspective and\nidentify the fundamental cause of this problem: the generator no longer tends\nto explore new strategies to uncover informative rationales, ultimately leading\nthe system to converge to a suboptimal game equilibrium (correct predictions\nv.s collapsed rationales). To solve this problem, we then propose a novel\napproach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),\nwhich progressively introduces policy interventions to address the game\nequilibrium in the cooperative game process, thereby guiding the model toward a\nmore optimal solution state. We theoretically analyse the cause of such a\nsuboptimal equilibrium and prove the feasibility of the proposed method.\nFurthermore, we validate our method on nine widely used real-world datasets and\ntwo synthetic settings, where PORAT achieves up to 8.1% performance\nimprovements over existing state-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2510.13393v1",
      "published": "2025-10-15T10:42:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13393v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation",
      "authors": [
        "Jiin Park",
        "Misuk Kim"
      ],
      "abstract": "Recent attempts to integrate large language models (LLMs) into recommender\nsystems have gained momentum, but most remain limited to simple text generation\nor static prompt-based inference, failing to capture the complexity of user\npreferences and real-world interactions. This study proposes the Multi-Aspect\nDriven LLM Agent MADRec, an autonomous LLM-based recommender that constructs\nuser and item profiles by unsupervised extraction of multi-aspect information\nfrom reviews and performs direct recommendation, sequential recommendation, and\nexplanation generation. MADRec generates structured profiles via\naspect-category-based summarization and applies Re-Ranking to construct\nhigh-density inputs. When the ground-truth item is missing from the output, the\nSelf-Feedback mechanism dynamically adjusts the inference criteria. Experiments\nacross multiple domains show that MADRec outperforms traditional and LLM-based\nbaselines in both precision and explainability, with human evaluation further\nconfirming the persuasiveness of the generated explanations.",
      "pdf_url": "http://arxiv.org/pdf/2510.13371v1",
      "published": "2025-10-15T10:03:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13371v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control",
      "authors": [
        "Nikita Kachaev",
        "Daniil Zelezetsky",
        "Egor Cherepanov",
        "Alexey K. Kovelev",
        "Aleksandr I. Panov"
      ],
      "abstract": "Despite their effectiveness and popularity in offline or model-based\nreinforcement learning (RL), transformers remain underexplored in online\nmodel-free RL due to their sensitivity to training setups and model design\ndecisions such as how to structure the policy and value networks, share\ncomponents, or handle temporal information. In this paper, we show that\ntransformers can be strong baselines for continuous control in online\nmodel-free RL. We investigate key design questions: how to condition inputs,\nshare components between actor and critic, and slice sequential data for\ntraining. Our experiments reveal stable architectural and training strategies\nenabling competitive performance across fully and partially observable tasks,\nand in both vector- and image-based settings. These findings offer practical\nguidance for applying transformers in online RL.",
      "pdf_url": "http://arxiv.org/pdf/2510.13367v1",
      "published": "2025-10-15T09:58:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.13367v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    }
  ]
}