{
  "last_updated": "2025-05-06T00:51:24.044498",
  "papers": [
    {
      "title": "GENMO: A GENeralist Model for Human MOtion",
      "authors": [
        "Jiefeng Li",
        "Jinkun Cao",
        "Haotian Zhang",
        "Davis Rempe",
        "Jan Kautz",
        "Umar Iqbal",
        "Ye Yuan"
      ],
      "abstract": "Human motion modeling traditionally separates motion generation and\nestimation into distinct tasks with specialized models. Motion generation\nmodels focus on creating diverse, realistic motions from inputs like text,\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\nmotion trajectories from observations like videos. Despite sharing underlying\nrepresentations of temporal dynamics and kinematics, this separation limits\nknowledge transfer between tasks and requires maintaining separate models. We\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\nestimation and generation in a single framework. Our key insight is to\nreformulate motion estimation as constrained motion generation, where the\noutput motion must precisely satisfy observed conditioning signals. Leveraging\nthe synergy between regression and diffusion, GENMO achieves accurate global\nmotion estimation while enabling diverse motion generation. We also introduce\nan estimation-guided training objective that exploits in-the-wild videos with\n2D annotations and text descriptions to enhance generative diversity.\nFurthermore, our novel architecture handles variable-length motions and mixed\nmultimodal conditions (text, audio, video) at different time intervals,\noffering flexible control. This unified approach creates synergistic benefits:\ngenerative priors improve estimated motions under challenging conditions like\nocclusions, while diverse video data enhances generation capabilities.\nExtensive experiments demonstrate GENMO's effectiveness as a generalist\nframework that successfully handles multiple human motion tasks within a single\nmodel.",
      "pdf_url": "http://arxiv.org/pdf/2505.01425v1",
      "published": "2025-05-02T17:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01425v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "SIME: Enhancing Policy Self-Improvement with Modal-level Exploration",
      "authors": [
        "Yang Jin",
        "Jun Lv",
        "Wenye Yu",
        "Hongjie Fang",
        "Yong-Lu Li",
        "Cewu Lu"
      ],
      "abstract": "Self-improvement requires robotic systems to initially learn from\nhuman-provided data and then gradually enhance their capabilities through\ninteraction with the environment. This is similar to how humans improve their\nskills through continuous practice. However, achieving effective\nself-improvement is challenging, primarily because robots tend to repeat their\nexisting abilities during interactions, often failing to generate new, valuable\ndata for learning. In this paper, we identify the key to successful\nself-improvement: modal-level exploration and data selection. By incorporating\na modal-level exploration mechanism during policy execution, the robot can\nproduce more diverse and multi-modal interactions. At the same time, we select\nthe most valuable trials and high-quality segments from these interactions for\nlearning. We successfully demonstrate effective robot self-improvement on both\nsimulation benchmarks and real-world experiments. The capability for\nself-improvement will enable us to develop more robust and high-success-rate\nrobotic control strategies at a lower cost. Our code and experiment scripts are\navailable at https://ericjin2002.github.io/SIME/",
      "pdf_url": "http://arxiv.org/pdf/2505.01396v1",
      "published": "2025-05-02T17:13:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01396v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer",
      "authors": [
        "Alice Natalina Caragliano",
        "Claudia Tacconi",
        "Carlo Greco",
        "Lorenzo Nibid",
        "Edy Ippolito",
        "Michele Fiore",
        "Giuseppe Perrone",
        "Sara Ramella",
        "Paolo Soda",
        "Valerio Guarrasi"
      ],
      "abstract": "This study proposes a novel approach combining Multimodal Deep Learning with\nintrinsic eXplainable Artificial Intelligence techniques to predict\npathological response in non-small cell lung cancer patients undergoing\nneoadjuvant therapy. Due to the limitations of existing radiomics and unimodal\ndeep learning approaches, we introduce an intermediate fusion strategy that\nintegrates imaging and clinical data, enabling efficient interaction between\ndata modalities. The proposed Multimodal Doctor-in-the-Loop method further\nenhances clinical relevance by embedding clinicians' domain knowledge directly\ninto the training process, guiding the model's focus gradually from broader\nlung regions to specific lesions. Results demonstrate improved predictive\naccuracy and explainability, providing insights into optimal data integration\nstrategies for clinical applications.",
      "pdf_url": "http://arxiv.org/pdf/2505.01390v1",
      "published": "2025-05-02T16:57:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01390v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research",
      "authors": [
        "Yan Miao",
        "Will Shen",
        "Hang Cui",
        "Sayan Mitra"
      ],
      "abstract": "We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing\nplatform for autonomy research. The hardware platform integrates a small\ncamera, a standard airframe, offboard computation, and radio communication for\nmanual overrides. We demonstrate FalconWing's capabilities by developing and\ndeploying a purely vision-based control policy for autonomous landing (without\nIMU or motion capture) using a novel real-to-sim-to-real learning approach. Our\nlearning approach: (1) constructs a photorealistic simulation environment via\n3D Gaussian splatting trained on real-world images; (2) identifies nonlinear\ndynamics from vision-estimated real-flight data; and (3) trains a multi-modal\nVision Transformer (ViT) policy through simulation-only imitation learning. The\nViT architecture fuses single RGB image with the history of control actions via\nself-attention, preserving temporal context while maintaining real-time 20 Hz\ninference. When deployed zero-shot on the hardware platform, this policy\nachieves an 80% success rate in vision-based autonomous landings. Together with\nthe hardware specifications, we also open-source the system dynamics, the\nsoftware for photorealistic simulator and the learning approach.",
      "pdf_url": "http://arxiv.org/pdf/2505.01383v1",
      "published": "2025-05-02T16:47:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01383v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii",
      "authors": [
        "Kola Ayonrinde",
        "Louis Jaburi"
      ],
      "abstract": "Mechanistic Interpretability (MI) aims to understand neural networks through\ncausal explanations. Though MI has many explanation-generating methods,\nprogress has been limited by the lack of a universal approach to evaluating\nexplanations. Here we analyse the fundamental question \"What makes a good\nexplanation?\" We introduce a pluralist Explanatory Virtues Framework drawing on\nfour perspectives from the Philosophy of Science - the Bayesian, Kuhnian,\nDeutschian, and Nomological - to systematically evaluate and improve\nexplanations in MI. We find that Compact Proofs consider many explanatory\nvirtues and are hence a promising approach. Fruitful research directions\nimplied by our framework include (1) clearly defining explanatory simplicity,\n(2) focusing on unifying explanations and (3) deriving universal principles for\nneural networks. Improved MI methods enhance our ability to monitor, predict,\nand steer AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.01372v1",
      "published": "2025-05-02T16:18:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01372v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ]
    },
    {
      "title": "Differentiable Nonlinear Model Predictive Control",
      "authors": [
        "Jonathan Frey",
        "Katrin Baumg√§rtner",
        "Gianluca Frison",
        "Dirk Reinhardt",
        "Jasper Hoffmann",
        "Leonard Fichtner",
        "Sebastien Gros",
        "Moritz Diehl"
      ],
      "abstract": "The efficient computation of parametric solution sensitivities is a key\nchallenge in the integration of learning-enhanced methods with nonlinear model\npredictive control (MPC), as their availability is crucial for many learning\nalgorithms. While approaches presented in the machine learning community are\nlimited to convex or unconstrained formulations, this paper discusses the\ncomputation of solution sensitivities of general nonlinear programs (NLPs)\nusing the implicit function theorem (IFT) and smoothed optimality conditions\ntreated in interior-point methods (IPM). We detail sensitivity computation\nwithin a sequential quadratic programming (SQP) method which employs an IPM for\nthe quadratic subproblems. The publication is accompanied by an efficient\nopen-source implementation within the framework, providing both forward and\nadjoint sensitivities for general optimal control problems, achieving speedups\nexceeding 3x over the state-of-the-art solver mpc.pytorch.",
      "pdf_url": "http://arxiv.org/pdf/2505.01353v1",
      "published": "2025-05-02T15:43:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01353v1",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing",
      "authors": [
        "Dongliang Guo",
        "Mengxuan Hu",
        "Zihan Guan",
        "Thomas Hartvigsen",
        "Sheng Li"
      ],
      "abstract": "Large multi-modal models inevitably decay over time as facts change and\npreviously learned information becomes outdated. Traditional approaches such as\nfine-tuning are often impractical for updating these models due to their size\nand complexity. Instead, direct knowledge editing within the models presents a\nmore viable solution. Current model editing techniques, however, typically\noverlook the unique influence ranges of different facts, leading to compromised\nmodel performance in terms of both generality and locality. To address this\nissue, we introduce the concept of the generality-locality trade-off in\nmulti-modal model editing. We develop a new model editing dataset named OKEDIT,\nspecifically designed to effectively evaluate this trade-off. Building on this\nfoundation, we propose BalancEdit, a novel method for balanced model editing\nthat dynamically achieves an optimal balance between generality and locality.\nBalancEdit utilizes a unique mechanism that generates both positive and\nnegative samples for each fact to accurately determine its influence scope and\nincorporates these insights into the model's latent space using a discrete,\nlocalized codebook of edits, without modifying the underlying model weights. To\nour knowledge, this is the first approach explicitly addressing the\ngenerality-locality trade-off in multi-modal model editing. Our comprehensive\nresults confirm the effectiveness of BalancEdit, demonstrating minimal\ntrade-offs while maintaining robust editing capabilities. Our code and dataset\nwill be available.",
      "pdf_url": "http://arxiv.org/pdf/2505.01343v1",
      "published": "2025-05-02T15:31:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01343v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability",
      "authors": [
        "Anass Grini",
        "Oumaima Taheri",
        "Btissam El Khamlichi",
        "Amal El Fallah-Seghrouchni"
      ],
      "abstract": "While machine learning has significantly advanced Network Intrusion Detection\nSystems (NIDS), particularly within IoT environments where devices generate\nlarge volumes of data and are increasingly susceptible to cyber threats, these\nmodels remain vulnerable to adversarial attacks. Our research reveals a\ncritical flaw in existing adversarial attack methodologies: the frequent\nviolation of domain-specific constraints, such as numerical and categorical\nlimits, inherent to IoT and network traffic. This leads to up to 80.3% of\nadversarial examples being invalid, significantly overstating real-world\nvulnerabilities. These invalid examples, though effective in fooling models, do\nnot represent feasible attacks within practical IoT deployments. Consequently,\nrelying on these results can mislead resource allocation for defense, inflating\nthe perceived susceptibility of IoT-enabled NIDS models to adversarial\nmanipulation. Furthermore, we demonstrate that simpler surrogate models like\nMulti-Layer Perceptron (MLP) generate more valid adversarial examples compared\nto complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,\nwe analyze the transferability of adversarial severity to other ML/DL models\ncommonly used in IoT contexts. This work underscores the importance of\nconsidering both domain constraints and model architecture when evaluating and\ndesigning robust ML/DL models for security-critical IoT and network\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2505.01328v1",
      "published": "2025-05-02T15:01:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01328v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System",
      "authors": [
        "Sheikh Samit Muhaimin",
        "Spyridon Mastorakis"
      ],
      "abstract": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.",
      "pdf_url": "http://arxiv.org/pdf/2505.01315v1",
      "published": "2025-05-02T14:42:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01315v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing SPARQL Query Rewriting for Complex Ontology Alignments",
      "authors": [
        "Anicet Lepetit Ondo",
        "Laurence Capus",
        "Mamadou Bousso"
      ],
      "abstract": "SPARQL query rewriting is a fundamental mechanism for uniformly querying\nheterogeneous ontologies in the Linked Data Web. However, the complexity of\nontology alignments, particularly rich correspondences (c : c), makes this\nprocess challenging. Existing approaches primarily focus on simple (s : s) and\npartially complex ( s : c) alignments, thereby overlooking the challenges posed\nby more expressive alignments. Moreover, the intricate syntax of SPARQL\npresents a barrier for non-expert users seeking to fully exploit the knowledge\nencapsulated in ontologies. This article proposes an innovative approach for\nthe automatic rewriting of SPARQL queries from a source ontology to a target\nontology, based on a user's need expressed in natural language. It leverages\nthe principles of equivalence transitivity as well as the advanced capabilities\nof large language models such as GPT-4. By integrating these elements, this\napproach stands out for its ability to efficiently handle complex alignments,\nparticularly (c : c) correspondences , by fully exploiting their\nexpressiveness. Additionally, it facilitates access to aligned ontologies for\nusers unfamiliar with SPARQL, providing a flexible solution for querying\nheterogeneous data.",
      "pdf_url": "http://arxiv.org/pdf/2505.01309v1",
      "published": "2025-05-02T14:38:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01309v1",
      "categories": [
        "cs.DB",
        "cs.AI"
      ]
    },
    {
      "title": "Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments",
      "authors": [
        "Regan Bolton",
        "Mohammadreza Sheikhfathollahi",
        "Simon Parkinson",
        "Vanessa Vulovic",
        "Gary Bamford",
        "Dan Basher",
        "Howard Parkinson"
      ],
      "abstract": "Safety critical software assessment requires robust assessment against\ncomplex regulatory frameworks, a process traditionally limited by manual\nevaluation. This paper presents Document Retrieval-Augmented Fine-Tuning\n(DRAFT), a novel approach that enhances the capabilities of a large language\nmodel (LLM) for safety-critical compliance assessment. DRAFT builds upon\nexisting Retrieval-Augmented Generation (RAG) techniques by introducing a novel\nfine-tuning framework that accommodates our dual-retrieval architecture, which\nsimultaneously accesses both software documentation and applicable reference\nstandards. To fine-tune DRAFT, we develop a semi-automated dataset generation\nmethodology that incorporates variable numbers of relevant documents with\nmeaningful distractors, closely mirroring real-world assessment scenarios.\nExperiments with GPT-4o-mini demonstrate a 7% improvement in correctness over\nthe baseline model, with qualitative improvements in evidence handling,\nresponse structure, and domain-specific reasoning. DRAFT represents a practical\napproach to improving compliance assessment systems while maintaining the\ntransparency and evidence-based reasoning essential in regulatory domains.",
      "pdf_url": "http://arxiv.org/pdf/2505.01307v1",
      "published": "2025-05-02T14:34:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01307v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System",
      "authors": [
        "Lo Pang-Yun Ting",
        "Hong-Pei Chen",
        "An-Shan Liu",
        "Chun-Yin Yeh",
        "Po-Lin Chen",
        "Kun-Ta Chuang"
      ],
      "abstract": "Early detection of patient deterioration is crucial for reducing mortality\nrates. Heart rate data has shown promise in assessing patient health, and\nwearable devices offer a cost-effective solution for real-time monitoring.\nHowever, extracting meaningful insights from diverse heart rate data and\nhandling missing values in wearable device data remain key challenges. To\naddress these challenges, we propose TARL, an innovative approach that models\nthe structural relationships of representative subsequences, known as\nshapelets, in heart rate time series. TARL creates a shapelet-transition\nknowledge graph to model shapelet dynamics in heart rate time series,\nindicating illness progression and potential future changes. We further\nintroduce a transition-aware knowledge embedding to reinforce relationships\namong shapelets and quantify the impact of missing values, enabling the\nformulation of comprehensive heart rate representations. These representations\ncapture explanatory structures and predict future heart rate trends, aiding\nearly illness detection. We collaborate with physicians and nurses to gather\nICU patient heart rate data from wearables and diagnostic metrics assessing\nillness severity for evaluating deterioration. Experiments on real-world ICU\ndata demonstrate that TARL achieves both high reliability and early detection.\nA case study further showcases TARL's explainable detection process,\nhighlighting its potential as an AI-driven tool to assist clinicians in\nrecognizing early signs of patient deterioration.",
      "pdf_url": "http://arxiv.org/pdf/2505.01305v1",
      "published": "2025-05-02T14:32:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01305v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow",
      "authors": [
        "Changhe Chen",
        "Quantao Yang",
        "Xiaohao Xu",
        "Nima Fazeli",
        "Olov Andersson"
      ],
      "abstract": "One of the central challenges preventing robots from acquiring complex\nmanipulation skills is the prohibitive cost of collecting large-scale robot\ndemonstrations. In contrast, humans are able to learn efficiently by watching\nothers interact with their environment. To bridge this gap, we introduce\nsemantic action flow as a core intermediate representation capturing the\nessential spatio-temporal manipulator-object interactions, invariant to\nsuperficial visual differences. We present ViSA-Flow, a framework that learns\nthis representation self-supervised from unlabeled large-scale video data.\nFirst, a generative model is pre-trained on semantic action flows automatically\nextracted from large-scale human-object interaction video data, learning a\nrobust prior over manipulation structure. Second, this prior is efficiently\nadapted to a target robot by fine-tuning on a small set of robot demonstrations\nprocessed through the same semantic abstraction pipeline. We demonstrate\nthrough extensive experiments on the CALVIN benchmark and real-world tasks that\nViSA-Flow achieves state-of-the-art performance, particularly in low-data\nregimes, outperforming prior methods by effectively transferring knowledge from\nhuman video observation to robotic execution. Videos are available at\nhttps://visaflow-web.github.io/ViSAFLOW.",
      "pdf_url": "http://arxiv.org/pdf/2505.01288v1",
      "published": "2025-05-02T14:03:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01288v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables",
      "authors": [
        "Yajuan Zhang",
        "Jiahai Jiang",
        "Yule Yan",
        "Liang Yang",
        "Ping Zhang"
      ],
      "abstract": "Accurate wind power forecasting can help formulate scientific dispatch plans,\nwhich is of great significance for maintaining the safety, stability, and\nefficient operation of the power system. In recent years, wind power\nforecasting methods based on deep learning have focused on extracting the\nspatiotemporal correlations among data, achieving significant improvements in\nforecasting accuracy. However, they exhibit two limitations. First, there is a\nlack of modeling for the inter-variable relationships, which limits the\naccuracy of the forecasts. Second, by treating endogenous and exogenous\nvariables equally, it leads to unnecessary interactions between the endogenous\nand exogenous variables, increasing the complexity of the model. In this paper,\nwe propose the 2DXformer, which, building upon the previous work's focus on\nspatiotemporal correlations, addresses the aforementioned two limitations.\nSpecifically, we classify the inputs of the model into three types: exogenous\nstatic variables, exogenous dynamic variables, and endogenous variables. First,\nwe embed these variables as variable tokens in a channel-independent manner.\nThen, we use the attention mechanism to capture the correlations among\nexogenous variables. Finally, we employ a multi-layer perceptron with residual\nconnections to model the impact of exogenous variables on endogenous variables.\nExperimental results on two real-world large-scale datasets indicate that our\nproposed 2DXformer can further improve the performance of wind power\nforecasting. The code is available in this repository:\n\\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.",
      "pdf_url": "http://arxiv.org/pdf/2505.01286v1",
      "published": "2025-05-02T14:00:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01286v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Reduced-order structure-property linkages for stochastic metamaterials",
      "authors": [
        "Hooman Danesh",
        "Maruthi Annamaraju",
        "Tim Brepols",
        "Stefanie Reese",
        "Surya R. Kalidindi"
      ],
      "abstract": "The capabilities of additive manufacturing have facilitated the design and\nproduction of mechanical metamaterials with diverse unit cell geometries.\nEstablishing linkages between the vast design space of unit cells and their\neffective mechanical properties is critical for the efficient design and\nperformance evaluation of such metamaterials. However, physics-based\nsimulations of metamaterial unit cells across the entire design space are\ncomputationally expensive, necessitating a materials informatics framework to\nefficiently capture complex structure-property relationships. In this work,\nprincipal component analysis of 2-point correlation functions is performed to\nextract the salient features from a large dataset of randomly generated 2D\nmetamaterials. Physics-based simulations are performed using a fast Fourier\ntransform (FFT)-based homogenization approach to efficiently compute the\nhomogenized effective elastic stiffness across the extensive unit cell designs.\nSubsequently, Gaussian process regression is used to generate reduced-order\nsurrogates, mapping unit cell designs to their homogenized effective elastic\nconstant. It is demonstrated that the adopted workflow enables a high-value\nlow-dimensional representation of the voluminous stochastic metamaterial\ndataset, facilitating the construction of robust structure-property maps.\nFinally, an uncertainty-based active learning framework is utilized to train a\nsurrogate model with a significantly smaller number of data points compared to\nthe original full dataset. It is shown that a dataset as small as $0.61\\%$ of\nthe entire dataset is sufficient to generate accurate and robust\nstructure-property maps.",
      "pdf_url": "http://arxiv.org/pdf/2505.01283v1",
      "published": "2025-05-02T13:58:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01283v1",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A Physics-preserved Transfer Learning Method for Differential Equations",
      "authors": [
        "Hao-Ran Yang",
        "Chuan-Xian Ren"
      ],
      "abstract": "While data-driven methods such as neural operator have achieved great success\nin solving differential equations (DEs), they suffer from domain shift problems\ncaused by different learning environments (with data bias or equation changes),\nwhich can be alleviated by transfer learning (TL). However, existing TL methods\nadopted in DEs problems lack either generalizability in general DEs problems or\nphysics preservation during training. In this work, we focus on a general\ntransfer learning method that adaptively correct the domain shift and preserve\nphysical information. Mathematically, we characterize the data domain as\nproduct distribution and the essential problems as distribution bias and\noperator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that\nsimultaneously admits generalizability to common DEs and physics preservation\nof specific problem is proposed to adapt the data-driven model to target domain\nutilizing the push-forward distribution induced by the POTT map. Extensive\nexperiments demonstrate the superior performance, generalizability and physics\npreservation of the proposed POTT method.",
      "pdf_url": "http://arxiv.org/pdf/2505.01281v1",
      "published": "2025-05-02T13:58:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01281v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications",
      "authors": [
        "Elie Saad",
        "Mariem Besbes",
        "Marc Zolghadri",
        "Victor Czmil",
        "Claude Baron",
        "Vincent Bourgeois"
      ],
      "abstract": "The challenge of electronic component obsolescence is particularly critical\nin systems with long life cycles. Various obsolescence management methods are\nemployed to mitigate its impact, with obsolescence forecasting being a highly\nsought-after and prominent approach. As a result, numerous machine\nlearning-based forecasting methods have been proposed. However, machine\nlearning models require a substantial amount of relevant data to achieve high\nprecision, which is lacking in the current obsolescence landscape in some\nsituations. This work introduces a novel framework for obsolescence forecasting\nbased on deep learning. The proposed framework solves the lack of available\ndata through deep generative modeling, where new obsolescence cases are\ngenerated and used to augment the training dataset. The augmented dataset is\nthen used to train a classical machine learning-based obsolescence forecasting\nmodel. To train classical forecasting models using augmented datasets, existing\nclassical supervised-learning classifiers are adapted for semi-supervised\nlearning within this framework. The proposed framework demonstrates\nstate-of-the-art results on benchmarking datasets.",
      "pdf_url": "http://arxiv.org/pdf/2505.01261v1",
      "published": "2025-05-02T13:28:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01261v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models",
      "authors": [
        "Mahdi Dhaini",
        "Kafaite Zahra Hussain",
        "Efstratios Zaradoukas",
        "Gjergji Kasneci"
      ],
      "abstract": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP.",
      "pdf_url": "http://arxiv.org/pdf/2505.01238v1",
      "published": "2025-05-02T13:00:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01238v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods",
      "authors": [
        "Mahdi Dhaini",
        "Ege Erdogan",
        "Nils Feldhus",
        "Gjergji Kasneci"
      ],
      "abstract": "While research on applications and evaluations of explanation methods\ncontinues to expand, fairness of the explanation methods concerning disparities\nin their performance across subgroups remains an often overlooked aspect. In\nthis paper, we address this gap by showing that, across three tasks and five\nlanguage models, widely used post-hoc feature attribution methods exhibit\nsignificant gender disparity with respect to their faithfulness, robustness,\nand complexity. These disparities persist even when the models are pre-trained\nor fine-tuned on particularly unbiased datasets, indicating that the\ndisparities we observe are not merely consequences of biased training data. Our\nresults highlight the importance of addressing disparities in explanations when\ndeveloping and applying explainability methods, as these can lead to biased\noutcomes against certain subgroups, with particularly critical implications in\nhigh-stakes contexts. Furthermore, our findings underscore the importance of\nincorporating the fairness of explanations, alongside overall model fairness\nand explainability, as a requirement in regulatory frameworks.",
      "pdf_url": "http://arxiv.org/pdf/2505.01198v1",
      "published": "2025-05-02T11:41:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01198v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions",
      "authors": [
        "Federico Maria Cau",
        "Lucio Davide Spano"
      ],
      "abstract": "Artificial Intelligence (AI) systems are increasingly used for\ndecision-making across domains, raising debates over the information and\nexplanations they should provide. Most research on Explainable AI (XAI) has\nfocused on feature-based explanations, with less attention on alternative\nstyles. Personality traits like the Need for Cognition (NFC) can also lead to\ndifferent decision-making outcomes among low and high NFC individuals. We\ninvestigated how presenting AI information (prediction, confidence, and\naccuracy) and different explanation styles (example-based, feature-based,\nrule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive\nload in a loan application scenario. We also examined low and high NFC\nindividuals' differences in prioritizing XAI interface elements (loan\nattributes, AI information, and explanations), accuracy, and cognitive load.\nOur findings show that high AI confidence significantly increases reliance on\nAI while reducing cognitive load. Feature-based explanations did not enhance\naccuracy compared to other conditions. Although counterfactual explanations\nwere less understandable, they enhanced overall accuracy, increasing reliance\non AI and reducing cognitive load when AI predictions were correct. Both low\nand high NFC individuals prioritized explanations after loan attributes,\nleaving AI information as the least important. However, we found no significant\ndifferences between low and high NFC groups in accuracy or cognitive load,\nraising questions about the role of personality traits in AI-assisted\ndecision-making. These findings highlight the need for user-centric\npersonalization in XAI interfaces, incorporating diverse explanation styles and\nexploring multiple personality traits and other user characteristics to\noptimize human-AI collaboration.",
      "pdf_url": "http://arxiv.org/pdf/2505.01192v1",
      "published": "2025-05-02T11:30:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01192v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks",
      "authors": [
        "M. Saeid HaghighiFard",
        "Sinem Coleri"
      ],
      "abstract": "Hierarchical Federated Learning (HFL) has recently emerged as a promising\nsolution for intelligent decision-making in vehicular networks, helping to\naddress challenges such as limited communication resources, high vehicle\nmobility, and data heterogeneity. However, HFL remains vulnerable to\nadversarial and unreliable vehicles, whose misleading updates can significantly\ncompromise the integrity and convergence of the global model. To address these\nchallenges, we propose a novel defense framework that integrates dynamic\nvehicle selection with robust anomaly detection within a cluster-based HFL\narchitecture, specifically designed to counter Gaussian noise and gradient\nascent attacks. The framework performs a comprehensive reliability assessment\nfor each vehicle by evaluating historical accuracy, contribution frequency, and\nanomaly records. Anomaly detection combines Z-score and cosine similarity\nanalyses on model updates to identify both statistical outliers and directional\ndeviations in model updates. To further refine detection, an adaptive\nthresholding mechanism is incorporated into the cosine similarity metric,\ndynamically adjusting the threshold based on the historical accuracy of each\nvehicle to enforce stricter standards for consistently high-performing\nvehicles. In addition, a weighted gradient averaging mechanism is implemented,\nwhich assigns higher weights to gradient updates from more trustworthy\nvehicles. To defend against coordinated attacks, a cross-cluster consistency\ncheck is applied to identify collaborative attacks in which multiple\ncompromised clusters coordinate misleading updates. Together, these mechanisms\nform a multi-level defense strategy to filter out malicious contributions\neffectively. Simulation results show that the proposed algorithm significantly\nreduces convergence time compared to benchmark methods across both 1-hop and\n3-hop topologies.",
      "pdf_url": "http://arxiv.org/pdf/2505.01186v1",
      "published": "2025-05-02T11:01:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01186v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing",
      "authors": [
        "Nahshon Mokua Obiri",
        "Kristof Van Laerhoven"
      ],
      "abstract": "LoRaWAN technology's extensive coverage positions it as a strong contender\nfor large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor\nlocalization remains challenging due to complex environmental conditions,\nmultipath fading, and transient obstructions. This paper proposes a lightweight\nbut robust approach combining adaptive filtering with an extended log-distance,\nmulti-wall path loss and shadowing (PLS) model. Our methodology augments\nconventional models with critical LoRaWAN parameters (received signal strength\nindicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic\nenvironmental indicators (temperature, humidity, carbon dioxide, particulate\nmatter, and barometric pressure). An adaptive Kalman filter reduces RSSI\nfluctuations, isolating persistent trends from momentary noise. Using a\nsix-month dataset of 1,328,334 field measurements, we evaluate three models:\nthe baseline COST 231 multi-wall model (MWM), the baseline model augmented with\nenvironmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered\nRSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF\nachieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP\n(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation\nreduces systematic errors by 41.22%, while Kalman filtering significantly\nenhances robustness under high RSSI volatility by 42.63%, on average across all\ndevices. These findings present an interpretable, efficient solution for\nprecise indoor LoRaWAN localization in dynamically changing environments.",
      "pdf_url": "http://arxiv.org/pdf/2505.01185v1",
      "published": "2025-05-02T11:00:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01185v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "TSTMotion: Training-free Scene-awarenText-to-motion Generation",
      "authors": [
        "Ziyan Guo",
        "Haoxuan Qu",
        "Hossein Rahmani",
        "Dewen Soh",
        "Ping Hu",
        "Qiuhong Ke",
        "Jun Liu"
      ],
      "abstract": "Text-to-motion generation has recently garnered significant research\ninterest, primarily focusing on generating human motion sequences in blank\nbackgrounds. However, human motions commonly occur within diverse 3D scenes,\nwhich has prompted exploration into scene-aware text-to-motion generation\nmethods. Yet, existing scene-aware methods often rely on large-scale\nground-truth motion sequences in diverse 3D scenes, which poses practical\nchallenges due to the expensive cost. To mitigate this challenge, we are the\nfirst to propose a \\textbf{T}raining-free \\textbf{S}cene-aware\n\\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that\nefficiently empowers pre-trained blank-background motion generators with the\nscene-aware capability. Specifically, conditioned on the given 3D scene and\ntext description, we adopt foundation models together to reason, predict and\nvalidate a scene-aware motion guidance. Then, the motion guidance is\nincorporated into the blank-background motion generators with two\nmodifications, resulting in scene-aware text-driven motion sequences. Extensive\nexperiments demonstrate the efficacy and generalizability of our proposed\nframework. We release our code in \\href{https://tstmotion.github.io/}{Project\nPage}.",
      "pdf_url": "http://arxiv.org/pdf/2505.01182v1",
      "published": "2025-05-02T10:50:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01182v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms",
      "authors": [
        "Mehrdad Asadi",
        "Roxana RƒÉdulescu",
        "Ann Now√©"
      ],
      "abstract": "Swarming systems, such as for example multi-drone networks, excel at\ncooperative tasks like monitoring, surveillance, or disaster assistance in\ncritical environments, where autonomous agents make decentralized decisions in\norder to fulfill team-level objectives in a robust and efficient manner.\nUnfortunately, team-level coordinated strategies in the wild are vulnerable to\ndata poisoning attacks, resulting in either inaccurate coordination or\nadversarial behavior among the agents. To address this challenge, we contribute\na framework that investigates the effects of such data poisoning attacks, using\nexplainable AI methods. We model the interaction among agents using\nevolutionary intelligence, where an optimal coalition strategically emerges to\nperform coordinated tasks. Then, through a rigorous evaluation, the swarm model\nis systematically poisoned using data manipulation attacks. We showcase the\napplicability of explainable AI methods to quantify the effects of poisoning on\nthe team strategy and extract footprint characterizations that enable\ndiagnosing. Our findings indicate that when the model is poisoned above 10%,\nnon-optimal strategies resulting in inefficient cooperation can be identified.",
      "pdf_url": "http://arxiv.org/pdf/2505.01181v1",
      "published": "2025-05-02T10:48:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01181v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures",
      "authors": [
        "Francisco Aguilera-Mart√≠nez",
        "Fernando Berzal"
      ],
      "abstract": "As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges.",
      "pdf_url": "http://arxiv.org/pdf/2505.01177v1",
      "published": "2025-05-02T10:35:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01177v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ]
    },
    {
      "title": "Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities",
      "authors": [
        "Pramook Khungurn",
        "Pratch Piyawongwisal",
        "Sira Sriswadi",
        "Supasorn Suwajanakorn"
      ],
      "abstract": "A flow matching model learns a time-dependent vector field $v_t(x)$ that\ngenerates a probability path $\\{ p_t \\}_{0 \\leq t \\leq 1}$ that interpolates\nbetween a well-known noise distribution ($p_0$) and the data distribution\n($p_1$). It can be distilled into a \\emph{two-timed flow model} (TTFM)\n$\\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an\ninitial time $s$ to another belonging to the distribution at a terminal time\n$t$ in one function evaluation. We present a new loss function for TTFM\ndistillation called the \\emph{initial/terminal velocity matching} (ITVM) loss\nthat extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi\net al. by adding redundant terms to match the initial velocities at time $s$,\nremoving the derivative from the terminal velocity term at time $t$, and using\na version of the model under training, stabilized by exponential moving\naveraging (EMA), to compute the target terminal average velocity. Preliminary\nexperiments show that our loss leads to better few-step generation performance\non multiple types of datasets and model architectures over baselines.",
      "pdf_url": "http://arxiv.org/pdf/2505.01169v1",
      "published": "2025-05-02T10:17:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01169v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability",
      "authors": [
        "Zhaoyang Ma",
        "Zhihao Wu",
        "Wang Lu",
        "Xin Gao",
        "Jinghang Yue",
        "Taolin Zhang",
        "Lipo Wang",
        "Youfang Lin",
        "Jing Wang"
      ],
      "abstract": "The development of model ensemble attacks has significantly improved the\ntransferability of adversarial examples, but this progress also poses severe\nthreats to the security of deep neural networks. Existing methods, however,\nface two critical challenges: insufficient capture of shared gradient\ndirections across models and a lack of adaptive weight allocation mechanisms.\nTo address these issues, we propose a novel method Harmonized Ensemble for\nAdversarial Transferability (HEAT), which introduces domain generalization into\nadversarial example generation for the first time. HEAT consists of two key\nmodules: Consensus Gradient Direction Synthesizer, which uses Singular Value\nDecomposition to synthesize shared gradient directions; and Dual-Harmony Weight\nOrchestrator which dynamically balances intra-domain coherence, stabilizing\ngradients within individual models, and inter-domain diversity, enhancing\ntransferability across models. Experimental results demonstrate that HEAT\nsignificantly outperforms existing methods across various datasets and\nsettings, offering a new perspective and direction for adversarial attack\nresearch.",
      "pdf_url": "http://arxiv.org/pdf/2505.01168v1",
      "published": "2025-05-02T10:17:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01168v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "On the Limitations of Steering in Language Model Alignment",
      "authors": [
        "Chebrolu Niranjan",
        "Kokil Jaidka",
        "Gerard Christopher Yeo"
      ],
      "abstract": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models.",
      "pdf_url": "http://arxiv.org/pdf/2505.01162v1",
      "published": "2025-05-02T10:08:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01162v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Risk Analysis and Design Against Adversarial Actions",
      "authors": [
        "Marco C. Campi",
        "Algo Car√®",
        "Luis G. Crespo",
        "Simone Garatti",
        "Federico A. Ramponi"
      ],
      "abstract": "Learning models capable of providing reliable predictions in the face of\nadversarial actions has become a central focus of the machine learning\ncommunity in recent years. This challenge arises from observing that data\nencountered at deployment time often deviate from the conditions under which\nthe model was trained. In this paper, we address deployment-time adversarial\nactions and propose a versatile, well-principled framework to evaluate the\nmodel's robustness against attacks of diverse types and intensities. While we\ninitially focus on Support Vector Regression (SVR), the proposed approach\nextends naturally to the broad domain of learning via relaxed optimization\ntechniques. Our results enable an assessment of the model vulnerability without\nrequiring additional test data and operate in a distribution-free setup. These\nresults not only provide a tool to enhance trust in the model's applicability\nbut also aid in selecting among competing alternatives. Later in the paper, we\nshow that our findings also offer useful insights for establishing new results\nwithin the out-of-distribution framework.",
      "pdf_url": "http://arxiv.org/pdf/2505.01130v1",
      "published": "2025-05-02T09:16:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01130v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ]
    },
    {
      "title": "Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study",
      "authors": [
        "Ali Mammadov",
        "Loic Le Folgoc",
        "Julien Adam",
        "Anne Buronfosse",
        "Gilles Hayem",
        "Guillaume Hocquet",
        "Pietro Gori"
      ],
      "abstract": "Multiple Instance Learning (MIL) has emerged as the best solution for Whole\nSlide Image (WSI) classification. It consists of dividing each slide into\npatches, which are treated as a bag of instances labeled with a global label.\nMIL includes two main approaches: instance-based and embedding-based. In the\nformer, each patch is classified independently, and then the patch scores are\naggregated to predict the bag label. In the latter, bag classification is\nperformed after aggregating patch embeddings. Even if instance-based methods\nare naturally more interpretable, embedding-based MILs have usually been\npreferred in the past due to their robustness to poor feature extractors.\nHowever, recently, the quality of feature embeddings has drastically increased\nusing self-supervised learning (SSL). Nevertheless, many authors continue to\nendorse the superiority of embedding-based MIL. To investigate this further, we\nconduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6\nself-supervised methods with 4 backbones, 4 foundation models, and various\npathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL\nmethods never used before in the pathology domain. Through these extensive\nexperiments, we show that with a good SSL feature extractor, simple\ninstance-based MILs, with very few parameters, obtain similar or better\nperformance than complex, state-of-the-art (SOTA) embedding-based MIL methods,\nsetting new SOTA results on the BRACS and Camelyon16 datasets. Since simple\ninstance-based MIL methods are naturally more interpretable and explainable to\nclinicians, our results suggest that more effort should be put into\nwell-adapted SSL methods for WSI rather than into complex embedding-based MIL\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2505.01109v1",
      "published": "2025-05-02T08:43:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01109v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-Objective Reinforcement Learning for Water Management",
      "authors": [
        "Zuzanna Osika",
        "Roxana Radelescu",
        "Jazmin Zatarain Salazar",
        "Frans Oliehoek",
        "Pradeep K. Murukannaiah"
      ],
      "abstract": "Many real-world problems (e.g., resource management, autonomous driving, drug\ndiscovery) require optimizing multiple, conflicting objectives. Multi-objective\nreinforcement learning (MORL) extends classic reinforcement learning to handle\nmultiple objectives simultaneously, yielding a set of policies that capture\nvarious trade-offs. However, the MORL field lacks complex, realistic\nenvironments and benchmarks. We introduce a water resource (Nile river basin)\nmanagement case study and model it as a MORL environment. We then benchmark\nexisting MORL algorithms on this task. Our results show that specialized water\nmanagement methods outperform state-of-the-art MORL approaches, underscoring\nthe scalability challenges MORL algorithms face in real-world scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2505.01094v1",
      "published": "2025-05-02T08:14:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01094v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation",
      "authors": [
        "Daniele Molino",
        "Francesco di Feola",
        "Linlin Shen",
        "Paolo Soda",
        "Valerio Guarrasi"
      ],
      "abstract": "Generative models have revolutionized Artificial Intelligence (AI),\nparticularly in multimodal applications. However, adapting these models to the\nmedical domain poses unique challenges due to the complexity of medical data\nand the stringent need for clinical accuracy. In this work, we introduce a\nframework specifically designed for multimodal medical data generation. By\nenabling the generation of multi-view chest X-rays and their associated\nclinical report, it bridges the gap between general-purpose vision-language\nmodels and the specialized requirements of healthcare. Leveraging the MIMIC-CXR\ndataset, the proposed framework shows superior performance in generating\nhigh-fidelity images and semantically coherent reports. Our quantitative\nevaluation reveals significant results in terms of FID and BLEU scores,\nshowcasing the quality of the generated data. Notably, our framework achieves\ncomparable or even superior performance compared to real data on downstream\ndisease classification tasks, underlining its potential as a tool for medical\nresearch and diagnostics. This study highlights the importance of\ndomain-specific adaptations in enhancing the relevance and utility of\ngenerative models for clinical applications, paving the way for future\nadvancements in synthetic multimodal medical data generation.",
      "pdf_url": "http://arxiv.org/pdf/2505.01091v1",
      "published": "2025-05-02T08:07:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01091v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Artificial Intelligence in Government: Why People Feel They Lose Control",
      "authors": [
        "Alexander Wuttke",
        "Adrian Rauchfleisch",
        "Andreas Jungherr"
      ],
      "abstract": "The use of Artificial Intelligence (AI) in public administration is expanding\nrapidly, moving from automating routine tasks to deploying generative and\nagentic systems that autonomously act on goals. While AI promises greater\nefficiency and responsiveness, its integration into government functions raises\nconcerns about fairness, transparency, and accountability. This article applies\nprincipal-agent theory (PAT) to conceptualize AI adoption as a special case of\ndelegation, highlighting three core tensions: assessability (can decisions be\nunderstood?), dependency (can the delegation be reversed?), and contestability\n(can decisions be challenged?). These structural challenges may lead to a\n\"failure-by-success\" dynamic, where early functional gains obscure long-term\nrisks to democratic legitimacy. To test this framework, we conducted a\npre-registered factorial survey experiment across tax, welfare, and law\nenforcement domains. Our findings show that although efficiency gains initially\nbolster trust, they simultaneously reduce citizens' perceived control. When the\nstructural risks come to the foreground, institutional trust and perceived\ncontrol both drop sharply, suggesting that hidden costs of AI adoption\nsignificantly shape public attitudes. The study demonstrates that PAT offers a\npowerful lens for understanding the institutional and political implications of\nAI in government, emphasizing the need for policymakers to address delegation\nrisks transparently to maintain public trust.",
      "pdf_url": "http://arxiv.org/pdf/2505.01085v1",
      "published": "2025-05-02T07:46:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01085v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC Benchmark",
      "authors": [
        "S√©bastien Ferr√©"
      ],
      "abstract": "Artificial Intelligence (AI) has achieved remarkable success in specialized\ntasks but struggles with efficient skill acquisition and generalization. The\nAbstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based\non minimal training requirements. While Large Language Models (LLMs) have\nrecently improved ARC performance, they rely on extensive pre-training and high\ncomputational costs. We introduce MADIL (MDL-based AI), a novel approach\nleveraging the Minimum Description Length (MDL) principle for efficient\ninductive learning. MADIL performs pattern-based decomposition, enabling\nstructured generalization. While its performance (7% at ArcPrize 2024) remains\nbelow LLM-based methods, it offers greater efficiency and interpretability.\nThis paper details MADIL's methodology, its application to ARC, and\nexperimental evaluations.",
      "pdf_url": "http://arxiv.org/pdf/2505.01081v1",
      "published": "2025-05-02T07:39:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01081v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Retrieval Augmented Learning: A Retrial-based Large Language Model Self-Supervised Learning and Autonomous Knowledge Generation",
      "authors": [
        "Zongyuan Li",
        "Pengfei Li",
        "Runnan Qi",
        "Yanan Ni",
        "Lumin Jiang",
        "Hui Wu",
        "Xuebo Zhang",
        "Kuihua Huang",
        "Xian Guo"
      ],
      "abstract": "The lack of domain-specific data in the pre-training of Large Language Models\n(LLMs) severely limits LLM-based decision systems in specialized applications,\nwhile post-training a model in the scenarios requires significant computational\nresources. In this paper, we present Retrial-Augmented Learning (RAL), a\nreward-free self-supervised learning framework for LLMs that operates without\nmodel training. By developing Retrieval-Augmented Generation (RAG) into a\nmodule for organizing intermediate data, we realized a three-stage autonomous\nknowledge generation of proposing a hypothesis, validating the hypothesis, and\ngenerating the knowledge. The method is evaluated in the LLM-PySC2 environment,\na representative decision-making platform that combines sufficient complexity\nwith domain-specific knowledge requirements. Experiments demonstrate that the\nproposed method effectively reduces hallucination by generating and utilizing\nvalidated knowledge, and increases decision-making performance at an extremely\nlow cost. Meanwhile, the approach exhibits potential in\nout-of-distribution(OOD) tasks, robustness, and transferability, making it a\ncost-friendly but effective solution for decision-making problems and\nautonomous knowledge generation.",
      "pdf_url": "http://arxiv.org/pdf/2505.01073v1",
      "published": "2025-05-02T07:25:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01073v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits",
      "authors": [
        "Edvin Fasth",
        "Sagar Singh"
      ],
      "abstract": "Knowledge distillation (KD) has become a powerful tool for training compact\nstudent models using larger, pretrained teacher models, often requiring less\ndata and computational resources. Teacher models typically possess more layers\nand thus exhibit richer feature representations compared to their student\ncounterparts. Furthermore, student models tend to learn simpler, surface-level\nfeatures in their early layers. This discrepancy can increase errors in groups\nwhere labels spuriously correlate with specific input attributes, leading to a\ndecline in group fairness even when overall accuracy remains comparable to the\nteacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs),\nwhich enable predictions at multiple intermediate layers, have been employed.\nConfidence margins derived from these early exits have been utilized to\nreweight both cross-entropy and distillation losses on a per-instance basis. In\nthis paper, we propose that leveraging Laplace approximation-based methods to\nobtain well-calibrated uncertainty estimates can also effectively reweight\nchallenging instances and improve group fairness. We hypothesize that Laplace\napproximation offers a more robust identification of difficult or ambiguous\ninstances compared to margin-based approaches. To validate our claims, we\nbenchmark our approach using a Bert-based model on the MultiNLI dataset.",
      "pdf_url": "http://arxiv.org/pdf/2505.01070v1",
      "published": "2025-05-02T07:18:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01070v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs",
      "authors": [
        "Yijie Jin",
        "Junjie Peng",
        "Xuanchao Lin",
        "Haochen Yuan",
        "Lan Wang",
        "Cangzhi Zheng"
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) is a rapidly developing field that\nintegrates multimodal information to recognize sentiments, and existing models\nhave made significant progress in this area. The central challenge in MSA is\nmultimodal fusion, which is predominantly addressed by Multimodal Transformers\n(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.\nIn this work, from the perspective of efficiency optimization, we propose and\nprove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and\nwe introduce the graph-structured representation pattern of MulTs. Based on\nthis pattern, we propose an Interlaced Mask (IM) mechanism to design the\nGraph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is\nformally equivalent to MulTs which achieves an efficient weight-sharing\nmechanism without information disorder through IM, enabling All-Modal-In-One\nfusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called\nDecomposition is implemented to ensure avoiding additional computational\noverhead. Moreover, it achieves significantly higher performance than\ntraditional MulTs. To further validate the effectiveness of GsiT itself and the\nHMHG concept, we integrate them into multiple state-of-the-art models and\ndemonstrate notable performance improvements and parameter reduction on widely\nused MSA datasets.",
      "pdf_url": "http://arxiv.org/pdf/2505.01068v1",
      "published": "2025-05-02T07:18:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01068v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories",
      "authors": [
        "Ziqi Ding",
        "Qian Fu",
        "Junchen Ding",
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have spurred the\ndevelopment of diverse AI applications from code generation and video editing\nto text generation; however, AI supply chains such as Hugging Face, which host\npretrained models and their associated configuration files contributed by the\npublic, face significant security challenges; in particular, configuration\nfiles originally intended to set up models by specifying parameters and initial\nsettings can be exploited to execute unauthorized code, yet research has\nlargely overlooked their security compared to that of the models themselves; in\nthis work, we present the first comprehensive study of malicious configurations\non Hugging Face, identifying three attack scenarios (file, website, and\nrepository operations) that expose inherent risks; to address these threats, we\nintroduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in\nthe context of their associated runtime code and critical libraries,\neffectively detecting suspicious elements with low false positive rates and\nhigh accuracy; our extensive evaluation uncovers thousands of suspicious\nrepositories and configuration files, underscoring the urgent need for enhanced\nsecurity validation in AI model hosting platforms.",
      "pdf_url": "http://arxiv.org/pdf/2505.01067v1",
      "published": "2025-05-02T07:16:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01067v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation",
      "authors": [
        "David Jin",
        "Qian Fu",
        "Yuekang Li"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, raising concerns about their potential for automated\nexploit generation (AEG). This paper presents the first systematic study on\nLLMs' effectiveness in AEG, evaluating both their cooperativeness and technical\nproficiency. To mitigate dataset bias, we introduce a benchmark with refactored\nversions of five software security labs. Additionally, we design an LLM-based\nattacker to systematically prompt LLMs for exploit generation. Our experiments\nreveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to\nuncensored models, while Llama3 is the most resistant. However, no model\nsuccessfully generates exploits for refactored labs, though GPT-4o's minimal\nerrors highlight the potential for LLM-driven AEG advancements.",
      "pdf_url": "http://arxiv.org/pdf/2505.01065v1",
      "published": "2025-05-02T07:15:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01065v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Model Tensor Planning",
      "authors": [
        "An T. Le",
        "Khai Nguyen",
        "Minh Nhat Vu",
        "Jo√£o Carvalho",
        "Jan Peters"
      ],
      "abstract": "Sampling-based model predictive control (MPC) offers strong performance in\nnonlinear and contact-rich robotic tasks, yet often suffers from poor\nexploration due to locally greedy sampling schemes. We propose \\emph{Model\nTensor Planning} (MTP), a novel sampling-based MPC framework that introduces\nhigh-entropy control trajectory generation through structured tensor sampling.\nBy sampling over randomized multipartite graphs and interpolating control\ntrajectories with B-splines and Akima splines, MTP ensures smooth and globally\ndiverse control candidates. We further propose a simple $\\beta$-mixing strategy\nthat blends local exploitative and global exploratory samples within the\nmodified Cross-Entropy Method (CEM) update, balancing control refinement and\nexploration. Theoretically, we show that MTP achieves asymptotic path coverage\nand maximum entropy in the control trajectory space in the limit of infinite\ntensor depth and width.\n  Our implementation is fully vectorized using JAX and compatible with MuJoCo\nXLA, supporting \\emph{Just-in-time} (JIT) compilation and batched rollouts for\nreal-time control with online domain randomization. Through experiments on\nvarious challenging robotic tasks, ranging from dexterous in-hand manipulation\nto humanoid locomotion, we demonstrate that MTP outperforms standard MPC and\nevolutionary strategy baselines in task success and control robustness. Design\nand sensitivity ablations confirm the effectiveness of MTP tensor sampling\nstructure, spline interpolation choices, and mixing strategy. Altogether, MTP\noffers a scalable framework for robust exploration in model-based planning and\ncontrol.",
      "pdf_url": "http://arxiv.org/pdf/2505.01059v1",
      "published": "2025-05-02T07:09:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01059v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities",
      "authors": [
        "Zhiwei Hao",
        "Jianyuan Guo",
        "Li Shen",
        "Yong Luo",
        "Han Hu",
        "Guoxia Wang",
        "Dianhai Yu",
        "Yonggang Wen",
        "Dacheng Tao"
      ],
      "abstract": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several components$\\unicode{x2013}$such\nas weights, activations, and gradients$\\unicode{x2013}$each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
      "pdf_url": "http://arxiv.org/pdf/2505.01043v1",
      "published": "2025-05-02T06:33:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01043v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Stagnation in Evolutionary Algorithms: Convergence $\\neq$ Optimality",
      "authors": [
        "Xiaojun Zhou"
      ],
      "abstract": "In the evolutionary computation community, it is widely believed that\nstagnation impedes convergence in evolutionary algorithms, and that convergence\ninherently indicates optimality. However, this perspective is misleading. In\nthis study, it is the first to highlight that the stagnation of an individual\ncan actually facilitate the convergence of the entire population, and\nconvergence does not necessarily imply optimality, not even local optimality.\nConvergence alone is insufficient to ensure the effectiveness of evolutionary\nalgorithms. Several counterexamples are provided to illustrate this argument.",
      "pdf_url": "http://arxiv.org/pdf/2505.01036v1",
      "published": "2025-05-02T06:19:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01036v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory",
      "authors": [
        "Huy Q. Ngo",
        "Mingyu Guo",
        "Hung Nguyen"
      ],
      "abstract": "Security vulnerabilities in Windows Active Directory (AD) systems are\ntypically modeled using an attack graph and hardening AD systems involves an\niterative workflow: security teams propose an edge to remove, and IT operations\nteams manually review these fixes before implementing the removal. As\nverification requires significant manual effort, we formulate an Adaptive Path\nRemoval Problem to minimize the number of steps in this iterative removal\nprocess. In our model, a wizard proposes an attack path in each step and\npresents it as a set of multiple-choice options to the IT admin. The IT admin\nthen selects one edge from the proposed set to remove. This process continues\nuntil the target $t$ is disconnected from source $s$ or the number of proposed\npaths reaches $B$. The model aims to optimize the human effort by minimizing\nthe expected number of interactions between the IT admin and the security\nwizard. We first prove that the problem is $\\mathcal{\\#P}$-hard. We then\npropose a set of solutions including an exact algorithm, an approximate\nalgorithm, and several scalable heuristics. Our best heuristic, called DPR, can\noperate effectively on larger-scale graphs compared to the exact algorithm and\nconsistently outperforms the approximate algorithm across all graphs. We verify\nthe effectiveness of our algorithms on several synthetic AD graphs and an AD\nattack graph collected from a real organization.",
      "pdf_url": "http://arxiv.org/pdf/2505.01028v1",
      "published": "2025-05-02T05:55:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01028v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance",
      "authors": [
        "Vishal Gandhi",
        "Sagar Gandhi"
      ],
      "abstract": "The success of large pre-trained object detectors hinges on their\nadaptability to diverse downstream tasks. While fine-tuning is the standard\nadaptation method, specializing these models for challenging fine-grained\ndomains necessitates careful consideration of feature granularity. The critical\nquestion remains: how deeply should the pre-trained backbone be fine-tuned to\noptimize for the specialized task without incurring catastrophic forgetting of\nthe original general capabilities? Addressing this, we present a systematic\nempirical study evaluating the impact of fine-tuning depth. We adapt a standard\nYOLOv8n model to a custom, fine-grained fruit detection dataset by\nprogressively unfreezing backbone layers (freeze points at layers 22, 15, and\n10) and training. Performance was rigorously evaluated on both the target fruit\ndataset and, using a dual-head evaluation architecture, on the original COCO\nvalidation set. Our results demonstrate unequivocally that deeper fine-tuning\n(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\\%\nabsolute mAP50) on the fine-grained fruit task compared to only training the\nhead. Strikingly, this significant adaptation and specialization resulted in\nnegligible performance degradation (<0.1\\% absolute mAP difference) on the COCO\nbenchmark across all tested freeze levels. We conclude that adapting\nmid-to-late backbone features is highly effective for fine-grained\nspecialization. Critically, our results demonstrate this adaptation can be\nachieved without the commonly expected penalty of catastrophic forgetting,\npresenting a compelling case for exploring deeper fine-tuning strategies,\nparticularly when targeting complex domains or when maximizing specialized\nperformance is paramount.",
      "pdf_url": "http://arxiv.org/pdf/2505.01016v1",
      "published": "2025-05-02T05:27:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01016v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark",
      "authors": [
        "Jongwook Han",
        "Dongmin Choi",
        "Woojung Song",
        "Eun-Ju Lee",
        "Yohan Jo"
      ],
      "abstract": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data.",
      "pdf_url": "http://arxiv.org/pdf/2505.01015v1",
      "published": "2025-05-02T05:26:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01015v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ]
    },
    {
      "title": "Improving Large Language Model Planning with Action Sequence Similarity",
      "authors": [
        "Xinran Zhao",
        "Hanie Sedghi",
        "Bernd Bohnet",
        "Dale Schuurmans",
        "Azade Nova"
      ],
      "abstract": "Planning is essential for artificial intelligence systems to look ahead and\nproactively determine a course of actions to reach objectives in the virtual\nand real world. Recent work on large language models (LLMs) sheds light on\ntheir planning capability in various tasks. However, it remains unclear what\nsignals in the context influence the model performance. In this work, we\nexplore how to improve the model planning capability through in-context\nlearning (ICL), specifically, what signals can help select the exemplars.\nThrough extensive experiments, we observe that commonly used problem similarity\nmay result in false positives with drastically different plans, which can\nmislead the model. In response, we propose to sample and filter exemplars\nleveraging plan side action sequence similarity (AS). We propose GRASE-DC: a\ntwo-stage pipeline that first re-samples high AS exemplars and then curates the\nselected exemplars with dynamic clustering on AS to achieve a balance of\nrelevance and diversity. Our experimental result confirms that GRASE-DC\nachieves significant performance improvement on various planning tasks (up to\n~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on\naverage). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a\nvalidator, we are able to even boost the performance by 18.9% more.\n  Extensive analysis validates the consistent performance improvement of\nGRASE-DC with various backbone LLMs and on both classical planning and natural\nlanguage planning benchmarks. GRASE-DC can further boost the planning accuracy\nby ~24 absolute points on harder problems using simpler problems as exemplars\nover a random baseline. This demonstrates its ability to generalize to\nout-of-distribution problems.",
      "pdf_url": "http://arxiv.org/pdf/2505.01009v1",
      "published": "2025-05-02T05:16:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01009v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Towards the Resistance of Neural Network Watermarking to Fine-tuning",
      "authors": [
        "Ling Tang",
        "Yuefeng Chen",
        "Hui Xue",
        "Quanshi Zhang"
      ],
      "abstract": "This paper proves a new watermarking method to embed the ownership\ninformation into a deep neural network (DNN), which is robust to fine-tuning.\nSpecifically, we prove that when the input feature of a convolutional layer\nonly contains low-frequency components, specific frequency components of the\nconvolutional filter will not be changed by gradient descent during the\nfine-tuning process, where we propose a revised Fourier transform to extract\nfrequency components from the convolutional filter. Additionally, we also prove\nthat these frequency components are equivariant to weight scaling and weight\npermutations. In this way, we design a watermark module to encode the watermark\ninformation to specific frequency components in a convolutional filter.\nPreliminary experiments demonstrate the effectiveness of our method.",
      "pdf_url": "http://arxiv.org/pdf/2505.01007v1",
      "published": "2025-05-02T05:11:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.01007v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "Toward Data-centric Directed Graph Learning: An Entropy-driven Approach",
      "authors": [
        "Xunkai Li",
        "Zhengyu Wu",
        "Kaichi Yu",
        "Hongchao Qin",
        "Guang Zeng",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "abstract": "The directed graph (digraph), as a generalization of undirected graphs,\nexhibits superior representation capability in modeling complex topology\nsystems and has garnered considerable attention in recent years. Despite the\nnotable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage\ndirected edges, they still fail to comprehensively delve into the abundant data\nknowledge concealed in the digraphs. This data-level limitation results in\nmodel-level sub-optimal predictive performance and underscores the necessity of\nfurther exploring the potential correlations between the directed edges\n(topology) and node profiles (feature and labels) from a data-centric\nperspective, thereby empowering model-centric neural networks with stronger\nencoding capabilities.\n  In this paper, we propose \\textbf{E}ntropy-driven \\textbf{D}igraph\nknowl\\textbf{E}dge distillatio\\textbf{N} (EDEN), which can serve as a\ndata-centric digraph learning paradigm or a model-agnostic hot-and-plug\ndata-centric Knowledge Distillation (KD) module. The core idea is to achieve\ndata-centric ML, guided by our proposed hierarchical encoding theory for\nstructured data. Specifically, EDEN first utilizes directed structural\nmeasurements from a topology perspective to construct a coarse-grained\nHierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual\ninformation of node profiles to refine knowledge flow in the HKT, enabling\ndata-centric KD supervision within model training. As a general framework, EDEN\ncan also naturally extend to undirected scenarios and demonstrate satisfactory\nperformance. In our experiments, EDEN has been widely evaluated on 14 (di)graph\ndatasets (homophily and heterophily) and across 4 downstream tasks. The results\ndemonstrate that EDEN attains SOTA performance and exhibits strong improvement\nfor prevalent (Di)GNNs.",
      "pdf_url": "http://arxiv.org/pdf/2505.00983v1",
      "published": "2025-05-02T04:06:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.00983v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.SI"
      ]
    },
    {
      "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models",
      "authors": [
        "Xuhui Jiang",
        "Shengjie Ma",
        "Chengjin Xu",
        "Cehao Yang",
        "Liyu Zhang",
        "Jian Guo"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability.",
      "pdf_url": "http://arxiv.org/pdf/2505.00979v1",
      "published": "2025-05-02T03:40:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.00979v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Attack and defense techniques in large language models: A survey and new perspectives",
      "authors": [
        "Zhiyu Liao",
        "Kang Chen",
        "Yuanguo Lin",
        "Kangkang Li",
        "Yunxuan Liu",
        "Hefeng Chen",
        "Xingwang Huang",
        "Yuanhui Yu"
      ],
      "abstract": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications.",
      "pdf_url": "http://arxiv.org/pdf/2505.00976v1",
      "published": "2025-05-02T03:37:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.00976v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    }
  ]
}