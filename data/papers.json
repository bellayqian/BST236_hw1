{
  "last_updated": "2025-03-14T00:45:21.000379",
  "papers": [
    {
      "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation",
      "authors": [
        "Ruohao Guo",
        "Wei Xu",
        "Alan Ritter"
      ],
      "abstract": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world user interactions. We curated ECHOMIST, the\nfirst comprehensive benchmark for implicit misinformation, where the\nmisinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based\non rigorous selection criteria and carefully curated data from diverse sources,\nincluding real-world human-AI conversations and social media interactions. We\nalso introduce a new evaluation metric to measure whether LLMs can recognize\nand counter false information rather than amplify users' misconceptions.\nThrough an extensive empirical study on a wide range of LLMs, including GPT-4,\nClaude, and Llama, we find that current models perform alarmingly poorly on\nthis task, often failing to detect false premises and generating misleading\nexplanations. Our findings underscore the critical need for an increased focus\non implicit misinformation in LLM safety research.",
      "pdf_url": "http://arxiv.org/pdf/2503.09598v1",
      "published": "2025-03-12T17:59:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09598v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
      "authors": [
        "Andrew Crossman",
        "Andrew R. Plummer",
        "Chandra Sekharudu",
        "Deepak Warrier",
        "Mohammad Yekrangian"
      ],
      "abstract": "We present Auspex - a threat modeling system built using a specialized\ncollection of generative artificial intelligence-based methods that capture\nthreat modeling tradecraft. This new approach, called tradecraft prompting,\ncenters on encoding the on-the-ground knowledge of threat modelers within the\nprompts that drive a generative AI-based threat modeling system. Auspex employs\ntradecraft prompts in two processing stages. The first stage centers on\ningesting and processing system architecture information using prompts that\nencode threat modeling tradecraft knowledge pertaining to system decomposition\nand description. The second stage centers on chaining the resulting system\nanalysis through a collection of prompts that encode tradecraft knowledge on\nthreat identification, classification, and mitigation. The two-stage process\nyields a threat matrix for a system that specifies threat scenarios, threat\ntypes, information security categorizations and potential mitigations. Auspex\nproduces formalized threat model output in minutes, relative to the weeks or\nmonths a manual process takes. More broadly, the focus on bespoke tradecraft\nprompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a\nlightweight, flexible, modular, and extensible foundational system capable of\naddressing the complexity, resource, and standardization limitations of both\nexisting manual and automated threat modeling processes. In this connection, we\nestablish the baseline value of Auspex to threat modelers through an evaluation\nprocedure based on feedback collected from cybersecurity subject matter experts\nmeasuring the quality and utility of threat models generated by Auspex on real\nbanking systems. We conclude with a discussion of system performance and plans\nfor enhancements to Auspex.",
      "pdf_url": "http://arxiv.org/pdf/2503.09586v1",
      "published": "2025-03-12T17:54:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09586v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
      "authors": [
        "Yingfa Chen",
        "Yutong Wu",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.",
      "pdf_url": "http://arxiv.org/pdf/2503.09579v1",
      "published": "2025-03-12T17:50:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09579v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
      "authors": [
        "Marianne Arriola",
        "Aaron Gokaslan",
        "Justin T Chiu",
        "Zhihan Yang",
        "Zhixuan Qi",
        "Jiaqi Han",
        "Subham Sekhar Sahoo",
        "Volodymyr Kuleshov"
      ],
      "abstract": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
      "pdf_url": "http://arxiv.org/pdf/2503.09573v1",
      "published": "2025-03-12T17:43:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09573v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
      "authors": [
        "Qiguang Chen",
        "Libo Qin",
        "Jinhao Liu",
        "Dengyun Peng",
        "Jiannan Guan",
        "Peng Wang",
        "Mengkang Hu",
        "Yuhang Zhou",
        "Te Gao",
        "Wanxiang Che"
      ],
      "abstract": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to\nfill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and test-time scaling, offering\ninsights into how these processes manifest in practice. (4) Finally, we\nidentify significant research gaps and highlight promising future directions,\nincluding the integration of multi-modal reasoning, efficiency improvements,\nand enhanced knowledge frameworks. By providing a structured overview, this\nsurvey aims to inspire future research and further the development of logical\nreasoning in artificial intelligence.",
      "pdf_url": "http://arxiv.org/pdf/2503.09567v2",
      "published": "2025-03-12T17:35:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09567v2",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $μ$P Parametrization",
      "authors": [
        "Zixiang Chen",
        "Greg Yang",
        "Qingyue Zhao",
        "Quanquan Gu"
      ],
      "abstract": "Despite deep neural networks' powerful representation learning capabilities,\ntheoretical understanding of how networks can simultaneously achieve meaningful\nfeature learning and global convergence remains elusive. Existing approaches\nlike the neural tangent kernel (NTK) are limited because features stay close to\ntheir initialization in this parametrization, leaving open questions about\nfeature properties during substantial evolution. In this paper, we investigate\nthe training dynamics of infinitely wide, $L$-layer neural networks using the\ntensor program (TP) framework. Specifically, we show that, when trained with\nstochastic gradient descent (SGD) under the Maximal Update parametrization\n($\\mu$P) and mild conditions on the activation function, SGD enables these\nnetworks to learn linearly independent features that substantially deviate from\ntheir initial values. This rich feature space captures relevant data\ninformation and ensures that any convergent point of the training process is a\nglobal minimum. Our analysis leverages both the interactions among features\nacross layers and the properties of Gaussian random variables, providing new\ninsights into deep representation learning. We further validate our theoretical\nfindings through experiments on real-world datasets.",
      "pdf_url": "http://arxiv.org/pdf/2503.09565v1",
      "published": "2025-03-12T17:33:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09565v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "title": "The Value of Goal Commitment in Planning",
      "authors": [
        "Alberto Pozanco",
        "Marianela Morales",
        "Daniel Borrajo",
        "Manuela Veloso"
      ],
      "abstract": "In this paper, we revisit the concept of goal commitment from early planners\nin the presence of current forward chaining heuristic planners. We present a\ncompilation that extends the original planning task with commit actions that\nenforce the persistence of specific goals once achieved, thereby committing to\nthem in the search sub-tree. This approach imposes a specific goal achievement\norder in parts of the search tree, potentially introducing dead-end states.\nThis can reduce search effort if the goal achievement order is correct.\nOtherwise, the search algorithm can expand nodes in the open list where goals\ndo not persist. Experimental results demonstrate that the reformulated tasks\nsuit state-of-the-art agile planners, enabling them to find better",
      "pdf_url": "http://arxiv.org/pdf/2503.09545v1",
      "published": "2025-03-12T17:00:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09545v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Differentially Private Equilibrium Finding in Polymatrix Games",
      "authors": [
        "Mingyang Liu",
        "Gabriele Farina",
        "Asuman Ozdaglar"
      ],
      "abstract": "We study equilibrium finding in polymatrix games under differential privacy\nconstraints. To start, we show that high accuracy and asymptotically vanishing\ndifferential privacy budget (as the number of players goes to infinity) cannot\nbe achieved simultaneously under either of the two settings: (i) We seek to\nestablish equilibrium approximation guarantees in terms of Euclidean distance\nto the equilibrium set, and (ii) the adversary has access to all communication\nchannels. Then, assuming the adversary has access to a constant number of\ncommunication channels, we develop a novel distributed algorithm that recovers\nstrategies with simultaneously vanishing Nash gap (in expected utility, also\nreferred to as exploitability and privacy budget as the number of players\nincreases.",
      "pdf_url": "http://arxiv.org/pdf/2503.09538v1",
      "published": "2025-03-12T16:54:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09538v1",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals",
      "authors": [
        "Shuokang Huang",
        "Julie A. McCann"
      ],
      "abstract": "Human pose estimation (HPE) detects the positions of human body joints for\nvarious applications. Compared to using cameras, HPE using radio frequency (RF)\nsignals is non-intrusive and more robust to adverse conditions, exploiting the\nsignal variations caused by human interference. However, existing studies focus\non single-domain HPE confined by domain-specific confounders, which cannot\ngeneralize to new domains and result in diminished HPE performance.\nSpecifically, the signal variations caused by different human body parts are\nentangled, containing subject-specific confounders. RF signals are also\nintertwined with environmental noise, involving environment-specific\nconfounders. In this paper, we propose GenHPE, a 3D HPE approach that generates\ncounterfactual RF signals to eliminate domain-specific confounders. GenHPE\ntrains generative models conditioned on human skeleton labels, learning how\nhuman body parts and confounders interfere with RF signals. We manipulate\nskeleton labels (i.e., removing body parts) as counterfactual conditions for\ngenerative models to synthesize counterfactual RF signals. The differences\nbetween counterfactual signals approximately eliminate domain-specific\nconfounders and regularize an encoder-decoder model to learn domain-independent\nrepresentations. Such representations help GenHPE generalize to new\nsubjects/environments for cross-domain 3D HPE. We evaluate GenHPE on three\npublic datasets from WiFi, ultra-wideband, and millimeter wave. Experimental\nresults show that GenHPE outperforms state-of-the-art methods and reduces\nestimation errors by up to 52.2mm for cross-subject HPE and 10.6mm for\ncross-environment HPE.",
      "pdf_url": "http://arxiv.org/pdf/2503.09537v1",
      "published": "2025-03-12T16:53:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09537v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "eess.SP"
      ]
    },
    {
      "title": "Evaluating Visual Explanations of Attention Maps for Transformer-based Medical Imaging",
      "authors": [
        "Minjae Chung",
        "Jong Bum Won",
        "Ganghyun Kim",
        "Yujin Kim",
        "Utku Ozbulak"
      ],
      "abstract": "Although Vision Transformers (ViTs) have recently demonstrated superior\nperformance in medical imaging problems, they face explainability issues\nsimilar to previous architectures such as convolutional neural networks. Recent\nresearch efforts suggest that attention maps, which are part of decision-making\nprocess of ViTs can potentially address the explainability issue by identifying\nregions influencing predictions, especially in models pretrained with\nself-supervised learning. In this work, we compare the visual explanations of\nattention maps to other commonly used methods for medical imaging problems. To\ndo so, we employ four distinct medical imaging datasets that involve the\nidentification of (1) colonic polyps, (2) breast tumors, (3) esophageal\ninflammation, and (4) bone fractures and hardware implants. Through large-scale\nexperiments on the aforementioned datasets using various supervised and\nself-supervised pretrained ViTs, we find that although attention maps show\npromise under certain conditions and generally surpass GradCAM in\nexplainability, they are outperformed by transformer-specific interpretability\nmethods. Our findings indicate that the efficacy of attention maps as a method\nof interpretability is context-dependent and may be limited as they do not\nconsistently provide the comprehensive insights required for robust medical\ndecision-making.",
      "pdf_url": "http://arxiv.org/pdf/2503.09535v1",
      "published": "2025-03-12T16:52:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09535v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games",
      "authors": [
        "Peng Chen",
        "Pi Bu",
        "Yingyao Wang",
        "Xinyi Wang",
        "Ziming Wang",
        "Jie Guo",
        "Yingxiu Zhao",
        "Qi Zhu",
        "Jun Song",
        "Siran Yang",
        "Jiamang Wang",
        "Bo Zheng"
      ],
      "abstract": "Recent advances in Vision-Language-Action models (VLAs) have expanded the\ncapabilities of embodied intelligence. However, significant challenges remain\nin real-time decision-making in complex 3D environments, which demand\nsecond-level responses, high-resolution perception, and tactical reasoning\nunder dynamic conditions. To advance the field, we introduce CombatVLA, an\nefficient VLA model optimized for combat tasks in 3D action role-playing\ngames(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action\npairs collected by an action tracker, where the data is formatted as\naction-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates\ninto an action execution framework, allowing efficient inference through our\ntruncated AoT strategy. Experimental results demonstrate that CombatVLA not\nonly outperforms all existing models on the combat understanding benchmark but\nalso achieves a 50-fold acceleration in game combat. Moreover, it has a higher\ntask success rate than human players. We will open-source all resources,\nincluding the action tracker, dataset, benchmark, model weights, training code,\nand the implementation of the framework at https://combatvla.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2503.09527v1",
      "published": "2025-03-12T16:42:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09527v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "PairVDN - Pair-wise Decomposed Value Functions",
      "authors": [
        "Zak Buzzard"
      ],
      "abstract": "Extending deep Q-learning to cooperative multi-agent settings is challenging\ndue to the exponential growth of the joint action space, the non-stationary\nenvironment, and the credit assignment problem. Value decomposition allows deep\nQ-learning to be applied at the joint agent level, at the cost of reduced\nexpressivity. Building on past work in this direction, our paper proposes\nPairVDN, a novel method for decomposing the value function into a collection of\npair-wise, rather than per-agent, functions, improving expressivity at the cost\nof requiring a more complex (but still efficient) dynamic programming\nmaximisation algorithm. Our method enables the representation of value\nfunctions which cannot be expressed as a monotonic combination of per-agent\nfunctions, unlike past approaches such as VDN and QMIX. We implement a novel\nmany-agent cooperative environment, Box Jump, and demonstrate improved\nperformance over these baselines in this setting. We open-source our code and\nenvironment at https://github.com/zzbuzzard/PairVDN.",
      "pdf_url": "http://arxiv.org/pdf/2503.09521v1",
      "published": "2025-03-12T16:38:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09521v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
      "authors": [
        "Bowen Jin",
        "Hansi Zeng",
        "Zhenrui Yue",
        "Dong Wang",
        "Hamed Zamani",
        "Jiawei Han"
      ],
      "abstract": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Retrieval augmentation and tool-use training approaches where a search\nengine is treated as a tool lack complex multi-turn retrieval flexibility or\nrequire large-scale supervised data. Prompting advanced LLMs with reasoning\ncapabilities during inference to use search engines is not optimal, since the\nLLM does not learn how to optimally interact with the search engine. This paper\nintroduces Search-R1, an extension of the DeepSeek-R1 model where the LLM\nlearns -- solely through reinforcement learning (RL) -- to autonomously\ngenerate (multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM rollouts with multi-turn search\ninteractions, leveraging retrieved token masking for stable RL training and a\nsimple outcome-based reward function. Experiments on seven question-answering\ndatasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21%\n(Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further\nprovides empirical insights into RL optimization methods, LLM choices, and\nresponse length dynamics in retrieval-augmented reasoning. The code and model\ncheckpoints are available at https://github.com/PeterGriffinJin/Search-R1.",
      "pdf_url": "http://arxiv.org/pdf/2503.09516v1",
      "published": "2025-03-12T16:26:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09516v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
      "authors": [
        "Md Morshed Alam",
        "Lokesh Chandra Das",
        "Sandip Roy",
        "Sachin Shetty",
        "Weichao Wang"
      ],
      "abstract": "Internet of Things (IoT) platforms with trigger-action capability allow event\nconditions to trigger actions in IoT devices autonomously by creating a chain\nof interactions. Adversaries exploit this chain of interactions to maliciously\ninject fake event conditions into IoT hubs, triggering unauthorized actions on\ntarget IoT devices to implement remote injection attacks. Existing defense\nmechanisms focus mainly on the verification of event transactions using\nphysical event fingerprints to enforce the security policies to block unsafe\nevent transactions. These approaches are designed to provide offline defense\nagainst injection attacks. The state-of-the-art online defense mechanisms offer\nreal-time defense, but extensive reliability on the inference of attack impacts\non the IoT network limits the generalization capability of these approaches. In\nthis paper, we propose a platform-independent multi-agent online defense\nsystem, namely RESTRAIN, to counter remote injection attacks at runtime.\nRESTRAIN allows the defense agent to profile attack actions at runtime and\nleverages reinforcement learning to optimize a defense policy that complies\nwith the security requirements of the IoT network. The experimental results\nshow that the defense agent effectively takes real-time defense actions against\ncomplex and dynamic remote injection attacks and maximizes the security gain\nwith minimal computational overhead.",
      "pdf_url": "http://arxiv.org/pdf/2503.09513v1",
      "published": "2025-03-12T16:23:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09513v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework",
      "authors": [
        "Bakary Badjie",
        "José Cecílio",
        "António Casimiro"
      ],
      "abstract": "The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL).\nHowever, its complex architecture and advantages over dense models in image\nclassification remain unclear. In previous studies, MoE performance has often\nbeen affected by noise and outliers in the input space. Some approaches\nincorporate input clustering for training MoE models, but most clustering\nalgorithms lack access to labeled data, limiting their effectiveness. This\npaper introduces the Double-stage Feature-level Clustering and\nPseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists\nof input feature extraction, feature-level clustering, and a computationally\nefficient pseudo-labeling strategy. This approach reduces the impact of noise\nand outliers while leveraging a small subset of labeled data to label a large\nportion of unlabeled inputs. We propose a conditional end-to-end joint training\nmethod that improves expert specialization by training the MoE model on\nwell-labeled, clustered inputs. Unlike traditional MoE and dense models, the\nDFCP-MoE framework effectively captures input space diversity, leading to\ncompetitive inference results. We validate our approach on three benchmark\ndatasets for multi-class classification tasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.09504v1",
      "published": "2025-03-12T16:13:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09504v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.LO"
      ]
    },
    {
      "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning",
      "authors": [
        "Ziyu Wan",
        "Yunxiang Li",
        "Yan Song",
        "Hanjing Wang",
        "Linyi Yang",
        "Mark Schmidt",
        "Jun Wang",
        "Weinan Zhang",
        "Shuyue Hu",
        "Ying Wen"
      ],
      "abstract": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2503.09501v1",
      "published": "2025-03-12T16:05:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09501v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging Questions",
      "authors": [
        "Zhe Xu",
        "Daoyuan Chen",
        "Zhenqing Ling",
        "Yaliang Li",
        "Ying Shen"
      ],
      "abstract": "Large vision-language models (VLMs) face challenges in achieving robust,\ntransferable reasoning abilities due to reliance on labor-intensive manual\ninstruction datasets or computationally expensive self-supervised methods. To\naddress these issues, we introduce MindGYM, a framework that enhances VLMs\nthrough synthetic self-challenging questions, consisting of three stages: (1)\nSeed Single-Hop Question Synthesis, generating cognitive questions across\ntextual (e.g., logical deduction) and multimodal contexts (e.g., diagram-based\nqueries) spanning eight semantic areas like ethical analysis; (2) Challenging\nMulti-Hop Question Synthesis, combining seed questions via diverse principles\nlike bridging, visual-textual alignment, to create multi-step problems\ndemanding deeper reasoning; and (3) Thinking-Induced Curriculum Fine-Tuning, a\nstructured pipeline that progressively trains the model from scaffolded\nreasoning to standalone inference. By leveraging the model's self-synthesis\ncapability, MindGYM achieves high data efficiency (e.g., +16% gains on\nMathVision-Mini with only 400 samples), computational efficiency (reducing both\ntraining and inference costs), and robust generalization across tasks.\nExtensive evaluations on seven benchmarks demonstrate superior performance over\nstrong baselines, with notable improvements (+15.77% win rates) in reasoning\ndepth and breadth validated via GPT-based scoring. MindGYM underscores the\nviability of self-challenging for refining VLM capabilities while minimizing\nhuman intervention and resource demands. Code and data are released to advance\nmultimodal reasoning research.",
      "pdf_url": "http://arxiv.org/pdf/2503.09499v1",
      "published": "2025-03-12T16:03:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09499v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Online Language Splatting",
      "authors": [
        "Saimouli Katragadda",
        "Cho-Ying Wu",
        "Yuliang Guo",
        "Xinyu Huang",
        "Guoquan Huang",
        "Liu Ren"
      ],
      "abstract": "To enable AI agents to interact seamlessly with both humans and 3D\nenvironments, they must not only perceive the 3D world accurately but also\nalign human language with 3D spatial representations. While prior work has made\nsignificant progress by integrating language features into geometrically\ndetailed 3D scene representations using 3D Gaussian Splatting (GS), these\napproaches rely on computationally intensive offline preprocessing of language\nfeatures for each input image, limiting adaptability to new environments. In\nthis work, we introduce Online Language Splatting, the first framework to\nachieve online, near real-time, open-vocabulary language mapping within a\n3DGS-SLAM system without requiring pre-generated language features. The key\nchallenge lies in efficiently fusing high-dimensional language features into 3D\nrepresentations while balancing the computation speed, memory usage, rendering\nquality and open-vocabulary capability. To this end, we innovatively design:\n(1) a high-resolution CLIP embedding module capable of generating detailed\nlanguage feature maps in 18ms per frame, (2) a two-stage online auto-encoder\nthat compresses 768-dimensional CLIP features to 15 dimensions while preserving\nopen-vocabulary capabilities, and (3) a color-language disentangled\noptimization approach to improve rendering quality. Experimental results show\nthat our online method not only surpasses the state-of-the-art offline methods\nin accuracy but also achieves more than 40x efficiency boost, demonstrating the\npotential for dynamic and interactive AI applications.",
      "pdf_url": "http://arxiv.org/pdf/2503.09447v1",
      "published": "2025-03-12T14:49:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09447v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in Text-to-Image Diffusion Models",
      "authors": [
        "Zhihua Tian",
        "Sirun Nan",
        "Ming Xu",
        "Shengfang Zhai",
        "Wenjie Qu",
        "Jian Liu",
        "Kui Ren",
        "Ruoxi Jia",
        "Jiaheng Zhang"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have achieved remarkable progress in\ngenerating high-quality images but also raise people's concerns about\ngenerating harmful or misleading content. While extensive approaches have been\nproposed to erase unwanted concepts without requiring retraining from scratch,\nthey inadvertently degrade performance on normal generation tasks. In this\nwork, we propose Interpret then Deactivate (ItD), a novel framework to enable\nprecise concept removal in T2I diffusion models while preserving overall\nperformance. ItD first employs a sparse autoencoder (SAE) to interpret each\nconcept as a combination of multiple features. By permanently deactivating the\nspecific features associated with target concepts, we repurpose SAE as a\nzero-shot classifier that identifies whether the input prompt includes target\nconcepts, allowing selective concept erasure in diffusion models. Moreover, we\ndemonstrate that ItD can be easily extended to erase multiple concepts without\nrequiring further training. Comprehensive experiments across celebrity\nidentities, artistic styles, and explicit content demonstrate ItD's\neffectiveness in eliminating targeted concepts without interfering with normal\nconcept generation. Additionally, ItD is also robust against adversarial\nprompts designed to circumvent content filters. Code is available at:\nhttps://github.com/NANSirun/Interpret-then-deactivate.",
      "pdf_url": "http://arxiv.org/pdf/2503.09446v1",
      "published": "2025-03-12T14:46:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09446v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
      "authors": [
        "Xiaoda Yang",
        "JunYu Lu",
        "Hongshun Qiu",
        "Sijing Li",
        "Hao Li",
        "Shengpeng Ji",
        "Xudong Tang",
        "Jiayang Xu",
        "Jiaqi Duan",
        "Ziyue Jiang",
        "Cong Lin",
        "Sihang Cai",
        "Zejian Xie",
        "Zhuoyang Song",
        "Songxin Zhang"
      ],
      "abstract": "Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures\nhave emerged as a pivotal paradigm in multimodal understanding, offering a\npowerful framework for integrating visual and linguistic information. However,\nthe increasing complexity and diversity of tasks present significant challenges\nin coordinating load balancing across heterogeneous visual experts, where\noptimizing one specialist's performance often compromises others' capabilities.\nTo address task heterogeneity and expert load imbalance, we propose Astrea, a\nnovel multi-expert collaborative VLM architecture based on progressive\npre-alignment. Astrea introduces three key innovations: 1) A heterogeneous\nexpert coordination mechanism that integrates four specialized models\n(detection, segmentation, classification, captioning) into a comprehensive\nexpert matrix covering essential visual comprehension elements; 2) A dynamic\nknowledge fusion strategy featuring progressive pre-alignment to harmonize\nexperts within the VLM latent space through contrastive learning, complemented\nby probabilistically activated stochastic residual connections to preserve\nknowledge continuity; 3) An enhanced optimization framework utilizing momentum\ncontrastive learning for long-range dependency modeling and adaptive weight\nallocators for real-time expert contribution calibration. Extensive evaluations\nacross 12 benchmark tasks spanning VQA, image captioning, and cross-modal\nretrieval demonstrate Astrea's superiority over state-of-the-art models,\nachieving an average performance gain of +4.7\\%. This study provides the first\nempirical demonstration that progressive pre-alignment strategies enable VLMs\nto overcome task heterogeneity limitations, establishing new methodological\nfoundations for developing general-purpose multimodal agents.",
      "pdf_url": "http://arxiv.org/pdf/2503.09445v1",
      "published": "2025-03-12T14:44:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09445v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "PromptMap: An Alternative Interaction Style for AI-Based Image Generation",
      "authors": [
        "Krzysztof Adamkiewicz",
        "Paweł W. Woźniak",
        "Julia Dominiak",
        "Andrzej Romanowski",
        "Jakob Karolus",
        "Stanislav Frolov"
      ],
      "abstract": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output.",
      "pdf_url": "http://arxiv.org/pdf/2503.09436v1",
      "published": "2025-03-12T14:31:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09436v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
      "authors": [
        "Richard A. Dubniczky",
        "Krisztofer Zoltán Horvát",
        "Tamás Bisztray",
        "Mohamed Amine Ferrag",
        "Lucas C. Cordeiro",
        "Norbert Tihanyi"
      ],
      "abstract": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark.",
      "pdf_url": "http://arxiv.org/pdf/2503.09433v1",
      "published": "2025-03-12T14:30:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09433v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Multimodal Language Modeling for High-Accuracy Single Cell Transcriptomics Analysis and Generation",
      "authors": [
        "Yaorui Shi",
        "Jiaqi Yang",
        "Sihang Li",
        "Junfeng Fang",
        "Xiang Wang",
        "Zhiyuan Liu",
        "Yang Zhang"
      ],
      "abstract": "Pre-trained language models (PLMs) have revolutionized scientific research,\nyet their application to single-cell analysis remains limited. Text PLMs cannot\nprocess single-cell RNA sequencing data, while cell PLMs lack the ability to\nhandle free text, restricting their use in multimodal tasks. Existing efforts\nto bridge these modalities often suffer from information loss or inadequate\nsingle-modal pre-training, leading to suboptimal performances. To address these\nchallenges, we propose Single-Cell MultiModal Generative Pre-trained\nTransformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT\neffectively integrates the state-of-the-art cell and text PLMs, facilitating\ncross-modal knowledge sharing for improved performance. To bridge the text-cell\nmodality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes\nextensive pre-training on 27 million cells -- the largest dataset for\nmultimodal cell-text PLMs to date. This large-scale pre-training enables\nscMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative\nimprovement of textual discrepancy for cell description generation, 20.5\\%\nhigher accuracy for cell type annotation, and 4\\% improvement in $k$-NN\naccuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
      "pdf_url": "http://arxiv.org/pdf/2503.09427v1",
      "published": "2025-03-12T14:26:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09427v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation",
      "authors": [
        "Claudius Kienle",
        "Benjamin Alt",
        "Finn Schneider",
        "Tobias Pertlwieser",
        "Rainer Jäkel",
        "Rania Rayyes"
      ],
      "abstract": "Despite the widespread adoption of industrial robots in automotive assembly,\nwire harness installation remains a largely manual process, as it requires\nprecise and flexible manipulation. To address this challenge, we design a novel\nAI-based framework that automates cable connector mating by integrating force\ncontrol with deep visuotactile learning. Our system optimizes\nsearch-and-insertion strategies using first-order optimization over a\nmultimodal transformer architecture trained on visual, tactile, and\nproprioceptive data. Additionally, we design a novel automated data collection\nand optimization pipeline that minimizes the need for machine learning\nexpertise. The framework optimizes robot programs that run natively on standard\nindustrial controllers, permitting human experts to audit and certify them.\nExperimental validations on a center console assembly task demonstrate\nsignificant improvements in cycle times and robustness compared to conventional\nrobot programming approaches. Videos are available under\nhttps://claudius-kienle.github.io/AppMuTT.",
      "pdf_url": "http://arxiv.org/pdf/2503.09409v1",
      "published": "2025-03-12T13:59:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09409v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "68T40",
        "I.2; J.2"
      ]
    },
    {
      "title": "Multi-Agent Image Restoration",
      "authors": [
        "Xu Jiang",
        "Gehui Li",
        "Bin Chen",
        "Jian Zhang"
      ],
      "abstract": "Image restoration (IR) is challenging due to the complexity of real-world\ndegradations. While many specialized and all-in-one IR models have been\ndeveloped, they fail to effectively handle complex, mixed degradations. Recent\nagentic methods RestoreAgent and AgenticIR leverage intelligent, autonomous\nworkflows to alleviate this issue, yet they suffer from suboptimal results and\ninefficiency due to their resource-intensive finetunings, and ineffective\nsearches and tool execution trials for satisfactory outputs. In this paper, we\npropose MAIR, a novel Multi-Agent approach for complex IR problems. We\nintroduce a real-world degradation prior, categorizing degradations into three\ntypes: (1) scene, (2) imaging, and (3) compression, which are observed to occur\nsequentially in real world, and reverse them in the opposite order. Built upon\nthis three-stage restoration framework, MAIR emulates a team of collaborative\nhuman specialists, including a \"scheduler\" for overall planning and multiple\n\"experts\" dedicated to specific degradations. This design minimizes search\nspace and trial efforts, improving image quality while reducing inference\ncosts. In addition, a registry mechanism is introduced to enable easy\nintegration of new tools. Experiments on both synthetic and real-world datasets\nshow that proposed MAIR achieves competitive performance and improved\nefficiency over the previous agentic IR system. Code and models will be made\navailable.",
      "pdf_url": "http://arxiv.org/pdf/2503.09403v1",
      "published": "2025-03-12T13:53:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09403v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation",
      "authors": [
        "Tobias Christian Nauen",
        "Brian Moser",
        "Federico Raue",
        "Stanislav Frolov",
        "Andreas Dengel"
      ],
      "abstract": "Transformers, particularly Vision Transformers (ViTs), have achieved\nstate-of-the-art performance in large-scale image classification. However, they\noften require large amounts of data and can exhibit biases that limit their\nrobustness and generalizability. This paper introduces ForAug, a novel data\naugmentation scheme that addresses these challenges and explicitly includes\ninductive biases, which commonly are part of the neural network architecture,\ninto the training data. ForAug is constructed by using pretrained foundation\nmodels to separate and recombine foreground objects with different backgrounds,\nenabling fine-grained control over image composition during training. It thus\nincreases the data diversity and effective number of training samples. We\ndemonstrate that training on ForNet, the application of ForAug to ImageNet,\nsignificantly improves the accuracy of ViTs and other architectures by up to\n4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks.\nImportantly, ForAug enables novel ways of analyzing model behavior and\nquantifying biases. Namely, we introduce metrics for background robustness,\nforeground focus, center bias, and size bias and show that training on ForNet\nsubstantially reduces these biases compared to training on ImageNet. In\nsummary, ForAug provides a valuable tool for analyzing and mitigating biases,\nenabling the development of more robust and reliable computer vision models.\nOur code and dataset are publicly available at https://github.com/tobna/ForAug.",
      "pdf_url": "http://arxiv.org/pdf/2503.09399v1",
      "published": "2025-03-12T13:49:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09399v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T45",
        "I.2.10; I.2.6; I.4.6"
      ]
    },
    {
      "title": "Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training",
      "authors": [
        "Jiatong Xia",
        "Lingqiao Liu"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in\nsynthesizing novel views after training on a given set of viewpoints. However,\nits rendering quality deteriorates when the synthesized view deviates\nsignificantly from the training views. This decline occurs due to (1) the\nmodel's difficulty in generalizing to out-of-distribution scenarios and (2)\nchallenges in interpolating fine details caused by substantial resolution\nchanges and occlusions. A notable case of this limitation is close-up view\ngeneration--producing views that are significantly closer to the object than\nthose in the training set. To tackle this issue, we propose a novel approach\nfor close-up view generation based by progressively training the 3DGS model\nwith self-generated data. Our solution is based on three key ideas. First, we\nleverage the See3D model, a recently introduced 3D-aware generative model, to\nenhance the details of rendered views. Second, we propose a strategy to\nprogressively expand the ``trust regions'' of the 3DGS model and update a set\nof reference views for See3D. Finally, we introduce a fine-tuning strategy to\ncarefully update the 3DGS model with training data generated from the above\nschemes. We further define metrics for close-up views evaluation to facilitate\nbetter research on this problem. By conducting evaluations on specifically\nselected scenarios for close-up views, our proposed approach demonstrates a\nclear advantage over competitive solutions.",
      "pdf_url": "http://arxiv.org/pdf/2503.09396v1",
      "published": "2025-03-12T13:44:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09396v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Next-Generation Recommender Systems: A Benchmark for Personalized Recommendation Assistant with LLMs",
      "authors": [
        "Jiani Huang",
        "Shijie Wang",
        "Liang-bo Ning",
        "Wenqi Fan",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Qing Li"
      ],
      "abstract": "Recommender systems (RecSys) are widely used across various modern digital\nplatforms and have garnered significant attention. Traditional recommender\nsystems usually focus only on fixed and simple recommendation scenarios, making\nit difficult to generalize to new and unseen recommendation tasks in an\ninteractive paradigm. Recently, the advancement of large language models (LLMs)\nhas revolutionized the foundational architecture of RecSys, driving their\nevolution into more intelligent and interactive personalized recommendation\nassistants. However, most existing studies rely on fixed task-specific prompt\ntemplates to generate recommendations and evaluate the performance of\npersonalized assistants, which limits the comprehensive assessments of their\ncapabilities. This is because commonly used datasets lack high-quality textual\nuser queries that reflect real-world recommendation scenarios, making them\nunsuitable for evaluating LLM-based personalized recommendation assistants. To\naddress this gap, we introduce RecBench+, a new dataset benchmark designed to\naccess LLMs' ability to handle intricate user recommendation needs in the era\nof LLMs. RecBench+ encompasses a diverse set of queries that span both hard\nconditions and soft preferences, with varying difficulty levels. We evaluated\ncommonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs\ndemonstrate preliminary abilities to act as recommendation assistants, 2) LLMs\nare better at handling queries with explicitly stated conditions, while facing\nchallenges with queries that require reasoning or contain misleading\ninformation. Our dataset has been released at\nhttps://github.com/jiani-huang/RecBench.git.",
      "pdf_url": "http://arxiv.org/pdf/2503.09382v1",
      "published": "2025-03-12T13:28:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09382v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Pig behavior dataset and Spatial-temporal perception and enhancement networks based on the attention mechanism for pig behavior recognition",
      "authors": [
        "Fangzheng Qi",
        "Zhenjie Hou",
        "En Lin",
        "Xing Li",
        "iuzhen Liang",
        "Xinwen Zhou"
      ],
      "abstract": "The recognition of pig behavior plays a crucial role in smart farming and\nwelfare assurance for pigs. Currently, in the field of pig behavior\nrecognition, the lack of publicly available behavioral datasets not only limits\nthe development of innovative algorithms but also hampers model robustness and\nalgorithm optimization.This paper proposes a dataset containing 13 pig\nbehaviors that significantly impact welfare.Based on this dataset, this paper\nproposes a spatial-temporal perception and enhancement networks based on the\nattention mechanism to model the spatiotemporal features of pig behaviors and\ntheir associated interaction areas in video data. The network is composed of a\nspatiotemporal perception network and a spatiotemporal feature enhancement\nnetwork. The spatiotemporal perception network is responsible for establishing\nconnections between the pigs and the key regions of their behaviors in the\nvideo data. The spatiotemporal feature enhancement network further strengthens\nthe important spatial features of individual pigs and captures the long-term\ndependencies of the spatiotemporal features of individual behaviors by\nremodeling these connections, thereby enhancing the model's perception of\nspatiotemporal changes in pig behaviors. Experimental results demonstrate that\non the dataset established in this paper, our proposed model achieves a MAP\nscore of 75.92%, which is an 8.17% improvement over the best-performing\ntraditional model. This study not only improces the accuracy and\ngeneralizability of individual pig behavior recognition but also provides new\ntechnological tools for modern smart farming. The dataset and related code will\nbe made publicly available alongside this paper.",
      "pdf_url": "http://arxiv.org/pdf/2503.09378v1",
      "published": "2025-03-12T13:27:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09378v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Revisiting Medical Image Retrieval via Knowledge Consolidation",
      "authors": [
        "Yang Nan",
        "Huichi Zhou",
        "Xiaodan Xing",
        "Giorgos Papanastasiou",
        "Lei Zhu",
        "Zhifan Gao",
        "Alejandro F Fangi",
        "Guang Yang"
      ],
      "abstract": "As artificial intelligence and digital medicine increasingly permeate\nhealthcare systems, robust governance frameworks are essential to ensure\nethical, secure, and effective implementation. In this context, medical image\nretrieval becomes a critical component of clinical data management, playing a\nvital role in decision-making and safeguarding patient information. Existing\nmethods usually learn hash functions using bottleneck features, which fail to\nproduce representative hash codes from blended embeddings. Although contrastive\nhashing has shown superior performance, current approaches often treat image\nretrieval as a classification task, using category labels to create\npositive/negative pairs. Moreover, many methods fail to address the\nout-of-distribution (OOD) issue when models encounter external OOD queries or\nadversarial attacks. In this work, we propose a novel method to consolidate\nknowledge of hierarchical features and optimisation functions. We formulate the\nknowledge consolidation by introducing Depth-aware Representation Fusion (DaRF)\nand Structure-aware Contrastive Hashing (SCH). DaRF adaptively integrates\nshallow and deep representations into blended features, and SCH incorporates\nimage fingerprints to enhance the adaptability of positive/negative pairings.\nThese blended features further facilitate OOD detection and content-based\nrecommendation, contributing to a secure AI-driven healthcare environment.\nMoreover, we present a content-guided ranking to improve the robustness and\nreproducibility of retrieval results. Our comprehensive assessments demonstrate\nthat the proposed method could effectively recognise OOD samples and\nsignificantly outperform existing approaches in medical image retrieval\n(p<0.05). In particular, our method achieves a 5.6-38.9% improvement in mean\nAverage Precision on the anatomical radiology dataset.",
      "pdf_url": "http://arxiv.org/pdf/2503.09370v1",
      "published": "2025-03-12T13:16:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09370v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity",
      "authors": [
        "Daniel Jiménez-López",
        "Nuria Rodríguez-Barroso",
        "M. Victoria Luzón",
        "Francisco Herrera"
      ],
      "abstract": "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information.",
      "pdf_url": "http://arxiv.org/pdf/2503.09365v1",
      "published": "2025-03-12T13:09:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09365v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports",
      "authors": [
        "Jiushen Cai",
        "Weihang Zhang",
        "Hanruo Liu",
        "Ningli Wang",
        "Huiqi Li"
      ],
      "abstract": "Standardization of clinical reports is crucial for improving the quality of\nhealthcare and facilitating data integration. The lack of unified standards,\nincluding format, terminology, and style, is a great challenge in clinical\nfundus diagnostic reports, which increases the difficulty for large language\nmodels (LLMs) to understand the data. To address this, we construct a bilingual\nstandard terminology, containing fundus clinical terms and commonly used\ndescriptions in clinical diagnosis. Then, we establish two models,\nRetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented\ndataset simulating clinical scenarios, demonstrates powerful standardization\nbehaviors. However, it encounters a challenge of limitation to cover a wider\nrange of diseases. To further enhance standardization performance, we build\nRetSTA-7B, which integrates a substantial amount of standardized data generated\nby RetSTA-7B-Zero along with corresponding English data, covering diverse\ncomplex clinical scenarios and achieving report-level standardization for the\nfirst time. Experimental results demonstrate that RetSTA-7B outperforms other\ncompared LLMs in bilingual standardization task, which validates its superior\nperformance and generalizability. The checkpoints are available at\nhttps://github.com/AB-Story/RetSTA-7B.",
      "pdf_url": "http://arxiv.org/pdf/2503.09358v1",
      "published": "2025-03-12T13:00:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09358v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach",
      "authors": [
        "Ruifeng She",
        "Bowen Pang",
        "Kai Li",
        "Zehua Liu",
        "Tao Zhong"
      ],
      "abstract": "As the artificial intelligence community advances into the era of large\nmodels with billions of parameters, distributed training and inference have\nbecome essential. While various parallelism strategies-data, model, sequence,\nand pipeline-have been successfully implemented for popular neural networks on\nmain-stream hardware, optimizing the distributed deployment schedule requires\nextensive expertise and manual effort. Further more, while existing frameworks\nwith most simple chain-like structures, they struggle with complex non-linear\narchitectures. Mixture-of-experts and multi-modal models feature intricate MIMO\nand branch-rich topologies that require fine-grained operator-level\nparallelization beyond the capabilities of existing frameworks. We propose\nformulating parallelism planning as a scheduling optimization problem using\nmixed-integer programming. We propose a bi-level solution framework balancing\noptimality with computational efficiency, automatically generating effective\ndistributed plans that capture both the heterogeneous structure of modern\nneural networks and the underlying hardware constraints. In experiments\ncomparing against expert-designed strategies like DeepSeek's DualPipe, our\nframework achieves comparable or superior performance, reducing computational\nbubbles by half under the same memory constraints. The framework's versatility\nextends beyond throughput optimization to incorporate hardware utilization\nmaximization, memory capacity constraints, and other considerations or\npotential strategies. Such capabilities position our solution as both a\nvaluable research tool for exploring optimal parallelization strategies and a\npractical industrial solution for large-scale AI deployment.",
      "pdf_url": "http://arxiv.org/pdf/2503.09357v1",
      "published": "2025-03-12T13:00:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09357v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.DM"
      ]
    },
    {
      "title": "MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding",
      "authors": [
        "Zhoutong Ye",
        "Mingze Sun",
        "Huan-ang Gao",
        "Chun Yu",
        "Yuanchun Shi"
      ],
      "abstract": "Large multimodal models (LMMs) have demonstrated significant potential as\ngeneralists in vision-language (VL) tasks. However, there remains a significant\ngap between state-of-the-art LMMs and human performance when it comes to\ncomplex tasks that require a combination of fundamental VL capabilities, as\nwell as tasks involving the grounding of complex instructions. To thoroughly\ninvestigate the human-LMM gap and its underlying causes, we propose MOAT, a\ndiverse benchmark with complex real-world VL tasks that are challenging for\nLMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist\nproblem solving by integrating fundamental VL capabilities such as reading\ntext, counting, understanding spatial relations, grounding textual and visual\ninstructions, etc. All these abilities fit into a taxonomy proposed by us that\ncontains 10 fundamental VL capabilities, enabling MOAT to provide a\nfine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first\nbenchmark to explicitly evaluate LMMs' ability to ground complex text and\nvisual instructions, which is essential to many real-world applications. We\nevaluate over 20 proprietary and open source LMMs, as well as humans, on MOAT,\nand found that humans achieved 82.7% accuracy while the best performing LMM\n(OpenAI o1) achieved only 38.8%. To guide future model development, we analyze\ncommon trends in our results and discuss the underlying causes of observed\nperformance gaps between LMMs and humans, focusing on which VL capability forms\nthe bottleneck in complex tasks, whether test time scaling improves performance\non MOAT, and how tiling harms LMMs' capability to count. Code and data are\navailable at https://cambrian-yzt.github.io/MOAT.",
      "pdf_url": "http://arxiv.org/pdf/2503.09348v1",
      "published": "2025-03-12T12:49:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09348v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
      "authors": [
        "Hongyu Chen",
        "Seraphina Goldfarb-Tarrant"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
      "pdf_url": "http://arxiv.org/pdf/2503.09347v1",
      "published": "2025-03-12T12:49:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09347v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model",
      "authors": [
        "Yuzhi Lai",
        "Shenghai Yuan",
        "Youssef Nassar",
        "Mingyu Fan",
        "Thomas Weber",
        "Matthias Rätsch"
      ],
      "abstract": "Effective Human-Robot Interaction (HRI) is crucial for future service robots\nin aging societies. Existing solutions are biased toward only well-trained\nobjects, creating a gap when dealing with new objects. Currently, HRI systems\nusing predefined gestures or language tokens for pretrained objects pose\nchallenges for all individuals, especially elderly ones. These challenges\ninclude difficulties in recalling commands, memorizing hand gestures, and\nlearning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI\nparadigm that combines voice commands and deictic posture. NVP-HRI utilizes the\nSegment Anything Model (SAM) to analyze visual cues and depth data, enabling\nprecise structural object representation. Through a pre-trained SAM network,\nNVP-HRI allows interaction with new objects via zero-shot prediction, even\nwithout prior knowledge. NVP-HRI also integrates with a large language model\n(LLM) for multimodal commands, coordinating them with object selection and\nscene distribution in real time for collision-free trajectory solutions. We\nalso regulate the action sequence with the essential control syntax to reduce\nLLM hallucination risks. The evaluation of diverse real-world tasks using a\nUniversal Robot showcased up to 59.2\\% efficiency improvement over traditional\ngesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our\ncode and design will be openly available at\nhttps://github.com/laiyuzhi/NVP-HRI.git.",
      "pdf_url": "http://arxiv.org/pdf/2503.09335v1",
      "published": "2025-03-12T12:30:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09335v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data",
      "authors": [
        "Adel ElZemity",
        "Budi Arief",
        "Shujun Li"
      ],
      "abstract": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance.",
      "pdf_url": "http://arxiv.org/pdf/2503.09334v1",
      "published": "2025-03-12T12:29:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09334v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D Scene Reconstruction",
      "authors": [
        "Dai Sun",
        "Huhao Guan",
        "Kun Zhang",
        "Xike Xie",
        "S. Kevin Zhou"
      ],
      "abstract": "Dynamic and static components in scenes often exhibit distinct properties,\nyet most 4D reconstruction methods treat them indiscriminately, leading to\nsuboptimal performance in both cases. This work introduces SDD-4DGS, the first\nframework for static-dynamic decoupled 4D scene reconstruction based on\nGaussian Splatting. Our approach is built upon a novel probabilistic dynamic\nperception coefficient that is naturally integrated into the Gaussian\nreconstruction pipeline, enabling adaptive separation of static and dynamic\ncomponents. With carefully designed implementation strategies to realize this\ntheoretical framework, our method effectively facilitates explicit learning of\nmotion patterns for dynamic elements while maintaining geometric stability for\nstatic structures. Extensive experiments on five benchmark datasets demonstrate\nthat SDD-4DGS consistently outperforms state-of-the-art methods in\nreconstruction fidelity, with enhanced detail restoration for static structures\nand precise modeling of dynamic motions. The code will be released.",
      "pdf_url": "http://arxiv.org/pdf/2503.09332v1",
      "published": "2025-03-12T12:25:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09332v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Group-robust Machine Unlearning",
      "authors": [
        "Thomas De Min",
        "Subhankar Roy",
        "Stéphane Lathuilière",
        "Elisa Ricci",
        "Massimiliano Mancini"
      ],
      "abstract": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning.",
      "pdf_url": "http://arxiv.org/pdf/2503.09330v1",
      "published": "2025-03-12T12:24:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09330v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models",
      "authors": [
        "Xin Li",
        "Zhuo Cai",
        "Shoujin Wang",
        "Kun Yu",
        "Fang Chen"
      ],
      "abstract": "Large language models (LLMs) have recently shown remarkable performance in\nlanguage tasks and beyond. However, due to their limited inherent causal\nreasoning ability, LLMs still face challenges in handling tasks that require\nrobust causal reasoning ability, such as health-care and economic analysis. As\na result, a growing body of research has focused on enhancing the causal\nreasoning ability of LLMs. Despite the booming research, there lacks a survey\nto well review the challenges, progress and future directions in this area. To\nbridge this significant gap, we systematically review literature on how to\nstrengthen LLMs' causal reasoning ability in this paper. We start from the\nintroduction of background and motivations of this topic, followed by the\nsummarisation of key challenges in this area. Thereafter, we propose a novel\ntaxonomy to systematically categorise existing methods, together with detailed\ncomparisons within and between classes of methods. Furthermore, we summarise\nexisting benchmarks and evaluation metrics for assessing LLMs' causal reasoning\nability. Finally, we outline future research directions for this emerging\nfield, offering insights and inspiration to researchers and practitioners in\nthe area.",
      "pdf_url": "http://arxiv.org/pdf/2503.09326v1",
      "published": "2025-03-12T12:20:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09326v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "DAVE: Diagnostic benchmark for Audio Visual Evaluation",
      "authors": [
        "Gorjan Radevski",
        "Teodora Popordanoska",
        "Matthew B. Blaschko",
        "Tinne Tuytelaars"
      ],
      "abstract": "Audio-visual understanding is a rapidly evolving field that seeks to\nintegrate and interpret information from both auditory and visual modalities.\nDespite recent advances in multi-modal learning, existing benchmarks often\nsuffer from strong visual bias -- where answers can be inferred from visual\ndata alone -- and provide only aggregate scores that conflate multiple sources\nof error. This makes it difficult to determine whether models struggle with\nvisual understanding, audio interpretation, or audio-visual alignment. In this\nwork, we introduce DAVE (Diagnostic Audio Visual Evaluation), a novel benchmark\ndataset designed to systematically evaluate audio-visual models across\ncontrolled challenges. DAVE alleviates existing limitations by (i) ensuring\nboth modalities are necessary to answer correctly and (ii) decoupling\nevaluation into atomic subcategories. Our detailed analysis of state-of-the-art\nmodels reveals specific failure modes and provides targeted insights for\nimprovement. By offering this standardized diagnostic framework, we aim to\nfacilitate more robust development of audio-visual models. The dataset is\nreleased: https://github.com/gorjanradevski/dave",
      "pdf_url": "http://arxiv.org/pdf/2503.09321v1",
      "published": "2025-03-12T12:12:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09321v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions",
      "authors": [
        "Fynn Bachmann",
        "Daan van der Weijden",
        "Lucien Heitz",
        "Cristina Sarasua",
        "Abraham Bernstein"
      ],
      "abstract": "Adaptive questionnaires dynamically select the next question for a survey\nparticipant based on their previous answers. Due to digitalisation, they have\nbecome a viable alternative to traditional surveys in application areas such as\npolitical science. One limitation, however, is their dependency on data to\ntrain the model for question selection. Often, such training data (i.e., user\ninteractions) are unavailable a priori. To address this problem, we (i) test\nwhether Large Language Models (LLM) can accurately generate such interaction\ndata and (ii) explore if these synthetic data can be used to pre-train the\nstatistical model of an adaptive political survey. To evaluate this approach,\nwe utilise existing data from the Swiss Voting Advice Application (VAA)\nSmartvote in two ways: First, we compare the distribution of LLM-generated\nsynthetic data to the real distribution to assess its similarity. Second, we\ncompare the performance of an adaptive questionnaire that is randomly\ninitialised with one pre-trained on synthetic data to assess their suitability\nfor training. We benchmark these results against an \"oracle\" questionnaire with\nperfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately\ngenerates answers to the Smartvote questionnaire from the perspective of\ndifferent Swiss parties. Furthermore, we demonstrate that initialising the\nstatistical model with synthetic data can (i) significantly reduce the error in\npredicting user responses and (ii) increase the candidate recommendation\naccuracy of the VAA. Our work emphasises the considerable potential of LLMs to\ncreate training data to improve the data collection process in adaptive\nquestionnaires in LLM-affine areas such as political surveys.",
      "pdf_url": "http://arxiv.org/pdf/2503.09311v1",
      "published": "2025-03-12T12:02:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09311v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Steering No-Regret Agents in MFGs under Model Uncertainty",
      "authors": [
        "Leo Widmer",
        "Jiawei Huang",
        "Niao He"
      ],
      "abstract": "Incentive design is a popular framework for guiding agents' learning dynamics\ntowards desired outcomes by providing additional payments beyond intrinsic\nrewards. However, most existing works focus on a finite, small set of agents or\nassume complete knowledge of the game, limiting their applicability to\nreal-world scenarios involving large populations and model uncertainty. To\naddress this gap, we study the design of steering rewards in Mean-Field Games\n(MFGs) with density-independent transitions, where both the transition dynamics\nand intrinsic reward functions are unknown. This setting presents non-trivial\nchallenges, as the mediator must incentivize the agents to explore for its\nmodel learning under uncertainty, while simultaneously steer them to converge\nto desired behaviors without incurring excessive incentive payments. Assuming\nagents exhibit no(-adaptive) regret behaviors, we contribute novel optimistic\nexploration algorithms. Theoretically, we establish sub-linear regret\nguarantees for the cumulative gaps between the agents' behaviors and the\ndesired ones. In terms of the steering cost, we demonstrate that our total\nincentive payments incur only sub-linear excess, competing with a baseline\nsteering strategy that stabilizes the target policy as an equilibrium. Our work\npresents an effective framework for steering agents behaviors in\nlarge-population systems under uncertainty.",
      "pdf_url": "http://arxiv.org/pdf/2503.09309v1",
      "published": "2025-03-12T12:02:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09309v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "stat.ML"
      ]
    },
    {
      "title": "Unmask It! AI-Generated Product Review Detection in Dravidian Languages",
      "authors": [
        "Somsubhra De",
        "Advait Vats"
      ],
      "abstract": "The rise of Generative AI has led to a surge in AI-generated reviews, often\nposing a serious threat to the credibility of online platforms. Reviews serve\nas the primary source of information about products and services. Authentic\nreviews play a vital role in consumer decision-making. The presence of\nfabricated content misleads consumers, undermines trust and facilitates\npotential fraud in digital marketplaces. This study focuses on detecting\nAI-generated product reviews in Tamil and Malayalam, two low-resource languages\nwhere research in this domain is relatively under-explored. We worked on a\nrange of approaches - from traditional machine learning methods to advanced\ntransformer-based models such as Indic-BERT, IndicSBERT, MuRIL, XLM-RoBERTa and\nMalayalamBERT. Our findings highlight the effectiveness of leveraging the\nstate-of-the-art transformers in accurately identifying AI-generated content,\ndemonstrating the potential in enhancing the detection of fake reviews in\nlow-resource language settings.",
      "pdf_url": "http://arxiv.org/pdf/2503.09289v1",
      "published": "2025-03-12T11:35:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09289v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer",
      "authors": [
        "Haoxuan Wang",
        "Jinlong Peng",
        "Qingdong He",
        "Hao Yang",
        "Ying Jin",
        "Jiafu Wu",
        "Xiaobin Hu",
        "Yanjie Pan",
        "Zhenye Gan",
        "Mingmin Chi",
        "Bo Peng",
        "Yabiao Wang"
      ],
      "abstract": "With the rapid development of diffusion models in image generation, the\ndemand for more powerful and flexible controllable frameworks is increasing.\nAlthough existing methods can guide generation beyond text prompts, the\nchallenge of effectively combining multiple conditional inputs while\nmaintaining consistency with all of them remains unsolved. To address this, we\nintroduce UniCombine, a DiT-based multi-conditional controllable generative\nframework capable of handling any combination of conditions, including but not\nlimited to text prompts, spatial maps, and subject images. Specifically, we\nintroduce a novel Conditional MMDiT Attention mechanism and incorporate a\ntrainable LoRA module to build both the training-free and training-based\nversions. Additionally, we propose a new pipeline to construct\nSubjectSpatial200K, the first dataset designed for multi-conditional generative\ntasks covering both the subject-driven and spatially-aligned conditions.\nExtensive experimental results on multi-conditional generation demonstrate the\noutstanding universality and powerful capability of our approach with\nstate-of-the-art performance.",
      "pdf_url": "http://arxiv.org/pdf/2503.09277v1",
      "published": "2025-03-12T11:22:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09277v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Single-Qudit Quantum Neural Networks for Multiclass Classification",
      "authors": [
        "Leandro C. Souza",
        "Renato Portugal"
      ],
      "abstract": "This paper proposes a single-qudit quantum neural network for multiclass\nclassification, by using the enhanced representational capacity of\nhigh-dimensional qudit states. Our design employs an $d$-dimensional unitary\noperator, where $d$ corresponds to the number of classes, constructed using the\nCayley transform of a skew-symmetric matrix, to efficiently encode and process\nclass information. This architecture enables a direct mapping between class\nlabels and quantum measurement outcomes, reducing circuit depth and\ncomputational overhead. To optimize network parameters, we introduce a hybrid\ntraining approach that combines an extended activation function -- derived from\na truncated multivariable Taylor series expansion -- with support vector\nmachine optimization for weight determination. We evaluate our model on the\nMNIST and EMNIST datasets, demonstrating competitive accuracy while maintaining\na compact single-qudit quantum circuit. Our findings highlight the potential of\nqudit-based QNNs as scalable alternatives to classical deep learning models,\nparticularly for multiclass classification. However, practical implementation\nremains constrained by current quantum hardware limitations. This research\nadvances quantum machine learning by demonstrating the feasibility of\nhigher-dimensional quantum systems for efficient learning tasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.09269v1",
      "published": "2025-03-12T11:12:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09269v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "DeepInnovation AI: A Global Dataset Mapping the AI innovation from Academic Research to Industrial Patents",
      "authors": [
        "Haixing Gong",
        "Hui Zou",
        "Xingzhou Liang",
        "Shiyuan Meng",
        "Pinlong Cai",
        "Xingcheng Xu",
        "Jingjing Qu"
      ],
      "abstract": "In the rapidly evolving field of artificial intelligence (AI), mapping\ninnovation patterns and understanding effective technology transfer from\nresearch to applications are essential for economic growth. However, existing\ndata infrastructures suffer from fragmentation, incomplete coverage, and\ninsufficient evaluative capacity. Here, we present DeepInnovationAI, a\ncomprehensive global dataset containing three structured files.\nDeepPatentAI.csv: Contains 2,356,204 patent records with 8 field-specific\nattributes. DeepDiveAI.csv: Encompasses 3,511,929 academic publications with 13\nmetadata fields. These two datasets leverage large language models,\nmultilingual text analysis and dual-layer BERT classifiers to accurately\nidentify AI-related content, while utilizing hypergraph analysis to create\nrobust innovation metrics. Additionally, DeepCosineAI.csv: By applying semantic\nvector proximity analysis, this file presents approximately one hundred million\ncalculated paper-patent similarity pairs to enhance understanding of how\ntheoretical advancements translate into commercial technologies.\nDeepInnovationAI enables researchers, policymakers, and industry leaders to\nanticipate trends and identify collaboration opportunities. With extensive\ntemporal and geographical scope, it supports detailed analysis of technological\ndevelopment patterns and international competition dynamics, establishing a\nfoundation for modeling AI innovation and technology transfer processes.",
      "pdf_url": "http://arxiv.org/pdf/2503.09257v2",
      "published": "2025-03-12T10:56:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09257v2",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.DL"
      ]
    },
    {
      "title": "SCOPE-DTI: Semi-Inductive Dataset Construction and Framework Optimization for Practical Usability Enhancement in Deep Learning-Based Drug Target Interaction Prediction",
      "authors": [
        "Yigang Chen",
        "Xiang Ji",
        "Ziyue Zhang",
        "Yuming Zhou",
        "Yang-Chi-Dung Lin",
        "Hsi-Yuan Huang",
        "Tao Zhang",
        "Yi Lai",
        "Ke Chen",
        "Chang Su",
        "Xingqiao Lin",
        "Zihao Zhu",
        "Yanggyi Zhang",
        "Kangping Wei",
        "Jiehui Fu",
        "Yixian Huang",
        "Shidong Cui",
        "Shih-Chung Yen",
        "Ariel Warshel",
        "Hsien-Da Huang"
      ],
      "abstract": "Deep learning-based drug-target interaction (DTI) prediction methods have\ndemonstrated strong performance; however, real-world applicability remains\nconstrained by limited data diversity and modeling complexity. To address these\nchallenges, we propose SCOPE-DTI, a unified framework combining a large-scale,\nbalanced semi-inductive human DTI dataset with advanced deep learning modeling.\nConstructed from 13 public repositories, the SCOPE dataset expands data volume\nby up to 100-fold compared to common benchmarks such as the Human dataset. The\nSCOPE model integrates three-dimensional protein and compound representations,\ngraph neural networks, and bilinear attention mechanisms to effectively capture\ncross domain interaction patterns, significantly outperforming state-of-the-art\nmethods across various DTI prediction tasks. Additionally, SCOPE-DTI provides a\nuser-friendly interface and database. We further validate its effectiveness by\nexperimentally identifying anticancer targets of Ginsenoside Rh1. By offering\ncomprehensive data, advanced modeling, and accessible tools, SCOPE-DTI\naccelerates drug discovery research.",
      "pdf_url": "http://arxiv.org/pdf/2503.09251v1",
      "published": "2025-03-12T10:46:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09251v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ]
    },
    {
      "title": "Considering Length Diversity in Retrieval-Augmented Summarization",
      "authors": [
        "Juseon-Do",
        "Jaesung Hwang",
        "Jingun Kwon",
        "Hidetaka Kamigaito",
        "Manabu Okumura"
      ],
      "abstract": "This study investigates retrieval-augmented summarization by specifically\nexamining the impact of exemplar summary lengths under length constraints, not\ncovered by previous work. We propose a Diverse Length-aware Maximal Marginal\nRelevance (DL-MMR) algorithm to better control summary lengths. This algorithm\ncombines the query relevance with diverse target lengths in retrieval-augmented\nsummarization. Unlike previous methods that necessitate exhaustive exemplar\nexemplar relevance comparisons using MMR, DL-MMR considers the exemplar target\nlength as well and avoids comparing exemplars to each other, thereby reducing\ncomputational cost and conserving memory during the construction of an exemplar\npool. Experimental results showed the effectiveness of DL-MMR, which considers\nlength diversity, compared to the original MMR algorithm. DL-MMR additionally\nshowed the effectiveness in memory saving of 781,513 times and computational\ncost reduction of 500,092 times, while maintaining the same level of\ninformativeness.",
      "pdf_url": "http://arxiv.org/pdf/2503.09249v1",
      "published": "2025-03-12T10:43:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09249v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ]
    },
    {
      "title": "In-Context Defense in Computer Agents: An Empirical Study",
      "authors": [
        "Pei Yang",
        "Hai Ci",
        "Mike Zheng Shou"
      ],
      "abstract": "Computer agents powered by vision-language models (VLMs) have significantly\nadvanced human-computer interaction, enabling users to perform complex tasks\nthrough natural language instructions. However, these agents are vulnerable to\ncontext deception attacks, an emerging threat where adversaries embed\nmisleading content into the agent's operational environment, such as a pop-up\nwindow containing deceptive instructions. Existing defenses, such as\ninstructing agents to ignore deceptive elements, have proven largely\nineffective. As the first systematic study on protecting computer agents, we\nintroduce textbf{in-context defense}, leveraging in-context learning and\nchain-of-thought (CoT) reasoning to counter such attacks. Our approach involves\naugmenting the agent's context with a small set of carefully curated exemplars\ncontaining both malicious environments and corresponding defensive responses.\nThese exemplars guide the agent to first perform explicit defensive reasoning\nbefore action planning, reducing susceptibility to deceptive attacks.\nExperiments demonstrate the effectiveness of our method, reducing attack\nsuccess rates by 91.2% on pop-up window attacks, 74.6% on average on\nenvironment injection attacks, while achieving 100% successful defenses against\ndistracting advertisements. Our findings highlight that (1) defensive reasoning\nmust precede action planning for optimal performance, and (2) a minimal number\nof exemplars (fewer than three) is sufficient to induce an agent's defensive\nbehavior.",
      "pdf_url": "http://arxiv.org/pdf/2503.09241v1",
      "published": "2025-03-12T10:38:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.09241v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}