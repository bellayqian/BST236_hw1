{
  "last_updated": "2025-06-17T00:53:13.759028",
  "papers": [
    {
      "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
      "authors": [
        "Hsi-Che Lin",
        "Yu-Chu Yu",
        "Kai-Po Chang",
        "Yu-Chiang Frank Wang"
      ],
      "abstract": "Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users.",
      "pdf_url": "http://arxiv.org/pdf/2506.12015v1",
      "published": "2025-06-13T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.12015v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "code_transformed: The Influence of Large Language Models on Code",
      "authors": [
        "Yuliang Xu",
        "Siming Huang",
        "Mingmeng Geng",
        "Yao Wan",
        "Xuanhua Shi",
        "Dongping Chen"
      ],
      "abstract": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.",
      "pdf_url": "http://arxiv.org/pdf/2506.12014v1",
      "published": "2025-06-13T17:59:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.12014v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "title": "Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making",
      "authors": [
        "Xiaopeng Yuan",
        "Xingjian Zhang",
        "Ke Xu",
        "Yifan Xu",
        "Lijun Yu",
        "Jindong Wang",
        "Yushun Dong",
        "Haohan Wang"
      ],
      "abstract": "Large language models (LLMs) are increasingly used for tasks that require\ncomplex reasoning. Most benchmarks focus on final outcomes but overlook the\nintermediate reasoning steps - such as planning, revision, and decision making\nunder resource constraints. We argue that measuring these internal processes is\nessential for understanding model behavior and improving reliability. We\npropose using strategic games as a natural evaluation environment: closed,\nrule-based systems with clear states, limited resources, and automatic\nfeedback. We introduce a framework that evaluates LLMs along three core\ndimensions: planning, revision, and resource-constrained decision making. To\noperationalize this, we define metrics beyond win rate, including\novercorrection risk rate, correction success rate, improvement slope, and\nover-budget ratio. In 4320 adversarial rounds across 12 leading models,\nChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7\npercent, a correction success rate of 78.6 percent, and an improvement slope of\n0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6\npercent, wins only 25.6 percent of its matches - primarily due to excessive\nresource use. We also observe a negative correlation between overcorrection\nrisk rate and correction success rate (Pearson r = -0.51, p = 0.093),\nsuggesting that more frequent edits do not always improve outcomes. Our\nfindings highlight the value of assessing not only what LLMs decide but how\nthey arrive at those decisions",
      "pdf_url": "http://arxiv.org/pdf/2506.12012v1",
      "published": "2025-06-13T17:59:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.12012v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Reimagining Dance: Real-time Music Co-creation between Dancers and AI",
      "authors": [
        "Olga Vechtomova",
        "Jeff Bos"
      ],
      "abstract": "Dance performance traditionally follows a unidirectional relationship where\nmovement responds to music. While AI has advanced in various creative domains,\nits application in dance has primarily focused on generating choreography from\nmusical input. We present a system that enables dancers to dynamically shape\nmusical environments through their movements. Our multi-modal architecture\ncreates a coherent musical composition by intelligently combining pre-recorded\nmusical clips in response to dance movements, establishing a bidirectional\ncreative partnership where dancers function as both performers and composers.\nThrough correlation analysis of performance data, we demonstrate emergent\ncommunication patterns between movement qualities and audio features. This\napproach reconceptualizes the role of AI in performing arts as a responsive\ncollaborator that expands possibilities for both professional dance performance\nand improvisational artistic expression across broader populations.",
      "pdf_url": "http://arxiv.org/pdf/2506.12008v1",
      "published": "2025-06-13T17:56:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.12008v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC",
        "eess.AS"
      ]
    },
    {
      "title": "Upgrade or Switch: Do We Need a New Registry Architecture for the Internet of AI Agents?",
      "authors": [
        "Ramesh Raskar",
        "Pradyumna Chari",
        "Jared James Grogan",
        "Mahesh Lambe",
        "Robert Lincourt",
        "Raghu Bala",
        "Abhishek Singh",
        "Ayush Chopra",
        "Rajesh Ranjan",
        "Shailja Gupta",
        "Dimitris Stripelis",
        "Maria Gorskikh",
        "Sichao Wang"
      ],
      "abstract": "The emerging Internet of AI Agents challenges existing web infrastructure\ndesigned for human-scale, reactive interactions. Unlike traditional web\nresources, autonomous AI agents initiate actions, maintain persistent state,\nspawn sub-agents, and negotiate directly with peers: demanding\nmillisecond-level discovery, instant credential revocation, and cryptographic\nbehavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes\nwhether to upgrade existing infrastructure or implement purpose-built registry\narchitectures for autonomous agents. We identify critical failure points: DNS\npropagation (24-48 hours vs. required milliseconds), certificate revocation\nunable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate\nfor agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2)\nSwitch options, (3) Hybrid registries. Drawing parallels to dialup-to-broadband\ntransitions, we find that agent requirements constitute qualitative, and not\nincremental, changes. While upgrades offer compatibility and faster deployment,\nclean-slate solutions provide better performance but require longer for\nadoption. Our analysis suggests hybrid approaches will emerge, with centralized\nregistries for critical agents and federated meshes for specialized use cases.",
      "pdf_url": "http://arxiv.org/pdf/2506.12003v1",
      "published": "2025-06-13T17:55:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.12003v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "VGR: Visual Grounded Reasoning",
      "authors": [
        "Jiacong Wang",
        "Zijiang Kang",
        "Haochen Wang",
        "Haiyong Jiang",
        "Jiawen Li",
        "Bohong Wu",
        "Ya Wang",
        "Jiao Ran",
        "Xiao Liang",
        "Chao Feng",
        "Jun Xiao"
      ],
      "abstract": "In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.",
      "pdf_url": "http://arxiv.org/pdf/2506.11991v1",
      "published": "2025-06-13T17:47:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11991v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task",
      "authors": [
        "Wuzhenghong Wen",
        "Su Pan",
        "yuwei Sun"
      ],
      "abstract": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.",
      "pdf_url": "http://arxiv.org/pdf/2506.11986v1",
      "published": "2025-06-13T17:46:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11986v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ]
    },
    {
      "title": "Technical Evaluation of a Disruptive Approach in Homomorphic AI",
      "authors": [
        "Eric Filiol"
      ],
      "abstract": "We present a technical evaluation of a new, disruptive cryptographic approach\nto data security, known as HbHAI (Hash-based Homomorphic Artificial\nIntelligence). HbHAI is based on a novel class of key-dependent hash functions\nthat naturally preserve most similarity properties, most AI algorithms rely on.\nAs a main claim, HbHAI makes now possible to analyze and process data in its\ncryptographically secure form while using existing native AI algorithms without\nmodification, with unprecedented performances compared to existing homomorphic\nencryption schemes.\n  We tested various HbHAI-protected datasets (non public preview) using\ntraditional unsupervised and supervised learning techniques (clustering,\nclassification, deep neural networks) with classical unmodified AI algorithms.\nThis paper presents technical results from an independent analysis conducted\nwith those different, off-the-shelf AI algorithms. The aim was to assess the\nsecurity, operability and performance claims regarding HbHAI techniques. As a\nresults, our results confirm most these claims, with only a few minor\nreservations.",
      "pdf_url": "http://arxiv.org/pdf/2506.11954v1",
      "published": "2025-06-13T17:06:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11954v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies",
      "authors": [
        "Nadun Ranawaka Arachchige",
        "Zhenyang Chen",
        "Wonsuhk Jung",
        "Woo Chul Shin",
        "Rohan Bansal",
        "Pierre Barroso",
        "Yu Hang He",
        "Yingyang Celine Lin",
        "Benjamin Joffe",
        "Shreyas Kousik",
        "Danfei Xu"
      ],
      "abstract": "Offline Imitation Learning (IL) methods such as Behavior Cloning are\neffective at acquiring complex robotic manipulation skills. However, existing\nIL-trained policies are confined to executing the task at the same speed as\nshown in demonstration data. This limits the task throughput of a robotic\nsystem, a critical requirement for applications such as industrial automation.\nIn this paper, we introduce and formalize the novel problem of enabling\nfaster-than-demonstration execution of visuomotor policies and identify\nfundamental challenges in robot dynamics and state-action distribution shifts.\nWe instantiate the key insights as SAIL (Speed Adaptation for Imitation\nLearning), a full-stack system integrating four tightly-connected components:\n(1) a consistency-preserving action inference algorithm for smooth motion at\nhigh speed, (2) high-fidelity tracking of controller-invariant motion targets,\n(3) adaptive speed modulation that dynamically adjusts execution speed based on\nmotion complexity, and (4) action scheduling to handle real-world system\nlatencies. Experiments on 12 tasks across simulation and two real, distinct\nrobot platforms show that SAIL achieves up to a 4x speedup over demonstration\nspeed in simulation and up to 3.2x speedup in the real world. Additional detail\nis available at https://nadunranawaka1.github.io/sail-policy",
      "pdf_url": "http://arxiv.org/pdf/2506.11948v1",
      "published": "2025-06-13T16:58:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11948v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Subjective Experience in AI Systems: What Do AI Researchers and the Public Believe?",
      "authors": [
        "Noemi Dreksler",
        "Lucius Caviola",
        "David Chalmers",
        "Carter Allen",
        "Alex Rand",
        "Joshua Lewis",
        "Philip Waggoner",
        "Kate Mays",
        "Jeff Sebo"
      ],
      "abstract": "We surveyed 582 AI researchers who have published in leading AI venues and\n838 nationally representative US participants about their views on the\npotential development of AI systems with subjective experience and how such\nsystems should be treated and governed. When asked to estimate the chances that\nsuch systems will exist on specific dates, the median responses were 1% (AI\nresearchers) and 5% (public) by 2024, 25% and 30% by 2034, and 70% and 60% by\n2100, respectively. The median member of the public thought there was a higher\nchance that AI systems with subjective experience would never exist (25%) than\nthe median AI researcher did (10%). Both groups perceived a need for\nmultidisciplinary expertise to assess AI subjective experience. Although\nsupport for welfare protections for such AI systems exceeded opposition, it\nremained far lower than support for protections for animals or the environment.\nAttitudes toward moral and governance issues were divided in both groups,\nespecially regarding whether such systems should be created and what rights or\nprotections they should receive. Yet a majority of respondents in both groups\nagreed that safeguards against the potential risks from AI systems with\nsubjective experience should be implemented by AI developers now, and if\ncreated, AI systems with subjective experience should treat others well, behave\nethically, and be held accountable. Overall, these results suggest that both AI\nresearchers and the public regard the emergence of AI systems with subjective\nexperience as a possibility this century, though substantial uncertainty and\ndisagreement remain about the timeline and appropriate response.",
      "pdf_url": "http://arxiv.org/pdf/2506.11945v1",
      "published": "2025-06-13T16:53:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11945v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Today's Cat Is Tomorrow's Dog: Accounting for Time-Based Changes in the Labels of ML Vulnerability Detection Approaches",
      "authors": [
        "Ranindya Paramitha",
        "Yuan Feng",
        "Fabio Massacci"
      ],
      "abstract": "Vulnerability datasets used for ML testing implicitly contain retrospective\ninformation. When tested on the field, one can only use the labels available at\nthe time of training and testing (e.g. seen and assumed negatives). As\nvulnerabilities are discovered across calendar time, labels change and past\nperformance is not necessarily aligned with future performance. Past works only\nconsidered the slices of the whole history (e.g. DiverseVUl) or individual\ndifferences between releases (e.g. Jimenez et al. ESEC/FSE 2019). Such\napproaches are either too optimistic in training (e.g. the whole history) or\ntoo conservative (e.g. consecutive releases). We propose a method to\nrestructure a dataset into a series of datasets in which both training and\ntesting labels change to account for the knowledge available at the time. If\nthe model is actually learning, it should improve its performance over time as\nmore data becomes available and data becomes more stable, an effect that can be\nchecked with the Mann-Kendall test. We validate our methodology for\nvulnerability detection with 4 time-based datasets (3 projects from BigVul\ndataset + Vuldeepecker's NVD) and 5 ML models (Code2Vec, CodeBERT, LineVul,\nReGVD, and Vuldeepecker). In contrast to the intuitive expectation (more\nretrospective information, better performance), the trend results show that\nperformance changes inconsistently across the years, showing that most models\nare not learning.",
      "pdf_url": "http://arxiv.org/pdf/2506.11939v1",
      "published": "2025-06-13T16:42:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11939v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "D.2; I.2"
      ]
    },
    {
      "title": "Improving Large Language Model Safety with Contrastive Representation Learning",
      "authors": [
        "Samuel Simko",
        "Mrinmaya Sachan",
        "Bernhard Schölkopf",
        "Zhijing Jin"
      ],
      "abstract": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense",
      "pdf_url": "http://arxiv.org/pdf/2506.11938v1",
      "published": "2025-06-13T16:42:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11938v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?",
      "authors": [
        "Zihan Zheng",
        "Zerui Cheng",
        "Zeyu Shen",
        "Shang Zhou",
        "Kaiyuan Liu",
        "Hansen He",
        "Dongruixuan Li",
        "Stanley Wei",
        "Hangyi Hao",
        "Jianzhu Yao",
        "Peiyao Sheng",
        "Zixuan Wang",
        "Wenhao Chai",
        "Aleksandra Korolova",
        "Peter Henderson",
        "Sanjeev Arora",
        "Pramod Viswanath",
        "Jingbo Shang",
        "Saining Xie"
      ],
      "abstract": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2506.11928v1",
      "published": "2025-06-13T16:29:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11928v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Real-World Deployment of a Lane Change Prediction Architecture Based on Knowledge Graph Embeddings and Bayesian Inference",
      "authors": [
        "M. Manzour",
        "Catherine M. Elias",
        "Omar M. Shehata",
        "R. Izquierdo",
        "M. A. Sotelo"
      ],
      "abstract": "Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely.",
      "pdf_url": "http://arxiv.org/pdf/2506.11925v1",
      "published": "2025-06-13T16:24:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11925v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Breaking Habits: On the Role of the Advantage Function in Learning Causal State Representations",
      "authors": [
        "Miguel Suau"
      ],
      "abstract": "Recent work has shown that reinforcement learning agents can develop policies\nthat exploit spurious correlations between rewards and observations. This\nphenomenon, known as policy confounding, arises because the agent's policy\ninfluences both past and future observation variables, creating a feedback loop\nthat can hinder the agent's ability to generalize beyond its usual\ntrajectories. In this paper, we show that the advantage function, commonly used\nin policy gradient methods, not only reduces the variance of gradient estimates\nbut also mitigates the effects of policy confounding. By adjusting action\nvalues relative to the state representation, the advantage function downweights\nstate-action pairs that are more likely under the current policy, breaking\nspurious correlations and encouraging the agent to focus on causal factors. We\nprovide both analytical and empirical evidence demonstrating that training with\nthe advantage function leads to improved out-of-trajectory performance.",
      "pdf_url": "http://arxiv.org/pdf/2506.11912v1",
      "published": "2025-06-13T16:06:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11912v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Spectra-to-Structure and Structure-to-Spectra Inference Across the Periodic Table",
      "authors": [
        "Yufeng Wang",
        "Peiyao Wang",
        "Lu Ma",
        "Yuewei Lin",
        "Qun Liu",
        "Haibin Ling"
      ],
      "abstract": "X-ray Absorption Spectroscopy (XAS) is a powerful technique for probing local\natomic environments, yet its interpretation remains limited by the need for\nexpert-driven analysis, computationally expensive simulations, and\nelement-specific heuristics. Recent advances in machine learning have shown\npromise for accelerating XAS interpretation, but many existing models are\nnarrowly focused on specific elements, edge types, or spectral regimes. In this\nwork, we present XAStruct, a learning framework capable of both predicting XAS\nspectra from crystal structures and inferring local structural descriptors from\nXAS input. XAStruct is trained on a large-scale dataset spanning over 70\nelements across the periodic table, enabling generalization to a wide variety\nof chemistries and bonding environments. The model includes the first machine\nlearning approach for predicting neighbor atom types directly from XAS spectra,\nas well as a unified regression model for mean nearest-neighbor distance that\nrequires no element-specific tuning. While we explored integrating the two\npipelines into a single end-to-end model, empirical results showed performance\ndegradation. As a result, the two tasks were trained independently to ensure\noptimal accuracy and task-specific performance. By combining deep neural\nnetworks for complex structure-property mappings with efficient baseline models\nfor simpler tasks, XAStruct offers a scalable and extensible solution for\ndata-driven XAS analysis and local structure inference. The source code will be\nreleased upon paper acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2506.11908v1",
      "published": "2025-06-13T15:58:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11908v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification",
      "authors": [
        "Lu Zhang",
        "Sangarapillai Lambotharan",
        "Gan Zheng",
        "Fabio Roli"
      ],
      "abstract": "Advantages of deep learning over traditional methods have been demonstrated\nfor radio signal classification in the recent years. However, various\nresearchers have discovered that even a small but intentional feature\nperturbation known as adversarial examples can significantly deteriorate the\nperformance of the deep learning based radio signal classification. Among\nvarious kinds of adversarial examples, universal adversarial perturbation has\ngained considerable attention due to its feature of being data independent,\nhence as a practical strategy to fool the radio signal classification with a\nhigh success rate. Therefore, in this paper, we investigate a defense system\ncalled neural rejection system to propose against universal adversarial\nperturbations, and evaluate its performance by generating white-box universal\nadversarial perturbations. We show that the proposed neural rejection system is\nable to defend universal adversarial perturbations with significantly higher\naccuracy than the undefended deep neural network.",
      "pdf_url": "http://arxiv.org/pdf/2506.11901v1",
      "published": "2025-06-13T15:52:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11901v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Attention-based Adversarial Robust Distillation in Radio Signal Classifications for Low-Power IoT Devices",
      "authors": [
        "Lu Zhang",
        "Sangarapillai Lambotharan",
        "Gan Zheng",
        "Guisheng Liao",
        "Basil AsSadhan",
        "Fabio Roli"
      ],
      "abstract": "Due to great success of transformers in many applications such as natural\nlanguage processing and computer vision, transformers have been successfully\napplied in automatic modulation classification. We have shown that\ntransformer-based radio signal classification is vulnerable to imperceptible\nand carefully crafted attacks called adversarial examples. Therefore, we\npropose a defense system against adversarial examples in transformer-based\nmodulation classifications. Considering the need for computationally efficient\narchitecture particularly for Internet of Things (IoT)-based applications or\noperation of devices in environment where power supply is limited, we propose a\ncompact transformer for modulation classification. The advantages of robust\ntraining such as adversarial training in transformers may not be attainable in\ncompact transformers. By demonstrating this, we propose a novel compact\ntransformer that can enhance robustness in the presence of adversarial attacks.\nThe new method is aimed at transferring the adversarial attention map from the\nrobustly trained large transformer to a compact transformer. The proposed\nmethod outperforms the state-of-the-art techniques for the considered white-box\nscenarios including fast gradient method and projected gradient descent\nattacks. We have provided reasoning of the underlying working mechanisms and\ninvestigated the transferability of the adversarial examples between different\narchitectures. The proposed method has the potential to protect the transformer\nfrom the transferability of adversarial examples.",
      "pdf_url": "http://arxiv.org/pdf/2506.11892v1",
      "published": "2025-06-13T15:39:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11892v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training",
      "authors": [
        "Judson Leroy Dean Haynes IV"
      ],
      "abstract": "Virtual Reality simulators offer a powerful tool for teacher training, yet\nthe integration of AI-powered student avatars presents a critical challenge:\ndetermining the optimal level of avatar realism for effective pedagogy. This\nliterature review examines the evolution of avatar realism in VR teacher\ntraining, synthesizes its theoretical implications, and proposes a new\npedagogical framework to guide future design. Through a systematic review, this\npaper traces the progression from human-controlled avatars to generative AI\nprototypes. Applying learning theories like Cognitive Load Theory, we argue\nthat hyper-realism is not always optimal, as high-fidelity avatars can impose\nexcessive extraneous cognitive load on novices, a stance supported by recent\nempirical findings. A significant gap exists between the technological drive\nfor photorealism and the pedagogical need for scaffolded learning. To address\nthis gap, we propose Graduated Realism, a framework advocating for starting\ntrainees with lower-fidelity avatars and progressively increasing behavioral\ncomplexity as skills develop. To make this computationally feasible, we outline\na novel single-call architecture, Crazy Slots, which uses a probabilistic\nengine and a Retrieval-Augmented Generation database to generate authentic,\nreal-time responses without the latency and cost of multi-step reasoning\nmodels. This review provides evidence-based principles for designing the next\ngeneration of AI simulators, arguing that a pedagogically grounded approach to\nrealism is essential for creating scalable and effective teacher education\ntools.",
      "pdf_url": "http://arxiv.org/pdf/2506.11890v1",
      "published": "2025-06-13T15:37:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11890v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making",
      "authors": [
        "Claudio Fanconi",
        "Mihaela van der Schaar"
      ],
      "abstract": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.",
      "pdf_url": "http://arxiv.org/pdf/2506.11887v1",
      "published": "2025-06-13T15:36:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11887v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing",
      "authors": [
        "Haochen Sun",
        "Yifan Liu",
        "Ahmed Al-Tahmeesschi",
        "Swarna Chetty",
        "Syed Ali Raza Zaidi",
        "Avishek Nag",
        "Hamed Ahmadi"
      ],
      "abstract": "Effective resource management and network slicing are essential to meet the\ndiverse service demands of vehicular networks, including Enhanced Mobile\nBroadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC).\nThis paper introduces an Explainable Deep Reinforcement Learning (XRL)\nframework for dynamic network slicing and resource allocation in vehicular\nnetworks, built upon a near-real-time RAN intelligent controller. By\nintegrating a feature-based approach that leverages Shapley values and an\nattention mechanism, we interpret and refine the decisions of our\nreinforcementlearning agents, addressing key reliability challenges in\nvehicular communication systems. Simulation results demonstrate that our\napproach provides clear, real-time insights into the resource allocation\nprocess and achieves higher interpretability precision than a pure attention\nmechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC\nservices increased from 78.0% to 80.13%, while that for eMBB services improved\nfrom 71.44% to 73.21%.",
      "pdf_url": "http://arxiv.org/pdf/2506.11882v1",
      "published": "2025-06-13T15:32:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11882v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment",
      "authors": [
        "Alejandro Peña",
        "Julian Fierrez",
        "Aythami Morales",
        "Gonzalo Mancera",
        "Miguel Lopez",
        "Ruben Tolosana"
      ],
      "abstract": "The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data.",
      "pdf_url": "http://arxiv.org/pdf/2506.11880v1",
      "published": "2025-06-13T15:29:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11880v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "authors": [
        "Jina Kim",
        "Jeffrey Willette",
        "Bruno Andreis",
        "Sung Ju Hwang"
      ],
      "abstract": "A widely recognized limitation of molecular prediction models is their\nreliance on structures observed in the training data, resulting in poor\ngeneralization to out-of-distribution compounds. Yet in drug discovery, the\ncompounds most critical for advancing research often lie beyond the training\nset, making the bias toward the training data particularly problematic. This\nmismatch introduces substantial covariate shift, under which standard deep\nlearning models produce unstable and inaccurate predictions. Furthermore, the\nscarcity of labeled data, stemming from the onerous and costly nature of\nexperimental validation, further exacerbates the difficulty of achieving\nreliable generalization. To address these limitations, we propose a novel\nmeta-learning-based approach that leverages unlabeled data to interpolate\nbetween in-distribution (ID) and out-of-distribution (OOD) data, enabling the\nmodel to meta-learn how to generalize beyond the training distribution. We\ndemonstrate significant performance gains over state-of-the-art methods on\nchallenging real-world datasets that exhibit substantial covariate shift.",
      "pdf_url": "http://arxiv.org/pdf/2506.11877v1",
      "published": "2025-06-13T15:27:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11877v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "How do Probabilistic Graphical Models and Graph Neural Networks Look at Network Data?",
      "authors": [
        "Michela Lapenna",
        "Caterina De Bacco"
      ],
      "abstract": "Graphs are a powerful data structure for representing relational data and are\nwidely used to describe complex real-world systems. Probabilistic Graphical\nModels (PGMs) and Graph Neural Networks (GNNs) can both leverage\ngraph-structured data, but their inherent functioning is different. The\nquestion is how do they compare in capturing the information contained in\nnetworked datasets? We address this objective by solving a link prediction task\nand we conduct three main experiments, on both synthetic and real networks: one\nfocuses on how PGMs and GNNs handle input features, while the other two\ninvestigate their robustness to noisy features and increasing heterophily of\nthe graph. PGMs do not necessarily require features on nodes, while GNNs cannot\nexploit the network edges alone, and the choice of input features matters. We\nfind that GNNs are outperformed by PGMs when input features are low-dimensional\nor noisy, mimicking many real scenarios where node attributes might be scalar\nor noisy. Then, we find that PGMs are more robust than GNNs when the\nheterophily of the graph is increased. Finally, to assess performance beyond\nprediction tasks, we also compare the two frameworks in terms of their\ncomputational complexity and interpretability.",
      "pdf_url": "http://arxiv.org/pdf/2506.11869v1",
      "published": "2025-06-13T15:19:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11869v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math-ph",
        "math.MP"
      ]
    },
    {
      "title": "MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command Line and Browser",
      "authors": [
        "Armina Fani",
        "Mike Doan",
        "Isabelle Le",
        "Alex Fedorov",
        "Malte Hoffmann",
        "Chris Rorden",
        "Sergey Plis"
      ],
      "abstract": "We developed MindGrab, a parameter- and memory-efficient deep\nfully-convolutional model for volumetric skull-stripping in head images of any\nmodality. Its architecture, informed by a spectral interpretation of dilated\nconvolutions, was trained exclusively on modality-agnostic synthetic data.\nMindGrab was evaluated on a retrospective dataset of 606 multimodal adult-brain\nscans (T1, T2, DWI, MRA, PDw MRI, EPI, CT, PET) sourced from the SynthStrip\ndataset. Performance was benchmarked against SynthStrip, ROBEX, and BET using\nDice scores, with Wilcoxon signed-rank significance tests. MindGrab achieved a\nmean Dice score of 95.9 with standard deviation (SD) 1.6 across modalities,\nsignificantly outperforming classical methods (ROBEX: 89.1 SD 7.7, P < 0.05;\nBET: 85.2 SD 14.4, P < 0.05). Compared to SynthStrip (96.5 SD 1.1, P=0.0352),\nMindGrab delivered equivalent or superior performance in nearly half of the\ntested scenarios, with minor differences (<3% Dice) in the others. MindGrab\nutilized 95% fewer parameters (146,237 vs. 2,566,561) than SynthStrip. This\nefficiency yielded at least 2x faster inference, 50% lower memory usage on\nGPUs, and enabled exceptional performance (e.g., 10-30x speedup, and up to 30x\nmemory reduction) and accessibility on a wider range of hardware, including\nsystems without high-end GPUs. MindGrab delivers state-of-the-art accuracy with\ndramatically lower resource demands, supported in brainchop-cli\n(https://pypi.org/project/brainchop/) and at brainchop.org.",
      "pdf_url": "http://arxiv.org/pdf/2506.11860v1",
      "published": "2025-06-13T15:09:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11860v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ]
    },
    {
      "title": "Regression-adjusted Monte Carlo Estimators for Shapley Values and Probabilistic Values",
      "authors": [
        "R. Teal Witter",
        "Yurong Liu",
        "Christopher Musco"
      ],
      "abstract": "With origins in game theory, probabilistic values like Shapley values,\nBanzhaf values, and semi-values have emerged as a central tool in explainable\nAI. They are used for feature attribution, data attribution, data valuation,\nand more. Since all of these values require exponential time to compute\nexactly, research has focused on efficient approximation methods using two\ntechniques: Monte Carlo sampling and linear regression formulations. In this\nwork, we present a new way of combining both of these techniques. Our approach\nis more flexible than prior algorithms, allowing for linear regression to be\nreplaced with any function family whose probabilistic values can be computed\nefficiently. This allows us to harness the accuracy of tree-based models like\nXGBoost, while still producing unbiased estimates. From experiments across\neight datasets, we find that our methods give state-of-the-art performance for\nestimating probabilistic values. For Shapley values, the error of our methods\ncan be $6.5\\times$ lower than Permutation SHAP (the most popular Monte Carlo\nmethod), $3.8\\times$ lower than Kernel SHAP (the most popular linear regression\nmethod), and $2.6\\times$ lower than Leverage SHAP (the prior state-of-the-art\nShapley value estimator). For more general probabilistic values, we can obtain\nerror $215\\times$ lower than the best estimator from prior work.",
      "pdf_url": "http://arxiv.org/pdf/2506.11849v1",
      "published": "2025-06-13T14:57:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11849v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks",
      "authors": [
        "Qihai Zhang",
        "Xinyue Sheng",
        "Yuanfu Sun",
        "Qiaoyu Tan"
      ],
      "abstract": "Inspired by the success of large language models (LLMs), there is a\nsignificant research shift from traditional graph learning methods to LLM-based\ngraph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning\npower of LLMs by integrating three key components: the textual attributes of\ninput nodes, the structural information of node neighborhoods, and\ntask-specific prompts that guide decision-making. Despite their promise, the\nrobustness of GraphLLMs against adversarial perturbations remains largely\nunexplored-a critical concern for deploying these models in high-stakes\nscenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study\nevaluating the vulnerability of GraphLLMs to adversarial attacks across three\ndimensions: text, graph structure, and prompt manipulations. We implement\nstate-of-the-art attack algorithms from each perspective to rigorously assess\nmodel resilience. Through extensive experiments on six benchmark datasets from\ndiverse domains, our findings reveal that GraphLLMs are highly susceptible to\ntext attacks that merely replace a few semantically similar words in a node's\ntextual attribute. We also find that standard graph structure attack methods\ncan significantly degrade model performance, while random shuffling of the\ncandidate label set in prompt templates leads to substantial performance drops.\nBeyond characterizing these vulnerabilities, we investigate defense techniques\ntailored to each attack vector through data-augmented training and adversarial\ntraining, which show promising potential to enhance the robustness of\nGraphLLMs. We hope that our open-sourced library will facilitate rapid,\nequitable evaluation and inspire further innovative research in this field.",
      "pdf_url": "http://arxiv.org/pdf/2506.11844v1",
      "published": "2025-06-13T14:48:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11844v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Revealing Political Bias in LLMs through Structured Multi-Agent Debate",
      "authors": [
        "Aishwarya Bandaru",
        "Fabian Bindley",
        "Trevor Bluth",
        "Nandini Chavda",
        "Baixu Chen",
        "Ethan Law"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to simulate social\nbehaviour, yet their political biases and interaction dynamics in debates\nremain underexplored. We investigate how LLM type and agent gender attributes\ninfluence political bias using a structured multi-agent debate framework, by\nengaging Neutral, Republican, and Democrat American LLM agents in debates on\npolitically sensitive topics. We systematically vary the underlying LLMs, agent\ngenders, and debate formats to examine how model provenance and agent personas\ninfluence political bias and attitudes throughout debates. We find that Neutral\nagents consistently align with Democrats, while Republicans shift closer to the\nNeutral; gender influences agent attitudes, with agents adapting their opinions\nwhen aware of other agents' genders; and contrary to prior research, agents\nwith shared political affiliations can form echo chambers, exhibiting the\nexpected intensification of attitudes as debates progress.",
      "pdf_url": "http://arxiv.org/pdf/2506.11825v1",
      "published": "2025-06-13T14:30:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11825v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ]
    },
    {
      "title": "Diffusion-Based Electrocardiography Noise Quantification via Anomaly Detection",
      "authors": [
        "Tae-Seong Han",
        "Jae-Wook Heo",
        "Hakseung Kim",
        "Cheol-Hui Lee",
        "Hyub Huh",
        "Eue-Keun Choi",
        "Dong-Joo Kim"
      ],
      "abstract": "Electrocardiography (ECG) signals are often degraded by noise, which\ncomplicates diagnosis in clinical and wearable settings. This study proposes a\ndiffusion-based framework for ECG noise quantification via reconstruction-based\nanomaly detection, addressing annotation inconsistencies and the limited\ngeneralizability of conventional methods. We introduce a distributional\nevaluation using the Wasserstein-1 distance ($W_1$), comparing the\nreconstruction error distributions between clean and noisy ECGs to mitigate\ninconsistent annotations. Our final model achieved robust noise quantification\nusing only three reverse diffusion steps. The model recorded a macro-average\n$W_1$ score of 1.308 across the benchmarks, outperforming the next-best method\nby over 48%. External validations demonstrated strong generalizability,\nsupporting the exclusion of low-quality segments to enhance diagnostic accuracy\nand enable timely clinical responses to signal degradation. The proposed method\nenhances clinical decision-making, diagnostic accuracy, and real-time ECG\nmonitoring capabilities, supporting future advancements in clinical and\nwearable ECG applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.11815v1",
      "published": "2025-06-13T14:19:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11815v1",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ]
    },
    {
      "title": "On the Performance of LLMs for Real Estate Appraisal",
      "authors": [
        "Margot Geerts",
        "Manon Reusens",
        "Bart Baesens",
        "Seppe vanden Broucke",
        "Jochen De Weerdt"
      ],
      "abstract": "The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders.",
      "pdf_url": "http://arxiv.org/pdf/2506.11812v1",
      "published": "2025-06-13T14:14:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11812v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Abstract Sound Fusion with Unconditioned Inversion Model",
      "authors": [
        "Jing Liu",
        "EnQi Lian"
      ],
      "abstract": "An abstract sound is defined as a sound that does not disclose identifiable\nreal-world sound events to a listener. Sound fusion aims to synthesize an\noriginal sound and a reference sound to generate a novel sound that exhibits\nauditory features beyond mere additive superposition of the sound constituents.\nTo achieve this fusion, we employ inversion techniques that preserve essential\nfeatures of the original sample while enabling controllable synthesis. We\npropose novel SDE and ODE inversion models based on DPMSolver++ samplers that\nreverse the sampling process by configuring model outputs as constants,\neliminating circular dependencies incurred by noise prediction terms. Our\ninversion approach requires no prompt conditioning while maintaining flexible\nguidance during sampling.",
      "pdf_url": "http://arxiv.org/pdf/2506.11811v1",
      "published": "2025-06-13T14:13:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11811v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models",
      "authors": [
        "Maximilian Kreutner",
        "Marlene Lutz",
        "Markus Strohmaier"
      ],
      "abstract": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.",
      "pdf_url": "http://arxiv.org/pdf/2506.11798v1",
      "published": "2025-06-13T14:02:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11798v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Why Do Class-Dependent Evaluation Effects Occur with Time Series Feature Attributions? A Synthetic Data Investigation",
      "authors": [
        "Gregor Baer",
        "Isel Grau",
        "Chao Zhang",
        "Pieter Van Gorp"
      ],
      "abstract": "Evaluating feature attribution methods represents a critical challenge in\nexplainable AI (XAI), as researchers typically rely on perturbation-based\nmetrics when ground truth is unavailable. However, recent work demonstrates\nthat these evaluation metrics can show different performance across predicted\nclasses within the same dataset. These \"class-dependent evaluation effects\"\nraise questions about whether perturbation analysis reliably measures\nattribution quality, with direct implications for XAI method development and\nthe trustworthiness of evaluation techniques. We investigate under which\nconditions these class-dependent effects arise by conducting controlled\nexperiments with synthetic time series data where ground truth feature\nlocations are known. We systematically vary feature types and class contrasts\nacross binary classification tasks, then compare perturbation-based degradation\nscores with ground truth-based precision-recall metrics using multiple\nattribution methods. Our experiments demonstrate that class-dependent effects\nemerge with both evaluation approaches even in simple scenarios with temporally\nlocalized features, triggered by basic variations in feature amplitude or\ntemporal extent between classes. Most critically, we find that\nperturbation-based and ground truth metrics frequently yield contradictory\nassessments of attribution quality across classes, with weak correlations\nbetween evaluation approaches. These findings suggest that researchers should\ninterpret perturbation-based metrics with care, as they may not always align\nwith whether attributions correctly identify discriminating features. These\nfindings reveal opportunities to reconsider what attribution evaluation\nactually measures and to develop more comprehensive evaluation frameworks that\ncapture multiple dimensions of attribution quality.",
      "pdf_url": "http://arxiv.org/pdf/2506.11790v1",
      "published": "2025-06-13T13:52:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11790v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation",
      "authors": [
        "Divyanshu Mishra",
        "Mohammadreza Salehi",
        "Pramit Saha",
        "Olga Patey",
        "Aris T. Papageorghiou",
        "Yuki M. Asano",
        "J. Alison Noble"
      ],
      "abstract": "Self-supervised learning (SSL) has achieved major advances in natural images\nand video understanding, but challenges remain in domains like echocardiography\n(heart ultrasound) due to subtle anatomical structures, complex temporal\ndynamics, and the current lack of domain-specific pre-trained models. Existing\nSSL approaches such as contrastive, masked modeling, and clustering-based\nmethods struggle with high intersample similarity, sensitivity to low PSNR\ninputs common in ultrasound, or aggressive augmentations that distort\nclinically relevant features. We present DISCOVR (Distilled Image Supervision\nfor Cross Modal Video Representation), a self-supervised dual branch framework\nfor cardiac ultrasound video representation learning. DISCOVR combines a\nclustering-based video encoder that models temporal dynamics with an online\nimage encoder that extracts fine-grained spatial semantics. These branches are\nconnected through a semantic cluster distillation loss that transfers\nanatomical knowledge from the evolving image encoder to the video encoder,\nenabling temporally coherent representations enriched with fine-grained\nsemantic understanding. Evaluated on six echocardiography datasets spanning\nfetal, pediatric, and adult populations, DISCOVR outperforms both specialized\nvideo anomaly detection methods and state-of-the-art video-SSL baselines in\nzero-shot and linear probing setups, and achieves superior segmentation\ntransfer.",
      "pdf_url": "http://arxiv.org/pdf/2506.11777v1",
      "published": "2025-06-13T13:36:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11777v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation",
      "authors": [
        "Abhishek Jaiswal",
        "Armeet Singh Luthra",
        "Purav Jangir",
        "Bhavya Garg",
        "Nisheeth Srivastava"
      ],
      "abstract": "Isometric exercises appeal to individuals seeking convenience, privacy, and\nminimal dependence on equipments. However, such fitness training is often\noverdependent on unreliable digital media content instead of expert\nsupervision, introducing serious risks, including incorrect posture, injury,\nand disengagement due to lack of corrective feedback. To address these\nchallenges, we present a real-time feedback system for assessing isometric\nposes. Our contributions include the release of the largest multiclass\nisometric exercise video dataset to date, comprising over 3,600 clips across\nsix poses with correct and incorrect variations. To support robust evaluation,\nwe benchmark state-of-the-art models-including graph-based networks-on this\ndataset and introduce a novel three-part metric that captures classification\naccuracy, mistake localization, and model confidence. Our results enhance the\nfeasibility of intelligent and personalized exercise training systems for home\nworkouts. This expert-level diagnosis, delivered directly to the users, also\nexpands the potential applications of these systems to rehabilitation,\nphysiotherapy, and various other fitness disciplines that involve physical\nmotion.",
      "pdf_url": "http://arxiv.org/pdf/2506.11774v1",
      "published": "2025-06-13T13:33:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11774v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "FeNN: A RISC-V vector processor for Spiking Neural Network acceleration",
      "authors": [
        "Zainab Aizaz",
        "James C. Knight",
        "Thomas Nowotny"
      ],
      "abstract": "Spiking Neural Networks (SNNs) have the potential to drastically reduce the\nenergy requirements of AI systems. However, mainstream accelerators like GPUs\nand TPUs are designed for the high arithmetic intensity of standard ANNs so are\nnot well-suited to SNN simulation. FPGAs are well-suited to applications with\nlow arithmetic intensity as they have high off-chip memory bandwidth and large\namounts of on-chip memory. Here, we present a novel RISC-V-based soft vector\nprocessor (FeNN), tailored to simulating SNNs on FPGAs. Unlike most dedicated\nneuromorphic hardware, FeNN is fully programmable and designed to be integrated\nwith applications running on standard computers from the edge to the cloud. We\ndemonstrate that, by using stochastic rounding and saturation, FeNN can achieve\nhigh numerical precision with low hardware utilisation and that a single FeNN\ncore can simulate an SNN classifier faster than both an embedded GPU and the\nLoihi neuromorphic system.",
      "pdf_url": "http://arxiv.org/pdf/2506.11760v1",
      "published": "2025-06-13T13:13:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11760v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.AR"
      ]
    },
    {
      "title": "Causal Effect Identification in Heterogeneous Environments from Higher-Order Moments",
      "authors": [
        "Yaroslav Kivva",
        "Sina Akbari",
        "Saber Salehkaleybar",
        "Negar Kiyavash"
      ],
      "abstract": "We investigate the estimation of the causal effect of a treatment variable on\nan outcome in the presence of a latent confounder. We first show that the\ncausal effect is identifiable under certain conditions when data is available\nfrom multiple environments, provided that the target causal effect remains\ninvariant across these environments. Secondly, we propose a moment-based\nalgorithm for estimating the causal effect as long as only a single parameter\nof the data-generating mechanism varies across environments -- whether it be\nthe exogenous noise distribution or the causal relationship between two\nvariables. Conversely, we prove that identifiability is lost if both exogenous\nnoise distributions of both the latent and treatment variables vary across\nenvironments. Finally, we propose a procedure to identify which parameter of\nthe data-generating mechanism has varied across the environments and evaluate\nthe performance of our proposed methods through experiments on synthetic data.",
      "pdf_url": "http://arxiv.org/pdf/2506.11756v1",
      "published": "2025-06-13T13:11:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11756v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.PR"
      ]
    },
    {
      "title": "Relational GNNs Cannot Learn $C_2$ Features for Planning",
      "authors": [
        "Dillon Z. Chen"
      ],
      "abstract": "Relational Graph Neural Networks (R-GNNs) are a GNN-based approach for\nlearning value functions that can generalise to unseen problems from a given\nplanning domain. R-GNNs were theoretically motivated by the well known\nconnection between the expressive power of GNNs and $C_2$, first-order logic\nwith two variables and counting. In the context of planning, $C_2$ features\nrefer to the set of formulae in $C_2$ with relations defined by the unary and\nbinary predicates of a planning domain. Some planning domains exhibit optimal\nvalue functions that can be decomposed as arithmetic expressions of $C_2$\nfeatures. We show that, contrary to empirical results, R-GNNs cannot learn\nvalue functions defined by $C_2$ features. We also identify prior GNN\narchitectures for planning that may better learn value functions defined by\n$C_2$ features.",
      "pdf_url": "http://arxiv.org/pdf/2506.11721v1",
      "published": "2025-06-13T12:35:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11721v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Interaction, Process, Infrastructure: A Unified Architecture for Human-Agent Collaboration",
      "authors": [
        "Yun Wang",
        "Yan Lu"
      ],
      "abstract": "As AI tools proliferate across domains, from chatbots and copilots to\nemerging agents, they increasingly support professional knowledge work. Yet\ndespite their growing capabilities, these systems remain fragmented: they\nassist with isolated tasks but lack the architectural scaffolding for\nsustained, adaptive collaboration. We propose a layered framework for\nhuman-agent systems that integrates three interdependent dimensions:\ninteraction, process, and infrastructure. Crucially, our architecture elevates\nprocess to a primary focus by making it explicit, inspectable, and adaptable,\nenabling humans and agents to align with evolving goals and coordinate over\ntime. This model clarifies limitations of current tools, unifies emerging\nsystem design approaches, and reveals new opportunities for researchers and AI\nsystem builders. By grounding intelligent behavior in structured collaboration,\nwe reimagine human-agent collaboration not as task-specific augmentation, but\nas a form of coherent and aligned system for real-world work.",
      "pdf_url": "http://arxiv.org/pdf/2506.11718v1",
      "published": "2025-06-13T12:34:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11718v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization",
      "authors": [
        "Wenqi Liu",
        "Xuemeng Song",
        "Jiaxi Li",
        "Yinwei Wei",
        "Na Zheng",
        "Jianhua Yin",
        "Liqiang Nie"
      ],
      "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for\nmitigating hallucination in Multimodal Large Language Models (MLLMs). Although\nexisting methods have achieved significant progress by utilizing\nvision-oriented contrastive objectives for enhancing MLLMs' attention to visual\ninputs and hence reducing hallucination, they suffer from non-rigorous\noptimization objective function and indirect preference supervision. To address\nthese limitations, we propose a Symmetric Multimodal Preference Optimization\n(SymMPO), which conducts symmetric preference learning with direct preference\nsupervision (i.e., response pairs) for visual understanding enhancement, while\nmaintaining rigorous theoretical alignment with standard DPO. In addition to\nconventional ordinal preference learning, SymMPO introduces a preference margin\nconsistency loss to quantitatively regulate the preference gap between\nsymmetric preference pairs. Comprehensive evaluation across five benchmarks\ndemonstrate SymMPO's superior performance, validating its effectiveness in\nhallucination mitigation of MLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2506.11712v1",
      "published": "2025-06-13T12:29:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11712v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
      "authors": [
        "Víctor Gallego"
      ],
      "abstract": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning",
      "pdf_url": "http://arxiv.org/pdf/2506.11702v1",
      "published": "2025-06-13T12:17:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11702v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Differential Privacy in Machine Learning: From Symbolic AI to LLMs",
      "authors": [
        "Francisco Aguilera-Martínez",
        "Fernando Berzal"
      ],
      "abstract": "Machine learning models should not reveal particular information that is not\notherwise accessible. Differential privacy provides a formal framework to\nmitigate privacy risks by ensuring that the inclusion or exclusion of any\nsingle data point does not significantly alter the output of an algorithm, thus\nlimiting the exposure of private information. This survey paper explores the\nfoundational definitions of differential privacy, reviews its original\nformulations and tracing its evolution through key research contributions. It\nthen provides an in-depth examination of how DP has been integrated into\nmachine learning models, analyzing existing proposals and methods to preserve\nprivacy when training ML models. Finally, it describes how DP-based ML\ntechniques can be evaluated in practice. %Finally, it discusses the broader\nimplications of DP, highlighting its potential for public benefit, its\nreal-world applications, and the challenges it faces, including vulnerabilities\nto adversarial attacks. By offering a comprehensive overview of differential\nprivacy in machine learning, this work aims to contribute to the ongoing\ndevelopment of secure and responsible AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.11687v1",
      "published": "2025-06-13T11:30:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11687v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ]
    },
    {
      "title": "MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space",
      "authors": [
        "Anshul Singh",
        "Chris Biemann",
        "Jan Strich"
      ],
      "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ninterpreting visual layouts and text. However, a significant challenge remains\nin their ability to interpret robustly and reason over multi-tabular data\npresented as images, a common occurrence in real-world scenarios like web pages\nand digital documents. Existing benchmarks typically address single tables or\nnon-visual data (text/structured). This leaves a critical gap: they don't\nassess the ability to parse diverse table images, correlate information across\nthem, and perform multi-hop reasoning on the combined visual data. We introduce\nMTabVQA, a novel benchmark specifically designed for multi-tabular visual\nquestion answering to bridge that gap. MTabVQA comprises 3,745 complex\nquestion-answer pairs that necessitate multi-hop reasoning across several\nvisually rendered table images. We provide extensive benchmark results for\nstate-of-the-art VLMs on MTabVQA, revealing significant performance\nlimitations. We further investigate post-training techniques to enhance these\nreasoning abilities and release MTabVQA-Instruct, a large-scale\ninstruction-tuning dataset. Our experiments show that fine-tuning VLMs with\nMTabVQA-Instruct substantially improves their performance on visual\nmulti-tabular reasoning. Code and dataset\n(https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval) are available online\n(https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E).",
      "pdf_url": "http://arxiv.org/pdf/2506.11684v1",
      "published": "2025-06-13T11:21:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11684v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LLMs on support of privacy and security of mobile apps: state of the art and research directions",
      "authors": [
        "Tran Thanh Lam Nguyen",
        "Barbara Carminati",
        "Elena Ferrari"
      ],
      "abstract": "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges.",
      "pdf_url": "http://arxiv.org/pdf/2506.11679v1",
      "published": "2025-06-13T11:17:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11679v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Pose Matters: Evaluating Vision Transformers and CNNs for Human Action Recognition on Small COCO Subsets",
      "authors": [
        "MingZe Tang",
        "Madiha Kazi"
      ],
      "abstract": "This study explores human action recognition using a three-class subset of\nthe COCO image corpus, benchmarking models from simple fully connected networks\nto transformer architectures. The binary Vision Transformer (ViT) achieved 90%\nmean test accuracy, significantly exceeding multiclass classifiers such as\nconvolutional networks (approximately 35%) and CLIP-based models (approximately\n62-64%). A one-way ANOVA (F = 61.37, p < 0.001) confirmed these differences are\nstatistically significant. Qualitative analysis with SHAP explainer and LeGrad\nheatmaps indicated that the ViT localizes pose-specific regions (e.g., lower\nlimbs for walking or running), while simpler feed-forward models often focus on\nbackground textures, explaining their errors. These findings emphasize the data\nefficiency of transformer representations and the importance of explainability\ntechniques in diagnosing class-specific failures.",
      "pdf_url": "http://arxiv.org/pdf/2506.11678v1",
      "published": "2025-06-13T11:16:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11678v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.0"
      ]
    },
    {
      "title": "Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE",
      "authors": [
        "Alicja Dobrzeniecka",
        "Antske Fokkens",
        "Pia Sommerauer"
      ],
      "abstract": "Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing.",
      "pdf_url": "http://arxiv.org/pdf/2506.11673v1",
      "published": "2025-06-13T11:07:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11673v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Converting Annotated Clinical Cases into Structured Case Report Forms",
      "authors": [
        "Pietro Ferrazzi",
        "Alberto Lavelli",
        "Bernardo Magnini"
      ],
      "abstract": "Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166",
      "pdf_url": "http://arxiv.org/pdf/2506.11666v1",
      "published": "2025-06-13T10:53:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11666v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "DISCO: Mitigating Bias in Deep Learning with Conditional Distance Correlation",
      "authors": [
        "Emre Kavak",
        "Tom Nuno Wolf",
        "Christian Wachinger"
      ],
      "abstract": "During prediction tasks, models can use any signal they receive to come up\nwith the final answer - including signals that are causally irrelevant. When\npredicting objects from images, for example, the lighting conditions could be\ncorrelated to different targets through selection bias, and an oblivious model\nmight use these signals as shortcuts to discern between various objects. A\npredictor that uses lighting conditions instead of real object-specific details\nis obviously undesirable. To address this challenge, we introduce a standard\nanti-causal prediction model (SAM) that creates a causal framework for\nanalyzing the information pathways influencing our predictor in anti-causal\nsettings. We demonstrate that a classifier satisfying a specific conditional\nindependence criterion will focus solely on the direct causal path from label\nto image, being counterfactually invariant to the remaining variables. Finally,\nwe propose DISCO, a novel regularization strategy that uses conditional\ndistance correlation to optimize for conditional independence in regression\ntasks. We can show that DISCO achieves competitive results in different bias\nmitigation experiments, deeming it a valid alternative to classical\nkernel-based methods.",
      "pdf_url": "http://arxiv.org/pdf/2506.11653v1",
      "published": "2025-06-13T10:29:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11653v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Robot Context Protocol (RCP): A Runtime-Agnostic Interface for Agent-Aware Robot Control",
      "authors": [
        "Lambert Lee",
        "Joshua Lau"
      ],
      "abstract": "The Robot Context Protocol (RCP) is a lightweight, middleware-agnostic\ncommunication protocol designed to simplify the complexity of robotic systems\nand enable seamless interaction between robots, users, and autonomous agents.\nRCP provides a unified and semantically meaningful interface that decouples\nclient-facing operations from backend implementations, supporting a wide range\nof deployment environments including physical robots, cloud-based\norchestrators, and simulated platforms. Built on HTTP and WebSocket transport\nlayers, the protocol defines a schema-driven message format with structured\noperations such as read, write, execute, and subscribe. It integrates features\nsuch as runtime introspection, asynchronous feedback, multi-tenant namespace\nisolation, and strict type validation to ensure robustness, scalability, and\nsecurity. The architecture, message structure, interface model, and\nadapter-based backend integration strategy of RCP are described, along with\ndeployment practices and applicability across industries including\nmanufacturing, logistics, and healthcare. RCP enables intelligent, resilient,\nand safe robotic operations in complex, multi-agent ecosystems.",
      "pdf_url": "http://arxiv.org/pdf/2506.11650v1",
      "published": "2025-06-13T10:24:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11650v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation",
      "authors": [
        "Yicheng Xiao",
        "Lin Song",
        "Rui Yang",
        "Cheng Cheng",
        "Yixiao Ge",
        "Xiu Li",
        "Ying Shan"
      ],
      "abstract": "Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.11638v1",
      "published": "2025-06-13T10:11:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.11638v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}