{
  "last_updated": "2025-09-04T00:46:04.139721",
  "papers": [
    {
      "title": "The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning",
      "authors": [
        "Yiming Lin",
        "Yuchen Niu",
        "Shang Wang",
        "Kaizhu Huang",
        "Qiufeng Wang",
        "Xiao-Bo Jin"
      ],
      "abstract": "Context recognition (SR) is a fundamental task in computer vision that aims\nto extract structured semantic summaries from images by identifying key events\nand their associated entities. Specifically, given an input image, the model\nmust first classify the main visual events (verb classification), then identify\nthe participating entities and their semantic roles (semantic role labeling),\nand finally localize these entities in the image (semantic role localization).\nExisting methods treat verb classification as a single-label problem, but we\nshow through a comprehensive analysis that this formulation fails to address\nthe inherent ambiguity in visual event recognition, as multiple verb categories\nmay reasonably describe the same image. This paper makes three key\ncontributions: First, we reveal through empirical analysis that verb\nclassification is inherently a multi-label problem due to the ubiquitous\nsemantic overlap between verb categories. Second, given the impracticality of\nfully annotating large-scale datasets with multiple labels, we propose to\nreformulate verb classification as a single positive multi-label learning\n(SPMLL) problem - a novel perspective in SR research. Third, we design a\ncomprehensive multi-label evaluation benchmark for SR that is carefully\ndesigned to fairly evaluate model performance in a multi-label setting. To\naddress the challenges of SPMLL, we futher develop the Graph Enhanced Verb\nMultilayer Perceptron (GE-VerbMLP), which combines graph neural networks to\ncapture label correlations and adversarial training to optimize decision\nboundaries. Extensive experiments on real-world datasets show that our approach\nachieves more than 3\\% MAP improvement while remaining competitive on\ntraditional top-1 and top-5 accuracy metrics.",
      "pdf_url": "http://arxiv.org/pdf/2508.21816v1",
      "published": "2025-08-29T17:51:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21816v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture",
      "authors": [
        "Yeawon Lee",
        "Xiaoyang Wang",
        "Christopher C. Yang"
      ],
      "abstract": "Accurate interpretation of clinical narratives is critical for patient care,\nbut the complexity of these notes makes automation challenging. While Large\nLanguage Models (LLMs) show promise, single-model approaches can lack the\nrobustness required for high-stakes clinical tasks. We introduce a\ncollaborative multi-agent system (MAS) that models a clinical consultation team\nto address this gap. The system is tasked with identifying clinical problems by\nanalyzing only the Subjective (S) and Objective (O) sections of SOAP notes,\nsimulating the diagnostic reasoning process of synthesizing raw data into an\nassessment. A Manager agent orchestrates a dynamically assigned team of\nspecialist agents who engage in a hierarchical, iterative debate to reach a\nconsensus. We evaluated our MAS against a single-agent baseline on a curated\ndataset of 420 MIMIC-III notes. The dynamic multi-agent configuration\ndemonstrated consistently improved performance in identifying congestive heart\nfailure, acute kidney injury, and sepsis. Qualitative analysis of the agent\ndebates reveals that this structure effectively surfaces and weighs conflicting\nevidence, though it can occasionally be susceptible to groupthink. By modeling\na clinical team's reasoning process, our system offers a promising path toward\nmore accurate, robust, and interpretable clinical decision support tools.",
      "pdf_url": "http://arxiv.org/pdf/2508.21803v1",
      "published": "2025-08-29T17:31:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21803v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Tree-Guided Diffusion Planner",
      "authors": [
        "Hyeonseong Jeon",
        "Cheolhong Min",
        "Jaesik Park"
      ],
      "abstract": "Planning with pretrained diffusion models has emerged as a promising approach\nfor solving test-time guided control problems. However, standard gradient\nguidance typically performs optimally under convex and differentiable reward\nlandscapes, showing substantially reduced effectiveness in real-world scenarios\ninvolving non-convex objectives, non-differentiable constraints, and\nmulti-reward structures. Furthermore, recent supervised planning approaches\nrequire task-specific training or value estimators, which limits test-time\nflexibility and zero-shot generalization. We propose a Tree-guided Diffusion\nPlanner (TDP), a zero-shot test-time planning framework that balances\nexploration and exploitation through structured trajectory generation. We frame\ntest-time planning as a tree search problem using a bi-level sampling process:\n(1) diverse parent trajectories are produced via training-free particle\nguidance to encourage broad exploration, and (2) sub-trajectories are refined\nthrough fast conditional denoising guided by task objectives. TDP addresses the\nlimitations of gradient guidance by exploring diverse trajectory regions and\nharnessing gradient information across this expanded solution space using only\npretrained models and test-time reward signals. We evaluate TDP on three\ndiverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze\nmulti-goal exploration. TDP consistently outperforms state-of-the-art\napproaches on all tasks. The project page can be found at:\ntree-diffusion-planner.github.io.",
      "pdf_url": "http://arxiv.org/pdf/2508.21800v1",
      "published": "2025-08-29T17:27:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21800v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "DynaMark: A Reinforcement Learning Framework for Dynamic Watermarking in Industrial Machine Tool Controllers",
      "authors": [
        "Navid Aftabi",
        "Abhishek Hanchate",
        "Satish Bukkapatnam",
        "Dan Li"
      ],
      "abstract": "Industry 4.0's highly networked Machine Tool Controllers (MTCs) are prime\ntargets for replay attacks that use outdated sensor data to manipulate\nactuators. Dynamic watermarking can reveal such tampering, but current schemes\nassume linear-Gaussian dynamics and use constant watermark statistics, making\nthem vulnerable to the time-varying, partly proprietary behavior of MTCs. We\nclose this gap with DynaMark, a reinforcement learning framework that models\ndynamic watermarking as a Markov decision process (MDP). It learns an adaptive\npolicy online that dynamically adapts the covariance of a zero-mean Gaussian\nwatermark using available measurements and detector feedback, without needing\nsystem knowledge. DynaMark maximizes a unique reward function balancing control\nperformance, energy consumption, and detection confidence dynamically. We\ndevelop a Bayesian belief updating mechanism for real-time detection confidence\nin linear systems. This approach, independent of specific system assumptions,\nunderpins the MDP for systems with linear dynamics. On a Siemens Sinumerik 828D\ncontroller digital twin, DynaMark achieves a reduction in watermark energy by\n70% while preserving the nominal trajectory, compared to constant variance\nbaselines. It also maintains an average detection delay equivalent to one\nsampling interval. A physical stepper-motor testbed validates these findings,\nrapidly triggering alarms with less control performance decline and exceeding\nexisting benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2508.21797v1",
      "published": "2025-08-29T17:24:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21797v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "cs.SY",
        "stat.AP"
      ]
    },
    {
      "title": "TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank",
      "authors": [
        "Jiawei Liu",
        "Jiahe Hou",
        "Wei Wang",
        "Jinsong Du",
        "Yang Cong",
        "Huijie Fan"
      ],
      "abstract": "Anomaly detection, which aims to identify anomalies deviating from normal\npatterns, is challenging due to the limited amount of normal data available.\nUnlike most existing unified methods that rely on carefully designed image\nfeature extractors and memory banks to capture logical relationships between\nobjects, we introduce a text memory bank to enhance the detection of logical\nanomalies. Specifically, we propose a Three-Memory framework for Unified\nstructural and logical Anomaly Detection (TMUAD). First, we build a class-level\ntext memory bank for logical anomaly detection by the proposed logic-aware text\nextractor, which can capture rich logical descriptions of objects from input\nimages. Second, we construct an object-level image memory bank that preserves\ncomplete object contours by extracting features from segmented objects. Third,\nwe employ visual encoders to extract patch-level image features for\nconstructing a patch-level memory bank for structural anomaly detection. These\nthree complementary memory banks are used to retrieve and compare normal images\nthat are most similar to the query image, compute anomaly scores at multiple\nlevels, and fuse them into a final anomaly score. By unifying structural and\nlogical anomaly detection through collaborative memory banks, TMUAD achieves\nstate-of-the-art performance across seven publicly available datasets involving\nindustrial and medical domains. The model and code are available at\nhttps://github.com/SIA-IDE/TMUAD.",
      "pdf_url": "http://arxiv.org/pdf/2508.21795v1",
      "published": "2025-08-29T17:22:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21795v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction",
      "authors": [
        "Xiaoyang Wang",
        "Christopher C. Yang"
      ],
      "abstract": "Healthcare systems generate diverse multimodal data, including Electronic\nHealth Records (EHR), clinical notes, and medical images. Effectively\nleveraging this data for clinical prediction is challenging, particularly as\nreal-world samples often present with varied or incomplete modalities. Existing\napproaches typically require complete modality data or rely on manual selection\nstrategies, limiting their applicability in real-world clinical settings where\ndata availability varies across patients and institutions. To address these\nlimitations, we propose MoE-Health, a novel Mixture of Experts framework\ndesigned for robust multimodal fusion in healthcare prediction. MoE-Health\narchitecture is specifically developed to handle samples with differing\nmodalities and improve performance on critical clinical tasks. By leveraging\nspecialized expert networks and a dynamic gating mechanism, our approach\ndynamically selects and combines relevant experts based on available data\nmodalities, enabling flexible adaptation to varying data availability\nscenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical\nclinical prediction tasks: in-hospital mortality prediction, long length of\nstay, and hospital readmission prediction. Experimental results demonstrate\nthat MoE-Health achieves superior performance compared to existing multimodal\nfusion methods while maintaining robustness across different modality\navailability patterns. The framework effectively integrates multimodal\ninformation, offering improved predictive performance and robustness in\nhandling heterogeneous and incomplete healthcare data, making it particularly\nsuitable for deployment in diverse healthcare environments with heterogeneous\ndata availability.",
      "pdf_url": "http://arxiv.org/pdf/2508.21793v1",
      "published": "2025-08-29T17:17:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21793v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval",
      "authors": [
        "Inés Altemir Marinas",
        "Anastasiia Kucherenko",
        "Andrei Kucharavy"
      ],
      "abstract": "Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.21788v1",
      "published": "2025-08-29T17:04:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21788v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "PiCSAR: Probabilistic Confidence Selection And Ranking",
      "authors": [
        "Joshua Ong Jun Leang",
        "Zheng Zhao",
        "Aryo Pradipta Gema",
        "Sohee Yang",
        "Wai-Chung Kwan",
        "Xuanli He",
        "Wenda Li",
        "Pasquale Minervini",
        "Eleonora Giunchiglia",
        "Shay B. Cohen"
      ],
      "abstract": "Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR.",
      "pdf_url": "http://arxiv.org/pdf/2508.21787v1",
      "published": "2025-08-29T17:03:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21787v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight",
      "authors": [
        "Ugur Dinc",
        "Jibak Sarkar",
        "Philipp Schubert",
        "Sabine Semrau",
        "Thomas Weissmann",
        "Andre Karius",
        "Johann Brand",
        "Bernd-Niklas Axer",
        "Ahmed Gomaa",
        "Pluvio Stephan",
        "Ishita Sheth",
        "Sogand Beirami",
        "Annette Schwarz",
        "Udo Gaipl",
        "Benjamin Frey",
        "Christoph Bert",
        "Stefanie Corradini",
        "Rainer Fietkau",
        "Florian Putz"
      ],
      "abstract": "Introduction: Large language models (LLM) have shown great potential in\nclinical decision support. GPT-5 is a novel LLM system that has been\nspecifically marketed towards oncology use.\n  Methods: Performance was assessed using two complementary benchmarks: (i) the\nACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300\nmultiple-choice items, and (ii) a curated set of 60 authentic radiation\noncologic vignettes representing diverse disease sites and treatment\nindications. For the vignette evaluation, GPT-5 was instructed to generate\nconcise therapeutic plans. Four board-certified radiation oncologists rated\ncorrectness, comprehensiveness, and hallucinations. Inter-rater reliability was\nquantified using Fleiss' \\k{appa}.\n  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,\noutperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were\nmost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's\ntreatment recommendations were rated highly for correctness (mean 3.24/4, 95%\nCI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).\nHallucinations were rare with no case reaching majority consensus for their\npresence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for\ncorrectness), reflecting inherent variability in clinical judgment. Errors\nclustered in complex scenarios requiring precise trial knowledge or detailed\nclinical adaptation.\n  Discussion: GPT-5 clearly outperformed prior model variants on the radiation\noncology multiple-choice benchmark. Although GPT-5 exhibited favorable\nperformance in generating real-world radiation oncology treatment\nrecommendations, correctness ratings indicate room for further improvement.\nWhile hallucinations were infrequent, the presence of substantive errors\nunderscores that GPT-5-generated recommendations require rigorous expert\noversight before clinical implementation.",
      "pdf_url": "http://arxiv.org/pdf/2508.21777v1",
      "published": "2025-08-29T16:55:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21777v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering",
      "authors": [
        "Nattapong Kurpukdee",
        "Adrian G. Bors"
      ],
      "abstract": "We propose a realistic scenario for the unsupervised video learning where\nneither task boundaries nor labels are provided when learning a succession of\ntasks. We also provide a non-parametric learning solution for the\nunder-explored problem of unsupervised video continual learning. Videos\nrepresent a complex and rich spatio-temporal media information, widely used in\nmany applications, but which have not been sufficiently explored in\nunsupervised continual learning. Prior studies have only focused on supervised\ncontinual learning, relying on the knowledge of labels and task boundaries,\nwhile having labeled data is costly and not practical. To address this gap, we\nstudy the unsupervised video continual learning (uVCL). uVCL raises more\nchallenges due to the additional computational and memory requirements of\nprocessing videos when compared to images. We introduce a general benchmark\nexperimental protocol for uVCL by considering the learning of unstructured\nvideo data categories during each task. We propose to use the Kernel Density\nEstimation (KDE) of deep embedded video features extracted by unsupervised\nvideo transformer networks as a non-parametric probabilistic representation of\nthe data. We introduce a novelty detection criterion for the incoming new task\ndata, dynamically enabling the expansion of memory clusters, aiming to capture\nnew knowledge when learning a succession of tasks. We leverage the use of\ntransfer learning from the previous tasks as an initial state for the knowledge\ntransfer to the current learning task. We found that the proposed methodology\nsubstantially enhances the performance of the model when successively learning\nmany tasks. We perform in-depth evaluations on three standard video action\nrecognition datasets, including UCF101, HMDB51, and Something-to-Something V2,\nwithout using any labels or class boundaries.",
      "pdf_url": "http://arxiv.org/pdf/2508.21773v1",
      "published": "2025-08-29T16:49:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21773v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Reasoning-Intensive Regression",
      "authors": [
        "Diane Tchuindjo",
        "Omar Khattab"
      ],
      "abstract": "AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR.",
      "pdf_url": "http://arxiv.org/pdf/2508.21762v1",
      "published": "2025-08-29T16:37:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21762v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions",
      "authors": [
        "Timothée Loranchet",
        "Charles K. Assaad"
      ],
      "abstract": "Understanding causal relations between temporal variables is a central\nchallenge in time series analysis, particularly when the full causal structure\nis unknown. Even when the full causal structure cannot be fully specified,\nexperts often succeed in providing a high-level abstraction of the causal\ngraph, known as a summary causal graph, which captures the main causal\nrelations between different time series while abstracting away micro-level\ndetails. In this work, we present conditions that guarantee the orientability\nof micro-level edges between temporal variables given the background knowledge\nencoded in a summary causal graph and assuming having access to a faithful and\ncausally sufficient distribution with respect to the true unknown graph. Our\nresults provide theoretical guarantees for edge orientation at the micro-level,\neven in the presence of cycles or bidirected edges at the macro-level. These\nfindings offer practical guidance for leveraging SCGs to inform causal\ndiscovery in complex temporal systems and highlight the value of incorporating\nexpert knowledge to improve causal inference from observational time series\ndata.",
      "pdf_url": "http://arxiv.org/pdf/2508.21742v1",
      "published": "2025-08-29T16:08:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21742v1",
      "categories": [
        "cs.AI",
        "stat.ME"
      ]
    },
    {
      "title": "Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL",
      "authors": [
        "Hamza Ezzaoui Rahali",
        "Abhilasha Dave",
        "Larry Ruckman",
        "Mohammad Mehdi Rahimifar",
        "Audrey C. Therrien",
        "James J. Russel",
        "Ryan T. Herbst"
      ],
      "abstract": "The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline\nexperiments at rates of up to 1~MHz, with detectors producing data throughputs\nexceeding 1 TB/s. Managing such massive data streams presents significant\nchallenges, as transmission and storage infrastructures become prohibitively\nexpensive. Machine learning (ML) offers a promising solution for real-time data\nreduction, but conventional implementations introduce excessive latency, making\nthem unsuitable for high-speed experimental environments. To address these\nchallenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized\nframework designed to deploy real-time ML inference models on\nField-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to\ndynamically update model weights without requiring FPGA resynthesis, enhancing\nflexibility for adaptive learning applications. To further enhance usability\nand accessibility, we introduce Auto-SNL, a Python extension that streamlines\nthe process of converting Python-based neural network models into\nSNL-compatible high-level synthesis code. This paper presents a benchmark\ncomparison against hls4ml, the current state-of-the-art tool, across multiple\nneural network architectures, fixed-point precisions, and synthesis\nconfigurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL\nachieves competitive or superior latency in most tested architectures, while in\nsome cases also offering FPGA resource savings. This adaptation demonstrates\nSNL's versatility, opening new opportunities for researchers and academics in\nfields such as high-energy physics, medical imaging, robotics, and many more.",
      "pdf_url": "http://arxiv.org/pdf/2508.21739v1",
      "published": "2025-08-29T16:04:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21739v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ]
    },
    {
      "title": "Developer Insights into Designing AI-Based Computer Perception Tools",
      "authors": [
        "Maya Guhan",
        "Meghan E. Hurley",
        "Eric A. Storch",
        "John Herrington",
        "Casey Zampella",
        "Julia Parish-Morris",
        "Gabriel Lázaro-Muñoz",
        "Kristin Kostick-Quenet"
      ],
      "abstract": "Artificial intelligence (AI)-based computer perception (CP) technologies use\nmobile sensors to collect behavioral and physiological data for clinical\ndecision-making. These tools can reshape how clinical knowledge is generated\nand interpreted. However, effective integration of these tools into clinical\nworkflows depends on how developers balance clinical utility with user\nacceptability and trustworthiness. Our study presents findings from 20 in-depth\ninterviews with developers of AI-based CP tools. Interviews were transcribed\nand inductive, thematic analysis was performed to identify 4 key design\npriorities: 1) to account for context and ensure explainability for both\npatients and clinicians; 2) align tools with existing clinical workflows; 3)\nappropriately customize to relevant stakeholders for usability and\nacceptability; and 4) push the boundaries of innovation while aligning with\nestablished paradigms. Our findings highlight that developers view themselves\nas not merely technical architects but also ethical stewards, designing tools\nthat are both acceptable by users and epistemically responsible (prioritizing\nobjectivity and pushing clinical knowledge forward). We offer the following\nsuggestions to help achieve this balance: documenting how design choices around\ncustomization are made, defining limits for customization choices,\ntransparently conveying information about outputs, and investing in user\ntraining. Achieving these goals will require interdisciplinary collaboration\nbetween developers, clinicians, and ethicists.",
      "pdf_url": "http://arxiv.org/pdf/2508.21733v1",
      "published": "2025-08-29T16:01:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21733v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models",
      "authors": [
        "João Valente",
        "Atabak Dehban",
        "Rodrigo Ventura"
      ],
      "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nimpressive capabilities across various multimodal tasks. They continue,\nhowever, to struggle with trivial scenarios such as reading values from Digital\nMeasurement Devices (DMDs), particularly in real-world conditions involving\nclutter, occlusions, extreme viewpoints, and motion blur; common in\nhead-mounted cameras and Augmented Reality (AR) applications. Motivated by\nthese limitations, this work introduces CAD2DMD-SET, a synthetic data\ngeneration tool designed to support visual question answering (VQA) tasks\ninvolving DMDs. By leveraging 3D CAD models, advanced rendering, and\nhigh-fidelity image composition, our tool produces diverse, VQA-labelled\nsynthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present\nDMDBench, a curated validation set of 1,000 annotated real-world images\ndesigned to evaluate model performance under practical constraints.\nBenchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein\nSimilarity (ANLS) and further fine-tuning LoRA's of these models with\nCAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL\nshowcasing a score increase of 200% without degrading on other tasks. This\ndemonstrates that the CAD2DMD-SET training dataset substantially improves the\nrobustness and performance of LVLMs when operating under the previously stated\nchallenging conditions. The CAD2DMD-SET tool is expected to be released as\nopen-source once the final version of this manuscript is prepared, allowing the\ncommunity to add different measurement devices and generate their own datasets.",
      "pdf_url": "http://arxiv.org/pdf/2508.21732v1",
      "published": "2025-08-29T15:57:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21732v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem",
      "authors": [
        "Fabrizio Fagiolo",
        "Nicolo' Vescera"
      ],
      "abstract": "In this paper we present a variational algorithm for the Traveling Salesman\nProblem (TSP) that combines (i) a compact encoding of permutations, which\nreduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:\nwhere the circuit topology (``Ansatz'') is first optimized on a training\ninstance by Simulated Annealing (SA), then ``frozen'' and re-used on novel\ninstances, limited to a rapid re-optimization of only the circuit parameters.\nThis pipeline eliminates costly structural research in testing, making the\nprocedure immediately implementable on NISQ hardware.\n  On a set of $40$ randomly generated symmetric instances that span $4 - 7$\ncities, the resulting Ansatz achieves an average optimal trip sampling\nprobability of $100\\%$ for 4 city cases, $90\\%$ for 5 city cases and $80\\%$ for\n6 city cases. With 7 cities the success rate drops markedly to an average of\n$\\sim 20\\%$, revealing the onset of scalability limitations of the proposed\nmethod.\n  The results show robust generalization ability for moderate problem sizes and\nindicate how freezing the Ansatz can dramatically reduce time-to-solution\nwithout degrading solution quality. The paper also discusses scalability\nlimitations, the impact of ``warm-start'' initialization of parameters, and\nprospects for extension to more complex problems, such as Vehicle Routing and\nJob-Shop Scheduling.",
      "pdf_url": "http://arxiv.org/pdf/2508.21730v1",
      "published": "2025-08-29T15:56:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21730v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization",
      "authors": [
        "Jiazheng Xing",
        "Hai Ci",
        "Hongbin Xu",
        "Hangjie Yuan",
        "Yong Liu",
        "Mike Zheng Shou"
      ],
      "abstract": "Watermarking diffusion-generated images is crucial for copyright protection\nand user tracking. However, current diffusion watermarking methods face\nsignificant limitations: zero-bit watermarking systems lack the capacity for\nlarge-scale user tracking, while multi-bit methods are highly sensitive to\ncertain image transformations or generative attacks, resulting in a lack of\ncomprehensive robustness. In this paper, we propose OptMark, an\noptimization-based approach that embeds a robust multi-bit watermark into the\nintermediate latents of the diffusion denoising process. OptMark strategically\ninserts a structural watermark early to resist generative attacks and a detail\nwatermark late to withstand image transformations, with tailored regularization\nterms to preserve image quality and ensure imperceptibility. To address the\nchallenge of memory consumption growing linearly with the number of denoising\nsteps during optimization, OptMark incorporates adjoint gradient methods,\nreducing memory usage from O(N) to O(1). Experimental results demonstrate that\nOptMark achieves invisible multi-bit watermarking while ensuring robust\nresilience against valuemetric transformations, geometric transformations,\nediting, and regeneration attacks.",
      "pdf_url": "http://arxiv.org/pdf/2508.21727v1",
      "published": "2025-08-29T15:50:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21727v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster Generation",
      "authors": [
        "Jiho Choi",
        "Seojeong Park",
        "Seongjong Song",
        "Hyunjung Shim"
      ],
      "abstract": "We present a novel training-free framework, \\textit{PosterForest}, for\nautomated scientific poster generation. Unlike prior approaches, which largely\nneglect the hierarchical structure of scientific documents and the semantic\nintegration of textual and visual elements, our method addresses both\nchallenges directly. We introduce the \\textit{Poster Tree}, a hierarchical\nintermediate representation that jointly encodes document structure and\nvisual-textual relationships at multiple levels. Our framework employs a\nmulti-agent collaboration strategy, where agents specializing in content\nsummarization and layout planning iteratively coordinate and provide mutual\nfeedback. This approach enables the joint optimization of logical consistency,\ncontent fidelity, and visual coherence. Extensive experiments on multiple\nacademic domains show that our method outperforms existing baselines in both\nqualitative and quantitative evaluations. The resulting posters achieve quality\nclosest to expert-designed ground truth and deliver superior information\npreservation, structural clarity, and user preference.",
      "pdf_url": "http://arxiv.org/pdf/2508.21720v1",
      "published": "2025-08-29T15:36:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21720v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Entropy-Based Non-Invasive Reliability Monitoring of Convolutional Neural Networks",
      "authors": [
        "Amirhossein Nazeri",
        "Wael Hafez"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) have become the foundation of modern\ncomputer vision, achieving unprecedented accuracy across diverse image\nrecognition tasks. While these networks excel on in-distribution data, they\nremain vulnerable to adversarial perturbations imperceptible input\nmodifications that cause misclassification with high confidence. However,\nexisting detection methods either require expensive retraining, modify network\narchitecture, or degrade performance on clean inputs. Here we show that\nadversarial perturbations create immediate, detectable entropy signatures in\nCNN activations that can be monitored without any model modification. Using\nparallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs\nconsistently shift activation entropy by 7% in early convolutional layers,\nenabling 90% detection accuracy with false positives and false negative rates\nbelow 20%. The complete separation between clean and adversarial entropy\ndistributions reveals that CNNs inherently encode distribution shifts in their\nactivation patterns. This work establishes that CNN reliability can be assessed\nthrough activation entropy alone, enabling practical deployment of\nself-diagnostic vision systems that detect adversarial inputs in real-time\nwithout compromising original model performance.",
      "pdf_url": "http://arxiv.org/pdf/2508.21715v1",
      "published": "2025-08-29T15:33:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21715v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.IT",
        "eess.IV",
        "math.IT"
      ]
    },
    {
      "title": "Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR",
      "authors": [
        "Shashank Vempati",
        "Nishit Anand",
        "Gaurav Talebailkar",
        "Arpan Garai",
        "Chetan Arora"
      ],
      "abstract": "Conventional optical character recognition (OCR) techniques segmented each\ncharacter and then recognized. This made them prone to error in character\nsegmentation, and devoid of context to exploit language models. Advances in\nsequence to sequence translation in last decade led to modern techniques first\ndetecting words and then inputting one word at a time to a model to directly\noutput full words as sequence of characters. This allowed better utilization of\nlanguage models and bypass error-prone character segmentation step. We observe\nthat the above transition in style has moved the bottleneck in accuracy to word\nsegmentation. Hence, in this paper, we propose a natural and logical\nprogression from word level OCR to line-level OCR. The proposal allows to\nbypass errors in word detection, and provides larger sentence context for\nbetter utilization of language models. We show that the proposed technique not\nonly improves the accuracy but also efficiency of OCR. Despite our thorough\nliterature survey, we did not find any public dataset to train and benchmark\nsuch shift from word to line-level OCR. Hence, we also contribute a\nmeticulously curated dataset of 251 English page images with line-level\nannotations. Our experimentation revealed a notable end-to-end accuracy\nimprovement of 5.4%, underscoring the potential benefits of transitioning\ntowards line-level OCR, especially for document images. We also report a 4\ntimes improvement in efficiency compared to word-based pipelines. With\ncontinuous improvements in large language models, our methodology also holds\npotential to exploit such advances. Project Website:\nhttps://nishitanand.github.io/line-level-ocr-website",
      "pdf_url": "http://arxiv.org/pdf/2508.21693v1",
      "published": "2025-08-29T15:02:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21693v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education",
      "authors": [
        "Imran S. A. Khan",
        "Emmanuel G. Blanchard",
        "Sébastien George"
      ],
      "abstract": "This paper introduces the Future Atmospheric Conditions Training System\n(FACTS), a novel platform that advances climate resilience education through\nplace-based, adaptive learning experiences. FACTS combines real-time\natmospheric data collected by IoT sensors with curated resources from a\nKnowledge Base to dynamically generate localized learning challenges. Learner\nresponses are analyzed by a Generative AI powered server, which delivers\npersonalized feedback and adaptive support. Results from a user evaluation\nindicate that participants found the system both easy to use and effective for\nbuilding knowledge related to climate resilience. These findings suggest that\nintegrating IoT and Generative AI into atmospherically adaptive learning\ntechnologies holds significant promise for enhancing educational engagement and\nfostering climate awareness.",
      "pdf_url": "http://arxiv.org/pdf/2508.21666v1",
      "published": "2025-08-29T14:30:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21666v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "title": "Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI",
      "authors": [
        "Farhad Abtahi",
        "Mehdi Astaraki",
        "Fernando Seoane"
      ],
      "abstract": "Bias in medical artificial intelligence is conventionally viewed as a defect\nrequiring elimination. However, human reasoning inherently incorporates biases\nshaped by education, culture, and experience, suggesting their presence may be\ninevitable and potentially valuable. We propose MEDLEY (Medical Ensemble\nDiagnostic system with Leveraged diversitY), a conceptual framework that\norchestrates multiple AI models while preserving their diverse outputs rather\nthan collapsing them into a consensus. Unlike traditional approaches that\nsuppress disagreement, MEDLEY documents model-specific biases as potential\nstrengths and treats hallucinations as provisional hypotheses for clinician\nverification. A proof-of-concept demonstrator was developed using over 30 large\nlanguage models, creating a minimum viable product that preserved both\nconsensus and minority views in synthetic cases, making diagnostic uncertainty\nand latent biases transparent for clinical oversight. While not yet a validated\nclinical tool, the demonstration illustrates how structured diversity can\nenhance medical reasoning under clinician supervision. By reframing AI\nimperfection as a resource, MEDLEY offers a paradigm shift that opens new\nregulatory, ethical, and innovation pathways for developing trustworthy medical\nAI systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.21648v1",
      "published": "2025-08-29T14:12:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21648v1",
      "categories": [
        "cs.AI",
        "68T07, 68T09, 68T20 (Primary) 62P10, 62C20, 62H30 (Secondary)"
      ]
    },
    {
      "title": "A-MHA*: Anytime Multi-Heuristic A*",
      "authors": [
        "Ramkumar Natarajan",
        "Muhammad Suhail Saleem",
        "William Xiao",
        "Sandip Aine",
        "Howie Choset",
        "Maxim Likhachev"
      ],
      "abstract": "Designing good heuristic functions for graph search requires adequate domain\nknowledge. It is often easy to design heuristics that perform well and\ncorrelate with the underlying true cost-to-go values in certain parts of the\nsearch space but these may not be admissible throughout the domain thereby\naffecting the optimality guarantees of the search. Bounded suboptimal search\nusing several such partially good but inadmissible heuristics was developed in\nMulti-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible\nheuristics to potentially generate a faster suboptimal solution, the original\nversion does not improve the solution over time. It is a one shot algorithm\nthat requires careful setting of inflation factors to obtain a desired one time\nsolution. In this work, we tackle this issue by extending MHA* to an anytime\nversion that finds a feasible suboptimal solution quickly and continually\nimproves it until time runs out. Our work is inspired from the Anytime\nRepairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*\nconcepts in the MHA* framework preserves the original suboptimal and\ncompleteness guarantees and enhances MHA* to perform in an anytime fashion.\nFurthermore, we report the performance of A-MHA* in 3-D path planning domain\nand sliding tiles puzzle and compare against MHA* and other anytime algorithms.",
      "pdf_url": "http://arxiv.org/pdf/2508.21637v1",
      "published": "2025-08-29T14:00:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21637v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "QZhou-Embedding Technical Report",
      "authors": [
        "Peng Yu",
        "En Xu",
        "Bin Chen",
        "Haibiao Chen",
        "Yinfei Xu"
      ],
      "abstract": "We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub.",
      "pdf_url": "http://arxiv.org/pdf/2508.21632v1",
      "published": "2025-08-29T13:47:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21632v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study",
      "authors": [
        "Saravanan Venkatachalam"
      ],
      "abstract": "This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making.",
      "pdf_url": "http://arxiv.org/pdf/2508.21622v1",
      "published": "2025-08-29T13:34:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21622v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Physics-Informed Spectral Modeling for Hyperspectral Imaging",
      "authors": [
        "Zuzanna Gawrysiak",
        "Krzysztof Krawiec"
      ],
      "abstract": "We present PhISM, a physics-informed deep learning architecture that learns\nwithout supervision to explicitly disentangle hyperspectral observations and\nmodel them with continuous basis functions. \\mname outperforms prior methods on\nseveral classification and regression benchmarks, requires limited labeled\ndata, and provides additional insights thanks to interpretable latent\nrepresentation.",
      "pdf_url": "http://arxiv.org/pdf/2508.21618v1",
      "published": "2025-08-29T13:32:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21618v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6; I.2.10; J.2"
      ]
    },
    {
      "title": "Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics",
      "authors": [
        "Yang You",
        "Alex Schutz",
        "Zhikun Li",
        "Bruno Lacerda",
        "Robert Skilton",
        "Nick Hawes"
      ],
      "abstract": "Many high-level multi-agent planning problems, including multi-robot\nnavigation and path planning, can be effectively modeled using deterministic\nactions and observations.\n  In this work, we focus on such domains and introduce the class of\nDeterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass of\nDec-POMDPs characterized by deterministic transitions and observations\nconditioned on the state and joint actions.\n  We then propose a practical solver called Iterative Deterministic POMDP\nPlanning (IDPP). This method builds on the classic Joint Equilibrium Search for\nPolicies framework and is specifically optimized to handle large-scale\nDet-Dec-POMDPs that current Dec-POMDP solvers are unable to address\nefficiently.",
      "pdf_url": "http://arxiv.org/pdf/2508.21595v1",
      "published": "2025-08-29T12:50:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21595v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning",
      "authors": [
        "Zinan Tang",
        "Xin Gao",
        "Qizhi Pei",
        "Zhuoshi Pan",
        "Mengzhang Cai",
        "Jiang Wu",
        "Conghui He",
        "Lijun Wu"
      ],
      "abstract": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon.",
      "pdf_url": "http://arxiv.org/pdf/2508.21589v1",
      "published": "2025-08-29T12:47:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21589v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Survey on Current Trends and Recent Advances in Text Anonymization",
      "authors": [
        "Tobias Deußer",
        "Lorenz Sparrenberg",
        "Armin Berger",
        "Max Hahnbück",
        "Christian Bauckhage",
        "Rafet Sifa"
      ],
      "abstract": "The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field.",
      "pdf_url": "http://arxiv.org/pdf/2508.21587v1",
      "published": "2025-08-29T12:43:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21587v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "NSPDI-SNN: An efficient lightweight SNN based on nonlinear synaptic pruning and dendritic integration",
      "authors": [
        "Wuque Cai",
        "Hongze Sun",
        "Jiayi He",
        "Qianqian Liao",
        "Yunliang Zang",
        "Duo Chen",
        "Dezhong Yao",
        "Daqing Guo"
      ],
      "abstract": "Spiking neural networks (SNNs) are artificial neural networks based on\nsimulated biological neurons and have attracted much attention in recent\nartificial intelligence technology studies. The dendrites in biological neurons\nhave efficient information processing ability and computational power; however,\nthe neurons of SNNs rarely match the complex structure of the dendrites.\nInspired by the nonlinear structure and highly sparse properties of neuronal\ndendrites, in this study, we propose an efficient, lightweight SNN method with\nnonlinear pruning and dendritic integration (NSPDI-SNN). In this method, we\nintroduce nonlinear dendritic integration (NDI) to improve the representation\nof the spatiotemporal information of neurons. We implement heterogeneous state\ntransition ratios of dendritic spines and construct a new and flexible\nnonlinear synaptic pruning (NSP) method to achieve the high sparsity of SNN. We\nconducted systematic experiments on three benchmark datasets (DVS128 Gesture,\nCIFAR10-DVS, and CIFAR10) and extended the evaluation to two complex tasks\n(speech recognition and reinforcement learning-based maze navigation task).\nAcross all tasks, NSPDI-SNN consistently achieved high sparsity with minimal\nperformance degradation. In particular, our method achieved the best\nexperimental results on all three event stream datasets. Further analysis\nshowed that NSPDI significantly improved the efficiency of synaptic information\ntransfer as sparsity increased. In conclusion, our results indicate that the\ncomplex structure and nonlinear computation of neuronal dendrites provide a\npromising approach for developing efficient SNN methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.21566v1",
      "published": "2025-08-29T12:22:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21566v1",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances",
      "authors": [
        "Issa Hanou",
        "Sebastijan Dumančić",
        "Mathijs de Weerdt"
      ],
      "abstract": "We propose a new framework for discovering landmarks that automatically\ngeneralize across a domain. These generalized landmarks are learned from a set\nof solved instances and describe intermediate goals for planning problems where\ntraditional landmark extraction algorithms fall short. Our generalized\nlandmarks extend beyond the predicates of a domain by using state functions\nthat are independent of the objects of a specific problem and apply to all\nsimilar objects, thus capturing repetition. Based on these functions, we\nconstruct a directed generalized landmark graph that defines the landmark\nprogression, including loop possibilities for repetitive subplans. We show how\nto use this graph in a heuristic to solve new problem instances of the same\ndomain. Our results show that the generalized landmark graphs learned from a\nfew small instances are also effective for larger instances in the same domain.\nIf a loop that indicates repetition is identified, we see a significant\nimprovement in heuristic performance over the baseline. Generalized landmarks\ncapture domain information that is interpretable and useful to an automated\nplanner. This information can be discovered from a small set of plans for the\nsame domain.",
      "pdf_url": "http://arxiv.org/pdf/2508.21564v1",
      "published": "2025-08-29T12:21:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21564v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Limitations of Physics-Informed Neural Networks: a Study on Smart Grid Surrogation",
      "authors": [
        "Julen Cestero",
        "Carmine Delle Femine",
        "Kenji S. Muro",
        "Marco Quartulli",
        "Marcello Restelli"
      ],
      "abstract": "Physics-Informed Neural Networks (PINNs) present a transformative approach\nfor smart grid modeling by integrating physical laws directly into learning\nframeworks, addressing critical challenges of data scarcity and physical\nconsistency in conventional data-driven methods. This paper evaluates PINNs'\ncapabilities as surrogate models for smart grid dynamics, comparing their\nperformance against XGBoost, Random Forest, and Linear Regression across three\nkey experiments: interpolation, cross-validation, and episodic trajectory\nprediction. By training PINNs exclusively through physics-based loss functions\n(enforcing power balance, operational constraints, and grid stability) we\ndemonstrate their superior generalization, outperforming data-driven models in\nerror reduction. Notably, PINNs maintain comparatively lower MAE in dynamic\ngrid operations, reliably capturing state transitions in both random and\nexpert-driven control scenarios, while traditional models exhibit erratic\nperformance. Despite slight degradation in extreme operational regimes, PINNs\nconsistently enforce physical feasibility, proving vital for safety-critical\napplications. Our results contribute to establishing PINNs as a\nparadigm-shifting tool for smart grid surrogation, bridging data-driven\nflexibility with first-principles rigor. This work advances real-time grid\ncontrol and scalable digital twins, emphasizing the necessity of physics-aware\narchitectures in mission-critical energy systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.21559v1",
      "published": "2025-08-29T12:15:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21559v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting",
      "authors": [
        "Yujin Park",
        "Haejun Chung",
        "Ikbeom Jang"
      ],
      "abstract": "Pairwise comparison is often favored over absolute rating or ordinal\nclassification in subjective or difficult annotation tasks due to its improved\nreliability. However, exhaustive comparisons require a massive number of\nannotations (O(n^2)). Recent work has greatly reduced the annotation burden\n(O(n log n)) by actively sampling pairwise comparisons using a sorting\nalgorithm. We further improve annotation efficiency by (1) roughly pre-ordering\nitems using the Contrastive Language-Image Pre-training (CLIP) model\nhierarchically without training, and (2) replacing easy, obvious human\ncomparisons with automated comparisons. The proposed EZ-Sort first produces a\nCLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,\nand finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation\nwas conducted using various datasets: face-age estimation (FGNET), historical\nimage chronology (DHCI), and retinal image quality assessment (EyePACS). It\nshowed that EZ-Sort reduced human annotation cost by 90.5% compared to\nexhaustive pairwise comparisons and by 19.8% compared to prior work (when n =\n100), while improving or maintaining inter-rater reliability. These results\ndemonstrate that combining CLIP-based priors with uncertainty-aware sampling\nyields an efficient and scalable solution for pairwise ranking.",
      "pdf_url": "http://arxiv.org/pdf/2508.21550v1",
      "published": "2025-08-29T12:06:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21550v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T05, 68T09",
        "I.5.4"
      ]
    },
    {
      "title": "What Data is Really Necessary? A Feasibility Study of Inference Data Minimization for Recommender Systems",
      "authors": [
        "Jens Leysen",
        "Marco Favier",
        "Bart Goethals"
      ],
      "abstract": "Data minimization is a legal principle requiring personal data processing to\nbe limited to what is necessary for a specified purpose. Operationalizing this\nprinciple for recommender systems, which rely on extensive personal data,\nremains a significant challenge. This paper conducts a feasibility study on\nminimizing implicit feedback inference data for such systems. We propose a\nnovel problem formulation, analyze various minimization techniques, and\ninvestigate key factors influencing their effectiveness. We demonstrate that\nsubstantial inference data reduction is technically feasible without\nsignificant performance loss. However, its practicality is critically\ndetermined by two factors: the technical setting (e.g., performance targets,\nchoice of model) and user characteristics (e.g., history size, preference\ncomplexity). Thus, while we establish its technical feasibility, we conclude\nthat data minimization remains practically challenging and its dependence on\nthe technical and user context makes a universal standard for data `necessity'\ndifficult to implement.",
      "pdf_url": "http://arxiv.org/pdf/2508.21547v1",
      "published": "2025-08-29T12:01:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21547v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion Models",
      "authors": [
        "Ziwei Liao",
        "Mohamed Sayed",
        "Steven L. Waslander",
        "Sara Vicente",
        "Daniyar Turmukhambetov",
        "Michael Firman"
      ],
      "abstract": "Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings.",
      "pdf_url": "http://arxiv.org/pdf/2508.21542v1",
      "published": "2025-08-29T11:55:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21542v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining",
      "authors": [
        "Eduardo Illueca-Fernandez",
        "Kaile Chen",
        "Fernando Seoane",
        "Farhad Abtahi"
      ],
      "abstract": "Process mining has emerged as a powerful analytical technique for\nunderstanding complex healthcare workflows. However, its application faces\nsignificant barriers, including technical complexity, a lack of standardized\napproaches, and limited access to practical training resources. We introduce\nHealthProcessAI, a GenAI framework designed to simplify process mining\napplications in healthcare and epidemiology by providing a comprehensive\nwrapper around existing Python (PM4PY) and R (bupaR) libraries. To address\nunfamiliarity and improve accessibility, the framework integrates multiple\nLarge Language Models (LLMs) for automated process map interpretation and\nreport generation, helping translate technical analyses into outputs that\ndiverse users can readily understand. We validated the framework using sepsis\nprogression data as a proof-of-concept example and compared the outputs of five\nstate-of-the-art LLM models through the OpenRouter platform. To test its\nfunctionality, the framework successfully processed sepsis data across four\nproof-of-concept scenarios, demonstrating robust technical performance and its\ncapability to generate reports through automated LLM analysis. LLM evaluation\nusing five independent LLMs as automated evaluators revealed distinct model\nstrengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency\nscores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By\nintegrating multiple Large Language Models (LLMs) for automated interpretation\nand report generation, the framework addresses widespread unfamiliarity with\nprocess mining outputs, making them more accessible to clinicians, data\nscientists, and researchers. This structured analytics and AI-driven\ninterpretation combination represents a novel methodological advance in\ntranslating complex process mining results into potentially actionable insights\nfor healthcare applications.",
      "pdf_url": "http://arxiv.org/pdf/2508.21540v1",
      "published": "2025-08-29T11:53:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21540v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Counterfactual Scenarios for Automated Planning",
      "authors": [
        "Nicola Gigante",
        "Francesco Leofante",
        "Andrea Micheli"
      ],
      "abstract": "Counterfactual Explanations (CEs) are a powerful technique used to explain\nMachine Learning models by showing how the input to a model should be minimally\nchanged for the model to produce a different output. Similar proposals have\nbeen made in the context of Automated Planning, where CEs have been\ncharacterised in terms of minimal modifications to an existing plan that would\nresult in the satisfaction of a different goal. While such explanations may\nhelp diagnose faults and reason about the characteristics of a plan, they fail\nto capture higher-level properties of the problem being solved. To address this\nlimitation, we propose a novel explanation paradigm that is based on\ncounterfactual scenarios. In particular, given a planning problem $P$ and an\n\\ltlf formula $\\psi$ defining desired properties of a plan, counterfactual\nscenarios identify minimal modifications to $P$ such that it admits plans that\ncomply with $\\psi$. In this paper, we present two qualitative instantiations of\ncounterfactual scenarios based on an explicit quantification over plans that\nmust satisfy $\\psi$. We then characterise the computational complexity of\ngenerating such counterfactual scenarios when different types of changes are\nallowed on $P$. We show that producing counterfactual scenarios is often only\nas expensive as computing a plan for $P$, thus demonstrating the practical\nviability of our proposal and ultimately providing a framework to construct\npractical algorithms in this area.",
      "pdf_url": "http://arxiv.org/pdf/2508.21521v1",
      "published": "2025-08-29T11:16:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21521v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis",
      "authors": [
        "Sweta Kaman",
        "Ankita Sharma",
        "Romi Banerjee"
      ],
      "abstract": "Background: Wisdom is a superordinate construct that embraces perspective\ntaking, reflectiveness, prosocial orientation, reflective empathetic action,\nand intellectual humility. Unlike conventional models of reasoning that are\nrigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,\nrequiring both graded evaluation and self-reflective humility. Current measures\ndepend on self-reports and seldom reflect the humility and uncertainty inherent\nin wise reasoning. A computational framework that takes into account both\nmultidimensionality and confidence has the potential to improve psychological\nscience and allow humane AI. Method: We present a fuzzy inference system with Z\nnumbers, each of the decisions being expressed in terms of a wisdom score\n(restriction) and confidence score (certainty). As part of this study,\nparticipants (N = 100) were exposed to culturally neutral pictorial moral\ndilemma tasks to which they generated think-aloud linguistic responses, which\nwere mapped into five theoretically based components of wisdom. The scores of\neach individual component were combined using a base of 21 rules, with\nmembership functions tuned via Gaussian kernel density estimation. Results: In\na proof of concept study, the system produced dual attribute wisdom\nrepresentations that correlated modestly but significantly with established\nscales while showing negligible relations with unrelated traits, supporting\nconvergent and divergent validity. Contribution: The contribution is to\nformalize wisdom as a multidimensional, uncertainty-conscious construct,\noperationalized in the form of Z-numbers. In addition to progressing\nmeasurement in psychology, it calculates how fuzzy Z numbers can provide AI\nsystems with interpretable, confidence-sensitive reasoning that affords a safe,\nmiddle ground between rigorous computation and human-like judgment.",
      "pdf_url": "http://arxiv.org/pdf/2508.21517v1",
      "published": "2025-08-29T11:03:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21517v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature",
      "authors": [
        "Geri Skenderi"
      ],
      "abstract": "Graph Neural Networks (GNNs) have recently shown promise as solvers for\nBoolean Satisfiability Problems (SATs) by operating on graph representations of\nlogical formulas. However, their performance degrades sharply on harder\ninstances, raising the question of whether this reflects fundamental\narchitectural limitations. In this work, we provide a geometric explanation\nthrough the lens of graph Ricci Curvature (RC), which quantifies local\nconnectivity bottlenecks. We prove that bipartite graphs derived from random\nk-SAT formulas are inherently negatively curved, and that this curvature\ndecreases with instance difficulty. Building on this, we show that GNN-based\nSAT solvers are affected by oversquashing, a phenomenon where long-range\ndependencies become impossible to compress into fixed-length representations.\nWe validate our claims empirically across different SAT benchmarks and confirm\nthat curvature is both a strong indicator of problem complexity and can be used\nto predict performance. Finally, we connect our findings to design principles\nof existing solvers and outline promising directions for future work.",
      "pdf_url": "http://arxiv.org/pdf/2508.21513v1",
      "published": "2025-08-29T10:54:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21513v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding",
      "authors": [
        "Hao Lu",
        "Jiahao Wang",
        "Yaolun Zhang",
        "Ruohui Wang",
        "Xuanyu Zheng",
        "Yepeng Tang",
        "Dahua Lin",
        "Lewei Lu"
      ],
      "abstract": "Video multimodal large language models (Video-MLLMs) have achieved remarkable\nprogress in video understanding. However, they remain vulnerable to\nhallucination-producing content inconsistent with or unrelated to video inputs.\nPrevious video hallucination benchmarks primarily focus on short-videos. They\nattribute hallucinations to factors such as strong language priors, missing\nframes, or vision-language biases introduced by the visual encoder. While these\ncauses indeed account for most hallucinations in short videos, they still\noversimplify the cause of hallucinations. Sometimes, models generate incorrect\noutputs but with correct frame-level semantics. We refer to this type of\nhallucination as Semantic Aggregation Hallucination (SAH), which arises during\nthe process of aggregating frame-level semantics into event-level semantic\ngroups. Given that SAH becomes particularly critical in long videos due to\nincreased semantic complexity across multiple events, it is essential to\nseparate and thoroughly investigate the causes of this type of hallucination.\nTo address the above issues, we introduce ELV-Halluc, the first benchmark\ndedicated to long-video hallucination, enabling a systematic investigation of\nSAH. Our experiments confirm the existence of SAH and show that it increases\nwith semantic complexity. Additionally, we find that models are more prone to\nSAH on rapidly changing semantics. Moreover, we discuss potential approaches to\nmitigate SAH. We demonstrate that positional encoding strategy contributes to\nalleviating SAH, and further adopt DPO strategy to enhance the model's ability\nto distinguish semantics within and across events. To support this, we curate a\ndataset of 8K adversarial data pairs and achieve improvements on both\nELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.",
      "pdf_url": "http://arxiv.org/pdf/2508.21496v2",
      "published": "2025-08-29T10:25:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21496v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning",
      "authors": [
        "Pascal R. van der Vaart",
        "Neil Yorke-Smith",
        "Matthijs T. J. Spaan"
      ],
      "abstract": "Uncertainty quantification in reinforcement learning can greatly improve\nexploration and robustness. Approximate Bayesian approaches have recently been\npopularized to quantify uncertainty in model-free algorithms. However, so far\nthe focus has been on improving the accuracy of the posterior approximation,\ninstead of studying the accuracy of the prior and likelihood assumptions\nunderlying the posterior. In this work, we demonstrate that there is a cold\nposterior effect in Bayesian deep Q-learning, where contrary to theory,\nperformance increases when reducing the temperature of the posterior. To\nidentify and overcome likely causes, we challenge common assumptions made on\nthe likelihood and priors in Bayesian model-free algorithms. We empirically\nstudy prior distributions and show through statistical tests that the common\nGaussian likelihood assumption is frequently violated. We argue that developing\nmore suitable likelihoods and priors should be a key focus in future Bayesian\nreinforcement learning research and we offer simple, implementable solutions\nfor better priors in deep Q-learning that lead to more performant Bayesian\nalgorithms.",
      "pdf_url": "http://arxiv.org/pdf/2508.21488v1",
      "published": "2025-08-29T10:12:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21488v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble",
      "authors": [
        "Sara B. Coutinho",
        "Rafael M. O. Cruz",
        "Francimaria R. S. Nascimento",
        "George D. C. Cavalcanti"
      ],
      "abstract": "Psychological biases, such as confirmation bias, make individuals\nparticularly vulnerable to believing and spreading fake news on social media,\nleading to significant consequences in domains such as public health and\npolitics. Machine learning-based fact-checking systems have been widely studied\nto mitigate this problem. Among them, ensemble methods are particularly\neffective in combining multiple classifiers to improve robustness. However,\ntheir performance heavily depends on the diversity of the constituent\nclassifiers-selecting genuinely diverse models remains a key challenge,\nespecially when models tend to learn redundant patterns. In this work, we\npropose a novel automatic classifier selection approach that prioritizes\ndiversity, also extended by performance. The method first computes pairwise\ndiversity between classifiers and applies hierarchical clustering to organize\nthem into groups at different levels of granularity. A HierarchySelect then\nexplores these hierarchical levels to select one pool of classifiers per level,\neach representing a distinct intra-pool diversity. The most diverse pool is\nidentified and selected for ensemble construction from these. The selection\nprocess incorporates an evaluation metric reflecting each classifiers's\nperformance to ensure the ensemble also generalises well. We conduct\nexperiments with 40 heterogeneous classifiers across six datasets from\ndifferent application domains and with varying numbers of classes. Our method\nis compared against the Elbow heuristic and state-of-the-art baselines. Results\nshow that our approach achieves the highest accuracy on two of six datasets.\nThe implementation details are available on the project's repository:\nhttps://github.com/SaraBCoutinho/HSFN .",
      "pdf_url": "http://arxiv.org/pdf/2508.21482v1",
      "published": "2025-08-29T10:09:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21482v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards",
      "authors": [
        "Xiaolong Wei",
        "Bo Lu",
        "Xingyu Zhang",
        "Zhejun Zhao",
        "Dongdong Shen",
        "Long Xia",
        "Dawei Yin"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable creative writing\ncapabilities, yet their substantial computational demands hinder widespread\nuse. Enhancing Small Language Models (SLMs) offers a promising alternative, but\ncurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, and\nReinforcement Learning from Human Feedback (RLHF) is costly. This paper\nexplores two distinct AI-driven reward strategies within a Reinforcement\nLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a\n7B-parameter SLM, specifically for generating Chinese greetings. The first\nstrategy employs a RM trained on high-quality preference data curated by a\nnovel multi-agent rejection sampling framework designed for creative tasks. The\nsecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose\nreward function is optimized via an adversarial training scheme with a\nreflection mechanism, to directly provide reward signals. Comprehensive\nexperiments reveal that while both approaches significantly enhance creative\noutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yields\nsuperior generation quality. Furthermore, it offers notable advantages in\ntraining efficiency and reduced dependency on human-annotated data, presenting\na more scalable and effective path towards creative SLMs. Our automated\nevaluation methods also exhibit strong alignment with human judgments. Our code\nand data are publicly available at\nhttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.",
      "pdf_url": "http://arxiv.org/pdf/2508.21476v1",
      "published": "2025-08-29T10:00:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21476v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents",
      "authors": [
        "Xijia Tao",
        "Yihua Teng",
        "Xinxing Su",
        "Xinyu Fu",
        "Jihao Wu",
        "Chaofan Tao",
        "Ziru Liu",
        "Haoli Bai",
        "Rui Liu",
        "Lingpeng Kong"
      ],
      "abstract": "Large multimodal language models (MLLMs) are increasingly deployed as web\nagents, yet many multimodal browsing benchmarks can be solved by shallow, fixed\nworkflows that lean on high-recall image search and nearby text-masking the\ngenuinely multimodal challenges of fine-grained visual reasoning, provenance\nverification, and long-horizon tool use. We introduce MMSearch-Plus, a\nbenchmark of 311 tasks that highly demand multimodal understanding while\npreserving the difficulty profile of strong text-only browsing suites. Each\nitem is constructed to contain multiple weak, localized visual signals that\nmust be extracted, propagated through iterative text-image search, and\ncross-validated under retrieval noise before answering. Our curation procedure,\nSpatial-Temporal Extrapolation, seeds questions whose answers require\nextrapolating from spatial cues (micro-text, part-level appearance, layouts,\nsignage) and temporal traces (broadcast overlays, seasonal context) to\nout-of-image facts such as events, dates, and venues. We provide a\nmodel-agnostic agent framework with browsing tools and evaluate a range of\nclosed and open MLLMs. The strongest agent (o3) attains 15.1% without search\nand 36.0% accuracy with rollout under our framework, while a strong open-source\nmodel (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20\nrounds of search. Beyond answer accuracy, we assess bounding-box production and\ncropped-image search, and conduct an error analysis that surfaces failures in\nsource verification, part-based reasoning, and long-horizon planning.",
      "pdf_url": "http://arxiv.org/pdf/2508.21475v1",
      "published": "2025-08-29T09:58:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21475v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration",
      "authors": [
        "Seungyeon Choi",
        "Hwanhee Kim",
        "Chihyun Park",
        "Dahyeon Lee",
        "Seungyong Lee",
        "Yoonju Kim",
        "Hyoungjoon Park",
        "Sein Kwon",
        "Youngwan Jo",
        "Sanghyun Park"
      ],
      "abstract": "Recent advances in Structure-based Drug Design (SBDD) have leveraged\ngenerative models for 3D molecular generation, predominantly evaluating model\nperformance by binding affinity to target proteins. However, practical drug\ndiscovery necessitates high binding affinity along with synthetic feasibility\nand selectivity, critical properties that were largely neglected in previous\nevaluations. To address this gap, we identify fundamental limitations of\nconventional diffusion-based generative models in effectively guiding molecule\ngeneration toward these diverse pharmacological properties. We propose CByG, a\nnovel framework extending Bayesian Flow Network into a gradient-based\nconditional generative model that robustly integrates property-specific\nguidance. Additionally, we introduce a comprehensive evaluation scheme\nincorporating practical benchmarks for binding affinity, synthetic feasibility,\nand selectivity, overcoming the limitations of conventional evaluation methods.\nExtensive experiments demonstrate that our proposed CByG framework\nsignificantly outperforms baseline models across multiple essential evaluation\ncriteria, highlighting its effectiveness and practicality for real-world drug\ndiscovery applications.",
      "pdf_url": "http://arxiv.org/pdf/2508.21468v1",
      "published": "2025-08-29T09:49:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21468v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Diffusion-based Multi-modal Synergy Interest Network for Click-through Rate Prediction",
      "authors": [
        "Xiaoxi Cui",
        "Weihai Lu",
        "Yu Tong",
        "Yiheng Li",
        "Zhejun Zhao"
      ],
      "abstract": "In click-through rate prediction, click-through rate prediction is used to\nmodel users' interests. However, most of the existing CTR prediction methods\nare mainly based on the ID modality. As a result, they are unable to\ncomprehensively model users' multi-modal preferences. Therefore, it is\nnecessary to introduce multi-modal CTR prediction. Although it seems appealing\nto directly apply the existing multi-modal fusion methods to click-through rate\nprediction models, these methods (1) fail to effectively disentangle\ncommonalities and specificities across different modalities; (2) fail to\nconsider the synergistic effects between modalities and model the complex\ninteractions between modalities.\n  To address the above issues, this paper proposes the Diffusion-based\nMulti-modal Synergy Interest Network (Diff-MSIN) framework for click-through\nprediction. This framework introduces three innovative modules: the Multi-modal\nFeature Enhancement (MFE) Module Synergistic Relationship Capture (SRC) Module,\nand the Feature Dynamic Adaptive Fusion (FDAF) Module. The MFE Module and SRC\nModule extract synergistic, common, and special information among different\nmodalities. They effectively enhances the representation of the modalities,\nimproving the overall quality of the fusion. To encourage distinctiveness among\ndifferent features, we design a Knowledge Decoupling method. Additionally, the\nFDAF Module focuses on capturing user preferences and reducing fusion noise. To\nvalidate the effectiveness of the Diff-MSIN framework, we conducted extensive\nexperiments using the Rec-Tmall and three Amazon datasets. The results\ndemonstrate that our approach yields a significant improvement of at least\n1.67% compared to the baseline, highlighting its potential for enhancing\nmulti-modal recommendation systems. Our code is available at the following\nlink: https://github.com/Cxx-0/Diff-MSIN.",
      "pdf_url": "http://arxiv.org/pdf/2508.21460v1",
      "published": "2025-08-29T09:46:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21460v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Learning Lifted Action Models From Traces of Incomplete Actions and States",
      "authors": [
        "Niklas Jansen",
        "Jonas Gösgens",
        "Hector Geffner"
      ],
      "abstract": "Consider the problem of learning a lifted STRIPS model of the sliding-tile\npuzzle from random state-action traces where the states represent the location\nof the tiles only, and the actions are the labels up, down, left, and right,\nwith no arguments. Two challenges are involved in this problem. First, the\nstates are not full STRIPS states, as some predicates are missing, like the\natoms representing the position of the ``blank''. Second, the actions are not\nfull STRIPS either, as they do not reveal all the objects involved in the\nactions effects and preconditions. Previous approaches have addressed different\nversions of this model learning problem, but most assume that actions in the\ntraces are full STRIPS actions or that the domain predicates are all\nobservable. The new setting considered in this work is more ``realistic'', as\nthe atoms observed convey the state of the world but not full STRIPS states,\nand the actions reveal the arguments needed for selecting the action but not\nthe ones needed for modeling it in STRIPS. For formulating and addressing the\nlearning problem, we introduce a variant of STRIPS, which we call STRIPS+,\nwhere certain STRIPS action arguments can be left implicit in preconditions\nwhich can also involve a limited form of existential quantification. The\nlearning problem becomes the problem of learning STRIPS+ models from STRIPS+\nstate-action traces. For this, the proposed learning algorithm, called SYNTH,\nconstructs a stratified sequence (conjunction) of precondition expressions or\n``queries'' for each action, that denote unique objects in the state and ground\nthe implicit action arguments in STRIPS+. The correctness and completeness of\nSYNTH is established, and its scalability is tested on state-action traces\nobtained from STRIPS+ models derived from existing STRIPS domains.",
      "pdf_url": "http://arxiv.org/pdf/2508.21449v1",
      "published": "2025-08-29T09:27:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21449v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A General Framework of Epistemic Forgetting and its Instantiation by Ranking Functions",
      "authors": [
        "Christoph Beierle",
        "Alexander Hahn",
        "Diana Howey",
        "Gabriele Kern-Isberner",
        "Kai Sauerwald"
      ],
      "abstract": "Forgetting as a knowledge management operation deliberately ignores parts of\nthe knowledge and beliefs of an agent, for various reasons. Forgetting has many\nfacets, one may want to forget parts of the syntax, a proposition, or a\nconditional. In the literature, two main operators suitable for performing\nforgetting have been proposed and investigated in depth: First, variable\nelimination is a syntactical method that blends out certain atomic variables to\nfocus on the rest of the language. It has been mainly used in the area of logic\nprogramming and answer set programming. Second, contraction in AGM belief\nrevision theory effectively removes propositions from belief sets under logical\ndeduction. Both operations rely mainly on classical logics. In this article, we\ntake an epistemic perspective and study forgetting operations in epistemic\nstates with richer semantic structures, but with clear links to propositional\nlogic. This allows us to investigate what forgetting in the epistemic\nbackground means, thereby lifting well-known and novel forgetting operations to\nthe epistemic level. We present five general types of epistemic forgetting and\ninstantiate them with seven concrete forgetting operations for Spohn's ranking\nfunctions. We take inspiration from postulates of forgetting both from logic\nprogramming and AGM theory to propose a rich landscape of axioms for evaluating\nforgetting operations. Finally, we evaluate all concrete forgetting operations\naccording to all postulates, leading to a novel comprehensive overview\nhighlighting differences and commonalities among the forgetting operators.",
      "pdf_url": "http://arxiv.org/pdf/2508.21441v1",
      "published": "2025-08-29T09:08:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21441v1",
      "categories": [
        "cs.AI",
        "68T30, 68T27"
      ]
    },
    {
      "title": "MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation",
      "authors": [
        "Francisco Caetano",
        "Christiaan Viviers",
        "Peter H. H. de With",
        "Fons van der Sommen"
      ],
      "abstract": "Synthetic medical data offers a scalable solution for training robust models,\nbut significant domain gaps limit its generalizability to real-world clinical\nsettings. This paper addresses the challenge of cross-domain translation\nbetween synthetic and real X-ray images of the head, focusing on bridging\ndiscrepancies in attenuation behavior, noise characteristics, and soft tissue\nrepresentation. We propose MedShift, a unified class-conditional generative\nmodel based on Flow Matching and Schrodinger Bridges, which enables\nhigh-fidelity, unpaired image translation across multiple domains. Unlike prior\napproaches that require domain-specific training or rely on paired data,\nMedShift learns a shared domain-agnostic latent space and supports seamless\ntranslation between any pair of domains seen during training. We introduce\nX-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays\nunder varying radiation doses, to benchmark domain translation models.\nExperimental results demonstrate that, despite its smaller model size compared\nto diffusion-based approaches, MedShift offers strong performance and remains\nflexible at inference time, as it can be tuned to prioritize either perceptual\nfidelity or structural consistency, making it a scalable and generalizable\nsolution for domain adaptation in medical imaging. The code and dataset are\navailable at https://caetas.github.io/medshift.html",
      "pdf_url": "http://arxiv.org/pdf/2508.21435v1",
      "published": "2025-08-29T09:04:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21435v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management",
      "authors": [
        "Tobias Lindenbauer",
        "Igor Slinko",
        "Ludwig Felder",
        "Egor Bogomolov",
        "Yaroslav Zharov"
      ],
      "abstract": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility",
      "pdf_url": "http://arxiv.org/pdf/2508.21433v1",
      "published": "2025-08-29T09:02:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.21433v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    }
  ]
}