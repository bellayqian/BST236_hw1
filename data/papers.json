{
  "last_updated": "2025-10-01T00:55:04.556822",
  "papers": [
    {
      "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following",
      "authors": [
        "FaQiang Qian",
        "WeiKun Zhang",
        "Ziliang Wang",
        "Kang An",
        "Xuhui Zheng",
        "Liangjian Wen",
        "Mengya Gao",
        "Yong Dai",
        "Yichao Wu"
      ],
      "abstract": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment.",
      "pdf_url": "http://arxiv.org/pdf/2509.25148v1",
      "published": "2025-09-29T17:53:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25148v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Fast Feature Field ($\\text{F}^3$): A Predictive Representation of Events",
      "authors": [
        "Richeek Das",
        "Kostas Daniilidis",
        "Pratik Chaudhari"
      ],
      "abstract": "This paper develops a mathematical argument and algorithms for building\nrepresentations of data from event-based cameras, that we call Fast Feature\nField ($\\text{F}^3$). We learn this representation by predicting future events\nfrom past events and show that it preserves scene structure and motion\ninformation. $\\text{F}^3$ exploits the sparsity of event data and is robust to\nnoise and variations in event rates. It can be computed efficiently using ideas\nfrom multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and\n440 Hz at VGA resolutions. $\\text{F}^3$ represents events within a contiguous\nspatiotemporal volume as a multi-channel image, enabling a range of downstream\ntasks. We obtain state-of-the-art performance on optical flow estimation,\nsemantic segmentation, and monocular metric depth estimation, on data from\nthree robotic platforms (a car, a quadruped robot and a flying platform),\nacross different lighting conditions (daytime, nighttime), environments\n(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors\n(resolutions and event rates). Our implementations can predict these tasks at\n25-75 Hz at HD resolution.",
      "pdf_url": "http://arxiv.org/pdf/2509.25146v1",
      "published": "2025-09-29T17:52:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25146v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation",
      "authors": [
        "Yen-Ju Lu",
        "Thomas Thebaud",
        "Laureano Moro-Velazquez",
        "Najim Dehak",
        "Jesus Villalba"
      ],
      "abstract": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis.",
      "pdf_url": "http://arxiv.org/pdf/2509.25144v1",
      "published": "2025-09-29T17:51:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25144v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Visual serial processing deficits explain divergences in human and VLM reasoning",
      "authors": [
        "Nicholas Budny",
        "Kia Ghods",
        "Declan Campbell",
        "Raja Marjieh",
        "Amogh Joshi",
        "Sreejan Kumar",
        "Jonathan D. Cohen",
        "Taylor W. Webb",
        "Thomas L. Griffiths"
      ],
      "abstract": "Why do Vision Language Models (VLMs), despite success on standard benchmarks,\noften fail to match human performance on surprisingly simple visual reasoning\ntasks? While the underlying computational principles are still debated, we\nhypothesize that a crucial factor is a deficit in visually-grounded serial\nprocessing. To test this hypothesis, we compared human and VLM performance\nacross tasks designed to vary serial processing demands in three distinct\ndomains: geometric reasoning, perceptual enumeration, and mental rotation.\nTasks within each domain varied serial processing load by manipulating factors\nsuch as geometric concept complexity, perceptual individuation load, and\ntransformation difficulty. Across all domains, our results revealed a\nconsistent pattern: decreased VLM accuracy was strongly correlated with\nincreased human reaction time (used as a proxy for serial processing load). As\ntasks require more demanding serial processing -- whether composing concepts,\nenumerating items, or performing mental transformations -- the VLM-human\nperformance gap widens reliably. These findings support our hypothesis,\nindicating that limitations in serial, visually grounded reasoning represent a\nfundamental bottleneck that distinguishes current VLMs from humans.",
      "pdf_url": "http://arxiv.org/pdf/2509.25142v1",
      "published": "2025-09-29T17:51:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25142v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory",
      "authors": [
        "Siru Ouyang",
        "Jun Yan",
        "I-Hung Hsu",
        "Yanfei Chen",
        "Ke Jiang",
        "Zifeng Wang",
        "Rujun Han",
        "Long T. Le",
        "Samira Daruki",
        "Xiangru Tang",
        "Vishy Tirumalashetty",
        "George Lee",
        "Mahsan Rofouei",
        "Hangfei Lin",
        "Jiawei Han",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "abstract": "With the growing adoption of large language model agents in persistent\nreal-world roles, they naturally encounter continuous streams of tasks. A key\nlimitation, however, is their failure to learn from the accumulated interaction\nhistory, forcing them to discard valuable insights and repeat past errors. We\npropose ReasoningBank, a novel memory framework that distills generalizable\nreasoning strategies from an agent's self-judged successful and failed\nexperiences. At test time, an agent retrieves relevant memories from\nReasoningBank to inform its interaction and then integrates new learnings back,\nenabling it to become more capable over time. Building on this powerful\nexperience learner, we further introduce memory-aware test-time scaling\n(MaTTS), which accelerates and diversifies this learning process by scaling up\nthe agent's interaction experience. By allocating more compute to each task,\nthe agent generates abundant, diverse experiences that provide rich contrastive\nsignals for synthesizing higher-quality memory. The better memory in turn\nguides more effective scaling, establishing a powerful synergy between memory\nand test-time scaling. Across web browsing and software engineering benchmarks,\nReasoningBank consistently outperforms existing memory mechanisms that store\nraw trajectories or only successful task routines, improving both effectiveness\nand efficiency; MaTTS further amplifies these gains. These findings establish\nmemory-driven experience scaling as a new scaling dimension, enabling agents to\nself-evolve with emergent behaviors naturally arise.",
      "pdf_url": "http://arxiv.org/pdf/2509.25140v1",
      "published": "2025-09-29T17:51:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25140v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs",
      "authors": [
        "Yue Zhang",
        "Tianyi Ma",
        "Zun Wang",
        "Yanyuan Qiao",
        "Parisa Kordjamshidi"
      ],
      "abstract": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance.",
      "pdf_url": "http://arxiv.org/pdf/2509.25139v1",
      "published": "2025-09-29T17:51:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25139v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ]
    },
    {
      "title": "The Era of Real-World Human Interaction: RL from User Conversations",
      "authors": [
        "Chuanyang Jin",
        "Jing Xu",
        "Bo Liu",
        "Leitian Tao",
        "Olga Golovneva",
        "Tianmin Shu",
        "Wenting Zhao",
        "Xian Li",
        "Jason Weston"
      ],
      "abstract": "We posit that to achieve continual model improvement and multifaceted\nalignment, future models must learn from natural human interaction. Current\nconversational models are aligned using pre-annotated, expert-generated human\nfeedback. In this work, we introduce Reinforcement Learning from Human\nInteraction (RLHI), a paradigm that learns directly from in-the-wild user\nconversations. We develop two complementary methods: (1) RLHI with User-Guided\nRewrites, which revises unsatisfactory model outputs based on users'\nnatural-language follow-up responses, (2) RLHI with User-Based Rewards, which\nlearns via a reward model conditioned on knowledge of the user's long-term\ninteraction history (termed persona). Together, these methods link long-term\nuser personas to turn-level preferences via persona-conditioned preference\noptimization. Trained on conversations derived from WildChat, both RLHI\nvariants outperform strong baselines in personalization and\ninstruction-following, and similar feedback enhances performance on reasoning\nbenchmarks. These results suggest organic human interaction offers scalable,\neffective supervision for personalized alignment.",
      "pdf_url": "http://arxiv.org/pdf/2509.25137v1",
      "published": "2025-09-29T17:50:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25137v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Rethinking Entropy Regularization in Large Reasoning Models",
      "authors": [
        "Yuxian Jiang",
        "Yafu Li",
        "Guanxu Chen",
        "Dongrui Liu",
        "Yu Cheng",
        "Jing Shao"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise\nin enhancing the reasoning abilities of large reasoning models (LRMs). However,\nit suffers from a critical issue: entropy collapse and premature convergence.\nNaive entropy regularization, a common approach for encouraging exploration in\nthe traditional RL literature, fails to address this problem in the context of\nLRM. Our analysis reveals that this failure stems from the vast action space\nand long trajectories in LRMs, which easily trigger a global entropy explosion\nas the model indiscriminately explores all possible actions and states. To\naddress this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method\nthat confines exploration to a meaningful subset of actions and states. SIREN\nachieves this through a two-step entropy masking mechanism, consisting of a\ntop-p mask and a peak-entropy mask. In addition, regularization is transformed\ninto a self-anchored form to stabilize training. Across five mathematical\nbenchmarks, SIREN attains superior average performance over previous\nentropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on\nAIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes\ngreater response diversity and maintains entropy at an appropriate level, which\nhelps to preserve the validation pass@k throughout training. This effectively\nmitigates the premature convergence problem common in RLVR for LRM.",
      "pdf_url": "http://arxiv.org/pdf/2509.25133v1",
      "published": "2025-09-29T17:49:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25133v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
      "authors": [
        "Chengyao Wang",
        "Zhisheng Zhong",
        "Bohao Peng",
        "Senqiao Yang",
        "Yuqi Liu",
        "Haokun Gui",
        "Bin Xia",
        "Jingyao Li",
        "Bei Yu",
        "Jiaya Jia"
      ],
      "abstract": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
      "pdf_url": "http://arxiv.org/pdf/2509.25131v1",
      "published": "2025-09-29T17:48:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25131v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.MM"
      ]
    },
    {
      "title": "Score Distillation of Flow Matching Models",
      "authors": [
        "Mingyuan Zhou",
        "Yi Gu",
        "Huangjie Zheng",
        "Liangchen Song",
        "Guande He",
        "Yizhe Zhang",
        "Wenze Hu",
        "Yinfei Yang"
      ],
      "abstract": "Diffusion models achieve high-quality image generation but are limited by\nslow iterative sampling. Distillation methods alleviate this by enabling one-\nor few-step generation. Flow matching, originally introduced as a distinct\nframework, has since been shown to be theoretically equivalent to diffusion\nunder Gaussian assumptions, raising the question of whether distillation\ntechniques such as score distillation transfer directly. We provide a simple\nderivation -- based on Bayes' rule and conditional expectations -- that unifies\nGaussian diffusion and flow matching without relying on ODE/SDE formulations.\nBuilding on this view, we extend Score identity Distillation (SiD) to\npretrained text-to-image flow-matching models, including SANA, SD3-Medium,\nSD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show\nthat, with only modest flow-matching- and DiT-specific adjustments, SiD works\nout of the box across these models, in both data-free and data-aided settings,\nwithout requiring teacher finetuning or architectural changes. This provides\nthe first systematic evidence that score distillation applies broadly to\ntext-to-image flow matching models, resolving prior concerns about stability\nand soundness and unifying acceleration techniques across diffusion- and\nflow-based generators. We will make the PyTorch implementation publicly\navailable.",
      "pdf_url": "http://arxiv.org/pdf/2509.25127v1",
      "published": "2025-09-29T17:45:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25127v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones",
      "authors": [
        "Lifan Yuan",
        "Weize Chen",
        "Yuchen Zhang",
        "Ganqu Cui",
        "Hanbin Wang",
        "Ziming You",
        "Ning Ding",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Hao Peng"
      ],
      "abstract": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
      "pdf_url": "http://arxiv.org/pdf/2509.25123v1",
      "published": "2025-09-29T17:44:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25123v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis",
      "authors": [
        "Yiquan Wang",
        "Tin-Yeh Huang",
        "Qingyun Gao",
        "Jialin Zhang"
      ],
      "abstract": "Heatwaves pose complex cascading risks across interconnected climate, social,\nand economic systems, but knowledge fragmentation in scientific literature\nhinders comprehensive understanding of these risk pathways. We introduce HeDA\n(Heatwave Discovery Agent), an intelligent multi-agent system designed for\nautomated scientific discovery through knowledge graph construction and\nmulti-layer risk propagation analysis. HeDA processes over 10,247 academic\npapers to construct a comprehensive knowledge graph with 23,156 nodes and\n89,472 relationships, employing novel multi-layer risk propagation analysis to\nsystematically identify overlooked risk transmission pathways. Our system\nachieves 78.9% accuracy on complex question-answering tasks, outperforming\nstate-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA\nsuccessfully discovered five previously unidentified high-impact risk chains,\nsuch as the pathway where a heatwave leads to a water demand surge, resulting\nin industrial water restrictions and ultimately causing small business\ndisruption, which were validated through historical case studies and domain\nexpert review. This work presents a new paradigm for AI-driven scientific\ndiscovery, providing actionable insights for developing more resilient climate\nadaptation strategies.",
      "pdf_url": "http://arxiv.org/pdf/2509.25112v1",
      "published": "2025-09-29T17:40:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25112v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Towards Personalized Deep Research: Benchmarks and Evaluations",
      "authors": [
        "Yuan Liang",
        "Jiaxian Li",
        "Yuqing Wang",
        "Piaohong Wang",
        "Motong Tian",
        "Pai Liu",
        "Shuofei Qiao",
        "Runnan Fang",
        "He Zhu",
        "Ge Zhang",
        "Minghao Liu",
        "Yuchen Eleanor Jiang",
        "Ningyu Zhang",
        "Wangchunshu Zhou"
      ],
      "abstract": "Deep Research Agents (DRAs) can autonomously conduct complex investigations\nand generate comprehensive reports, demonstrating strong real-world potential.\nHowever, existing evaluations mostly rely on close-ended benchmarks, while\nopen-ended deep research benchmarks remain scarce and typically neglect\npersonalized scenarios. To bridge this gap, we introduce Personalized Deep\nResearch Bench, the first benchmark for evaluating personalization in DRAs. It\npairs 50 diverse research tasks across 10 domains with 25 authentic user\nprofiles that combine structured persona attributes with dynamic real-world\ncontexts, yielding 250 realistic user-task queries. To assess system\nperformance, we propose the PQR Evaluation Framework, which jointly measures\n(P) Personalization Alignment, (Q) Content Quality, and (R) Factual\nReliability. Our experiments on a range of systems highlight current\ncapabilities and limitations in handling personalized deep research. This work\nestablishes a rigorous foundation for developing and evaluating the next\ngeneration of truly personalized AI research assistants.",
      "pdf_url": "http://arxiv.org/pdf/2509.25106v1",
      "published": "2025-09-29T17:39:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25106v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation",
      "authors": [
        "Aasheesh Singh",
        "Vishal Vaddina",
        "Dagnachew Birru"
      ],
      "abstract": "We introduce ORPO-Distill, a general-purpose method for cross-architecture\nLLM distillation that formulates the problem as a preference optimization task.\nUnlike standard CoT distillation, the approach transfers knowledge through\ndiverse reasoning traces. It employs an Odds-Ratio Preference Optimization\nobjective that contrasts teacher and student traces for more effective\nlearning, and adopts a mixed-policy strategy for utilizing student-generated\noutputs, outperforming both off- and on-policy alternatives. Experiments on\nfive datasets and multiple student models show consistent improvements over\nconventional black-box KD baselines.",
      "pdf_url": "http://arxiv.org/pdf/2509.25100v1",
      "published": "2025-09-29T17:34:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25100v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Scaling with Collapse: Efficient and Predictable Training of LLM Families",
      "authors": [
        "Shane Bergsma",
        "Bin Claire Zhang",
        "Nolan Dey",
        "Shaheer Muhammad",
        "Gurpreet Gosal",
        "Joel Hestness"
      ],
      "abstract": "Effective LLM training relies on *consistency*, meaning that key quantities\n-- such as final losses and optimal hyperparameters -- scale predictably across\nmodel sizes. Qiu et al. (2025) recently showed that this consistency extends\nbeyond scalars: whole training loss curves can *collapse* onto a universal\ntrajectory after a simple normalization. What remains unclear is whether this\nphenomenon holds for LLM families trained under *practical scaling recipes*,\nwhere width, depth, learning rate, batch size, and weight decay are scaled\njointly. We show that it does: loss curves collapse across scales precisely\nwhen optimization hyperparameters are set optimally for the given data budget,\nin accordance with recent empirical scaling laws. Collapse thus emerges as a\nsignature of compute-efficient training. We demonstrate two applications at\nscale: (1) deviation-from-collapse provides a sensitive, early diagnostic of\ntraining pathologies, and (2) the predictability of collapsed curves enables\nearly stopping in large-scale hyperparameter tuning. Finally, we train a\ncompetitive LLM family, *Celerity*, using these insights, highlighting collapse\nas an effective tool for developing efficient LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.25087v1",
      "published": "2025-09-29T17:26:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25087v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "jina-reranker-v3: Last but Not Late Interaction for Document Reranking",
      "authors": [
        "Feng Wang",
        "Yuqing Li",
        "Han Xiao"
      ],
      "abstract": "jina-reranker-v3 is a 0.6B parameter multilingual document reranker that\nintroduces a novel last but not late interaction. Unlike late interaction\nmodels such as ColBERT that perform separate encoding followed by multi-vector\nmatching, our approach conducts causal self-attention between query and\ndocuments within the same context window, enabling rich cross-document\ninteractions before extracting contextual embeddings from the last token of\neach document. This compact architecture achieves state-of-the-art BEIR\nperformance with 61.94 nDCG@10 while being ten times smaller than generative\nlistwise rerankers.",
      "pdf_url": "http://arxiv.org/pdf/2509.25085v1",
      "published": "2025-09-29T17:23:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25085v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "68T50",
        "I.2.7"
      ]
    },
    {
      "title": "Scaling Generalist Data-Analytic Agents",
      "authors": [
        "Shuofei Qiao",
        "Yanqiu Zhao",
        "Zhisong Qiu",
        "Xiaobin Wang",
        "Jintian Zhang",
        "Zhao Bin",
        "Ningyu Zhang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
      ],
      "abstract": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.",
      "pdf_url": "http://arxiv.org/pdf/2509.25084v1",
      "published": "2025-09-29T17:23:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25084v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation",
      "authors": [
        "Guanjun Wu",
        "Jiemin Fang",
        "Chen Yang",
        "Sikuang Li",
        "Taoran Yi",
        "Jia Lu",
        "Zanwei Zhou",
        "Jiazhong Cen",
        "Lingxi Xie",
        "Xiaopeng Zhang",
        "Wei Wei",
        "Wenyu Liu",
        "Xinggang Wang",
        "Qi Tian"
      ],
      "abstract": "High-fidelity 3D asset generation is crucial for various industries. While\nrecent 3D pretrained models show strong capability in producing realistic\ncontent, most are built upon diffusion models and follow a two-stage pipeline\nthat first generates geometry and then synthesizes appearance. Such a decoupled\ndesign tends to produce geometry-texture misalignment and non-negligible cost.\nIn this paper, we propose UniLat3D, a unified framework that encodes geometry\nand appearance in a single latent space, enabling direct single-stage\ngeneration. Our key contribution is a geometry-appearance Unified VAE, which\ncompresses high-resolution sparse features into a compact latent representation\n-- UniLat. UniLat integrates structural and visual information into a dense\nlow-resolution latent, which can be efficiently decoded into diverse 3D\nformats, e.g., 3D Gaussians and meshes. Based on this unified representation,\nwe train a single flow-matching model to map Gaussian noise directly into\nUniLat, eliminating redundant stages. Trained solely on public datasets,\nUniLat3D produces high-quality 3D assets in seconds from a single image,\nachieving superior appearance fidelity and geometric quality. More demos \\&\ncode are available at https://unilat3d.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2509.25079v1",
      "published": "2025-09-29T17:21:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25079v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation",
      "authors": [
        "Dingning Liu",
        "Haoyu Guo",
        "Jingyi Zhou",
        "Tong He"
      ],
      "abstract": "Monocular Depth Estimation (MDE) is a foundational task for computer vision.\nTraditional methods are limited by data scarcity and quality, hindering their\nrobustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image\n(D2I) generation framework that synthesizes over 20M realistic and\ngeometrically accurate RGB images, each intrinsically paired with its ground\ntruth depth, from diverse source depth maps. Then we train our depth estimation\nmodel on this dataset, employing a hybrid supervision strategy that integrates\nteacher pseudo-labels with ground truth depth for comprehensive and robust\ntraining. This innovative data generation and training paradigm enables BRIDGE\nto achieve breakthroughs in scale and domain diversity, consistently\noutperforming existing state-of-the-art approaches quantitatively and in\ncomplex scene detail capture, thereby fostering general and robust depth\nfeatures. Code and models are available at\nhttps://dingning-liu.github.io/bridge.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2509.25077v1",
      "published": "2025-09-29T17:19:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25077v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications",
      "authors": [
        "Yaman Jandali",
        "Ruisi Zhang",
        "Nojan Sheybani",
        "Farinaz Koushanfar"
      ],
      "abstract": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference.",
      "pdf_url": "http://arxiv.org/pdf/2509.25072v1",
      "published": "2025-09-29T17:16:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25072v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning",
      "authors": [
        "Sai Wang",
        "Yu Wu",
        "Zhongwen Xu"
      ],
      "abstract": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.",
      "pdf_url": "http://arxiv.org/pdf/2509.25052v1",
      "published": "2025-09-29T17:02:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25052v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Scaling Synthetic Task Generation for Agents via Exploration",
      "authors": [
        "Ram Ramrakhya",
        "Andrew Szot",
        "Omar Attia",
        "Yuhao Yang",
        "Anh Nguyen",
        "Bogdan Mazoure",
        "Zhe Gan",
        "Harsh Agrawal",
        "Alexander Toshev"
      ],
      "abstract": "Post-Training Multimodal Large Language Models (MLLMs) to build interactive\nagents holds promise across domains such as computer-use, web navigation, and\nrobotics. A key challenge in scaling such post-training is lack of high-quality\ndownstream agentic task datasets with tasks that are diverse, feasible, and\nverifiable. Existing approaches for task generation rely heavily on human\nannotation or prompting MLLM with limited downstream environment information,\nwhich is either costly or poorly scalable as it yield tasks with limited\ncoverage. To remedy this, we present AutoPlay, a scalable pipeline for task\ngeneration that explicitly explores interactive environments to discover\npossible interactions and current state information to synthesize\nenvironment-grounded tasks. AutoPlay operates in two stages: (i) an exploration\nphase, where an MLLM explorer agent systematically uncovers novel environment\nstates and functionalities, and (ii) a task generation phase, where a task\ngenerator leverages exploration trajectories and a set of task guideline\nprompts as context to synthesize diverse, executable, and verifiable tasks. We\nshow AutoPlay generates 20k tasks across 20 Android applications and 10k tasks\nacross 13 applications Ubuntu applications to train mobile-use and computer-use\nagents. AutoPlay generated tasks enable large-scale task demonstration\nsynthesis without human annotation by employing an MLLM task executor and\nverifier. This data enables training MLLM-based UI agents that improve success\nrates up to $20.0\\%$ on mobile-use and $10.9\\%$ on computer-use scenarios. In\naddition, AutoPlay generated tasks combined with MLLM verifier-based rewards\nenable scaling reinforcement learning training of UI agents, leading to an\nadditional $5.7\\%$ gain. coverage. These results establish AutoPlay as a\nscalable approach for post-training capable MLLM agents reducing reliance on\nhuman annotation.",
      "pdf_url": "http://arxiv.org/pdf/2509.25047v1",
      "published": "2025-09-29T17:00:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25047v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures",
      "authors": [
        "Marco Bronzini",
        "Carlo Nicolini",
        "Bruno Lepri",
        "Jacopo Staiano",
        "Andrea Passerini"
      ],
      "abstract": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.",
      "pdf_url": "http://arxiv.org/pdf/2509.25045v1",
      "published": "2025-09-29T16:59:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25045v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Large Language Models for Software Testing: A Research Roadmap",
      "authors": [
        "Cristian Augusto",
        "Antonia Bertolino",
        "Guglielmo De Angelis",
        "Francesca Lonetti",
        "Jesús Morán"
      ],
      "abstract": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field.",
      "pdf_url": "http://arxiv.org/pdf/2509.25043v1",
      "published": "2025-09-29T16:58:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25043v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Fast Real-Time Pipeline for Robust Arm Gesture Recognition",
      "authors": [
        "Milán Zsolt Bagladi",
        "László Gulyás",
        "Gergő Szalay"
      ],
      "abstract": "This paper presents a real-time pipeline for dynamic arm gesture recognition\nbased on OpenPose keypoint estimation, keypoint normalization, and a recurrent\nneural network classifier. The 1 x 1 normalization scheme and two feature\nrepresentations (coordinate- and angle-based) are presented for the pipeline.\nIn addition, an efficient method to improve robustness against camera angle\nvariations is also introduced by using artificially rotated training data.\nExperiments on a custom traffic-control gesture dataset demonstrate high\naccuracy across varying viewing angles and speeds. Finally, an approach to\ncalculate the speed of the arm signal (if necessary) is also presented.",
      "pdf_url": "http://arxiv.org/pdf/2509.25042v1",
      "published": "2025-09-29T16:57:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25042v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct",
      "authors": [
        "Haoyang Zheng",
        "Xinyang Liu",
        "Cindy Xiangrui Kong",
        "Nan Jiang",
        "Zheyuan Hu",
        "Weijian Luo",
        "Wei Deng",
        "Guang Lin"
      ],
      "abstract": "Fast generation of language texts is the holy grail that people pursue in the\nAI era. In this work, we introduced Discrete Diffusion Divergence Instruct\n(DiDi-Instruct), a training-based method that leads to fast language generation\nmodels by initializing from a pre-trained (masked) discrete diffusion language\nmodel (dLLM). The resulting DiDi-Instruct model outperforms the dLLM\ncounterparts and the GPT-2 baseline with 64x acceleration. In the theoretical\npart of the paper, we build the foundation of DiDi-Instruct in a framework of\nintegral KL-divergence minimization, with practical training algorithms. We\nalso introduce techniques like grouped reward normalization, intermediate-state\nmatching, and the reward-guided ancestral sampler (RGAS) that significantly\nimprove the training stability, the model coverage, and the inference\nperformances. On OpenWebText, DiDi-Instruct outperforms all accelerated\nlanguage generation models as well as the GPT-2 baseline and the standard\ndLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128\nNFEs). These performance gains are accomplished with a negligible entropy loss\nof about 1% and 20x less additional training wall-clock time. We further\nvalidate the robustness and effectiveness of DiDi-Instruct through extensive\nablation studies, model scaling, and the generation of discrete protein\nsequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct.",
      "pdf_url": "http://arxiv.org/pdf/2509.25035v1",
      "published": "2025-09-29T16:55:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25035v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation",
      "authors": [
        "Ryosuke Takanami",
        "Petr Khrapchenkov",
        "Shu Morikuni",
        "Jumpei Arima",
        "Yuta Takaba",
        "Shunsuke Maeda",
        "Takuya Okubo",
        "Genki Sano",
        "Satoshi Sekioka",
        "Aoi Kadoya",
        "Motonari Kambara",
        "Naoya Nishiura",
        "Haruto Suzuki",
        "Takanori Yoshimoto",
        "Koya Sakamoto",
        "Shinnosuke Ono",
        "Hu Yang",
        "Daichi Yashima",
        "Aoi Horo",
        "Tomohiro Motoda",
        "Kensuke Chiyoma",
        "Hiroshi Ito",
        "Koki Fukuda",
        "Akihito Goto",
        "Kazumi Morinaga",
        "Yuya Ikeda",
        "Riko Kawada",
        "Masaki Yoshikawa",
        "Norio Kosuge",
        "Yuki Noguchi",
        "Kei Ota",
        "Tatsuya Matsushima",
        "Yusuke Iwasawa",
        "Yutaka Matsuo",
        "Tetsuya Ogata"
      ],
      "abstract": "As robots transition from controlled settings to unstructured human\nenvironments, building generalist agents that can reliably follow natural\nlanguage instructions remains a central challenge. Progress in robust mobile\nmanipulation requires large-scale multimodal datasets that capture contact-rich\nand long-horizon tasks, yet existing resources lack synchronized force-torque\nsensing, hierarchical annotations, and explicit failure cases. We address this\ngap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset\nfor mobile manipulation. It includes synchronized RGB images, joint states,\nsix-axis wrist force-torque signals, and internal robot states, together with a\nnovel two-layer annotation schema of sub-goals and primitive actions for\nhierarchical learning and error analysis. The initial dataset comprises 25,469\nepisodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is\nfully standardized in the LeRobot v2.1 format. By uniquely integrating mobile\nmanipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa\nprovides a critical benchmark for advancing the next generation of\nVision-Language-Action models. The first version of our dataset is now\navailable at https://huggingface.co/datasets/airoa-org/airoa-moma .",
      "pdf_url": "http://arxiv.org/pdf/2509.25032v1",
      "published": "2025-09-29T16:51:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25032v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image Segmentation",
      "authors": [
        "Max Curie",
        "Paulo da Costa"
      ],
      "abstract": "We introduce CLASP (Clustering via Adaptive Spectral Processing), a\nlightweight framework for unsupervised image segmentation that operates without\nany labeled data or finetuning. CLASP first extracts per patch features using a\nself supervised ViT encoder (DINO); then, it builds an affinity matrix and\napplies spectral clustering. To avoid manual tuning, we select the segment\ncount automatically with a eigengap silhouette search, and we sharpen the\nboundaries with a fully connected DenseCRF. Despite its simplicity and training\nfree nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff\nand ADE20K, matching recent unsupervised baselines. The zero training design\nmakes CLASP a strong, easily reproducible baseline for large unannotated\ncorpora especially common in digital advertising and marketing workflows such\nas brand safety screening, creative asset curation, and social media content\nmoderation",
      "pdf_url": "http://arxiv.org/pdf/2509.25016v1",
      "published": "2025-09-29T16:41:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25016v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning",
      "authors": [
        "Shijie Zhang",
        "Guohao Sun",
        "Kevin Zhang",
        "Xiang Guo",
        "Rujun Guo"
      ],
      "abstract": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has\nbecome a key paradigm for enhancing the reasoning capabilities of Large\nLanguage Models (LLMs). However, existing methods typically treat all training\nsamples uniformly, overlooking the vast differences in problem difficulty\nrelative to the model's current capabilities. This uniform training strategy\nleads to inefficient exploration of problems the model has already mastered,\nwhile concurrently lacking effective guidance on problems that are challenging\nits abilities the most, limiting both learning efficiency and upper-bound\nperformance. To address this, we propose CLPO (Curriculum-guided Learning for\nPolicy Optimization), a novel algorithm that creates a dynamic pedagogical\nfeedback loop within the policy optimization process. The core of CLPO\nleverages the model's own rollout performance to conduct real-time difficulty\nassessment, thereby constructing an Online Curriculum. This curriculum then\nguides an Adaptive Problem Restructuring mechanism, where the model acts as its\nown teacher: it diversifies medium-difficulty problems to promote\ngeneralization and simplifies challenging problems to make them more\nattainable. Our approach transforms the static training procedure into a\ndynamic process that co-evolves with the model's capabilities. Experiments show\nthat CLPO achieves state-of-the-art performance across eight challenging\nmathematical and general reasoning benchmarks, with an average pass@1\nimprovement of 6.96% over other methods, demonstrating its potential for more\nefficiently training more capable reasoning models.",
      "pdf_url": "http://arxiv.org/pdf/2509.25004v1",
      "published": "2025-09-29T16:29:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.25004v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns",
      "authors": [
        "Hanqi Xiao",
        "Vaidehi Patil",
        "Hyunji Lee",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Generating accurate and calibrated confidence estimates is critical for\ndeploying LLMs in high-stakes or user-facing applications, and remains an open\nchallenge. Prior research has often framed confidence as a problem of eliciting\na model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its\nown answers are correct; this approach implicitly assumes that there is some\nprivileged information about the answer's correctness that is accessible to the\nmodel itself. However, our experiments reveal that an LLM attempting to predict\nthe correctness of its own outputs generally performs no better than an\nunrelated LLM. Moreover, we hypothesize that a key factor in building a\n\"Correctness Model\" (CM) is exposure to a target model's historical\npredictions. We propose multiple methods to inject this historical correctness\ninformation, creating a Generalized Correctness Model (GCM). We first show that\nGCMs can be trained on the correctness data from many LLMs and learn patterns\nfor correctness prediction applicable across datasets and models. We then use\nCMs as a lens for studying the source of correctness prediction ability and its\ngeneralization, systematically controlling their training data and finding that\nanswer phrasing is a strong predictor for correctness. We further explore\nalternative methods of injecting history without training an LLM, finding that\nincluding history as in-context examples can help improve correctness\nprediction, and post-hoc calibration can provide complementary reductions in\ncalibration error. We evaluate GCMs based on Qwen3-8B across 5 model families\nand the MMLU and TriviaQA datasets, as well as on a downstream selective\nprediction task, finding that reliable LLM confidence estimation is a\ngeneralizable and model-agnostic skill learned by systematically encoding\ncorrectness history rather than a model-specific skill reliant on\nself-introspection.",
      "pdf_url": "http://arxiv.org/pdf/2509.24988v1",
      "published": "2025-09-29T16:19:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24988v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes",
      "authors": [
        "Yuhan Wang",
        "Weikai Chen",
        "Zeyu Hu",
        "Runze Zhang",
        "Yingda Yin",
        "Ruoyu Wu",
        "Keyang Luo",
        "Shengju Qian",
        "Yiyan Ma",
        "Hongyi Li",
        "Yuan Gao",
        "Yuhuan Zhou",
        "Hao Luo",
        "Wan Wang",
        "Xiaobin Shen",
        "Zhaowei Li",
        "Kuixin Zhu",
        "Chuanlang Hong",
        "Yueyue Wang",
        "Lijie Feng",
        "Xin Wang",
        "Chen Change Loy"
      ],
      "abstract": "In user-generated-content (UGC) applications, non-expert users often rely on\nimage-to-3D generative models to create 3D assets. In this context,\nprimitive-based shape abstraction offers a promising solution for UGC scenarios\nby compressing high-resolution meshes into compact, editable representations.\nTowards this end, effective shape abstraction must therefore be\nstructure-aware, characterized by low overlap between primitives, part-aware\nalignment, and primitive compactness. We present Light-SQ, a novel\nsuperquadric-based optimization framework that explicitly emphasizes\nstructure-awareness from three aspects. (a) We introduce SDF carving to\niteratively udpate the target signed distance field, discouraging overlap\nbetween primitives. (b) We propose a block-regrow-fill strategy guided by\nstructure-aware volumetric decomposition, enabling structural partitioning to\ndrive primitive placement. (c) We implement adaptive residual pruning based on\nSDF update history to surpress over-segmentation and ensure compact results. In\naddition, Light-SQ supports multiscale fitting, enabling localized refinement\nto preserve fine geometric details. To evaluate our method, we introduce\n3DGen-Prim, a benchmark extending 3DGen-Bench with new metrics for both\nreconstruction quality and primitive-level editability. Extensive experiments\ndemonstrate that Light-SQ enables efficient, high-fidelity, and editable shape\nabstraction with superquadrics for complex generated geometry, advancing the\nfeasibility of 3D UGC creation.",
      "pdf_url": "http://arxiv.org/pdf/2509.24986v1",
      "published": "2025-09-29T16:18:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24986v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards",
      "authors": [
        "Haoran He",
        "Yuxiao Ye",
        "Qingpeng Cai",
        "Chen Hu",
        "Binxing Jiao",
        "Daxin Jiang",
        "Ling Pan"
      ],
      "abstract": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1,\n\\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite\nits radical simplification compared to strong, complicated existing methods.",
      "pdf_url": "http://arxiv.org/pdf/2509.24981v1",
      "published": "2025-09-29T16:09:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24981v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Agentic Exploration of Physics Models",
      "authors": [
        "Maximilian Nägele",
        "Florian Marquardt"
      ],
      "abstract": "The process of scientific discovery relies on an interplay of observations,\nanalysis, and hypothesis generation. Machine learning is increasingly being\nadopted to address individual aspects of this process. However, it remains an\nopen challenge to fully automate the open-ended, heuristic, iterative loop\nrequired to discover the laws of an unknown system by exploring it through\nexperiments and analysis, without tailoring the approach to the specifics of a\ngiven task. Here, we introduce SciExplorer, an agent that leverages large\nlanguage model tool-use capabilities to enable free-form exploration of systems\nwithout any domain-specific blueprints, and apply it to the exploration of\nphysical systems that are initially unknown to the agent. We test SciExplorer\non a broad set of models spanning mechanical dynamical systems, wave evolution,\nand quantum many-body physics. Despite using a minimal set of tools, primarily\nbased on code execution, we observe impressive performance on tasks such as\nrecovering equations of motion from observed dynamics and inferring\nHamiltonians from expectation values. The demonstrated effectiveness of this\nsetup opens the door towards similar scientific exploration in other domains,\nwithout the need for finetuning or task-specific instructions.",
      "pdf_url": "http://arxiv.org/pdf/2509.24978v2",
      "published": "2025-09-29T16:07:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24978v2",
      "categories": [
        "cs.AI",
        "cond-mat.quant-gas",
        "quant-ph"
      ]
    },
    {
      "title": "SecInfer: Preventing Prompt Injection via Inference-time Scaling",
      "authors": [
        "Yupei Liu",
        "Yanting Wang",
        "Yuqi Jia",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "abstract": "Prompt injection attacks pose a pervasive threat to the security of Large\nLanguage Models (LLMs). State-of-the-art prevention-based defenses typically\nrely on fine-tuning an LLM to enhance its security, but they achieve limited\neffectiveness against strong attacks. In this work, we propose \\emph{SecInfer},\na novel defense against prompt injection attacks built on \\emph{inference-time\nscaling}, an emerging paradigm that boosts LLM capability by allocating more\ncompute resources for reasoning during inference. SecInfer consists of two key\nsteps: \\emph{system-prompt-guided sampling}, which generates multiple responses\nfor a given input by exploring diverse reasoning paths through a varied set of\nsystem prompts, and \\emph{target-task-guided aggregation}, which selects the\nresponse most likely to accomplish the intended task. Extensive experiments\nshow that, by leveraging additional compute at inference, SecInfer effectively\nmitigates both existing and adaptive prompt injection attacks, outperforming\nstate-of-the-art defenses as well as existing inference-time scaling\napproaches.",
      "pdf_url": "http://arxiv.org/pdf/2509.24967v1",
      "published": "2025-09-29T16:00:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24967v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation",
      "authors": [
        "Jan Ole von Hartz",
        "Lukas Schweizer",
        "Joschka Boedecker",
        "Abhinav Valada"
      ],
      "abstract": "Generative robot policies such as Flow Matching offer flexible, multi-modal\npolicy learning but are sample-inefficient. Although object-centric policies\nimprove sample efficiency, it does not resolve this limitation. In this work,\nwe propose Multi-Stream Generative Policy (MSG), an inference-time composition\nframework that trains multiple object-centric policies and combines them at\ninference to improve generalization and sample efficiency. MSG is\nmodel-agnostic and inference-only, hence widely applicable to various\ngenerative policies and training paradigms. We perform extensive experiments\nboth in simulation and on a real robot, demonstrating that our approach learns\nhigh-quality generative policies from as few as five demonstrations, resulting\nin a 95% reduction in demonstrations, and improves policy performance by 89\npercent compared to single-stream approaches. Furthermore, we present\ncomprehensive ablation studies on various composition strategies and provide\npractical recommendations for deployment. Finally, MSG enables zero-shot object\ninstance transfer. We make our code publicly available at\nhttps://msg.cs.uni-freiburg.de.",
      "pdf_url": "http://arxiv.org/pdf/2509.24956v1",
      "published": "2025-09-29T15:50:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24956v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer",
      "authors": [
        "Sooraj Sathish",
        "Keshav Goyal",
        "Raghuram Bharadwaj Diddigi"
      ],
      "abstract": "Deep Reinforcement Learning (RL) has demonstrated success in solving complex\nsequential decision-making problems by integrating neural networks with the RL\nframework. However, training deep RL models poses several challenges, such as\nthe need for extensive hyperparameter tuning and high computational costs.\nTransfer learning has emerged as a promising strategy to address these\nchallenges by enabling the reuse of knowledge from previously learned tasks for\nnew, related tasks. This avoids the need for retraining models entirely from\nscratch. A commonly used approach for transfer learning in RL is to leverage\nthe internal representations learned by the neural network during training.\nSpecifically, the activations from the last hidden layer can be viewed as\nrefined state representations that encapsulate the essential features of the\ninput. In this work, we investigate whether these representations can be used\nas input for training simpler models, such as linear function approximators, on\nnew tasks. We observe that the representations learned by standard deep RL\nmodels can be highly correlated, which limits their effectiveness when used\nwith linear function approximation. To mitigate this problem, we propose a\nnovel deep Q-learning approach that introduces a regularization term to reduce\npositive correlations between feature representation of states. By leveraging\nthese reduced correlated features, we enable more effective use of linear\nfunction approximation in transfer learning. Through experiments and ablation\nstudies on standard RL benchmarks and MinAtar games, we demonstrate the\nefficacy of our approach in improving transfer learning performance and thereby\nreducing computational overhead.",
      "pdf_url": "http://arxiv.org/pdf/2509.24947v1",
      "published": "2025-09-29T15:44:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24947v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes",
      "authors": [
        "Changsheng Zhao",
        "Ernie Chang",
        "Zechun Liu",
        "Chia-Jung Chang",
        "Wei Wen",
        "Chen Lai",
        "Rick Cao",
        "Yuandong Tian",
        "Raghuraman Krishnamoorthi",
        "Yangyang Shi",
        "Vikas Chandra"
      ],
      "abstract": "The paradigm shift in large language models (LLMs) from instinctive responses\nto chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)\nreasoning capabilities only emerge in sufficiently large models, and (2) such\ncapabilities require training on massive datasets. While the first assumption\nhas already been challenged by recent sub-billion-parameter reasoning models\nsuch as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely\nunquestioned. In this work, we revisit the necessity of scaling to extremely\nlarge corpora (>10T tokens) for reasoning emergence. By carefully curating and\nresampling open-source datasets that we identify as beneficial under our\ndesigned metrics, we demonstrate that strong reasoning abilities can emerge\nwith far less data. Specifically, we show that only ~2T tokens of high-quality\ndata are sufficient, and pre-training with 4.2T tokens on the dataset resampled\nfrom these ~2T tokens, followed by a established post-training procedure,\nenables the development of MobileLLM-R1, a series of sub-billion-parameter\nreasoning models that substantially outperform prior models trained on fully\nopen-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of\n15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.\nRemarkably, despite being trained on only 11.7% of the tokens compared to\nQwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches\nor surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate\nfurther research in this direction, we have released the complete training\nrecipe, data sources, data mixing ratio, and model checkpoints, together with\nthe key insights obtained throughout this study.",
      "pdf_url": "http://arxiv.org/pdf/2509.24945v1",
      "published": "2025-09-29T15:43:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24945v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Scalable GANs with Transformers",
      "authors": [
        "Sangeek Hyun",
        "MinKyu Lee",
        "Jae-Pil Heo"
      ],
      "abstract": "Scalability has driven recent advances in generative modeling, yet its\nprinciples remain underexplored for adversarial learning. We investigate the\nscalability of Generative Adversarial Networks (GANs) through two design\nchoices that have proven to be effective in other types of generative models:\ntraining in a compact Variational Autoencoder latent space and adopting purely\ntransformer-based generators and discriminators. Training in latent space\nenables efficient computation while preserving perceptual fidelity, and this\nefficiency pairs naturally with plain transformers, whose performance scales\nwith computational budget. Building on these choices, we analyze failure modes\nthat emerge when naively scaling GANs. Specifically, we find issues as\nunderutilization of early layers in the generator and optimization instability\nas the network scales. Accordingly, we provide simple and scale-friendly\nsolutions as lightweight intermediate supervision and width-aware learning-rate\nadjustment. Our experiments show that GAT, a purely transformer-based and\nlatent-space GANs, can be easily trained reliably across a wide range of\ncapacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art\nsingle-step, class-conditional generation performance (FID of 2.96) on\nImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.",
      "pdf_url": "http://arxiv.org/pdf/2509.24935v1",
      "published": "2025-09-29T15:36:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24935v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "KIRETT -- A wearable device to support rescue operations using artificial intelligence to improve first aid",
      "authors": [
        "Johannes Zenkert",
        "Christian Weber",
        "Mubaris Nadeem",
        "Lisa Bender",
        "Madjid Fathi",
        "Abu Shad Ahammed",
        "Aniebiet Micheal Ezekiel",
        "Roman Obermaisser",
        "Maximilian Bradford"
      ],
      "abstract": "This short paper presents first steps in the scientific part of the KIRETT\nproject, which aims to improve first aid during rescue operations using a\nwearable device. The wearable is used for computer-aided situation recognition\nby means of artificial intelligence. It provides contextual recommendations for\nactions and operations to rescue personnel and is intended to minimize damage\nto patients due to incorrect treatment, as well as increase the probability of\nsurvival. The paper describes a first overview of research approaches within\nthe project.",
      "pdf_url": "http://arxiv.org/pdf/2509.24934v1",
      "published": "2025-09-29T15:36:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24934v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?",
      "authors": [
        "An Guo",
        "Shuoxiao Zhang",
        "Enyi Tang",
        "Xinyu Gao",
        "Haomin Pang",
        "Haoxiang Tian",
        "Yanzhou Mu",
        "Wu Wen",
        "Chunrong Fang",
        "Zhenyu Chen"
      ],
      "abstract": "With the tremendous advancement of deep learning and communication\ntechnology, Vehicle-to-Everything (V2X) cooperative perception has the\npotential to address limitations in sensing distant objects and occlusion for a\nsingle-agent perception system. V2X cooperative perception systems are software\nsystems characterized by diverse sensor types and cooperative agents, varying\nfusion schemes, and operation under different communication conditions.\nTherefore, their complex composition gives rise to numerous operational\nchallenges. Furthermore, when cooperative perception systems produce erroneous\npredictions, the types of errors and their underlying causes remain\ninsufficiently explored. To bridge this gap, we take an initial step by\nconducting an empirical study of V2X cooperative perception. To systematically\nevaluate the impact of cooperative perception on the ego vehicle's perception\nperformance, we identify and analyze six prevalent error patterns in\ncooperative perception systems. We further conduct a systematic evaluation of\nthe critical components of these systems through our large-scale study and\nidentify the following key findings: (1) The LiDAR-based cooperation\nconfiguration exhibits the highest perception performance; (2)\nVehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication\nexhibit distinct cooperative perception performance under different fusion\nschemes; (3) Increased cooperative perception errors may result in a higher\nfrequency of driving violations; (4) Cooperative perception systems are not\nrobust against communication interference when running online. Our results\nreveal potential risks and vulnerabilities in critical components of\ncooperative perception systems. We hope that our findings can better promote\nthe design and repair of cooperative perception systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.24927v1",
      "published": "2025-09-29T15:28:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24927v1",
      "categories": [
        "cs.AI",
        "cs.RO",
        "cs.SE"
      ]
    },
    {
      "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training",
      "authors": [
        "Sanxing Chen",
        "Xiaoyin Chen",
        "Yukun Huang",
        "Roy Xie",
        "Bhuwan Dhingra"
      ],
      "abstract": "While Large Language Models (LLMs) hold promise to become autonomous agents,\nthey often explore suboptimally in sequential decision-making. Recent work has\nsought to enhance this capability via supervised fine-tuning (SFT) or\nreinforcement learning (RL), improving regret on the classic multi-armed bandit\ntask. However, it remains unclear how these learning methods shape exploration\nstrategies and how well they generalize. We investigate both paradigms by\ntraining LLMs with SFT on expert trajectories and RL with a range of tailored\nreward signals including a strategic, regret-shaped reward to reduce variance,\nand an algorithmic reward that enables oracle imitation. The resulting agents\noutperform pre-trained models and achieve performance comparable to Upper\nConfidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x\nlonger horizons and across bandit families. Behavioral analysis reveals that\ngains often stem from more sophisticated but greedier exploitation: RL/SFT\nagents are more prone to early catastrophic failure than pre-trained models,\nprematurely abandoning exploration. Furthermore, agents trained to imitate UCB\nlearn to outperform their teacher by adopting more exploitative variants. Our\nfindings clarify when each training paradigm is preferable and advocate\ntailored reward design and evaluation beyond average regret to promote robust\nexploratory behavior.",
      "pdf_url": "http://arxiv.org/pdf/2509.24923v1",
      "published": "2025-09-29T15:25:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24923v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning",
      "authors": [
        "Huihao Jing",
        "Wenbin Hu",
        "Hongyu Luo",
        "Jianhui Yang",
        "Wei Fan",
        "Haoran Li",
        "Yangqiu Song"
      ],
      "abstract": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures.",
      "pdf_url": "http://arxiv.org/pdf/2509.24922v1",
      "published": "2025-09-29T15:24:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24922v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Meta-Learning Theory-Informed Inductive Biases using Deep Kernel Gaussian Processes",
      "authors": [
        "Bahti Zakirov",
        "Gašper Tkačik"
      ],
      "abstract": "Normative and task-driven theories offer powerful top-down explanations for\nbiological systems, yet the goals of quantitatively arbitrating between\ncompeting theories, and utilizing them as inductive biases to improve\ndata-driven fits of real biological datasets are prohibitively laborious, and\noften impossible. To this end, we introduce a Bayesian meta-learning framework\ndesigned to automatically convert raw functional predictions from normative\ntheories into tractable probabilistic models. We employ adaptive deep kernel\nGaussian processes, meta-learning a kernel on synthetic data generated from a\nnormative theory. This Theory-Informed Kernel specifies a probabilistic model\nrepresenting the theory predictions -- usable for both fitting data and\nrigorously validating the theory. As a demonstration, we apply our framework to\nthe early visual system, using efficient coding as our normative theory. We\nshow improved response prediction accuracy in ex vivo recordings of mouse\nretinal ganglion cells stimulated by natural scenes compared to conventional\ndata-driven baselines, while providing well-calibrated uncertainty estimates\nand interpretable representations. Using exact Bayesian model selection, we\nalso show that our informed kernel can accurately infer the degree of\ntheory-match from data, confirming faithful encapsulation of theory structure.\nThis work provides a more general, scalable, and automated approach for\nintegrating theoretical knowledge into data-driven scientific inquiry in\nneuroscience and beyond.",
      "pdf_url": "http://arxiv.org/pdf/2509.24919v1",
      "published": "2025-09-29T15:23:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24919v1",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ]
    },
    {
      "title": "Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis",
      "authors": [
        "Tian Xia",
        "Matthew Sinclair",
        "Andreas Schuh",
        "Fabio De Sousa Ribeiro",
        "Raghav Mehta",
        "Rajat Rasal",
        "Esther Puyol-Antón",
        "Samuel Gerber",
        "Kersten Petersen",
        "Michiel Schaap",
        "Ben Glocker"
      ],
      "abstract": "Counterfactual image generation is a powerful tool for augmenting training\ndata, de-biasing datasets, and modeling disease. Current approaches rely on\nexternal classifiers or regressors to increase the effectiveness of\nsubject-level interventions (e.g., changing the patient's age). For\nstructure-specific interventions (e.g., changing the area of the left lung in a\nchest radiograph), we show that this is insufficient, and can result in\nundesirable global effects across the image domain. Previous work used\npixel-level label maps as guidance, requiring a user to provide hypothetical\nsegmentations which are tedious and difficult to obtain. We propose\nSegmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the\nsimplicity of intervening on scalar-valued, structure-specific variables while\nproducing locally coherent and effective counterfactuals. We demonstrate the\ncapability of generating realistic chest radiographs, and we show promising\nresults for modeling coronary artery disease. Code:\nhttps://github.com/biomedia-mira/seg-cft.",
      "pdf_url": "http://arxiv.org/pdf/2509.24913v1",
      "published": "2025-09-29T15:19:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24913v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Neural network embeddings recover value dimensions from psychometric survey items on par with human data",
      "authors": [
        "Max Pellert",
        "Clemens M. Lechner",
        "Indira Sen",
        "Markus Strohmaier"
      ],
      "abstract": "This study introduces \"Survey and Questionnaire Item Embeddings\nDifferentials\" (SQuID), a novel methodological approach that enables neural\nnetwork embeddings to effectively recover latent dimensions from psychometric\nsurvey items. We demonstrate that embeddings derived from large language\nmodels, when processed with SQuID, can recover the structure of human values\nobtained from human rater judgments on the Revised Portrait Value Questionnaire\n(PVQ-RR). Our experimental validation compares multiple embedding models across\na number of evaluation metrics. Unlike previous approaches, SQuID successfully\naddresses the challenge of obtaining negative correlations between dimensions\nwithout requiring domain-specific fine-tuning. Quantitative analysis reveals\nthat our embedding-based approach explains 55% of variance in\ndimension-dimension similarities compared to human data. Multidimensional\nscaling configurations from both types of data show fair factor congruence\ncoefficients and largely follow the underlying theory. These results\ndemonstrate that semantic embeddings can effectively replicate psychometric\nstructures previously established through extensive human surveys. The approach\noffers substantial advantages in cost, scalability and flexibility while\nmaintaining comparable quality to traditional methods. Our findings have\nsignificant implications for psychometrics and social science research,\nproviding a complementary methodology that could expand the scope of human\nbehavior and experience represented in measurement tools.",
      "pdf_url": "http://arxiv.org/pdf/2509.24906v1",
      "published": "2025-09-29T15:14:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24906v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing",
      "authors": [
        "Zhihong Chen",
        "Xuehai Bai",
        "Yang Shi",
        "Chaoyou Fu",
        "Huanyu Zhang",
        "Haotian Wang",
        "Xiaoyan Sun",
        "Zhang Zhang",
        "Liang Wang",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Yi-Fan Zhang"
      ],
      "abstract": "The performance of unified multimodal models for image generation and editing\nis fundamentally constrained by the quality and comprehensiveness of their\ntraining data. While existing datasets have covered basic tasks like style\ntransfer and simple object manipulation, they often lack the systematic\nstructure and challenging scenarios required for real-world applications. To\naddress this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset\nconstructed using a novel methodology that combines hierarchical task taxonomy\nwith automated data generation. Our taxonomy not only includes fundamental\ncapabilities such as text rendering and style control but also introduces\nhighly practical yet challenging categories like scientific imagery for\nchemistry illustrations and complex instruction editing requiring simultaneous\nexecution of multiple operations. Through an automated pipeline leveraging\nstructured resource pools and GPT-4o, we generate 80k high-quality\ninstruction-image pairs with controlled diversity, covering 11 major domains\nand 51 subtasks. Extensive experiments show that fine-tuning leading models on\nour dataset achieves significant performance gains across multiple benchmarks,\nwith improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)\nand 13% on generation tasks (Harmon on GenEval). Our work demonstrates that\nsystematic data construction is key to advancing multimodal AI capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2509.24900v1",
      "published": "2025-09-29T15:11:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24900v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark",
      "authors": [
        "Yang Shi",
        "Yuhao Dong",
        "Yue Ding",
        "Yuran Wang",
        "Xuanyu Zhu",
        "Sheng Zhou",
        "Wenting Liu",
        "Haochen Tian",
        "Rundong Wang",
        "Huanqian Wang",
        "Zuyan Liu",
        "Bohan Zeng",
        "Ruizhe Chen",
        "Qixun Wang",
        "Zhuoran Zhang",
        "Xinlong Chen",
        "Chengzhuo Tong",
        "Bozhou Li",
        "Chaoyou Fu",
        "Qiang Liu",
        "Haotian Wang",
        "Wenjing Yang",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Yi-Fan Zhang",
        "Ziwei Liu"
      ],
      "abstract": "The integration of visual understanding and generation into unified\nmultimodal models represents a significant stride toward general-purpose AI.\nHowever, a fundamental question remains unanswered by existing benchmarks: does\nthis architectural unification actually enable synergetic interaction between\nthe constituent capabilities? Existing evaluation paradigms, which primarily\nassess understanding and generation in isolation, are insufficient for\ndetermining whether a unified model can leverage its understanding to enhance\nits generation, or use generative simulation to facilitate deeper\ncomprehension. To address this critical gap, we introduce RealUnify, a\nbenchmark specifically designed to evaluate bidirectional capability synergy.\nRealUnify comprises 1,000 meticulously human-annotated instances spanning 10\ncategories and 32 subtasks. It is structured around two core axes: 1)\nUnderstanding Enhances Generation, which requires reasoning (e.g., commonsense,\nlogic) to guide image generation, and 2) Generation Enhances Understanding,\nwhich necessitates mental simulation or reconstruction (e.g., of transformed or\ndisordered visual inputs) to solve reasoning tasks. A key contribution is our\ndual-evaluation protocol, which combines direct end-to-end assessment with a\ndiagnostic stepwise evaluation that decomposes tasks into distinct\nunderstanding and generation phases. This protocol allows us to precisely\ndiscern whether performance bottlenecks stem from deficiencies in core\nabilities or from a failure to integrate them. Through large-scale evaluations\nof 12 leading unified models and 6 specialized baselines, we find that current\nunified models still struggle to achieve effective synergy, indicating that\narchitectural unification alone is insufficient. These results highlight the\nneed for new training strategies and inductive biases to fully unlock the\npotential of unified modeling.",
      "pdf_url": "http://arxiv.org/pdf/2509.24897v1",
      "published": "2025-09-29T15:07:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24897v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime",
      "authors": [
        "Leonardo Defilippis",
        "Yizhou Xu",
        "Julius Girardin",
        "Emanuele Troiani",
        "Vittorio Erba",
        "Lenka Zdeborová",
        "Bruno Loureiro",
        "Florent Krzakala"
      ],
      "abstract": "Neural scaling laws underlie many of the recent advances in deep learning,\nyet their theoretical understanding remains largely confined to linear models.\nIn this work, we present a systematic analysis of scaling laws for quadratic\nand diagonal neural networks in the feature learning regime. Leveraging\nconnections with matrix compressed sensing and LASSO, we derive a detailed\nphase diagram for the scaling exponents of the excess risk as a function of\nsample complexity and weight decay. This analysis uncovers crossovers between\ndistinct scaling regimes and plateau behaviors, mirroring phenomena widely\nreported in the empirical neural scaling literature. Furthermore, we establish\na precise link between these regimes and the spectral properties of the trained\nnetwork weights, which we characterize in detail. As a consequence, we provide\na theoretical validation of recent empirical observations connecting the\nemergence of power-law tails in the weight spectrum with network generalization\nperformance, yielding an interpretation from first principles.",
      "pdf_url": "http://arxiv.org/pdf/2509.24882v1",
      "published": "2025-09-29T14:58:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24882v1",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Vehicle Classification under Extreme Imbalance: A Comparative Study of Ensemble Learning and CNNs",
      "authors": [
        "Abu Hanif Muhammad Syarubany"
      ],
      "abstract": "Accurate vehicle type recognition underpins intelligent transportation and\nlogistics, but severe class imbalance in public datasets suppresses performance\non rare categories. We curate a 16-class corpus (~47k images) by merging\nKaggle, ImageNet, and web-crawled data, and create six balanced variants via\nSMOTE oversampling and targeted undersampling. Lightweight ensembles, such as\nRandom Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2\nfeatures are benchmarked against a configurable ResNet-style CNN trained with\nstrong augmentation and label smoothing. The best ensemble (SMOTE-combined)\nattains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set\nand 81.25% on an unseen inference batch, confirming the advantage of deep\nmodels. Nonetheless, the most under-represented class (Barge) remains a failure\nmode, highlighting the limits of rebalancing alone. Results suggest\nprioritizing additional minority-class collection and cost-sensitive objectives\n(e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine\ninterpretability with representational power.",
      "pdf_url": "http://arxiv.org/pdf/2509.24880v1",
      "published": "2025-09-29T14:56:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24880v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Emergence of Social Science of Large Language Models",
      "authors": [
        "Xiao Jia",
        "Zhanzhan Zhao"
      ],
      "abstract": "The social science of large language models (LLMs) examines how these systems\nevoke mind attributions, interact with one another, and transform human\nactivity and institutions. We conducted a systematic review of 270 studies,\ncombining text embeddings, unsupervised clustering and topic modeling to build\na computational taxonomy. Three domains emerge organically across the reviewed\nliterature. LLM as Social Minds examines whether and when models display\nbehaviors that elicit attributions of cognition, morality and bias, while\naddressing challenges such as test leakage and surface cues. LLM Societies\nexamines multi-agent settings where interaction protocols, architectures and\nmechanism design shape coordination, norms, institutions and collective\nepistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,\nlearning, trust, work and governance, and how risks arise at the human-AI\ninterface. This taxonomy provides a reproducible map of a fragmented field,\nclarifies evidentiary standards across levels of analysis, and highlights\nopportunities for cumulative progress in the social science of artificial\nintelligence.",
      "pdf_url": "http://arxiv.org/pdf/2509.24877v1",
      "published": "2025-09-29T14:55:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.24877v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}