{
  "last_updated": "2025-04-15T00:50:24.822164",
  "papers": [
    {
      "title": "Towards an Understanding of Context Utilization in Code Intelligence",
      "authors": [
        "Yanlin Wang",
        "Kefeng Duan",
        "Dewu Zheng",
        "Ensheng Shi",
        "Fengji Zhang",
        "Yanli Wang",
        "Jiachi Chen",
        "Xilin Liu",
        "Yuchi Ma",
        "Hongyu Zhang",
        "Qianxiang Wang",
        "Zibin Zheng"
      ],
      "abstract": "Code intelligence is an emerging domain in software engineering, aiming to\nimprove the effectiveness and efficiency of various code-related tasks. Recent\nresearch suggests that incorporating contextual information beyond the basic\noriginal task inputs (i.e., source code) can substantially enhance model\nperformance. Such contextual signals may be obtained directly or indirectly\nfrom sources such as API documentation or intermediate representations like\nabstract syntax trees can significantly improve the effectiveness of code\nintelligence. Despite growing academic interest, there is a lack of systematic\nanalysis of context in code intelligence. To address this gap, we conduct an\nextensive literature review of 146 relevant studies published between September\n2007 and August 2024. Our investigation yields four main contributions. (1) A\nquantitative analysis of the research landscape, including publication trends,\nvenues, and the explored domains; (2) A novel taxonomy of context types used in\ncode intelligence; (3) A task-oriented analysis investigating context\nintegration strategies across diverse code intelligence tasks; (4) A critical\nevaluation of evaluation methodologies for context-aware methods. Based on\nthese findings, we identify fundamental challenges in context utilization in\ncurrent code intelligence systems and propose a research roadmap that outlines\nkey opportunities for future research.",
      "pdf_url": "http://arxiv.org/pdf/2504.08734v1",
      "published": "2025-04-11T17:59:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08734v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Steering CLIP's vision transformer with sparse autoencoders",
      "authors": [
        "Sonia Joseph",
        "Praneet Suresh",
        "Ethan Goldfarb",
        "Lorenz Hufe",
        "Yossi Gandelsman",
        "Robert Graham",
        "Danilo Bzdok",
        "Wojciech Samek",
        "Blake Aaron Richards"
      ],
      "abstract": "While vision models are highly capable, their internal mechanisms remain\npoorly understood -- a challenge which sparse autoencoders (SAEs) have helped\naddress in language, but which remains underexplored in vision. We address this\ngap by training SAEs on CLIP's vision transformer and uncover key differences\nbetween vision and language processing, including distinct sparsity patterns\nfor SAEs trained across layers and token types. We then provide the first\nsystematic analysis on the steerability of CLIP's vision transformer by\nintroducing metrics to quantify how precisely SAE features can be steered to\naffect the model's output. We find that 10-15\\% of neurons and features are\nsteerable, with SAEs providing thousands more steerable features than the base\nmodel. Through targeted suppression of SAE features, we then demonstrate\nimproved performance on three vision disentanglement tasks (CelebA, Waterbirds,\nand typographic attacks), finding optimal disentanglement in middle model\nlayers, and achieving state-of-the-art performance on defense against\ntypographic attacks.",
      "pdf_url": "http://arxiv.org/pdf/2504.08729v1",
      "published": "2025-04-11T17:56:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08729v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images",
      "authors": [
        "Boyang Deng",
        "Songyou Peng",
        "Kyle Genova",
        "Gordon Wetzstein",
        "Noah Snavely",
        "Leonidas Guibas",
        "Thomas Funkhouser"
      ],
      "abstract": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
      "pdf_url": "http://arxiv.org/pdf/2504.08727v1",
      "published": "2025-04-11T17:55:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08727v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "DocAgent: A Multi-Agent System for Automated Code Documentation Generation",
      "authors": [
        "Dayu Yang",
        "Antoine Simoulin",
        "Xin Qian",
        "Xiaoyi Liu",
        "Yuwei Cao",
        "Zhaopu Teng",
        "Grey Yang"
      ],
      "abstract": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories.",
      "pdf_url": "http://arxiv.org/pdf/2504.08725v1",
      "published": "2025-04-11T17:50:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08725v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning",
      "authors": [
        "Sahil Sethi",
        "David Chen",
        "Thomas Statchen",
        "Michael C. Burkhart",
        "Nipun Bhandari",
        "Bashar Ramadan",
        "Brett Beaulieu-Jones"
      ],
      "abstract": "Deep learning-based electrocardiogram (ECG) classification has shown\nimpressive performance but clinical adoption has been slowed by the lack of\ntransparent and faithful explanations. Post hoc methods such as saliency maps\nmay fail to reflect a model's true decision process. Prototype-based reasoning\noffers a more transparent alternative by grounding decisions in similarity to\nlearned representations of real ECG segments, enabling faithful, case-based\nexplanations. We introduce ProtoECGNet, a prototype-based deep learning model\nfor interpretable, multi-label ECG classification. ProtoECGNet employs a\nstructured, multi-branch architecture that reflects clinical interpretation\nworkflows: it integrates a 1D CNN with global prototypes for rhythm\nclassification, a 2D CNN with time-localized prototypes for morphology-based\nreasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each\nbranch is trained with a prototype loss designed for multi-label learning,\ncombining clustering, separation, diversity, and a novel contrastive loss that\nencourages appropriate separation between prototypes of unrelated classes while\nallowing clustering for frequently co-occurring diagnoses. We evaluate\nProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating\ncompetitive performance relative to state-of-the-art black-box models while\nproviding structured, case-based explanations. To assess prototype quality, we\nconduct a structured clinician review of the final model's projected\nprototypes, finding that they are rated as representative and clear.\nProtoECGNet shows that prototype learning can be effectively scaled to complex,\nmulti-label time-series classification, offering a practical path toward\ntransparent and trustworthy deep learning models for clinical decision support.",
      "pdf_url": "http://arxiv.org/pdf/2504.08713v1",
      "published": "2025-04-11T17:23:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08713v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models",
      "authors": [
        "Yiliu Sun",
        "Yanfang Zhang",
        "Zicheng Zhao",
        "Sheng Wan",
        "Dacheng Tao",
        "Chen Gong"
      ],
      "abstract": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve\ncomplex tasks. To face the challenge, task decomposition has become an\neffective way, which proposes to divide a complex task into multiple simpler\nsubtasks and then solve them separately so that the difficulty of the original\ntask can be reduced. However, the performance of existing task decomposition\nmethods can be suboptimal when the task contains overly complex logic and\nconstraints. In this situation, the solution generated by LLMs may deviate from\nthe original purpose of the task, or contain redundant or even erroneous\ncontent. Therefore, inspired by the fact that humans possess two thinking\nsystems including fast thinking and slow thinking, this paper introduces a new\ntask decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates\nLLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow\nThinking (ST) steps. Here FT focuses more on the general and concise aspect of\nthe task, and ST focuses more on the details of the task. In FT, LLMs are\nprompted to remove the constraints of the original task, therefore simplifying\nit to a general and concise one. In ST, we recall the constraints removed in\nFT, so that LLMs can improve the answer generated in FT to meet the\nrequirements of the original task. Therefore, our FST method enables LLMs to\nconsider a complex problem via a human-like cognition process from coarse to\nfine, the effectiveness of which has been well demonstrated by the experiments\non three types of tasks.",
      "pdf_url": "http://arxiv.org/pdf/2504.08690v1",
      "published": "2025-04-11T16:57:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08690v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Voice Interaction With Conversational AI Could Facilitate Thoughtful Reflection and Substantive Revision in Writing",
      "authors": [
        "Jiho Kim",
        "Philippe Laban",
        "Xiang 'Anthony' Chen",
        "Kenneth C. Arnold"
      ],
      "abstract": "Writing well requires not only expressing ideas but also refining them\nthrough revision, a process facilitated by reflection. Prior research suggests\nthat feedback delivered through dialogues, such as those in writing center\ntutoring sessions, can help writers reflect more thoughtfully on their work\ncompared to static feedback. Recent advancements in multi-modal large language\nmodels (LLMs) now offer new possibilities for supporting interactive and\nexpressive voice-based reflection in writing. In particular, we propose that\nLLM-generated static feedback can be repurposed as conversation starters,\nallowing writers to seek clarification, request examples, and ask follow-up\nquestions, thereby fostering deeper reflection on their writing. We argue that\nvoice-based interaction can naturally facilitate this conversational exchange,\nencouraging writers' engagement with higher-order concerns, facilitating\niterative refinement of their reflections, and reduce cognitive load compared\nto text-based interactions. To investigate these effects, we propose a\nformative study exploring how text vs. voice input influence writers'\nreflection and subsequent revisions. Findings from this study will inform the\ndesign of intelligent and interactive writing tools, offering insights into how\nvoice-based interactions with LLM-powered conversational agents can support\nreflection and revision.",
      "pdf_url": "http://arxiv.org/pdf/2504.08687v1",
      "published": "2025-04-11T16:54:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08687v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "H.5.2; I.2.7"
      ]
    },
    {
      "title": "Pobogot -- An Open-Hardware Open-Source Low Cost Robot for Swarm Robotics",
      "authors": [
        "Alessia Loi",
        "Loona Macabre",
        "Jérémy Fersula",
        "Keivan Amini",
        "Leo Cazenille",
        "Fabien Caura",
        "Alexandre Guerre",
        "Stéphane Gourichon",
        "Olivier Dauchot",
        "Nicolas Bredeche"
      ],
      "abstract": "This paper describes the Pogobot, an open-source and open-hardware platform\nspecifically designed for research involving swarm robotics. Pogobot features\nvibration-based locomotion, infrared communication, and an array of sensors in\na cost-effective package (approx. 250~euros/unit). The platform's modular\ndesign, comprehensive API, and extensible architecture facilitate the\nimplementation of swarm intelligence algorithms and distributed online\nreinforcement learning algorithms. Pogobots offer an accessible alternative to\nexisting platforms while providing advanced capabilities including directional\ncommunication between units. More than 200 Pogobots are already being used on a\ndaily basis at Sorbonne Universit\\'e and PSL to study self-organizing systems,\nprogrammable active matter, discrete reaction-diffusion-advection systems as\nwell as models of social learning and evolution.",
      "pdf_url": "http://arxiv.org/pdf/2504.08686v1",
      "published": "2025-04-11T16:47:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08686v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
      "authors": [
        "Team Seawead",
        "Ceyuan Yang",
        "Zhijie Lin",
        "Yang Zhao",
        "Shanchuan Lin",
        "Zhibei Ma",
        "Haoyuan Guo",
        "Hao Chen",
        "Lu Qi",
        "Sen Wang",
        "Feng Cheng",
        "Feilong Zuo Xuejiao Zeng",
        "Ziyan Yang",
        "Fangyuan Kong",
        "Zhiwu Qing",
        "Fei Xiao",
        "Meng Wei",
        "Tuyen Hoang",
        "Siyu Zhang",
        "Peihao Zhu",
        "Qi Zhao",
        "Jiangqiao Yan",
        "Liangke Gui",
        "Sheng Bi",
        "Jiashi Li",
        "Yuxi Ren",
        "Rui Wang",
        "Huixia Li",
        "Xuefeng Xiao",
        "Shu Liu",
        "Feng Ling",
        "Heng Zhang",
        "Houmin Wei",
        "Huafeng Kuang",
        "Jerry Duncan",
        "Junda Zhang",
        "Junru Zheng",
        "Li Sun",
        "Manlin Zhang",
        "Renfei Sun",
        "Xiaobin Zhuang",
        "Xiaojie Li",
        "Xin Xia",
        "Xuyan Chi",
        "Yanghua Peng",
        "Yuping Wang",
        "Yuxuan Wang",
        "Zhongkai Zhao",
        "Zhuo Chen",
        "Zuquan Song",
        "Zhenheng Yang",
        "Jiashi Feng",
        "Jianchao Yang",
        "Lu Jiang"
      ],
      "abstract": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/",
      "pdf_url": "http://arxiv.org/pdf/2504.08685v1",
      "published": "2025-04-11T16:46:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08685v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning",
      "authors": [
        "Fangzhi Xu",
        "Hang Yan",
        "Chang Ma",
        "Haiteng Zhao",
        "Qiushi Sun",
        "Kanzhi Cheng",
        "Junxian He",
        "Jun Liu",
        "Zhiyong Wu"
      ],
      "abstract": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
      "pdf_url": "http://arxiv.org/pdf/2504.08672v1",
      "published": "2025-04-11T16:26:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08672v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Designing Child-Friendly AI Interfaces: Six Developmentally-Appropriate Design Insights from Analysing Disney Animation",
      "authors": [
        "Nomisha Kurian"
      ],
      "abstract": "To build AI interfaces that children can intuitively understand and use,\ndesigners need a design grammar that truly serves children's developmental\nneeds. This paper bridges Artificial Intelligence design for children -- an\nemerging field still defining its best practices -- and children's animation, a\nwell-established field with decades of experience in engaging young viewers\nthrough emotionally resonant, cognitively accessible storytelling. Pairing\nPiagetian developmental theory with design pattern extraction from 52 works of\nDisney animation, the paper presents six design insights transferable to\nchild-centred AI interface design: (1) emotional expressiveness and visual\nclarity, (2) musical and auditory scaffolding, (3) audiovisual synchrony for\nemotional comfort, (4) sidekick-style personas, (5) support for symbolic play\nand imaginative exploration, and (6) predictable and scaffolded interaction\nstructures. These strategies -- long refined in Disney animation -- function as\nmultimodal scaffolds for attention, understanding, and emotional attunement,\nthereby forming a structured design grammar familiar to children and\ntransferable to AI interface design. By reframing cinematic storytelling as\ndesign logic for AI, the paper offers heuristics for crafting intuitive AI\ninterfaces that align with children's cognitive stages and emotional needs. The\nwork contributes to design theory by showing how sensory, affective and\nnarrative techniques can inform developmentally attuned AI design for children.\nFuture directions include empirical testing, cultural adaptation, and\nparticipatory co-design.",
      "pdf_url": "http://arxiv.org/pdf/2504.08670v1",
      "published": "2025-04-11T16:23:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08670v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Variability-Driven User-Story Generation using LLM and Triadic Concept Analysis",
      "authors": [
        "Alexandre Bazin",
        "Alain Gutierrez",
        "Marianne Huchard",
        "Pierre Martin",
        "Yulin",
        "Zhang"
      ],
      "abstract": "A widely used Agile practice for requirements is to produce a set of user\nstories (also called ``agile product backlog''), which roughly includes a list\nof pairs (role, feature), where the role handles the feature for a certain\npurpose. In the context of Software Product Lines, the requirements for a\nfamily of similar systems is thus a family of user-story sets, one per system,\nleading to a 3-dimensional dataset composed of sets of triples (system, role,\nfeature). In this paper, we combine Triadic Concept Analysis (TCA) and Large\nLanguage Model (LLM) prompting to suggest the user-story set required to\ndevelop a new system relying on the variability logic of an existing system\nfamily. This process consists in 1) computing 3-dimensional variability\nexpressed as a set of TCA implications, 2) providing the designer with\nintelligible design options, 3) capturing the designer's selection of options,\n4) proposing a first user-story set corresponding to this selection, 5)\nconsolidating its validity according to the implications identified in step 1,\nwhile completing it if necessary, and 6) leveraging LLM to have a more\ncomprehensive website. This process is evaluated with a dataset comprising the\nuser-story sets of 67 similar-purpose websites.",
      "pdf_url": "http://arxiv.org/pdf/2504.08666v1",
      "published": "2025-04-11T16:15:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08666v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Title block detection and information extraction for enhanced building drawings search",
      "authors": [
        "Alessio Lombardi",
        "Li Duan",
        "Ahmed Elnagar",
        "Ahmed Zaalouk",
        "Khalid Ismail",
        "Edlira Vakaj"
      ],
      "abstract": "The architecture, engineering, and construction (AEC) industry still heavily\nrelies on information stored in drawings for building construction,\nmaintenance, compliance and error checks. However, information extraction (IE)\nfrom building drawings is often time-consuming and costly, especially when\ndealing with historical buildings. Drawing search can be simplified by\nleveraging the information stored in the title block portion of the drawing,\nwhich can be seen as drawing metadata. However, title block IE can be complex\nespecially when dealing with historical drawings which do not follow existing\nstandards for uniformity. This work performs a comparison of existing methods\nfor this kind of IE task, and then proposes a novel title block detection and\nIE pipeline which outperforms existing methods, in particular when dealing with\ncomplex, noisy historical drawings. The pipeline is obtained by combining a\nlightweight Convolutional Neural Network and GPT-4o, the proposed inference\npipeline detects building engineering title blocks with high accuracy, and then\nextract structured drawing metadata from the title blocks, which can be used\nfor drawing search, filtering and grouping. The work demonstrates high accuracy\nand efficiency in IE for both vector (CAD) and hand-drawn (historical)\ndrawings. A user interface (UI) that leverages the extracted metadata for\ndrawing search is established and deployed on real projects, which demonstrates\nsignificant time savings. Additionally, an extensible domain-expert-annotated\ndataset for title block detection is developed, via an efficient AEC-friendly\nannotation workflow that lays the foundation for future work.",
      "pdf_url": "http://arxiv.org/pdf/2504.08645v1",
      "published": "2025-04-11T15:45:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08645v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization",
      "authors": [
        "Jialu Li",
        "Shoubin Yu",
        "Han Lin",
        "Jaemin Cho",
        "Jaehong Yoon",
        "Mohit Bansal"
      ],
      "abstract": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.",
      "pdf_url": "http://arxiv.org/pdf/2504.08641v1",
      "published": "2025-04-11T15:41:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08641v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents",
      "authors": [
        "Alessio Buscemi",
        "Daniele Proverbio",
        "Paolo Bova",
        "Nataliya Balabanova",
        "Adeela Bashir",
        "Theodor Cimpeanu",
        "Henrique Correia da Fonseca",
        "Manh Hong Duong",
        "Elias Fernandez Domingos",
        "Antonio M. Fernandes",
        "Marcus Krellner",
        "Ndidi Bianca Ogbo",
        "Simon T. Powers",
        "Fernando P. Santos",
        "Zia Ush Shamszaman",
        "Zhao Song",
        "Alessandro Di Stefano",
        "The Anh Han"
      ],
      "abstract": "There is general agreement that fostering trust and cooperation within the AI\ndevelopment ecosystem is essential to promote the adoption of trustworthy AI\nsystems. By embedding Large Language Model (LLM) agents within an evolutionary\ngame-theoretic framework, this paper investigates the complex interplay between\nAI developers, regulators and users, modelling their strategic choices under\ndifferent regulatory scenarios. Evolutionary game theory (EGT) is used to\nquantitatively model the dilemmas faced by each actor, and LLMs provide\nadditional degrees of complexity and nuances and enable repeated games and\nincorporation of personality traits. Our research identifies emerging\nbehaviours of strategic AI agents, which tend to adopt more \"pessimistic\" (not\ntrusting and defective) stances than pure game-theoretic agents. We observe\nthat, in case of full trust by users, incentives are effective to promote\neffective regulation; however, conditional trust may deteriorate the \"social\npact\". Establishing a virtuous feedback between users' trust and regulators'\nreputation thus appears to be key to nudge developers towards creating safe AI.\nHowever, the level at which this trust emerges may depend on the specific LLM\nused for testing. Our results thus provide guidance for AI regulation systems,\nand help predict the outcome of strategic LLM agents, should they be used to\naid regulation itself.",
      "pdf_url": "http://arxiv.org/pdf/2504.08640v1",
      "published": "2025-04-11T15:41:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08640v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.GT",
        "nlin.CD"
      ]
    },
    {
      "title": "Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines",
      "authors": [
        "Athanasios Athanasopoulos",
        "Matúš Mihalák",
        "Marcin Pietrasik"
      ],
      "abstract": "One of the key safety considerations of battery manufacturing is thermal\nrunaway, the uncontrolled increase in temperature which can lead to fires,\nexplosions, and emissions of toxic gasses. As such, development of automated\nsystems capable of detecting such events is of considerable importance in both\nacademic and industrial contexts. In this work, we investigate the use of deep\nlearning for detecting thermal runaway in the battery production line of VDL\nNedcar, a Dutch automobile manufacturer. Specifically, we collect data from the\nproduction line to represent both baseline (non thermal runaway) and thermal\nrunaway conditions. Thermal runaway was simulated through the use of external\nheat and smoke sources. The data consisted of both optical and thermal images\nwhich were then preprocessed and fused before serving as input to our models.\nIn this regard, we evaluated three deep-learning models widely used in computer\nvision including shallow convolutional neural networks, residual neural\nnetworks, and vision transformers on two performance metrics. Furthermore, we\nevaluated these models using explainability methods to gain insight into their\nability to capture the relevant feature information from their inputs. The\nobtained results indicate that the use of deep learning is a viable approach to\nthermal runaway detection in battery production lines.",
      "pdf_url": "http://arxiv.org/pdf/2504.08632v1",
      "published": "2025-04-11T15:35:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08632v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Task-conditioned Ensemble of Expert Models for Continuous Learning",
      "authors": [
        "Renu Sharma",
        "Debasmita Pal",
        "Arun Ross"
      ],
      "abstract": "One of the major challenges in machine learning is maintaining the accuracy\nof the deployed model (e.g., a classifier) in a non-stationary environment. The\nnon-stationary environment results in distribution shifts and, consequently, a\ndegradation in accuracy. Continuous learning of the deployed model with new\ndata could be one remedy. However, the question arises as to how we should\nupdate the model with new training data so that it retains its accuracy on the\nold data while adapting to the new data. In this work, we propose a\ntask-conditioned ensemble of models to maintain the performance of the existing\nmodel. The method involves an ensemble of expert models based on task\nmembership information. The in-domain models-based on the local outlier concept\n(different from the expert models) provide task membership information\ndynamically at run-time to each probe sample. To evaluate the proposed method,\nwe experiment with three setups: the first represents distribution shift\nbetween tasks (LivDet-Iris-2017), the second represents distribution shift both\nbetween and within tasks (LivDet-Iris-2020), and the third represents disjoint\ndistribution between tasks (Split MNIST). The experiments highlight the\nbenefits of the proposed method. The source code is available at\nhttps://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.",
      "pdf_url": "http://arxiv.org/pdf/2504.08626v1",
      "published": "2025-04-11T15:27:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08626v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies",
      "authors": [
        "Vineeth Sai Narajala",
        "Idan Habler"
      ],
      "abstract": "The Model Context Protocol (MCP), introduced by Anthropic, provides a\nstandardized framework for artificial intelligence (AI) systems to interact\nwith external data sources and tools in real-time. While MCP offers significant\nadvantages for AI integration and capability extension, it introduces novel\nsecurity challenges that demand rigorous analysis and mitigation. This paper\nbuilds upon foundational research into MCP architecture and preliminary\nsecurity assessments to deliver enterprise-grade mitigation frameworks and\ndetailed technical implementation strategies. Through systematic threat\nmodeling and analysis of MCP implementations and analysis of potential attack\nvectors, including sophisticated threats like tool poisoning, we present\nactionable security patterns tailored for MCP implementers and adopters. The\nprimary contribution of this research lies in translating theoretical security\nconcerns into a practical, implementable framework with actionable controls,\nthereby providing essential guidance for the secure enterprise adoption and\ngovernance of integrated AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2504.08623v1",
      "published": "2025-04-11T15:25:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08623v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English",
      "authors": [
        "Julian Bäumler",
        "Louis Blöcher",
        "Lars-Joel Frey",
        "Xian Chen",
        "Markus Bayer",
        "Christian Reuter"
      ],
      "abstract": "The dissemination of online hate speech can have serious negative\nconsequences for individuals, online communities, and entire societies. This\nand the large volume of hateful online content prompted both practitioners',\ni.e., in content moderation or law enforcement, and researchers' interest in\nmachine learning models to automatically classify instances of hate speech.\nWhereas most scientific works address hate speech classification as a binary\ntask, practice often requires a differentiation into sub-types, e.g., according\nto target, severity, or legality, which may overlap for individual content.\nHence, researchers created datasets and machine learning models that approach\nhate speech classification in textual data as a multi-label problem. This work\npresents the first systematic and comprehensive survey of scientific literature\non this emerging research landscape in English (N=46). We contribute with a\nconcise overview of 28 datasets suited for training multi-label classification\nmodels that reveals significant heterogeneity regarding label-set, size,\nmeta-concept, annotation process, and inter-annotator agreement. Our analysis\nof 24 publications proposing suitable classification models further establishes\ninconsistency in evaluation and a preference for architectures based on\nBidirectional Encoder Representation from Transformers (BERT) and Recurrent\nNeural Networks (RNNs). We identify imbalanced training data, reliance on\ncrowdsourcing platforms, small and sparse datasets, and missing methodological\nalignment as critical open issues and formulate ten recommendations for\nresearch.",
      "pdf_url": "http://arxiv.org/pdf/2504.08609v1",
      "published": "2025-04-11T15:16:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08609v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Neural Fidelity Calibration for Informative Sim-to-Real Adaptation",
      "authors": [
        "Youwei Yu",
        "Lantao Liu"
      ],
      "abstract": "Deep reinforcement learning can seamlessly transfer agile locomotion and\nnavigation skills from the simulator to real world. However, bridging the\nsim-to-real gap with domain randomization or adversarial methods often demands\nexpert physics knowledge to ensure policy robustness. Even so, cutting-edge\nsimulators may fall short of capturing every real-world detail, and the\nreconstructed environment may introduce errors due to various perception\nuncertainties. To address these challenges, we propose Neural Fidelity\nCalibration (NFC), a novel framework that employs conditional score-based\ndiffusion models to calibrate simulator physical coefficients and residual\nfidelity domains online during robot execution. Specifically, the residual\nfidelity reflects the simulation model shift relative to the real-world\ndynamics and captures the uncertainty of the perceived environment, enabling us\nto sample realistic environments under the inferred distribution for policy\nfine-tuning. Our framework is informative and adaptive in three key ways: (a)\nwe fine-tune the pretrained policy only under anomalous scenarios, (b) we build\nsequential NFC online with the pretrained NFC's proposal prior, reducing the\ndiffusion model's training burden, and (c) when NFC uncertainty is high and may\ndegrade policy improvement, we leverage optimistic exploration to enable\nhallucinated policy optimization. Our framework achieves superior simulator\ncalibration precision compared to state-of-the-art methods across diverse\nrobots with high-dimensional parametric spaces. We study the critical\ncontribution of residual fidelity to policy improvement in simulation and\nreal-world experiments. Notably, our approach demonstrates robust robot\nnavigation under challenging real-world conditions, such as a broken wheel axle\non snowy surfaces.",
      "pdf_url": "http://arxiv.org/pdf/2504.08604v1",
      "published": "2025-04-11T15:12:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08604v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment",
      "authors": [
        "Sebastián Barbas Laina",
        "Simon Boche",
        "Sotiris Papatheodorou",
        "Simon Schaefer",
        "Jaehyung Jung",
        "Stefan Leutenegger"
      ],
      "abstract": "Geometrically accurate and semantically expressive map representations have\nproven invaluable to facilitate robust and safe mobile robot navigation and\ntask planning. Nevertheless, real-time, open-vocabulary semantic understanding\nof large-scale unknown environments is still an open problem. In this paper we\npresent FindAnything, an open-world mapping and exploration framework that\nincorporates vision-language information into dense volumetric submaps. Thanks\nto the use of vision-language features, FindAnything bridges the gap between\npure geometric and open-vocabulary semantic information for a higher level of\nunderstanding while allowing to explore any environment without the help of any\nexternal source of ground-truth pose information. We represent the environment\nas a series of volumetric occupancy submaps, resulting in a robust and accurate\nmap representation that deforms upon pose updates when the underlying SLAM\nsystem corrects its drift, allowing for a locally consistent representation\nbetween submaps. Pixel-wise vision-language features are aggregated from\nefficient SAM (eSAM)-generated segments, which are in turn integrated into\nobject-centric volumetric submaps, providing a mapping from open-vocabulary\nqueries to 3D geometry that is scalable also in terms of memory usage. The\nopen-vocabulary map representation of FindAnything achieves state-of-the-art\nsemantic accuracy in closed-set evaluations on the Replica dataset. This level\nof scene understanding allows a robot to explore environments based on objects\nor areas of interest selected via natural language queries. Our system is the\nfirst of its kind to be deployed on resource-constrained devices, such as MAVs,\nleveraging vision-language information for real-world robotic tasks.",
      "pdf_url": "http://arxiv.org/pdf/2504.08603v1",
      "published": "2025-04-11T15:12:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08603v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "On Background Bias of Post-Hoc Concept Embeddings in Computer Vision DNNs",
      "authors": [
        "Gesina Schwalbe",
        "Georgii Mikriukov",
        "Edgar Heinert",
        "Stavros Gerolymatos",
        "Mert Keser",
        "Alois Knoll",
        "Matthias Rottmann",
        "Annika Mütze"
      ],
      "abstract": "The thriving research field of concept-based explainable artificial\nintelligence (C-XAI) investigates how human-interpretable semantic concepts\nembed in the latent spaces of deep neural networks (DNNs). Post-hoc approaches\ntherein use a set of examples to specify a concept, and determine its\nembeddings in DNN latent space using data driven techniques. This proved useful\nto uncover biases between different target (foreground or concept) classes.\nHowever, given that the background is mostly uncontrolled during training, an\nimportant question has been left unattended so far: Are/to what extent are\nstate-of-the-art, data-driven post-hoc C-XAI approaches themselves prone to\nbiases with respect to their backgrounds? E.g., wild animals mostly occur\nagainst vegetation backgrounds, and they seldom appear on roads. Even simple\nand robust C-XAI methods might abuse this shortcut for enhanced performance. A\ndangerous performance degradation of the concept-corner cases of animals on the\nroad could thus remain undiscovered. This work validates and thoroughly\nconfirms that established Net2Vec-based concept segmentation techniques\nfrequently capture background biases, including alarming ones, such as\nunderperformance on road scenes. For the analysis, we compare 3 established\ntechniques from the domain of background randomization on >50 concepts from 2\ndatasets, and 7 diverse DNN architectures. Our results indicate that even\nlow-cost setups can provide both valuable insight and improved background\nrobustness.",
      "pdf_url": "http://arxiv.org/pdf/2504.08602v1",
      "published": "2025-04-11T15:10:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08602v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MedHal: An Evaluation Dataset for Medical Hallucination Detection",
      "authors": [
        "Gaya Mehenni",
        "Amal Zouaq"
      ],
      "abstract": "We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research.",
      "pdf_url": "http://arxiv.org/pdf/2504.08596v1",
      "published": "2025-04-11T14:55:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08596v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ]
    },
    {
      "title": "Hands-On: Segmenting Individual Signs from Continuous Sequences",
      "authors": [
        "Low Jian He",
        "Harry Walsh",
        "Ozge Mercanoglu Sincan",
        "Richard Bowden"
      ],
      "abstract": "This work tackles the challenge of continuous sign language segmentation, a\nkey task with huge implications for sign language translation and data\nannotation. We propose a transformer-based architecture that models the\ntemporal dynamics of signing and frames segmentation as a sequence labeling\nproblem using the Begin-In-Out (BIO) tagging scheme. Our method leverages the\nHaMeR hand features, and is complemented with 3D Angles. Extensive experiments\nshow that our model achieves state-of-the-art results on the DGS Corpus, while\nour features surpass prior benchmarks on BSLCorpus.",
      "pdf_url": "http://arxiv.org/pdf/2504.08593v1",
      "published": "2025-04-11T14:52:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08593v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Ready, Bid, Go! On-Demand Delivery Using Fleets of Drones with Unknown, Heterogeneous Energy Storage Constraints",
      "authors": [
        "Mohamed S. Talamali",
        "Genki Miyauchi",
        "Thomas Watteyne",
        "Micael S. Couceiro",
        "Roderich Gross"
      ],
      "abstract": "Unmanned Aerial Vehicles (UAVs) are expected to transform logistics, reducing\ndelivery time, costs, and emissions. This study addresses an on-demand delivery\n, in which fleets of UAVs are deployed to fulfil orders that arrive\nstochastically. Unlike previous work, it considers UAVs with heterogeneous,\nunknown energy storage capacities and assumes no knowledge of the energy\nconsumption models. We propose a decentralised deployment strategy that\ncombines auction-based task allocation with online learning. Each UAV\nindependently decides whether to bid for orders based on its energy storage\ncharge level, the parcel mass, and delivery distance. Over time, it refines its\npolicy to bid only for orders within its capability. Simulations using\nrealistic UAV energy models reveal that, counter-intuitively, assigning orders\nto the least confident bidders reduces delivery times and increases the number\nof successfully fulfilled orders. This strategy is shown to outperform\nthreshold-based methods which require UAVs to exceed specific charge levels at\ndeployment. We propose a variant of the strategy which uses learned policies\nfor forecasting. This enables UAVs with insufficient charge levels to commit to\nfulfilling orders at specific future times, helping to prioritise early orders.\nOur work provides new insights into long-term deployment of UAV swarms,\nhighlighting the advantages of decentralised energy-aware decision-making\ncoupled with online learning in real-world dynamic environments.",
      "pdf_url": "http://arxiv.org/pdf/2504.08585v1",
      "published": "2025-04-11T14:39:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08585v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Boosting multi-demographic federated learning for chest x-ray analysis using general-purpose self-supervised representations",
      "authors": [
        "Mahshad Lotfinia",
        "Arash Tayebiarasteh",
        "Samaneh Samiei",
        "Mehdi Joodaki",
        "Soroosh Tayebi Arasteh"
      ],
      "abstract": "Reliable artificial intelligence (AI) models for medical image analysis often\ndepend on large and diverse labeled datasets. Federated learning (FL) offers a\ndecentralized and privacy-preserving approach to training but struggles in\nhighly non-independent and identically distributed (non-IID) settings, where\ninstitutions with more representative data may experience degraded performance.\nMoreover, existing large-scale FL studies have been limited to adult datasets,\nneglecting the unique challenges posed by pediatric data, which introduces\nadditional non-IID variability. To address these limitations, we analyzed\nn=398,523 adult chest radiographs from diverse institutions across multiple\ncountries and n=9,125 pediatric images, leveraging transfer learning from\ngeneral-purpose self-supervised image representations to classify pneumonia and\ncases with no abnormality. Using state-of-the-art vision transformers, we found\nthat FL improved performance only for smaller adult datasets (P<0.001) but\ndegraded performance for larger datasets (P<0.064) and pediatric cases\n(P=0.242). However, equipping FL with self-supervised weights significantly\nenhanced outcomes across pediatric cases (P=0.031) and most adult datasets\n(P<0.008), except the largest dataset (P=0.052). These findings underscore the\npotential of easily deployable general-purpose self-supervised image\nrepresentations to address non-IID challenges in clinical FL applications and\nhighlight their promise for enhancing patient outcomes and advancing pediatric\nhealthcare, where data scarcity and variability remain persistent obstacles.",
      "pdf_url": "http://arxiv.org/pdf/2504.08584v1",
      "published": "2025-04-11T14:38:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08584v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Uncovering the Structure of Explanation Quality with Spectral Analysis",
      "authors": [
        "Johannes Maeß",
        "Grégoire Montavon",
        "Shinichi Nakajima",
        "Klaus-Robert Müller",
        "Thomas Schnake"
      ],
      "abstract": "As machine learning models are increasingly considered for high-stakes\ndomains, effective explanation methods are crucial to ensure that their\nprediction strategies are transparent to the user. Over the years, numerous\nmetrics have been proposed to assess quality of explanations. However, their\npractical applicability remains unclear, in particular due to a limited\nunderstanding of which specific aspects each metric rewards. In this paper we\npropose a new framework based on spectral analysis of explanation outcomes to\nsystematically capture the multifaceted properties of different explanation\ntechniques. Our analysis uncovers two distinct factors of explanation\nquality-stability and target sensitivity-that can be directly observed through\nspectral decomposition. Experiments on both MNIST and ImageNet show that\npopular evaluation techniques (e.g., pixel-flipping, entropy) partially capture\nthe trade-offs between these factors. Overall, our framework provides a\nfoundational basis for understanding explanation quality, guiding the\ndevelopment of more reliable techniques for evaluating explanations.",
      "pdf_url": "http://arxiv.org/pdf/2504.08553v1",
      "published": "2025-04-11T14:03:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08553v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards an Evaluation Framework for Explainable Artificial Intelligence Systems for Health and Well-being",
      "authors": [
        "Esperança Amengual-Alcover",
        "Antoni Jaume-i-Capó",
        "Miquel Miró-Nicolau",
        "Gabriel Moyà-Alcover",
        "Antonia Paniza-Fullana"
      ],
      "abstract": "The integration of Artificial Intelligence in the development of computer\nsystems presents a new challenge: make intelligent systems explainable to\nhumans. This is especially vital in the field of health and well-being, where\ntransparency in decision support systems enables healthcare professionals to\nunderstand and trust automated decisions and predictions. To address this need,\ntools are required to guide the development of explainable AI systems. In this\npaper, we introduce an evaluation framework designed to support the development\nof explainable AI systems for health and well-being. Additionally, we present a\ncase study that illustrates the application of the framework in practice. We\nbelieve that our framework can serve as a valuable tool not only for developing\nexplainable AI systems in healthcare but also for any AI system that has a\nsignificant impact on individuals.",
      "pdf_url": "http://arxiv.org/pdf/2504.08552v1",
      "published": "2025-04-11T14:02:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08552v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Proxy-Anchor and EVT-Driven Continual Learning Method for Generalized Category Discovery",
      "authors": [
        "Alireza Fathalizadeh",
        "Roozbeh Razavi-Far"
      ],
      "abstract": "Continual generalized category discovery has been introduced and studied in\nthe literature as a method that aims to continuously discover and learn novel\ncategories in incoming data batches while avoiding catastrophic forgetting of\npreviously learned categories. A key component in addressing this challenge is\nthe model's ability to separate novel samples, where Extreme Value Theory (EVT)\nhas been effectively employed. In this work, we propose a novel method that\nintegrates EVT with proxy anchors to define boundaries around proxies using a\nprobability of inclusion function, enabling the rejection of unknown samples.\nAdditionally, we introduce a novel EVT-based loss function to enhance the\nlearned representation, achieving superior performance compared to other\ndeep-metric learning methods in similar settings. Using the derived probability\nfunctions, novel samples are effectively separated from previously known\ncategories. However, category discovery within these novel samples can\nsometimes overestimate the number of new categories. To mitigate this issue, we\npropose a novel EVT-based approach to reduce the model size and discard\nredundant proxies. We also incorporate experience replay and knowledge\ndistillation mechanisms during the continual learning stage to prevent\ncatastrophic forgetting. Experimental results demonstrate that our proposed\napproach outperforms state-of-the-art methods in continual generalized category\ndiscovery scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2504.08550v1",
      "published": "2025-04-11T14:01:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08550v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset",
      "authors": [
        "Zhao Dong",
        "Ka Chen",
        "Zhaoyang Lv",
        "Hong-Xing Yu",
        "Yunzhi Zhang",
        "Cheng Zhang",
        "Yufeng Zhu",
        "Stephen Tian",
        "Zhengqin Li",
        "Geordie Moffatt",
        "Sean Christofferson",
        "James Fort",
        "Xiaqing Pan",
        "Mingfei Yan",
        "Jiajun Wu",
        "Carl Yuheng Ren",
        "Richard Newcombe"
      ],
      "abstract": "We introduce Digital Twin Catalog (DTC), a new large-scale photorealistic 3D\nobject digital twin dataset. A digital twin of a 3D object is a highly\ndetailed, virtually indistinguishable representation of a physical object,\naccurately capturing its shape, appearance, physical properties, and other\nattributes. Recent advances in neural-based 3D reconstruction and inverse\nrendering have significantly improved the quality of 3D object reconstruction.\nDespite these advancements, there remains a lack of a large-scale, digital twin\nquality real-world dataset and benchmark that can quantitatively assess and\ncompare the performance of different reconstruction methods, as well as improve\nreconstruction quality through training or fine-tuning. Moreover, to\ndemocratize 3D digital twin creation, it is essential to integrate creation\ntechniques with next-generation egocentric computing platforms, such as AR\nglasses. Currently, there is no dataset available to evaluate 3D object\nreconstruction using egocentric captured images. To address these gaps, the DTC\ndataset features 2,000 scanned digital twin-quality 3D objects, along with\nimage sequences captured under different lighting conditions using DSLR cameras\nand egocentric AR glasses. This dataset establishes the first comprehensive\nreal-world evaluation benchmark for 3D digital twin creation tasks, offering a\nrobust foundation for comparing and improving existing reconstruction methods.\nThe DTC dataset is already released at\nhttps://www.projectaria.com/datasets/dtc/ and we will also make the baseline\nevaluations open-source.",
      "pdf_url": "http://arxiv.org/pdf/2504.08541v1",
      "published": "2025-04-11T13:54:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08541v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "Explainability and Continual Learning meet Federated Learning at the Network Edge",
      "authors": [
        "Thomas Tsouparopoulos",
        "Iordanis Koutsopoulos"
      ],
      "abstract": "As edge devices become more capable and pervasive in wireless networks, there\nis growing interest in leveraging their collective compute power for\ndistributed learning. However, optimizing learning at the network edge entails\nunique challenges, particularly when moving beyond conventional settings and\nobjectives. While Federated Learning (FL) has emerged as a key paradigm for\ndistributed model training, critical challenges persist. First, existing\napproaches often overlook the trade-off between predictive accuracy and\ninterpretability. Second, they struggle to integrate inherently explainable\nmodels such as decision trees because their non-differentiable structure makes\nthem not amenable to backpropagation-based training algorithms. Lastly, they\nlack meaningful mechanisms for continual Machine Learning (ML) model adaptation\nthrough Continual Learning (CL) in resource-limited environments. In this\npaper, we pave the way for a set of novel optimization problems that emerge in\ndistributed learning at the network edge with wirelessly interconnected edge\ndevices, and we identify key challenges and future directions. Specifically, we\ndiscuss how Multi-objective optimization (MOO) can be used to address the\ntrade-off between predictive accuracy and explainability when using complex\npredictive models. Next, we discuss the implications of integrating inherently\nexplainable tree-based models into distributed learning settings. Finally, we\ninvestigate how CL strategies can be effectively combined with FL to support\nadaptive, lifelong learning when limited-size buffers are used to store past\ndata for retraining. Our approach offers a cohesive set of tools for designing\nprivacy-preserving, adaptive, and trustworthy ML solutions tailored to the\ndemands of edge computing and intelligent services.",
      "pdf_url": "http://arxiv.org/pdf/2504.08536v1",
      "published": "2025-04-11T13:45:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08536v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LGRPool: Hierarchical Graph Pooling Via Local-Global Regularisation",
      "authors": [
        "Farshad Noravesh",
        "Reza Haffari",
        "Layki Soon",
        "Arghya Pal"
      ],
      "abstract": "Hierarchical graph pooling(HGP) are designed to consider the fact that\nconventional graph neural networks(GNN) are inherently flat and are also not\nmultiscale. However, most HGP methods suffer not only from lack of considering\nglobal topology of the graph and focusing on the feature learning aspect, but\nalso they do not align local and global features since graphs should inherently\nbe analyzed in a multiscale way. LGRPool is proposed in the present paper as a\nHGP in the framework of expectation maximization in machine learning that\naligns local and global aspects of message passing with each other using a\nregularizer to force the global topological information to be inline with the\nlocal message passing at different scales through the representations at\ndifferent layers of HGP. Experimental results on some graph classification\nbenchmarks show that it slightly outperforms some baselines.",
      "pdf_url": "http://arxiv.org/pdf/2504.08530v1",
      "published": "2025-04-11T13:41:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08530v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Hallucination, reliability, and the role of generative AI in science",
      "authors": [
        "Charles Rathkopf"
      ],
      "abstract": "Generative AI is increasingly used in scientific domains, from protein\nfolding to climate modeling. But these models produce distinctive errors known\nas hallucinations - outputs that are incorrect yet superficially plausible.\nWorse, some arguments suggest that hallucinations are an inevitable consequence\nof the mechanisms underlying generative inference. Fortunately, such arguments\nrely on a conception of hallucination defined solely with respect to internal\nproperties of the model, rather than in reference to the empirical target\nsystem. This conception fails to distinguish epistemically benign errors from\nthose that threaten scientific inference. I introduce the concept of corrosive\nhallucination to capture the epistemically troubling subclass:\nmisrepresentations that are substantively misleading and resistant to\nsystematic anticipation. I argue that although corrosive hallucinations do pose\na threat to scientific reliability, they are not inevitable. Scientific\nworkflows such as those surrounding AlphaFold and GenCast, both of which serve\nas case studies, can neutralize their effects by imposing theoretical\nconstraints during training, and by strategically screening for errors at\ninference time. When embedded in such workflows, generative AI can reliably\ncontribute to scientific knowledge.",
      "pdf_url": "http://arxiv.org/pdf/2504.08526v1",
      "published": "2025-04-11T13:38:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08526v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks",
      "authors": [
        "Ye Ye"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. The full\nimplementation of TME is available at\nhttps://github.com/biubiutomato/TME-Agent.",
      "pdf_url": "http://arxiv.org/pdf/2504.08525v1",
      "published": "2025-04-11T13:38:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08525v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "68T05",
        "I.2.6; I.2.8; H.3.3"
      ]
    },
    {
      "title": "Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion",
      "authors": [
        "Na Li",
        "Chuke Wang",
        "Yu Gu",
        "Zhifeng Li"
      ],
      "abstract": "Voice conversion (VC) transforms source speech into a target voice by\npreserving the content. However, timbre information from the source speaker is\ninherently embedded in the content representations, causing significant timbre\nleakage and reducing similarity to the target speaker. To address this, we\nintroduce a residual block to a content extractor. The residual block consists\nof two weighted branches: 1) universal semantic dictionary based Content\nFeature Re-expression (CFR) module, supplying timbre-free content\nrepresentation. 2) skip connection to the original content layer, providing\ncomplementary fine-grained information. In the CFR module, each dictionary\nentry in the universal semantic dictionary represents a phoneme class, computed\nstatistically using speech from multiple speakers, creating a stable,\nspeaker-independent semantic set. We introduce a CFR method to obtain\ntimbre-free content representations by expressing each content frame as a\nweighted linear combination of dictionary entries using corresponding phoneme\nposteriors as weights. Extensive experiments across various VC frameworks\ndemonstrate that our approach effectively mitigates timbre leakage and\nsignificantly improves similarity to the target speaker.",
      "pdf_url": "http://arxiv.org/pdf/2504.08524v1",
      "published": "2025-04-11T13:36:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08524v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "title": "Adopting Large Language Models to Automated System Integration",
      "authors": [
        "Robin D. Pesl"
      ],
      "abstract": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents.",
      "pdf_url": "http://arxiv.org/pdf/2504.08490v1",
      "published": "2025-04-11T12:42:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08490v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification",
      "authors": [
        "Kerol Djoumessi",
        "Samuel Ofosu Mensah",
        "Philipp Berens"
      ],
      "abstract": "In many medical imaging tasks, convolutional neural networks (CNNs)\nefficiently extract local features hierarchically. More recently, vision\ntransformers (ViTs) have gained popularity, using self-attention mechanisms to\ncapture global dependencies, but lacking the inherent spatial localization of\nconvolutions. Therefore, hybrid models combining CNNs and ViTs have been\ndeveloped to combine the strengths of both architectures. However, such hybrid\nCNN-ViT models are difficult to interpret, which hinders their application in\nmedical imaging. In this work, we introduce an interpretable-by-design hybrid\nfully convolutional CNN-Transformer architecture for medical image\nclassification. Unlike widely used post-hoc saliency methods for ViTs, our\napproach generates faithful and localized evidence maps that directly reflect\nthe model's decision process. We evaluated our method on two medical image\nclassification tasks using color fundus images. Our model not only achieves\nstate-of-the-art predictive performance compared to both black-box and\ninterpretable models but also provides class-specific sparse evidence maps in a\nsingle forward pass. The code is available at:\nhttps://anonymous.4open.science/r/Expl-CNN-Transformer/.",
      "pdf_url": "http://arxiv.org/pdf/2504.08481v1",
      "published": "2025-04-11T12:15:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08481v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "On the Design of Diffusion-based Neural Speech Codecs",
      "authors": [
        "Pietro Foti",
        "Andreas Brendel"
      ],
      "abstract": "Recently, neural speech codecs (NSCs) trained as generative models have shown\nsuperior performance compared to conventional codecs at low bitrates. Although\nmost state-of-the-art NSCs are trained as Generative Adversarial Networks\n(GANs), Diffusion Models (DMs), a recent class of generative models, represent\na promising alternative due to their superior performance in image generation\nrelative to GANs. Consequently, DMs have been successfully applied for audio\nand speech coding among various other audio generation applications. However,\nthe design of diffusion-based NSCs has not yet been explored in a systematic\nway. We address this by providing a comprehensive analysis of diffusion-based\nNSCs divided into three contributions. First, we propose a categorization based\non the conditioning and output domains of the DM. This simple conceptual\nframework allows us to define a design space for diffusion-based NSCs and to\nassign a category to existing approaches in the literature. Second, we\nsystematically investigate unexplored designs by creating and evaluating new\ndiffusion-based NSCs within the conceptual framework. Finally, we compare the\nproposed models to existing GAN and DM baselines through objective metrics and\nsubjective listening tests.",
      "pdf_url": "http://arxiv.org/pdf/2504.08470v1",
      "published": "2025-04-11T11:58:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08470v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ]
    },
    {
      "title": "Generalization Bounds in Hybrid Quantum-Classical Machine Learning Models",
      "authors": [
        "Tongyan Wu",
        "Amine Bentellis",
        "Alona Sakhnenko",
        "Jeanette Miriam Lorenz"
      ],
      "abstract": "Hybrid classical-quantum models aim to harness the strengths of both quantum\ncomputing and classical machine learning, but their practical potential remains\npoorly understood. In this work, we develop a unified mathematical framework\nfor analyzing generalization in hybrid models, offering insight into how these\nsystems learn from data. We establish a novel generalization bound of the form\n$O\\big( \\sqrt{\\frac{T\\log{T}}{N}} + \\frac{\\alpha}{\\sqrt{N}}\\big)$ for $N$\ntraining data points, $T$ trainable quantum gates, and bounded fully-connected\nlayers $||F|| \\leq \\alpha$. This bound decomposes cleanly into quantum and\nclassical contributions, extending prior work on both components and clarifying\ntheir interaction. We apply our results to the quantum-classical convolutional\nneural network (QCCNN), an architecture that integrates quantum convolutional\nlayers with classical processing. Alongside the bound, we highlight conceptual\nlimitations of applying classical statistical learning theory in the hybrid\nsetting and suggest promising directions for future theoretical work.",
      "pdf_url": "http://arxiv.org/pdf/2504.08456v1",
      "published": "2025-04-11T11:35:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08456v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness",
      "authors": [
        "Yilin Ning",
        "Yian Ma",
        "Mingxuan Liu",
        "Xin Li",
        "Nan Liu"
      ],
      "abstract": "Fairness in artificial intelligence (AI) prediction models is increasingly\nemphasized to support responsible adoption in high-stakes domains such as\nhealth care and criminal justice. Guidelines and implementation frameworks\nhighlight the importance of both predictive accuracy and equitable outcomes.\nHowever, current fairness toolkits often evaluate classification performance\ndisparities in isolation, with limited attention to other critical aspects such\nas calibration. To address these gaps, we present seeBias, an R package for\ncomprehensive evaluation of model fairness and predictive performance. seeBias\noffers an integrated evaluation across classification, calibration, and other\nperformance domains, providing a more complete view of model behavior. It\nincludes customizable visualizations to support transparent reporting and\nresponsible AI implementation. Using public datasets from criminal justice and\nhealthcare, we demonstrate how seeBias supports fairness evaluations, and\nuncovers disparities that conventional fairness metrics may overlook. The R\npackage is available on GitHub, and a Python version is under development.",
      "pdf_url": "http://arxiv.org/pdf/2504.08418v1",
      "published": "2025-04-11T10:23:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08418v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability",
      "authors": [
        "Paul J. Pritz",
        "Kin K. Leung"
      ],
      "abstract": "Reinforcement learning in partially observable environments is typically\nchallenging, as it requires agents to learn an estimate of the underlying\nsystem state. These challenges are exacerbated in multi-agent settings, where\nagents learn simultaneously and influence the underlying state as well as each\nothers' observations. We propose the use of learned beliefs on the underlying\nstate of the system to overcome these challenges and enable reinforcement\nlearning with fully decentralized training and execution. Our approach\nleverages state information to pre-train a probabilistic belief model in a\nself-supervised fashion. The resulting belief states, which capture both\ninferred state information as well as uncertainty over this information, are\nthen used in a state-based reinforcement learning algorithm to create an\nend-to-end model for cooperative multi-agent reinforcement learning under\npartial observability. By separating the belief and reinforcement learning\ntasks, we are able to significantly simplify the policy and value function\nlearning tasks and improve both the convergence speed and the final\nperformance. We evaluate our proposed method on diverse partially observable\nmulti-agent tasks designed to exhibit different variants of partial\nobservability.",
      "pdf_url": "http://arxiv.org/pdf/2504.08417v1",
      "published": "2025-04-11T10:21:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08417v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Constrained Machine Learning Through Hyperspherical Representation",
      "authors": [
        "Gaetano Signorelli",
        "Michele Lombardi"
      ],
      "abstract": "The problem of ensuring constraints satisfaction on the output of machine\nlearning models is critical for many applications, especially in\nsafety-critical domains. Modern approaches rely on penalty-based methods at\ntraining time, which do not guarantee to avoid constraints violations; or\nconstraint-specific model architectures (e.g., for monotonocity); or on output\nprojection, which requires to solve an optimization problem that might be\ncomputationally demanding. We present the Hypersherical Constrained\nRepresentation, a novel method to enforce constraints in the output space for\nconvex and bounded feasibility regions (generalizable to star domains). Our\nmethod operates on a different representation system, where Euclidean\ncoordinates are converted into hyperspherical coordinates relative to the\nconstrained region, which can only inherently represent feasible points.\nExperiments on a synthetic and a real-world dataset show that our method has\npredictive performance comparable to the other approaches, can guarantee 100%\nconstraint satisfaction, and has a minimal computational cost at inference\ntime.",
      "pdf_url": "http://arxiv.org/pdf/2504.08415v1",
      "published": "2025-04-11T10:19:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08415v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation",
      "authors": [
        "Dawei Zhou",
        "Suzhi Gang",
        "Decheng Liu",
        "Tongliang Liu",
        "Nannan Wang",
        "Xinbo Gao"
      ],
      "abstract": "Malicious applications of visual manipulation have raised serious threats to\nthe security and reputation of users in many fields. To alleviate these issues,\nadversarial noise-based defenses have been enthusiastically studied in recent\nyears. However, ``data-only\" methods tend to distort fake samples in the\nlow-level feature space rather than the high-level semantic space, leading to\nlimitations in resisting malicious manipulation. Frontier research has shown\nthat integrating knowledge in deep learning can produce reliable and\ngeneralizable solutions. Inspired by these, we propose a knowledge-guided\nadversarial defense (KGAD) to actively force malicious manipulation models to\noutput semantically confusing samples. Specifically, in the process of\ngenerating adversarial noise, we focus on constructing significant semantic\nconfusions at the domain-specific knowledge level, and exploit a metric closely\nrelated to visual perception to replace the general pixel-wise metrics. The\ngenerated adversarial noise can actively interfere with the malicious\nmanipulation model by triggering knowledge-guided and perception-related\ndisruptions in the fake samples. To validate the effectiveness of the proposed\nmethod, we conduct qualitative and quantitative experiments on human perception\nand visual quality assessment. The results on two different tasks both show\nthat our defense provides better protection compared to state-of-the-art\nmethods and achieves great generalizability.",
      "pdf_url": "http://arxiv.org/pdf/2504.08411v1",
      "published": "2025-04-11T10:18:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08411v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models",
      "authors": [
        "Yin Jou Huang",
        "Rafik Hadfi"
      ],
      "abstract": "There is a growing interest in assessing the personality traits of Large\nlanguage models (LLMs). However, traditional personality assessments based on\nself-report questionnaires may fail to capture their true behavioral nuances\ndue to inherent biases and meta-knowledge contamination. This paper introduces\na novel multi-observer framework for LLM personality assessment that draws\ninspiration from informant-report methods in psychology. Instead of relying\nsolely on self-assessments, our approach employs multiple observer agents\nconfigured with a specific relationship context (e.g., family, friend, or\nworkplace) to simulate interactive scenarios with a subject LLM. These\nobservers engage in dialogues and subsequently provide ratings across the Big\nFive personality dimensions. Our experiments reveal that LLMs possess\nsystematic biases in self-report personality ratings. Moreover, aggregating\nobserver ratings effectively reduces non-systematic biases and achieves optimal\nreliability with 5-7 observers. The findings highlight the significant impact\nof relationship context on personality perception and demonstrate that a\nmulti-observer paradigm yields a more robust and context-sensitive evaluation\nof LLM personality traits.",
      "pdf_url": "http://arxiv.org/pdf/2504.08399v1",
      "published": "2025-04-11T10:03:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08399v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Human strategies for correcting `human-robot' errors during a laundry sorting task",
      "authors": [
        "Pepita Barnard",
        "Maria J Galvez Trigo",
        "Dominic Price",
        "Sue Cobb",
        "Gisela Reyes-Cruz",
        "Gustavo Berumen",
        "David Branson III",
        "Mojtaba A. Khanesar",
        "Mercedes Torres Torres",
        "Michel Valstar"
      ],
      "abstract": "Mental models and expectations underlying human-human interaction (HHI)\ninform human-robot interaction (HRI) with domestic robots. To ease\ncollaborative home tasks by improving domestic robot speech and behaviours for\nhuman-robot communication, we designed a study to understand how people\ncommunicated when failure occurs. To identify patterns of natural\ncommunication, particularly in response to robotic failures, participants\ninstructed Laundrobot to move laundry into baskets using natural language and\ngestures. Laundrobot either worked error-free, or in one of two error modes.\nParticipants were not advised Laundrobot would be a human actor, nor given\ninformation about error modes. Video analysis from 42 participants found speech\npatterns, included laughter, verbal expressions, and filler words, such as\n``oh'' and ``ok'', also, sequences of body movements, including touching one's\nown face, increased pointing with a static finger, and expressions of surprise.\nCommon strategies deployed when errors occurred, included correcting and\nteaching, taking responsibility, and displays of frustration. The strength of\nreaction to errors diminished with exposure, possibly indicating acceptance or\nresignation. Some used strategies similar to those used to communicate with\nother technologies, such as smart assistants. An anthropomorphic robot may not\nbe ideally suited to this kind of task. Laundrobot's appearance, morphology,\nvoice, capabilities, and recovery strategies may have impacted how it was\nperceived. Some participants indicated Laundrobot's actual skills were not\naligned with expectations; this made it difficult to know what to expect and\nhow much Laundrobot understood. Expertise, personality, and cultural\ndifferences may affect responses, however these were not assessed.",
      "pdf_url": "http://arxiv.org/pdf/2504.08395v1",
      "published": "2025-04-11T09:53:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08395v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft",
      "authors": [
        "Junliang Guo",
        "Yang Ye",
        "Tianyu He",
        "Haoyu Wu",
        "Yushu Jiang",
        "Tim Pearce",
        "Jiang Bian"
      ],
      "abstract": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate $4$ to $7$ frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
      "pdf_url": "http://arxiv.org/pdf/2504.08388v1",
      "published": "2025-04-11T09:41:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08388v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented Generation",
      "authors": [
        "Arman Khaledian",
        "Amirreza Ghadiridehkordi",
        "Nariman Khaledian"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\ngrounding large language models in external knowledge sources, improving the\nprecision of agents responses. However, high-dimensional language model\nembeddings, often in the range of hundreds to thousands of dimensions, can\npresent scalability challenges in terms of storage and latency, especially when\nprocessing massive financial text corpora. This paper investigates the use of\nPrincipal Component Analysis (PCA) to reduce embedding dimensionality, thereby\nmitigating computational bottlenecks without incurring large accuracy losses.\nWe experiment with a real-world dataset and compare different similarity and\ndistance metrics under both full-dimensional and PCA-compressed embeddings. Our\nresults show that reducing vectors from 3,072 to 110 dimensions provides a\nsizeable (up to $60\\times$) speedup in retrieval operations and a $\\sim\n28.6\\times$ reduction in index size, with only moderate declines in correlation\nmetrics relative to human-annotated similarity scores. These findings\ndemonstrate that PCA-based compression offers a viable balance between\nretrieval fidelity and resource efficiency, essential for real-time systems\nsuch as Zanista AI's \\textit{Newswitch} platform. Ultimately, our study\nunderscores the practicality of leveraging classical dimensionality reduction\ntechniques to scale RAG architectures for knowledge-intensive applications in\nfinance and trading, where speed, memory efficiency, and accuracy must jointly\nbe optimized.",
      "pdf_url": "http://arxiv.org/pdf/2504.08386v1",
      "published": "2025-04-11T09:38:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08386v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "stat.ML"
      ]
    },
    {
      "title": "Scholar Inbox: Personalized Paper Recommendations for Scientists",
      "authors": [
        "Markus Flicke",
        "Glenn Angrabeit",
        "Madhav Iyengar",
        "Vitalii Protsenko",
        "Illia Shakun",
        "Jovan Cicvaric",
        "Bora Kargi",
        "Haoyu He",
        "Lukas Schuler",
        "Lewin Scholz",
        "Kavyanjali Agnihotri",
        "Yong Cao",
        "Andreas Geiger"
      ],
      "abstract": "Scholar Inbox is a new open-access platform designed to address the\nchallenges researchers face in staying current with the rapidly expanding\nvolume of scientific literature. We provide personalized recommendations,\ncontinuous updates from open-access archives (arXiv, bioRxiv, etc.), visual\npaper summaries, semantic search, and a range of tools to streamline research\nworkflows and promote open research access. The platform's personalized\nrecommendation system is trained on user ratings, ensuring that recommendations\nare tailored to individual researchers' interests. To further enhance the user\nexperience, Scholar Inbox also offers a map of science that provides an\noverview of research across domains, enabling users to easily explore specific\ntopics. We use this map to address the cold start problem common in recommender\nsystems, as well as an active learning strategy that iteratively prompts users\nto rate a selection of papers, allowing the system to learn user preferences\nquickly. We evaluate the quality of our recommendation system on a novel\ndataset of 800k user ratings, which we make publicly available, as well as via\nan extensive user study. https://www.scholar-inbox.com/",
      "pdf_url": "http://arxiv.org/pdf/2504.08385v1",
      "published": "2025-04-11T09:37:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08385v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Passive Underwater Acoustic Signal Separation based on Feature Decoupling Dual-path Network",
      "authors": [
        "Yucheng Liu",
        "Longyu Jiang"
      ],
      "abstract": "Signal separation in the passive underwater acoustic domain has heavily\nrelied on deep learning techniques to isolate ship radiated noise. However, the\nseparation networks commonly used in this domain stem from speech separation\napplications and may not fully consider the unique aspects of underwater\nacoustics beforehand, such as the influence of different propagation media,\nsignal frequencies and modulation characteristics. This oversight highlights\nthe need for tailored approaches that account for the specific characteristics\nof underwater sound propagation. This study introduces a novel temporal network\ndesigned to separate ship radiated noise by employing a dual-path model and a\nfeature decoupling approach. The mixed signals' features are transformed into a\nspace where they exhibit greater independence, with each dimension's\nsignificance decoupled. Subsequently, a fusion of local and global attention\nmechanisms is employed in the separation layer. Extensive comparisons showcase\nthe effectiveness of this method when compared to other prevalent network\nmodels, as evidenced by its performance in the ShipsEar and DeepShip datasets.",
      "pdf_url": "http://arxiv.org/pdf/2504.08371v1",
      "published": "2025-04-11T09:16:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08371v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "68T10",
        "I.5.4; I.2.6; J.2"
      ]
    },
    {
      "title": "Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset",
      "authors": [
        "Hoang-Loc La",
        "Phuong Hoai Ha"
      ],
      "abstract": "Many studies estimate energy consumption using proxy metrics like memory\nusage, FLOPs, and inference latency, with the assumption that reducing these\nmetrics will also lower energy consumption in neural networks. This paper,\nhowever, takes a different approach by introducing an energy-efficient Neural\nArchitecture Search (NAS) method that directly focuses on identifying\narchitectures that minimize energy consumption while maintaining acceptable\naccuracy. Unlike previous methods that primarily target vision and language\ntasks, the approach proposed here specifically addresses tabular datasets.\nRemarkably, the optimal architecture suggested by this method can reduce energy\nconsumption by up to 92% compared to architectures recommended by conventional\nNAS.",
      "pdf_url": "http://arxiv.org/pdf/2504.08359v1",
      "published": "2025-04-11T08:48:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.08359v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}