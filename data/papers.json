{
  "last_updated": "2025-03-28T00:47:02.269252",
  "papers": [
    {
      "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark",
      "authors": [
        "Sondos Mahmoud Bsharat",
        "Mukul Ranjan",
        "Aidar Myrzakhan",
        "Jiacheng Liu",
        "Bowei Guo",
        "Shengkun Tang",
        "Zhuang Liu",
        "Yuanzhi Li",
        "Zhiqiang Shen"
      ],
      "abstract": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.",
      "pdf_url": "http://arxiv.org/pdf/2503.20786v1",
      "published": "2025-03-26T17:59:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20786v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
      "authors": [
        "Zichen Liu",
        "Changyu Chen",
        "Wenjun Li",
        "Penghui Qi",
        "Tianyu Pang",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
      "pdf_url": "http://arxiv.org/pdf/2503.20783v1",
      "published": "2025-03-26T17:59:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20783v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems",
      "authors": [
        "Chenxi Wang",
        "Jizhan Fang",
        "Xiang Chen",
        "Bozhong Tian",
        "Ziwen Xu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.",
      "pdf_url": "http://arxiv.org/pdf/2503.20756v1",
      "published": "2025-03-26T17:45:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20756v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning",
      "authors": [
        "Huajie Tan",
        "Yuheng Ji",
        "Xiaoshuai Hao",
        "Minglan Lin",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Shanghang Zhang"
      ],
      "abstract": "Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation. Experimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.\nProject website: https://tanhuajie.github.io/ReasonRFT",
      "pdf_url": "http://arxiv.org/pdf/2503.20752v2",
      "published": "2025-03-26T17:38:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20752v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Optimal Scaling Laws for Efficiency Gains in a Theoretical Transformer-Augmented Sectional MoE Framework",
      "authors": [
        "Soham Sane"
      ],
      "abstract": "This paper introduces a theoretical framework for a Transformer-augmented,\nsectional Mixture-of-Experts (MoE) architecture that aims to enhance\ncomputational efficiency while preserving model scalability. Unlike\nconventional MoE models, which route entire token embeddings to selected\nexperts, our approach portions the embedding dimension itself -- assigning\nsegments of each token's representation to dedicated experts. To combat losses\nin token representation, we utilize a pre-expert transformer layer to recompute\nattention across tokens and reduce the sequence length dimensionality. We\nextend our theory by deriving optimal scaling laws that a non-linear\nrelationship between the number of experts and factors such as model\ndimensionality, sequence length, and system overhead. These formulations yield\nclosed-form and numerically-solvable expressions for identifying the optimal\nexpert count under given architectural and hardware constraints. As a result,\nour framework not only provides theoretical bounds for computing efficiency\nwith varying frameworks but also guides practical design choices for scaling\nlarge models effectively. While empirical validation is pending, we present a\ncomprehensive experimental road map to evaluate the framework's efficiency,\nscalability, and practicality in future work.",
      "pdf_url": "http://arxiv.org/pdf/2503.20750v1",
      "published": "2025-03-26T17:33:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20750v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "High Quality Diffusion Distillation on a Single GPU with Relative and Absolute Position Matching",
      "authors": [
        "Guoqiang Zhang",
        "Kenta Niwa",
        "J. P. Lewis",
        "Cedric Mesnage",
        "W. Bastiaan Kleijn"
      ],
      "abstract": "We introduce relative and absolute position matching (RAPM), a diffusion\ndistillation method resulting in high quality generation that can be trained\nefficiently on a single GPU. Recent diffusion distillation research has\nachieved excellent results for high-resolution text-to-image generation with\nmethods such as phased consistency models (PCM) and improved distribution\nmatching distillation (DMD2). However, these methods generally require many\nGPUs (e.g.~8-64) and significant batchsizes (e.g.~128-2048) during training,\nresulting in memory and compute requirements that are beyond the resources of\nsome researchers. RAPM provides effective single-GPU diffusion distillation\ntraining with a batchsize of 1. The new method attempts to mimic the sampling\ntrajectories of the teacher model by matching the relative and absolute\npositions. The design of relative positions is inspired by PCM. Two\ndiscriminators are introduced accordingly in RAPM, one for matching relative\npositions and the other for absolute positions. Experimental results on\nStableDiffusion (SD) V1.5 and SDXL indicate that RAPM with 4 timesteps produces\ncomparable FID scores as the best method with 1 timestep under very limited\ncomputational resources.",
      "pdf_url": "http://arxiv.org/pdf/2503.20744v1",
      "published": "2025-03-26T17:29:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20744v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Quantum Neural Network Restatement of Markov Jump Process",
      "authors": [
        "Z. Zarezadeh",
        "N. Zarezadeh"
      ],
      "abstract": "Despite the many challenges in exploratory data analysis, artificial neural\nnetworks have motivated strong interests in scientists and researchers both in\ntheoretical as well as practical applications. Among sources of such popularity\nof artificial neural networks the ability of modeling non-linear dynamical\nsystems, generalization, and adaptation possibilities should be mentioned.\nDespite this, there is still significant debate about the role of various\nunderlying stochastic processes in stabilizing a unique structure for data\nlearning and prediction. One of such obstacles to the theoretical and numerical\nstudy of machine intelligent systems is the curse of dimensionality and the\nsampling from high-dimensional probability distributions. In general, this\ncurse prevents efficient description of states, providing a significant\ncomplexity barrier for the system to be efficiently described and studied. In\nthis strand of research, direct treatment and description of such abstract\nnotions of learning theory in terms of quantum information be one of the most\nfavorable candidates. Hence, the subject matter of these articles is devoted to\nproblems of design, adaptation and the formulations of computationally hard\nproblems in terms of quantum mechanical systems. In order to characterize the\nmicroscopic description of such dynamics in the language of inferential\nstatistics, covariance matrix estimation of d-dimensional Gaussian densities\nand Bayesian interpretation of eigenvalue problem for dynamical systems is\nassessed.",
      "pdf_url": "http://arxiv.org/pdf/2503.20742v1",
      "published": "2025-03-26T17:25:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20742v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ]
    },
    {
      "title": "Emotion Detection and Music Recommendation System",
      "authors": [
        "Swetha Kambham",
        "Hubert Jhonson",
        "Sai Prathap Reddy Kambham"
      ],
      "abstract": "As artificial intelligence becomes more and more ingrained in daily life, we\npresent a novel system that uses deep learning for music recommendation and\nemotion-based detection. Through the use of facial recognition and the DeepFace\nframework, our method analyses human emotions in real-time and then plays music\nthat reflects the mood it has discovered. The system uses a webcam to take\npictures, analyses the most common facial expression, and then pulls a playlist\nfrom local storage that corresponds to the mood it has detected. An engaging\nand customised experience is ensured by allowing users to manually change the\nsong selection via a dropdown menu or navigation buttons. By continuously\nlooping over the playlist, the technology guarantees continuity. The objective\nof our system is to improve emotional well-being through music therapy by\noffering a responsive and automated music-selection experience.",
      "pdf_url": "http://arxiv.org/pdf/2503.20739v1",
      "published": "2025-03-26T17:22:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20739v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Graph-Enhanced Model-Free Reinforcement Learning Agents for Efficient Power Grid Topological Control",
      "authors": [
        "Eloy Anguiano Batanero",
        "Ángela Fernández",
        "Álvaro Barbero"
      ],
      "abstract": "The increasing complexity of power grid management, driven by the emergence\nof prosumers and the demand for cleaner energy solutions, has needed innovative\napproaches to ensure stability and efficiency. This paper presents a novel\napproach within the model-free framework of reinforcement learning, aimed at\noptimizing power network operations without prior expert knowledge. We\nintroduce a masked topological action space, enabling agents to explore diverse\nstrategies for cost reduction while maintaining reliable service using the\nstate logic as a guide for choosing proper actions. Through extensive\nexperimentation across 20 different scenarios in a simulated 5-substation\nenvironment, we demonstrate that our approach achieves a consistent reduction\nin power losses, while ensuring grid stability against potential blackouts. The\nresults underscore the effectiveness of combining dynamic observation\nformalization with opponent-based training, showing a viable way for autonomous\nmanagement solutions in modern energy systems or even for building a\nfoundational model for this field.",
      "pdf_url": "http://arxiv.org/pdf/2503.20688v1",
      "published": "2025-03-26T16:20:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20688v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound",
      "authors": [
        "Yuhao Huang",
        "Ao Chang",
        "Haoran Dou",
        "Xing Tao",
        "Xinrui Zhou",
        "Yan Cao",
        "Ruobing Huang",
        "Alejandro F Frangi",
        "Lingyun Bao",
        "Xin Yang",
        "Dong Ni"
      ],
      "abstract": "Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D\nautomated breast ultrasound (ABUS) is crucial for clinical diagnosis and\ntreatment planning. Therefore, developing an automated system for nodule\nsegmentation can enhance user independence and expedite clinical analysis.\nUnlike fully-supervised learning, weakly-supervised segmentation (WSS) can\nstreamline the laborious and intricate annotation process. However, current WSS\nmethods face challenges in achieving precise nodule segmentation, as many of\nthem depend on inaccurate activation maps or inefficient pseudo-mask generation\nalgorithms. In this study, we introduce a novel multi-agent reinforcement\nlearning-based WSS framework called Flip Learning, which relies solely on 2D/3D\nboxes for accurate segmentation. Specifically, multiple agents are employed to\nerase the target from the box to facilitate classification tag flipping, with\nthe erased region serving as the predicted segmentation mask. The key\ncontributions of this research are as follows: (1) Adoption of a\nsuperpixel/supervoxel-based approach to encode the standardized environment,\ncapturing boundary priors and expediting the learning process. (2) Introduction\nof three meticulously designed rewards, comprising a classification score\nreward and two intensity distribution rewards, to steer the agents' erasing\nprocess precisely, thereby avoiding both under- and over-segmentation. (3)\nImplementation of a progressive curriculum learning strategy to enable agents\nto interact with the environment in a progressively challenging manner, thereby\nenhancing learning efficiency. Extensively validated on the large in-house BUS\nand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS\nmethods and foundation models, and achieves comparable performance as\nfully-supervised learning algorithms.",
      "pdf_url": "http://arxiv.org/pdf/2503.20685v2",
      "published": "2025-03-26T16:20:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20685v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Inductive Link Prediction on N-ary Relational Facts via Semantic Hypergraph Reasoning",
      "authors": [
        "Gongzhu Yin",
        "Hongli Zhang",
        "Yuchen Yang",
        "Yi Luo"
      ],
      "abstract": "N-ary relational facts represent semantic correlations among more than two\nentities. While recent studies have developed link prediction (LP) methods to\ninfer missing relations for knowledge graphs (KGs) containing n-ary relational\nfacts, they are generally limited to transductive settings. Fully inductive\nsettings, where predictions are made on previously unseen entities, remain a\nsignificant challenge. As existing methods are mainly entity embedding-based,\nthey struggle to capture entity-independent logical rules. To fill in this gap,\nwe propose an n-ary subgraph reasoning framework for fully inductive link\nprediction (ILP) on n-ary relational facts. This framework reasons over local\nsubgraphs and has a strong inductive inference ability to capture n-ary\npatterns. Specifically, we introduce a novel graph structure, the n-ary\nsemantic hypergraph, to facilitate subgraph extraction. Moreover, we develop a\nsubgraph aggregating network, NS-HART, to effectively mine complex semantic\ncorrelations within subgraphs. Theoretically, we provide a thorough analysis\nfrom the score function optimization perspective to shed light on NS-HART's\neffectiveness for n-ary ILP tasks. Empirically, we conduct extensive\nexperiments on a series of inductive benchmarks, including transfer reasoning\n(with and without entity features) and pairwise subgraph reasoning. The results\nhighlight the superiority of the n-ary subgraph reasoning framework and the\nexceptional inductive ability of NS-HART. The source code of this paper has\nbeen made publicly available at\nhttps://github.com/yin-gz/Nary-Inductive-SubGraph.",
      "pdf_url": "http://arxiv.org/pdf/2503.20676v1",
      "published": "2025-03-26T16:09:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20676v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "I.2.4"
      ]
    },
    {
      "title": "Probabilistic Forecasting for Network Resource Analysis in Integrated Terrestrial and Non-Terrestrial Networks",
      "authors": [
        "Cristian J. Vaca-Rubio",
        "Vaishnavi Kasuluru",
        "Engin Zeydan",
        "Luis Blanco",
        "Roberto Pereira",
        "Marius Caus",
        "Kapal Dev"
      ],
      "abstract": "Efficient resource management is critical for Non-Terrestrial Networks (NTNs)\nto provide consistent, high-quality service in remote and under-served regions.\nWhile traditional single-point prediction methods, such as Long-Short Term\nMemory (LSTM), have been used in terrestrial networks, they often fall short in\nNTNs due to the complexity of satellite dynamics, signal latency and coverage\nvariability. Probabilistic forecasting, which quantifies the uncertainties of\nthe predictions, is a robust alternative. In this paper, we evaluate the\napplication of probabilistic forecasting techniques, in particular SFF, to NTN\nresource allocation scenarios. Our results show their effectiveness in\npredicting bandwidth and capacity requirements in different NTN segments of\nprobabilistic forecasting compared to single-point prediction techniques such\nas LSTM. The results show the potential of black probabilistic forecasting\nmodels to provide accurate and reliable predictions and to quantify their\nuncertainty, making them indispensable for optimizing NTN resource allocation.\nAt the end of the paper, we also present application scenarios and a\nstandardization roadmap for the use of probabilistic forecasting in integrated\nTerrestrial Network (TN)-NTN environments.",
      "pdf_url": "http://arxiv.org/pdf/2503.20658v1",
      "published": "2025-03-26T15:54:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20658v1",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ]
    },
    {
      "title": "AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports",
      "authors": [
        "Xiangwen Zhang",
        "Qian Zhang",
        "Longfei Han",
        "Qiang Qu",
        "Xiaoming Chen"
      ],
      "abstract": "Collecting real-world vehicle accident videos for autonomous driving research\nis challenging due to their rarity and complexity. While existing driving video\ngeneration methods may produce visually realistic videos, they often fail to\ndeliver physically realistic simulations because they lack the capability to\ngenerate accurate post-collision trajectories. In this paper, we introduce\nAccidentSim, a novel framework that generates physically realistic vehicle\ncollision videos by extracting and utilizing the physical clues and contextual\ninformation available in real-world vehicle accident reports. Specifically,\nAccidentSim leverages a reliable physical simulator to replicate post-collision\nvehicle trajectories from the physical and contextual information in the\naccident reports and to build a vehicle collision trajectory dataset. This\ndataset is then used to fine-tune a language model, enabling it to respond to\nuser prompts and predict physically consistent post-collision trajectories\nacross various driving scenarios based on user descriptions. Finally, we employ\nNeural Radiance Fields (NeRF) to render high-quality backgrounds, merging them\nwith the foreground vehicles that exhibit physically realistic trajectories to\ngenerate vehicle collision videos. Experimental results demonstrate that the\nvideos produced by AccidentSim excel in both visual and physical authenticity.",
      "pdf_url": "http://arxiv.org/pdf/2503.20654v1",
      "published": "2025-03-26T15:50:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20654v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of Behavioral Therapy Notes",
      "authors": [
        "Raj Sanjay Shah",
        "Lei Xu",
        "Qianchu Liu",
        "Jon Burnsky",
        "Drew Bertagnolli",
        "Chaitanya Shivade"
      ],
      "abstract": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes.",
      "pdf_url": "http://arxiv.org/pdf/2503.20648v1",
      "published": "2025-03-26T15:40:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20648v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Procedural Knowledge Ontology (PKO)",
      "authors": [
        "Valentina Anita Carriero",
        "Mario Scrocca",
        "Ilaria Baroni",
        "Antonia Azzini",
        "Irene Celino"
      ],
      "abstract": "Processes, workflows and guidelines are core to ensure the correct\nfunctioning of industrial companies: for the successful operations of factory\nlines, machinery or services, often industry operators rely on their past\nexperience and know-how. The effect is that this Procedural Knowledge (PK)\nremains tacit and, as such, difficult to exploit efficiently and effectively.\nThis paper presents PKO, the Procedural Knowledge Ontology, which enables the\nexplicit modeling of procedures and their executions, by reusing and extending\nexisting ontologies. PKO is built on requirements collected from three\nheterogeneous industrial use cases and can be exploited by any AI and\ndata-driven tools that rely on a shared and interoperable representation to\nsupport the governance of PK throughout its life cycle. We describe its\nstructure and design methodology, and outline its relevance, quality, and\nimpact by discussing applications leveraging PKO for PK elicitation and\nexploitation.",
      "pdf_url": "http://arxiv.org/pdf/2503.20634v1",
      "published": "2025-03-26T15:28:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20634v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "$β$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation",
      "authors": [
        "Haci Ismail Aslan",
        "Philipp Wiesner",
        "Ping Xiong",
        "Odej Kao"
      ],
      "abstract": "Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance.",
      "pdf_url": "http://arxiv.org/pdf/2503.20630v1",
      "published": "2025-03-26T15:24:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20630v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions",
      "authors": [
        "Alessandro Maisto"
      ],
      "abstract": "Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools.",
      "pdf_url": "http://arxiv.org/pdf/2503.20623v1",
      "published": "2025-03-26T15:10:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20623v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
      "authors": [
        "Zongyuan Zhang",
        "Tianyang Duan",
        "Zheng Lin",
        "Dong Huang",
        "Zihan Fang",
        "Zekai Sun",
        "Ling Xiong",
        "Hongbin Liang",
        "Heming Cui",
        "Yong Cui"
      ],
      "abstract": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2503.20613v1",
      "published": "2025-03-26T15:00:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20613v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "A decision-theoretic approach to dealing with uncertainty in quantum mechanics",
      "authors": [
        "Keano De Vos",
        "Gert de Cooman",
        "Alexander Erreygers",
        "Jasper De Bock"
      ],
      "abstract": "We provide a decision-theoretic framework for dealing with uncertainty in\nquantum mechanics. This uncertainty is two-fold: on the one hand there may be\nuncertainty about the state the quantum system is in, and on the other hand, as\nis essential to quantum mechanical uncertainty, even if the quantum state is\nknown, measurements may still produce an uncertain outcome. In our framework,\nmeasurements therefore play the role of acts with an uncertain outcome and our\nsimple decision-theoretic postulates ensure that Born's rule is encapsulated in\nthe utility functions associated with such acts. This approach allows us to\nuncouple (precise) probability theory from quantum mechanics, in the sense that\nit leaves room for a more general, so-called imprecise probabilities approach.\nWe discuss the mathematical implications of our findings, which allow us to\ngive a decision-theoretic foundation to recent seminal work by Benavoli,\nFacchini and Zaffalon, and we compare our approach to earlier and different\napproaches by Deutsch and Wallace.",
      "pdf_url": "http://arxiv.org/pdf/2503.20607v1",
      "published": "2025-03-26T14:53:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20607v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "math.PR"
      ]
    },
    {
      "title": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs",
      "authors": [
        "Zhicheng Guo",
        "Sijie Cheng",
        "Yuchen Niu",
        "Hao Wang",
        "Sicheng Zhou",
        "Wenbing Huang",
        "Yang Liu"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench.",
      "pdf_url": "http://arxiv.org/pdf/2503.20527v1",
      "published": "2025-03-26T13:13:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20527v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving",
      "authors": [
        "Lloyd Russell",
        "Anthony Hu",
        "Lorenzo Bertoni",
        "George Fedoseev",
        "Jamie Shotton",
        "Elahe Arani",
        "Gianluca Corrado"
      ],
      "abstract": "Generative models offer a scalable and flexible paradigm for simulating\ncomplex environments, yet current approaches fall short in addressing the\ndomain-specific requirements of autonomous driving - such as multi-agent\ninteractions, fine-grained control, and multi-camera consistency. We introduce\nGAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies\nthese capabilities within a single generative framework. GAIA-2 supports\ncontrollable video generation conditioned on a rich set of structured inputs:\nego-vehicle dynamics, agent configurations, environmental factors, and road\nsemantics. It generates high-resolution, spatiotemporally consistent\nmulti-camera videos across geographically diverse driving environments (UK, US,\nGermany). The model integrates both structured conditioning and external latent\nembeddings (e.g., from a proprietary driving model) to facilitate flexible and\nsemantically grounded scene synthesis. Through this integration, GAIA-2 enables\nscalable simulation of both common and rare driving scenarios, advancing the\nuse of generative world models as a core tool in the development of autonomous\nsystems. Videos are available at https://wayve.ai/thinking/gaia-2.",
      "pdf_url": "http://arxiv.org/pdf/2503.20523v1",
      "published": "2025-03-26T13:11:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20523v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Design and Evaluation of Neural Network-Based Receiver Architectures for Reliable Communication",
      "authors": [
        "Hüseyin Çevik",
        "Erhan Karakoca",
        "İbrahim Hökelek",
        "Ali Görçin"
      ],
      "abstract": "Neural network-based receivers leverage deep learning to optimize signal\ndetection and decoding, significantly improving bit-error rate (BER) and\nblock-error rate (BLER) in challenging environments. This study evaluates\nvarious architectures and compares their BER and BLER performance across\ndifferent noise levels. Two novel models, the Dual Attention Transformer (DAT)\nand the Residual Dual Non-Local Attention Network (RDNLA), integrate\nself-attention and residual learning to enhance signal reconstruction. These\nmodels bypass conventional channel estimation and equalization by directly\npredicting log-likelihood ratios (LLRs) from received signals, with noise\nvariance as an additional input. Simulations show that DAT and RDNLA outperform\ntraditional and other neural receiver models under varying signal-to-noise\nratios (SNR), while their computational efficiency supports their feasibility\nfor next-generation communication systems.",
      "pdf_url": "http://arxiv.org/pdf/2503.20500v1",
      "published": "2025-03-26T12:39:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20500v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Efficient and General-Purpose Few-Shot Misclassification Detection for Vision-Language Models",
      "authors": [
        "Fanhu Zeng",
        "Zhen Cheng",
        "Fei Zhu",
        "Xu-Yao Zhang"
      ],
      "abstract": "Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach.",
      "pdf_url": "http://arxiv.org/pdf/2503.20492v1",
      "published": "2025-03-26T12:31:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20492v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Underwater Image Enhancement by Convolutional Spiking Neural Networks",
      "authors": [
        "Vidya Sudevan",
        "Fakhreddine Zayer",
        "Rizwana Kausar",
        "Sajid Javed",
        "Hamad Karki",
        "Giulia De Masi",
        "Jorge Dias"
      ],
      "abstract": "Underwater image enhancement (UIE) is fundamental for marine applications,\nincluding autonomous vision-based navigation. Deep learning methods using\nconvolutional neural networks (CNN) and vision transformers advanced UIE\nperformance. Recently, spiking neural networks (SNN) have gained attention for\ntheir lightweight design, energy efficiency, and scalability. This paper\nintroduces UIE-SNN, the first SNN-based UIE algorithm to improve visibility of\nunderwater images. UIE-SNN is a 19- layered convolutional spiking\nencoder-decoder framework with skip connections, directly trained using\nsurrogate gradient-based backpropagation through time (BPTT) strategy. We\nexplore and validate the influence of training datasets on energy reduction, a\nunique advantage of UIE-SNN architecture, in contrast to the conventional\nlearning-based architectures, where energy consumption is model-dependent.\nUIE-SNN optimizes the loss function in latent space representation to\nreconstruct clear underwater images. Our algorithm performs on par with its\nnon-spiking counterpart methods in terms of PSNR and structural similarity\nindex (SSIM) at reduced timesteps ($T=5$) and energy consumption of $85\\%$. The\nalgorithm is trained on two publicly available benchmark datasets, UIEB and\nEUVP, and tested on unseen images from UIEB, EUVP, LSUI, U45, and our custom\nUIE dataset. The UIE-SNN algorithm achieves PSNR of \\(17.7801~dB\\) and SSIM of\n\\(0.7454\\) on UIEB, and PSNR of \\(23.1725~dB\\) and SSIM of \\(0.7890\\) on EUVP.\nUIE-SNN achieves this algorithmic performance with fewer operators (\\(147.49\\)\nGSOPs) and energy (\\(0.1327~J\\)) compared to its non-spiking counterpart\n(GFLOPs = \\(218.88\\) and Energy=\\(1.0068~J\\)). Compared with existing SOTA UIE\nmethods, UIE-SNN achieves an average of \\(6.5\\times\\) improvement in energy\nefficiency. The source code is available at\n\\href{https://github.com/vidya-rejul/UIE-SNN.git}{UIE-SNN}.",
      "pdf_url": "http://arxiv.org/pdf/2503.20485v1",
      "published": "2025-03-26T12:15:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20485v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.PF"
      ]
    },
    {
      "title": "Contrastive Learning Guided Latent Diffusion Model for Image-to-Image Translation",
      "authors": [
        "Qi Si",
        "Bo Wang",
        "Zhao Zhang"
      ],
      "abstract": "The diffusion model has demonstrated superior performance in synthesizing\ndiverse and high-quality images for text-guided image translation. However,\nthere remains room for improvement in both the formulation of text prompts and\nthe preservation of reference image content. First, variations in target text\nprompts can significantly influence the quality of the generated images, and it\nis often challenging for users to craft an optimal prompt that fully captures\nthe content of the input image. Second, while existing models can introduce\ndesired modifications to specific regions of the reference image, they\nfrequently induce unintended alterations in areas that should remain unchanged.\nTo address these challenges, we propose pix2pix-zeroCon, a zero-shot\ndiffusion-based method that eliminates the need for additional training by\nleveraging patch-wise contrastive loss. Specifically, we automatically\ndetermine the editing direction in the text embedding space based on the\nreference image and target prompts. Furthermore, to ensure precise content and\nstructural preservation in the edited image, we introduce cross-attention\nguiding loss and patch-wise contrastive loss between the generated and original\nimage embeddings within a pre-trained diffusion model. Notably, our approach\nrequires no additional training and operates directly on a pre-trained\ntext-to-image diffusion model. Extensive experiments demonstrate that our\nmethod surpasses existing models in image-to-image translation, achieving\nenhanced fidelity and controllability.",
      "pdf_url": "http://arxiv.org/pdf/2503.20484v1",
      "published": "2025-03-26T12:15:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20484v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A multi-agentic framework for real-time, autonomous freeform metasurface design",
      "authors": [
        "Robert Lupoiu",
        "Yixuan Shao",
        "Tianxiang Dai",
        "Chenkai Mao",
        "Kofi Edee",
        "Jonathan A. Fan"
      ],
      "abstract": "Innovation in nanophotonics currently relies on human experts who synergize\nspecialized knowledge in photonics and coding with simulation and optimization\nalgorithms, entailing design cycles that are time-consuming, computationally\ndemanding, and frequently suboptimal. We introduce MetaChat, a multi-agentic\ndesign framework that can translate semantically described photonic design\ngoals into high-performance, freeform device layouts in an automated, nearly\nreal-time manner. Multi-step reasoning is enabled by our Agentic Iterative\nMonologue (AIM) paradigm, which coherently interfaces agents with code-based\ntools, other specialized agents, and human designers. Design acceleration is\nfacilitated by Feature-wise Linear Modulation-conditioned Maxwell surrogate\nsolvers that support the generalized evaluation of metasurface structures. We\nuse freeform dielectric metasurfaces as a model system and demonstrate with\nMetaChat the design of multi-objective, multi-wavelength metasurfaces orders of\nmagnitude faster than conventional methods. These concepts present a scientific\ncomputing blueprint for utilizing specialist design agents, surrogate solvers,\nand human interactions to drive multi-physics innovation and discovery.",
      "pdf_url": "http://arxiv.org/pdf/2503.20479v1",
      "published": "2025-03-26T12:10:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20479v1",
      "categories": [
        "physics.app-ph",
        "cs.AI",
        "cs.MA",
        "physics.comp-ph"
      ]
    },
    {
      "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment",
      "authors": [
        "Yucheng Suo",
        "Fan Ma",
        "Linchao Zhu",
        "Tianyi Wang",
        "Fengyun Rao",
        "Yi Yang"
      ],
      "abstract": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2503.20472v1",
      "published": "2025-03-26T11:53:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20472v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Attention Xception UNet (AXUNet): A Novel Combination of CNN and Self-Attention for Brain Tumor Segmentation",
      "authors": [
        "Farzan Moodi",
        "Fereshteh Khodadadi Shoushtari",
        "Gelareh Valizadeh",
        "Dornaz Mazinani",
        "Hanieh Mobarak Salari",
        "Hamidreza Saligheh Rad"
      ],
      "abstract": "Accurate segmentation of glioma brain tumors is crucial for diagnosis and\ntreatment planning. Deep learning techniques offer promising solutions, but\noptimal model architectures remain under investigation. We used the BraTS 2021\ndataset, selecting T1 with contrast enhancement (T1CE), T2, and\nFluid-Attenuated Inversion Recovery (FLAIR) sequences for model development.\nThe proposed Attention Xception UNet (AXUNet) architecture integrates an\nXception backbone with dot-product self-attention modules, inspired by\nstate-of-the-art (SOTA) large language models such as Google Bard and OpenAI\nChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models.\nComparative evaluation on the test set demonstrated improved results over\nbaseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of\n90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean\nDice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET)\namong all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of\n90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It\ndemonstrated superior Dice scores across whole tumor (WT) and tumor core (TC)\nregions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The\nintegration of the Xception backbone and dot-product self-attention mechanisms\nin AXUNet showcases enhanced performance in capturing spatial and contextual\ninformation. The findings underscore the potential utility of AXUNet in\nfacilitating precise tumor delineation.",
      "pdf_url": "http://arxiv.org/pdf/2503.20446v1",
      "published": "2025-03-26T11:22:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20446v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Evaluating Facial Expression Recognition Datasets for Deep Learning: A Benchmark Study with Novel Similarity Metrics",
      "authors": [
        "F. Xavier Gaya-Morey",
        "Cristina Manresa-Yee",
        "Célia Martinie",
        "Jose M. Buades-Rubio"
      ],
      "abstract": "This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems.",
      "pdf_url": "http://arxiv.org/pdf/2503.20428v1",
      "published": "2025-03-26T11:01:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20428v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation",
      "authors": [
        "Kevin Alcedo",
        "Pedro U. Lima",
        "Rachid Alami"
      ],
      "abstract": "Navigating in environments alongside humans requires agents to reason under\nuncertainty and account for the beliefs and intentions of those around them.\nUnder a sequential decision-making framework, egocentric navigation can\nnaturally be represented as a Markov Decision Process (MDP). However, social\nnavigation additionally requires reasoning about the hidden beliefs of others,\ninherently leading to a Partially Observable Markov Decision Process (POMDP),\nwhere agents lack direct access to others' mental states. Inspired by Theory of\nMind and Epistemic Planning, we propose (1) a neuro-symbolic model-based\nreinforcement learning architecture for social navigation, addressing the\nchallenge of belief tracking in partially observable environments; and (2) a\nperspective-shift operator for belief estimation, leveraging recent work on\nInfluence-based Abstractions (IBA) in structured multi-agent settings.",
      "pdf_url": "http://arxiv.org/pdf/2503.20425v1",
      "published": "2025-03-26T10:59:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20425v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Including local feature interactions in deep non-negative matrix factorization networks improves performance",
      "authors": [
        "Mahbod Nouri",
        "David Rotermund",
        "Alberto Garcia-Ortiz",
        "Klaus R. Pawelzik"
      ],
      "abstract": "The brain uses positive signals as a means of signaling. Forward interactions\nin the early visual cortex are also positive, realized by excitatory synapses.\nOnly local interactions also include inhibition. Non-negative matrix\nfactorization (NMF) captures the biological constraint of positive long-range\ninteractions and can be implemented with stochastic spikes. While NMF can serve\nas an abstract formalization of early neural processing in the visual system,\nthe performance of deep convolutional networks with NMF modules does not match\nthat of CNNs of similar size. However, when the local NMF modules are each\nfollowed by a module that mixes the NMF's positive activities, the performances\non the benchmark data exceed that of vanilla deep convolutional networks of\nsimilar size. This setting can be considered a biologically more plausible\nemulation of the processing in cortical (hyper-)columns with the potential to\nimprove the performance of deep networks.",
      "pdf_url": "http://arxiv.org/pdf/2503.20398v1",
      "published": "2025-03-26T10:21:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20398v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "FastFT: Accelerating Reinforced Feature Transformation via Advanced Exploration Strategies",
      "authors": [
        "Tianqi He",
        "Xiaohan Huang",
        "Yi Du",
        "Qingqing Long",
        "Ziyue Qiao",
        "Min Wu",
        "Yanjie Fu",
        "Yuanchun Zhou",
        "Meng Xiao"
      ],
      "abstract": "Feature Transformation is crucial for classic machine learning that aims to\ngenerate feature combinations to enhance the performance of downstream tasks\nfrom a data-centric perspective. Current methodologies, such as manual\nexpert-driven processes, iterative-feedback techniques, and\nexploration-generative tactics, have shown promise in automating such data\nengineering workflow by minimizing human involvement. However, three challenges\nremain in those frameworks: (1) It predominantly depends on downstream task\nperformance metrics, as assessment is time-consuming, especially for large\ndatasets. (2) The diversity of feature combinations will hardly be guaranteed\nafter random exploration ends. (3) Rare significant transformations lead to\nsparse valuable feedback that hinders the learning processes or leads to less\neffective results. In response to these challenges, we introduce FastFT, an\ninnovative framework that leverages a trio of advanced strategies.We first\ndecouple the feature transformation evaluation from the outcomes of the\ngenerated datasets via the performance predictor. To address the issue of\nreward sparsity, we developed a method to evaluate the novelty of generated\ntransformation sequences. Incorporating this novelty into the reward function\naccelerates the model's exploration of effective transformations, thereby\nimproving the search productivity. Additionally, we combine novelty and\nperformance to create a prioritized memory buffer, ensuring that essential\nexperiences are effectively revisited during exploration. Our extensive\nexperimental evaluations validate the performance, efficiency, and traceability\nof our proposed framework, showcasing its superiority in handling complex\nfeature transformation tasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.20394v1",
      "published": "2025-03-26T10:17:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20394v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation",
      "authors": [
        "Rongyu Zhang",
        "Menghang Dong",
        "Yuan Zhang",
        "Liang Heng",
        "Xiaowei Chi",
        "Gaole Dai",
        "Li Du",
        "Dan Wang",
        "Yuan Du",
        "Shanghang Zhang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2503.20384v1",
      "published": "2025-03-26T10:05:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20384v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "VideoGEM: Training-free Action Grounding in Videos",
      "authors": [
        "Felix Vogel",
        "Walid Bousselham",
        "Anna Kukleva",
        "Nina Shvetsova",
        "Hilde Kuehne"
      ],
      "abstract": "Vision-language foundation models have shown impressive capabilities across\nvarious zero-shot tasks, including training-free localization and grounding,\nprimarily focusing on localizing objects in images. However, leveraging those\ncapabilities to localize actions and events in videos is challenging, as\nactions have less physical outline and are usually described by higher-level\nconcepts. In this work, we propose VideoGEM, the first training-free spatial\naction grounding method based on pretrained image- and video-language\nbackbones. Namely, we adapt the self-self attention formulation of GEM to\nspatial activity grounding. We observe that high-level semantic concepts, such\nas actions, usually emerge in the higher layers of the image- and\nvideo-language models. We, therefore, propose a layer weighting in the\nself-attention path to prioritize higher layers. Additionally, we introduce a\ndynamic weighting method to automatically tune layer weights to capture each\nlayer`s relevance to a specific prompt. Finally, we introduce a prompt\ndecomposition, processing action, verb, and object prompts separately,\nresulting in a better spatial localization of actions. We evaluate the proposed\napproach on three image- and video-language backbones, CLIP, OpenCLIP, and\nViCLIP, and on four video grounding datasets, V-HICO, DALY,\nYouCook-Interactions, and GroundingYouTube, showing that the proposed\ntraining-free approach is able to outperform current trained state-of-the-art\napproaches for spatial video grounding.",
      "pdf_url": "http://arxiv.org/pdf/2503.20348v1",
      "published": "2025-03-26T09:20:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20348v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context",
      "authors": [
        "Francesco Micheli",
        "Efe C. Balta",
        "Anastasios Tsiamis",
        "John Lygeros"
      ],
      "abstract": "We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method.",
      "pdf_url": "http://arxiv.org/pdf/2503.20341v1",
      "published": "2025-03-26T09:11:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20341v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models",
      "authors": [
        "Shih-Wen Ke",
        "Guan-Yu Lai",
        "Guo-Lin Fang",
        "Hsi-Yuan Kao"
      ],
      "abstract": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt.",
      "pdf_url": "http://arxiv.org/pdf/2503.20320v1",
      "published": "2025-03-26T08:40:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20320v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications",
      "authors": [
        "Sunayana Sitaram",
        "Adrian de Wynter",
        "Isobel McCrum",
        "Qilong Gu",
        "Si-Qing Chen"
      ],
      "abstract": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard large language\nmodel-based application (meeting transcript summarization), where both the data\ngeneration and the annotation steps followed a human-in-the-loop approach. We\nfind that the proposed guardrails are very effective in reducing misgendering\nrates across all languages in the summaries generated, and without incurring\nloss of quality. Our human-in-the-loop approach demonstrates a method to\nfeasibly scale inclusive and responsible AI-based solutions across multiple\nlanguages and cultures.",
      "pdf_url": "http://arxiv.org/pdf/2503.20302v1",
      "published": "2025-03-26T08:01:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20302v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Context-Aware Weakly Supervised Image Manipulation Localization with SAM Refinement",
      "authors": [
        "Xinghao Wang",
        "Changtao Miao",
        "Dianmo Sheng",
        "Tao Gong",
        "Qi Chu",
        "Bin Liu",
        "Nenghai Yu"
      ],
      "abstract": "Malicious image manipulation poses societal risks, increasing the importance\nof effective image manipulation detection methods. Recent approaches in image\nmanipulation detection have largely been driven by fully supervised approaches,\nwhich require labor-intensive pixel-level annotations. Thus, it is essential to\nexplore weakly supervised image manipulation localization methods that only\nrequire image-level binary labels for training. However, existing weakly\nsupervised image manipulation methods overlook the importance of edge\ninformation for accurate localization, leading to suboptimal localization\nperformance. To address this, we propose a Context-Aware Boundary Localization\n(CABL) module to aggregate boundary features and learn context-inconsistency\nfor localizing manipulated areas. Furthermore, by leveraging Class Activation\nMapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM\nRefinement (CGSR) module to generate more accurate manipulation localization\nmaps. By integrating two modules, we present a novel weakly supervised\nframework based on a dual-branch Transformer-CNN architecture. Our method\nachieves outstanding localization performance across multiple datasets.",
      "pdf_url": "http://arxiv.org/pdf/2503.20294v1",
      "published": "2025-03-26T07:35:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20294v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets",
      "authors": [
        "Chenwei Zhang",
        "Anne Condon",
        "Khanh Dao Duc"
      ],
      "abstract": "Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.",
      "pdf_url": "http://arxiv.org/pdf/2503.20291v1",
      "published": "2025-03-26T07:33:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20291v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.BM"
      ]
    },
    {
      "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions",
      "authors": [
        "Siyin Wang",
        "Wenyi Yu",
        "Xianzhao Chen",
        "Xiaohai Tian",
        "Jun Zhang",
        "Yu Tsao",
        "Junichi Yamagishi",
        "Yuxuan Wang",
        "Chao Zhang"
      ],
      "abstract": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech.",
      "pdf_url": "http://arxiv.org/pdf/2503.20290v1",
      "published": "2025-03-26T07:32:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20290v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ]
    },
    {
      "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
      "authors": [
        "Hongye Cao",
        "Fan Feng",
        "Jing Huo",
        "Shangdong Yang",
        "Meng Fang",
        "Tianpei Yang",
        "Yang Gao"
      ],
      "abstract": "Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2503.20285v1",
      "published": "2025-03-26T07:24:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20285v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Faster Parameter-Efficient Tuning with Token Redundancy Reduction",
      "authors": [
        "Kwonyoung Kim",
        "Jungin Park",
        "Jin Kim",
        "Hyeongjun Kwon",
        "Kwanghoon Sohn"
      ],
      "abstract": "Parameter-efficient tuning (PET) aims to transfer pre-trained foundation\nmodels to downstream tasks by learning a small number of parameters. Compared\nto traditional fine-tuning, which updates the entire model, PET significantly\nreduces storage and transfer costs for each task regardless of exponentially\nincreasing pre-trained model capacity. However, most PET methods inherit the\ninference latency of their large backbone models and often introduce additional\ncomputational overhead due to additional modules (e.g. adapters), limiting\ntheir practicality for compute-intensive applications. In this paper, we\npropose Faster Parameter-Efficient Tuning (FPET), a novel approach that\nenhances inference speed and training efficiency while maintaining high storage\nefficiency. Specifically, we introduce a plug-and-play token redundancy\nreduction module delicately designed for PET. This module refines tokens from\nthe self-attention layer using an adapter to learn the accurate similarity\nbetween tokens and cuts off the tokens through a fully-differentiable token\nmerging strategy, which uses a straight-through estimator for optimal token\nreduction. Experimental results prove that our FPET achieves faster inference\nand higher memory efficiency than the pre-trained backbone while keeping\ncompetitive performance on par with state-of-the-art PET methods.",
      "pdf_url": "http://arxiv.org/pdf/2503.20282v1",
      "published": "2025-03-26T07:15:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20282v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Are We There Yet? Unraveling the State-of-the-Art Graph Network Intrusion Detection Systems",
      "authors": [
        "Chenglong Wang",
        "Pujia Zheng",
        "Jiaping Gui",
        "Cunqing Hua",
        "Wajih Ul Hassan"
      ],
      "abstract": "Network Intrusion Detection Systems (NIDS) are vital for ensuring enterprise\nsecurity. Recently, Graph-based NIDS (GIDS) have attracted considerable\nattention because of their capability to effectively capture the complex\nrelationships within the graph structures of data communications. Despite their\npromise, the reproducibility and replicability of these GIDS remain largely\nunexplored, posing challenges for developing reliable and robust detection\nsystems. This study bridges this gap by designing a systematic approach to\nevaluate state-of-the-art GIDS, which includes critically assessing, extending,\nand clarifying the findings of these systems. We further assess the robustness\nof GIDS under adversarial attacks. Evaluations were conducted on three public\ndatasets as well as a newly collected large-scale enterprise dataset. Our\nfindings reveal significant performance discrepancies, highlighting challenges\nrelated to dataset scale, model inputs, and implementation settings. We\ndemonstrate difficulties in reproducing and replicating results, particularly\nconcerning false positive rates and robustness against adversarial attacks.\nThis work provides valuable insights and recommendations for future research,\nemphasizing the importance of rigorous reproduction and replication studies in\ndeveloping robust and generalizable GIDS solutions.",
      "pdf_url": "http://arxiv.org/pdf/2503.20281v1",
      "published": "2025-03-26T07:11:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20281v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "sudo rm -rf agentic_security",
      "authors": [
        "Sejin Lee",
        "Jian Kim",
        "Haon Park",
        "Ashkan Yousefpour",
        "Sangyoon Yu",
        "Min Song"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs.",
      "pdf_url": "http://arxiv.org/pdf/2503.20279v1",
      "published": "2025-03-26T07:08:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20279v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "LogicQA: Logical Anomaly Detection with Vision Language Model Generated Questions",
      "authors": [
        "Yejin Kwon",
        "Daeun Moon",
        "Youngje Oh",
        "Hyunsoo Yoon"
      ],
      "abstract": "Anomaly Detection (AD) focuses on detecting samples that differ from the\nstandard pattern, making it a vital tool in process control. Logical anomalies\nmay appear visually normal yet violate predefined constraints on object\npresence, arrangement, or quantity, depending on reasoning and explainability.\nWe introduce LogicQA, a framework that enhances AD by providing industrial\noperators with explanations for logical anomalies. LogicQA compiles\nautomatically generated questions into a checklist and collects responses to\nidentify violations of logical constraints. LogicQA is training-free,\nannotation-free, and operates in a few-shot setting. We achieve\nstate-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO\nAD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the\nexplanations of anomalies. Also, our approach has shown outstanding performance\non semiconductor SEM corporate data, further validating its effectiveness in\nindustrial applications.",
      "pdf_url": "http://arxiv.org/pdf/2503.20252v1",
      "published": "2025-03-26T05:38:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20252v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "ESSR: An 8K@30FPS Super-Resolution Accelerator With Edge Selective Network",
      "authors": [
        "Chih-Chia Hsu",
        "Tian-Sheuan Chang"
      ],
      "abstract": "Deep learning-based super-resolution (SR) is challenging to implement in\nresource-constrained edge devices for resolutions beyond full HD due to its\nhigh computational complexity and memory bandwidth requirements. This paper\nintroduces an 8K@30FPS SR accelerator with edge-selective dynamic input\nprocessing. Dynamic processing chooses the appropriate subnets for different\npatches based on simple input edge criteria, achieving a 50\\% MAC reduction\nwith only a 0.1dB PSNR decrease. The quality of reconstruction images is\nguaranteed and maximized its potential with \\textit{resource adaptive model\nswitching} even under resource constraints. In conjunction with\nhardware-specific refinements, the model size is reduced by 84\\% to 51K, but\nwith a decrease of less than 0.6dB PSNR. Additionally, to support dynamic\nprocessing with high utilization, this design incorporates a\n\\textit{configurable group of layer mapping} that synergizes with the\n\\textit{structure-friendly fusion block}, resulting in 77\\% hardware\nutilization and up to 79\\% reduction in feature SRAM access. The\nimplementation, using the TSMC 28nm process, can achieve 8K@30FPS throughput at\n800MHz with a gate count of 2749K, 0.2075W power consumption, and 4797Mpixels/J\nenergy efficiency, exceeding previous work.",
      "pdf_url": "http://arxiv.org/pdf/2503.20245v1",
      "published": "2025-03-26T05:27:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20245v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.MM",
        "eess.IV"
      ]
    },
    {
      "title": "LGR: LLM-Guided Ranking of Frontiers for Object Goal Navigation",
      "authors": [
        "Mitsuaki Uno",
        "Kanji Tanaka",
        "Daiki Iwata",
        "Yudai Noda",
        "Shoya Miyazaki",
        "Kouki Terashima"
      ],
      "abstract": "Object Goal Navigation (OGN) is a fundamental task for robots and AI, with\nkey applications such as mobile robot image databases (MRID). In particular,\nmapless OGN is essential in scenarios involving unknown or dynamic\nenvironments. This study aims to enhance recent modular mapless OGN systems by\nleveraging the commonsense reasoning capabilities of large language models\n(LLMs). Specifically, we address the challenge of determining the visiting\norder in frontier-based exploration by framing it as a frontier ranking\nproblem. Our approach is grounded in recent findings that, while LLMs cannot\ndetermine the absolute value of a frontier, they excel at evaluating the\nrelative value between multiple frontiers viewed within a single image using\nthe view image as context. We dynamically manage the frontier list by adding\nand removing elements, using an LLM as a ranking model. The ranking results are\nrepresented as reciprocal rank vectors, which are ideal for multi-view,\nmulti-query information fusion. We validate the effectiveness of our method\nthrough evaluations in Habitat-Sim.",
      "pdf_url": "http://arxiv.org/pdf/2503.20241v1",
      "published": "2025-03-26T05:15:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20241v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Dynamic Learning and Productivity for Data Analysts: A Bayesian Hidden Markov Model Perspective",
      "authors": [
        "Yue Yin"
      ],
      "abstract": "Data analysts are essential in organizations, transforming raw data into\ninsights that drive decision-making and strategy. This study explores how\nanalysts' productivity evolves on a collaborative platform, focusing on two key\nlearning activities: writing queries and viewing peer queries. While\ntraditional research often assumes static models, where performance improves\nsteadily with cumulative learning, such models fail to capture the dynamic\nnature of real-world learning. To address this, we propose a Hidden Markov\nModel (HMM) that tracks how analysts transition between distinct learning\nstates based on their participation in these activities.\n  Using an industry dataset with 2,001 analysts and 79,797 queries, this study\nidentifies three learning states: novice, intermediate, and advanced.\nProductivity increases as analysts advance to higher states, reflecting the\ncumulative benefits of learning. Writing queries benefits analysts across all\nstates, with the largest gains observed for novices. Viewing peer queries\nsupports novices but may hinder analysts in higher states due to cognitive\noverload or inefficiencies. Transitions between states are also uneven, with\nprogression from intermediate to advanced being particularly challenging. This\nstudy advances understanding of into dynamic learning behavior of knowledge\nworker and offers practical implications for designing systems, optimizing\ntraining, enabling personalized learning, and fostering effective knowledge\nsharing.",
      "pdf_url": "http://arxiv.org/pdf/2503.20233v1",
      "published": "2025-03-26T04:57:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20233v1",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CE",
        "cs.HC"
      ]
    },
    {
      "title": "Dynamics of Algorithmic Content Amplification on TikTok",
      "authors": [
        "Fabian Baumann",
        "Nipun Arora",
        "Iyad Rahwan",
        "Agnieszka Czaplicka"
      ],
      "abstract": "Intelligent algorithms increasingly shape the content we encounter and engage\nwith online. TikTok's For You feed exemplifies extreme algorithm-driven\ncuration, tailoring the stream of video content almost exclusively based on\nusers' explicit and implicit interactions with the platform. Despite growing\nattention, the dynamics of content amplification on TikTok remain largely\nunquantified. How quickly, and to what extent, does TikTok's algorithm amplify\ncontent aligned with users' interests? To address these questions, we conduct a\nsock-puppet audit, deploying bots with different interests to engage with\nTikTok's \"For You\" feed. Our findings reveal that content aligned with the\nbots' interests undergoes strong amplification, with rapid reinforcement\ntypically occurring within the first 200 videos watched. While amplification is\nconsistently observed across all interests, its intensity varies by interest,\nindicating the emergence of topic-specific biases. Time series analyses and\nMarkov models uncover distinct phases of recommendation dynamics, including\npersistent content reinforcement and a gradual decline in content diversity\nover time. Although TikTok's algorithm preserves some content diversity, we\nfind a strong negative correlation between amplification and exploration: as\nthe amplification of interest-aligned content increases, engagement with unseen\nhashtags declines. These findings contribute to discussions on\nsocio-algorithmic feedback loops in the digital age and the trade-offs between\npersonalization and content diversity.",
      "pdf_url": "http://arxiv.org/pdf/2503.20231v1",
      "published": "2025-03-26T04:54:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20231v1",
      "categories": [
        "physics.soc-ph",
        "cs.AI",
        "cs.SI"
      ]
    },
    {
      "title": "TraNCE: Transformative Non-linear Concept Explainer for CNNs",
      "authors": [
        "Ugochukwu Ejike Akpudo",
        "Yongsheng Gao",
        "Jun Zhou",
        "Andrew Lewis"
      ],
      "abstract": "Convolutional neural networks (CNNs) have succeeded remarkably in various\ncomputer vision tasks. However, they are not intrinsically explainable. While\nthe feature-level understanding of CNNs reveals where the models looked,\nconcept-based explainability methods provide insights into what the models saw.\nHowever, their assumption of linear reconstructability of image activations\nfails to capture the intricate relationships within these activations. Their\nFidelity-only approach to evaluating global explanations also presents a new\nconcern. For the first time, we address these limitations with the novel\nTransformative Nonlinear Concept Explainer (TraNCE) for CNNs. Unlike linear\nreconstruction assumptions made by existing methods, TraNCE captures the\nintricate relationships within the activations. This study presents three\noriginal contributions to the CNN explainability literature: (i) An automatic\nconcept discovery mechanism based on variational autoencoders (VAEs). This\ntransformative concept discovery process enhances the identification of\nmeaningful concepts from image activations. (ii) A visualization module that\nleverages the Bessel function to create a smooth transition between\nprototypical image pixels, revealing not only what the CNN saw but also what\nthe CNN avoided, thereby mitigating the challenges of concept duplication as\ndocumented in previous works. (iii) A new metric, the Faith score, integrates\nboth Coherence and Fidelity for a comprehensive evaluation of explainer\nfaithfulness and consistency.",
      "pdf_url": "http://arxiv.org/pdf/2503.20230v1",
      "published": "2025-03-26T04:49:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.20230v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    }
  ]
}