{
  "last_updated": "2025-04-08T00:47:37.896039",
  "papers": [
    {
      "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning",
      "authors": [
        "Kate Sanders",
        "Benjamin Van Durme"
      ],
      "abstract": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.",
      "pdf_url": "http://arxiv.org/pdf/2504.03640v1",
      "published": "2025-04-04T17:59:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03640v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "68T50, 68T37",
        "I.2.7"
      ]
    },
    {
      "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning",
      "authors": [
        "Xinyi Wang",
        "Shawn Tan",
        "Mingyu Jin",
        "William Yang Wang",
        "Rameswar Panda",
        "Yikang Shen"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks.",
      "pdf_url": "http://arxiv.org/pdf/2504.03635v1",
      "published": "2025-04-04T17:57:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03635v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
      "authors": [
        "NVIDIA",
        ":",
        "Aaron Blakeman",
        "Aarti Basant",
        "Abhinav Khattar",
        "Adithya Renduchintala",
        "Akhiad Bercovich",
        "Aleksander Ficek",
        "Alexis Bjorlin",
        "Ali Taghibakhshi",
        "Amala Sanjay Deshmukh",
        "Ameya Sunil Mahabaleshwarkar",
        "Andrew Tao",
        "Anna Shors",
        "Ashwath Aithal",
        "Ashwin Poojary",
        "Ayush Dattagupta",
        "Balaram Buddharaju",
        "Bobby Chen",
        "Boris Ginsburg",
        "Boxin Wang",
        "Brandon Norick",
        "Brian Butterfield",
        "Bryan Catanzaro",
        "Carlo del Mundo",
        "Chengyu Dong",
        "Christine Harvey",
        "Christopher Parisien",
        "Dan Su",
        "Daniel Korzekwa",
        "Danny Yin",
        "Daria Gitman",
        "David Mosallanezhad",
        "Deepak Narayanan",
        "Denys Fridman",
        "Dima Rekesh",
        "Ding Ma",
        "Dmytro Pykhtar",
        "Dong Ahn",
        "Duncan Riach",
        "Dusan Stosic",
        "Eileen Long",
        "Elad Segal",
        "Ellie Evans",
        "Eric Chung",
        "Erick Galinkin",
        "Evelina Bakhturina",
        "Ewa Dobrowolska",
        "Fei Jia",
        "Fuxiao Liu",
        "Gargi Prasad",
        "Gerald Shen",
        "Guilin Liu",
        "Guo Chen",
        "Haifeng Qian",
        "Helen Ngo",
        "Hongbin Liu",
        "Hui Li",
        "Igor Gitman",
        "Ilia Karmanov",
        "Ivan Moshkov",
        "Izik Golan",
        "Jan Kautz",
        "Jane Polak Scowcroft",
        "Jared Casper",
        "Jarno Seppanen",
        "Jason Lu",
        "Jason Sewall",
        "Jiaqi Zeng",
        "Jiaxuan You",
        "Jimmy Zhang",
        "Jing Zhang",
        "Jining Huang",
        "Jinze Xue",
        "Jocelyn Huang",
        "Joey Conway",
        "John Kamalu",
        "Jon Barker",
        "Jonathan Cohen",
        "Joseph Jennings",
        "Jupinder Parmar",
        "Karan Sapra",
        "Kari Briski",
        "Kateryna Chumachenko",
        "Katherine Luna",
        "Keshav Santhanam",
        "Kezhi Kong",
        "Kirthi Sivamani",
        "Krzysztof Pawelec",
        "Kumar Anik",
        "Kunlun Li",
        "Lawrence McAfee",
        "Leon Derczynski",
        "Lindsey Pavao",
        "Luis Vega",
        "Lukas Voegtle",
        "Maciej Bala",
        "Maer Rodrigues de Melo",
        "Makesh Narsimhan Sreedhar",
        "Marcin Chochowski",
        "Markus Kliegl",
        "Marta Stepniewska-Dziubinska",
        "Matthieu Le",
        "Matvei Novikov",
        "Mehrzad Samadi",
        "Michael Andersch",
        "Michael Evans",
        "Miguel Martinez",
        "Mike Chrzanowski",
        "Mike Ranzinger",
        "Mikolaj Blaz",
        "Misha Smelyanskiy",
        "Mohamed Fawzy",
        "Mohammad Shoeybi",
        "Mostofa Patwary",
        "Nayeon Lee",
        "Nima Tajbakhsh",
        "Ning Xu",
        "Oleg Rybakov",
        "Oleksii Kuchaiev",
        "Olivier Delalleau",
        "Osvald Nitski",
        "Parth Chadha",
        "Pasha Shamis",
        "Paulius Micikevicius",
        "Pavlo Molchanov",
        "Peter Dykas",
        "Philipp Fischer",
        "Pierre-Yves Aquilanti",
        "Piotr Bialecki",
        "Prasoon Varshney",
        "Pritam Gundecha",
        "Przemek Tredak",
        "Rabeeh Karimi",
        "Rahul Kandu",
        "Ran El-Yaniv",
        "Raviraj Joshi",
        "Roger Waleffe",
        "Ruoxi Zhang",
        "Sabrina Kavanaugh",
        "Sahil Jain",
        "Samuel Kriman",
        "Sangkug Lym",
        "Sanjeev Satheesh",
        "Saurav Muralidharan",
        "Sean Narenthiran",
        "Selvaraj Anandaraj",
        "Seonmyeong Bak",
        "Sergey Kashirsky",
        "Seungju Han",
        "Shantanu Acharya",
        "Shaona Ghosh",
        "Sharath Turuvekere Sreenivas",
        "Sharon Clay",
        "Shelby Thomas",
        "Shrimai Prabhumoye",
        "Shubham Pachori",
        "Shubham Toshniwal",
        "Shyamala Prayaga",
        "Siddhartha Jain",
        "Sirshak Das",
        "Slawek Kierat",
        "Somshubra Majumdar",
        "Song Han",
        "Soumye Singhal",
        "Sriharsha Niverty",
        "Stefania Alborghetti",
        "Suseella Panguluri",
        "Swetha Bhendigeri",
        "Syeda Nahida Akter",
        "Szymon Migacz",
        "Tal Shiri",
        "Terry Kong",
        "Timo Roman",
        "Tomer Ronen",
        "Trisha Saar",
        "Tugrul Konuk",
        "Tuomas Rintamaki",
        "Tyler Poon",
        "Ushnish De",
        "Vahid Noroozi",
        "Varun Singh",
        "Vijay Korthikanti",
        "Vitaly Kurin",
        "Wasi Uddin Ahmad",
        "Wei Du",
        "Wei Ping",
        "Wenliang Dai",
        "Wonmin Byeon",
        "Xiaowei Ren",
        "Yao Xu",
        "Yejin Choi",
        "Yian Zhang",
        "Ying Lin",
        "Yoshi Suhara",
        "Zhiding Yu",
        "Zhiqi Li",
        "Zhiyu Li",
        "Zhongbo Zhu",
        "Zhuolin Yang",
        "Zijia Chen"
      ],
      "abstract": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. All Nemotron-H models will be released, with\nsupport in Hugging Face, NeMo, and Megatron-LM.",
      "pdf_url": "http://arxiv.org/pdf/2504.03624v1",
      "published": "2025-04-04T17:41:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03624v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Align to Structure: Aligning Large Language Models with Structural Information",
      "authors": [
        "Zae Myung Kim",
        "Anand Ramachandran",
        "Farideh Tavazoee",
        "Joo-Kyung Kim",
        "Oleg Rokhlenko",
        "Dongyeop Kang"
      ],
      "abstract": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align.",
      "pdf_url": "http://arxiv.org/pdf/2504.03622v1",
      "published": "2025-04-04T17:40:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03622v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task",
      "authors": [
        "Leonardo Ranaldi",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages.",
      "pdf_url": "http://arxiv.org/pdf/2504.03616v1",
      "published": "2025-04-04T17:35:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03616v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Autonomous and Self-Adapting System for Synthetic Media Detection and Attribution",
      "authors": [
        "Aref Azizpour",
        "Tai D. Nguyen",
        "Matthew C. Stamm"
      ],
      "abstract": "Rapid advances in generative AI have enabled the creation of highly realistic\nsynthetic images, which, while beneficial in many domains, also pose serious\nrisks in terms of disinformation, fraud, and other malicious applications.\nCurrent synthetic image identification systems are typically static, relying on\nfeature representations learned from known generators; as new generative models\nemerge, these systems suffer from severe performance degradation. In this\npaper, we introduce the concept of an autonomous self-adaptive synthetic media\nidentification system -- one that not only detects synthetic images and\nattributes them to known sources but also autonomously identifies and\nincorporates novel generators without human intervention. Our approach\nleverages an open-set identification strategy with an evolvable embedding space\nthat distinguishes between known and unknown sources. By employing an\nunsupervised clustering method to aggregate unknown samples into\nhigh-confidence clusters and continuously refining its decision boundaries, our\nsystem maintains robust detection and attribution performance even as the\ngenerative landscape evolves. Extensive experiments demonstrate that our method\nsignificantly outperforms existing approaches, marking a crucial step toward\nuniversal, adaptable forensic systems in the era of rapidly advancing\ngenerative models.",
      "pdf_url": "http://arxiv.org/pdf/2504.03615v1",
      "published": "2025-04-04T17:33:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03615v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Towards deployment-centric multimodal AI beyond vision and language",
      "authors": [
        "Xianyuan Liu",
        "Jiayang Zhang",
        "Shuo Zhou",
        "Thijs L. van der Plas",
        "Avish Vijayaraghavan",
        "Anastasiia Grishina",
        "Mengdie Zhuang",
        "Daniel Schofield",
        "Christopher Tomlinson",
        "Yuhan Wang",
        "Ruizhe Li",
        "Louisa van Zeeland",
        "Sina Tabakhi",
        "Cyndie Demeocq",
        "Xiang Li",
        "Arunav Das",
        "Orlando Timmerman",
        "Thomas Baldwin-McDonald",
        "Jinge Wu",
        "Peizhen Bai",
        "Zahraa Al Sahili",
        "Omnia Alwazzan",
        "Thao N. Do",
        "Mohammod N. I. Suvon",
        "Angeline Wang",
        "Lucia Cipolina-Kun",
        "Luigi A. Moretti",
        "Lucas Farndale",
        "Nitisha Jain",
        "Natalia Efremova",
        "Yan Ge",
        "Marta Varela",
        "Hak-Keung Lam",
        "Oya Celiktutan",
        "Ben R. Evans",
        "Alejandro Coca-Castro",
        "Honghan Wu",
        "Zahraa S. Abdallah",
        "Chen Chen",
        "Valentin Danchev",
        "Nataliya Tkachenko",
        "Lei Lu",
        "Tingting Zhu",
        "Gregory G. Slabaugh",
        "Roger K. Moore",
        "William K. Cheung",
        "Peter H. Charlton",
        "Haiping Lu"
      ],
      "abstract": "Multimodal artificial intelligence (AI) integrates diverse types of data via\nmachine learning to improve understanding, prediction, and decision-making\nacross disciplines such as healthcare, science, and engineering. However, most\nmultimodal AI advances focus on models for vision and language data, while\ntheir deployability remains a key challenge. We advocate a deployment-centric\nworkflow that incorporates deployment constraints early to reduce the\nlikelihood of undeployable solutions, complementing data-centric and\nmodel-centric approaches. We also emphasise deeper integration across multiple\nlevels of multimodality and multidisciplinary collaboration to significantly\nbroaden the research scope beyond vision and language. To facilitate this\napproach, we identify common multimodal-AI-specific challenges shared across\ndisciplines and examine three real-world use cases: pandemic response,\nself-driving car design, and climate change adaptation, drawing expertise from\nhealthcare, social science, engineering, science, sustainability, and finance.\nBy fostering multidisciplinary dialogue and open research practices, our\ncommunity can accelerate deployment-centric development for broad societal\nimpact.",
      "pdf_url": "http://arxiv.org/pdf/2504.03603v1",
      "published": "2025-04-04T17:20:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03603v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
      "authors": [
        "Akshara Prabhakar",
        "Zuxin Liu",
        "Weiran Yao",
        "Jianguo Zhang",
        "Ming Zhu",
        "Shiyu Wang",
        "Zhiwei Liu",
        "Tulika Awalgaonkar",
        "Haolin Chen",
        "Thai Hoang",
        "Juan Carlos Niebles",
        "Shelby Heinecke",
        "Huan Wang",
        "Silvio Savarese",
        "Caiming Xiong"
      ],
      "abstract": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
      "pdf_url": "http://arxiv.org/pdf/2504.03601v1",
      "published": "2025-04-04T17:13:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03601v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos",
      "authors": [
        "Jun Ma",
        "Zongxin Yang",
        "Sumin Kim",
        "Bihui Chen",
        "Mohammed Baharoon",
        "Adibvafa Fallahpour",
        "Reza Asakereh",
        "Hongwei Lyu",
        "Bo Wang"
      ],
      "abstract": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments.",
      "pdf_url": "http://arxiv.org/pdf/2504.03600v1",
      "published": "2025-04-04T17:13:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03600v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline",
      "authors": [
        "Peter Baile Chen",
        "Tomer Wolfson",
        "Michael Cafarella",
        "Dan Roth"
      ],
      "abstract": "Existing information retrieval systems excel in cases where the language of\ntarget documents closely matches that of the user query. However, real-world\nretrieval systems are often required to implicitly reason whether a document is\nrelevant. For example, when retrieving technical texts or tables, their\nrelevance to the user query may be implied through a particular jargon or\nstructure, rather than explicitly expressed in their content. Large language\nmodels (LLMs) hold great potential in identifying such implied relevance by\nleveraging their reasoning skills. Nevertheless, current LLM-augmented\nretrieval is hindered by high latency and computation cost, as the LLM\ntypically computes the query-document relevance online, for every query anew.\nTo tackle this issue we introduce EnrichIndex, a retrieval approach which\ninstead uses the LLM offline to build semantically-enriched retrieval indices,\nby performing a single pass over all documents in the retrieval corpus once\nduring ingestion time. Furthermore, the semantically-enriched indices can\ncomplement existing online retrieval approaches, boosting the performance of\nLLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving\npassages and tables, and found that it outperforms strong online LLM-based\nretrieval systems, with an average improvement of 11.7 points in recall @ 10\nand 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online\ncalls to the LLM, it processes 293.3 times fewer tokens which greatly reduces\nthe online latency and cost. Overall, EnrichIndex is an effective way to build\nbetter retrieval indices offline by leveraging the strong reasoning skills of\nLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2504.03598v1",
      "published": "2025-04-04T17:08:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03598v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation",
      "authors": [
        "Jad Abou-Chakra",
        "Lingfeng Sun",
        "Krishan Rana",
        "Brandon May",
        "Karl Schmeckpeper",
        "Maria Vittoria Minniti",
        "Laura Herlant"
      ],
      "abstract": "Recent advancements in behavior cloning have enabled robots to perform\ncomplex manipulation tasks. However, accurately assessing training performance\nremains challenging, particularly for real-world applications, as behavior\ncloning losses often correlate poorly with actual task success. Consequently,\nresearchers resort to success rate metrics derived from costly and\ntime-consuming real-world evaluations, making the identification of optimal\npolicies and detection of overfitting or underfitting impractical. To address\nthese issues, we propose real-is-sim, a novel behavior cloning framework that\nincorporates a dynamic digital twin (based on Embodied Gaussians) throughout\nthe entire policy development pipeline: data collection, training, and\ndeployment. By continuously aligning the simulated world with the physical\nworld, demonstrations can be collected in the real world with states extracted\nfrom the simulator. The simulator enables flexible state representations by\nrendering image inputs from any viewpoint or extracting low-level state\ninformation from objects embodied within the scene. During training, policies\ncan be directly evaluated within the simulator in an offline and highly\nparallelizable manner. Finally, during deployment, policies are run within the\nsimulator where the real robot directly tracks the simulated robot's joints,\neffectively decoupling policy execution from real hardware and mitigating\ntraditional domain-transfer challenges. We validate real-is-sim on the PushT\nmanipulation task, demonstrating strong correlation between success rates\nobtained in the simulator and real-world evaluations. Videos of our system can\nbe found at https://realissim.rai-inst.com.",
      "pdf_url": "http://arxiv.org/pdf/2504.03597v1",
      "published": "2025-04-04T17:05:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03597v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement",
      "authors": [
        "Runnan Fang",
        "Xiaobin Wang",
        "Yuan Liang",
        "Shuofei Qiao",
        "Jialong Wu",
        "Zekun Xi",
        "Ningyu Zhang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
      ],
      "abstract": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
      "pdf_url": "http://arxiv.org/pdf/2504.03561v1",
      "published": "2025-04-04T16:10:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03561v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Agentic Knowledgeable Self-awareness",
      "authors": [
        "Shuofei Qiao",
        "Zhisong Qiu",
        "Baochang Ren",
        "Xiaobin Wang",
        "Xiangyuan Ru",
        "Ningyu Zhang",
        "Xiang Chen",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
      ],
      "abstract": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
      "pdf_url": "http://arxiv.org/pdf/2504.03553v1",
      "published": "2025-04-04T16:03:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03553v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation",
      "authors": [
        "Khai Le-Duc",
        "Tuyen Tran",
        "Bach Phan Tat",
        "Nguyen Kim Hai Bui",
        "Quan Dang",
        "Hung-Phong Tran",
        "Thanh-Thuy Nguyen",
        "Ly Nguyen",
        "Tuan-Minh Phan",
        "Thi Thu Phuong Tran",
        "Chris Ngo",
        "Nguyen X. Khanh",
        "Thanh Nguyen-Tang"
      ],
      "abstract": "Multilingual speech translation (ST) in the medical domain enhances patient\ncare by enabling efficient communication across language barriers, alleviating\nspecialized workforce shortages, and facilitating improved diagnosis and\ntreatment, particularly during pandemics. In this work, we present the first\nsystematic study on medical ST, to our best knowledge, by releasing\nMultiMed-ST, a large-scale ST dataset for the medical domain, spanning all\ntranslation directions in five languages: Vietnamese, English, German, French,\nTraditional Chinese and Simplified Chinese, together with the models. With\n290,000 samples, our dataset is the largest medical machine translation (MT)\ndataset and the largest many-to-many multilingual ST among all domains.\nSecondly, we present the most extensive analysis study in ST research to date,\nincluding: empirical baselines, bilingual-multilingual comparative study,\nend-to-end vs. cascaded comparative study, task-specific vs. multi-task\nsequence-to-sequence (seq2seq) comparative study, code-switch analysis, and\nquantitative-qualitative error analysis. All code, data, and models are\navailable online: https://github.com/leduckhai/MultiMed-ST.",
      "pdf_url": "http://arxiv.org/pdf/2504.03546v1",
      "published": "2025-04-04T15:49:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03546v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Dense Neural Network Based Arrhythmia Classification on Low-cost and Low-compute Micro-controller",
      "authors": [
        "Md Abu Obaida Zishan",
        "H M Shihab",
        "Sabik Sadman Islam",
        "Maliha Alam Riya",
        "Gazi Mashrur Rahman",
        "Jannatun Noor"
      ],
      "abstract": "The electrocardiogram (ECG) monitoring device is an expensive albeit\nessential device for the treatment and diagnosis of cardiovascular diseases\n(CVD). The cost of this device typically ranges from $2000 to $10000. Several\nstudies have implemented ECG monitoring systems in micro-controller units (MCU)\nto reduce industrial development costs by up to 20 times. However, to match\nindustry-grade systems and display heartbeats effectively, it is essential to\ndevelop an efficient algorithm for detecting arrhythmia (irregular heartbeat).\nHence in this study, a dense neural network is developed to detect arrhythmia\non the Arduino Nano. The Nano consists of the ATMega328 microcontroller with a\n16MHz clock, 2KB of SRAM, and 32KB of program memory. Additionally, the AD8232\nSparkFun Single-Lead Heart Rate Monitor is used as the ECG sensor. The\nimplemented neural network model consists of two layers (excluding the input)\nwith 10 and four neurons respectively with sigmoid activation function.\nHowever, four approaches are explored to choose the appropriate activation\nfunctions. The model has a size of 1.267 KB, achieves an F1 score\n(macro-average) of 78.3\\% for classifying four types of arrhythmia, an accuracy\nrate of 96.38%, and requires 0.001314 MOps of floating-point operations\n(FLOPs).",
      "pdf_url": "http://arxiv.org/pdf/2504.03531v1",
      "published": "2025-04-04T15:30:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03531v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.1; I.2.6; C.3"
      ]
    },
    {
      "title": "Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems",
      "authors": [
        "Alexander Windmann",
        "Henrik Steude",
        "Daniel Boschmann",
        "Oliver Niggemann"
      ],
      "abstract": "Cyber-Physical Systems (CPS) in domains such as manufacturing and energy\ndistribution generate complex time series data crucial for Prognostics and\nHealth Management (PHM). While Deep Learning (DL) methods have demonstrated\nstrong forecasting capabilities, their adoption in industrial CPS remains\nlimited due insufficient robustness. Existing robustness evaluations primarily\nfocus on formal verification or adversarial perturbations, inadequately\nrepresenting the complexities encountered in real-world CPS scenarios. To\naddress this, we introduce a practical robustness definition grounded in\ndistributional robustness, explicitly tailored to industrial CPS, and propose a\nsystematic framework for robustness evaluation. Our framework simulates\nrealistic disturbances, such as sensor drift, noise and irregular sampling,\nenabling thorough robustness analyses of forecasting models on real-world CPS\ndatasets. The robustness definition provides a standardized score to quantify\nand compare model performance across diverse datasets, assisting in informed\nmodel selection and architecture design. Through extensive empirical studies\nevaluating prominent DL architectures (including recurrent, convolutional,\nattention-based, modular, and structured state-space models) we demonstrate the\napplicability and effectiveness of our approach. We publicly release our\nrobustness benchmark to encourage further research and reproducibility.",
      "pdf_url": "http://arxiv.org/pdf/2504.03494v1",
      "published": "2025-04-04T14:50:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03494v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for Single Image Super-Resolution",
      "authors": [
        "Zihao He",
        "Shengchuan Zhang",
        "Runze Hu",
        "Yunhang Shen",
        "Yan Zhang"
      ],
      "abstract": "Super-resolution (SR) techniques are critical for enhancing image quality,\nparticularly in scenarios where high-resolution imagery is essential yet\nlimited by hardware constraints. Existing diffusion models for SR have relied\npredominantly on Gaussian models for noise generation, which often fall short\nwhen dealing with the complex and variable texture inherent in natural scenes.\nTo address these deficiencies, we introduce the Bayesian Uncertainty Guided\nDiffusion Probabilistic Model (BUFF). BUFF distinguishes itself by\nincorporating a Bayesian network to generate high-resolution uncertainty masks.\nThese masks guide the diffusion process, allowing for the adjustment of noise\nintensity in a manner that is both context-aware and adaptive. This novel\napproach not only enhances the fidelity of super-resolved images to their\noriginal high-resolution counterparts but also significantly mitigates\nartifacts and blurring in areas characterized by complex textures and fine\ndetails. The model demonstrates exceptional robustness against complex noise\npatterns and showcases superior adaptability in handling textures and edges\nwithin images. Empirical evidence, supported by visual results, illustrates the\nmodel's robustness, especially in challenging scenarios, and its effectiveness\nin addressing common SR issues such as blurring. Experimental evaluations\nconducted on the DIV2K dataset reveal that BUFF achieves a notable improvement,\nwith a +0.61 increase compared to baseline in SSIM on BSD100, surpassing\ntraditional diffusion approaches by an average additional +0.20dB PSNR gain.\nThese findings underscore the potential of Bayesian methods in enhancing\ndiffusion processes for SR, paving the way for future advancements in the\nfield.",
      "pdf_url": "http://arxiv.org/pdf/2504.03490v1",
      "published": "2025-04-04T14:43:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03490v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T45",
        "I.2.10; J.0"
      ]
    },
    {
      "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej",
      "authors": [
        "Shubham Kumar Nigam",
        "Balaramamahanthi Deepak Patnaik",
        "Ajay Varghese Thomas",
        "Noel Shallum",
        "Kripabandhu Ghosh",
        "Arnab Bhattacharya"
      ],
      "abstract": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation.",
      "pdf_url": "http://arxiv.org/pdf/2504.03486v1",
      "published": "2025-04-04T14:41:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03486v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Physics-informed 4D X-ray image reconstruction from ultra-sparse spatiotemporal data",
      "authors": [
        "Zisheng Yao",
        "Yuhe Zhang",
        "Zhe Hu",
        "Robert Klöfkorn",
        "Tobias Ritschel",
        "Pablo Villanueva-Perez"
      ],
      "abstract": "The unprecedented X-ray flux density provided by modern X-ray sources offers\nnew spatiotemporal possibilities for X-ray imaging of fast dynamic processes.\nApproaches to exploit such possibilities often result in either i) a limited\nnumber of projections or spatial information due to limited scanning speed, as\nin time-resolved tomography, or ii) a limited number of time points, as in\nstroboscopic imaging, making the reconstruction problem ill-posed and unlikely\nto be solved by classical reconstruction approaches. 4D reconstruction from\nsuch data requires sample priors, which can be included via deep learning (DL).\nState-of-the-art 4D reconstruction methods for X-ray imaging combine the power\nof AI and the physics of X-ray propagation to tackle the challenge of sparse\nviews. However, most approaches do not constrain the physics of the studied\nprocess, i.e., a full physical model. Here we present 4D physics-informed\noptimized neural implicit X-ray imaging (4D-PIONIX), a novel physics-informed\n4D X-ray image reconstruction method combining the full physical model and a\nstate-of-the-art DL-based reconstruction method for 4D X-ray imaging from\nsparse views. We demonstrate and evaluate the potential of our approach by\nretrieving 4D information from ultra-sparse spatiotemporal acquisitions of\nsimulated binary droplet collisions, a relevant fluid dynamic process. We\nenvision that this work will open new spatiotemporal possibilities for various\n4D X-ray imaging modalities, such as time-resolved X-ray tomography and more\nnovel sparse acquisition approaches like X-ray multi-projection imaging, which\nwill pave the way for investigations of various rapid 4D dynamics, such as\nfluid dynamics and composite testing.",
      "pdf_url": "http://arxiv.org/pdf/2504.03469v1",
      "published": "2025-04-04T14:18:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03469v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "physics.data-an"
      ]
    },
    {
      "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing",
      "authors": [
        "William Fleshman",
        "Benjamin Van Durme"
      ],
      "abstract": "Training large, general-purpose language models poses significant challenges.\nThe growing availability of specialized expert models, fine-tuned from\npretrained models for specific tasks or domains, offers a promising\nalternative. Leveraging the potential of these existing expert models in\nreal-world applications requires effective methods to select or merge the\nmodels best suited for a given task. This paper introduces SPECTR, an approach\nfor dynamically composing expert models at each time step during inference.\nNotably, our method requires no additional training and enables flexible,\ntoken- and layer-wise model combinations. Our experimental results demonstrate\nthat SPECTR improves routing accuracy over alternative training-free methods,\nincreasing task performance across expert domains.",
      "pdf_url": "http://arxiv.org/pdf/2504.03454v1",
      "published": "2025-04-04T13:58:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03454v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The AI Cosmologist I: An Agentic System for Automated Data Analysis",
      "authors": [
        "Adam Moss"
      ],
      "abstract": "We present the AI Cosmologist, an agentic system designed to automate\ncosmological/astronomical data analysis and machine learning research\nworkflows. This implements a complete pipeline from idea generation to\nexperimental evaluation and research dissemination, mimicking the scientific\nprocess typically performed by human researchers. The system employs\nspecialized agents for planning, coding, execution, analysis, and synthesis\nthat work together to develop novel approaches. Unlike traditional auto\nmachine-learning systems, the AI Cosmologist generates diverse implementation\nstrategies, writes complete code, handles execution errors, analyzes results,\nand synthesizes new approaches based on experimental outcomes. We demonstrate\nthe AI Cosmologist capabilities across several machine learning tasks, showing\nhow it can successfully explore solution spaces, iterate based on experimental\nresults, and combine successful elements from different approaches. Our results\nindicate that agentic systems can automate portions of the research process,\npotentially accelerating scientific discovery. The code and experimental data\nused in this paper are available on GitHub at\nhttps://github.com/adammoss/aicosmologist. Example papers included in the\nappendix demonstrate the system's capability to autonomously produce complete\nscientific publications, starting from only the dataset and task description",
      "pdf_url": "http://arxiv.org/pdf/2504.03424v1",
      "published": "2025-04-04T13:12:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03424v1",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "astro-ph.GA",
        "cs.AI",
        "physics.data-an"
      ]
    },
    {
      "title": "Autonomous state-space segmentation for Deep-RL sparse reward scenarios",
      "authors": [
        "Gianluca Maselli",
        "Vieri Giuliano Santucci"
      ],
      "abstract": "Dealing with environments with sparse rewards has always been crucial for\nsystems developed to operate in autonomous open-ended learning settings.\nIntrinsic Motivations could be an effective way to help Deep Reinforcement\nLearning algorithms learn in such scenarios. In fact, intrinsic reward signals,\nsuch as novelty or curiosity, are generally adopted to improve exploration when\nextrinsic rewards are delayed or absent. Building on previous works, we tackle\nthe problem of learning policies in the presence of sparse rewards by proposing\na two-level architecture that alternates an ''intrinsically driven'' phase of\nexploration and autonomous sub-goal generation, to a phase of sparse reward,\ngoal-directed policy learning. The idea is to build several small networks,\neach one specialized on a particular sub-path, and use them as starting points\nfor future exploration without the need to further explore from scratch\npreviously learnt paths. Two versions of the system have been trained and\ntested in the Gym SuperMarioBros environment without considering any additional\nextrinsic reward. The results show the validity of our approach and the\nimportance of autonomously segment the environment to generate an efficient\npath towards the final goal.",
      "pdf_url": "http://arxiv.org/pdf/2504.03420v1",
      "published": "2025-04-04T13:06:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03420v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning",
      "authors": [
        "Sanghwan Bae",
        "Jiwoo Hong",
        "Min Young Lee",
        "Hanbyul Kim",
        "JeongYeon Nam",
        "Donghyun Kwak"
      ],
      "abstract": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set.",
      "pdf_url": "http://arxiv.org/pdf/2504.03380v1",
      "published": "2025-04-04T11:52:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03380v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency",
      "authors": [
        "Erik Johannes Husom",
        "Arda Goknil",
        "Merve Astekin",
        "Lwin Khin Shar",
        "Andre Kåsen",
        "Sagar Sen",
        "Benedikt Andreas Mithassel",
        "Ahmet Soylu"
      ],
      "abstract": "Deploying Large Language Models (LLMs) on edge devices presents significant\nchallenges due to computational constraints, memory limitations, inference\nspeed, and energy consumption. Model quantization has emerged as a key\ntechnique to enable efficient LLM inference by reducing model size and\ncomputational overhead. In this study, we conduct a comprehensive analysis of\n28 quantized LLMs from the Ollama library, which applies by default\nPost-Training Quantization (PTQ) and weight-only quantization techniques,\ndeployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy\nefficiency, inference performance, and output accuracy across multiple\nquantization levels and task types. Models are benchmarked on five standardized\ndatasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and\nwe employ a high-resolution, hardware-based energy measurement tool to capture\nreal-world power consumption. Our findings reveal the trade-offs between energy\nefficiency, inference speed, and accuracy in different quantization settings,\nhighlighting configurations that optimize LLM deployment for\nresource-constrained environments. By integrating hardware-level energy\nprofiling with LLM benchmarking, this study provides actionable insights for\nsustainable AI, bridging a critical gap in existing research on energy-aware\nLLM deployment.",
      "pdf_url": "http://arxiv.org/pdf/2504.03360v1",
      "published": "2025-04-04T11:29:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03360v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Decentralized Collective World Model for Emergent Communication and Coordination",
      "authors": [
        "Kentaro Nomura",
        "Tatsuya Aoki",
        "Tadahiro Taniguchi",
        "Takato Horii"
      ],
      "abstract": "We propose a fully decentralized multi-agent world model that enables both\nsymbol emergence for communication and coordinated behavior through temporal\nextension of collective predictive coding. Unlike previous research that\nfocuses on either communication or coordination separately, our approach\nachieves both simultaneously. Our method integrates world models with\ncommunication channels, enabling agents to predict environmental dynamics,\nestimate states from partial observations, and share critical information\nthrough bidirectional message exchange with contrastive learning for message\nalignment. Using a two-agent trajectory drawing task, we demonstrate that our\ncommunication-based approach outperforms non-communicative models when agents\nhave divergent perceptual capabilities, achieving the second-best coordination\nafter centralized models. Importantly, our distributed approach with\nconstraints preventing direct access to other agents' internal states\nfacilitates the emergence of more meaningful symbol systems that accurately\nreflect environmental states. These findings demonstrate the effectiveness of\ndecentralized communication for supporting coordination while developing shared\nrepresentations of the environment.",
      "pdf_url": "http://arxiv.org/pdf/2504.03353v1",
      "published": "2025-04-04T11:17:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03353v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered Chatbots on the Web",
      "authors": [
        "Lars Krupp",
        "Daniel Geißler",
        "Peter Hevesi",
        "Marco Hirsch",
        "Paul Lukowicz",
        "Jakob Karolus"
      ],
      "abstract": "Integrated into websites, LLM-powered chatbots offer alternative means of\nnavigation and information retrieval, leading to a shift in how users access\ninformation on the web. Yet, predominantly closed-sourced solutions limit\nproliferation among web hosts and suffer from a lack of transparency with\nregard to implementation details and energy efficiency. In this work, we\npropose our openly available agent Talk2X leveraging an adapted\nretrieval-augmented generation approach (RAG) combined with an automatically\ngenerated vector database, benefiting energy efficiency. Talk2X's architecture\nis generalizable to arbitrary websites offering developers a ready to use tool\nfor integration. Using a mixed-methods approach, we evaluated Talk2X's\nusability by tasking users to acquire specific assets from an open science\nrepository. Talk2X significantly improved task completion time, correctness,\nand user experience supporting users in quickly pinpointing specific\ninformation as compared to standard user-website interaction. Our findings\ncontribute technical advancements to an ongoing paradigm shift of how we access\ninformation on the web.",
      "pdf_url": "http://arxiv.org/pdf/2504.03343v1",
      "published": "2025-04-04T10:58:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03343v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ]
    },
    {
      "title": "EOOD: Entropy-based Out-of-distribution Detection",
      "authors": [
        "Guide Yang",
        "Chao Hou",
        "Weilong Peng",
        "Xiang Fang",
        "Yongwei Nie",
        "Peican Zhu",
        "Keke Tang"
      ],
      "abstract": "Deep neural networks (DNNs) often exhibit overconfidence when encountering\nout-of-distribution (OOD) samples, posing significant challenges for\ndeployment. Since DNNs are trained on in-distribution (ID) datasets, the\ninformation flow of ID samples through DNNs inevitably differs from that of OOD\nsamples. In this paper, we propose an Entropy-based Out-Of-distribution\nDetection (EOOD) framework. EOOD first identifies specific block where the\ninformation flow differences between ID and OOD samples are more pronounced,\nusing both ID and pseudo-OOD samples. It then calculates the conditional\nentropy on the selected block as the OOD confidence score. Comprehensive\nexperiments conducted across various ID and OOD settings demonstrate the\neffectiveness of EOOD in OOD detection and its superiority over\nstate-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2504.03342v1",
      "published": "2025-04-04T10:57:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03342v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Mind the Prompt: Prompting Strategies in Audio Generations for Improving Sound Classification",
      "authors": [
        "Francesca Ronchini",
        "Ho-Hsiang Wu",
        "Wei-Cheng Lin",
        "Fabio Antonacci"
      ],
      "abstract": "This paper investigates the design of effective prompt strategies for\ngenerating realistic datasets using Text-To-Audio (TTA) models. We also analyze\ndifferent techniques for efficiently combining these datasets to enhance their\nutility in sound classification tasks. By evaluating two sound classification\ndatasets with two TTA models, we apply a range of prompt strategies. Our\nfindings reveal that task-specific prompt strategies significantly outperform\nbasic prompt approaches in data generation. Furthermore, merging datasets\ngenerated using different TTA models proves to enhance classification\nperformance more effectively than merely increasing the training dataset size.\nOverall, our results underscore the advantages of these methods as effective\ndata augmentation techniques using synthetic data.",
      "pdf_url": "http://arxiv.org/pdf/2504.03329v1",
      "published": "2025-04-04T10:14:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03329v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ]
    },
    {
      "title": "Policy Optimization Algorithms in a Unified Framework",
      "authors": [
        "Shuang Wu"
      ],
      "abstract": "Policy optimization algorithms are crucial in many fields but challenging to\ngrasp and implement, often due to complex calculations related to Markov\ndecision processes and varying use of discount and average reward setups. This\npaper presents a unified framework that applies generalized ergodicity theory\nand perturbation analysis to clarify and enhance the application of these\nalgorithms. Generalized ergodicity theory sheds light on the steady-state\nbehavior of stochastic processes, aiding understanding of both discounted and\naverage rewards. Perturbation analysis provides in-depth insights into the\nfundamental principles of policy optimization algorithms. We use this framework\nto identify common implementation errors and demonstrate the correct\napproaches. Through a case study on Linear Quadratic Regulator problems, we\nillustrate how slight variations in algorithm design affect implementation\noutcomes. We aim to make policy optimization algorithms more accessible and\nreduce their misuse in practice.",
      "pdf_url": "http://arxiv.org/pdf/2504.03328v1",
      "published": "2025-04-04T10:14:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03328v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ]
    },
    {
      "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models",
      "authors": [
        "Afshin Khadangi",
        "Amir Sartipi",
        "Igor Tchappi",
        "Ramin Bahmani"
      ],
      "abstract": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility.",
      "pdf_url": "http://arxiv.org/pdf/2504.03302v1",
      "published": "2025-04-04T09:27:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03302v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset and Task",
      "authors": [
        "Bingqian Wang",
        "Quan Fang",
        "Jiachen Sun",
        "Xiaoxiao Ma"
      ],
      "abstract": "Formulating statements that support diverse or controversial stances on\nspecific topics is vital for platforms that enable user expression, reshape\npolitical discourse, and drive social critique and information dissemination.\nWith the rise of Large Language Models (LLMs), controllable text generation\ntowards specific stances has become a promising research area with applications\nin shaping public opinion and commercial marketing. However, current datasets\noften focus solely on pure texts, lacking multimodal content and effective\ncontext, particularly in the context of stance detection. In this paper, we\nformally define and study the new problem of stance-driven controllable content\ngeneration for tweets with text and images, where given a multimodal post (text\nand image/video), a model generates a stance-controlled response. To this end,\nwe create the Multimodal Stance Generation Dataset (StanceGen2024), the first\nresource explicitly designed for multimodal stance-controllable text generation\nin political discourse. It includes posts and user comments from the 2024 U.S.\npresidential election, featuring text, images, videos, and stance annotations\nto explore how multimodal political content shapes stance expression.\nFurthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework\nthat integrates weighted fusion of multimodal features and stance guidance to\nimprove semantic consistency and stance control. We release the dataset and\ncode (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and\nfurther research.",
      "pdf_url": "http://arxiv.org/pdf/2504.03295v1",
      "published": "2025-04-04T09:20:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03295v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Effective EU E-Participation: The Development of AskThePublic",
      "authors": [
        "Kilian Sprenkamp",
        "Nils Messerschmidt",
        "Amir Sartipi",
        "Igor Tchappi",
        "Xiaohui Wu",
        "Liudmila Zavolokina",
        "Gilbert Fridgen"
      ],
      "abstract": "E-participation platforms can be an important asset for governments in\nincreasing trust and fostering democratic societies. By engaging\nnon-governmental and private institutions, domain experts, and even the general\npublic, policymakers can make informed and inclusive decisions. Drawing on the\nMedia Richness Theory and applying the Design Science Research method, we\nexplore how a chatbot can be designed to improve the effectiveness of the\npolicy-making process of existing citizen involvement platforms. Leveraging the\nHave Your Say platform, which solicits feedback on European Commission\ninitiatives and regulations, a Large Language Model based chatbot, called\nAskThePublic is created, providing policymakers, journalists, researchers, and\ninterested citizens with a convenient channel to explore and engage with public\ninput. By conducting 11 semistructured interviews, the results show that the\nparticipants value the interactive and structured responses as well as enhanced\nlanguage capabilities, thus increasing their likelihood of engaging with\nAskThePublic over the existing platform. An outlook for future iterations is\nprovided and discussed with regard to the perspectives of the different\nstakeholders.",
      "pdf_url": "http://arxiv.org/pdf/2504.03287v1",
      "published": "2025-04-04T09:15:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03287v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "JanusDDG: A Thermodynamics-Compliant Model for Sequence-Based Protein Stability via Two-Fronts Multi-Head Attention",
      "authors": [
        "Guido Barducci",
        "Ivan Rossi",
        "Francesco Codicè",
        "Cesare Rollo",
        "Valeria Repetto",
        "Corrado Pancotti",
        "Virginia Iannibelli",
        "Tiziana Sanavia",
        "Piero Fariselli"
      ],
      "abstract": "Understanding how residue variations affect protein stability is crucial for\ndesigning functional proteins and deciphering the molecular mechanisms\nunderlying disease-related mutations. Recent advances in protein language\nmodels (PLMs) have revolutionized computational protein analysis, enabling,\namong other things, more accurate predictions of mutational effects. In this\nwork, we introduce JanusDDG, a deep learning framework that leverages\nPLM-derived embeddings and a bidirectional cross-attention transformer\narchitecture to predict $\\Delta \\Delta G$ of single and multiple-residue\nmutations while simultaneously being constrained to respect fundamental\nthermodynamic properties, such as antisymmetry and transitivity. Unlike\nconventional self-attention, JanusDDG computes queries (Q) and values (V) as\nthe difference between wild-type and mutant embeddings, while keys (K)\nalternate between the two. This cross-interleaved attention mechanism enables\nthe model to capture mutation-induced perturbations while preserving essential\ncontextual information. Experimental results show that JanusDDG achieves\nstate-of-the-art performance in predicting $\\Delta \\Delta G$ from sequence\nalone, matching or exceeding the accuracy of structure-based methods for both\nsingle and multiple mutations.",
      "pdf_url": "http://arxiv.org/pdf/2504.03278v1",
      "published": "2025-04-04T09:02:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03278v1",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ]
    },
    {
      "title": "Monte Carlo Graph Coloring",
      "authors": [
        "Tristan Cazenave",
        "Benjamin Negrevergne",
        "Florian Sikora"
      ],
      "abstract": "Graph Coloring is probably one of the most studied and famous problem in\ngraph algorithms. Exact methods fail to solve instances with more than few\nhundred vertices, therefore, a large number of heuristics have been proposed.\nNested Monte Carlo Search (NMCS) and Nested Rollout Policy Adaptation (NRPA)\nare Monte Carlo search algorithms for single player games. Surprisingly, few\nwork has been dedicated to evaluating Monte Carlo search algorithms to\ncombinatorial graph problems. In this paper we expose how to efficiently apply\nMonte Carlo search to Graph Coloring and compare this approach to existing\nones.",
      "pdf_url": "http://arxiv.org/pdf/2504.03277v1",
      "published": "2025-04-04T08:57:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03277v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Do Large Language Models Solve the Problems of Agent-Based Modeling? A Critical Review of Generative Social Simulations",
      "authors": [
        "Maik Larooij",
        "Petter Törnberg"
      ],
      "abstract": "Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as\nthe integration of Large Language Models (LLMs) has led to the emergence of\n``generative ABMs'' as a novel approach to simulating social systems. While\nABMs offer means to bridge micro-level interactions with macro-level patterns,\nthey have long faced criticisms from social scientists, pointing to e.g., lack\nof realism, computational complexity, and challenges of calibrating and\nvalidating against empirical data. This paper reviews the generative ABM\nliterature to assess how this new approach adequately addresses these\nlong-standing criticisms. Our findings show that studies show limited awareness\nof historical debates. Validation remains poorly addressed, with many studies\nrelying solely on subjective assessments of model `believability', and even the\nmost rigorous validation failing to adequately evidence operational validity.\nWe argue that there are reasons to believe that LLMs will exacerbate rather\nthan resolve the long-standing challenges of ABMs. The black-box nature of LLMs\nmoreover limit their usefulness for disentangling complex emergent causal\nmechanisms. While generative ABMs are still in a stage of early\nexperimentation, these findings question of whether and how the field can\ntransition to the type of rigorous modeling needed to contribute to social\nscientific theory.",
      "pdf_url": "http://arxiv.org/pdf/2504.03274v1",
      "published": "2025-04-04T08:48:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03274v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "Verification of Autonomous Neural Car Control with KeYmaera X",
      "authors": [
        "Enguerrand Prebet",
        "Samuel Teuber",
        "André Platzer"
      ],
      "abstract": "This article presents a formal model and formal safety proofs for the ABZ'25\ncase study in differential dynamic logic (dL). The case study considers an\nautonomous car driving on a highway avoiding collisions with neighbouring cars.\nUsing KeYmaera X's dL implementation, we prove absence of collision on an\ninfinite time horizon which ensures that safety is preserved independently of\ntrip length. The safety guarantees hold for time-varying reaction time and\nbrake force. Our dL model considers the single lane scenario with cars ahead or\nbehind. We demonstrate that dL with its tools is a rigorous foundation for\nruntime monitoring, shielding, and neural network verification. Doing so sheds\nlight on inconsistencies between the provided specification and simulation\nenvironment highway-env of the ABZ'25 study. We attempt to fix these\ninconsistencies and uncover numerous counterexamples which also indicate issues\nin the provided reinforcement learning environment.",
      "pdf_url": "http://arxiv.org/pdf/2504.03272v1",
      "published": "2025-04-04T08:43:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03272v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "cs.SY"
      ]
    },
    {
      "title": "An Extended Symbolic-Arithmetic Model for Teaching Double-Black Removal with Rotation in Red-Black Trees",
      "authors": [
        "Kennedy E. Ehimwenma",
        "Hongyu Zhou",
        "Junfeng Wang",
        "Ze Zheng"
      ],
      "abstract": "Double-black (DB) nodes have no place in red-black (RB) trees. So when DB\nnodes are formed, they are immediately removed. The removal of DB nodes that\ncause rotation and recoloring of other connected nodes poses greater challenges\nin the teaching and learning of RB trees. To ease this difficulty, this paper\nextends our previous work on the symbolic arithmetic algebraic (SA) method for\nremoving DB nodes. The SA operations that are given as, Red + Black = Black;\nBlack - Black = Red; Black + Black = DB; and DB - Black = Black removes DB\nnodes and rebalances black heights in RB trees. By extension, this paper\nprojects three SA mathematical equations, namely, general symbolic arithmetic\nrule; partial symbolic arithmetic rule1; and partial symbolic arithmetic rule2.\nThe removal of a DB node ultimately affects black heights in RB trees. To\nbalance black heights using the SA equations, all the RB tree cases, namely,\nLR, RL, LL, and RR, were considered in this work; and the position of the nodes\nconnected directly or indirectly to the DB node was also tested. In this study,\nto balance a RB tree, the issues considered w.r.t. the different cases of the\nRB tree were i) whether a DB node has an inner, outer, or both inner and outer\nblack nephews; or ii) whether a DB node has an inner, outer or both inner and\nouter red nephews. The nephews r and x in this work are the children of the\nsibling s to a DB, and further up the tree, the parent p of a DB is their\ngrandparent g. Thus, r and x have indirect relationships to a DB at the point\nof formation of the DB node. The novelty of the SA equations is in their\neffectiveness in the removal of DB that involves rotation of nodes as well as\nthe recoloring of nodes along any simple path so as to balance black heights in\na tree.",
      "pdf_url": "http://arxiv.org/pdf/2504.03259v1",
      "published": "2025-04-04T08:19:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03259v1",
      "categories": [
        "cs.DS",
        "cs.AI"
      ]
    },
    {
      "title": "Seeing is Believing: Belief-Space Planning with Foundation Models as Uncertainty Estimators",
      "authors": [
        "Linfeng Zhao",
        "Willie McClinton",
        "Aidan Curtis",
        "Nishanth Kumar",
        "Tom Silver",
        "Leslie Pack Kaelbling",
        "Lawson L. S. Wong"
      ],
      "abstract": "Generalizable robotic mobile manipulation in open-world environments poses\nsignificant challenges due to long horizons, complex goals, and partial\nobservability. A promising approach to address these challenges involves\nplanning with a library of parameterized skills, where a task planner sequences\nthese skills to achieve goals specified in structured languages, such as\nlogical expressions over symbolic facts. While vision-language models (VLMs)\ncan be used to ground these expressions, they often assume full observability,\nleading to suboptimal behavior when the agent lacks sufficient information to\nevaluate facts with certainty. This paper introduces a novel framework that\nleverages VLMs as a perception module to estimate uncertainty and facilitate\nsymbolic grounding. Our approach constructs a symbolic belief representation\nand uses a belief-space planner to generate uncertainty-aware plans that\nincorporate strategic information gathering. This enables the agent to\neffectively reason about partial observability and property uncertainty. We\ndemonstrate our system on a range of challenging real-world tasks that require\nreasoning in partially observable environments. Simulated evaluations show that\nour approach outperforms both vanilla VLM-based end-to-end planning or\nVLM-based state estimation baselines by planning for and executing strategic\ninformation gathering. This work highlights the potential of VLMs to construct\nbelief-space symbolic scene representations, enabling downstream tasks such as\nuncertainty-aware planning.",
      "pdf_url": "http://arxiv.org/pdf/2504.03245v1",
      "published": "2025-04-04T07:48:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03245v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Rotation Invariance in Floor Plan Digitization using Zernike Moments",
      "authors": [
        "Marius Graumann",
        "Jan Marius Stürmer",
        "Tobias Koch"
      ],
      "abstract": "Nowadays, a lot of old floor plans exist in printed form or are stored as\nscanned raster images. Slight rotations or shifts may occur during scanning.\nBringing floor plans of this form into a machine readable form to enable\nfurther use, still poses a problem. Therefore, we propose an end-to-end\npipeline that pre-processes the image and leverages a novel approach to create\na region adjacency graph (RAG) from the pre-processed image and predict its\nnodes. By incorporating normalization steps into the RAG feature extraction, we\nsignificantly improved the rotation invariance of the RAG feature calculation.\nMoreover, applying our method leads to an improved F1 score and IoU on rotated\ndata. Furthermore, we proposed a wall splitting algorithm for partitioning\nwalls into segments associated with the corresponding rooms.",
      "pdf_url": "http://arxiv.org/pdf/2504.03241v1",
      "published": "2025-04-04T07:44:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03241v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Malware Detection in Docker Containers: An Image is Worth a Thousand Logs",
      "authors": [
        "Akis Nousias",
        "Efklidis Katsaros",
        "Evangelos Syrmos",
        "Panagiotis Radoglou-Grammatikis",
        "Thomas Lagkas",
        "Vasileios Argyriou",
        "Ioannis Moscholios",
        "Evangelos Markakis",
        "Sotirios Goudos",
        "Panagiotis Sarigiannidis"
      ],
      "abstract": "Malware detection is increasingly challenged by evolving techniques like\nobfuscation and polymorphism, limiting the effectiveness of traditional\nmethods. Meanwhile, the widespread adoption of software containers has\nintroduced new security challenges, including the growing threat of malicious\nsoftware injection, where a container, once compromised, can serve as entry\npoint for further cyberattacks. In this work, we address these security issues\nby introducing a method to identify compromised containers through machine\nlearning analysis of their file systems. We cast the entire software containers\ninto large RGB images via their tarball representations, and propose to use\nestablished Convolutional Neural Network architectures on a streaming,\npatch-based manner. To support our experiments, we release the COSOCO\ndataset--the first of its kind--containing 3364 large-scale RGB images of\nbenign and compromised software containers at\nhttps://huggingface.co/datasets/k3ylabs/cosoco-image-dataset. Our method\ndetects more malware and achieves higher F1 and Recall scores than all\nindividual and ensembles of VirusTotal engines, demonstrating its effectiveness\nand setting a new standard for identifying malware-compromised software\ncontainers.",
      "pdf_url": "http://arxiv.org/pdf/2504.03238v1",
      "published": "2025-04-04T07:38:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03238v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Crash Time Matters: HybridMamba for Fine-Grained Temporal Localization in Traffic Surveillance Footage",
      "authors": [
        "Ibne Farabi Shihab",
        "Anuj Sharma"
      ],
      "abstract": "Traffic crash detection in long-form surveillance videos is critical for\nemergency response and infrastructure planning but remains difficult due to the\nbrief and rare nature of crash events. We introduce HybridMamba, a novel\narchitecture that combines visual transformers with state-space temporal\nmodeling to achieve accurate crash time localization. Our method uses\nmulti-level token compression and hierarchical temporal processing to remain\ncomputationally efficient without sacrificing temporal resolution. Evaluated on\na large-scale dataset from the Iowa Department of Transportation, HybridMamba\nachieves a mean absolute error of 1.50 seconds, with 65.2 percent of\npredictions within one second of the ground truth. It outperforms recent\nvideo-language models such as TimeChat and VideoLLaMA2 by up to 2.8 seconds,\nwhile using significantly fewer parameters. Our results demonstrate strong\ngeneralization across videos ranging from 2 to 40 minutes in diverse\nconditions. HybridMamba offers a robust and efficient solution for fine-grained\ntemporal localization in traffic surveillance. The code will be released upon\npublication.",
      "pdf_url": "http://arxiv.org/pdf/2504.03235v1",
      "published": "2025-04-04T07:35:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03235v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Think When You Need: Self-Adaptive Chain-of-Thought Learning",
      "authors": [
        "Junjie Yang",
        "Ke Lin",
        "Xing Yu"
      ],
      "abstract": "Chain of Thought (CoT) reasoning enhances language models' performance but\noften leads to inefficient \"overthinking\" on simple problems. We identify that\nexisting approaches directly penalizing reasoning length fail to account for\nvarying problem complexity. Our approach constructs rewards through length and\nquality comparisons, guided by theoretical assumptions that jointly enhance\nsolution correctness with conciseness. Moreover, we further demonstrate our\nmethod to fuzzy tasks where ground truth is unavailable. Experiments across\nmultiple reasoning benchmarks demonstrate that our method maintains accuracy\nwhile generating significantly more concise explanations, effectively teaching\nmodels to \"think when needed.\"",
      "pdf_url": "http://arxiv.org/pdf/2504.03234v1",
      "published": "2025-04-04T07:34:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03234v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Persuasive Calibration",
      "authors": [
        "Yiding Feng",
        "Wei Tang"
      ],
      "abstract": "We introduce and study the persuasive calibration problem, where a principal\naims to provide trustworthy predictions about underlying events to a downstream\nagent to make desired decisions. We adopt the standard calibration framework\nthat regulates predictions to be unbiased conditional on their own value, and\nthus, they can reliably be interpreted at the face value by the agent. Allowing\na small calibration error budget, we aim to answer the following question: what\nis and how to compute the optimal predictor under this calibration error\nbudget, especially when there exists incentive misalignment between the\nprincipal and the agent? We focus on standard Lt-norm Expected Calibration\nError (ECE) metric.\n  We develop a general framework by viewing predictors as post-processed\nversions of perfectly calibrated predictors. Using this framework, we first\ncharacterize the structure of the optimal predictor. Specifically, when the\nprincipal's utility is event-independent and for L1-norm ECE, we show: (1) the\noptimal predictor is over-(resp. under-) confident for high (resp. low) true\nexpected outcomes, while remaining perfectly calibrated in the middle; (2) the\nmiscalibrated predictions exhibit a collinearity structure with the principal's\nutility function. On the algorithmic side, we provide a FPTAS for computing\napproximately optimal predictor for general principal utility and general\nLt-norm ECE. Moreover, for the L1- and L-Infinity-norm ECE, we provide\npolynomial-time algorithms that compute the exact optimal predictor.",
      "pdf_url": "http://arxiv.org/pdf/2504.03211v1",
      "published": "2025-04-04T06:49:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03211v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "econ.TH"
      ]
    },
    {
      "title": "Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted Decision-Making",
      "authors": [
        "Zelun Tony Zhang",
        "Leon Reicherts"
      ],
      "abstract": "How can we use generative AI to design tools that augment rather than replace\nhuman cognition? In this position paper, we review our own research on\nAI-assisted decision-making for lessons to learn. We observe that in both\nAI-assisted decision-making and generative AI, a popular approach is to suggest\nAI-generated end-to-end solutions to users, which users can then accept,\nreject, or edit. Alternatively, AI tools could offer more incremental support\nto help users solve tasks themselves, which we call process-oriented support.\nWe describe findings on the challenges of end-to-end solutions, and how\nprocess-oriented support can address them. We also discuss the applicability of\nthese findings to generative AI based on a recent study in which we compared\nboth approaches to assist users in a complex decision-making task with LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2504.03207v1",
      "published": "2025-04-04T06:40:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03207v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
      "authors": [
        "Yanming Wan",
        "Jiaxing Wu",
        "Marwa Abdulhai",
        "Lior Shani",
        "Natasha Jaques"
      ],
      "abstract": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them.",
      "pdf_url": "http://arxiv.org/pdf/2504.03206v1",
      "published": "2025-04-04T06:35:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03206v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video",
      "authors": [
        "Jiaxin Guo",
        "Wenzhen Dong",
        "Tianyu Huang",
        "Hao Ding",
        "Ziyi Wang",
        "Haomin Kuang",
        "Qi Dou",
        "Yun-Hui Liu"
      ],
      "abstract": "Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's\nperception and therefore plays a vital role in various computer-assisted\nsurgery tasks. However, achieving scale-consistent reconstruction remains an\nopen challenge due to inherent issues in endoscopic videos, such as dynamic\ndeformations and textureless surfaces. Despite recent advances, current methods\neither rely on calibration or instrument priors to estimate scale, or employ\nSfM-like multi-stage pipelines, leading to error accumulation and requiring\noffline optimization. In this paper, we present Endo3R, a unified 3D foundation\nmodel for online scale-consistent reconstruction from monocular surgical video,\nwithout any priors or extra optimization. Our model unifies the tasks by\npredicting globally aligned pointmaps, scale-consistent video depths, and\ncamera parameters without any offline optimization. The core contribution of\nour method is expanding the capability of the recent pairwise reconstruction\nmodel to long-term incremental dynamic reconstruction by an uncertainty-aware\ndual memory mechanism. The mechanism maintains history tokens of both\nshort-term dynamics and long-term spatial consistency. Notably, to tackle the\nhighly dynamic nature of surgical scenes, we measure the uncertainty of tokens\nvia Sampson distance and filter out tokens with high uncertainty. Regarding the\nscarcity of endoscopic datasets with ground-truth depth and camera poses, we\nfurther devise a self-supervised mechanism with a novel dynamics-aware flow\nloss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our\nsuperior performance in zero-shot surgical video depth prediction and camera\npose estimation with online efficiency. Project page:\nhttps://wrld.github.io/Endo3R/.",
      "pdf_url": "http://arxiv.org/pdf/2504.03198v1",
      "published": "2025-04-04T06:05:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03198v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents",
      "authors": [
        "Jaymari Chua",
        "Chen Wang",
        "Lina Yao"
      ],
      "abstract": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings.",
      "pdf_url": "http://arxiv.org/pdf/2504.03185v1",
      "published": "2025-04-04T05:26:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03185v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; I.2.4; I.2.6; I.2.8"
      ]
    },
    {
      "title": "Real-Time Roadway Obstacle Detection for Electric Scooters Using Deep Learning and Multi-Sensor Fusion",
      "authors": [
        "Zeyang Zheng",
        "Arman Hosseini",
        "Dong Chen",
        "Omid Shoghli",
        "Arsalan Heydarian"
      ],
      "abstract": "The increasing adoption of electric scooters (e-scooters) in urban areas has\ncoincided with a rise in traffic accidents and injuries, largely due to their\nsmall wheels, lack of suspension, and sensitivity to uneven surfaces. While\ndeep learning-based object detection has been widely used to improve automobile\nsafety, its application for e-scooter obstacle detection remains unexplored.\nThis study introduces a novel ground obstacle detection system for e-scooters,\nintegrating an RGB camera, and a depth camera to enhance real-time road hazard\ndetection. Additionally, the Inertial Measurement Unit (IMU) measures linear\nvertical acceleration to identify surface vibrations, guiding the selection of\nsix obstacle categories: tree branches, manhole covers, potholes, pine cones,\nnon-directional cracks, and truncated domes. All sensors, including the RGB\ncamera, depth camera, and IMU, are integrated within the Intel RealSense Camera\nD435i. A deep learning model powered by YOLO detects road hazards and utilizes\ndepth data to estimate obstacle proximity. Evaluated on the seven hours of\nnaturalistic riding dataset, the system achieves a high mean average precision\n(mAP) of 0.827 and demonstrates excellent real-time performance. This approach\nprovides an effective solution to enhance e-scooter safety through advanced\ncomputer vision and data fusion. The dataset is accessible at\nhttps://zenodo.org/records/14583718, and the project code is hosted on\nhttps://github.com/Zeyang-Zheng/Real-Time-Roadway-Obstacle-Detection-for-Electric-Scooters.",
      "pdf_url": "http://arxiv.org/pdf/2504.03171v1",
      "published": "2025-04-04T05:01:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03171v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving",
      "authors": [
        "Kexin Tian",
        "Jingrui Mao",
        "Yunlong Zhang",
        "Jiwan Jiang",
        "Yang Zhou",
        "Zhengzhong Tu"
      ],
      "abstract": "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong\npotential for autonomous driving tasks. However, their spatial understanding\nand reasoning-key capabilities for autonomous driving-still exhibit significant\nlimitations. Notably, none of the existing benchmarks systematically evaluate\nVLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we\npropose NuScenes-SpatialQA, the first large-scale ground-truth-based\nQuestion-Answer (QA) benchmark specifically designed to evaluate the spatial\nunderstanding and reasoning capabilities of VLMs in autonomous driving. Built\nupon the NuScenes dataset, the benchmark is constructed through an automated 3D\nscene graph generation pipeline and a QA generation pipeline. The benchmark\nsystematically evaluates VLMs' performance in both spatial understanding and\nreasoning across multiple dimensions. Using this benchmark, we conduct\nextensive experiments on diverse VLMs, including both general and\nspatial-enhanced models, providing the first comprehensive evaluation of their\nspatial capabilities in autonomous driving. Surprisingly, the experimental\nresults show that the spatial-enhanced VLM outperforms in qualitative QA but\ndoes not demonstrate competitiveness in quantitative QA. In general, VLMs still\nface considerable challenges in spatial understanding and reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2504.03164v1",
      "published": "2025-04-04T04:43:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03164v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
      "authors": [
        "Yuxiang Zheng",
        "Dayuan Fu",
        "Xiangkun Hu",
        "Xiaojie Cai",
        "Lyumanshan Ye",
        "Pengrui Lu",
        "Pengfei Liu"
      ],
      "abstract": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.",
      "pdf_url": "http://arxiv.org/pdf/2504.03160v1",
      "published": "2025-04-04T04:41:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.03160v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    }
  ]
}