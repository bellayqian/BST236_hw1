{
  "last_updated": "2025-05-13T00:52:07.337140",
  "papers": [
    {
      "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails",
      "authors": [
        "Kwan-Yee Lin",
        "Stella X. Yu"
      ],
      "abstract": "Hiking on complex trails demands balance, agility, and adaptive\ndecision-making over unpredictable terrain. Current humanoid research remains\nfragmented and inadequate for hiking: locomotion focuses on motor skills\nwithout long-term goals or situational awareness, while semantic navigation\noverlooks real-world embodiment and local terrain variability. We propose\ntraining humanoids to hike on complex trails, driving integrative skill\ndevelopment across visual perception, decision making, and motor execution. We\ndevelop a learning framework, LEGO-H, that enables a vision-equipped humanoid\nrobot to hike complex trails autonomously. We introduce two technical\ninnovations: 1) A temporal vision transformer variant - tailored into\nHierarchical Reinforcement Learning framework - anticipates future local goals\nto guide movement, seamlessly integrating locomotion with goal-directed\nnavigation. 2) Latent representations of joint movement patterns, combined with\nhierarchical metric learning - enhance Privileged Learning scheme - enable\nsmooth policy transfer from privileged training to onboard execution. These\ncomponents allow LEGO-H to handle diverse physical and environmental challenges\nwithout relying on predefined motion patterns. Experiments across varied\nsimulated trails and robot morphologies highlight LEGO-H's versatility and\nrobustness, positioning hiking as a compelling testbed for embodied autonomy\nand LEGO-H as a baseline for future humanoid development.",
      "pdf_url": "http://arxiv.org/pdf/2505.06218v1",
      "published": "2025-05-09T17:53:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06218v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Neuro-Symbolic Concepts",
      "authors": [
        "Jiayuan Mao",
        "Joshua B. Tenenbaum",
        "Jiajun Wu"
      ],
      "abstract": "This article presents a concept-centric paradigm for building agents that can\nlearn continually and reason flexibly. The concept-centric agent utilizes a\nvocabulary of neuro-symbolic concepts. These concepts, such as object,\nrelation, and action concepts, are grounded on sensory inputs and actuation\noutputs. They are also compositional, allowing for the creation of novel\nconcepts through their structural combination. To facilitate learning and\nreasoning, the concepts are typed and represented using a combination of\nsymbolic programs and neural network representations. Leveraging such\nneuro-symbolic concepts, the agent can efficiently learn and recombine them to\nsolve various tasks across different domains, ranging from 2D images, videos,\n3D scenes, and robotic manipulation tasks. This concept-centric framework\noffers several advantages, including data efficiency, compositional\ngeneralization, continual learning, and zero-shot transfer.",
      "pdf_url": "http://arxiv.org/pdf/2505.06191v1",
      "published": "2025-05-09T17:02:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06191v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies",
      "authors": [
        "Massimiliano Pronesti",
        "Joao Bettencourt-Silva",
        "Paul Flanagan",
        "Alessandra Pascale",
        "Oisin Redmond",
        "Anya Belz",
        "Yufang Hou"
      ],
      "abstract": "Extracting scientific evidence from biomedical studies for clinical research\nquestions (e.g., Does stem cell transplantation improve quality of life in\npatients with medically refractory Crohn's disease compared to placebo?) is a\ncrucial step in synthesising biomedical evidence. In this paper, we focus on\nthe task of document-level scientific evidence extraction for clinical\nquestions with conflicting evidence. To support this task, we create a dataset\ncalled CochraneForest, leveraging forest plots from Cochrane systematic\nreviews. It comprises 202 annotated forest plots, associated clinical research\nquestions, full texts of studies, and study-specific conclusions. Building on\nCochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a\nretrieval-augmented generation framework designed to tackle the unique\nchallenges of evidence extraction. Our experiments show that URCA outperforms\nthe best existing methods by up to 10.3% in F1 score on this task. However, the\nresults also underscore the complexity of CochraneForest, establishing it as a\nchallenging testbed for advancing automated evidence synthesis systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.06186v1",
      "published": "2025-05-09T16:55:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06186v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Turbo-ICL: In-Context Learning-Based Turbo Equalization",
      "authors": [
        "Zihang Song",
        "Matteo Zecchin",
        "Bipin Rajendran",
        "Osvaldo Simeone"
      ],
      "abstract": "This paper introduces a novel in-context learning (ICL) framework, inspired\nby large language models (LLMs), for soft-input soft-output channel\nequalization in coded multiple-input multiple-output (MIMO) systems. The\nproposed approach learns to infer posterior symbol distributions directly from\na prompt of pilot signals and decoder feedback. A key innovation is the use of\nprompt augmentation to incorporate extrinsic information from the decoder\noutput as additional context, enabling the ICL model to refine its symbol\nestimates iteratively across turbo decoding iterations. Two model variants,\nbased on Transformer and state-space architectures, are developed and\nevaluated. Extensive simulations demonstrate that, when traditional linear\nassumptions break down, e.g., in the presence of low-resolution quantization,\nICL equalizers consistently outperform conventional model-based baselines, even\nwhen the latter are provided with perfect channel state information. Results\nalso highlight the advantage of Transformer-based models under limited training\ndiversity, as well as the efficiency of state-space models in\nresource-constrained scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2505.06175v1",
      "published": "2025-05-09T16:29:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06175v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks",
      "authors": [
        "Wenqi Zeng",
        "Yuqi Sun",
        "Chenxi Ma",
        "Weimin Tan",
        "Bo Yan"
      ],
      "abstract": "Medical vision-language models (VLMs) have shown promise as clinical\nassistants across various medical fields. However, specialized dermatology VLM\ncapable of delivering professional and detailed diagnostic analysis remains\nunderdeveloped, primarily due to less specialized text descriptions in current\ndermatology multimodal datasets. To address this issue, we propose MM-Skin, the\nfirst large-scale multimodal dermatology dataset that encompasses 3 imaging\nmodalities, including clinical, dermoscopic, and pathological and nearly 10k\nhigh-quality image-text pairs collected from professional textbooks. In\naddition, we generate over 27k diverse, instruction-following vision question\nanswering (VQA) samples (9 times the size of current largest dermatology VQA\ndataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a\ndermatology-specific VLM designed for precise and nuanced skin disease\ninterpretation. Comprehensive benchmark evaluations of SkinVL on VQA,\nsupervised fine-tuning (SFT) and zero-shot classification tasks across 8\ndatasets, reveal its exceptional performance for skin diseases in comparison to\nboth general and medical VLM models. The introduction of MM-Skin and SkinVL\noffers a meaningful contribution to advancing the development of clinical\ndermatology VLM assistants. MM-Skin is available at\nhttps://github.com/ZwQ803/MM-Skin",
      "pdf_url": "http://arxiv.org/pdf/2505.06152v1",
      "published": "2025-05-09T16:03:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06152v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets",
      "authors": [
        "Ryan Lagasse",
        "Aidan Kiernans",
        "Avijit Ghosh",
        "Shiri Dori-Hacohen"
      ],
      "abstract": "We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings.",
      "pdf_url": "http://arxiv.org/pdf/2505.06150v1",
      "published": "2025-05-09T16:02:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06150v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena",
      "authors": [
        "Philip Naumann",
        "Jacob Kauffmann",
        "Grégoire Montavon"
      ],
      "abstract": "Wasserstein distances provide a powerful framework for comparing data\ndistributions. They can be used to analyze processes over time or to detect\ninhomogeneities within data. However, simply calculating the Wasserstein\ndistance or analyzing the corresponding transport map (or coupling) may not be\nsufficient for understanding what factors contribute to a high or low\nWasserstein distance. In this work, we propose a novel solution based on\nExplainable AI that allows us to efficiently and accurately attribute\nWasserstein distances to various data components, including data subgroups,\ninput features, or interpretable subspaces. Our method achieves high accuracy\nacross diverse datasets and Wasserstein distance specifications, and its\npractical utility is demonstrated in two use cases.",
      "pdf_url": "http://arxiv.org/pdf/2505.06123v1",
      "published": "2025-05-09T15:26:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06123v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review",
      "authors": [
        "Jingguo Qu",
        "Xinyang Han",
        "Man-Lik Chui",
        "Yao Pu",
        "Simon Takadiyi Gunda",
        "Ziman Chen",
        "Jing Qin",
        "Ann Dorothy King",
        "Winnie Chiu-Wing Chu",
        "Jing Cai",
        "Michael Tin-Cheung Ying"
      ],
      "abstract": "Automatic lymph node segmentation is the cornerstone for advances in computer\nvision tasks for early detection and staging of cancer. Traditional\nsegmentation methods are constrained by manual delineation and variability in\noperator proficiency, limiting their ability to achieve high accuracy. The\nintroduction of deep learning technologies offers new possibilities for\nimproving the accuracy of lymph node image analysis. This study evaluates the\napplication of deep learning in lymph node segmentation and discusses the\nmethodologies of various deep learning architectures such as convolutional\nneural networks, encoder-decoder networks, and transformers in analyzing\nmedical imaging data across different modalities. Despite the advancements, it\nstill confronts challenges like the shape diversity of lymph nodes, the\nscarcity of accurately labeled datasets, and the inadequate development of\nmethods that are robust and generalizable across different imaging modalities.\nTo the best of our knowledge, this is the first study that provides a\ncomprehensive overview of the application of deep learning techniques in lymph\nnode segmentation task. Furthermore, this study also explores potential future\nresearch directions, including multimodal fusion techniques, transfer learning,\nand the use of large-scale pre-trained models to overcome current limitations\nwhile enhancing cancer diagnosis and treatment planning strategies.",
      "pdf_url": "http://arxiv.org/pdf/2505.06118v1",
      "published": "2025-05-09T15:17:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06118v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
      "authors": [
        "Qingwen Bu",
        "Yanting Yang",
        "Jisong Cai",
        "Shenyuan Gao",
        "Guanghui Ren",
        "Maoqing Yao",
        "Ping Luo",
        "Hongyang Li"
      ],
      "abstract": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
      "pdf_url": "http://arxiv.org/pdf/2505.06111v1",
      "published": "2025-05-09T15:11:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06111v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models",
      "authors": [
        "Jugal Gajjar",
        "Kaustik Ranaware"
      ],
      "abstract": "This project performs multimodal sentiment analysis using the CMU-MOSEI\ndataset, using transformer-based models with early fusion to integrate text,\naudio, and visual modalities. We employ BERT-based encoders for each modality,\nextracting embeddings that are concatenated before classification. The model\nachieves strong performance, with 97.87\\% 7-class accuracy and a 0.9682\nF1-score on the test set, demonstrating the effectiveness of early fusion in\ncapturing cross-modal interactions. The training utilized Adam optimization\n(lr=1e-4), dropout (0.3), and early stopping to ensure generalization and\nrobustness. Results highlight the superiority of transformer architectures in\nmodeling multimodal sentiment, with a low MAE (0.1060) indicating precise\nsentiment intensity prediction. Future work may compare fusion strategies or\nenhance interpretability. This approach utilizes multimodal learning by\neffectively combining linguistic, acoustic, and visual cues for sentiment\nanalysis.",
      "pdf_url": "http://arxiv.org/pdf/2505.06110v1",
      "published": "2025-05-09T15:10:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06110v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LLMs Outperform Experts on Challenging Biology Benchmarks",
      "authors": [
        "Lennart Justen"
      ],
      "abstract": "This study systematically evaluates 27 frontier Large Language Models on\neight diverse biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with the top model now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including LAB-Bench CloningScenarios and the biology\nsubsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance.",
      "pdf_url": "http://arxiv.org/pdf/2505.06108v1",
      "published": "2025-05-09T15:05:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06108v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ]
    },
    {
      "title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs",
      "authors": [
        "Sam Bush",
        "Matthew DeLorenzo",
        "Phat Tieu",
        "Jeyavijayan Rajendran"
      ],
      "abstract": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%.",
      "pdf_url": "http://arxiv.org/pdf/2505.06096v1",
      "published": "2025-05-09T14:44:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06096v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "UniSymNet: A Unified Symbolic Network Guided by Transformer",
      "authors": [
        "Xinxin Li",
        "Juan Zhang",
        "Da Li",
        "Xingyu Liu",
        "Jin Xu",
        "Junping Yin"
      ],
      "abstract": "Symbolic Regression (SR) is a powerful technique for automatically\ndiscovering mathematical expressions from input data. Mainstream SR algorithms\nsearch for the optimal symbolic tree in a vast function space, but the\nincreasing complexity of the tree structure limits their performance. Inspired\nby neural networks, symbolic networks have emerged as a promising new paradigm.\nHowever, most existing symbolic networks still face certain challenges: binary\nnonlinear operators $\\{\\times, \\div\\}$ cannot be naturally extended to\nmultivariate operators, and training with fixed architecture often leads to\nhigher complexity and overfitting. In this work, we propose a Unified Symbolic\nNetwork that unifies nonlinear binary operators into nested unary operators and\ndefine the conditions under which UniSymNet can reduce complexity. Moreover, we\npre-train a Transformer model with a novel label encoding method to guide\nstructural selection, and adopt objective-specific optimization strategies to\nlearn the parameters of the symbolic network. UniSymNet shows high fitting\naccuracy, excellent symbolic solution rate, and relatively low expression\ncomplexity, achieving competitive performance on low-dimensional Standard\nBenchmarks and high-dimensional SRBench.",
      "pdf_url": "http://arxiv.org/pdf/2505.06091v1",
      "published": "2025-05-09T14:38:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06091v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC"
      ]
    },
    {
      "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities",
      "authors": [
        "Hiari Pizzini Cavagna",
        "Daniele Cesarini",
        "Andrea Bartolini"
      ],
      "abstract": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.",
      "pdf_url": "http://arxiv.org/pdf/2505.06085v1",
      "published": "2025-05-09T14:29:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06085v1",
      "categories": [
        "cs.PF",
        "cs.AI",
        "cs.AR"
      ]
    },
    {
      "title": "Seqret: Mining Rule Sets from Event Sequences",
      "authors": [
        "Aleena Siji",
        "Joscha Cüppers",
        "Osman Ali Mian",
        "Jilles Vreeken"
      ],
      "abstract": "Summarizing event sequences is a key aspect of data mining. Most existing\nmethods neglect conditional dependencies and focus on discovering sequential\npatterns only. In this paper, we study the problem of discovering both\nconditional and unconditional dependencies from event sequence data. We do so\nby discovering rules of the form $X \\rightarrow Y$ where $X$ and $Y$ are\nsequential patterns. Rules like these are simple to understand and provide a\nclear description of the relation between the antecedent and the consequent. To\ndiscover succinct and non-redundant sets of rules we formalize the problem in\nterms of the Minimum Description Length principle. As the search space is\nenormous and does not exhibit helpful structure, we propose the Seqret method\nto discover high-quality rule sets in practice. Through extensive empirical\nevaluation we show that unlike the state of the art, Seqret ably recovers the\nground truth on synthetic datasets and finds useful rules from real datasets.",
      "pdf_url": "http://arxiv.org/pdf/2505.06049v1",
      "published": "2025-05-09T13:44:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06049v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "PYRREGULAR: A Unified Framework for Irregular Time Series, with Classification Benchmarks",
      "authors": [
        "Francesco Spinnato",
        "Cristiano Landi"
      ],
      "abstract": "Irregular temporal data, characterized by varying recording frequencies,\ndiffering observation durations, and missing values, presents significant\nchallenges across fields like mobility, healthcare, and environmental science.\nExisting research communities often overlook or address these challenges in\nisolation, leading to fragmented tools and methods. To bridge this gap, we\nintroduce a unified framework, and the first standardized dataset repository\nfor irregular time series classification, built on a common array format to\nenhance interoperability. This repository comprises 34 datasets on which we\nbenchmark 12 classifier models from diverse domains and communities. This work\naims to centralize research efforts and enable a more robust evaluation of\nirregular temporal data analysis methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.06047v1",
      "published": "2025-05-09T13:43:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06047v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects",
      "authors": [
        "Tobias Preintner",
        "Weixuan Yuan",
        "Qi Huang",
        "Adrian König",
        "Thomas Bäck",
        "Elena Raponi",
        "Niki van Stein"
      ],
      "abstract": "Combining natural language and geometric shapes is an emerging research area\nwith multiple applications in robotics and language-assisted design. A crucial\ntask in this domain is object referent identification, which involves selecting\na 3D object given a textual description of the target. Variability in language\ndescriptions and spatial relationships of 3D objects makes this a complex task,\nincreasing the need to better understand the behavior of neural network models\nin this domain. However, limited research has been conducted in this area.\nSpecifically, when a model makes an incorrect prediction despite being provided\nwith a seemingly correct object description, practitioners are left wondering:\n\"Why is the model wrong?\". In this work, we present a method answering this\nquestion by generating counterfactual examples. Our method takes a\nmisclassified sample, which includes two objects and a text description, and\ngenerates an alternative yet similar formulation that would have resulted in a\ncorrect prediction by the model. We have evaluated our approach with data from\nthe ShapeTalk dataset along with three distinct models. Our counterfactual\nexamples maintain the structure of the original description, are semantically\nsimilar and meaningful. They reveal weaknesses in the description, model bias\nand enhance the understanding of the models behavior. Theses insights help\npractitioners to better interact with systems as well as engineers to improve\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2505.06030v1",
      "published": "2025-05-09T13:24:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06030v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Universal Approximation Theorem for Deep Q-Learning via FBSDE System",
      "authors": [
        "Qian Qi"
      ],
      "abstract": "The approximation capabilities of Deep Q-Networks (DQNs) are commonly\njustified by general Universal Approximation Theorems (UATs) that do not\nleverage the intrinsic structural properties of the optimal Q-function, the\nsolution to a Bellman equation. This paper establishes a UAT for a class of\nDQNs whose architecture is designed to emulate the iterative refinement process\ninherent in Bellman updates. A central element of our analysis is the\npropagation of regularity: while the transformation induced by a single Bellman\noperator application exhibits regularity, for which Backward Stochastic\nDifferential Equations (BSDEs) theory provides analytical tools, the uniform\nregularity of the entire sequence of value iteration iterates--specifically,\ntheir uniform Lipschitz continuity on compact domains under standard Lipschitz\nassumptions on the problem data--is derived from finite-horizon dynamic\nprogramming principles. We demonstrate that layers of a deep residual network,\nconceived as neural operators acting on function spaces, can approximate the\naction of the Bellman operator. The resulting approximation theorem is thus\nintrinsically linked to the control problem's structure, offering a proof\ntechnique wherein network depth directly corresponds to iterations of value\nfunction refinement, accompanied by controlled error propagation. This\nperspective reveals a dynamic systems view of the network's operation on a\nspace of value functions.",
      "pdf_url": "http://arxiv.org/pdf/2505.06023v1",
      "published": "2025-05-09T13:11:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06023v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ]
    },
    {
      "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding",
      "authors": [
        "Shuai Wang",
        "Ivona Najdenkoska",
        "Hongyi Zhu",
        "Stevan Rudinac",
        "Monika Kackovic",
        "Nachoem Wijnberg",
        "Marcel Worring"
      ],
      "abstract": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations.",
      "pdf_url": "http://arxiv.org/pdf/2505.06020v1",
      "published": "2025-05-09T13:08:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.06020v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Minimal Sequent Calculus for Teaching First-Order Logic: Lessons Learned",
      "authors": [
        "Jørgen Villadsen"
      ],
      "abstract": "MiniCalc is a web app for teaching first-order logic based on a minimal\nsequent calculus. As an option the proofs can be verified in the Isabelle proof\nassistant. We present the lessons learned using the tool in recent years at our\nuniversity.",
      "pdf_url": "http://arxiv.org/pdf/2505.05988v1",
      "published": "2025-05-09T12:18:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05988v1",
      "categories": [
        "cs.LO",
        "cs.AI",
        "F.4; I.2.3; K.3.1"
      ]
    },
    {
      "title": "Pseudo-Boolean d-DNNF Compilation for Expressive Feature Modeling Constructs",
      "authors": [
        "Chico Sundermann",
        "Stefan Vill",
        "Elias Kuiter",
        "Sebastian Krieter",
        "Thomas Thüm",
        "Matthias Tichy"
      ],
      "abstract": "Configurable systems typically consist of reusable assets that have\ndependencies between each other. To specify such dependencies, feature models\nare commonly used. As feature models in practice are often complex, automated\nreasoning is typically employed to analyze the dependencies. Here, the de facto\nstandard is translating the feature model to conjunctive normal form (CNF) to\nenable employing off-the-shelf tools, such as SAT or #SAT solvers. However,\nmodern feature-modeling dialects often contain constructs, such as cardinality\nconstraints, that are ill-suited for conversion to CNF. This mismatch between\nthe input of reasoning engines and the available feature-modeling dialects\nlimits the applicability of the more expressive constructs. In this work, we\nshorten this gap between expressive constructs and scalable automated\nreasoning. Our contribution is twofold: First, we provide a pseudo-Boolean\nencoding for feature models, which facilitates smaller representations of\ncommonly employed constructs compared to Boolean encoding. Second, we propose a\nnovel method to compile pseudo-Boolean formulas to Boolean d-DNNF. With the\ncompiled d-DNNFs, we can resort to a plethora of efficient analyses already\nused in feature modeling. Our empirical evaluation shows that our proposal\nsubstantially outperforms the state-of-the-art based on CNF inputs for\nexpressive constructs. For every considered dataset representing different\nfeature models and feature-modeling constructs, the feature models can be\nsignificantly faster translated to pseudo-Boolean than to CNF. Overall,\nderiving d-DNNFs from a feature model with the targeted expressive constraints\ncan be substantially accelerated using our pseudo-Boolean approach.\nFurthermore, our approach is competitive on feature models with only basic\nconstructs.",
      "pdf_url": "http://arxiv.org/pdf/2505.05976v1",
      "published": "2025-05-09T12:00:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05976v1",
      "categories": [
        "cs.AI",
        "cs.LO",
        "cs.SE"
      ]
    },
    {
      "title": "A Noise-Resilient Semi-Supervised Graph Autoencoder for Overlapping Semantic Community Detection",
      "authors": [
        "Abdelfateh Bekkair",
        "Slimane Bellaouar",
        "Slimane Oulad-Naoui"
      ],
      "abstract": "Community detection in networks with overlapping structures remains a\nsignificant challenge, particularly in noisy real-world environments where\nintegrating topology, node attributes, and prior information is critical. To\naddress this, we propose a semi-supervised graph autoencoder that combines\ngraph multi-head attention and modularity maximization to robustly detect\noverlapping communities. The model learns semantic representations by fusing\nstructural, attribute, and prior knowledge while explicitly addressing noise in\nnode features. Key innovations include a noise-resistant architecture and a\nsemantic semi-supervised design optimized for community quality through\nmodularity constraints. Experiments demonstrate superior performance the model\noutperforms state-of-the-art methods in overlapping community detection\n(improvements in NMI and F1-score) and exhibits exceptional robustness to\nattribute noise, maintaining stable performance under 60\\% feature corruption.\nThese results highlight the importance of integrating attribute semantics and\nstructural patterns for accurate community discovery in complex networks.",
      "pdf_url": "http://arxiv.org/pdf/2505.05965v1",
      "published": "2025-05-09T11:34:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05965v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2",
      "authors": [
        "Vytenis Šliogeris",
        "Povilas Daniušis",
        "Artūras Nakvosas"
      ],
      "abstract": "This technical report describes an experiment on autoregressive pre-training\nof Gemma2 2 billion parameter large language model (LLM) with 10\\% on the\nLithuanian language component of CulturaX from the point of view of continual\nlearning. We apply elastic weight consolidation (EWC) to the full set of the\nmodel's parameters and investigate language understanding benchmarks,\nconsisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande\nsets (both in English and Lithuanian versions), and perplexity benchmarks. We\nempirically demonstrate that EWC regularisation allows us not only to mitigate\ncatastrophic forgetting effects but also that it is potentially beneficial for\nlearning of the new task with LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2505.05946v1",
      "published": "2025-05-09T10:43:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05946v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Achieving 3D Attention via Triplet Squeeze and Excitation Block",
      "authors": [
        "Maan Alhazmi",
        "Abdulrahman Altahhan"
      ],
      "abstract": "The emergence of ConvNeXt and its variants has reaffirmed the conceptual and\nstructural suitability of CNN-based models for vision tasks, re-establishing\nthem as key players in image classification in general, and in facial\nexpression recognition (FER) in particular. In this paper, we propose a new set\nof models that build on these advancements by incorporating a new set of\nattention mechanisms that combines Triplet attention with\nSqueeze-and-Excitation (TripSE) in four different variants. We demonstrate the\neffectiveness of these variants by applying them to the ResNet18, DenseNet and\nConvNext architectures to validate their versatility and impact. Our study\nshows that incorporating a TripSE block in these CNN models boosts their\nperformances, particularly for the ConvNeXt architecture, indicating its\nutility. We evaluate the proposed mechanisms and associated models across four\ndatasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where\nConvNext with TripSE achieves state-of-the-art results with an accuracy of\n\\textbf{78.27\\%} on the popular FER2013 dataset, a new feat for this dataset.",
      "pdf_url": "http://arxiv.org/pdf/2505.05943v1",
      "published": "2025-05-09T10:36:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05943v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "IRNN: Innovation-driven Recurrent Neural Network for Time-Series Data Modeling and Prediction",
      "authors": [
        "Yifan Zhou",
        "Yibo Wang",
        "Chao Shang"
      ],
      "abstract": "Many real-world datasets are time series that are sequentially collected and\ncontain rich temporal information. Thus, a common interest in practice is to\ncapture dynamics of time series and predict their future evolutions. To this\nend, the recurrent neural network (RNN) has been a prevalent and effective\nmachine learning option, which admits a nonlinear state-space model\nrepresentation. Motivated by the resemblance between RNN and Kalman filter (KF)\nfor linear state-space models, we propose in this paper Innovation-driven RNN\n(IRNN), a novel RNN architecture tailored to time-series data modeling and\nprediction tasks. By adapting the concept of \"innovation\" from KF to RNN, past\nprediction errors are adopted as additional input signals to update hidden\nstates of RNN and boost prediction performance. Since innovation data depend on\nnetwork parameters, existing training algorithms for RNN do not apply to IRNN\nstraightforwardly. Thus, a tailored training algorithm dubbed input\nupdating-based back-propagation through time (IU-BPTT) is further proposed,\nwhich alternates between updating innovations and optimizing network parameters\nvia gradient descent. Experiments on real-world benchmark datasets show that\nthe integration of innovations into various forms of RNN leads to remarkably\nimproved prediction accuracy of IRNN without increasing the training cost\nsubstantially.",
      "pdf_url": "http://arxiv.org/pdf/2505.05916v1",
      "published": "2025-05-09T09:43:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05916v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection",
      "authors": [
        "Hanzhe Liang",
        "Aoran Wang",
        "Jie Zhou",
        "Xin Jin",
        "Can Gao",
        "Jinbao Wang"
      ],
      "abstract": "In this paper, we go beyond identifying anomalies only in structural terms\nand think about better anomaly detection motivated by anomaly causes. Most\nanomalies are regarded as the result of unpredictable defective forces from\ninternal and external sources, and their opposite forces are sought to correct\nthe anomalies. We introduced a Mechanics Complementary framework for 3D anomaly\ndetection (MC4AD) to generate internal and external Corrective forces for each\npoint. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to\nsimulate various anomalies. Then, we present a Corrective Force Prediction\nNetwork (CFP-Net) with complementary representations for point-level\nrepresentation to simulate the different contributions of internal and external\ncorrective forces. A combined loss was proposed, including a new symmetric loss\nand an overall loss, to constrain the corrective forces properly. As a\nhighlight, we consider 3D anomaly detection in industry more comprehensively,\ncreating a hierarchical quality control strategy based on a three-way decision\nand contributing a dataset named Anomaly-IntraVariance with intraclass variance\nto evaluate the model. On the proposed and existing five datasets, we obtained\nnine state-of-the-art performers with the minimum parameters and the fastest\ninference speed. The source is available at\nhttps://github.com/hzzzzzhappy/MC4AD",
      "pdf_url": "http://arxiv.org/pdf/2505.05901v1",
      "published": "2025-05-09T09:09:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05901v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI",
      "authors": [
        "Benjamin Raphael Ernhofer",
        "Daniil Prokhorov",
        "Jannica Langner",
        "Dominik Bollmann"
      ],
      "abstract": "Modern automotive infotainment systems require intelligent and adaptive\nsolutions to handle frequent User Interface (UI) updates and diverse design\nvariations. We introduce a vision-language framework for understanding and\ninteracting with automotive infotainment systems, enabling seamless adaptation\nacross different UI designs. To further support research in this field, we\nrelease AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208\nannotations. Additionally, we present a synthetic data pipeline to generate\ntraining data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation\n(LoRa) and incorporating reasoning generated by our pipeline, along with visual\ngrounding and evaluation capabilities. The fine-tuned Evaluative Large Action\nModel (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and\ndataset are available on Hugging Face) and demonstrating strong cross-domain\ngeneralization, including a +5.2% improvement on ScreenSpot over the baseline\nmodel. Notably, our approach achieves 80.4% average accuracy on ScreenSpot,\nclosely matching or even surpassing specialized models for desktop, mobile, and\nweb, such as ShowUI, despite being trained for the infotainment domain. This\nresearch investigates how data collection and subsequent fine-tuning can lead\nto AI-driven progress within automotive UI understanding and interaction. The\napplied method is cost-efficient and fine-tuned models can be deployed on\nconsumer-grade GPUs.",
      "pdf_url": "http://arxiv.org/pdf/2505.05895v1",
      "published": "2025-05-09T09:01:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05895v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization",
      "authors": [
        "Seunghee Han",
        "Soongyu Choi",
        "Joo-Young Kim"
      ],
      "abstract": "Recent advances in Protein Structure Prediction Models (PPMs), such as\nAlphaFold2 and ESMFold, have revolutionized computational biology by achieving\nunprecedented accuracy in predicting three-dimensional protein folding\nstructures. However, these models face significant scalability challenges,\nparticularly when processing proteins with long amino acid sequences (e.g.,\nsequence length > 1,000). The primary bottleneck that arises from the\nexponential growth in activation sizes is driven by the unique data structure\nin PPM, which introduces an additional dimension that leads to substantial\nmemory and computational demands. These limitations have hindered the effective\nscaling of PPM for real-world applications, such as analyzing large proteins or\ncomplex multimers with critical biological and pharmaceutical relevance.\n  In this paper, we present LightNobel, the first hardware-software co-designed\naccelerator developed to overcome scalability limitations on the sequence\nlength in PPM. At the software level, we propose Token-wise Adaptive Activation\nQuantization (AAQ), which leverages unique token-wise characteristics, such as\ndistogram patterns in PPM activations, to enable fine-grained quantization\ntechniques without compromising accuracy. At the hardware level, LightNobel\nintegrates the multi-precision reconfigurable matrix processing unit (RMPU) and\nversatile vector processing unit (VVPU) to enable the efficient execution of\nAAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup\nand 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100\nGPUs, respectively, while maintaining negligible accuracy loss. It also reduces\nthe peak memory requirement up to 120.05x in PPM, enabling scalable processing\nfor proteins with long sequences.",
      "pdf_url": "http://arxiv.org/pdf/2505.05893v1",
      "published": "2025-05-09T09:01:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05893v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "B.7; I.2; J.3"
      ]
    },
    {
      "title": "Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams",
      "authors": [
        "Bettina Fazzinga",
        "Sergio Flesca",
        "Filippo Furfaro",
        "Luigi Pontieri",
        "Francesco Scala"
      ],
      "abstract": "Monitoring and analyzing process traces is a critical task for modern\ncompanies and organizations. In scenarios where there is a gap between trace\nevents and reference business activities, this entails an interpretation\nproblem, amounting to translating each event of any ongoing trace into the\ncorresponding step of the activity instance. Building on a recent approach that\nframes the interpretation problem as an acceptance problem within an Abstract\nArgumentation Framework (AAF), one can elegantly analyze plausible event\ninterpretations (possibly in an aggregated form), as well as offer explanations\nfor those that conflict with prior process knowledge. Since, in settings where\nevent-to-activity mapping is highly uncertain (or simply under-specified) this\nreasoning-based approach may yield lowly-informative results and heavy\ncomputation, one can think of discovering a sequencetagging model, trained to\nsuggest highly-probable candidate event interpretations in a context-aware way.\nHowever, training such a model optimally may require using a large amount of\nmanually-annotated example traces. Considering the urgent need of developing\nGreen AI solutions enabling environmental and societal sustainability (with\nreduced labor/computational costs and carbon footprint), we propose a\ndata/computation-efficient neuro-symbolic approach to the problem, where the\ncandidate interpretations returned by the example-driven sequence tagger is\nrefined by the AAF-based reasoner. This allows us to also leverage prior\nknowledge to compensate for the scarcity of example data, as confirmed by\nexperimental results; clearly, this property is particularly useful in settings\nwhere data annotation and model optimization costs are subject to stringent\nconstraints.",
      "pdf_url": "http://arxiv.org/pdf/2505.05880v1",
      "published": "2025-05-09T08:45:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05880v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Multi-Modal Molecular Representation Learning via Structure Awareness",
      "authors": [
        "Rong Yin",
        "Ruyue Liu",
        "Xiaoshuai Hao",
        "Xingrui Zhou",
        "Yong Liu",
        "Can Ma",
        "Weiping Wang"
      ],
      "abstract": "Accurate extraction of molecular representations is a critical step in the\ndrug discovery process. In recent years, significant progress has been made in\nmolecular representation learning methods, among which multi-modal molecular\nrepresentation methods based on images, and 2D/3D topologies have become\nincreasingly mainstream. However, existing these multi-modal approaches often\ndirectly fuse information from different modalities, overlooking the potential\nof intermodal interactions and failing to adequately capture the complex\nhigher-order relationships and invariant features between molecules. To\novercome these challenges, we propose a structure-awareness-based multi-modal\nself-supervised molecular representation pre-training framework (MMSA) designed\nto enhance molecular graph representations by leveraging invariant knowledge\nbetween molecules. The framework consists of two main modules: the multi-modal\nmolecular representation learning module and the structure-awareness module.\nThe multi-modal molecular representation learning module collaboratively\nprocesses information from different modalities of the same molecule to\novercome intermodal differences and generate a unified molecular embedding.\nSubsequently, the structure-awareness module enhances the molecular\nrepresentation by constructing a hypergraph structure to model higher-order\ncorrelations between molecules. This module also introduces a memory mechanism\nfor storing typical molecular representations, aligning them with memory\nanchors in the memory bank to integrate invariant knowledge, thereby improving\nthe model generalization ability. Extensive experiments have demonstrated the\neffectiveness of MMSA, which achieves state-of-the-art performance on the\nMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to\n9.6% over baseline methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.05877v1",
      "published": "2025-05-09T08:37:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05877v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Facial Image Compression with Consistency Preserving Diffusion Prior",
      "authors": [
        "Yimin Zhou",
        "Yichong Xia",
        "Bin Chen",
        "Baoyi An",
        "Haoqian Wang",
        "Zhi Wang",
        "Yaowei Wang",
        "Zikun Zhou"
      ],
      "abstract": "With the widespread application of facial image data across various domains,\nthe efficient storage and transmission of facial images has garnered\nsignificant attention. However, the existing learned face image compression\nmethods often produce unsatisfactory reconstructed image quality at low bit\nrates. Simply adapting diffusion-based compression methods to facial\ncompression tasks results in reconstructed images that perform poorly in\ndownstream applications due to insufficient preservation of high-frequency\ninformation. To further explore the diffusion prior in facial image\ncompression, we propose Facial Image Compression with a Stable Diffusion Prior\n(FaSDiff), a method that preserves consistency through frequency enhancement.\nFaSDiff employs a high-frequency-sensitive compressor in an end-to-end\nframework to capture fine image details and produce robust visual prompts.\nAdditionally, we introduce a hybrid low-frequency enhancement module that\ndisentangles low-frequency facial semantics and stably modulates the diffusion\nprior alongside visual prompts. The proposed modules allow FaSDiff to leverage\ndiffusion priors for superior human visual perception while minimizing\nperformance loss in machine vision due to semantic inconsistency. Extensive\nexperiments show that FaSDiff outperforms state-of-the-art methods in balancing\nhuman visual quality and machine vision accuracy. The code will be released\nafter the paper is accepted.",
      "pdf_url": "http://arxiv.org/pdf/2505.05870v1",
      "published": "2025-05-09T08:13:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05870v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "Generative Discovery of Partial Differential Equations by Learning from Math Handbooks",
      "authors": [
        "Hao Xu",
        "Yuntian Chen",
        "Rui Cao",
        "Tianning Tang",
        "Mengge Du",
        "Jian Li",
        "Adrian H. Callaghan",
        "Dongxiao Zhang"
      ],
      "abstract": "Data driven discovery of partial differential equations (PDEs) is a promising\napproach for uncovering the underlying laws governing complex systems. However,\npurely data driven techniques face the dilemma of balancing search space with\noptimization efficiency. This study introduces a knowledge guided approach that\nincorporates existing PDEs documented in a mathematical handbook to facilitate\nthe discovery process. These PDEs are encoded as sentence like structures\ncomposed of operators and basic terms, and used to train a generative model,\ncalled EqGPT, which enables the generation of free form PDEs. A loop of\ngeneration evaluation optimization is constructed to autonomously identify the\nmost suitable PDE. Experimental results demonstrate that this framework can\nrecover a variety of PDE forms with high accuracy and computational efficiency,\nparticularly in cases involving complex temporal derivatives or intricate\nspatial terms, which are often beyond the reach of conventional methods. The\napproach also exhibits generalizability to irregular spatial domains and higher\ndimensional settings. Notably, it succeeds in discovering a previously\nunreported PDE governing strongly nonlinear surface gravity waves propagating\ntoward breaking, based on real world experimental data, highlighting its\napplicability to practical scenarios and its potential to support scientific\ndiscovery.",
      "pdf_url": "http://arxiv.org/pdf/2505.05869v1",
      "published": "2025-05-09T08:09:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05869v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ]
    },
    {
      "title": "Evolutionary ecology of words",
      "authors": [
        "Reiji Suzuki",
        "Takaya Arita"
      ],
      "abstract": "We propose a model for the evolutionary ecology of words as one attempt to\nextend evolutionary game theory and agent-based models by utilizing the rich\nlinguistic expressions of Large Language Models (LLMs). Our model enables the\nemergence and evolution of diverse and infinite options for interactions among\nagents. Within the population, each agent possesses a short word (or phrase)\ngenerated by an LLM and moves within a spatial environment. When agents become\nadjacent, the outcome of their interaction is determined by the LLM based on\nthe relationship between their words, with the loser's word being replaced by\nthe winner's. Word mutations, also based on LLM outputs, may occur. We\nconducted preliminary experiments assuming that ``strong animal species\" would\nsurvive. The results showed that from an initial population consisting of\nwell-known species, many species emerged both gradually and in a punctuated\nequilibrium manner. Each trial demonstrated the unique evolution of diverse\npopulations, with one type of large species becoming dominant, such as\nterrestrial animals, marine life, or extinct species, which were ecologically\nspecialized and adapted ones across diverse extreme habitats. We also conducted\na long-term experiment with a large population, demonstrating the emergence and\ncoexistence of diverse species.",
      "pdf_url": "http://arxiv.org/pdf/2505.05863v1",
      "published": "2025-05-09T07:57:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05863v1",
      "categories": [
        "q-bio.PE",
        "cs.AI",
        "cs.CL",
        "92B20"
      ]
    },
    {
      "title": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents",
      "authors": [
        "Zhun Wang",
        "Vincent Siu",
        "Zhe Ye",
        "Tianneng Shi",
        "Yuzhou Nie",
        "Xuandong Zhao",
        "Chenguang Wang",
        "Wenbo Guo",
        "Dawn Song"
      ],
      "abstract": "The strong planning and reasoning capabilities of Large Language Models\n(LLMs) have fostered the development of agent-based systems capable of\nleveraging external tools and interacting with increasingly complex\nenvironments. However, these powerful features also introduce a critical\nsecurity risk: indirect prompt injection, a sophisticated attack vector that\ncompromises the core of these agents, the LLM, by manipulating contextual\ninformation rather than direct user prompts. In this work, we propose a generic\nblack-box fuzzing framework, AgentXploit, designed to automatically discover\nand exploit indirect prompt injection vulnerabilities across diverse LLM\nagents. Our approach starts by constructing a high-quality initial seed corpus,\nthen employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)\nto iteratively refine inputs, thereby maximizing the likelihood of uncovering\nagent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo\nand VWA-adv, where it achieves 71% and 70% success rates against agents based\non o3-mini and GPT-4o, respectively, nearly doubling the performance of\nbaseline attacks. Moreover, AgentXploit exhibits strong transferability across\nunseen tasks and internal LLMs, as well as promising results against defenses.\nBeyond benchmark evaluations, we apply our attacks in real-world environments,\nsuccessfully misleading agents to navigate to arbitrary URLs, including\nmalicious sites.",
      "pdf_url": "http://arxiv.org/pdf/2505.05849v1",
      "published": "2025-05-09T07:40:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05849v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design",
      "authors": [
        "Haojie Duanmu",
        "Xiuhong Li",
        "Zhihang Yuan",
        "Size Zheng",
        "Jiangfei Duan",
        "Xingcheng Zhang",
        "Dahua Lin"
      ],
      "abstract": "Mixture-of-Experts (MoE) models face deployment challenges due to their large\nparameter counts and computational demands. We explore quantization for MoE\nmodels and highlight two key insights: 1) linear blocks exhibit varying\nquantization sensitivity, and 2) divergent expert activation frequencies create\nheterogeneous computational characteristics. Based on these observations, we\nintroduce MxMoE, a mixed-precision optimization framework for MoE models that\nconsiders both algorithmic and system perspectives. MxMoE navigates the design\nspace defined by parameter sensitivity, expert activation dynamics, and\nhardware resources to derive efficient mixed-precision configurations.\nAdditionally, MxMoE automatically generates optimized mixed-precision GroupGEMM\nkernels, enabling parallel execution of GEMMs with different precisions.\nEvaluations show that MxMoE outperforms existing methods, achieving 2.4 lower\nWikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup\nover full precision, as well as up to 29.4% speedup over uniform quantization\nat equivalent accuracy with 5-bit weight-activation quantization. Our code is\navailable at https://github.com/cat538/MxMoE.",
      "pdf_url": "http://arxiv.org/pdf/2505.05799v1",
      "published": "2025-05-09T05:32:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05799v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency",
      "authors": [
        "Xinyu Liang",
        "Frits de Nijs",
        "Buser Say",
        "Hao Wang"
      ],
      "abstract": "Heating, Ventilation, and Air Conditioning (HVAC) systems account for\napproximately 38% of building energy consumption globally, making them one of\nthe most energy-intensive services. The increasing emphasis on energy\nefficiency and sustainability, combined with the need for enhanced occupant\ncomfort, presents a significant challenge for traditional HVAC systems. These\nsystems often fail to dynamically adjust to real-time changes in electricity\nmarket rates or individual comfort preferences, leading to increased energy\ncosts and reduced comfort. In response, we propose a Human-in-the-Loop (HITL)\nArtificial Intelligence framework that optimizes HVAC performance by\nincorporating real-time user feedback and responding to fluctuating electricity\nprices. Unlike conventional systems that require predefined information about\noccupancy or comfort levels, our approach learns and adapts based on ongoing\nuser input. By integrating the occupancy prediction model with reinforcement\nlearning, the system improves operational efficiency and reduces energy costs\nin line with electricity market dynamics, thereby contributing to demand\nresponse initiatives. Through simulations, we demonstrate that our method\nachieves significant cost reductions compared to baseline approaches while\nmaintaining or enhancing occupant comfort. This feedback-driven approach\nensures personalized comfort control without the need for predefined settings,\noffering a scalable solution that balances individual preferences with economic\nand environmental goals.",
      "pdf_url": "http://arxiv.org/pdf/2505.05796v1",
      "published": "2025-05-09T05:23:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05796v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY",
        "math.OC"
      ]
    },
    {
      "title": "What Is Next for LLMs? Next-Generation AI Computing Hardware Using Photonic Chips",
      "authors": [
        "Renjie Li",
        "Wenjie Wei",
        "Qi Xin",
        "Xiaoli Liu",
        "Sixuan Mao",
        "Erik Ma",
        "Zijian Chen",
        "Malu Zhang",
        "Haizhou Li",
        "Zhaoyu Zhang"
      ],
      "abstract": "Large language models (LLMs) are rapidly pushing the limits of contemporary\ncomputing hardware. For example, training GPT-3 has been estimated to consume\naround 1300 MWh of electricity, and projections suggest future models may\nrequire city-scale (gigawatt) power budgets. These demands motivate exploration\nof computing paradigms beyond conventional von Neumann architectures. This\nreview surveys emerging photonic hardware optimized for next-generation\ngenerative AI computing. We discuss integrated photonic neural network\narchitectures (e.g., Mach-Zehnder interferometer meshes, lasers,\nwavelength-multiplexed microring resonators) that perform ultrafast matrix\noperations. We also examine promising alternative neuromorphic devices,\nincluding spiking neural network circuits and hybrid spintronic-photonic\nsynapses, which combine memory and processing. The integration of\ntwo-dimensional materials (graphene, TMDCs) into silicon photonic platforms is\nreviewed for tunable modulators and on-chip synaptic elements.\nTransformer-based LLM architectures (self-attention and feed-forward layers)\nare analyzed in this context, identifying strategies and challenges for mapping\ndynamic matrix multiplications onto these novel hardware substrates. We then\ndissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and\nLLaMA, highlighting their architectural similarities and differences. We\nsynthesize state-of-the-art components, algorithms, and integration methods,\nhighlighting key advances and open issues in scaling such systems to mega-sized\nLLM models. We find that photonic computing systems could potentially surpass\nelectronic processors by orders of magnitude in throughput and energy\nefficiency, but require breakthroughs in memory, especially for long-context\nwindows and long token sequences, and in storage of ultra-large datasets.",
      "pdf_url": "http://arxiv.org/pdf/2505.05794v1",
      "published": "2025-05-09T05:19:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05794v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "FlowHFT: Flow Policy Induced Optimal High-Frequency Trading under Diverse Market Conditions",
      "authors": [
        "Yang Li",
        "Zhi Chen",
        "Steve Yang"
      ],
      "abstract": "High-frequency trading (HFT) is an investing strategy that continuously\nmonitors market states and places bid and ask orders at millisecond speeds.\nTraditional HFT approaches fit models with historical data and assume that\nfuture market states follow similar patterns. This limits the effectiveness of\nany single model to the specific conditions it was trained for. Additionally,\nthese models achieve optimal solutions only under specific market conditions,\nsuch as assumptions about stock price's stochastic process, stable order flow,\nand the absence of sudden volatility. Real-world markets, however, are dynamic,\ndiverse, and frequently volatile. To address these challenges, we propose the\nFlowHFT, a novel imitation learning framework based on flow matching policy.\nFlowHFT simultaneously learns strategies from numerous expert models, each\nproficient in particular market scenarios. As a result, our framework can\nadaptively adjust investment decisions according to the prevailing market\nstate. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism.\nThis allows it to refine strategies and achieve superior performance even in\ncomplex or extreme market scenarios where expert strategies may be suboptimal.\nWe test FlowHFT in multiple market environments. We first show that flow\nmatching policy is applicable in stochastic market environments, thus enabling\nFlowHFT to learn trading strategies under different market conditions. Notably,\nour single framework consistently achieves performance superior to the best\nexpert for each market condition.",
      "pdf_url": "http://arxiv.org/pdf/2505.05784v1",
      "published": "2025-05-09T04:58:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05784v1",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "cs.CE",
        "q-fin.CP"
      ]
    },
    {
      "title": "PyResBugs: A Dataset of Residual Python Bugs for Natural Language-Driven Fault Injection",
      "authors": [
        "Domenico Cotroneo",
        "Giuseppe De Rosa",
        "Pietro Liguori"
      ],
      "abstract": "This paper presents PyResBugs, a curated dataset of residual bugs, i.e.,\ndefects that persist undetected during traditional testing but later surface in\nproduction, collected from major Python frameworks. Each bug in the dataset is\npaired with its corresponding fault-free (fixed) version and annotated with\nmulti-level natural language (NL) descriptions. These NL descriptions enable\nnatural language-driven fault injection, offering a novel approach to\nsimulating real-world faults in software systems. By bridging the gap between\nsoftware fault injection techniques and real-world representativeness,\nPyResBugs provides researchers with a high-quality resource for advancing\nAI-driven automated testing in Python systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.05777v1",
      "published": "2025-05-09T04:39:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05777v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition",
      "authors": [
        "Weiyi Zhang",
        "Peranut Chotcomwongse",
        "Yinwen Li",
        "Pusheng Xu",
        "Ruijie Yao",
        "Lianhao Zhou",
        "Yuxuan Zhou",
        "Hui Feng",
        "Qiping Zhou",
        "Xinyue Wang",
        "Shoujin Huang",
        "Zihao Jin",
        "Florence H. T. Chung",
        "Shujun Wang",
        "Yalin Zheng",
        "Mingguang He",
        "Danli Shi",
        "Paisan Ruamviboonsuk"
      ],
      "abstract": "Diabetic macular edema (DME) significantly contributes to visual impairment\nin diabetic patients. Treatment responses to intravitreal therapies vary,\nhighlighting the need for patient stratification to predict therapeutic\nbenefits and enable personalized strategies. To our knowledge, this study is\nthe first to explore pre-treatment stratification for predicting DME treatment\nresponses. To advance this research, we organized the 2nd Asia-Pacific\nTele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The\ncompetition focused on improving predictive accuracy for anti-VEGF therapy\nresponses using ophthalmic OCT images. We provided a dataset containing tens of\nthousands of OCT images from 2,000 patients with labels across four sub-tasks.\nThis paper details the competition's structure, dataset, leading methods, and\nevaluation metrics. The competition attracted strong scientific community\nparticipation, with 170 teams initially registering and 41 reaching the final\nround. The top-performing team achieved an AUC of 80.06%, highlighting the\npotential of AI in personalized DME treatment and clinical decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2505.05768v1",
      "published": "2025-05-09T04:12:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05768v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Multi-Agent Systems for Robotic Autonomy with LLMs",
      "authors": [
        "Junhong Chen",
        "Ziqi Yang",
        "Haoyuan G Xu",
        "Dandan Zhang",
        "George Mylonas"
      ],
      "abstract": "Since the advent of Large Language Models (LLMs), various research based on\nsuch models have maintained significant academic attention and impact,\nespecially in AI and robotics. In this paper, we propose a multi-agent\nframework with LLMs to construct an integrated system for robotic task\nanalysis, mechanical design, and path generation. The framework includes three\ncore agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer.\nOutputs are formatted as multimodal results, such as code files or technical\nreports, for stronger understandability and usability. To evaluate\ngeneralizability comparatively, we conducted experiments with models from both\nGPT and DeepSeek. Results demonstrate that the proposed system can design\nfeasible robots with control strategies when appropriate task inputs are\nprovided, exhibiting substantial potential for enhancing the efficiency and\naccessibility of robotic system development in research and industrial\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2505.05762v1",
      "published": "2025-05-09T03:52:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05762v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning",
      "authors": [
        "Azim Ospanov",
        "Roozbeh Yousefzadeh"
      ],
      "abstract": "Formal reasoning and automated theorem proving constitute a challenging\nsubfield of machine learning, in which machines are tasked with proving\nmathematical theorems using formal languages like Lean. A formal verification\nsystem can check whether a formal proof is correct or not almost\ninstantaneously, but generating a completely correct formal proof with large\nlanguage models (LLMs) remains a formidable task. The usual approach in the\nliterature is to prompt the LLM many times (up to several thousands) until one\nof the generated proofs passes the verification system. In this work, we\npresent APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a\nmodular, model-agnostic pipeline that combines the strengths of the Lean\ncompiler with an LLM's reasoning abilities to achieve better proof-generation\nresults at a low sampling budget. Apollo directs a fully automated process in\nwhich the LLM generates proofs for theorems, a set of agents analyze the\nproofs, fix the syntax errors, identify the mistakes in the proofs using Lean,\nisolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on\neach remaining goal with a low top-K budget. The repaired sub-proofs are\nrecombined and reverified, iterating up to a user-controlled maximum number of\nattempts. On the miniF2F benchmark, we establish a new state-of-the-art\naccuracy of 75.0% among 7B-parameter models while keeping the sampling budget\nbelow one thousand. Moreover, Apollo raises the state-of-the-art accuracy for\nGoedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few\nhundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%\naccuracy. Our results demonstrate that targeted, compiler-guided repair of LLM\noutputs yields dramatic gains in both efficiency and correctness, suggesting a\ngeneral paradigm for scalable automated theorem proving.",
      "pdf_url": "http://arxiv.org/pdf/2505.05758v1",
      "published": "2025-05-09T03:38:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05758v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "Evolutionary thoughts: integration of large language models and evolutionary algorithms",
      "authors": [
        "Antonio Jimeno Yepes",
        "Pieter Barnard"
      ],
      "abstract": "Large Language Models (LLMs) have unveiled remarkable capabilities in\nunderstanding and generating both natural language and code, but LLM reasoning\nis prone to hallucination and struggle with complex, novel scenarios, often\ngetting stuck on partial or incorrect solutions. However, the inherent ability\nof Evolutionary Algorithms (EAs) to explore extensive and complex search spaces\nmakes them particularly effective in scenarios where traditional optimization\nmethodologies may falter. However, EAs explore a vast search space when applied\nto complex problems.\n  To address the computational bottleneck of evaluating large populations,\nparticularly crucial for complex evolutionary tasks, we introduce a highly\nefficient evaluation framework. This implementation maintains compatibility\nwith existing primitive definitions, ensuring the generation of valid\nindividuals.\n  Using LLMs, we propose an enhanced evolutionary search strategy that enables\na more focused exploration of expansive solution spaces. LLMs facilitate the\ngeneration of superior candidate solutions, as evidenced by empirical results\ndemonstrating their efficacy in producing improved outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2505.05756v1",
      "published": "2025-05-09T03:32:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05756v1",
      "categories": [
        "cs.NE",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Embodiment Scaling Laws in Robot Locomotion",
      "authors": [
        "Bo Ai",
        "Liu Dai",
        "Nico Bohlinger",
        "Dichen Li",
        "Tongzhou Mu",
        "Zhanxin Wu",
        "K. Fay",
        "Henrik I. Christensen",
        "Jan Peters",
        "Hao Su"
      ],
      "abstract": "Developing generalist agents that can operate across diverse tasks,\nenvironments, and physical embodiments is a grand challenge in robotics and\nartificial intelligence. In this work, we focus on the axis of embodiment and\ninvestigate embodiment scaling laws$\\unicode{x2013}$the hypothesis that\nincreasing the number of training embodiments improves generalization to unseen\nones. Using robot locomotion as a test bed, we procedurally generate a dataset\nof $\\sim$1,000 varied embodiments, spanning humanoids, quadrupeds, and\nhexapods, and train generalist policies capable of handling diverse observation\nand action spaces on random subsets. We find that increasing the number of\ntraining embodiments improves generalization to unseen ones, and scaling\nembodiments is more effective in enabling embodiment-level generalization than\nscaling data on small, fixed sets of embodiments. Notably, our best policy,\ntrained on the full dataset, zero-shot transfers to novel embodiments in the\nreal world, such as Unitree Go2 and H1. These results represent a step toward\ngeneral embodied intelligence, with potential relevance to adaptive control for\nconfigurable robots, co-design of morphology and control, and beyond.",
      "pdf_url": "http://arxiv.org/pdf/2505.05753v1",
      "published": "2025-05-09T03:25:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05753v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "authors": [
        "Yiming Niu",
        "Jinliang Deng",
        "Lulu Zhang",
        "Zimu Zhou",
        "Yongxin Tong"
      ],
      "abstract": "Accurate and efficient multivariate time series (MTS) forecasting is\nessential for applications such as traffic management and weather prediction,\nwhich depend on capturing long-range temporal dependencies and interactions\nbetween entities. Existing methods, particularly those based on Transformer\narchitectures, compute pairwise dependencies across all time steps, leading to\na computational complexity that scales quadratically with the length of the\ninput. To overcome these challenges, we introduce the Forecaster with Offline\nClustering Using Segments (FOCUS), a novel approach to MTS forecasting that\nsimplifies long-range dependency modeling through the use of prototypes\nextracted via offline clustering. These prototypes encapsulate high-level\nevents in the real-world system underlying the data, summarizing the key\ncharacteristics of similar time segments. In the online phase, FOCUS\ndynamically adapts these patterns to the current input and captures\ndependencies between the input segment and high-level events, enabling both\naccurate and efficient forecasting. By identifying prototypes during the\noffline clustering phase, FOCUS reduces the computational complexity of\nmodeling long-range dependencies in the online phase to linear scaling.\nExtensive experiments across diverse benchmarks demonstrate that FOCUS achieves\nstate-of-the-art accuracy while significantly reducing computational costs.",
      "pdf_url": "http://arxiv.org/pdf/2505.05738v1",
      "published": "2025-05-09T02:34:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05738v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder",
      "authors": [
        "Wooyoung Jeong",
        "Hyun Jae Park",
        "Seonghun Jeong",
        "Jong Wook Jang",
        "Tae Hoon Lim",
        "Dae Seoung Kim"
      ],
      "abstract": "Hyperspectral imagery provides rich spectral detail but poses unique\nchallenges because of its high dimensionality in both spatial and spectral\ndomains. We propose \\textit{HyperspectralMAE}, a Transformer-based foundation\nmodel for hyperspectral data that employs a \\textit{dual masking} strategy:\nduring pre-training we randomly occlude 50\\% of spatial patches and 50\\% of\nspectral bands. This forces the model to learn representations capable of\nreconstructing missing information across both dimensions. To encode spectral\norder, we introduce learnable harmonic Fourier positional embeddings based on\nwavelength. The reconstruction objective combines mean-squared error (MSE) with\nthe spectral angle mapper (SAM) to balance pixel-level accuracy and\nspectral-shape fidelity.\n  The resulting model contains about $1.8\\times10^{8}$ parameters and produces\n768-dimensional embeddings, giving it sufficient capacity for transfer\nlearning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --\nNASA EO-1 Hyperion ($\\sim$1\\,600 scenes, $\\sim$$3\\times10^{11}$ pixel spectra)\nand DLR EnMAP Level-0 ($\\sim$1\\,300 scenes, $\\sim$$3\\times10^{11}$ pixel\nspectra) -- and fine-tuned it for land-cover classification on the Indian Pines\nbenchmark. HyperspectralMAE achieves state-of-the-art transfer-learning\naccuracy on Indian Pines, confirming that masked dual-dimensional pre-training\nyields robust spectral-spatial representations. These results demonstrate that\ndual masking and wavelength-aware embeddings advance hyperspectral image\nreconstruction and downstream analysis.",
      "pdf_url": "http://arxiv.org/pdf/2505.05710v1",
      "published": "2025-05-09T01:16:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05710v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models",
      "authors": [
        "Julia Shuieh",
        "Prasann Singhal",
        "Apaar Shanker",
        "John Heyer",
        "George Pu",
        "Samuel Denton"
      ],
      "abstract": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations.",
      "pdf_url": "http://arxiv.org/pdf/2505.05704v1",
      "published": "2025-05-09T00:39:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05704v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning",
      "authors": [
        "Jongchan Park",
        "Mingyu Park",
        "Donghwan Lee"
      ],
      "abstract": "Offline reinforcement learning (RL) aims to learn a policy from a static\ndataset without further interactions with the environment. Collecting\nsufficiently large datasets for offline RL is exhausting since this data\ncollection requires colossus interactions with environments and becomes tricky\nwhen the interaction with the environment is restricted. Hence, how an agent\nlearns the best policy with a minimal static dataset is a crucial issue in\noffline RL, similar to the sample efficiency problem in online RL. In this\npaper, we propose a simple yet effective plug-and-play pretraining method to\ninitialize a feature of a $Q$-network to enhance data efficiency in offline RL.\nSpecifically, we introduce a shared $Q$-network structure that outputs\npredictions of the next state and $Q$-value. We pretrain the shared $Q$-network\nthrough a supervised regression task that predicts a next state and trains the\nshared $Q$-network using diverse offline RL methods. Through extensive\nexperiments, we empirically demonstrate that our method enhances the\nperformance of existing popular offline RL methods on the D4RL, Robomimic and\nV-D4RL benchmarks. Furthermore, we show that our method significantly boosts\ndata-efficient offline RL across various data qualities and data distributions\ntrough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of\nthe dataset outperforms standard algorithms even with full datasets.",
      "pdf_url": "http://arxiv.org/pdf/2505.05701v1",
      "published": "2025-05-09T00:26:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05701v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Prompted Meta-Learning for Few-shot Knowledge Graph Completion",
      "authors": [
        "Han Wu",
        "Jie Yin"
      ],
      "abstract": "Few-shot knowledge graph completion (KGC) has obtained significant attention\ndue to its practical applications in real-world scenarios, where new knowledge\noften emerges with limited available data. While most existing methods for\nfew-shot KGC have predominantly focused on leveraging relational information,\nrich semantics inherent in KGs have been largely overlooked. To address this\ngap, we propose a novel prompted meta-learning (PromptMeta) framework that\nseamlessly integrates meta-semantics with relational information for few-shot\nKGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that\ncaptures and consolidates high-level meta-semantics, enabling effective\nknowledge transfer and adaptation to rare and newly emerging relations. (2) a\nlearnable fusion prompt that dynamically combines meta-semantic information\nwith task-specific relational information tailored to different few-shot tasks.\nBoth components are optimized together with model parameters within a\nmeta-learning framework. Extensive experiments on two benchmark datasets\ndemonstrate the effectiveness of our approach.",
      "pdf_url": "http://arxiv.org/pdf/2505.05684v1",
      "published": "2025-05-08T22:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05684v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Interactive Diabetes Risk Prediction Using Explainable Machine Learning: A Dash-Based Approach with SHAP, LIME, and Comorbidity Insights",
      "authors": [
        "Udaya Allani"
      ],
      "abstract": "This study presents a web-based interactive health risk prediction tool\ndesigned to assess diabetes risk using machine learning models. Built on the\n2015 CDC BRFSS dataset, the study evaluates models including Logistic\nRegression, Random Forest, XGBoost, LightGBM, KNN, and Neural Networks under\noriginal, SMOTE, and undersampling strategies. LightGBM with undersampling\nachieved the best recall, making it ideal for risk detection. The tool\nintegrates SHAP and LIME to explain predictions and highlights comorbidity\ncorrelations using Pearson analysis. A Dash-based UI enables user-friendly\ninteraction with model predictions, personalized suggestions, and feature\ninsights, supporting data-driven health awareness.",
      "pdf_url": "http://arxiv.org/pdf/2505.05683v1",
      "published": "2025-05-08T22:57:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.05683v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.1; I.5.2; J.3"
      ]
    }
  ]
}