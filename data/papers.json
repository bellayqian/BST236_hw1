{
  "last_updated": "2025-07-03T00:54:01.261292",
  "papers": [
    {
      "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation",
      "authors": [
        "Jiacheng Cui",
        "Xinyue Bi",
        "Yaxin Luo",
        "Xiaohan Zhao",
        "Jiacheng Liu",
        "Zhiqiang Shen"
      ],
      "abstract": "Residual connection has been extensively studied and widely applied at the\nmodel architecture level. However, its potential in the more challenging\ndata-centric approaches remains unexplored. In this work, we introduce the\nconcept of Data Residual Matching for the first time, leveraging data-level\nskip connections to facilitate data generation and mitigate data information\nvanishing. This approach maintains a balance between newly acquired knowledge\nthrough pixel space optimization and existing core local information\nidentification within raw data modalities, specifically for the dataset\ndistillation task. Furthermore, by incorporating optimization-level\nrefinements, our method significantly improves computational efficiency,\nachieving superior performance while reducing training time and peak GPU memory\nusage by 50%. Consequently, the proposed method Fast and Accurate Data Residual\nMatching for Dataset Distillation (FADRM) establishes a new state-of-the-art,\ndemonstrating substantial improvements over existing methods across multiple\ndataset benchmarks in both efficiency and effectiveness. For instance, with\nResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the\nmethod achieves 47.7% test accuracy in single-model dataset distillation and\n50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and\noutperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%\nand +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.",
      "pdf_url": "http://arxiv.org/pdf/2506.24125v1",
      "published": "2025-06-30T17:59:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24125v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime",
      "authors": [
        "Yuqing Wang",
        "Shangding Gu"
      ],
      "abstract": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.",
      "pdf_url": "http://arxiv.org/pdf/2506.24120v1",
      "published": "2025-06-30T17:58:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24120v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
      "authors": [
        "Bo Liu",
        "Leon Guertler",
        "Simon Yu",
        "Zichen Liu",
        "Penghui Qi",
        "Daniel Balcells",
        "Mickel Liu",
        "Cheston Tan",
        "Weiyan Shi",
        "Min Lin",
        "Wee Sun Lee",
        "Natasha Jaques"
      ],
      "abstract": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
      "pdf_url": "http://arxiv.org/pdf/2506.24119v2",
      "published": "2025-06-30T17:58:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24119v2",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Navigating with Annealing Guidance Scale in Diffusion Space",
      "authors": [
        "Shai Yehezkel",
        "Omer Dahary",
        "Andrey Voynov",
        "Daniel Cohen-Or"
      ],
      "abstract": "Denoising diffusion models excel at generating high-quality images\nconditioned on text prompts, yet their effectiveness heavily relies on careful\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice of the\nguidance scale has a critical impact on the convergence toward a visually\nappealing and prompt-adherent image. In this work, we propose an annealing\nguidance scheduler which dynamically adjusts the guidance scale over time based\non the conditional noisy signal. By learning a scheduling policy, our method\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\nour guidance scheduler significantly enhances image quality and alignment with\nthe text prompt, advancing the performance of text-to-image generation.\nNotably, our novel scheduler requires no additional activations or memory\nconsumption, and can seamlessly replace the common classifier-free guidance,\noffering an improved trade-off between prompt alignment and quality.",
      "pdf_url": "http://arxiv.org/pdf/2506.24108v1",
      "published": "2025-06-30T17:55:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24108v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "On the Predictive Power of Representation Dispersion in Language Models",
      "authors": [
        "Yanhong Li",
        "Ming Li",
        "Karen Livescu",
        "Jiawei Zhou"
      ],
      "abstract": "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach.",
      "pdf_url": "http://arxiv.org/pdf/2506.24106v1",
      "published": "2025-06-30T17:53:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24106v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies",
      "authors": [
        "Paul Wachter",
        "Lukas Niehaus",
        "Julius Schöning"
      ],
      "abstract": "Synthetic data has emerged as a cost-effective alternative to real data for\ntraining artificial neural networks (ANN). However, the disparity between\nsynthetic and real data results in a domain gap. That gap leads to poor\nperformance and generalization of the trained ANN when applied to real-world\nscenarios. Several strategies have been developed to bridge this gap, which\ncombine synthetic and real data, known as mixed training using hybrid datasets.\nWhile these strategies have been shown to mitigate the domain gap, a systematic\nevaluation of their generalizability and robustness across various tasks and\narchitectures remains underexplored. To address this challenge, our study\ncomprehensively analyzes two widely used mixing strategies on three prevalent\narchitectures and three distinct hybrid datasets. From these datasets, we\nsample subsets with varying proportions of synthetic to real data to\ninvestigate the impact of synthetic and real components. The findings of this\npaper provide valuable insights into optimizing the use of synthetic data in\nthe training process of any ANN, contributing to enhancing robustness and\nefficacy.",
      "pdf_url": "http://arxiv.org/pdf/2506.24093v1",
      "published": "2025-06-30T17:48:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24093v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.1; I.2.0; F.2.3"
      ]
    },
    {
      "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention",
      "authors": [
        "Wonwoong Cho",
        "Yanxia Zhang",
        "Yan-Ying Chen",
        "David I. Inouye"
      ],
      "abstract": "Blending visual and textual concepts into a new visual concept is a unique\nand powerful trait of human beings that can fuel creativity. However, in\npractice, cross-modal conceptual blending for humans is prone to cognitive\nbiases, like design fixation, which leads to local minima in the design space.\nIn this paper, we propose a T2I diffusion adapter \"IT-Blender\" that can\nautomate the blending process to enhance human creativity. Prior works related\nto cross-modal conceptual blending are limited in encoding a real image without\nloss of details or in disentangling the image and text inputs. To address these\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\nthe latent representations of a clean reference image with those of the noisy\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\nthe real reference image without loss of details and blends the visual concept\nwith the object specified by the text in a disentangled way. Our experiment\nresults show that IT-Blender outperforms the baselines by a large margin in\nblending visual and textual concepts, shedding light on the new application of\nimage generative models to augment human creativity.",
      "pdf_url": "http://arxiv.org/pdf/2506.24085v1",
      "published": "2025-06-30T17:41:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24085v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks",
      "authors": [
        "Rahul Kumar",
        "Wenqi Wei",
        "Ying Mao",
        "Junaid Farooq",
        "Ying Wang",
        "Juntao Chen"
      ],
      "abstract": "We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to\nsabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.\nSQUASH is executed by inserting SWAP gate(s) into the variational quantum\ncircuit of the victim HQNN. Unlike conventional noise-based or adversarial\ninput attacks, SQUASH directly manipulates the circuit structure, leading to\nqubit misalignment and disrupting quantum state evolution. This attack is\nhighly stealthy, as it does not require access to training data or introduce\ndetectable perturbations in input states. Our results demonstrate that SQUASH\nsignificantly degrades classification performance, with untargeted SWAP attacks\nreducing accuracy by up to 74.08\\% and targeted SWAP attacks reducing target\nclass accuracy by up to 79.78\\%. These findings reveal a critical vulnerability\nin HQNN implementations, underscoring the need for more resilient architectures\nagainst circuit-level adversarial interventions.",
      "pdf_url": "http://arxiv.org/pdf/2506.24081v1",
      "published": "2025-06-30T17:36:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24081v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines",
      "authors": [
        "Ian R. McKenzie",
        "Oskar J. Hollinsworth",
        "Tom Tseng",
        "Xander Davies",
        "Stephen Casper",
        "Aaron D. Tucker",
        "Robert Kirk",
        "Adam Gleave"
      ],
      "abstract": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.",
      "pdf_url": "http://arxiv.org/pdf/2506.24068v1",
      "published": "2025-06-30T17:21:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24068v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
      "authors": [
        "Sicong Jiang",
        "Zilin Huang",
        "Kangan Qian",
        "Ziang Luo",
        "Tianze Zhu",
        "Yang Zhong",
        "Yihong Tang",
        "Menglin Kong",
        "Yunlong Wang",
        "Siwen Jiao",
        "Hao Ye",
        "Zihao Sheng",
        "Xin Zhao",
        "Tuopu Wen",
        "Zheng Fu",
        "Sikai Chen",
        "Kun Jiang",
        "Diange Yang",
        "Seongjin Choi",
        "Lijun Sun"
      ],
      "abstract": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\n\\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.",
      "pdf_url": "http://arxiv.org/pdf/2506.24044v1",
      "published": "2025-06-30T16:50:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24044v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Constructing Non-Markovian Decision Process via History Aggregator",
      "authors": [
        "Yongyi Wang",
        "Wenxin Li"
      ],
      "abstract": "In the domain of algorithmic decision-making, non-Markovian dynamics manifest\nas a significant impediment, especially for paradigms such as Reinforcement\nLearning (RL), thereby exerting far-reaching consequences on the advancement\nand effectiveness of the associated systems. Nevertheless, the existing\nbenchmarks are deficient in comprehensively assessing the capacity of decision\nalgorithms to handle non-Markovian dynamics. To address this deficiency, we\nhave devised a generalized methodology grounded in category theory. Notably, we\nestablished the category of Markov Decision Processes (MDP) and the category of\nnon-Markovian Decision Processes (NMDP), and proved the equivalence\nrelationship between them. This theoretical foundation provides a novel\nperspective for understanding and addressing non-Markovian dynamics. We further\nintroduced non-Markovianity into decision-making problem settings via the\nHistory Aggregator for State (HAS). With HAS, we can precisely control the\nstate dependency structure of decision-making problems in the time series. Our\nanalysis demonstrates the effectiveness of our method in representing a broad\nrange of non-Markovian dynamics. This approach facilitates a more rigorous and\nflexible evaluation of decision algorithms by testing them in problem settings\nwhere non-Markovian dynamics are explicitly constructed.",
      "pdf_url": "http://arxiv.org/pdf/2506.24026v1",
      "published": "2025-06-30T16:32:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24026v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks",
      "authors": [
        "Veronica Lachi",
        "Francesco Ferrini",
        "Antonio Longa",
        "Bruno Lepri",
        "Andrea Passerini",
        "Manfred Jaeger"
      ],
      "abstract": "Graph Neural Networks (GNNs) are widely used to compute representations of\nnode pairs for downstream tasks such as link prediction. Yet, theoretical\nunderstanding of their expressive power has focused almost entirely on\ngraph-level representations. In this work, we shift the focus to links and\nprovide the first comprehensive study of GNN expressiveness in link\nrepresentation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$\nframework, that subsumes existing message-passing link models and enables\nformal expressiveness comparisons. Using this framework, we derive a hierarchy\nof state-of-the-art methods and offer theoretical tools to analyze future\narchitectures. To complement our analysis, we propose a synthetic evaluation\nprotocol comprising the first benchmark specifically designed to assess\nlink-level expressiveness. Finally, we ask: does expressiveness matter in\npractice? We use a graph symmetry metric that quantifies the difficulty of\ndistinguishing links and show that while expressive models may underperform on\nstandard benchmarks, they significantly outperform simpler ones as symmetry\nincreases, highlighting the need for dataset-aware model selection.",
      "pdf_url": "http://arxiv.org/pdf/2506.24018v1",
      "published": "2025-06-30T16:22:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24018v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations",
      "authors": [
        "Hyunjong Kim",
        "Sangyeop Kim",
        "Jongheon Jeong",
        "Yeongjae Cho",
        "Sungzoon Cho"
      ],
      "abstract": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.",
      "pdf_url": "http://arxiv.org/pdf/2506.24016v1",
      "published": "2025-06-30T16:20:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24016v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems",
      "authors": [
        "Xinquan Wang",
        "Fenghao Zhu",
        "Zhaohui Yang",
        "Chongwen Huang",
        "Xiaoming Chen",
        "Zhaoyang Zhang",
        "Sami Muhaidat",
        "Mérouane Debbah"
      ],
      "abstract": "Large artificial intelligence (AI) models offer revolutionary potential for\nfuture wireless systems, promising unprecedented capabilities in network\noptimization and performance. However, current paradigms largely overlook\ncrucial physical interactions. This oversight means they primarily rely on\noffline datasets, leading to difficulties in handling real-time wireless\ndynamics and non-stationary environments. Furthermore, these models often lack\nthe capability for active environmental probing. This paper proposes a\nfundamental paradigm shift towards wireless embodied large AI (WELAI), moving\nfrom passive observation to active embodiment. We first identify key challenges\nfaced by existing models, then we explore the design principles and system\nstructure of WELAI. Besides, we outline prospective applications in\nnext-generation wireless. Finally, through an illustrative case study, we\ndemonstrate the effectiveness of WELAI and point out promising research\ndirections for realizing adaptive, robust, and autonomous wireless systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.24009v1",
      "published": "2025-06-30T16:13:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.24009v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "math.IT"
      ]
    },
    {
      "title": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems",
      "authors": [
        "Mingfei Cheng",
        "Renzhi Wang",
        "Xiaofei Xie",
        "Yuan Zhou",
        "Lei Ma"
      ],
      "abstract": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline.",
      "pdf_url": "http://arxiv.org/pdf/2506.23995v1",
      "published": "2025-06-30T15:58:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23995v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Harnessing AI Agents to Advance Research on Refugee Child Mental Health",
      "authors": [
        "Aditya Shrivastava",
        "Komal Gupta",
        "Shraddha Arora"
      ],
      "abstract": "The international refugee crisis deepens, exposing millions of dis placed\nchildren to extreme psychological trauma. This research suggests a com pact,\nAI-based framework for processing unstructured refugee health data and\ndistilling knowledge on child mental health. We compare two Retrieval-Aug\nmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to\ndetermine how well they process challenging humanitarian datasets while avoid\ning hallucination hazards. By combining cutting-edge AI methods with migration\nresearch and child psychology, this study presents a scalable strategy to\nassist policymakers, mental health practitioners, and humanitarian agencies to\nbetter assist displaced children and recognize their mental wellbeing. In\ntotal, both the models worked properly but significantly Deepseek R1 is\nsuperior to Zephyr with an accuracy of answer relevance 0.91",
      "pdf_url": "http://arxiv.org/pdf/2506.23992v1",
      "published": "2025-06-30T15:55:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23992v1",
      "categories": [
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning",
      "authors": [
        "Mingfei Cheng",
        "Xiaofei Xie",
        "Renzhi Wang",
        "Yuan Zhou",
        "Ming Hu"
      ],
      "abstract": "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\nto the inherent limitations in their design and performance capabilities.\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\nruntime safety and reliability of ADSs. Existing online repair solutions\nenforce ADS compliance by transforming unacceptable trajectories into\nacceptable ones based on predefined specifications, such as rule-based\nconstraints or training datasets. However, these approaches often lack\ngeneralizability, adaptability and tend to be overly conservative, resulting in\nineffective repairs that not only fail to mitigate safety risks sufficiently\nbut also degrade the overall driving experience. To address this issue, we\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\nthat identifies safety-critical states through offline learning from failed\ntests and generates appropriate mitigation actions to improve ADS safety.\nSpecifically, ADReFT incorporates a transformer-based model with two joint\nheads, State Monitor and Decision Adapter, designed to capture complex driving\nenvironment interactions to evaluate state safety severity and generate\nadaptive repair actions. Given the absence of oracles for state safety\nidentification, we first pretrain ADReFT using supervised learning with coarse\nannotations, i.e., labeling states preceding violations as positive samples and\nothers as negative samples. It establishes ADReFT's foundational capability to\nmitigate safety-critical violations, though it may result in somewhat\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\nusing reinforcement learning to improve its initial capability and generate\nmore precise and contextually appropriate repair decisions. Our evaluation\nresults illustrate that ADReFT achieves better repair performance.",
      "pdf_url": "http://arxiv.org/pdf/2506.23960v1",
      "published": "2025-06-30T15:29:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23960v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support",
      "authors": [
        "Stefan Buijsman",
        "Sarah Carter",
        "Juan Pablo Bermúdez"
      ],
      "abstract": "AI systems increasingly support human decision-making across domains of\nprofessional, skill-based, and personal activity. While previous work has\nexamined how AI might affect human autonomy globally, the effects of AI on\ndomain-specific autonomy -- the capacity for self-governed action within\ndefined realms of skill or expertise -- remain understudied. We analyze how AI\ndecision-support systems affect two key components of domain-specific autonomy:\nskilled competence (the ability to make informed judgments within one's domain)\nand authentic value-formation (the capacity to form genuine domain-relevant\nvalues and preferences). By engaging with prior investigations and analyzing\nempirical cases across medical, financial, and educational domains, we\ndemonstrate how the absence of reliable failure indicators and the potential\nfor unconscious value shifts can erode domain-specific autonomy both\nimmediately and over time. We then develop a constructive framework for\nautonomy-preserving AI support systems. We propose specific socio-technical\ndesign patterns -- including careful role specification, implementation of\ndefeater mechanisms, and support for reflective practice -- that can help\nmaintain domain-specific autonomy while leveraging AI capabilities. This\nframework provides concrete guidance for developing AI systems that enhance\nrather than diminish human agency within specialized domains of action.",
      "pdf_url": "http://arxiv.org/pdf/2506.23952v2",
      "published": "2025-06-30T15:20:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23952v2",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models",
      "authors": [
        "Anthony M. Barrett",
        "Jessica Newman",
        "Brandie Nonnecke",
        "Nada Madkour",
        "Dan Hendrycks",
        "Evan R. Murphy",
        "Krystal Jackson",
        "Deepika Raman"
      ],
      "abstract": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.",
      "pdf_url": "http://arxiv.org/pdf/2506.23949v1",
      "published": "2025-06-30T15:18:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23949v1",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.CY"
      ]
    },
    {
      "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning",
      "authors": [
        "Fuhang Kuang",
        "Jiacheng You",
        "Yingdong Hu",
        "Tong Zhang",
        "Chuan Wen",
        "Yang Gao"
      ],
      "abstract": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.",
      "pdf_url": "http://arxiv.org/pdf/2506.23944v2",
      "published": "2025-06-30T15:09:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23944v2",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference",
      "authors": [
        "Xiangchen Li",
        "Saeid Ghafouri",
        "Bo Ji",
        "Hans Vandierendonck",
        "Deepu John",
        "Dimitrios S. Nikolopoulos"
      ],
      "abstract": "As machine learning inferences increasingly move to edge devices, adapting to\ndiverse computational capabilities, hardware, and memory constraints becomes\nmore critical. Instead of relying on a pre-trained model fixed for all future\ninference queries across diverse edge devices, we argue that planning an\ninference pattern with a request-specific model tailored to the device's\ncomputational capacity, accuracy requirements, and time constraints is more\ncost-efficient and robust to diverse scenarios. To this end, we propose an\naccuracy-aware and workload-balanced inference system that integrates joint\nmodel quantization and inference partitioning. In this approach, the server\ndynamically responds to inference queries by sending a quantized model and\nadaptively sharing the inference workload with the device. Meanwhile, the\ndevice's computational power, channel capacity, and accuracy requirements are\nconsidered when deciding.\n  Furthermore, we introduce a new optimization framework for the inference\nsystem, incorporating joint model quantization and partitioning. Our approach\noptimizes layer-wise quantization bit width and partition points to minimize\ntime consumption and cost while accounting for varying accuracy requirements of\ntasks through an accuracy degradation metric in our optimization model. To our\nknowledge, this work represents the first exploration of optimizing\nquantization layer-wise bit-width in the inference serving system, by\nintroducing theoretical measurement of accuracy degradation. Simulation results\ndemonstrate a substantial reduction in overall time and power consumption, with\ncomputation payloads decreasing by over 80% and accuracy degradation kept below\n1%.",
      "pdf_url": "http://arxiv.org/pdf/2506.23934v1",
      "published": "2025-06-30T15:03:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23934v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ]
    },
    {
      "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages",
      "authors": [
        "Ruhina Tabasshum Prome",
        "Tarikul Islam Tamiti",
        "Anomadarshi Barua"
      ],
      "abstract": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.",
      "pdf_url": "http://arxiv.org/pdf/2506.23930v1",
      "published": "2025-06-30T14:59:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23930v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system",
      "authors": [
        "Junping Wang",
        "Bicheng Wang",
        "Yibo Xuea",
        "Yuan Xie"
      ],
      "abstract": "Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain.",
      "pdf_url": "http://arxiv.org/pdf/2506.23926v1",
      "published": "2025-06-30T14:54:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23926v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice",
      "authors": [
        "Akshit Kumar",
        "Tianyi Peng",
        "Yuhang Wu",
        "Assaf Zeevi"
      ],
      "abstract": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.",
      "pdf_url": "http://arxiv.org/pdf/2506.23924v1",
      "published": "2025-06-30T14:54:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23924v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System",
      "authors": [
        "Miguel Camacho-Sánchez",
        "Fernando García-Torres",
        "Jesper John Lisegaard",
        "Rocío del Amor",
        "Sankhya Mohanty",
        "Valery Naranjo"
      ],
      "abstract": "Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\nparticularly for large-scale applications such as wind turbine blades.\nControlling the resin flow dynamics in these processes is critical to ensure\nthe uniform impregnation of the fibre reinforcements, thereby preventing\nresidual porosities and dry spots that impact the consequent structural\nintegrity of the final component. This paper presents a reinforcement learning\n(RL) based strategy, established using process simulations, for synchronising\nthe different resin flow fronts in an infusion scenario involving two resin\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\napproach addresses the challenge of managing the fluid dynamics in a partially\nobservable environment. The results demonstrate the effectiveness of the RL\napproach in achieving an accurate flow convergence, highlighting its potential\ntowards improving process control and product quality in composites\nmanufacturing.",
      "pdf_url": "http://arxiv.org/pdf/2506.23923v1",
      "published": "2025-06-30T14:50:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23923v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence",
      "authors": [
        "András György",
        "Tor Lattimore",
        "Nevena Lazić",
        "Csaba Szepesvári"
      ],
      "abstract": "Sound deductive reasoning -- the ability to derive new knowledge from\nexisting facts and rules -- is an indisputably desirable aspect of general\nintelligence. Despite the major advances of AI systems in areas such as math\nand science, especially since the introduction of transformer architectures, it\nis well-documented that even the most advanced frontier systems regularly and\nconsistently falter on easily-solvable deductive reasoning tasks. Hence, these\nsystems are unfit to fulfill the dream of achieving artificial general\nintelligence capable of sound deductive reasoning. We argue that their unsound\nbehavior is a consequence of the statistical learning approach powering their\ndevelopment. To overcome this, we contend that to achieve reliable deductive\nreasoning in learning-based AI systems, researchers must fundamentally shift\nfrom optimizing for statistical performance against distributions on reasoning\nproblems and algorithmic tasks to embracing the more ambitious exact learning\nparadigm, which demands correctness on all inputs. We argue that exact learning\nis both essential and possible, and that this ambitious objective should guide\nalgorithm design.",
      "pdf_url": "http://arxiv.org/pdf/2506.23908v1",
      "published": "2025-06-30T14:37:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23908v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models",
      "authors": [
        "Hamza Rasaee",
        "Taha Koleilat",
        "Hassan Rivaz"
      ],
      "abstract": "Accurate and generalizable object segmentation in ultrasound imaging remains\na significant challenge due to anatomical variability, diverse imaging\nprotocols, and limited annotated data. In this study, we propose a\nprompt-driven vision-language model (VLM) that integrates Grounding DINO with\nSAM2 to enable object segmentation across multiple ultrasound organs. A total\nof 18 public ultrasound datasets, encompassing the breast, thyroid, liver,\nprostate, kidney, and paraspinal muscle, were utilized. These datasets were\ndivided into 15 for fine-tuning and validation of Grounding DINO using Low Rank\nAdaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for\ntesting to evaluate performance in unseen distributions. Comprehensive\nexperiments demonstrate that our approach outperforms state-of-the-art\nsegmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,\nand SAMUS on most seen datasets while maintaining strong performance on unseen\ndatasets without additional fine-tuning. These results underscore the promise\nof VLMs in scalable and robust ultrasound image analysis, reducing dependence\non large, organ-specific annotated datasets. We will publish our code on\ncode.sonography.ai after acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2506.23903v1",
      "published": "2025-06-30T14:33:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23903v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic",
      "authors": [
        "Yuta Sato",
        "Kazuhiko Kawamoto",
        "Hiroshi Kera"
      ],
      "abstract": "The chain of thought is fundamental in Transformers, which is to perform\nstep-by-step reasoning. Besides what intermediate steps work, the order of\nthese steps critically affects the difficulty of the reasoning. This study\naddresses a novel task of unraveling chain of thought - reordering decoder\ninput tokens to a learning-friendly sequence for Transformers to learn\narithmetic tasks. The proposed pipeline first trains a Transformer on a mixture\nof target sequences arranged in different orders and then identifies benign\norders as those with fast loss drops in the early stage. As the search space\ngrows factorially with sequence length, we propose a two-stage hierarchical\napproach for inter- and intra-block reordering. Experiments on four\norder-sensitive arithmetic tasks show that our method identifies a\nlearning-friendly order out of a few billion candidates. Notably, on the\nmultiplication task, it recovered the reverse-digit order reported in prior\nstudies.",
      "pdf_url": "http://arxiv.org/pdf/2506.23875v1",
      "published": "2025-06-30T14:05:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23875v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance",
      "authors": [
        "Louis Bradshaw",
        "Honglu Fan",
        "Alexander Spangher",
        "Stella Biderman",
        "Simon Colton"
      ],
      "abstract": "We study the capabilities of generative autoregressive transformer models\ntrained on large amounts of symbolic solo-piano transcriptions. After first\npretraining on approximately 60,000 hours of music, we use a comparatively\nsmaller, high-quality subset, to finetune models to produce musical\ncontinuations, perform symbolic classification tasks, and produce\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\nsymbolic music. When evaluating piano continuation coherence, our generative\nmodel outperforms leading symbolic generation techniques and remains\ncompetitive with proprietary audio generation models. On MIR classification\nbenchmarks, frozen representations from our contrastive model achieve\nstate-of-the-art results in linear probe experiments, while direct finetuning\ndemonstrates the generalizability of pretrained representations, often\nrequiring only a few hundred labeled examples to specialize to downstream\ntasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.23869v1",
      "published": "2025-06-30T14:00:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23869v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "Differentially Private Synthetic Data Release for Topics API Outputs",
      "authors": [
        "Travis Dick",
        "Alessandro Epasto",
        "Adel Javanmard",
        "Josh Karlin",
        "Andres Munoz Medina",
        "Vahab Mirrokni",
        "Sergei Vassilvitskii",
        "Peilin Zhong"
      ],
      "abstract": "The analysis of the privacy properties of Privacy-Preserving Ads APIs is an\narea of research that has received strong interest from academics, industry,\nand regulators. Despite this interest, the empirical study of these methods is\nhindered by the lack of publicly available data. Reliable empirical analysis of\nthe privacy properties of an API, in fact, requires access to a dataset\nconsisting of realistic API outputs; however, privacy concerns prevent the\ngeneral release of such data to the public.\n  In this work, we develop a novel methodology to construct synthetic API\noutputs that are simultaneously realistic enough to enable accurate study and\nprovide strong privacy protections. We focus on one Privacy-Preserving Ads\nAPIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a\nmethodology to generate a differentially-private dataset that closely matches\nthe re-identification risk properties of the real Topics API data. The use of\ndifferential privacy provides strong theoretical bounds on the leakage of\nprivate user information from this release.\n  Our methodology is based on first computing a large number of\ndifferentially-private statistics describing how output API traces evolve over\ntime. Then, we design a parameterized distribution over sequences of API traces\nand optimize its parameters so that they closely match the statistics obtained.\nFinally, we create the synthetic data by drawing from this distribution.\n  Our work is complemented by an open-source release of the anonymized dataset\nobtained by this methodology. We hope this will enable external researchers to\nanalyze the API in-depth and replicate prior and future work on a realistic\nlarge-scale dataset. We believe that this work will contribute to fostering\ntransparency regarding the privacy properties of Privacy-Preserving Ads APIs.",
      "pdf_url": "http://arxiv.org/pdf/2506.23855v1",
      "published": "2025-06-30T13:46:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23855v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts",
      "authors": [
        "Kenny Peng",
        "Rajiv Movva",
        "Jon Kleinberg",
        "Emma Pierson",
        "Nikhil Garg"
      ],
      "abstract": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.",
      "pdf_url": "http://arxiv.org/pdf/2506.23845v1",
      "published": "2025-06-30T13:35:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23845v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
      "authors": [
        "Hang Su",
        "Jun Luo",
        "Chang Liu",
        "Xiao Yang",
        "Yichi Zhang",
        "Yinpeng Dong",
        "Jun Zhu"
      ],
      "abstract": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.",
      "pdf_url": "http://arxiv.org/pdf/2506.23844v1",
      "published": "2025-06-30T13:34:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23844v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model",
      "authors": [
        "Bowen Ding",
        "Yuhan Chen",
        "Futing Wang",
        "Lingfeng Ming",
        "Tao Lin"
      ],
      "abstract": "Large Reasoning Models (LRMs) excel at solving complex problems but face an\noverthinking dilemma. When handling simple tasks, they often produce verbose\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\ntrigger unnecessary high-level reasoning behaviors like reflection and\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\nthese thinking-token-induced behaviors are not essential for effective\nproblem-solving and may even hinder correct reasoning within constrained token\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\nexposure to responses with and without thinking tokens; (2) A fine-grained\nadvantage control technique to dynamically regulate the prediction of target\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\nthinking tokens. Experimental results on five popular math reasoning benchmarks\nshow that DuP-PO performs well on the popular LRM, which significantly improves\ntheir token efficiency during reasoning, while achieving superior performance\nof the base model.",
      "pdf_url": "http://arxiv.org/pdf/2506.23840v1",
      "published": "2025-06-30T13:30:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23840v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins",
      "authors": [
        "Lluís C. Coll",
        "Martin W. Lauer-Schmaltz",
        "Philip Cash",
        "John P. Hansen",
        "Anja Maier"
      ],
      "abstract": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.",
      "pdf_url": "http://arxiv.org/pdf/2506.23826v1",
      "published": "2025-06-30T13:18:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23826v1",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.IR"
      ]
    },
    {
      "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment",
      "authors": [
        "Patrick Stokkink"
      ],
      "abstract": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.",
      "pdf_url": "http://arxiv.org/pdf/2506.23815v2",
      "published": "2025-06-30T13:02:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23815v2",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning",
      "authors": [
        "Anton Andreychuk",
        "Konstantin Yakovlev",
        "Aleksandr Panov",
        "Alexey Skrynnik"
      ],
      "abstract": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.",
      "pdf_url": "http://arxiv.org/pdf/2506.23793v1",
      "published": "2025-06-30T12:34:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23793v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)",
      "authors": [
        "Parosh Aziz Abdulla",
        "Mohamed Faouzi Atig",
        "Julie Cailler",
        "Chencheng Liang",
        "Philipp Rümmer"
      ],
      "abstract": "Nielsen transformation is a standard approach for solving word equations: by\nrepeatedly splitting equations and applying simplification steps, equations are\nrewritten until a solution is reached. When solving a conjunction of word\nequations in this way, the performance of the solver will depend considerably\non the order in which equations are processed. In this work, the use of Graph\nNeural Networks (GNNs) for ranking word equations before and during the solving\nprocess is explored. For this, a novel graph-based representation for word\nequations is presented, preserving global information across conjuncts,\nenabling the GNN to have a holistic view during ranking. To handle the variable\nnumber of conjuncts, three approaches to adapt a multi-classification task to\nthe problem of ranking equations are proposed. The training of the GNN is done\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\nexperimental results show that, compared to state-of-the-art string solvers,\nthe new framework solves more problems in benchmarks where each variable\nappears at most once in each equation.",
      "pdf_url": "http://arxiv.org/pdf/2506.23784v1",
      "published": "2025-06-30T12:24:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23784v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking",
      "authors": [
        "Shiao Wang",
        "Ju Huang",
        "Qingchuan Ma",
        "Jinfeng Gao",
        "Chunyi Xu",
        "Xiao Wang",
        "Lan Chen",
        "Bo Jiang"
      ],
      "abstract": "Combining traditional RGB cameras with bio-inspired event cameras for robust\nobject tracking has garnered increasing attention in recent years. However,\nmost existing multimodal tracking algorithms depend heavily on high-complexity\nVision Transformer architectures for feature extraction and fusion across\nmodalities. This not only leads to substantial computational overhead but also\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\nan efficient RGB-Event object tracking framework based on the linear-complexity\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\nlightweight Prompt Generator that utilizes embedded features from each\nmodality, together with a shared prompt pool, to dynamically generate\nmodality-specific learnable prompt vectors. These prompts, along with the\nmodality-specific embedded features, are then fed into a Vision Mamba-based\nFEMamba backbone, which facilitates prompt-guided feature extraction,\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\nrepresentations are passed to the tracking head for accurate target\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\nproposed tracking framework. The source code and pre-trained models will be\nreleased on https://github.com/Event-AHU/Mamba_FETrack",
      "pdf_url": "http://arxiv.org/pdf/2506.23783v1",
      "published": "2025-06-30T12:24:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23783v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling",
      "authors": [
        "Xiaoyang Li",
        "Linwei Tao",
        "Haohui Lu",
        "Minjing Dong",
        "Junbin Gao",
        "Chang Xu"
      ],
      "abstract": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.",
      "pdf_url": "http://arxiv.org/pdf/2506.23782v1",
      "published": "2025-06-30T12:23:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23782v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "BayesL: Towards a Logical Framework for Bayesian Networks",
      "authors": [
        "Stefano M. Nicoletti",
        "Mariëlle Stoelinga"
      ],
      "abstract": "We introduce BayesL, a novel logical framework for specifying, querying, and\nverifying the behaviour of Bayesian networks (BNs). BayesL (pronounced \"Basil\")\nis a structured language that allows for the creation of queries over BNs. It\nfacilitates versatile reasoning concerning causal and evidence-based\nrelationships, and permits comprehensive what-if scenario evaluations without\nthe need for manual modifications to the model.",
      "pdf_url": "http://arxiv.org/pdf/2506.23773v1",
      "published": "2025-06-30T12:18:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23773v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving",
      "authors": [
        "Guizhe Jin",
        "Zhuoren Li",
        "Bo Leng",
        "Ran Yu",
        "Lu Xiong"
      ],
      "abstract": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.",
      "pdf_url": "http://arxiv.org/pdf/2506.23771v1",
      "published": "2025-06-30T12:17:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23771v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead",
      "authors": [
        "Hongzhou Rao",
        "Yanjie Zhao",
        "Xinyi Hou",
        "Shenao Wang",
        "Haoyu Wang"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment.",
      "pdf_url": "http://arxiv.org/pdf/2506.23762v1",
      "published": "2025-06-30T12:09:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23762v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data",
      "authors": [
        "JiaRu Wu",
        "Mingwei Liu"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.",
      "pdf_url": "http://arxiv.org/pdf/2506.23735v1",
      "published": "2025-06-30T11:18:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23735v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment",
      "authors": [
        "Hao Shi",
        "Xi Li",
        "Fangfang Xie"
      ],
      "abstract": "Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex\ndynamics like intransitivity and the Red Queen effect, leading to unstable\nconvergence. To counter these challenges, this paper introduces the Marker Gene\nMethod (MGM), a framework that establishes stability by using a 'marker gene'\nas a dynamic benchmark and an adaptive weighting mechanism to balance\nexploration and exploitation. We provide rigorous mathematical proofs\ndemonstrating that MGM creates strong attractors near Nash Equilibria within\nthe Strictly Competitive Game framework. Empirically, MGM demonstrates its\nefficacy across a spectrum of challenges: it stabilizes the canonical\nRock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D\non ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it\nsuccessfully tames the notoriously pathological Shapley Biased Game. This work\npresents a theoretically sound and empirically validated framework that\nsubstantially enhances the stability and robustness of CCEAs in complex\ncompetitive environments.",
      "pdf_url": "http://arxiv.org/pdf/2506.23734v1",
      "published": "2025-06-30T11:13:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23734v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "System-Embedded Diffusion Bridge Models",
      "authors": [
        "Bartlomiej Sobieski",
        "Matthew Tivnan",
        "Yuang Wang",
        "Siyeop Yoon",
        "Pengfei Jin",
        "Dufan Wu",
        "Quanzheng Li",
        "Przemyslaw Biecek"
      ],
      "abstract": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.23726v1",
      "published": "2025-06-30T10:58:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23726v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?",
      "authors": [
        "Atharva Gundawar",
        "Som Sagar",
        "Ransalu Senanayake"
      ],
      "abstract": "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\nfailure detection. However, their proficiency in these high-level applications\noften assumes a deep understanding of low-level physical prerequisites, a\ncapability that remains largely unverified. For robots to perform actions\nreliably, they must comprehend intrinsic object properties (e.g., material,\nweight), action affordances (e.g., graspable, stackable), and physical\nconstraints (e.g., stability, reachability, or an object's state, such as being\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\nthat off-the-shelf models may lack this granular, physically grounded\nunderstanding, as such prerequisites are often overlooked during training.\n  To address this critical gap, we introduce PAC Bench, a comprehensive\nbenchmark designed to systematically evaluate VLMs on their understanding of\ncore Properties, Affordances, and Constraints (PAC) from a task executability\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\nand 120 unique simulated constraint scenarios across four tasks.\n  Our evaluations reveal significant gaps in the ability of current VLMs to\ngrasp fundamental physical concepts, highlighting limitations in their\nsuitability for reliable robot manipulation and pointing to key areas for\ntargeted research. PAC Bench also serves as a standardized benchmark for\nrigorously evaluating physical reasoning in VLMs and guiding the development of\nmore robust, physically grounded models for robotic applications.\n  Project Page: https://pacbench.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2506.23725v1",
      "published": "2025-06-30T10:58:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23725v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation",
      "authors": [
        "Chang'an Yi",
        "Xiaohui Deng",
        "Guohao Chen",
        "Yan Zhou",
        "Qinghua Lu",
        "Shuaicheng Niu"
      ],
      "abstract": "Test-time Adaptation (TTA) adapts a given model to testing domain data with\npotential domain shifts through online unsupervised learning, yielding\nimpressive performance. However, to date, existing TTA methods primarily focus\non single-model adaptation. In this work, we investigate an intriguing\nquestion: how does cross-model knowledge influence the TTA process? Our\nfindings reveal that, in TTA's unsupervised online setting, each model can\nprovide complementary, confident knowledge to the others, even when there are\nsubstantial differences in model size. For instance, a smaller model like\nMobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base\n(86.6M parameters). In light of this, we propose COCA, a Cross-Model\nCo-Learning framework for TTA, which mainly consists of two main strategies. 1)\nCo-adaptation adaptively integrates complementary knowledge from other models\nthroughout the TTA process, reducing individual model biases. 2)\nSelf-adaptation enhances each model's unique strengths via unsupervised\nlearning, enabling diverse adaptation to the target domain. Extensive\nexperiments show that COCA, which can also serve as a plug-and-play module,\nsignificantly boosts existing SOTAs, on models with various sizes--including\nResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,\nwith Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy\non ImageNet-C from 51.7% to 64.5%. The code is publicly available at\nhttps://github.com/ycarobot/COCA.",
      "pdf_url": "http://arxiv.org/pdf/2506.23724v1",
      "published": "2025-06-30T10:54:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23724v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound",
      "authors": [
        "Gijs Luijten",
        "Roberto Maria Scardigno",
        "Lisle Faray de Paiva",
        "Peter Hoyer",
        "Jens Kleesiek",
        "Domenico Buongiorno",
        "Vitoantonio Bevilacqua",
        "Jan Egger"
      ],
      "abstract": "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings.",
      "pdf_url": "http://arxiv.org/pdf/2506.23721v1",
      "published": "2025-06-30T10:49:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23721v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning",
      "authors": [
        "Alex Egg",
        "Martin Iglesias Goyanes",
        "Friso Kingma",
        "Andreu Mora",
        "Leandro von Werra",
        "Thomas Wolf"
      ],
      "abstract": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.",
      "pdf_url": "http://arxiv.org/pdf/2506.23719v1",
      "published": "2025-06-30T10:49:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23719v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation",
      "authors": [
        "Xingting Yao",
        "Qinghao Hu",
        "Fei Zhou",
        "Tielong Liu",
        "Gang Li",
        "Peisong Wang",
        "Jian Cheng"
      ],
      "abstract": "Multi-bit spiking neural networks (SNNs) have recently become a heated\nresearch spot, pursuing energy-efficient and high-accurate AI. However, with\nmore bits involved, the associated memory and computation demands escalate to\nthe point where the performance improvements become disproportionate. Based on\nthe insight that different layers demonstrate different importance and extra\nbits could be wasted and interfering, this paper presents an adaptive bit\nallocation strategy for direct-trained SNNs, achieving fine-grained layer-wise\nallocation of memory and computation resources. Thus, SNN's efficiency and\naccuracy can be improved. Specifically, we parametrize the temporal lengths and\nthe bit widths of weights and spikes, and make them learnable and controllable\nthrough gradients. To address the challenges caused by changeable bit widths\nand temporal lengths, we propose the refined spiking neuron, which can handle\ndifferent temporal lengths, enable the derivation of gradients for temporal\nlengths, and suit spike quantization better. In addition, we theoretically\nformulate the step-size mismatch problem of learnable bit widths, which may\nincur severe quantization errors to SNN, and accordingly propose the step-size\nrenewal mechanism to alleviate this issue. Experiments on various datasets,\nincluding the static CIFAR and ImageNet and the dynamic CIFAR-DVS and\nDVS-GESTURE, demonstrate that our methods can reduce the overall memory and\ncomputation cost while achieving higher accuracy. Particularly, our\nSEWResNet-34 can achieve a 2.69\\% accuracy gain and 4.16$\\times$ lower bit\nbudgets over the advanced baseline work on ImageNet. This work will be fully\nopen-sourced.",
      "pdf_url": "http://arxiv.org/pdf/2506.23717v1",
      "published": "2025-06-30T10:45:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.23717v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    }
  ]
}