{
  "last_updated": "2025-08-26T00:51:03.785756",
  "papers": [
    {
      "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
      "authors": [
        "Yosef Dayani",
        "Omer Benishu",
        "Sagie Benaim"
      ],
      "abstract": "Text-to-3D generation approaches have advanced significantly by leveraging\npretrained 2D diffusion priors, producing high-quality and 3D-consistent\noutputs. However, they often fail to produce out-of-domain (OOD) or rare\nconcepts, yielding inconsistent or inaccurate results. To this end, we propose\nMV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images\nfrom a large in-the-wild 2D database and then conditions a multiview diffusion\nmodel on these images to synthesize consistent and accurate multiview outputs.\nTraining such a retrieval-conditioned model is achieved via a novel hybrid\nstrategy bridging structured multiview data and diverse 2D image collections.\nThis involves training on multiview data using augmented conditioning views\nthat simulate retrieval variance for view-specific reconstruction, alongside\ntraining on sets of retrieved real-world 2D images using a distinctive held-out\nview prediction objective: the model predicts the held-out view from the other\nviews to infer 3D consistency from 2D data. To facilitate a rigorous OOD\nevaluation, we introduce a new collection of challenging OOD prompts.\nExperiments against state-of-the-art text-to-3D, image-to-3D, and\npersonalization baselines show that our approach significantly improves 3D\nconsistency, photorealism, and text adherence for OOD/rare concepts, while\nmaintaining competitive performance on standard benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2508.16577v1",
      "published": "2025-08-22T17:59:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16577v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems",
      "authors": [
        "Yizhi Wang",
        "Degang Xu",
        "Yongfang Xie",
        "Shuzhong Tan",
        "Xianan Zhou",
        "Peng Chen"
      ],
      "abstract": "This paper presents a hierarchical decision-making framework for autonomous\nnavigation in four-wheel independent steering and driving (4WISD) systems. The\nproposed approach integrates deep reinforcement learning (DRL) for high-level\nnavigation with fuzzy logic for low-level control to ensure both task\nperformance and physical feasibility. The DRL agent generates global motion\ncommands, while the fuzzy logic controller enforces kinematic constraints to\nprevent mechanical strain and wheel slippage. Simulation experiments\ndemonstrate that the proposed framework outperforms traditional navigation\nmethods, offering enhanced training efficiency and stability and mitigating\nerratic behaviors compared to purely DRL-based solutions. Real-world\nvalidations further confirm the framework's ability to navigate safely and\neffectively in dynamic industrial settings. Overall, this work provides a\nscalable and reliable solution for deploying 4WISD mobile robots in complex,\nreal-world scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2508.16574v1",
      "published": "2025-08-22T17:57:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16574v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence",
      "authors": [
        "Alisa Vinogradova",
        "Vlad Vinogradov",
        "Dmitrii Radkevich",
        "Ilya Yasny",
        "Dmitry Kobyzev",
        "Ivan Izmailov",
        "Katsiaryna Yanchanka",
        "Andrey Doronichev"
      ],
      "abstract": "In this paper, we describe and benchmark a competitor-discovery component\nused within an agentic AI system for fast drug asset due diligence. A\ncompetitor-discovery AI agent, given an indication, retrieves all drugs\ncomprising the competitive landscape of that indication and extracts canonical\nattributes for these drugs. The competitor definition is investor-specific, and\ndata is paywalled/licensed, fragmented across registries, ontology-mismatched\nby indication, alias-heavy for drug names, multimodal, and rapidly changing.\nAlthough considered the best tool for this problem, the current LLM-based AI\nsystems aren't capable of reliably retrieving all competing drug names, and\nthere is no accepted public benchmark for this task. To address the lack of\nevaluation, we use LLM-based agents to transform five years of multi-modal,\nunstructured diligence memos from a private biotech VC fund into a structured\nevaluation corpus mapping indications to competitor drugs with normalized\nattributes. We also introduce a competitor validating LLM-as-a-judge agent that\nfilters out false positives from the list of predicted competitors to maximize\nprecision and suppress hallucinations. On this benchmark, our\ncompetitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research\n(65%) and Perplexity Labs (60%). The system is deployed in production with\nenterprise users; in a case study with a biotech VC investment fund, analyst\nturnaround time dropped from 2.5 days to $\\sim$3 hours ($\\sim$20x) for the\ncompetitive analysis.",
      "pdf_url": "http://arxiv.org/pdf/2508.16571v1",
      "published": "2025-08-22T17:50:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16571v1",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.MA"
      ]
    },
    {
      "title": "A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer",
      "authors": [
        "Yuhui Tao",
        "Zhongwei Zhao",
        "Zilong Wang",
        "Xufang Luo",
        "Feng Chen",
        "Kang Wang",
        "Chuanfu Wu",
        "Xue Zhang",
        "Shaoting Zhang",
        "Jiaxi Yao",
        "Xingwei Jin",
        "Xinyang Jiang",
        "Yifan Yang",
        "Dongsheng Li",
        "Lili Qiu",
        "Zhiqiang Shao",
        "Jianming Guo",
        "Nengwang Yu",
        "Shuo Wang",
        "Ying Xiong"
      ],
      "abstract": "The non-invasive assessment of increasingly incidentally discovered renal\nmasses is a critical challenge in urologic oncology, where diagnostic\nuncertainty frequently leads to the overtreatment of benign or indolent tumors.\nIn this study, we developed and validated RenalCLIP using a dataset of 27,866\nCT scans from 8,809 patients across nine Chinese medical centers and the public\nTCIA cohort, a visual-language foundation model for characterization, diagnosis\nand prognosis of renal mass. The model was developed via a two-stage\npre-training strategy that first enhances the image and text encoders with\ndomain-specific knowledge before aligning them through a contrastive learning\nobjective, to create robust representations for superior generalization and\ndiagnostic precision. RenalCLIP achieved better performance and superior\ngeneralizability across 10 core tasks spanning the full clinical workflow of\nkidney cancer, including anatomical assessment, diagnostic classification, and\nsurvival prediction, compared with other state-of-the-art general-purpose CT\nfoundation models. Especially, for complicated task like recurrence-free\nsurvival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726,\nrepresenting a substantial improvement of approximately 20% over the leading\nbaselines. Furthermore, RenalCLIP's pre-training imparted remarkable data\nefficiency; in the diagnostic classification task, it only needs 20% training\ndata to achieve the peak performance of all baseline models even after they\nwere fully fine-tuned on 100% of the data. Additionally, it achieved superior\nperformance in report generation, image-text retrieval and zero-shot diagnosis\ntasks. Our findings establish that RenalCLIP provides a robust tool with the\npotential to enhance diagnostic accuracy, refine prognostic stratification, and\npersonalize the management of patients with kidney cancer.",
      "pdf_url": "http://arxiv.org/pdf/2508.16569v1",
      "published": "2025-08-22T17:48:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16569v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders",
      "authors": [
        "David Chanin",
        "Adri√† Garriga-Alonso"
      ],
      "abstract": "Sparse Autoencoders (SAEs) extract features from LLM internal activations,\nmeant to correspond to single concepts. A core SAE training hyperparameter is\nL0: how many features should fire per token on average. Existing work compares\nSAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a\nfree parameter with no single correct value. In this work we study the effect\nof L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE\nfails to learn the underlying features of the LLM. If L0 is too low, the SAE\nwill mix correlated features to improve reconstruction. If L0 is too high, the\nSAE finds degenerate solutions that also mix features. Further, we demonstrate\na method to determine the correct L0 value for an SAE on a given training\ndistribution, which finds the true L0 in toy models and coincides with peak\nsparse probing performance in LLMs. We find that most commonly used SAEs have\nan L0 that is too low. Our work shows that, to train SAEs with correct\nfeatures, practitioners must set L0 correctly.",
      "pdf_url": "http://arxiv.org/pdf/2508.16560v1",
      "published": "2025-08-22T17:26:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16560v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution",
      "authors": [
        "Tainyi Zhang",
        "Zheng-Peng Duan",
        "Peng-Tao Jiang",
        "Bo Li",
        "Ming-Ming Cheng",
        "Chun-Le Guo",
        "Chongyi Li"
      ],
      "abstract": "Diffusion-based real-world image super-resolution (Real-ISR) methods have\ndemonstrated impressive performance. To achieve efficient Real-ISR, many works\nemploy Variational Score Distillation (VSD) to distill pre-trained\nstable-diffusion (SD) model for one-step SR with a fixed timestep. However, due\nto the different noise injection timesteps, the SD will perform different\ngenerative priors. Therefore, a fixed timestep is difficult for these methods\nto fully leverage the generative priors in SD, leading to suboptimal\nperformance. To address this, we propose a Time-Aware one-step Diffusion\nNetwork for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder,\nwhich projects the same image into different latent features based on\ntimesteps. Through joint dynamic variation of timesteps and latent features,\nthe student model can better align with the input pattern distribution of the\npre-trained SD, thereby enabling more effective utilization of SD's generative\ncapabilities. To better activate the generative prior of SD at different\ntimesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the\nstudent model and those of the teacher model, thereby producing more consistent\ngenerative prior guidance conditioned on timesteps. Additionally, though\nutilizing the generative prior in SD at different timesteps, our method can\nnaturally achieve controllable trade-offs between fidelity and realism by\nchanging the timestep condition. Experimental results demonstrate that our\nmethod achieves both state-of-the-art performance and controllable SR results\nwith only a single step.",
      "pdf_url": "http://arxiv.org/pdf/2508.16557v1",
      "published": "2025-08-22T17:23:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16557v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Enhanced NIRMAL Optimizer With Damped Nesterov Acceleration: A Comparative Analysis",
      "authors": [
        "Nirmal Gaud",
        "Prasad Krishna Murthy",
        "Mostaque Md. Morshedur Hassan",
        "Abhijit Ganguly",
        "Vinay Mali",
        "Ms Lalita Bhagwat Randive",
        "Abhaypratap Singh"
      ],
      "abstract": "This study introduces the Enhanced NIRMAL (Novel Integrated Robust\nMulti-Adaptation Learning with Damped Nesterov Acceleration) optimizer, an\nimproved version of the original NIRMAL optimizer. By incorporating an\n$(\\alpha, r)$-damped Nesterov acceleration mechanism, Enhanced NIRMAL improves\nconvergence stability while retaining chess-inspired strategies of gradient\ndescent, momentum, stochastic perturbations, adaptive learning rates, and\nnon-linear transformations.\n  We evaluate Enhanced NIRMAL against Adam, SGD with Momentum, Nesterov, and\nthe original NIRMAL on four benchmark image classification datasets: MNIST,\nFashionMNIST, CIFAR-10, and CIFAR-100, using tailored convolutional neural\nnetwork (CNN) architectures.\n  Enhanced NIRMAL achieves a test accuracy of 46.06\\% and the lowest test loss\n(1.960435) on CIFAR-100, surpassing the original NIRMAL (44.34\\% accuracy) and\nclosely rivaling SGD with Momentum (46.43\\% accuracy). These results underscore\nEnhanced NIRMAL's superior generalization and stability, particularly on\ncomplex datasets.",
      "pdf_url": "http://arxiv.org/pdf/2508.16550v1",
      "published": "2025-08-22T17:16:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16550v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs",
      "authors": [
        "Hangzhan Jin",
        "Sicheng Lv",
        "Sifan Wu",
        "Mohammad Hamdaqa"
      ],
      "abstract": "Training large language models (LLMs) from scratch is increasingly\nimpractical, making post-training methods such as supervised fine-tuning (SFT)\nand reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern\npractice. Using an out-of-distribution (OOD) variant of the 24-point card game\nand new spectrum-based diagnostics, we revisit how these two stages reshape\nmodel representation and OOD performance. Our key findings are- (1) RL-FT can\nrestore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to\n15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and\na clear distribution shift, RL-FT cannot fully recover OOD performance. (2)\nDirection shifts of singular vectors matter more than singular value\nmagnitudes. These shifts concentrate on directions linked to the largest and\nsmallest singular values, leaving the bulk spectrum intact. (3) Low-rank and\nshallow recovery is effective: restoring singular vector directions for the top\n20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)\nStronger SFT checkpoints enable better recovery by RL, while overfitted ones\nresist restoration. These results reconcile prior reports of RL superior OOD\nperformance: RL primarily counteracts SFT-induced directional drift rather than\nfinding new solutions. Our spectrum-aware analysis highlights inexpensive\nrecovery knobs low-rank UV merging and shallow-layer resets that practitioners\ncan use before costly RL fine-tuning.",
      "pdf_url": "http://arxiv.org/pdf/2508.16546v1",
      "published": "2025-08-22T17:10:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16546v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Open World Detection: A Survey",
      "authors": [
        "Andrei-Stefan Bulzan",
        "Cosmin Cernazanu-Glavan"
      ],
      "abstract": "For decades, Computer Vision has aimed at enabling machines to perceive the\nexternal world. Initial limitations led to the development of highly\nspecialized niches. As success in each task accrued and research progressed,\nincreasingly complex perception tasks emerged. This survey charts the\nconvergence of these tasks and, in doing so, introduces Open World Detection\n(OWD), an umbrella term we propose to unify class-agnostic and generally\napplicable detection models in the vision domain. We start from the history of\nfoundational vision subdomains and cover key concepts, methodologies and\ndatasets making up today's state-of-the-art landscape. This traverses topics\nstarting from early saliency detection, foreground/background separation, out\nof distribution detection and leading up to open world object detection,\nzero-shot detection and Vision Large Language Models (VLLMs). We explore the\noverlap between these subdomains, their increasing convergence, and their\npotential to unify into a singular domain in the future, perception.",
      "pdf_url": "http://arxiv.org/pdf/2508.16527v1",
      "published": "2025-08-22T16:49:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16527v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T45",
        "A.1; I.2; I.4"
      ]
    },
    {
      "title": "Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning",
      "authors": [
        "Xuan Zhang",
        "Zhijian Zhou",
        "Weidi Xu",
        "Yanting Miao",
        "Chao Qu",
        "Yuan Qi"
      ],
      "abstract": "Enabling neural networks to learn complex logical constraints and fulfill\nsymbolic reasoning is a critical challenge. Bridging this gap often requires\nguiding the neural network's output distribution to move closer to the symbolic\nconstraints. While diffusion models have shown remarkable generative capability\nacross various domains, we employ the powerful architecture to perform\nneuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline\nadopts a two-stage training strategy: the first stage focuses on cultivating\nbasic reasoning abilities, while the second emphasizes systematic learning of\nlogical constraints. To impose hard constraints on neural outputs in the second\nstage, we formulate the diffusion reasoner as a Markov decision process and\ninnovatively fine-tune it with an improved proximal policy optimization\nalgorithm. We utilize a rule-based reward signal derived from the logical\nconsistency of neural outputs and adopt a flexible strategy to optimize the\ndiffusion reasoner's policy. We evaluate our methodology on some classical\nsymbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and\npreference learning. Experimental results demonstrate that our approach\nachieves outstanding accuracy and logical consistency among neural networks.",
      "pdf_url": "http://arxiv.org/pdf/2508.16524v1",
      "published": "2025-08-22T16:47:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16524v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation",
      "authors": [
        "Zhijian Zhou",
        "Junyi An",
        "Zongkai Liu",
        "Yunfei Shi",
        "Xuan Zhang",
        "Fenglei Cao",
        "Chao Qu",
        "Yuan Qi"
      ],
      "abstract": "Generating physically realistic 3D molecular structures remains a core\nchallenge in molecular generative modeling. While diffusion models equipped\nwith equivariant neural networks have made progress in capturing molecular\ngeometries, they often struggle to produce equilibrium structures that adhere\nto physical principles such as force field consistency. To bridge this gap, we\npropose Reinforcement Learning with Physical Feedback (RLPF), a novel framework\nthat extends Denoising Diffusion Policy Optimization to 3D molecular\ngeneration. RLPF formulates the task as a Markov decision process and applies\nproximal policy optimization to fine-tune equivariant diffusion models.\nCrucially, RLPF introduces reward functions derived from force-field\nevaluations, providing direct physical feedback to guide the generation toward\nenergetically stable and physically meaningful structures. Experiments on the\nQM9 and GEOM-drug datasets demonstrate that RLPF significantly improves\nmolecular stability compared to existing methods. These results highlight the\nvalue of incorporating physics-based feedback into generative modeling. The\ncode is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.",
      "pdf_url": "http://arxiv.org/pdf/2508.16521v1",
      "published": "2025-08-22T16:44:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16521v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Comparative Analysis of UAV Path Planning Algorithms for Efficient Navigation in Urban 3D Environments",
      "authors": [
        "Hichem Cheriet",
        "Khellat Kihel Badra",
        "Chouraqui Samira"
      ],
      "abstract": "The most crucial challenges for UAVs are planning paths and avoiding\nobstacles in their way. In recent years, a wide variety of path-planning\nalgorithms have been developed. These algorithms have successfully solved\npath-planning problems; however, they suffer from multiple challenges and\nlimitations. To test the effectiveness and efficiency of three widely used\nalgorithms, namely A*, RRT*, and Particle Swarm Optimization (PSO), this paper\nconducts extensive experiments in 3D urban city environments cluttered with\nobstacles. Three experiments were designed with two scenarios each to test the\naforementioned algorithms. These experiments consider different city map sizes,\ndifferent altitudes, and varying obstacle densities and sizes in the\nenvironment. According to the experimental results, the A* algorithm\noutperforms the others in both computation efficiency and path quality. PSO is\nespecially suitable for tight turns and dense environments, and RRT* offers a\nbalance and works well across all experiments due to its randomized approach to\nfinding solutions.",
      "pdf_url": "http://arxiv.org/pdf/2508.16515v1",
      "published": "2025-08-22T16:37:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16515v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline",
      "authors": [
        "Parker Seegmiller",
        "Kartik Mehta",
        "Soumya Saha",
        "Chenyang Tao",
        "Shereen Oraby",
        "Arpit Gupta",
        "Tagyoung Chung",
        "Mohit Bansal",
        "Nanyun Peng"
      ],
      "abstract": "Recent works improving LLM math reasoning with synthetic data have used\nunique setups, making comparison of data synthesis strategies impractical. This\nleaves many unanswered questions about the roles of different factors in the\nsynthetic data pipeline, such as the impact of filtering low-quality problems.\nTo address this gap, we introduce FLAMES, a Framework for LLM Assessment of\nMath rEasoning Data Synthesis, and perform a systematic study of 10 existing\ndata synthesis strategies and multiple other factors impacting the performance\nof synthetic math reasoning data. Our FLAMES experiments provide several\nvaluable insights about the optimal balance of difficulty and diversity of\nsynthetic data. First, data agents designed to increase problem complexity lead\nto best improvements on most math metrics. Second, with a fixed data generation\nbudget, keeping higher problem coverage is more important than keeping only\nproblems with reliable solutions. Third, GSM8K- and MATH-based synthetic data\ncan lead to improvements on competition-level benchmarks, showcasing\neasy-to-hard generalization. Leveraging insights from our FLAMES experiments,\nwe design two novel data synthesis strategies for improving out-of-domain\ngeneralization and robustness. Further, we develop the FLAMES dataset, an\neffective blend of our novel and existing data synthesis strategies,\noutperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),\nGSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES\ndataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and\nClaude 3.5 Sonnet.",
      "pdf_url": "http://arxiv.org/pdf/2508.16514v1",
      "published": "2025-08-22T16:37:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16514v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "On Zero-Shot Reinforcement Learning",
      "authors": [
        "Scott Jeen"
      ],
      "abstract": "Modern reinforcement learning (RL) systems capture deep truths about general,\nhuman problem-solving. In domains where new data can be simulated cheaply,\nthese systems uncover sequential decision-making policies that far exceed the\nability of any human. Society faces many problems whose solutions require this\nskill, but they are often in domains where new data cannot be cheaply\nsimulated. In such scenarios, we can learn simulators from existing data, but\nthese will only ever be approximately correct, and can be pathologically\nincorrect when queried outside of their training distribution. As a result, a\nmisalignment between the environments in which we train our agents and the\nreal-world in which we wish to deploy our agents is inevitable. Dealing with\nthis misalignment is the primary concern of zero-shot reinforcement learning, a\nproblem setting where the agent must generalise to a new task or domain with\nzero practice shots. Whilst impressive progress has been made on methods that\nperform zero-shot RL in idealised settings, new work is needed if these results\nare to be replicated in real-world settings. In this thesis, we argue that\ndoing so requires us to navigate (at least) three constraints. First, the data\nquality constraint: real-world datasets are small and homogeneous. Second, the\nobservability constraint: states, dynamics and rewards in the real-world are\noften only partially observed. And third, the data availability constraint: a\npriori access to data cannot always be assumed. This work proposes a suite of\nmethods that perform zero-shot RL subject to these constraints. In a series of\nempirical studies we expose the failings of existing methods, and justify our\ntechniques for remedying them. We believe these designs take us a step closer\nto RL methods that can be deployed to solve real-world problems.",
      "pdf_url": "http://arxiv.org/pdf/2508.16496v1",
      "published": "2025-08-22T16:20:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16496v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Post Hoc Regression Refinement via Pairwise Rankings",
      "authors": [
        "Kevin Tirta Wijaya",
        "Michael Sun",
        "Minghao Guo",
        "Hans-Peter Seidel",
        "Wojciech Matusik",
        "Vahid Babaei"
      ],
      "abstract": "Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.16495v1",
      "published": "2025-08-22T16:17:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16495v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SafeSpace: An Integrated Web Application for Digital Safety and Emotional Well-being",
      "authors": [
        "Kayenat Fatmi",
        "Mohammad Abbas"
      ],
      "abstract": "In the digital era, individuals are increasingly exposed to online harms such\nas toxicity, manipulation, and grooming, which often pose emotional and safety\nrisks. Existing systems for detecting abusive content or issuing safety alerts\noperate in isolation and rarely combine digital safety with emotional\nwell-being. In this paper, we present SafeSpace, a unified web application that\nintegrates three modules: (1) toxicity detection in chats and screenshots using\nNLP models and Google's Perspective API, (2) a configurable safety ping system\nthat issues emergency alerts with the user's live location (longitude and\nlatitude) via SMTP-based emails when check-ins are missed or SOS alerts are\nmanually triggered, and (3) a reflective questionnaire that evaluates\nrelationship health and emotional resilience. The system employs Firebase for\nalert management and a modular architecture designed for usability, privacy,\nand scalability. The experimental evaluation shows 93% precision in toxicity\ndetection, 100% reliability in safety alerts under emulator tests, and 92%\nalignment between automated and manual questionnaire scoring. SafeSpace,\nimplemented as a web application, demonstrates the feasibility of integrating\ndetection, protection, and reflection within a single platform, with future\ndeployment envisioned as a mobile application for broader accessibility.",
      "pdf_url": "http://arxiv.org/pdf/2508.16488v1",
      "published": "2025-08-22T16:07:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16488v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "FraPPE: Fast and Efficient Preference-based Pure Exploration",
      "authors": [
        "Udvas Das",
        "Apurv Shukla",
        "Debabrota Basu"
      ],
      "abstract": "Preference-based Pure Exploration (PrePEx) aims to identify with a given\nconfidence level the set of Pareto optimal arms in a vector-valued (aka\nmulti-objective) bandit, where the reward vectors are ordered via a (given)\npreference cone $\\mathcal{C}$. Though PrePEx and its variants are well-studied,\nthere does not exist a computationally efficient algorithm that can optimally\ntrack the existing lower bound for arbitrary preference cones. We successfully\nfill this gap by efficiently solving the minimisation and maximisation problems\nin the lower bound. First, we derive three structural properties of the lower\nbound that yield a computationally tractable reduction of the minimisation\nproblem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation\nproblem in the lower bound. Together, these techniques solve the maxmin\noptimisation problem in $\\mathcal{O}(KL^{2})$ time for a bandit instance with\n$K$ arms and $L$ dimensional reward, which is a significant acceleration over\nthe literature. We further prove that our proposed PrePEx algorithm, FraPPE,\nasymptotically achieves the optimal sample complexity. Finally, we perform\nnumerical experiments across synthetic and real datasets demonstrating that\nFraPPE achieves the lowest sample complexities to identify the exact Pareto set\namong the existing algorithms.",
      "pdf_url": "http://arxiv.org/pdf/2508.16487v1",
      "published": "2025-08-22T16:02:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16487v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ]
    },
    {
      "title": "Disentangled Multi-modal Learning of Histology and Transcriptomics for Cancer Characterization",
      "authors": [
        "Yupei Zhang",
        "Xiaofei Wang",
        "Anran Liu",
        "Lequan Yu",
        "Chao Li"
      ],
      "abstract": "Histopathology remains the gold standard for cancer diagnosis and prognosis.\nWith the advent of transcriptome profiling, multi-modal learning combining\ntranscriptomics with histology offers more comprehensive information. However,\nexisting multi-modal approaches are challenged by intrinsic multi-modal\nheterogeneity, insufficient multi-scale integration, and reliance on paired\ndata, restricting clinical applicability. To address these challenges, we\npropose a disentangled multi-modal framework with four contributions: 1) To\nmitigate multi-modal heterogeneity, we decompose WSIs and transcriptomes into\ntumor and microenvironment subspaces using a disentangled multi-modal fusion\nmodule, and introduce a confidence-guided gradient coordination strategy to\nbalance subspace optimization. 2) To enhance multi-scale integration, we\npropose an inter-magnification gene-expression consistency strategy that aligns\ntranscriptomic signals across WSI magnifications. 3) To reduce dependency on\npaired data, we propose a subspace knowledge distillation strategy enabling\ntranscriptome-agnostic inference through a WSI-only student model. 4) To\nimprove inference efficiency, we propose an informative token aggregation\nmodule that suppresses WSI redundancy while preserving subspace semantics.\nExtensive experiments on cancer diagnosis, prognosis, and survival prediction\ndemonstrate our superiority over state-of-the-art methods across multiple\nsettings. Code is available at\nhttps://github.com/helenypzhang/Disentangled-Multimodal-Learning.",
      "pdf_url": "http://arxiv.org/pdf/2508.16479v1",
      "published": "2025-08-22T15:51:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16479v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images",
      "authors": [
        "Anilkumar Swamy",
        "Vincent Leroy",
        "Philippe Weinzaepfel",
        "Jean-S√©bastien Franco",
        "Gr√©gory Rogez"
      ],
      "abstract": "Hand-object 3D reconstruction has become increasingly important for\napplications in human-robot interaction and immersive AR/VR experiences. A\ncommon approach for object-agnostic hand-object reconstruction from RGB\nsequences involves a two-stage pipeline: hand-object 3D tracking followed by\nmulti-view 3D reconstruction. However, existing methods rely on keypoint\ndetection techniques, such as Structure from Motion (SfM) and hand-keypoint\noptimization, which struggle with diverse object geometries, weak textures, and\nmutual hand-object occlusions, limiting scalability and generalization. As a\nkey enabler to generic and seamless, non-intrusive applicability, we propose in\nthis work a robust, keypoint detector-free approach to estimating hand-object\n3D transformations from monocular motion video/images. We further integrate\nthis with a multi-view reconstruction pipeline to accurately recover\nhand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely\non pre-scanned object templates or camera intrinsics, and reaches\nstate-of-the-art performance for the tasks of object-agnostic hand-object 3D\ntransformation and shape estimation on the SHOWMe benchmark. We also experiment\non sequences from the HO3D dataset, demonstrating generalization to unseen\nobject categories.",
      "pdf_url": "http://arxiv.org/pdf/2508.16465v1",
      "published": "2025-08-22T15:30:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16465v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Modular Embedding Recomposition for Incremental Learning",
      "authors": [
        "Aniello Panariello",
        "Emanuele Frascaroli",
        "Pietro Buzzega",
        "Lorenzo Bonicelli",
        "Angelo Porrello",
        "Simone Calderara"
      ],
      "abstract": "The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth.",
      "pdf_url": "http://arxiv.org/pdf/2508.16463v1",
      "published": "2025-08-22T15:25:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16463v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark",
      "authors": [
        "Adil Bahaj",
        "Mounir Ghogho"
      ],
      "abstract": "Large language models (LLMs) and vision-augmented LLMs (VLMs) have\nsignificantly advanced medical informatics, diagnostics, and decision support.\nHowever, these models exhibit systematic biases, particularly age bias,\ncompromising their reliability and equity. This is evident in their poorer\nperformance on pediatric-focused text and visual question-answering tasks. This\nbias reflects a broader imbalance in medical research, where pediatric studies\nreceive less funding and representation despite the significant disease burden\nin children. To address these issues, a new comprehensive multi-modal pediatric\nquestion-answering benchmark, PediatricsMQA, has been introduced. It consists\nof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric\ntopics across seven developmental stages (prenatal to adolescent) and 2,067\nvision-based MCQs using 634 pediatric images from 67 imaging modalities and 256\nanatomical regions. The dataset was developed using a hybrid manual-automatic\npipeline, incorporating peer-reviewed pediatric literature, validated question\nbanks, existing benchmarks, and existing QA resources. Evaluating\nstate-of-the-art open models, we find dramatic performance drops in younger\ncohorts, highlighting the need for age-aware methods to ensure equitable AI\nsupport in pediatric care.",
      "pdf_url": "http://arxiv.org/pdf/2508.16439v1",
      "published": "2025-08-22T14:50:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16439v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.GR",
        "cs.MM"
      ]
    },
    {
      "title": "OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval",
      "authors": [
        "Yu Liu",
        "Yanbing Liu",
        "Fangfang Yuan",
        "Cong Cao",
        "Youbang Sun",
        "Kun Peng",
        "WeiZhuo Chen",
        "Jianjun Li",
        "Zhiyuan Ma"
      ],
      "abstract": "Recent advances in large language models (LLMs) and dense retrievers have\ndriven significant progress in retrieval-augmented generation (RAG). However,\nexisting approaches face significant challenges in complex reasoning-oriented\nmulti-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior\nmethods struggle to generate robust multi-step plans for complex queries, as\nrule-based decomposers perform poorly on out-of-template questions. 2)\nSuboptimal reasoning-driven retrieval: Related methods employ limited query\nreformulation, leading to iterative retrieval loops that often fail to locate\ngolden documents. 3) Insufficient reasoning-guided filtering: Prevailing\nmethods lack the fine-grained reasoning to effectively filter salient\ninformation from noisy results, hindering utilization of retrieved knowledge.\nFundamentally, these limitations all stem from the weak coupling between\nretrieval and reasoning in current RAG architectures. We introduce the\nOrchestrated Planner-Executor Reasoning Architecture (OPERA), a novel\nreasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM)\ndecomposes questions into sub-goals, which are executed by a Reason-Execute\nModule (REM) with specialized components for precise reasoning and effective\nretrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative\nPolicy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex\nmulti-hop benchmarks show OPERA's superior performance, validating both the\nMAPGRPO method and OPERA's design. Code is available at\nhttps://github.com/Ameame1/OPERA.",
      "pdf_url": "http://arxiv.org/pdf/2508.16438v1",
      "published": "2025-08-22T14:50:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16438v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish",
      "authors": [
        "Yakup Abrek Er",
        "Ilker Kesen",
        "G√∂zde G√ºl ≈ûahin",
        "Aykut Erdem"
      ],
      "abstract": "We introduce Cetvel, a comprehensive benchmark designed to evaluate large\nlanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lack\neither task diversity or culturally relevant content, or both. Cetvel addresses\nthese gaps by combining a broad range of both discriminative and generative\ntasks ensuring content that reflects the linguistic and cultural richness of\nTurkish language. Cetvel covers 23 tasks grouped into seven categories,\nincluding tasks such as grammatical error correction, machine translation, and\nquestion answering rooted in Turkish history and idiomatic language. We\nevaluate 33 open-weight LLMs (up to 70B parameters) covering different model\nfamilies and instruction paradigms. Our experiments reveal that Turkish-centric\ninstruction-tuned models generally underperform relative to multilingual or\ngeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored for\nthe language. Moreover, we show that tasks such as grammatical error correction\nand extractive question answering are particularly discriminative in\ndifferentiating model capabilities. Cetvel offers a comprehensive and\nculturally grounded evaluation suite for advancing the development and\nassessment of LLMs in Turkish.",
      "pdf_url": "http://arxiv.org/pdf/2508.16431v1",
      "published": "2025-08-22T14:42:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16431v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ]
    },
    {
      "title": "A Lightweight Group Multiscale Bidirectional Interactive Network for Real-Time Steel Surface Defect Detection",
      "authors": [
        "Yong Zhang",
        "Cunjian Chen",
        "Qiang Gao",
        "Yi Wang",
        "Bin Fang"
      ],
      "abstract": "Real-time surface defect detection is critical for maintaining product\nquality and production efficiency in the steel manufacturing industry. Despite\npromising accuracy, existing deep learning methods often suffer from high\ncomputational complexity and slow inference speeds, which limit their\ndeployment in resource-constrained industrial environments. Recent lightweight\napproaches adopt multibranch architectures based on depthwise separable\nconvolution (DSConv) to capture multiscale contextual information. However,\nthese methods often suffer from increased computational overhead and lack\neffective cross-scale feature interaction, limiting their ability to fully\nleverage multiscale representations. To address these challenges, we propose\nGMBINet, a lightweight framework that enhances multiscale feature extraction\nand interaction through novel Group Multiscale Bidirectional Interactive (GMBI)\nmodules. The GMBI adopts a group-wise strategy for multiscale feature\nextraction, ensuring scale-agnostic computational complexity. It further\nintegrates a Bidirectional Progressive Feature Interactor (BPFI) and a\nparameter-free Element-Wise Multiplication-Summation (EWMS) operation to\nenhance cross-scale interaction without introducing additional computational\noverhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that\nGMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU\nand 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters.\nAdditional evaluations on the NEU-CLS defect classification dataset further\nconfirm the strong generalization ability of our method, demonstrating its\npotential for broader industrial vision applications beyond surface defect\ndetection. The dataset and code are publicly available at:\nhttps://github.com/zhangyongcode/GMBINet.",
      "pdf_url": "http://arxiv.org/pdf/2508.16397v1",
      "published": "2025-08-22T13:58:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16397v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Domain-aligned generative downscaling enhances projections of extreme climate events",
      "authors": [
        "Ruian Tie",
        "Xiaohui Zhong",
        "Zhengyu Shi",
        "Hao Li",
        "Jun Liu",
        "Wu Libo"
      ],
      "abstract": "Climate change is exacerbating extreme weather events globally, including\nhigh temperatures, extreme precipitation, strong winds, and tropical cyclones,\nposing severe threats to human health, infrastructure, food security, and\nsocio-economic systems. Although existing global climate models (GCMs) provide\nessential tools for climate prediction, they face limitations such as\ninsufficient resolution and high computational costs when simulating extreme\nevents. To address these issues, this study proposes a spatiotemporal\ndownscaling model based on generative machine learning-the Domain Aligned\nClimate Downscaling model (DACD), designed to enhance the simulation\ncapabilities for extreme weather events. The proposed model employs domain\nadaptation tricks and a Flow Matching training framework to transform global\nlow-resolution climate data into high-resolution local-scale climate\ninformation while achieving precise simulation of multivariable and temporal\nscales. The results show that during the historical period (2005-2014), our\nmodel outperformed existing methods in simulating high temperatures, extreme\nprecipitation, strong wind, and tropical cyclone tracks, significantly reducing\nerrors and improving the ability to capture extreme events. Under different\nfuture scenarios (2015-2100), the model reveals a significant increasing trend\nin the frequency and intensity of extreme events, particularly under the\nhigh-emission scenario (SSP585). Compared to traditional methods, our model\nmore accurately simulates the spatial distribution and dynamic changes of\nextreme events, providing an essential tool for understanding the impacts of\nclimate change. This study offers a new technological pathway for\nhigh-resolution climate analysis and extreme event prediction, providing\nscientific support for addressing future climate change and formulating\nadaptation strategies.",
      "pdf_url": "http://arxiv.org/pdf/2508.16396v1",
      "published": "2025-08-22T13:56:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16396v1",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ]
    },
    {
      "title": "RoMedQA: The First Benchmark for Romanian Medical Question Answering",
      "authors": [
        "Ana-Cristina Rogoz",
        "Radu Tudor Ionescu",
        "Alexandra-Valentina Anghel",
        "Ionut-Lucian Antone-Iordache",
        "Simona Coniac",
        "Andreea Iuliana Ionescu"
      ],
      "abstract": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nRoMedQA, the first Romanian QA benchmark for the medical domain, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. RoMedQA is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on RoMedQA. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on RoMedQA. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/RoMedQA.",
      "pdf_url": "http://arxiv.org/pdf/2508.16390v1",
      "published": "2025-08-22T13:48:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16390v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "GLARE: Agentic Reasoning for Legal Judgment Prediction",
      "authors": [
        "Xinyu Yang",
        "Chenlong Deng",
        "Zhicheng Dou"
      ],
      "abstract": "Legal judgment prediction (LJP) has become increasingly important in the\nlegal field. In this paper, we identify that existing large language models\n(LLMs) have significant problems of insufficient reasoning due to a lack of\nlegal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning\nframework that dynamically acquires key legal knowledge by invoking different\nmodules, thereby improving the breadth and depth of reasoning. Experiments\nconducted on the real-world dataset verify the effectiveness of our method.\nFurthermore, the reasoning chain generated during the analysis process can\nincrease interpretability and provide the possibility for practical\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2508.16383v1",
      "published": "2025-08-22T13:38:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16383v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering",
      "authors": [
        "Adil Bahaj",
        "Mounir Ghogho"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in natural language processing (NLP). However, their\neffectiveness in specialized, low-resource domains-such as Arabic legal\ncontexts-remains limited. This paper introduces MizanQA (pronounced Mizan,\nmeaning \"scale\" in Arabic, a universal symbol of justice), a benchmark designed\nto evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised\nby rich linguistic and legal complexity. The dataset draws on Modern Standard\nArabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal\ninfluences. Comprising over 1,700 multiple-choice questions, including\nmulti-answer formats, MizanQA captures the nuances of authentic legal\nreasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs\nreveal substantial performance gaps, highlighting the need for tailored\nevaluation metrics and culturally grounded, domain-specific LLM development.",
      "pdf_url": "http://arxiv.org/pdf/2508.16357v1",
      "published": "2025-08-22T13:04:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16357v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management",
      "authors": [
        "Nasir Khan",
        "Asmaa Abdallah",
        "Abdulkadir Celik",
        "Ahmed M. Eltawil",
        "Sinem Coleri"
      ],
      "abstract": "Efficient and reliable beam alignment is a critical requirement for mmWave\nmultiple-input multiple-output (MIMO) systems, especially in 6G and beyond,\nwhere communication must be fast, adaptive, and resilient to real-world\nuncertainties. Existing deep learning (DL)-based beam alignment methods often\nneglect the underlying causal relationships between inputs and outputs, leading\nto limited interpretability, poor generalization, and unnecessary beam sweeping\noverhead. In this work, we propose a causally-aware DL framework that\nintegrates causal discovery into beam management pipeline. Particularly, we\npropose a novel two-stage causal beam selection algorithm to identify a minimal\nset of relevant inputs for beam prediction. First, causal discovery learns a\nBayesian graph capturing dependencies between received power inputs and the\noptimal beam. Then, this graph guides causal feature selection for the DL-based\nclassifier. Simulation results reveal that the proposed causal beam selection\nmatches the performance of conventional methods while drastically reducing\ninput selection time by 94.4% and beam sweeping overhead by 59.4% by focusing\nonly on causally relevant features.",
      "pdf_url": "http://arxiv.org/pdf/2508.16352v1",
      "published": "2025-08-22T12:56:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16352v1",
      "categories": [
        "cs.AI",
        "eess.SP"
      ]
    },
    {
      "title": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs",
      "authors": [
        "Yu Yan",
        "Sheng Sun",
        "Zhe Wang",
        "Yijun Lin",
        "Zenghao Duan",
        "zhifei zheng",
        "Min Liu",
        "Zhiyi yin",
        "Jianping Zhang"
      ],
      "abstract": "With the development of Large Language Models (LLMs), numerous efforts have\nrevealed their vulnerabilities to jailbreak attacks. Although these studies\nhave driven the progress in LLMs' safety alignment, it remains unclear whether\nLLMs have internalized authentic knowledge to deal with real-world crimes, or\nare merely forced to simulate toxic language patterns. This ambiguity raises\nconcerns that jailbreak success is often attributable to a hallucination loop\nbetween jailbroken LLM and judger LLM. By decoupling the use of jailbreak\ntechniques, we construct knowledge-intensive Q\\&A to investigate the misuse\nthreats of LLMs in terms of dangerous knowledge possession, harmful task\nplanning utility, and harmfulness judgment robustness. Experiments reveal a\nmismatch between jailbreak success rates and harmful knowledge possession in\nLLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness\njudgments on toxic language patterns. Our study reveals a gap between existing\nLLM safety assessments and real-world threat potential.",
      "pdf_url": "http://arxiv.org/pdf/2508.16347v1",
      "published": "2025-08-22T12:41:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16347v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Uppaal Coshy: Automatic Synthesis of Compact Shields for Hybrid Systems",
      "authors": [
        "Asger Horn Brorholt",
        "Andreas Holck H√∏eg-Petersen",
        "Peter Gj√∏l Jensen",
        "Kim Guldstrand Larsen",
        "Marius Mikuƒçionis",
        "Christian Schilling",
        "Andrzej WƒÖsowski"
      ],
      "abstract": "We present Uppaal Coshy, a tool for automatic synthesis of a safety strategy\n-- or shield -- for Markov decision processes over continuous state spaces and\ncomplex hybrid dynamics. The general methodology is to partition the state\nspace and then solve a two-player safety game, which entails a number of\nalgorithmically hard problems such as reachability for hybrid systems. The\ngeneral philosophy of Uppaal Coshy is to approximate hard-to-obtain solutions\nusing simulations. Our implementation is fully automatic and supports the\nexpressive formalism of Uppaal models, which encompass stochastic hybrid\nautomata. The precision of our partition-based approach benefits from using\nfiner grids, which however are not efficient to store. We include an algorithm\ncalled Caap to efficiently compute a compact representation of a shield in the\nform of a decision tree, which yields significant reductions.",
      "pdf_url": "http://arxiv.org/pdf/2508.16345v1",
      "published": "2025-08-22T12:39:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16345v1",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks",
      "authors": [
        "Jin Li",
        "Kleanthis Malialis",
        "Stelios G. Vrachimis",
        "Marios M. Polycarpou"
      ],
      "abstract": "Water Distribution Networks (WDNs), critical to public well-being and\neconomic stability, face challenges such as pipe blockages and background\nleakages, exacerbated by operational constraints such as data non-stationarity\nand limited labeled data. This paper proposes an unsupervised, online learning\nframework that aims to detect two types of faults in WDNs: pipe blockages,\nmodeled as collective anomalies, and background leakages, modeled as concept\ndrift. Our approach combines a Long Short-Term Memory Variational Autoencoder\n(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and\nadaptation under non-stationary conditions. Its lightweight, memory-efficient\ndesign enables real-time, edge-level monitoring. Experiments on two realistic\nWDNs show that the proposed approach consistently outperforms strong baselines\nin detecting anomalies and adapting to recurrent drift, demonstrating its\neffectiveness in unsupervised event detection for dynamic WDN environments.",
      "pdf_url": "http://arxiv.org/pdf/2508.16336v1",
      "published": "2025-08-22T12:23:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16336v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning",
      "authors": [
        "Xueyao Zhang",
        "Junan Zhang",
        "Yuancheng Wang",
        "Chaoren Wang",
        "Yuanzhe Chen",
        "Dongya Jia",
        "Zhuo Chen",
        "Zhizheng Wu"
      ],
      "abstract": "Controllable human voice generation, particularly for expressive domains like\nsinging, remains a significant challenge. This paper introduces Vevo2, a\nunified framework for controllable speech and singing voice generation. To\ntackle issues like the scarcity of annotated singing data and to enable\nflexible controllability, Vevo2 introduces two audio tokenizers: (1) a\nmusic-notation-free prosody tokenizer that captures prosody and melody from\nspeech, singing, and even instrumental sounds, and (2) a low-frame-rate (12.5\nHz) content-style tokenizer that encodes linguistic content, prosody, and style\nfor both speech and singing, while enabling timbre disentanglement. Vevo2\nconsists of an auto-regressive (AR) content-style modeling stage, which aims to\nenable controllability over text, prosody, and style, as well as a\nflow-matching acoustic modeling stage that allows for timbre control.\nParticularly, during pre-training of the AR model, we propose both explicit and\nimplicit prosody learning strategies to bridge speech and singing voice.\nMoreover, to further enhance the AR model's ability to follow text and prosody,\nwe design a multi-objective post-training task that integrates both\nintelligibility and prosody similarity alignment. Experimental results show\nthat the unified modeling in Vevo2 brings mutual benefits to both speech and\nsinging voice generation. Additionally, Vevo2's effectiveness across a wide\nrange of synthesis, conversion, and editing tasks for both speech and singing\nfurther demonstrates its strong generalization ability and versatility. Audio\nsamples are are available at https://versasinger.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2508.16332v1",
      "published": "2025-08-22T12:20:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16332v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts",
      "authors": [
        "Darpan Aswal",
        "C√©line Hudelot"
      ],
      "abstract": "Large Language Models have found success in a variety of applications;\nhowever, their safety remains a matter of concern due to the existence of\nvarious types of jailbreaking methods. Despite significant efforts, alignment\nand safety fine-tuning only provide a certain degree of robustness against\njailbreak attacks that covertly mislead LLMs towards the generation of harmful\ncontent. This leaves them prone to a number of vulnerabilities, ranging from\ntargeted misuse to accidental profiling of users. This work introduces\n\\textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders\n(SAEs) to identify interpretable concepts within LLM internals associated with\ndifferent jailbreak themes. By extracting semantically meaningful internal\nrepresentations, LLMSymGuard enables building symbolic, logical safety\nguardrails -- offering transparent and robust defenses without sacrificing\nmodel capabilities or requiring further fine-tuning. Leveraging advances in\nmechanistic interpretability of LLMs, our approach demonstrates that LLMs learn\nhuman-interpretable concepts from jailbreaks, and provides a foundation for\ndesigning more interpretable and logical safeguard measures against attackers.\nCode will be released upon publication.",
      "pdf_url": "http://arxiv.org/pdf/2508.16325v1",
      "published": "2025-08-22T12:13:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16325v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SC"
      ]
    },
    {
      "title": "Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links",
      "authors": [
        "Selen Gecgel Cetin",
        "Tolga Ovatman",
        "Gunes Karabulut Kurt"
      ],
      "abstract": "This letter addresses essential aspects of threat assessment by proposing\nintent-driven threat models that incorporate both capabilities and intents. We\npropose a holistic framework for cyber physical awareness (CPA) in space\nnetworks, pointing out that analyzing reliability and security separately can\nlead to overfitting on system-specific criteria. We structure our proposed\nframework in three main steps. First, we suggest an algorithm that extracts\ncharacteristic properties of the received signal to facilitate an intuitive\nunderstanding of potential threats. Second, we develop a multitask learning\narchitecture where one task evaluates reliability-related capabilities while\nthe other deciphers the underlying intentions of the signal. Finally, we\npropose an adaptable threat assessment that aligns with varying security and\nreliability requirements. The proposed framework enhances the robustness of\nthreat detection and assessment, outperforming conventional sequential methods,\nand enables space networks with emerging intershell links to effectively\naddress complex threat scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2508.16314v1",
      "published": "2025-08-22T11:51:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16314v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "Retrieval Enhanced Feedback via In-context Neural Error-book",
      "authors": [
        "Jongyeop Hyun",
        "Bumsoo Kim"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved reasoning capabilities, with in-context learning (ICL) emerging as a\nkey technique for adaptation without retraining. While previous works have\nfocused on leveraging correct examples, recent research highlights the\nimportance of learning from errors to enhance performance. However, existing\nmethods lack a structured framework for analyzing and mitigating errors,\nparticularly in Multimodal Large Language Models (MLLMs), where integrating\nvisual and textual inputs adds complexity. To address this issue, we propose\nREFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a\nteacher-student framework that systematically structures errors and provides\ntargeted feedback. REFINE introduces three systematic queries to construct\nstructured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance\nmultimodal reasoning by prioritizing relevant visual information, diagnosing\ncritical failure points, and formulating corrective actions. Unlike prior\napproaches that rely on redundant retrievals, REFINE optimizes structured\nfeedback retrieval, improving inference efficiency, token usage, and\nscalability. Our results demonstrate substantial speedup, reduced computational\ncosts, and successful generalization, highlighting REFINE's potential for\nenhancing multimodal reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2508.16313v1",
      "published": "2025-08-22T11:50:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16313v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers",
      "authors": [
        "Lucas Maisonnave",
        "Karim Haroun",
        "Tom Pegeot"
      ],
      "abstract": "Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where\neach attention head contributes to the final representation. However, their\ncomputational complexity and high memory demands due to MHSA hinders their\ndeployment at the edge. In this work, we analyze and exploit information\nredundancy in attention maps to accelerate model inference. By quantifying the\ninformation captured by each attention head using Shannon entropy, our analysis\nreveals that attention heads with lower entropy, i.e., exhibiting more\ndeterministic behavior, tend to contribute less information, motivating\ntargeted compression strategies. Relying on these insights, we propose Entropy\nAttention Maps (EAM), a model that freezes the weights of low-entropy attention\nmaps and quantizes these values to low precision to avoid redundant\nre-computation. Empirical validation on ImageNet-1k shows that EAM achieves\nsimilar or higher accuracy at $\\leq$20\\% sparsity in attention maps and\ncompetitive performance beyond this level for the DeiT and Swin Transformer\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2508.16311v1",
      "published": "2025-08-22T11:43:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16311v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension",
      "authors": [
        "Mohammad Zia Ur Rehman",
        "Devraj Raghuvanshi",
        "Umang Jain",
        "Shubhi Bansal",
        "Nagendra Kumar"
      ],
      "abstract": "A major challenge in multimodal learning is the presence of noise within\nindividual modalities. This noise inherently affects the resulting multimodal\nrepresentations, especially when these representations are obtained through\nexplicit interactions between different modalities. Moreover, the multimodal\nfusion techniques while aiming to achieve a strong joint representation, can\nneglect valuable discriminative information within the individual modalities.\nTo this end, we propose a Multimodal-Multitask framework with crOss-modal\nRelation and hIErarchical iNteractive aTtention (MM-ORIENT) that is effective\nfor multiple tasks. The proposed approach acquires multimodal representations\ncross-modally without explicit interaction between different modalities,\nreducing the noise effect at the latent stage. To achieve this, we propose\ncross-modal relation graphs that reconstruct monomodal features to acquire\nmultimodal representations. The features are reconstructed based on the node\nneighborhood, where the neighborhood is decided by the features of a different\nmodality. We also propose Hierarchical Interactive Monomadal Attention (HIMA)\nto focus on pertinent information within a modality. While cross-modal relation\ngraphs help comprehend high-order relationships between two modalities, HIMA\nhelps in multitasking by learning discriminative features of individual\nmodalities before late-fusing them. Finally, extensive experimental evaluation\non three datasets demonstrates that the proposed approach effectively\ncomprehends multimodal content for multiple tasks.",
      "pdf_url": "http://arxiv.org/pdf/2508.16300v1",
      "published": "2025-08-22T11:10:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16300v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible",
      "authors": [
        "Wen-Han Hsieh",
        "Elvis Hsieh",
        "Dantong Niu",
        "Trevor Darrell",
        "Roei Herzig",
        "David M. Chan"
      ],
      "abstract": "Recently, Vision-Language-Action (VLA) models have demonstrated strong\nperformance on a range of robotic tasks. These models rely on multimodal\ninputs, with language instructions playing a crucial role -- not only in\npredicting actions, but also in robustly interpreting user intent, even when\nthe requests are impossible to fulfill. In this work, we investigate how VLAs\ncan recognize, interpret, and respond to false-premise instructions: natural\nlanguage commands that reference objects or conditions absent from the\nenvironment. We propose Instruct-Verify-and-Act (IVA), a unified framework that\n(i) detects when an instruction cannot be executed due to a false premise, (ii)\nengages in language-based clarification or correction, and (iii) grounds\nplausible alternatives in perception and action. Towards this end, we construct\na large-scale instruction tuning setup with structured language prompts and\ntrain a VLA model capable of handling both accurate and erroneous requests. Our\napproach leverages a contextually augmented, semi-synthetic dataset containing\npaired positive and false-premise instructions, enabling robust detection and\nnatural language correction. Our experiments show that IVA improves false\npremise detection accuracy by 97.56% over baselines, while increasing\nsuccessful responses in false-premise scenarios by 50.78%.",
      "pdf_url": "http://arxiv.org/pdf/2508.16292v1",
      "published": "2025-08-22T10:54:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16292v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications",
      "authors": [
        "Dawei Gao",
        "Zitao Li",
        "Yuexiang Xie",
        "Weirui Kuang",
        "Liuyi Yao",
        "Bingchen Qian",
        "Zhijian Ma",
        "Yue Cui",
        "Haohao Luo",
        "Shen Li",
        "Lu Yi",
        "Yi Yu",
        "Shiqi He",
        "Zhiling Luo",
        "Wenmeng Zhou",
        "Zhicheng Zhang",
        "Xuguang He",
        "Ziqian Chen",
        "Weikai Liao",
        "Farruh Isakulovich Kushnazarov",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.",
      "pdf_url": "http://arxiv.org/pdf/2508.16279v1",
      "published": "2025-08-22T10:35:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16279v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The next question after Turing's question: Introducing the Grow-AI test",
      "authors": [
        "Alexandru Tugui"
      ],
      "abstract": "This study aims to extend the framework for assessing artificial\nintelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),\ndesigned to answer the question \"Can machines grow up?\" -- a natural successor\nto the Turing Test. The methodology applied is based on a system of six primary\ncriteria (C1-C6), each assessed through a specific \"game\", divided into four\narenas that explore both the human dimension and its transposition into AI. All\ndecisions and actions of the entity are recorded in a standardized AI Journal,\nthe primary source for calculating composite scores. The assessment uses the\nprior expert method to establish initial weights, and the global score -- Grow\nUp Index -- is calculated as the arithmetic mean of the six scores, with\ninterpretation on maturity thresholds. The results show that the methodology\nallows for a coherent and comparable assessment of the level of \"growth\" of AI\nentities, regardless of their type (robots, software agents, LLMs). The\nmulti-game structure highlights strengths and vulnerable areas, and the use of\na unified journal guarantees traceability and replicability in the evaluation.\nThe originality of the work lies in the conceptual transposition of the process\nof \"growing\" from the human world to that of artificial intelligence, in an\nintegrated testing format that combines perspectives from psychology, robotics,\ncomputer science, and ethics. Through this approach, GROW-AI not only measures\nperformance but also captures the evolutionary path of an AI entity towards\nmaturity.",
      "pdf_url": "http://arxiv.org/pdf/2508.16277v1",
      "published": "2025-08-22T10:19:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16277v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "68T01, 68T05, 68T42, 91A80",
        "I.2; K.4"
      ]
    },
    {
      "title": "Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation",
      "authors": [
        "Yahya Badran",
        "Christine Preisach"
      ],
      "abstract": "Personalized recommendation is a key feature of intelligent tutoring systems,\ntypically relying on accurate models of student knowledge. Knowledge Tracing\n(KT) models enable this by estimating a student's mastery based on their\nhistorical interactions. Many KT models rely on human-annotated knowledge\nconcepts (KCs), which tag each exercise with one or more skills or concepts\nbelieved to be necessary for solving it. However, these KCs can be incomplete,\nerror-prone, or overly general.\n  In this paper, we propose a deep learning model that learns sparse binary\nrepresentations of exercises, where each bit indicates the presence or absence\nof a latent concept. We refer to these representations as auxiliary KCs. These\nrepresentations capture conceptual structure beyond human-defined annotations\nand are compatible with both classical models (e.g., BKT) and modern deep\nlearning KT architectures.\n  We demonstrate that incorporating auxiliary KCs improves both student\nmodeling and adaptive exercise recommendation. For student modeling, we show\nthat augmenting classical models like BKT with auxiliary KCs leads to improved\npredictive performance. For recommendation, we show that using auxiliary KCs\nenhances both reinforcement learning-based policies and a simple planning-based\nmethod (expectimax), resulting in measurable gains in student learning outcomes\nwithin a simulated student environment.",
      "pdf_url": "http://arxiv.org/pdf/2508.16269v1",
      "published": "2025-08-22T10:12:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16269v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "From Confidence to Collapse in LLM Factual Robustness",
      "authors": [
        "Alina Fastowski",
        "Bardh Prenkaj",
        "Gjergji Kasneci"
      ],
      "abstract": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models.",
      "pdf_url": "http://arxiv.org/pdf/2508.16267v1",
      "published": "2025-08-22T09:59:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16267v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use",
      "authors": [
        "Fei Lei",
        "Yibo Yang",
        "Wenxiu Sun",
        "Dahua Lin"
      ],
      "abstract": "Large Language Models (LLMs) are evolving from text generators into reasoning\nagents. This transition makes their ability to use external tools a critical\ncapability. However, evaluating this skill presents a significant challenge.\nExisting benchmarks are often limited by their reliance on synthetic tools and\nseverely constrained action spaces. To address these limitations, we introduce\nMCPVerse, an expansive, real-world benchmark for evaluating agentic tool use.\nMCPVerse integrates more than 550 real-world, executable tools to create an\nunprecedented action space exceeding 140k tokens, and employs outcome-based\nevaluation with real-time ground truth for time-sensitive tasks. We benchmarked\nthe state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale),\nrevealing that while most models suffer performance degradation when confronted\nwith larger tool sets, the agentic models, such as Claude-4-Sonnet, can\neffectively leverage expanded exploration spaces to improve accuracy. This\nfinding not only exposes the limitations of state-of-the-art models in complex,\nreal-world scenarios but also establishes MCPVerse as a critical benchmark for\nmeasuring and advancing agentic tool use capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2508.16260v1",
      "published": "2025-08-22T09:47:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16260v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Reduction of Input/Output Logics to SAT",
      "authors": [
        "Alexander Steen"
      ],
      "abstract": "Deontic logics are formalisms for reasoning over norms, obligations,\npermissions and prohibitions. Input/Output (I/O) Logics are a particular family\nof so-called norm-based deontic logics that formalize conditional norms outside\nof the underlying object logic language, where conditional norms do not carry a\ntruth-value themselves. In this paper, an automation approach for I/O logics is\npresented that makes use of suitable reductions to (sequences of) propositional\nsatisfiability problems. A prototypical implementation, named rio (reasoner for\ninput/output logics), of the proposed procedures is presented and applied to\nillustrative examples.",
      "pdf_url": "http://arxiv.org/pdf/2508.16242v1",
      "published": "2025-08-22T09:22:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16242v1",
      "categories": [
        "cs.LO",
        "cs.AI",
        "68T27",
        "I.2.3"
      ]
    },
    {
      "title": "A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease",
      "authors": [
        "Patricia Amado-Caballero",
        "Luis M. San-Jos√©-Revuelta",
        "Xinheng Wang",
        "Jos√© Ram√≥n Garmendia-Leiza",
        "Carlos Alberola-L√≥pez",
        "Pablo Casaseca-de-la-Higuera"
      ],
      "abstract": "This paper presents an explainable artificial intelligence (XAI)-based\nframework for the spectral analysis of cough sounds associated with chronic\nrespiratory diseases, with a particular focus on Chronic Obstructive Pulmonary\nDisease (COPD). A Convolutional Neural Network (CNN) is trained on\ntime-frequency representations of cough signals, and occlusion maps are used to\nidentify diagnostically relevant regions within the spectrograms. These\nhighlighted areas are subsequently decomposed into five frequency subbands,\nenabling targeted spectral feature extraction and analysis. The results reveal\nthat spectral patterns differ across subbands and disease groups, uncovering\ncomplementary and compensatory trends across the frequency spectrum.\nNoteworthy, the approach distinguishes COPD from other respiratory conditions,\nand chronic from non-chronic patient groups, based on interpretable spectral\nmarkers. These findings provide insight into the underlying pathophysiological\ncharacteristics of cough acoustics and demonstrate the value of\nfrequency-resolved, XAI-enhanced analysis for biomedical signal interpretation\nand translational respiratory disease diagnostics.",
      "pdf_url": "http://arxiv.org/pdf/2508.16237v1",
      "published": "2025-08-22T09:16:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16237v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ]
    },
    {
      "title": "FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing",
      "authors": [
        "Jiahao Chen",
        "Zhiyong Ma",
        "Wenbiao Du",
        "Qingyuan Chuai"
      ],
      "abstract": "Multi-modal creative writing (MMCW) aims to produce illustrated articles.\nUnlike common multi-modal generative (MMG) tasks such as storytelling or\ncaption generation, MMCW is an entirely new and more abstract challenge where\ntextual and visual contexts are not strictly related to each other. Existing\nmethods for related tasks can be forcibly migrated to this track, but they\nrequire specific modality inputs or costly training, and often suffer from\nsemantic inconsistencies between modalities. Therefore, the main challenge lies\nin economically performing MMCW with flexible interactive patterns, where the\nsemantics between the modalities of the output are more aligned. In this work,\nwe propose FlexMUSE with a T2I module to enable optional visual input. FlexMUSE\npromotes creativity and emphasizes the unification between modalities by\nproposing the modality semantic alignment gating (msaGate) to restrict the\ntextual input. Besides, an attention-based cross-modality fusion is proposed to\naugment the input features for semantic enhancement. The modality semantic\ncreative direct preference optimization (mscDPO) within FlexMUSE is designed by\nextending the rejected samples to facilitate the writing creativity. Moreover,\nto advance the MMCW, we expose a dataset called ArtMUSE which contains with\naround 3k calibrated text-image pairs. FlexMUSE achieves promising results,\ndemonstrating its consistency, creativity and coherence.",
      "pdf_url": "http://arxiv.org/pdf/2508.16230v1",
      "published": "2025-08-22T09:01:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16230v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "An Investigation of Visual Foundation Models Robustness",
      "authors": [
        "Sandeep Gupta",
        "Roberto Passerone"
      ],
      "abstract": "Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision,\npowering systems for diverse tasks such as object detection, image\nclassification, segmentation, pose estimation, and motion tracking. VFMs are\ncapitalizing on seminal innovations in deep learning models, such as LeNet-5,\nAlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver\nsuperior performance across a range of critical computer vision applications.\nThese include security-sensitive domains like biometric verification,\nautonomous vehicle perception, and medical image analysis, where robustness is\nessential to fostering trust between technology and the end-users. This article\ninvestigates network robustness requirements crucial in computer vision systems\nto adapt effectively to dynamic environments influenced by factors such as\nlighting, weather conditions, and sensor characteristics. We examine the\nprevalent empirical defenses and robust training employed to enhance vision\nnetwork robustness against real-world challenges such as distributional shifts,\nnoisy and spatially distorted inputs, and adversarial attacks. Subsequently, we\nprovide a comprehensive analysis of the challenges associated with these\ndefense mechanisms, including network properties and components to guide\nablation studies and benchmarking metrics to evaluate network robustness.",
      "pdf_url": "http://arxiv.org/pdf/2508.16225v1",
      "published": "2025-08-22T08:54:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16225v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models",
      "authors": [
        "Huanpeng Chu",
        "Wei Wu",
        "Guanyu Fen",
        "Yutao Zhang"
      ],
      "abstract": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure.In addition, during cache reuse, we dynamically estimate the\ncorresponding noise and filter it out to reduce its impact on the sampling\ndirection.Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
      "pdf_url": "http://arxiv.org/pdf/2508.16212v1",
      "published": "2025-08-22T08:36:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16212v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Competition and Attraction Improve Model Fusion",
      "authors": [
        "Jo√£o Abrantes",
        "Robert Tjarko Lange",
        "Yujin Tang"
      ],
      "abstract": "Model merging is a powerful technique for integrating the specialized\nknowledge of multiple machine learning models into a single model. However,\nexisting methods require manually partitioning model parameters into fixed\ngroups for merging, which restricts the exploration of potential combinations\nand limits performance. To overcome these limitations, we propose Model Merging\nof Natural Niches (M2N2), an evolutionary algorithm with three key features:\n(1) dynamic adjustment of merging boundaries to progressively explore a broader\nrange of parameter combinations; (2) a diversity preservation mechanism\ninspired by the competition for resources in nature, to maintain a population\nof diverse, high-performing models that are particularly well-suited for\nmerging; and (3) a heuristicbased attraction metric to identify the most\npromising pairs of models for fusion. Our experimental results demonstrate, for\nthe first time, that model merging can be used to evolve models entirely from\nscratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch\nand achieve performance comparable to CMA-ES, while being computationally more\nefficient. Furthermore, M2N2 scales to merge specialized language and image\ngeneration models, achieving state-of-the-art performance. Notably, it\npreserves crucial model capabilities beyond those explicitly optimized by the\nfitness function, highlighting its robustness and versatility. Our code is\navailable at https://github.com/SakanaAI/natural_niches",
      "pdf_url": "http://arxiv.org/pdf/2508.16204v1",
      "published": "2025-08-22T08:24:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.16204v1",
      "categories": [
        "cs.AI",
        "cs.NE"
      ]
    }
  ]
}