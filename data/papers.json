{
  "last_updated": "2025-11-17T00:53:54.043157",
  "papers": [
    {
      "title": "Black-Box On-Policy Distillation of Large Language Models",
      "authors": [
        "Tianzhu Ye",
        "Li Dong",
        "Zewen Chi",
        "Xun Wu",
        "Shaohan Huang",
        "Furu Wei"
      ],
      "abstract": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
      "pdf_url": "https://arxiv.org/pdf/2511.10643v1",
      "published": "2025-11-13T18:58:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10643v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Instella: Fully Open Language Models with Stellar Performance",
      "authors": [
        "Jiang Liu",
        "Jialian Wu",
        "Xiaodong Yu",
        "Yusheng Su",
        "Prakamya Mishra",
        "Gowtham Ramesh",
        "Sudhanshu Ranjan",
        "Chaitanya Manem",
        "Ximeng Sun",
        "Ze Wang",
        "Pratik Prabhanjan Brahma",
        "Zicheng Liu",
        "Emad Barsoum"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.",
      "pdf_url": "https://arxiv.org/pdf/2511.10628v1",
      "published": "2025-11-13T18:52:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10628v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Querying Labeled Time Series Data with Scenario Programs",
      "authors": [
        "Edward Kim",
        "Devan Shanker",
        "Varun Bharadwaj",
        "Hongbeen Park",
        "Jinkyu Kim",
        "Hazem Torfah",
        "Daniel J Fremont",
        "Sanjit A Seshia"
      ],
      "abstract": "Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.",
      "pdf_url": "https://arxiv.org/pdf/2511.10627v1",
      "published": "2025-11-13T18:52:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10627v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.FL",
        "cs.LG"
      ]
    },
    {
      "title": "SSR: Socratic Self-Refine for Large Language Model Reasoning",
      "authors": [
        "Haizhou Shi",
        "Ye Liu",
        "Bo Pang",
        "Zeyu Leo Liu",
        "Hao Wang",
        "Silvio Savarese",
        "Caiming Xiong",
        "Yingbo Zhou",
        "Semih Yavuz"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2511.10621v1",
      "published": "2025-11-13T18:47:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10621v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Know Your Limits: Entropy Estimation Modeling for Compression and Generalization",
      "authors": [
        "Benjamin L. Badger",
        "Matthew Neligeorge"
      ],
      "abstract": "Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.",
      "pdf_url": "https://arxiv.org/pdf/2511.10618v1",
      "published": "2025-11-13T18:46:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10618v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ]
    },
    {
      "title": "Towards an Agentic Workflow for Internet Measurement Research",
      "authors": [
        "Alagappan Ramanathan",
        "Eunju Kang",
        "Dongsu Han",
        "Sangeetha Abdu Jyothi"
      ],
      "abstract": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.\n  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.",
      "pdf_url": "https://arxiv.org/pdf/2511.10611v1",
      "published": "2025-11-13T18:44:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10611v1",
      "categories": [
        "cs.NI",
        "cs.AI"
      ]
    },
    {
      "title": "Regular Games -- an Automata-Based General Game Playing Language",
      "authors": [
        "Radosław Miernik",
        "Marek Szykuła",
        "Jakub Kowalski",
        "Jakub Cieśluk",
        "Łukasz Galas",
        "Wojciech Pawlik"
      ],
      "abstract": "We propose a new General Game Playing (GGP) system called Regular Games (RG). The main goal of RG is to be both computationally efficient and convenient for game design. The system consists of several languages. The core component is a low-level language that defines the rules by a finite automaton. It is minimal with only a few mechanisms, which makes it easy for automatic processing (by agents, analysis, optimization, etc.). The language is universal for the class of all finite turn-based games with imperfect information. Higher-level languages are introduced for game design (by humans or Procedural Content Generation), which are eventually translated to a low-level language. RG generates faster forward models than the current state of the art, beating other GGP systems (Regular Boardgames, Ludii) in terms of efficiency. Additionally, RG's ecosystem includes an editor with LSP, automaton visualization, benchmarking tools, and a debugger of game description transformations.",
      "pdf_url": "https://arxiv.org/pdf/2511.10593v1",
      "published": "2025-11-13T18:29:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10593v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question Answering",
      "authors": [
        "Bavana Durgapraveen",
        "Sornaraj Sivasankaran",
        "Abhinand Balachandran",
        "Sriram Rajkumar"
      ],
      "abstract": "The rapid expansion of asynchronous remote care has intensified provider workload, creating demand for AI systems that can assist clinicians in managing patient queries more efficiently. The MEDIQA-WV 2025 shared task addresses this challenge by focusing on generating free-text responses to wound care queries paired with images. In this work, we present two complementary approaches developed for the English track. The first leverages a mined prompting strategy, where training data is embedded and the top-k most similar examples are retrieved to serve as few-shot demonstrations during generation. The second approach builds on a metadata ablation study, which identified four metadata attributes that consistently enhance response quality. We train classifiers to predict these attributes for test cases and incorporate them into the generation pipeline, dynamically adjusting outputs based on prediction confidence. Experimental results demonstrate that mined prompting improves response relevance, while metadata-guided generation further refines clinical precision. Together, these methods highlight promising directions for developing AI-driven tools that can provide reliable and efficient wound care support.",
      "pdf_url": "https://arxiv.org/pdf/2511.10591v1",
      "published": "2025-11-13T18:28:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10591v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Textual understanding boost in the WikiRace",
      "authors": [
        "Raman Ebrahimi",
        "Sean Fuhrman",
        "Kendrick Nguyen",
        "Harini Gurusankar",
        "Massimo Franceschetti"
      ],
      "abstract": "The WikiRace game, where players navigate between Wikipedia articles using only hyperlinks, serves as a compelling benchmark for goal-directed search in complex information networks. This paper presents a systematic evaluation of navigation strategies for this task, comparing agents guided by graph-theoretic structure (betweenness centrality), semantic meaning (language model embeddings), and hybrid approaches. Through rigorous benchmarking on a large Wikipedia subgraph, we demonstrate that a purely greedy agent guided by the semantic similarity of article titles is overwhelmingly effective. This strategy, when combined with a simple loop-avoidance mechanism, achieved a perfect success rate and navigated the network with an efficiency an order of magnitude better than structural or hybrid methods. Our findings highlight the critical limitations of purely structural heuristics for goal-directed search and underscore the transformative potential of large language models to act as powerful, zero-shot semantic navigators in complex information spaces.",
      "pdf_url": "https://arxiv.org/pdf/2511.10585v1",
      "published": "2025-11-13T18:25:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10585v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Prompting Strategies with MedGemma for Medical Order Extraction",
      "authors": [
        "Abhinand Balachandran",
        "Bavana Durgapraveen",
        "Gowsikkan Sikkan Sudhagar",
        "Vidhya Varshany J S",
        "Sriram Rajkumar"
      ],
      "abstract": "The accurate extraction of medical orders from doctor-patient conversations is a critical task for reducing clinical documentation burdens and ensuring patient safety. This paper details our team submission to the MEDIQA-OE-2025 Shared Task. We investigate the performance of MedGemma, a new domain-specific open-source language model, for structured order extraction. We systematically evaluate three distinct prompting paradigms: a straightforward one-Shot approach, a reasoning-focused ReAct framework, and a multi-step agentic workflow. Our experiments reveal that while more complex frameworks like ReAct and agentic flows are powerful, the simpler one-shot prompting method achieved the highest performance on the official validation set. We posit that on manually annotated transcripts, complex reasoning chains can lead to \"overthinking\" and introduce noise, making a direct approach more robust and efficient. Our work provides valuable insights into selecting appropriate prompting strategies for clinical information extraction in varied data conditions.",
      "pdf_url": "https://arxiv.org/pdf/2511.10583v1",
      "published": "2025-11-13T18:22:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10583v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Emotionally Intelligent and Responsible Reinforcement Learning",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Personalized decision systems in healthcare and behavioral support often rely on static rule-based or engagement-maximizing heuristics that overlook users' emotional context and ethical constraints. Such approaches risk recommending insensitive or unsafe interventions, especially in domains involving serious mental illness, substance use disorders, or depression. To address this limitation, we propose a Responsible Reinforcement Learning (RRL) framework that integrates emotional and contextual understanding with ethical considerations into the sequential decision-making process. RRL formulates personalization as a Constrained Markov Decision Process (CMDP), where the agent optimizes engagement and adherence while ensuring emotional alignment and ethical safety. We introduce a multi-objective reward function that explicitly balances short-term behavioral engagement with long-term user well-being, and define an emotion-informed state representation that captures fluctuations in emotional readiness, affect, and risk. The proposed architecture can be instantiated with any RL algorithm (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization. Conceptually, this framework operationalizes empathy and responsibility within machine learning policy optimization, bridging safe RL, affective computing and responsible AI. We discuss the implications of this approach for human-centric domains such as behavioral health, education, and digital therapeutics, and outline simulation-based validation paths for future empirical work. This paper aims to initiate a methodological conversation about ethically aligned reinforcement learning for emotionally aware and trustworthy personalization systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.10573v1",
      "published": "2025-11-13T18:09:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10573v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.MA"
      ]
    },
    {
      "title": "Bi-Level Contextual Bandits for Individualized Resource Allocation under Delayed Feedback",
      "authors": [
        "Mohammadsina Almasi",
        "Hadis Anahideh"
      ],
      "abstract": "Equitably allocating limited resources in high-stakes domains-such as education, employment, and healthcare-requires balancing short-term utility with long-term impact, while accounting for delayed outcomes, hidden heterogeneity, and ethical constraints. However, most learning-based allocation frameworks either assume immediate feedback or ignore the complex interplay between individual characteristics and intervention dynamics. We propose a novel bi-level contextual bandit framework for individualized resource allocation under delayed feedback, designed to operate in real-world settings with dynamic populations, capacity constraints, and time-sensitive impact. At the meta level, the model optimizes subgroup-level budget allocations to satisfy fairness and operational constraints. At the base level, it identifies the most responsive individuals within each group using a neural network trained on observational data, while respecting cooldown windows and delayed treatment effects modeled via resource-specific delay kernels. By explicitly modeling temporal dynamics and feedback delays, the algorithm continually refines its policy as new data arrive, enabling more responsive and adaptive decision-making. We validate our approach on two real-world datasets from education and workforce development, showing that it achieves higher cumulative outcomes, better adapts to delay structures, and ensures equitable distribution across subgroups. Our results highlight the potential of delay-aware, data-driven decision-making systems to improve institutional policy and social welfare.",
      "pdf_url": "https://arxiv.org/pdf/2511.10572v1",
      "published": "2025-11-13T18:09:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10572v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Impact of Layer Norm on Memorization and Generalization in Transformers",
      "authors": [
        "Rishi Singhal",
        "Jung-Eun Kim"
      ],
      "abstract": "Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle/later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers.",
      "pdf_url": "https://arxiv.org/pdf/2511.10566v1",
      "published": "2025-11-13T18:07:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10566v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
      "authors": [
        "Huijie Liu",
        "Shuhao Cui",
        "Haoxiang Cao",
        "Shuai Ma",
        "Kai Wu",
        "Guoliang Kang"
      ],
      "abstract": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.",
      "pdf_url": "https://arxiv.org/pdf/2511.10555v1",
      "published": "2025-11-13T17:56:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10555v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Preview, Accept or Discard? A Predictive Low-Motion Interaction Paradigm",
      "authors": [
        "Jose Berengueres"
      ],
      "abstract": "Repetitive strain injury (RSI) affects roughly one in five computer users and remains largely unresolved despite decades of ergonomic mouse redesign. All such devices share a fundamental limitation: they still require fine-motor motion to operate. This work investigates whether predictive, AI-assisted input can reduce that motion by replacing physical pointing with ranked on-screen suggestions. To preserve user agency, we introduce Preview Accept Discard (PAD), a zero-click interaction paradigm that lets users preview predicted GUI targets, cycle through a small set of ranked alternatives, and accept or discard them via key-release timing. We evaluate PAD in two settings: a browser-based email client and a ISO 9241-9 keyboard-prediction task under varying top-3 accuracies. Across both studies, PAD substantially reduces hand motion relative to trackpad use while maintaining comparable task times with the trackpad only when accuracies are similar to those of the best spell-checkers.",
      "pdf_url": "https://arxiv.org/pdf/2511.10532v1",
      "published": "2025-11-13T17:31:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10532v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Rethinking Science in the Age of Artificial Intelligence",
      "authors": [
        "Maksim E. Eren",
        "Dorianis M. Perez"
      ],
      "abstract": "Artificial intelligence (AI) is reshaping how research is conceived, conducted, and communicated across fields from chemistry to biomedicine. This commentary examines how AI is transforming the research workflow. AI systems now help researchers manage the information deluge, filtering the literature, surfacing cross-disciplinary links for ideas and collaborations, generating hypotheses, and designing and executing experiments. These developments mark a shift from AI as a mere computational tool to AI as an active collaborator in science. Yet this transformation demands thoughtful integration and governance. We argue that at this time AI must augment but not replace human judgment in academic workflows such as peer review, ethical evaluation, and validation of results. This paper calls for the deliberate adoption of AI within the scientific practice through policies that promote transparency, reproducibility, and accountability.",
      "pdf_url": "https://arxiv.org/pdf/2511.10524v1",
      "published": "2025-11-13T17:26:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10524v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Say It Differently: Linguistic Styles as Jailbreak Vectors",
      "authors": [
        "Srikant Panda",
        "Avinash Rai"
      ],
      "abstract": "Large Language Models (LLMs) are commonly evaluated for robustness against paraphrased or semantically equivalent jailbreak prompts, yet little attention has been paid to linguistic variation as an attack surface. In this work, we systematically study how linguistic styles such as fear or curiosity can reframe harmful intent and elicit unsafe responses from aligned models. We construct style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 distinct linguistic styles using handcrafted templates and LLM-based rewrites, while preserving semantic intent. Evaluating 16 open- and close-source instruction-tuned models, we find that stylistic reframing increases jailbreak success rates by up to +57 percentage points. Styles such as fearful, curious and compassionate are most effective and contextualized rewrites outperform templated variants.\n  To mitigate this, we introduce a style neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues from user inputs, significantly reducing jailbreak success rates. Our findings reveal a systemic and scaling-resistant vulnerability overlooked in current safety pipelines.",
      "pdf_url": "https://arxiv.org/pdf/2511.10519v1",
      "published": "2025-11-13T17:24:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10519v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025",
      "authors": [
        "Dong-Shan Jian",
        "Xiang Li",
        "Chen-Xu Yan",
        "Hui-Wen Zheng",
        "Zhi-Zhang Bian",
        "You-Le Fang",
        "Sheng-Qi Zhang",
        "Bing-Rui Gong",
        "Ren-Xi He",
        "Jing-Tian Zhang",
        "Ce Meng",
        "Yan-Qing Ma"
      ],
      "abstract": "Olympiad-level physics problem-solving presents a significant challenge for both humans and artificial intelligence (AI), as it requires a sophisticated integration of precise calculation, abstract reasoning, and a fundamental grasp of physical principles. The Chinese Physics Olympiad (CPhO), renowned for its complexity and depth, serves as an ideal and rigorous testbed for these advanced capabilities. In this paper, we introduce LOCA-R (LOgical Chain Augmentation for Reasoning), an improved version of the LOCA framework adapted for complex reasoning, and apply it to the CPhO 2025 theory examination. LOCA-R achieves a near-perfect score of 313 out of 320 points, solidly surpassing the highest-scoring human competitor and significantly outperforming all baseline methods.",
      "pdf_url": "https://arxiv.org/pdf/2511.10515v1",
      "published": "2025-11-13T17:20:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10515v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "physics.ed-ph"
      ]
    },
    {
      "title": "On the Detectability of Active Gradient Inversion Attacks in Federated Learning",
      "authors": [
        "Vincenzo Carletti",
        "Pasquale Foggia",
        "Carlo Mazzocca",
        "Giuseppe Parrella",
        "Mario Vento"
      ],
      "abstract": "One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL. GIAs can be launched by either a passive or an active server. In the latter case, a malicious server manipulates the global model to facilitate data reconstruction. While effective, earlier attacks falling under this category have been demonstrated to be detectable by clients, limiting their real-world applicability. Recently, novel active GIAs have emerged, claiming to be far stealthier than previous approaches. This work provides the first comprehensive analysis of these claims, investigating four state-of-the-art GIAs. We propose novel lightweight client-side detection techniques, based on statistically improbable weight structures and anomalous loss and gradient dynamics. Extensive evaluation across several configurations demonstrates that our methods enable clients to effectively detect active GIAs without any modifications to the FL training protocol.",
      "pdf_url": "https://arxiv.org/pdf/2511.10502v1",
      "published": "2025-11-13T17:06:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10502v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling",
      "authors": [
        "Georgios Chalkiadakis",
        "Charilaos Akasiadis",
        "Gerasimos Koresis",
        "Stergios Plataniots",
        "Leonidas Bakopoulos"
      ],
      "abstract": "This paper provides a comprehensive review of mainly Graph Neural Networks, Deep Reinforcement Learning, and Probabilistic Topic Modeling methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) Machine Learning methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of Graph Neural Networks (GNN). Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of Reinforcement Learning (RL), and in particular that of Multiagent Deep Reinforcement Learning (MADRL). Following, we describe existing relevant game theoretic solution concepts and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes PTM in domains other than that of document analysis and classification. The capability of PTM to estimate unknown underlying distributions can help with tackling heterogeneity and unknown agent beliefs. Finally, we identify certain open challenges specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.",
      "pdf_url": "https://arxiv.org/pdf/2511.10501v1",
      "published": "2025-11-13T17:06:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10501v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes",
      "authors": [
        "Tejas Sudharshan Mathai",
        "Anisa V. Prasad",
        "Xinya Wang",
        "Praveen T. S. Balamuralikrishna",
        "Yan Zhuang",
        "Abhinav Suri",
        "Jianfei Liu",
        "Perry J. Pickhardt",
        "Ronald M. Summers"
      ],
      "abstract": "Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p=0.01) at 4.26 $\\pm$ 8.32 compared to 3.19 $\\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\\pm$ 0.17 and lowest ASSD error of 1.94 $\\pm$ 2.63 mm (p$<$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7\\% sensitivity, and 91.9\\% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.",
      "pdf_url": "https://arxiv.org/pdf/2511.10484v1",
      "published": "2025-11-13T16:51:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10484v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Proceedings of The third international workshop on eXplainable AI for the Arts (XAIxArts)",
      "authors": [
        "Corey Ford",
        "Elizabeth Wilson",
        "Shuoyang Zheng",
        "Gabriel Vigliensoni",
        "Jeba Rezwana",
        "Lanxi Xiao",
        "Michael Clemens",
        "Makayla Lewis",
        "Drew Hemment",
        "Alan Chamberlain",
        "Helen Kennedy",
        "Nick Bryan-Kinns"
      ],
      "abstract": "This third international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts. Workshop held at the 17th ACM Conference on Creativity and Cognition (C&C 2025), online.",
      "pdf_url": "https://arxiv.org/pdf/2511.10482v1",
      "published": "2025-11-13T16:48:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10482v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MM",
        "cs.SD"
      ]
    },
    {
      "title": "Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs",
      "authors": [
        "Changhai Man",
        "Joongun Park",
        "Hanjiang Wu",
        "Huan Xu",
        "Srinivas Sridharan",
        "Tushar Krishna"
      ],
      "abstract": "Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.10480v1",
      "published": "2025-11-13T16:44:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10480v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks",
      "authors": [
        "Yunzhe Xu",
        "Zhuosheng Zhang",
        "Zhe Liu"
      ],
      "abstract": "While prompt optimization has emerged as a critical technique for enhancing language model performance, existing approaches primarily focus on elicitation-based strategies that search for optimal prompts to activate models' capabilities. These methods exhibit fundamental limitations when addressing knowledge-intensive tasks, as they operate within fixed parametric boundaries rather than providing the factual knowledge, terminology precision, and reasoning patterns required in specialized domains. To address these limitations, we propose Knowledge-Provision-based Prompt Optimization (KPPO), a framework that reformulates prompt optimization as systematic knowledge integration rather than potential elicitation. KPPO introduces three key innovations: 1) a knowledge gap filling mechanism for knowledge gap identification and targeted remediation; 2) a batch-wise candidate evaluation approach that considers both performance improvement and distributional stability; 3) an adaptive knowledge pruning strategy that balances performance and token efficiency, reducing up to 29% token usage. Extensive evaluation on 15 knowledge-intensive benchmarks from various domains demonstrates KPPO's superiority over elicitation-based methods, with an average performance improvement of ~6% over the strongest baseline while achieving comparable or lower token consumption. Code at: https://github.com/xyz9911/KPPO.",
      "pdf_url": "https://arxiv.org/pdf/2511.10465v1",
      "published": "2025-11-13T16:33:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10465v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning",
      "authors": [
        "Zihan Gao",
        "Yifei Xu",
        "Jacob Thebault-Spieker"
      ],
      "abstract": "Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.",
      "pdf_url": "https://arxiv.org/pdf/2511.10459v1",
      "published": "2025-11-13T16:26:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10459v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Reasoning About Intent for Ambiguous Requests",
      "authors": [
        "Irina Saparina",
        "Mirella Lapata"
      ],
      "abstract": "Large language models often respond to ambiguous requests by implicitly committing to one interpretation. Intent misunderstandings can frustrate users and create safety risks. To address this, we propose generating multiple interpretation-answer pairs in a single structured response to ambiguous requests. Our models are trained with reinforcement learning and customized reward functions using multiple valid answers as supervision. Experiments on conversational question answering and semantic parsing demonstrate that our method achieves higher coverage of valid answers than baseline approaches. Human evaluation confirms that predicted interpretations are highly aligned with their answers. Our approach promotes transparency with explicit interpretations, achieves efficiency by requiring only one generation step, and supports downstream applications through its structured output format.",
      "pdf_url": "https://arxiv.org/pdf/2511.10453v1",
      "published": "2025-11-13T16:18:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10453v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Non-Monotonic S4F Standpoint Logic",
      "authors": [
        "Piotr Gorczyca",
        "Hannes Strass"
      ],
      "abstract": "Standpoint logics offer unified modal logic-based formalisms for representing multiple heterogeneous viewpoints. At the same time, many non-monotonic reasoning frameworks can be naturally captured using modal logics, in particular using the modal logic S4F. In this work, we propose a novel formalism called S4F Standpoint Logic, which generalises both S4F and standpoint propositional logic and is therefore capable of expressing multi-viewpoint, non-monotonic semantic commitments. We define its syntax and semantics and analyze its computational complexity, obtaining the result that S4F Standpoint Logic is not computationally harder than its constituent logics, whether in monotonic or non-monotonic form. We also outline mechanisms for credulous and sceptical acceptance and illustrate the framework with an example.",
      "pdf_url": "https://arxiv.org/pdf/2511.10449v1",
      "published": "2025-11-13T16:14:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10449v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Completion of partial structures using Patterson maps with the CrysFormer machine learning model",
      "authors": [
        "Tom Pan",
        "Evan Dramko",
        "Mitchell D. Miller",
        "Anastasios Kyrillidis",
        "George N. Phillips"
      ],
      "abstract": "Protein structure determination has long been one of the primary challenges of structural biology, to which deep machine learning (ML)-based approaches have increasingly been applied. However, these ML models generally do not incorporate the experimental measurements directly, such as X-ray crystallographic diffraction data. To this end, we explore an approach that more tightly couples these traditional crystallographic and recent ML-based methods, by training a hybrid 3-d vision transformer and convolutional network on inputs from both domains. We make use of two distinct input constructs / Patterson maps, which are directly obtainable from crystallographic data, and ``partial structure'' template maps derived from predicted structures deposited in the AlphaFold Protein Structure Database with subsequently omitted residues. With these, we predict electron density maps that are then post-processed into atomic models through standard crystallographic refinement processes. Introducing an initial dataset of small protein fragments taken from Protein Data Bank entries and placing them in hypothetical crystal settings, we demonstrate that our method is effective at both improving the phases of the crystallographic structure factors and completing the regions missing from partial structure templates, as well as improving the agreement of the electron density maps with the ground truth atomic structures.",
      "pdf_url": "https://arxiv.org/pdf/2511.10440v1",
      "published": "2025-11-13T16:04:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10440v1",
      "categories": [
        "physics.bio-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration",
      "authors": [
        "Thomas Decker",
        "Volker Tresp",
        "Florian Buettner"
      ],
      "abstract": "Perturbation-based explanations are widely utilized to enhance the transparency of machine-learning models in practice. However, their reliability is often compromised by the unknown model behavior under the specific perturbations used. This paper investigates the relationship between uncertainty calibration - the alignment of model confidence with actual accuracy - and perturbation-based explanations. We show that models systematically produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines global and local explanation quality. To address this, we introduce ReCalX, a novel approach to recalibrate models for improved explanations while preserving their original predictions. Empirical evaluations across diverse models and datasets demonstrate that ReCalX consistently reduces perturbation-specific miscalibration most effectively while enhancing explanation robustness and the identification of globally important input features.",
      "pdf_url": "https://arxiv.org/pdf/2511.10439v1",
      "published": "2025-11-13T16:04:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10439v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Preference Elicitation for Step-Wise Explanations in Logic Puzzles",
      "authors": [
        "Marco Foschini",
        "Marianne Defresne",
        "Emilio Gamba",
        "Bart Bogaerts",
        "Tias Guns"
      ],
      "abstract": "Step-wise explanations can explain logic puzzles and other satisfaction problems by showing how to derive decisions step by step. Each step consists of a set of constraints that derive an assignment to one or more decision variables. However, many candidate explanation steps exist, with different sets of constraints and different decisions they derive. To identify the most comprehensible one, a user-defined objective function is required to quantify the quality of each step. However, defining a good objective function is challenging. Here, interactive preference elicitation methods from the wider machine learning community can offer a way to learn user preferences from pairwise comparisons. We investigate the feasibility of this approach for step-wise explanations and address several limitations that distinguish it from elicitation for standard combinatorial problems. First, because the explanation quality is measured using multiple sub-objectives that can vary a lot in scale, we propose two dynamic normalization techniques to rescale these features and stabilize the learning process. We also observed that many generated comparisons involve similar explanations. For this reason, we introduce MACHOP (Multi-Armed CHOice Perceptron), a novel query generation strategy that integrates non-domination constraints with upper confidence bound-based diversification. We evaluate the elicitation techniques on Sudokus and Logic-Grid puzzles using artificial users, and validate them with a real-user evaluation. In both settings, MACHOP consistently produces higher-quality explanations than the standard approach.",
      "pdf_url": "https://arxiv.org/pdf/2511.10436v1",
      "published": "2025-11-13T16:00:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10436v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Using Certifying Constraint Solvers for Generating Step-wise Explanations",
      "authors": [
        "Ignace Bleukx",
        "Maarten Flippo",
        "Bart Bogaerts",
        "Emir Demirović",
        "Tias Guns"
      ],
      "abstract": "In the field of Explainable Constraint Solving, it is common to explain to a user why a problem is unsatisfiable. A recently proposed method for this is to compute a sequence of explanation steps. Such a step-wise explanation shows individual reasoning steps involving constraints from the original specification, that in the end explain a conflict. However, computing a step-wise explanation is computationally expensive, limiting the scope of problems for which it can be used. We investigate how we can use proofs generated by a constraint solver as a starting point for computing step-wise explanations, instead of computing them step-by-step. More specifically, we define a framework of abstract proofs, in which both proofs and step-wise explanations can be represented. We then propose several methods for converting a proof to a step-wise explanation sequence, with special attention to trimming and simplification techniques to keep the sequence and its individual steps small. Our results show our method significantly speeds up the generation of step-wise explanation sequences, while the resulting step-wise explanation has a quality similar to the current state-of-the-art.",
      "pdf_url": "https://arxiv.org/pdf/2511.10428v1",
      "published": "2025-11-13T15:47:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10428v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Generalizing Analogical Inference from Boolean to Continuous Domains",
      "authors": [
        "Francisco Cunha",
        "Yves Lepage",
        "Zied Bouraoui",
        "Miguel Couceiro"
      ],
      "abstract": "Analogical reasoning is a powerful inductive mechanism, widely used in human cognition and increasingly applied in artificial intelligence. Formal frameworks for analogical inference have been developed for Boolean domains, where inference is provably sound for affine functions and approximately correct for functions close to affine. These results have informed the design of analogy-based classifiers. However, they do not extend to regression tasks or continuous domains. In this paper, we revisit analogical inference from a foundational perspective. We first present a counterexample showing that existing generalization bounds fail even in the Boolean setting. We then introduce a unified framework for analogical reasoning in real-valued domains based on parameterized analogies defined via generalized means. This model subsumes both Boolean classification and regression, and supports analogical inference over continuous functions. We characterize the class of analogy-preserving functions in this setting and derive both worst-case and average-case error bounds under smoothness assumptions. Our results offer a general theory of analogical inference across discrete and continuous domains.",
      "pdf_url": "https://arxiv.org/pdf/2511.10416v1",
      "published": "2025-11-13T15:37:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10416v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Explaining Decentralized Multi-Agent Reinforcement Learning Policies",
      "authors": [
        "Kayla Boggess",
        "Sarit Kraus",
        "Lu Feng"
      ],
      "abstract": "Multi-Agent Reinforcement Learning (MARL) has gained significant interest in recent years, enabling sequential decision-making across multiple agents in various domains. However, most existing explanation methods focus on centralized MARL, failing to address the uncertainty and nondeterminism inherent in decentralized settings. We propose methods to generate policy summarizations that capture task ordering and agent cooperation in decentralized MARL policies, along with query-based explanations for When, Why Not, and What types of user queries about specific agent behaviors. We evaluate our approach across four MARL domains and two decentralized MARL algorithms, demonstrating its generalizability and computational efficiency. User studies show that our summarizations and explanations significantly improve user question-answering performance and enhance subjective ratings on metrics such as understanding and satisfaction.",
      "pdf_url": "https://arxiv.org/pdf/2511.10409v1",
      "published": "2025-11-13T15:30:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10409v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation",
      "authors": [
        "Mingxing Peng",
        "Ruoyu Yao",
        "Xusen Guo",
        "Jun Ma"
      ],
      "abstract": "Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.",
      "pdf_url": "https://arxiv.org/pdf/2511.10403v1",
      "published": "2025-11-13T15:23:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10403v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance",
      "authors": [
        "Lifan Zheng",
        "Jiawei Chen",
        "Qinghong Yin",
        "Jingyuan Zhang",
        "Xinyi Zeng",
        "Yu Tian"
      ],
      "abstract": "Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.",
      "pdf_url": "https://arxiv.org/pdf/2511.10400v1",
      "published": "2025-11-13T15:20:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10400v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
      "authors": [
        "Yunpeng Zhai",
        "Shuchang Tao",
        "Cheng Chen",
        "Anni Zou",
        "Ziqian Chen",
        "Qingxu Fu",
        "Shinji Mai",
        "Li Yu",
        "Jiaji Deng",
        "Zouying Cao",
        "Zhaoyang Liu",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.",
      "pdf_url": "https://arxiv.org/pdf/2511.10395v1",
      "published": "2025-11-13T15:14:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10395v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Enhancing Kernel Power K-means: Scalable and Robust Clustering with Random Fourier Features and Possibilistic Method",
      "authors": [
        "Yixi Chen",
        "Weixuan Liang",
        "Tianrui Liu",
        "Jun-Jie Huang",
        "Ao Li",
        "Xueling Zhu",
        "Xinwang Liu"
      ],
      "abstract": "Kernel power $k$-means (KPKM) leverages a family of means to mitigate local minima issues in kernel $k$-means. However, KPKM faces two key limitations: (1) the computational burden of the full kernel matrix restricts its use on extensive data, and (2) the lack of authentic centroid-sample assignment learning reduces its noise robustness. To overcome these challenges, we propose RFF-KPKM, introducing the first approximation theory for applying random Fourier features (RFF) to KPKM. RFF-KPKM employs RFF to generate efficient, low-dimensional feature maps, bypassing the need for the whole kernel matrix. Crucially, we are the first to establish strong theoretical guarantees for this combination: (1) an excess risk bound of $\\mathcal{O}(\\sqrt{k^3/n})$, (2) strong consistency with membership values, and (3) a $(1+\\varepsilon)$ relative error bound achievable using the RFF of dimension $\\mathrm{poly}(\\varepsilon^{-1}\\log k)$. Furthermore, to improve robustness and the ability to learn multiple kernels, we propose IP-RFF-MKPKM, an improved possibilistic RFF-based multiple kernel power $k$-means. IP-RFF-MKPKM ensures the scalability of MKPKM via RFF and refines cluster assignments by combining the merits of the possibilistic membership and fuzzy membership. Experiments on large-scale datasets demonstrate the superior efficiency and clustering accuracy of the proposed methods compared to the state-of-the-art alternatives.",
      "pdf_url": "https://arxiv.org/pdf/2511.10392v1",
      "published": "2025-11-13T15:12:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10392v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns",
      "authors": [
        "Jiarui Zhang",
        "Yuliang Liu",
        "Zijun Wu",
        "Guosheng Pang",
        "Zhili Ye",
        "Yupei Zhong",
        "Junteng Ma",
        "Tao Wei",
        "Haiyang Xu",
        "Weikai Chen",
        "Zeen Wang",
        "Qiangjun Ji",
        "Fanxi Zhou",
        "Qi Zhang",
        "Yuanrui Hu",
        "Jiahao Liu",
        "Zhang Li",
        "Ziyang Zhang",
        "Qiang Liu",
        "Xiang Bai"
      ],
      "abstract": "Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.",
      "pdf_url": "https://arxiv.org/pdf/2511.10390v1",
      "published": "2025-11-13T15:12:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10390v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Simulating Misinformation Propagation in Social Networks using Large Language Models",
      "authors": [
        "Raj Gaurav Maurya",
        "Vaibhav Shukla",
        "Raj Abhijit Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "abstract": "Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.",
      "pdf_url": "https://arxiv.org/pdf/2511.10384v1",
      "published": "2025-11-13T15:01:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10384v1",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    },
    {
      "title": "SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation",
      "authors": [
        "Kai-Hendrik Cohrs",
        "Zuzanna Osika",
        "Maria Gonzalez-Calabuig",
        "Vishal Nedungadi",
        "Ruben Cartuyvels",
        "Steffen Knoblauch",
        "Joppe Massant",
        "Shruti Nath",
        "Patrick Ebel",
        "Vasileios Sitokonstantinou"
      ],
      "abstract": "Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.",
      "pdf_url": "https://arxiv.org/pdf/2511.10370v1",
      "published": "2025-11-13T14:48:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10370v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "DermAI: Clinical dermatology acquisition through quality-driven image collection for AI classification in mobile",
      "authors": [
        "Thales Bezerra",
        "Emanoel Thyago",
        "Kelvin Cunha",
        "Rodrigo Abreu",
        "Fábio Papais",
        "Francisco Mauro",
        "Natália Lopes",
        "Érico Medeiros",
        "Jéssica Guido",
        "Shirley Cruz",
        "Paulo Borba",
        "Tsang Ing Ren"
      ],
      "abstract": "AI-based dermatology adoption remains limited by biased datasets, variable image quality, and limited validation. We introduce DermAI, a lightweight, smartphone-based application that enables real-time capture, annotation, and classification of skin lesions during routine consultations. Unlike prior dermoscopy-focused tools, DermAI performs on-device quality checks, and local model adaptation. The DermAI clinical dataset, encompasses a wide range of skin tones, ethinicity and source devices. In preliminary experiments, models trained on public datasets failed to generalize to our samples, while fine-tuning with local data improved performance. These results highlight the importance of standardized, diverse data collection aligned with healthcare needs and oriented to machine learning development.",
      "pdf_url": "https://arxiv.org/pdf/2511.10367v1",
      "published": "2025-11-13T14:48:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10367v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SITA: A Framework for Structure-to-Instance Theorem Autoformalization",
      "authors": [
        "Chenyi Li",
        "Wanli Ma",
        "Zichen Wang",
        "Zaiwen Wen"
      ],
      "abstract": "While large language models (LLMs) have shown progress in mathematical reasoning, they still face challenges in formalizing theorems that arise from instantiating abstract structures in concrete settings. With the goal of auto-formalizing mathematical results at the research level, we develop a framework for structure-to-instance theorem autoformalization (SITA), which systematically bridges the gap between abstract mathematical theories and their concrete applications in Lean proof assistant. Formalized abstract structures are treated as modular templates that contain definitions, assumptions, operations, and theorems. These templates serve as reusable guides for the formalization of concrete instances. Given a specific instantiation, we generate corresponding Lean definitions and instance declarations, integrate them using Lean's typeclass mechanism, and construct verified theorems by checking structural assumptions. We incorporate LLM-based generation with feedback-guided refinement to ensure both automation and formal correctness. Experiments on a dataset of optimization problems demonstrate that SITA effectively formalizes diverse instances grounded in abstract structures.",
      "pdf_url": "https://arxiv.org/pdf/2511.10356v1",
      "published": "2025-11-13T14:33:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10356v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Massively Parallel Proof-Number Search for Impartial Games and Beyond",
      "authors": [
        "Tomáš Čížek",
        "Martin Balko",
        "Martin Schmid"
      ],
      "abstract": "Proof-Number Search is a best-first search algorithm with many successful applications, especially in game solving. As large-scale computing clusters become increasingly accessible, parallelization is a natural way to accelerate computation. However, existing parallel versions of Proof-Number Search are known to scale poorly on many CPU cores. Using two parallelized levels and shared information among workers, we present the first massively parallel version of Proof-Number Search that scales efficiently even on a large number of CPUs. We apply our solver, enhanced with Grundy numbers for reducing game trees, to the Sprouts game, a case study motivated by the long-standing Sprouts Conjecture. Our solver achieves a significantly improved 332.9$\\times$ speedup when run on 1024 cores, enabling it to outperform the state-of-the-art Sprouts solver GLOP by four orders of magnitude in runtime and to generate proofs 1,000$\\times$ more complex. Despite exponential growth in game tree size, our solver verified the Sprouts Conjecture for 42 new positions, nearly doubling the number of known outcomes.",
      "pdf_url": "https://arxiv.org/pdf/2511.10339v1",
      "published": "2025-11-13T14:12:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10339v1",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.GT"
      ]
    },
    {
      "title": "BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages",
      "authors": [
        "Guduru Manoj",
        "Neel Prabhanjan Rachamalla",
        "Ashish Kulkarni",
        "Gautam Rajeev",
        "Jay Piplodiya",
        "Arul Menezes",
        "Shaharukh Khan",
        "Souvik Rana",
        "Manya Sah",
        "Chandra Khatri",
        "Shubham Agarwal"
      ],
      "abstract": "In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora.",
      "pdf_url": "https://arxiv.org/pdf/2511.10338v1",
      "published": "2025-11-13T14:12:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10338v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision",
      "authors": [
        "Yu Deng",
        "Baozhu Zhao",
        "Junyan Su",
        "Xiaohan Zhang",
        "Qi Liu"
      ],
      "abstract": "Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.",
      "pdf_url": "https://arxiv.org/pdf/2511.10316v1",
      "published": "2025-11-13T13:51:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10316v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking Visual Information Processing in Multimodal LLMs",
      "authors": [
        "Dongwan Kim",
        "Viresh Ranjan",
        "Takashi Nagata",
        "Arnab Dhua",
        "Amit Kumar K C"
      ],
      "abstract": "Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.",
      "pdf_url": "https://arxiv.org/pdf/2511.10301v1",
      "published": "2025-11-13T13:36:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10301v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models",
      "authors": [
        "Zhengtao Zou",
        "Ya Gao",
        "Jiarui Guan",
        "Bin Li",
        "Pekka Marttinen"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.",
      "pdf_url": "https://arxiv.org/pdf/2511.10292v1",
      "published": "2025-11-13T13:29:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10292v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Verification: Abductive Explanations for Post-AI Assessment of Privacy Leakage",
      "authors": [
        "Belona Sonna",
        "Alban Grastien",
        "Claire Benn"
      ],
      "abstract": "Privacy leakage in AI-based decision processes poses significant risks, particularly when sensitive information can be inferred. We propose a formal framework to audit privacy leakage using abductive explanations, which identifies minimal sufficient evidence justifying model decisions and determines whether sensitive information disclosed. Our framework formalizes both individual and system-level leakage, introducing the notion of Potentially Applicable Explanations (PAE) to identify individuals whose outcomes can shield those with sensitive features. This approach provides rigorous privacy guarantees while producing human understandable explanations, a key requirement for auditing tools. Experimental evaluation on the German Credit Dataset illustrates how the importance of sensitive literal in the model decision process affects privacy leakage. Despite computational challenges and simplifying assumptions, our results demonstrate that abductive reasoning enables interpretable privacy auditing, offering a practical pathway to reconcile transparency, model interpretability, and privacy preserving in AI decision-making.",
      "pdf_url": "https://arxiv.org/pdf/2511.10284v1",
      "published": "2025-11-13T13:14:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10284v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Torch-Uncertainty: A Deep Learning Framework for Uncertainty Quantification",
      "authors": [
        "Adrien Lafage",
        "Olivier Laurent",
        "Firas Gabetni",
        "Gianni Franchi"
      ],
      "abstract": "Deep Neural Networks (DNNs) have demonstrated remarkable performance across various domains, including computer vision and natural language processing. However, they often struggle to accurately quantify the uncertainty of their predictions, limiting their broader adoption in critical real-world applications. Uncertainty Quantification (UQ) for Deep Learning seeks to address this challenge by providing methods to improve the reliability of uncertainty estimates. Although numerous techniques have been proposed, a unified tool offering a seamless workflow to evaluate and integrate these methods remains lacking. To bridge this gap, we introduce Torch-Uncertainty, a PyTorch and Lightning-based framework designed to streamline DNN training and evaluation with UQ techniques and metrics. In this paper, we outline the foundational principles of our library and present comprehensive experimental results that benchmark a diverse set of UQ methods across classification, segmentation, and regression tasks. Our library is available at https://github.com/ENSTA-U2IS-AI/Torch-Uncertainty",
      "pdf_url": "https://arxiv.org/pdf/2511.10282v1",
      "published": "2025-11-13T13:12:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10282v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "FactGuard: Event-Centric and Commonsense-Guided Fake News Detection",
      "authors": [
        "Jing He",
        "Han Zhang",
        "Yuanhui Xiao",
        "Wei Guo",
        "Shaowen Yao",
        "Renyang Liu"
      ],
      "abstract": "Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.",
      "pdf_url": "https://arxiv.org/pdf/2511.10281v1",
      "published": "2025-11-13T13:11:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.10281v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    }
  ]
}