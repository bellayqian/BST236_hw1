{
  "last_updated": "2026-01-22T00:57:41.672946",
  "papers": [
    {
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "authors": [
        "Sangbeom Lim",
        "Seoung Wug Oh",
        "Jiahui Huang",
        "Heeji Yoon",
        "Seungryong Kim",
        "Joon-Young Lee"
      ],
      "abstract": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
      "pdf_url": "https://arxiv.org/pdf/2601.14255v1",
      "published": "2026-01-20T18:59:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14255v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "APEX-Agents",
      "authors": [
        "Bertie Vidgen",
        "Austin Mann",
        "Abby Fennelly",
        "John Wright Stanly",
        "Lucas Rothman",
        "Marco Burstein",
        "Julien Benchek",
        "David Ostrofsky",
        "Anirudh Ravichandran",
        "Debnil Sur",
        "Neel Venugopal",
        "Alannah Hsia",
        "Isaac Robinson",
        "Calix Huang",
        "Olivia Varones",
        "Daniyal Khan",
        "Michael Haines",
        "Zach Richards",
        "Chirag Mahapatra",
        "Brendan Foody",
        "Osvald Nitski"
      ],
      "abstract": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.",
      "pdf_url": "https://arxiv.org/pdf/2601.14242v1",
      "published": "2026-01-20T18:53:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14242v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration",
      "authors": [
        "LSST Dark Energy Science Collaboration",
        "Eric Aubourg",
        "Camille Avestruz",
        "Matthew R. Becker",
        "Biswajit Biswas",
        "Rahul Biswas",
        "Boris Bolliet",
        "Adam S. Bolton",
        "Clecio R. Bom",
        "Raphaël Bonnet-Guerrini",
        "Alexandre Boucaud",
        "Jean-Eric Campagne",
        "Chihway Chang",
        "Aleksandra Ćiprijanović",
        "Johann Cohen-Tanugi",
        "Michael W. Coughlin",
        "John Franklin Crenshaw",
        "Juan C. Cuevas-Tello",
        "Juan de Vicente",
        "Seth W. Digel",
        "Steven Dillmann",
        "Mariano Javier de León Dominguez Romero",
        "Alex Drlica-Wagner",
        "Sydney Erickson",
        "Alexander T. Gagliano",
        "Christos Georgiou",
        "Aritra Ghosh",
        "Matthew Grayling",
        "Kirill A. Grishin",
        "Alan Heavens",
        "Lindsay R. House",
        "Mustapha Ishak",
        "Wassim Kabalan",
        "Arun Kannawadi",
        "François Lanusse",
        "C. Danielle Leonard",
        "Pierre-François Léget",
        "Michelle Lochner",
        "Yao-Yuan Mao",
        "Peter Melchior",
        "Grant Merz",
        "Martin Millon",
        "Anais Möller",
        "Gautham Narayan",
        "Yuuki Omori",
        "Hiranya Peiris",
        "Laurence Perreault-Levasseur",
        "Andrés A. Plazas Malagón",
        "Nesar Ramachandra",
        "Benjamin Remy",
        "Cécile Roucelle",
        "Jaime Ruiz-Zapatero",
        "Stefan Schuldt",
        "Ignacio Sevilla-Noarbe",
        "Ved G. Shah",
        "Tjitske Starkenburg",
        "Stephen Thorp",
        "Laura Toribio San Cipriano",
        "Tilman Tröster",
        "Roberto Trotta",
        "Padma Venkatraman",
        "Amanda Wasserman",
        "Tim White",
        "Justine Zeghal",
        "Tianqing Zhang",
        "Yuanyuan Zhang"
      ],
      "abstract": "The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC's primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.",
      "pdf_url": "https://arxiv.org/pdf/2601.14235v1",
      "published": "2026-01-20T18:46:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14235v1",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Q-learning with Adjoint Matching",
      "authors": [
        "Qiyang Li",
        "Sergey Levine"
      ],
      "abstract": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.",
      "pdf_url": "https://arxiv.org/pdf/2601.14234v1",
      "published": "2026-01-20T18:45:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14234v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ]
    },
    {
      "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
      "authors": [
        "Egor Cherepanov",
        "Daniil Zelezetsky",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.",
      "pdf_url": "https://arxiv.org/pdf/2601.14232v1",
      "published": "2026-01-20T18:44:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14232v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems",
      "authors": [
        "Yiyang Wang",
        "Yiqiao Jin",
        "Alex Cabral",
        "Josiah Hester"
      ],
      "abstract": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.",
      "pdf_url": "https://arxiv.org/pdf/2601.14230v1",
      "published": "2026-01-20T18:44:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14230v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
      "authors": [
        "Matthew Y. R. Yang",
        "Hao Bai",
        "Ian Wu",
        "Gene Yang",
        "Amrith Setlur",
        "Aviral Kumar"
      ],
      "abstract": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.",
      "pdf_url": "https://arxiv.org/pdf/2601.14209v1",
      "published": "2026-01-20T18:15:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14209v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
      "authors": [
        "Xiaofang Yang",
        "Lijun Li",
        "Heng Zhou",
        "Tong Zhu",
        "Xiaoye Qu",
        "Yuchen Fan",
        "Qianshan Wei",
        "Rui Ye",
        "Li Kang",
        "Yiran Qin",
        "Zhiqiang Kou",
        "Daizong Liu",
        "Qi Li",
        "Ning Ding",
        "Siheng Chen",
        "Jing Shao"
      ],
      "abstract": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.",
      "pdf_url": "https://arxiv.org/pdf/2601.14192v1",
      "published": "2026-01-20T17:51:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14192v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A model of errors in transformers",
      "authors": [
        "Suvrat Raju",
        "Praneeth Netrapalli"
      ],
      "abstract": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.",
      "pdf_url": "https://arxiv.org/pdf/2601.14175v1",
      "published": "2026-01-20T17:27:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14175v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "hep-th"
      ]
    },
    {
      "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum",
      "authors": [
        "Víctor Yeste",
        "Paolo Rosso"
      ],
      "abstract": "We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (\"does any value appear?\") and show that it is learnable from single sentences (positive-class F1 $\\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.",
      "pdf_url": "https://arxiv.org/pdf/2601.14172v1",
      "published": "2026-01-20T17:25:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14172v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
      "authors": [
        "Qianli Ma",
        "Chang Guo",
        "Zhiheng Tian",
        "Siyu Wang",
        "Jipeng Xiao",
        "Yuanhao Yue",
        "Zhipeng Zhang"
      ],
      "abstract": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.",
      "pdf_url": "https://arxiv.org/pdf/2601.14171v1",
      "published": "2026-01-20T17:23:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14171v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law",
      "authors": [
        "Ali Hamza Bashir",
        "Muhammad Rehan Khalid",
        "Kostadin Cvejoski",
        "Jana Birr",
        "Jule Berghaus",
        "Armin Berger",
        "Sandra Halscheidt",
        "Christian Temath",
        "Rafet Sifa",
        "David Berghaus"
      ],
      "abstract": "Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.",
      "pdf_url": "https://arxiv.org/pdf/2601.14160v1",
      "published": "2026-01-20T17:11:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14160v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
      "authors": [
        "Bruno Sienkiewicz",
        "Łukasz Neumann",
        "Mateusz Modrzejewski"
      ],
      "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
      "pdf_url": "https://arxiv.org/pdf/2601.14157v1",
      "published": "2026-01-20T17:04:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14157v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery",
      "authors": [
        "Shubham Pandey",
        "Bhavin Jawade",
        "Srirangaraj Setlur",
        "Venu Govindaraju",
        "Kenneth Seastedt"
      ],
      "abstract": "Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.",
      "pdf_url": "https://arxiv.org/pdf/2601.14154v1",
      "published": "2026-01-20T16:58:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14154v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
      "authors": [
        "Hyunjong Ok",
        "Jaeho Lee"
      ],
      "abstract": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.",
      "pdf_url": "https://arxiv.org/pdf/2601.14152v1",
      "published": "2026-01-20T16:54:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14152v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic",
      "authors": [
        "Saad Mankarious",
        "Aya Zirikly"
      ],
      "abstract": "Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.",
      "pdf_url": "https://arxiv.org/pdf/2601.14124v1",
      "published": "2026-01-20T16:21:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14124v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Riemannian Liquid Spatio-Temporal Graph Network",
      "authors": [
        "Liangsi Lu",
        "Jingchao Wang",
        "Zhaorong Dai",
        "Hanqian Liu",
        "Yang Shi"
      ],
      "abstract": "Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io",
      "pdf_url": "https://arxiv.org/pdf/2601.14115v1",
      "published": "2026-01-20T16:09:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14115v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping",
      "authors": [
        "Shi-Shun Chen",
        "Xiao-Yang Li",
        "Enrico Zio"
      ],
      "abstract": "Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.",
      "pdf_url": "https://arxiv.org/pdf/2601.14099v1",
      "published": "2026-01-20T15:58:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14099v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems",
      "authors": [
        "Benedikt Hartl",
        "Léo Pio-Lopez",
        "Chris Fields",
        "Michael Levin"
      ],
      "abstract": "The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.",
      "pdf_url": "https://arxiv.org/pdf/2601.14096v1",
      "published": "2026-01-20T15:57:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14096v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems",
      "authors": [
        "Hossein Naderi",
        "Alireza Shojaei",
        "Lifu Huang",
        "Philip Agee",
        "Kereshmeh Afsari",
        "Abiola Akanmu"
      ],
      "abstract": "Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.",
      "pdf_url": "https://arxiv.org/pdf/2601.14091v1",
      "published": "2026-01-20T15:54:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14091v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "'1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators",
      "authors": [
        "Ruichi Han",
        "Yizhi Chen",
        "Tong Lei",
        "Jordi Altayo Gonzalez",
        "Ahmed Hemani"
      ],
      "abstract": "Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\\% BT reduction compared to 20.42% of precise implementation.",
      "pdf_url": "https://arxiv.org/pdf/2601.14087v1",
      "published": "2026-01-20T15:47:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14087v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Two-Stream temporal transformer for video action classification",
      "authors": [
        "Nattapong Kurpukdee",
        "Adrian G. Bors"
      ],
      "abstract": "Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.",
      "pdf_url": "https://arxiv.org/pdf/2601.14086v1",
      "published": "2026-01-20T15:47:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14086v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning",
      "authors": [
        "Abdurrahim Yilmaz",
        "Ozan Erdem",
        "Ece Gokyayla",
        "Ayda Acar",
        "Burc Bugra Dagtas",
        "Dilara Ilhan Erdil",
        "Gulsum Gencoglan",
        "Burak Temelkuran"
      ],
      "abstract": "Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.",
      "pdf_url": "https://arxiv.org/pdf/2601.14084v1",
      "published": "2026-01-20T15:44:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14084v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management",
      "authors": [
        "Nattapong Kurpukdee",
        "Adrian G. Bors"
      ],
      "abstract": "Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.",
      "pdf_url": "https://arxiv.org/pdf/2601.14069v1",
      "published": "2026-01-20T15:25:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14069v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs",
      "authors": [
        "Mohsinul Kabir",
        "Tasnim Ahmed",
        "Md Mezbaur Rahman",
        "Shaoxiong Ji",
        "Hassan Alhuzali",
        "Sophia Ananiadou"
      ],
      "abstract": "Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.",
      "pdf_url": "https://arxiv.org/pdf/2601.14063v1",
      "published": "2026-01-20T15:21:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14063v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion",
      "authors": [
        "Andrea Rigo",
        "Luca Stornaiuolo",
        "Weijie Wang",
        "Mauro Martino",
        "Bruno Lepri",
        "Nicu Sebe"
      ],
      "abstract": "We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.",
      "pdf_url": "https://arxiv.org/pdf/2601.14056v1",
      "published": "2026-01-20T15:13:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14056v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI",
      "authors": [
        "Andrea Protani",
        "Marc Molina Van Den Bosch",
        "Lorenzo Giusti",
        "Heloisa Barbosa Da Silva",
        "Paolo Cacace",
        "Albert Sund Aillet",
        "Miguel Angel Gonzalez Ballester",
        "Friedhelm Hummel",
        "Luigi Serio"
      ],
      "abstract": "Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.",
      "pdf_url": "https://arxiv.org/pdf/2601.14055v1",
      "published": "2026-01-20T15:13:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14055v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems",
      "authors": [
        "Badri N. Patro",
        "Vijay S. Agneeswaran"
      ],
      "abstract": "The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.",
      "pdf_url": "https://arxiv.org/pdf/2601.14053v1",
      "published": "2026-01-20T15:06:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14053v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MA",
        "eess.IV"
      ]
    },
    {
      "title": "Kakugo: Distillation of Low-Resource Languages into Small Language Models",
      "authors": [
        "Peter Devine",
        "Mardhiyah Sanni",
        "Farid Adilazuarda",
        "Julieta Gil Loizaga",
        "Barry Haddow"
      ],
      "abstract": "We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.",
      "pdf_url": "https://arxiv.org/pdf/2601.14051v1",
      "published": "2026-01-20T15:05:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14051v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure",
      "authors": [
        "Alexey V. Osipov",
        "Nikolay N. Osipov"
      ],
      "abstract": "Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.",
      "pdf_url": "https://arxiv.org/pdf/2601.14047v1",
      "published": "2026-01-20T15:01:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14047v1",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA",
        "cs.SI",
        "econ.TH"
      ]
    },
    {
      "title": "Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants",
      "authors": [
        "Yunhe Wang",
        "Kai Han",
        "Huiling Zhen",
        "Yuchuan Tian",
        "Hanting Chen",
        "Yongbing Huang",
        "Yufei Cui",
        "Yingte Shu",
        "Shan Gao",
        "Ismail Elezi",
        "Roy Vaughan Miles",
        "Songcen Xu",
        "Feng Wen",
        "Chao Xu",
        "Sinan Zeng",
        "Dacheng Tao"
      ],
      "abstract": "The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.",
      "pdf_url": "https://arxiv.org/pdf/2601.14041v1",
      "published": "2026-01-20T14:58:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14041v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation",
      "authors": [
        "Wesam Moustafa",
        "Hossam Elsafty",
        "Helen Schneider",
        "Lorenz Sparrenberg",
        "Rafet Sifa"
      ],
      "abstract": "Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.",
      "pdf_url": "https://arxiv.org/pdf/2601.14039v1",
      "published": "2026-01-20T14:57:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14039v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
      "authors": [
        "Junqi Liu",
        "Zihao Zhou",
        "Zekai Zhu",
        "Marco Dos Santos",
        "Weikun He",
        "Jiawei Liu",
        "Ran Wang",
        "Yunzhou Xie",
        "Junqiao Zhao",
        "Qiufeng Wang",
        "Lihong Zhi",
        "Jia Li",
        "Wenda Li"
      ],
      "abstract": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.",
      "pdf_url": "https://arxiv.org/pdf/2601.14027v1",
      "published": "2026-01-20T14:51:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14027v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment",
      "authors": [
        "Rodrigo Pereira David",
        "Luciano Araujo Dourado Filho",
        "Daniel Marques da Silva",
        "João Alfredo Cal-Braz"
      ],
      "abstract": "Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.",
      "pdf_url": "https://arxiv.org/pdf/2601.14022v1",
      "published": "2026-01-20T14:43:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14022v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting",
      "authors": [
        "Youngmoon Jung",
        "Myunghun Jung",
        "Joon-Young Yang",
        "Yong-Hyeok Lee",
        "Jaeyoung Roh",
        "Hoon-Young Cho"
      ],
      "abstract": "Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings (\"prefixes\"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.",
      "pdf_url": "https://arxiv.org/pdf/2601.14012v1",
      "published": "2026-01-20T14:30:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.14012v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification",
      "authors": [
        "Youngmoon Jung",
        "Joon-Young Yang",
        "Ju-ho Kim",
        "Jaeyoung Roh",
        "Chang Woo Han",
        "Hoon-Young Cho"
      ],
      "abstract": "Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.",
      "pdf_url": "https://arxiv.org/pdf/2601.13999v1",
      "published": "2026-01-20T14:20:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13999v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch",
      "authors": [
        "Mingyuan Chi"
      ],
      "abstract": "Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \\torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \\textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\\mathcal{O}(1)$ computational graph nodes (for autograd) and $\\mathcal{O}(\\text{nnz})$ memory -- independent of solver iterations. \\torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.",
      "pdf_url": "https://arxiv.org/pdf/2601.13994v1",
      "published": "2026-01-20T14:06:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13994v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "\"The Whole Is Greater Than the Sum of Its Parts\": A Compatibility-Aware Multi-Teacher CoT Distillation Framework",
      "authors": [
        "Jin Cui",
        "Jiaqi Guo",
        "Jiepeng Zhou",
        "Ruixuan Yang",
        "Jiayi Lu",
        "Jiajun Xu",
        "Jiangcheng Song",
        "Boran Zhao",
        "Pengju Ren"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect \"epiphany moments\" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.",
      "pdf_url": "https://arxiv.org/pdf/2601.13992v1",
      "published": "2026-01-20T14:05:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13992v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval",
      "authors": [
        "Joaquín Polonuer",
        "Lucas Vittor",
        "Iñaki Arango",
        "Ayush Noori",
        "David A. Clifton",
        "Luciano Del Corro",
        "Marinka Zitnik"
      ],
      "abstract": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.",
      "pdf_url": "https://arxiv.org/pdf/2601.13969v1",
      "published": "2026-01-20T13:46:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13969v1",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning",
      "authors": [
        "Cheol-Hui Lee",
        "Hwa-Yeon Lee",
        "Dong-Joo Kim"
      ],
      "abstract": "The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\\% and 8.80\\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\\% probability for sleep stage classification and Crop \\& Resize with a 77\\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \\href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.",
      "pdf_url": "https://arxiv.org/pdf/2601.13964v1",
      "published": "2026-01-20T13:38:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13964v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models",
      "authors": [
        "Nikita Kuzmin",
        "Songting Liu",
        "Kong Aik Lee",
        "Eng Siong Chng"
      ],
      "abstract": "Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.",
      "pdf_url": "https://arxiv.org/pdf/2601.13948v1",
      "published": "2026-01-20T13:23:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13948v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning",
      "authors": [
        "Hongbo Bai",
        "Yujin Zhou",
        "Yile Wu",
        "Chi-Min Chan",
        "Pengcheng Wen",
        "Kunhao Pan",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.",
      "pdf_url": "https://arxiv.org/pdf/2601.13942v1",
      "published": "2026-01-20T13:18:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13942v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization",
      "authors": [
        "Heyang Zhou",
        "JiaJia Chen",
        "Xiaolu Chen",
        "Jie Bao",
        "Zhen Chen",
        "Yong Liao"
      ],
      "abstract": "As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a \"diverge-then-converge\" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.",
      "pdf_url": "https://arxiv.org/pdf/2601.13938v1",
      "published": "2026-01-20T13:13:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13938v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Asymmetric regularization mechanism for GAN training with Variational Inequalities",
      "authors": [
        "Spyridon C. Giagtzoglou",
        "Mark H. M. Winands",
        "Barbara Franci"
      ],
      "abstract": "We formulate the training of generative adversarial networks (GANs) as a Nash equilibrium seeking problem. To stabilize the training process and find a Nash equilibrium, we propose an asymmetric regularization mechanism based on the classic Tikhonov step and on a novel zero-centered gradient penalty. Under smoothness and a local identifiability condition induced by a Gauss-Newton Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for the regularized operator. These constants ensure last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations on an academic example show that, even when strong monotonicity cannot be achieved, the asymmetric regularization is enough to converge to an equilibrium and stabilize the trajectory.",
      "pdf_url": "https://arxiv.org/pdf/2601.13920v1",
      "published": "2026-01-20T12:50:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13920v1",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation",
      "authors": [
        "Jaeyoung Moon",
        "Youjin Choi",
        "Yucheon Park",
        "David Melhart",
        "Georgios N. Yannakakis",
        "Kyung-Joong Kim"
      ],
      "abstract": "Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.",
      "pdf_url": "https://arxiv.org/pdf/2601.13904v1",
      "published": "2026-01-20T12:30:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13904v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography",
      "authors": [
        "Ankita Joshi",
        "Ashutosh Sharma",
        "Anoushkrit Goel",
        "Ranjeet Ranjan Jha",
        "Chirag Ahuja",
        "Arnav Bhavsar",
        "Aditya Nigam"
      ],
      "abstract": "Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.",
      "pdf_url": "https://arxiv.org/pdf/2601.13897v1",
      "published": "2026-01-20T12:26:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13897v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3",
      "authors": [
        "Xu Zhang",
        "Danyang Li",
        "Yingjie Xia",
        "Xiaohang Dong",
        "Hualong Yu",
        "Jianye Wang",
        "Qicheng Li"
      ],
      "abstract": "Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.",
      "pdf_url": "https://arxiv.org/pdf/2601.13895v1",
      "published": "2026-01-20T12:25:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13895v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems",
      "authors": [
        "Hong Su"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.",
      "pdf_url": "https://arxiv.org/pdf/2601.13887v1",
      "published": "2026-01-20T12:00:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13887v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores",
      "authors": [
        "Esma Balkır",
        "Alice Pernthaller",
        "Marco Basaldella",
        "José Hernández-Orallo",
        "Nigel Collier"
      ],
      "abstract": "Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.",
      "pdf_url": "https://arxiv.org/pdf/2601.13885v1",
      "published": "2026-01-20T11:59:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13885v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health",
      "authors": [
        "Ye Tian",
        "Zihao Wang",
        "Onat Gungor",
        "Xiaoran Fan",
        "Tajana Rosing"
      ],
      "abstract": "Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.",
      "pdf_url": "https://arxiv.org/pdf/2601.13880v1",
      "published": "2026-01-20T11:51:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.13880v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}