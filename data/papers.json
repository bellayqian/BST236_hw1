{
  "last_updated": "2025-08-12T00:53:19.628027",
  "papers": [
    {
      "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion",
      "authors": [
        "Sofiane Bouaziz",
        "Adel Hafiane",
        "Raphael Canals",
        "Rachid Nedjai"
      ],
      "abstract": "Urbanization, climate change, and agricultural stress are increasing the\ndemand for precise and timely environmental monitoring. Land Surface\nTemperature (LST) is a key variable in this context and is retrieved from\nremote sensing satellites. However, these systems face a trade-off between\nspatial and temporal resolution. While spatio-temporal fusion methods offer\npromising solutions, few have addressed the estimation of daily LST at 10 m\nresolution. In this study, we present WGAST, a Weakly-Supervised Generative\nNetwork for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra\nMODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning\nframework designed for this task. It adopts a conditional generative\nadversarial architecture, with a generator composed of four stages: feature\nextraction, fusion, LST reconstruction, and noise suppression. The first stage\nemploys a set of encoders to extract multi-level latent representations from\nthe inputs, which are then fused in the second stage using cosine similarity,\nnormalization, and temporal attention mechanisms. The third stage decodes the\nfused features into high-resolution LST, followed by a Gaussian filter to\nsuppress high-frequency noise. Training follows a weakly supervised strategy\nbased on physical averaging principles and reinforced by a PatchGAN\ndiscriminator. Experiments demonstrate that WGAST outperforms existing methods\nin both quantitative and qualitative evaluations. Compared to the\nbest-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves\nSSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and\neffectively captures fine-scale thermal patterns, as validated against 33\nground-based sensors. The code is available at\nhttps://github.com/Sofianebouaziz1/WGAST.git.",
      "pdf_url": "http://arxiv.org/pdf/2508.06485v1",
      "published": "2025-08-08T17:49:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06485v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Post-training for Efficient Communication via Convention Formation",
      "authors": [
        "Yilun Hua",
        "Evan Wang",
        "Yoav Artzi"
      ],
      "abstract": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2508.06482v1",
      "published": "2025-08-08T17:42:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06482v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Intuition emerges in Maximum Caliber models at criticality",
      "authors": [
        "Lluís Arola-Fernández"
      ],
      "abstract": "Whether large predictive models merely parrot their training data or produce\ngenuine insight lacks a physical explanation. This work reports a primitive\nform of intuition that emerges as a metastable phase of learning that\ncritically balances next-token prediction against future path-entropy. The\nintuition mechanism is discovered via mind-tuning, the minimal principle that\nimposes Maximum Caliber in predictive models with a control temperature-like\nparameter $\\lambda$. Training on random walks in deterministic mazes reveals a\nrich phase diagram: imitation (low $\\lambda$), rule-breaking hallucination\n(high $\\lambda$), and a fragile in-between window exhibiting strong\nprotocol-dependence (hysteresis) and multistability, where models spontaneously\ndiscover novel goal-directed strategies. These results are captured by an\neffective low-dimensional theory and frame intuition as an emergent property at\nthe critical balance between memorizing what is and wondering what could be.",
      "pdf_url": "http://arxiv.org/pdf/2508.06477v1",
      "published": "2025-08-08T17:27:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06477v1",
      "categories": [
        "physics.soc-ph",
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls",
      "authors": [
        "Sanket Badhe"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI.",
      "pdf_url": "http://arxiv.org/pdf/2508.06457v1",
      "published": "2025-08-08T17:01:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06457v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ]
    },
    {
      "title": "What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting",
      "authors": [
        "Joshua Caiata",
        "Ben Armstrong",
        "Kate Larson"
      ],
      "abstract": "Committee-selection problems arise in many contexts and applications, and\nthere has been increasing interest within the social choice research community\non identifying which properties are satisfied by different multi-winner voting\nrules. In this work, we propose a data-driven framework to evaluate how\nfrequently voting rules violate axioms across diverse preference distributions\nin practice, shifting away from the binary perspective of axiom satisfaction\ngiven by worst-case analysis. Using this framework, we analyze the relationship\nbetween multi-winner voting rules and their axiomatic performance under several\npreference distributions. We then show that neural networks, acting as voting\nrules, can outperform traditional rules in minimizing axiom violations. Our\nresults suggest that data-driven approaches to social choice can inform the\ndesign of new voting systems and support the continuation of data-driven\nresearch in social choice.",
      "pdf_url": "http://arxiv.org/pdf/2508.06454v1",
      "published": "2025-08-08T16:54:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06454v1",
      "categories": [
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation",
      "authors": [
        "Ruida Cheng",
        "Tejas Sudharshan Mathai",
        "Pritam Mukherjee",
        "Benjamin Hou",
        "Qingqing Zhu",
        "Zhiyong Lu",
        "Matthew McAuliffe",
        "Ronald M. Summers"
      ],
      "abstract": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba",
      "pdf_url": "http://arxiv.org/pdf/2508.06453v1",
      "published": "2025-08-08T16:54:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06453v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking",
      "authors": [
        "Abolfazl Ansari",
        "Delvin Ce Zhang",
        "Nafis Irtiza Tripto",
        "Dongwon Lee"
      ],
      "abstract": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.",
      "pdf_url": "http://arxiv.org/pdf/2508.06445v1",
      "published": "2025-08-08T16:38:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06445v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time",
      "authors": [
        "Debabrota Basu",
        "Udvas Das"
      ],
      "abstract": "An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify\ndifferent types of bias (also known as unfairness) exhibited in the predictions\nof ML algorithms, and to design new algorithms to mitigate them. Often, the\ndefinitions of bias used in the literature are observational, i.e. they use the\ninput and output of a pre-trained algorithm to quantify a bias under concern.\nIn reality,these definitions are often conflicting in nature and can only be\ndeployed if either the ground truth is known or only in retrospect after\ndeploying the algorithm. Thus,there is a gap between what we want Fair ML to\nachieve and what it does in a dynamic social environment. Hence, we propose an\nalternative dynamic mechanism,\"Fair Game\",to assure fairness in the predictions\nof an ML algorithm and to adapt its predictions as the society interacts with\nthe algorithm over time. \"Fair Game\" puts together an Auditor and a Debiasing\nalgorithm in a loop around an ML algorithm. The \"Fair Game\" puts these two\ncomponents in a loop by leveraging Reinforcement Learning (RL). RL algorithms\ninteract with an environment to take decisions, which yields new observations\n(also known as data/feedback) from the environment and in turn, adapts future\ndecisions. RL is already used in algorithms with pre-fixed long-term fairness\ngoals. \"Fair Game\" provides a unique framework where the fairness goals can be\nadapted over time by only modifying the auditor and the different biases it\nquantifies. Thus,\"Fair Game\" aims to simulate the evolution of ethical and\nlegal frameworks in the society by creating an auditor which sends feedback to\na debiasing algorithm deployed around an ML system. This allows us to develop a\nflexible and adaptive-over-time framework to build Fair ML systems pre- and\npost-deployment.",
      "pdf_url": "http://arxiv.org/pdf/2508.06443v1",
      "published": "2025-08-08T16:36:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06443v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.ET",
        "cs.GT"
      ]
    },
    {
      "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages",
      "authors": [
        "Andrea Nasuto",
        "Stefano Maria Iacus",
        "Francisco Rowe",
        "Devika Jain"
      ],
      "abstract": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.",
      "pdf_url": "http://arxiv.org/pdf/2508.06435v1",
      "published": "2025-08-08T16:23:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06435v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment",
      "authors": [
        "Shengzhu Yang",
        "Jiawei Du",
        "Shuai Lu",
        "Weihang Zhang",
        "Ningli Wang",
        "Huiqi Li"
      ],
      "abstract": "Large-scale natural image-text datasets, especially those automatically\ncollected from the web, often suffer from loose semantic alignment due to weak\nsupervision, while medical datasets tend to have high cross-modal correlation\nbut low content diversity. These properties pose a common challenge for\ncontrastive language-image pretraining (CLIP): they hinder the model's ability\nto learn robust and generalizable representations. In this work, we propose\nCLIPin, a unified non-contrastive plug-in that can be seamlessly integrated\ninto CLIP-style architectures to improve multimodal semantic alignment,\nproviding stronger supervision and enhancing alignment robustness. Furthermore,\ntwo shared pre-projectors are designed for image and text modalities\nrespectively to facilitate the integration of contrastive and non-contrastive\nlearning in a parameter-compromise manner. Extensive experiments on diverse\ndownstream tasks demonstrate the effectiveness and generality of CLIPin as a\nplug-and-play component compatible with various contrastive frameworks. Code is\navailable at https://github.com/T6Yang/CLIPin.",
      "pdf_url": "http://arxiv.org/pdf/2508.06434v1",
      "published": "2025-08-08T16:23:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06434v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Memp: Exploring Agent Procedural Memory",
      "authors": [
        "Runnan Fang",
        "Yuan Liang",
        "Xiaobin Wang",
        "Jialong Wu",
        "Shuofei Qiao",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.",
      "pdf_url": "http://arxiv.org/pdf/2508.06433v1",
      "published": "2025-08-08T16:20:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06433v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation",
      "authors": [
        "Guido Manni",
        "Clemente Lauretti",
        "Loredana Zollo",
        "Paolo Soda"
      ],
      "abstract": "Deep learning has revolutionized medical imaging, but its effectiveness is\nseverely limited by insufficient labeled training data. This paper introduces a\nnovel GAN-based semi-supervised learning framework specifically designed for\nlow labeled-data regimes, evaluated across settings with 5 to 50 labeled\nsamples per class. Our approach integrates three specialized neural networks --\na generator for class-conditioned image translation, a discriminator for\nauthenticity assessment and classification, and a dedicated classifier --\nwithin a three-phase training framework. The method alternates between\nsupervised training on limited labeled data and unsupervised learning that\nleverages abundant unlabeled images through image-to-image translation rather\nthan generation from noise. We employ ensemble-based pseudo-labeling that\ncombines confidence-weighted predictions from the discriminator and classifier\nwith temporal consistency through exponential moving averaging, enabling\nreliable label estimation for unlabeled data. Comprehensive evaluation across\neleven MedMNIST datasets demonstrates that our approach achieves statistically\nsignificant improvements over six state-of-the-art GAN-based semi-supervised\nmethods, with particularly strong performance in the extreme 5-shot setting\nwhere the scarcity of labeled data is most challenging. The framework maintains\nits superiority across all evaluated settings (5, 10, 20, and 50 shots per\nclass). Our approach offers a practical solution for medical imaging\napplications where annotation costs are prohibitive, enabling robust\nclassification performance even with minimal labeled data. Code is available at\nhttps://github.com/GuidoManni/SPARSE.",
      "pdf_url": "http://arxiv.org/pdf/2508.06429v1",
      "published": "2025-08-08T16:16:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06429v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation",
      "authors": [
        "Youguang Xing",
        "Xu Luo",
        "Junlin Xie",
        "Lianli Gao",
        "Hengtao Shen",
        "Jingkuan Song"
      ],
      "abstract": "Generalist robot policies trained on large-scale datasets such as Open\nX-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.\nHowever, they often struggle to generalize beyond the distribution of their\ntraining data. In this paper, we investigate the underlying cause of this\nlimited generalization capability. We identify shortcut learning -- the\nreliance on task-irrelevant features -- as a key impediment to generalization.\nThrough comprehensive theoretical and empirical analysis, we uncover two\nprimary contributors to shortcut learning: (1) limited diversity within\nindividual sub-datasets, and (2) significant distributional disparities across\nsub-datasets, leading to dataset fragmentation. These issues arise from the\ninherent structure of large-scale datasets like OXE, which are typically\ncomposed of multiple sub-datasets collected independently across varied\nenvironments and embodiments. Our findings provide critical insights into\ndataset collection strategies that can reduce shortcut learning and enhance the\ngeneralization ability of generalist robot policies. Moreover, in scenarios\nwhere acquiring new large-scale data is impractical, we demonstrate that\ncarefully selected robotic data augmentation strategies can effectively reduce\nshortcut learning in existing offline datasets, thereby improving\ngeneralization capabilities of generalist robot policies, e.g., $\\pi_0$, in\nboth simulation and real-world environments. More information at\nhttps://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.",
      "pdf_url": "http://arxiv.org/pdf/2508.06426v1",
      "published": "2025-08-08T16:14:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06426v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks",
      "authors": [
        "Ze Shen Chin"
      ],
      "abstract": "Although discourse around the risks of Artificial Intelligence (AI) has\ngrown, it often lacks a comprehensive, multidimensional framework, and concrete\ncausal pathways mapping hazard to harm. This paper aims to bridge this gap by\nexamining six commonly discussed AI catastrophic risks: CBRN, cyber offense,\nsudden loss of control, gradual loss of control, environmental risk, and\ngeopolitical risk. First, we characterize these risks across seven key\ndimensions, namely intent, competency, entity, polarity, linearity, reach, and\norder. Next, we conduct risk pathway modeling by mapping step-by-step\nprogressions from the initial hazard to the resulting harms. The dimensional\napproach supports systematic risk identification and generalizable mitigation\nstrategies, while risk pathway models help identify scenario-specific\ninterventions. Together, these methods offer a more structured and actionable\nfoundation for managing catastrophic AI risks across the value chain.",
      "pdf_url": "http://arxiv.org/pdf/2508.06411v1",
      "published": "2025-08-08T15:56:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06411v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery",
      "authors": [
        "Ch Muhammad Awais",
        "Marco Reggiannini",
        "Davide Moroni",
        "Oktay Karakus"
      ],
      "abstract": "High-resolution imagery plays a critical role in improving the performance of\nvisual recognition tasks such as classification, detection, and segmentation.\nIn many domains, including remote sensing and surveillance, low-resolution\nimages can limit the accuracy of automated analysis. To address this,\nsuper-resolution (SR) techniques have been widely adopted to attempt to\nreconstruct high-resolution images from low-resolution inputs. Related\ntraditional approaches focus solely on enhancing image quality based on\npixel-level metrics, leaving the relationship between super-resolved image\nfidelity and downstream classification performance largely underexplored. This\nraises a key question: can integrating classification objectives directly into\nthe super-resolution process further improve classification accuracy? In this\npaper, we try to respond to this question by investigating the relationship\nbetween super-resolution and classification through the deployment of a\nspecialised algorithmic strategy. We propose a novel methodology that increases\nthe resolution of synthetic aperture radar imagery by optimising loss functions\nthat account for both image quality and classification performance. Our\napproach improves image quality, as measured by scientifically ascertained\nimage quality indicators, while also enhancing classification accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.06407v1",
      "published": "2025-08-08T15:50:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06407v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges",
      "authors": [
        "Andrew Brown",
        "Muhammad Roman",
        "Barry Devereux"
      ],
      "abstract": "This systematic review of the research literature on retrieval-augmented\ngeneration (RAG) provides a focused analysis of the most highly cited studies\npublished between 2020 and May 2025. A total of 128 articles met our inclusion\ncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).\nRAG couples a neural retriever with a generative language model, grounding\noutput in up-to-date, non-parametric memory while retaining the semantic\ngeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we\n(i) specify explicit inclusion and exclusion criteria based on citation count\nand research questions, (ii) catalogue datasets, architectures, and evaluation\npractices, and (iii) synthesise empirical evidence on the effectiveness and\nlimitations of RAG. To mitigate citation-lag bias, we applied a lower\ncitation-count threshold to papers published in 2025 so that emerging\nbreakthroughs with naturally fewer citations were still captured. This review\nclarifies the current research landscape, highlights methodological gaps, and\ncharts priority directions for future research.",
      "pdf_url": "http://arxiv.org/pdf/2508.06401v1",
      "published": "2025-08-08T15:37:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06401v1",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ]
    },
    {
      "title": "Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling",
      "authors": [
        "Md Asif Jalal",
        "Luca Remaggi",
        "Vasileios Moschopoulos",
        "Thanasis Kotsiopoulos",
        "Vandana Rajan",
        "Karthikeyan Saravanan",
        "Anastasis Drosou",
        "Junho Heo",
        "Hyuk Oh",
        "Seokyeong Jeong"
      ],
      "abstract": "Traditional speech separation and speaker diarization approaches rely on\nprior knowledge of target speakers or a predetermined number of participants in\naudio signals. To address these limitations, recent advances focus on\ndeveloping enrollment-free methods capable of identifying targets without\nexplicit speaker labeling. This work introduces a new approach to train\nsimultaneous speech separation and diarization using automatic identification\nof target speaker embeddings, within mixtures. Our proposed model employs a\ndual-stage training pipeline designed to learn robust speaker representation\nfeatures that are resilient to background noise interference. Furthermore, we\npresent an overlapping spectral loss function specifically tailored for\nenhancing diarization accuracy during overlapped speech frames. Experimental\nresults show significant performance gains compared to the current SOTA\nbaseline, achieving 71% relative improvement in DER and 69% in cpWER.",
      "pdf_url": "http://arxiv.org/pdf/2508.06393v1",
      "published": "2025-08-08T15:24:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06393v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Identity Increases Stability in Neural Cellular Automata",
      "authors": [
        "James Stovold"
      ],
      "abstract": "Neural Cellular Automata (NCAs) offer a way to study the growth of\ntwo-dimensional artificial organisms from a single seed cell. From the outset,\nNCA-grown organisms have had issues with stability, their natural boundary\noften breaking down and exhibiting tumour-like growth or failing to maintain\nthe expected shape. In this paper, we present a method for improving the\nstability of NCA-grown organisms by introducing an 'identity' layer with simple\nconstraints during training.\n  Results show that NCAs grown in close proximity are more stable compared with\nthe original NCA model. Moreover, only a single identity value is required to\nachieve this increase in stability. We observe emergent movement from the\nstable organisms, with increasing prevalence for models with multiple identity\nvalues.\n  This work lays the foundation for further study of the interaction between\nNCA-grown organisms, paving the way for studying social interaction at a\ncellular level in artificial organisms.",
      "pdf_url": "http://arxiv.org/pdf/2508.06389v1",
      "published": "2025-08-08T15:18:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06389v1",
      "categories": [
        "cs.NE",
        "cs.AI"
      ]
    },
    {
      "title": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation",
      "authors": [
        "Anurag Tripathi",
        "Vaibhav Patle",
        "Abhinav Jain",
        "Ayush Pundir",
        "Sairam Menon",
        "Ajeet Kumar Singh"
      ],
      "abstract": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.06387v1",
      "published": "2025-08-08T15:16:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06387v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models",
      "authors": [
        "Han Yin",
        "Yafeng Chen",
        "Chong Deng",
        "Luyao Cheng",
        "Hui Wang",
        "Chao-Hong Tan",
        "Qian Chen",
        "Wen Wang",
        "Xiangang Li"
      ],
      "abstract": "The Speaker Diarization and Recognition (SDR) task aims to predict \"who spoke\nwhen and what\" within an audio clip, which is a crucial task in various\nreal-world multi-speaker scenarios such as meeting transcription and dialogue\nsystems. Existing SDR systems typically adopt a cascaded framework, combining\nmultiple modules such as speaker diarization (SD) and automatic speech\nrecognition (ASR). The cascaded systems suffer from several limitations, such\nas error propagation, difficulty in handling overlapping speech, and lack of\njoint optimization for exploring the synergy between SD and ASR tasks. To\naddress these limitations, we introduce SpeakerLM, a unified multimodal large\nlanguage model for SDR that jointly performs SD and ASR in an end-to-end\nmanner. Moreover, to facilitate diverse real-world scenarios, we incorporate a\nflexible speaker registration mechanism into SpeakerLM, enabling SDR under\ndifferent speaker registration settings. SpeakerLM is progressively developed\nwith a multi-stage training strategy on large-scale real data. Extensive\nexperiments show that SpeakerLM demonstrates strong data scaling capability and\ngeneralizability, outperforming state-of-the-art cascaded baselines on both\nin-domain and out-of-domain public SDR benchmarks. Furthermore, experimental\nresults show that the proposed speaker registration mechanism effectively\nensures robust SDR performance of SpeakerLM across diverse speaker registration\nconditions and varying numbers of registered speakers.",
      "pdf_url": "http://arxiv.org/pdf/2508.06372v1",
      "published": "2025-08-08T15:04:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06372v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned",
      "authors": [
        "Claudia dAmato",
        "Giuseppe Rubini",
        "Francesco Didio",
        "Donato Francioso",
        "Fatima Zahra Amara",
        "Nicola Fanizzi"
      ],
      "abstract": "Legal decision-making process requires the availability of comprehensive and\ndetailed legislative background knowledge and up-to-date information on legal\ncases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a\nvaluable tool to facilitate access to legal information, to be queried and\nexploited for the purpose, and to enable advanced reasoning and machine\nlearning applications. Indeed, legal KGs may act as knowledge intensive\ncomponent to be used by pre-dictive machine learning solutions supporting the\ndecision process of the legal expert. Nevertheless, a few KGs can be found in\nthe legal domain. To fill this gap, we developed a legal KG targeting legal\ncases of violence against women, along with clear adopted methodologies.\nSpecifically, the paper introduces two complementary approaches for automated\nlegal KG construction; a systematic bottom-up approach, customized for the\nlegal domain, and a new solution leveraging Large Language Models. Starting\nfrom legal sentences publicly available from the European Court of Justice, the\nsolutions integrate structured data extraction, ontology development, and\nsemantic enrichment to produce KGs tailored for legal cases involving violence\nagainst women. After analyzing and comparing the results of the two approaches,\nthe developed KGs are validated via suitable competency questions. The obtained\nKG may be impactful for multiple purposes: can improve the accessibility to\nlegal information both to humans and machine, can enable complex queries and\nmay constitute an important knowledge component to be possibly exploited by\nmachine learning tools tailored for predictive justice.",
      "pdf_url": "http://arxiv.org/pdf/2508.06368v1",
      "published": "2025-08-08T14:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06368v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design",
      "authors": [
        "Renyi Zhou",
        "Huimin Zhu",
        "Jing Tang",
        "Min Li"
      ],
      "abstract": "Achieving precise control over a molecule's biological activity-encompassing\ntargeted activation/inhibition, cooperative multi-target modulation, and\noff-target toxicity mitigation-remains a critical challenge in de novo drug\ndesign. However, existing generative methods primarily focus on producing\nmolecules with a single desired activity, lacking integrated mechanisms for the\nsimultaneous management of multiple intended and unintended molecular\ninteractions. Here, we propose ActivityDiff, a generative approach based on the\nclassifier-guidance technique of diffusion models. It leverages separately\ntrained drug-target classifiers for both positive and negative guidance,\nenabling the model to enhance desired activities while minimizing harmful\noff-target effects. Experimental results show that ActivityDiff effectively\nhandles essential drug design tasks, including single-/dual-target generation,\nfragment-constrained dual-target design, selective generation to enhance target\nspecificity, and reduction of off-target effects. These results demonstrate the\neffectiveness of classifier-guided diffusion in balancing efficacy and safety\nin molecular design. Overall, our work introduces a novel paradigm for\nachieving integrated control over molecular activity, and provides ActivityDiff\nas a versatile and extensible framework.",
      "pdf_url": "http://arxiv.org/pdf/2508.06364v1",
      "published": "2025-08-08T14:48:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06364v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ]
    },
    {
      "title": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts",
      "authors": [
        "Zhaomin Wu",
        "Mingzhe Du",
        "See-Kiong Ng",
        "Bingsheng He"
      ],
      "abstract": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains.",
      "pdf_url": "http://arxiv.org/pdf/2508.06361v1",
      "published": "2025-08-08T14:46:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06361v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd",
      "authors": [
        "Aman Bhatta",
        "Maria Dhakal",
        "Michael C. King",
        "Kevin W. Bowyer"
      ],
      "abstract": "A central problem in one-to-many facial identification is that the person in\nthe probe image may or may not have enrolled image(s) in the gallery; that is,\nmay be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one\nresult is Out-of-gallery have mostly focused on finding a suitable threshold on\nthe similarity score. We take a new approach, using the additional enrolled\nimages of the identity with the rank-one result to predict if the rank-one\nresult is In-gallery / Out-of-gallery. Given a gallery of identities and\nimages, we generate In-gallery and Out-of-gallery training data by extracting\nthe ranks of additional enrolled images corresponding to the rank-one identity.\nWe then train a classifier to utilize this feature vector to predict whether a\nrank-one result is In-gallery or Out-of-gallery. Using two different datasets\nand four different matchers, we present experimental results showing that our\napproach is viable for mugshot quality probe images, and also, importantly, for\nprobes degraded by blur, reduced resolution, atmospheric turbulence and\nsunglasses. We also analyze results across demographic groups, and show that\nIn-gallery / Out-of-gallery classification accuracy is similar across\ndemographics. Our approach has the potential to provide an objective estimate\nof whether a one-to-many facial identification is Out-of-gallery, and thereby\nto reduce false positive identifications, wrongful arrests, and wasted\ninvestigative time. Interestingly, comparing the results of older deep\nCNN-based face matchers with newer ones suggests that the effectiveness of our\nOut-of-gallery detection approach emerges only with matchers trained using\nadvanced margin-based loss functions.",
      "pdf_url": "http://arxiv.org/pdf/2508.06357v1",
      "published": "2025-08-08T14:39:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06357v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI",
      "authors": [
        "Christian Meske",
        "Justin Brenne",
        "Erdi Uenal",
        "Sabahat Oelcer",
        "Ayseguel Doganguen"
      ],
      "abstract": "Current explainable AI (XAI) approaches prioritize algorithmic transparency\nand present explanations in abstract, non-adaptive formats that often fail to\nsupport meaningful end-user understanding. This paper introduces \"Explanatory\nAI\" as a complementary paradigm that leverages generative AI capabilities to\nserve as explanatory partners for human understanding rather than providers of\nalgorithmic transparency. While XAI reveals algorithmic decision processes for\nmodel validation, Explanatory AI addresses contextual reasoning to support\nhuman decision-making in sociotechnical contexts. We develop a definition and\nsystematic eight-dimensional conceptual model distinguishing Explanatory AI\nthrough narrative communication, adaptive personalization, and progressive\ndisclosure principles. Empirical validation through Rapid Contextual Design\nmethodology with healthcare professionals demonstrates that users consistently\nprefer context-sensitive, multimodal explanations over technical transparency.\nOur findings reveal the practical urgency for AI systems designed for human\ncomprehension rather than algorithmic introspection, establishing a\ncomprehensive research agenda for advancing user-centered AI explanation\napproaches across diverse domains and cultural contexts.",
      "pdf_url": "http://arxiv.org/pdf/2508.06352v1",
      "published": "2025-08-08T14:32:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06352v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games",
      "authors": [
        "Mille Mei Zhen Loo",
        "Gert Luzkov",
        "Paolo Burelli"
      ],
      "abstract": "Cheating in online video games compromises the integrity of gaming\nexperiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face\nsignificant challenges in keeping pace with evolving cheating methods without\nimposing invasive measures on users' systems. This paper presents\nAntiCheatPT\\_256, a transformer-based machine learning model designed to detect\ncheating behaviour in Counter-Strike 2 using gameplay data. To support this, we\nintroduce and publicly release CS2CD: A labelled dataset of 795 matches. Using\nthis dataset, 90,707 context windows were created and subsequently augmented to\naddress class imbalance. The transformer model, trained on these windows,\nachieved an accuracy of 89.17\\% and an AUC of 93.36\\% on an unaugmented test\nset. This approach emphasizes reproducibility and real-world applicability,\noffering a robust baseline for future research in data-driven cheat detection.",
      "pdf_url": "http://arxiv.org/pdf/2508.06348v1",
      "published": "2025-08-08T14:22:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06348v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Structural Equation-VAE: Disentangled Latent Representations for Tabular Data",
      "authors": [
        "Ruiyu Zhang",
        "Ce Zhao",
        "Xin Zhao",
        "Lin Nie",
        "Wai-Fung Lam"
      ],
      "abstract": "Learning interpretable latent representations from tabular data remains a\nchallenge in deep generative modeling. We introduce SE-VAE (Structural\nEquation-Variational Autoencoder), a novel architecture that embeds measurement\nstructure directly into the design of a variational autoencoder. Inspired by\nstructural equation modeling, SE-VAE aligns latent subspaces with known\nindicator groupings and introduces a global nuisance latent to isolate\nconstruct-specific confounding variation. This modular architecture enables\ndisentanglement through design rather than through statistical regularizers\nalone. We evaluate SE-VAE on a suite of simulated tabular datasets and\nbenchmark its performance against a series of leading baselines using standard\ndisentanglement metrics. SE-VAE consistently outperforms alternatives in factor\nrecovery, interpretability, and robustness to nuisance variation. Ablation\nresults reveal that architectural structure, rather than regularization\nstrength, is the key driver of performance. SE-VAE offers a principled\nframework for white-box generative modeling in scientific and social domains\nwhere latent constructs are theory-driven and measurement validity is\nessential.",
      "pdf_url": "http://arxiv.org/pdf/2508.06347v1",
      "published": "2025-08-08T14:21:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06347v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering",
      "authors": [
        "Yanbin Wei",
        "Jiangyue Yan",
        "Chun Kang",
        "Yang Chen",
        "Hua Liu",
        "James T. Kwok",
        "Yu Zhang"
      ],
      "abstract": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy",
      "pdf_url": "http://arxiv.org/pdf/2508.06345v1",
      "published": "2025-08-08T14:18:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06345v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ]
    },
    {
      "title": "On Approximate MMS Allocations on Restricted Graph Classes",
      "authors": [
        "Václav Blažej",
        "Michał Dębski ad Zbigniew Lonc",
        "Marta Piecyk",
        "Paweł Rzążewski"
      ],
      "abstract": "We study the problem of fair division of a set of indivisible goods with\nconnectivity constraints. Specifically, we assume that the goods are\nrepresented as vertices of a connected graph, and sets of goods allocated to\nthe agents are connected subgraphs of this graph. We focus on the\nwidely-studied maximin share criterion of fairness. It has been shown that an\nallocation satisfying this criterion may not exist even without connectivity\nconstraints, i.e., if the graph of goods is complete. In view of this, it is\nnatural to seek approximate allocations that guarantee each agent a connected\nbundle of goods with value at least a constant fraction of the maximin share\nvalue to the agent. It is known that for some classes of graphs, such as\ncomplete graphs, cycles, and $d$-claw-free graphs for any fixed $d$, such\napproximate allocations indeed exist. However, it is an open problem whether\nthey exist for the class of all graphs.\n  In this paper, we continue the systematic study of the existence of\napproximate allocations on restricted graph classes. In particular, we show\nthat such allocations exist for several well-studied classes, including block\ngraphs, cacti, complete multipartite graphs, and split graphs.",
      "pdf_url": "http://arxiv.org/pdf/2508.06343v1",
      "published": "2025-08-08T14:17:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06343v1",
      "categories": [
        "cs.DM",
        "cs.AI"
      ]
    },
    {
      "title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork",
      "authors": [
        "Constantin Ruhdorfer",
        "Matteo Bortoletto",
        "Victor Oei",
        "Anna Penzkofer",
        "Andreas Bulling"
      ],
      "abstract": "We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating.",
      "pdf_url": "http://arxiv.org/pdf/2508.06336v1",
      "published": "2025-08-08T14:11:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06336v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ]
    },
    {
      "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection",
      "authors": [
        "Giacomo D'Amicantonio",
        "Snehashis Majhi",
        "Quan Kong",
        "Lorenzo Garattoni",
        "Gianpiero Francesca",
        "François Bremond",
        "Egor Bondarev"
      ],
      "abstract": "Video Anomaly Detection (VAD) is a challenging task due to the variability of\nanomalous events and the limited availability of labeled data. Under the\nWeakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided\nduring training, while predictions are made at the frame level. Although\nstate-of-the-art models perform well on simple anomalies (e.g., explosions),\nthey struggle with complex real-world events (e.g., shoplifting). This\ndifficulty stems from two key issues: (1) the inability of current models to\naddress the diversity of anomaly types, as they process all categories with a\nshared model, overlooking category-specific features; and (2) the weak\nsupervision signal, which lacks precise temporal information, limiting the\nability to capture nuanced anomalous patterns blended with normal events. To\naddress these challenges, we propose Gaussian Splatting-guided Mixture of\nExperts (GS-MoE), a novel framework that employs a set of expert models, each\nspecialized in capturing specific anomaly types. These experts are guided by a\ntemporal Gaussian splatting loss, enabling the model to leverage temporal\nconsistency and enhance weak supervision. The Gaussian splatting approach\nencourages a more precise and comprehensive representation of anomalies by\nfocusing on temporal segments most likely to contain abnormal events. The\npredictions from these specialized experts are integrated through a\nmixture-of-experts mechanism to model complex relationships across diverse\nanomaly patterns. Our approach achieves state-of-the-art performance, with a\n91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on\nXD-Violence and MSAD datasets. By leveraging category-specific expertise and\ntemporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.",
      "pdf_url": "http://arxiv.org/pdf/2508.06318v1",
      "published": "2025-08-08T13:48:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06318v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields",
      "authors": [
        "Junhyeog Yun",
        "Minui Hong",
        "Gunhee Kim"
      ],
      "abstract": "Neural fields provide a memory-efficient representation of data, which can\neffectively handle diverse modalities and large-scale data. However, learning\nto map neural fields often requires large amounts of training data and\ncomputations, which can be limited to resource-constrained edge devices. One\napproach to tackle this limitation is to leverage Federated Meta-Learning\n(FML), but traditional FML approaches suffer from privacy leakage. To address\nthese issues, we introduce a novel FML approach called FedMeNF. FedMeNF\nutilizes a new privacy-preserving loss function that regulates privacy leakage\nin the local meta-optimization. This enables the local meta-learner to optimize\nquickly and efficiently without retaining the client's private data. Our\nexperiments demonstrate that FedMeNF achieves fast optimization speed and\nrobust reconstruction performance, even with few-shot or non-IID data across\ndiverse data modalities, while preserving client data privacy.",
      "pdf_url": "http://arxiv.org/pdf/2508.06301v1",
      "published": "2025-08-08T13:24:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06301v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC"
      ]
    },
    {
      "title": "LLM Robustness Leaderboard v1 --Technical report",
      "authors": [
        "Pierre Peigné - Lefebvre",
        "Quentin Feuillade-Montixi",
        "Tom David",
        "Nicolas Miailhe"
      ],
      "abstract": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.",
      "pdf_url": "http://arxiv.org/pdf/2508.06296v1",
      "published": "2025-08-08T13:15:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06296v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification",
      "authors": [
        "Mobarak Abumohsen",
        "Enrique Costa-Montenegro",
        "Silvia García-Méndez",
        "Amani Yousef Owda",
        "Majdi Owda"
      ],
      "abstract": "Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one\nof the most common causes of death for men and women worldwide. Computed\nTomography (CT) images are the most preferred diagnosis method because of their\nlow cost and their faster processing times. Many researchers have proposed\nvarious ways of identifying lung cancer using CT images. However, such\ntechniques suffer from significant false positives, leading to low accuracy.\nThe fundamental reason results from employing a small and imbalanced dataset.\nThis paper introduces an innovative approach for LC detection and\nclassification from CT images based on the DenseNet201 model. Our approach\ncomprises several advanced methods such as Focal Loss, data augmentation, and\nregularization to overcome the imbalanced data issue and overfitting challenge.\nThe findings show the appropriateness of the proposal, attaining a promising\nperformance of 98.95% accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.06287v1",
      "published": "2025-08-08T13:09:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06287v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "OM2P: Offline Multi-Agent Mean-Flow Policy",
      "authors": [
        "Zhuoran Li",
        "Xun Wang",
        "Hai Zhong",
        "Longbo Huang"
      ],
      "abstract": "Generative models, especially diffusion and flow-based models, have been\npromising in offline multi-agent reinforcement learning. However, integrating\npowerful generative models into this framework poses unique challenges. In\nparticular, diffusion and flow-based policies suffer from low sampling\nefficiency due to their iterative generation processes, making them impractical\nin time-sensitive or resource-constrained settings. To tackle these\ndifficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel\noffline MARL algorithm to achieve efficient one-step action sampling. To\naddress the misalignment between generative objectives and reward maximization,\nwe introduce a reward-aware optimization scheme that integrates a\ncarefully-designed mean-flow matching loss with Q-function supervision.\nAdditionally, we design a generalized timestep distribution and a\nderivative-free estimation strategy to reduce memory overhead and improve\ntraining stability. Empirical evaluations on Multi-Agent Particle and MuJoCo\nbenchmarks demonstrate that OM2P achieves superior performance, with up to a\n3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.\nOur approach represents the first to successfully integrate mean-flow model\ninto offline MARL, paving the way for practical and scalable generative\npolicies in cooperative multi-agent settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.06269v1",
      "published": "2025-08-08T12:38:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06269v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Numerical Considerations in Weighted Model Counting",
      "authors": [
        "Randal E. Bryant"
      ],
      "abstract": "Weighted model counting computes the sum of the rational-valued weights\nassociated with the satisfying assignments for a Boolean formula, where the\nweight of an assignment is given by the product of the weights assigned to the\npositive and negated variables comprising the assignment. Weighted model\ncounting finds applications across a variety of domains including probabilistic\nreasoning and quantitative risk assessment.\n  Most weighted model counting programs operate by (explicitly or implicitly)\nconverting the input formula into a form that enables arithmetic evaluation,\nusing multiplication for conjunctions and addition for disjunctions. Performing\nthis evaluation using floating-point arithmetic can yield inaccurate results,\nand it cannot quantify the level of precision achieved. Computing with rational\narithmetic gives exact results, but it is costly in both time and space.\n  This paper describes how to combine multiple numeric representations to\nefficiently compute weighted model counts that are guaranteed to achieve a\nuser-specified precision. When all weights are nonnegative, we prove that the\nprecision loss of arithmetic evaluation using floating-point arithmetic can be\ntightly bounded. We show that supplementing a standard IEEE double-precision\nrepresentation with a separate 64-bit exponent, a format we call extended-range\ndouble (ERD), avoids the underflow and overflow issues commonly encountered in\nweighted model counting. For problems with mixed negative and positive weights,\nwe show that a combination of interval floating-point arithmetic and rational\narithmetic can achieve the twin goals of efficiency and guaranteed precision.\nFor our evaluations, we have devised especially challenging formulas and weight\nassignments, demonstrating the robustness of our approach.",
      "pdf_url": "http://arxiv.org/pdf/2508.06264v1",
      "published": "2025-08-08T12:28:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06264v1",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.LO",
        "cs.NA"
      ]
    },
    {
      "title": "Symmetry breaking for inductive logic programming",
      "authors": [
        "Andrew Cropper",
        "David M. Cerna",
        "Matti Järvisalo"
      ],
      "abstract": "The goal of inductive logic programming is to search for a hypothesis that\ngeneralises training data and background knowledge. The challenge is searching\nvast hypothesis spaces, which is exacerbated because many logically equivalent\nhypotheses exist. To address this challenge, we introduce a method to break\nsymmetries in the hypothesis space. We implement our idea in answer set\nprogramming. Our experiments on multiple domains, including visual reasoning\nand game playing, show that our approach can reduce solving times from over an\nhour to just 17 seconds.",
      "pdf_url": "http://arxiv.org/pdf/2508.06263v1",
      "published": "2025-08-08T12:28:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06263v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning",
      "authors": [
        "Zhangquan Chen",
        "Ruihui Zhao",
        "Chuwei Luo",
        "Mingze Sun",
        "Xinlei Yu",
        "Yangyang Kang",
        "Ruqi Huang"
      ],
      "abstract": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method.",
      "pdf_url": "http://arxiv.org/pdf/2508.06259v1",
      "published": "2025-08-08T12:26:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06259v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.10"
      ]
    },
    {
      "title": "Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)",
      "authors": [
        "Alejandro Moreno R.",
        "Desale Fentaw",
        "Samuel Palmer",
        "Raúl Salles de Padua",
        "Ninad Dixit",
        "Samuel Mugel",
        "Roman Orús",
        "Manuel Radons",
        "Josef Menter",
        "Ali Abedi"
      ],
      "abstract": "Synthetic data generation is a key technique in modern artificial\nintelligence, addressing data scarcity, privacy constraints, and the need for\ndiverse datasets in training robust models. In this work, we propose a method\nfor generating privacy-preserving high-quality synthetic tabular data using\nTensor Networks, specifically Matrix Product States (MPS). We benchmark the\nMPS-based generative model against state-of-the-art models such as CTGAN, VAE,\nand PrivBayes, focusing on both fidelity and privacy-preserving capabilities.\nTo ensure differential privacy (DP), we integrate noise injection and gradient\nclipping during training, enabling privacy guarantees via R\\'enyi Differential\nPrivacy accounting. Across multiple metrics analyzing data fidelity and\ndownstream machine learning task performance, our results show that MPS\noutperforms classical models, particularly under strict privacy constraints.\nThis work highlights MPS as a promising tool for privacy-aware synthetic data\ngeneration. By combining the expressive power of tensor network representations\nwith formal privacy mechanisms, the proposed approach offers an interpretable\nand scalable alternative for secure data sharing. Its structured design\nfacilitates integration into sensitive domains where both data quality and\nconfidentiality are critical.",
      "pdf_url": "http://arxiv.org/pdf/2508.06251v1",
      "published": "2025-08-08T12:14:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06251v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "quant-ph"
      ]
    },
    {
      "title": "In-Training Defenses against Emergent Misalignment in Language Models",
      "authors": [
        "David Kaczér",
        "Magnus Jørgenvåg",
        "Clemens Vetter",
        "Lucie Flek",
        "Florian Mai"
      ],
      "abstract": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research.",
      "pdf_url": "http://arxiv.org/pdf/2508.06249v1",
      "published": "2025-08-08T12:10:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06249v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Membership Inference Attack with Partial Features",
      "authors": [
        "Xurun Wang",
        "Guangrui Liu",
        "Xinjie Li",
        "Haoyu He",
        "Lin Yao",
        "Weizhe Zhang"
      ],
      "abstract": "Machine learning models have been shown to be susceptible to membership\ninference attack, which can be used to determine whether a given sample appears\nin the training data. Existing membership inference methods commonly assume\nthat the adversary has full access to the features of the target sample. This\nassumption, however, does not hold in many real-world scenarios where only\npartial features information is available, thereby limiting the applicability\nof these methods. In this work, we study an inference scenario where the\nadversary observes only partial features of each sample and aims to infer\nwhether this observed subset was present in the training set of the target\nmodel. We define this problem as Partial Feature Membership Inference (PFMI).\nTo address this problem, we propose MRAD (Memory-guided Reconstruction and\nAnomaly Detection), a two-stage attack framework. In the first stage, MRAD\noptimizes the unknown feature values to minimize the loss of the sample. In the\nsecond stage, it measures the deviation between the reconstructed sample and\nthe training distribution using anomaly detection. Empirical results\ndemonstrate that MRAD is effective across a range of datasets, and maintains\ncompatibility with various off-the-shelf anomaly detection techniques. For\nexample, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of\nthe missing features.",
      "pdf_url": "http://arxiv.org/pdf/2508.06244v1",
      "published": "2025-08-08T11:56:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06244v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Learning Logical Rules using Minimum Message Length",
      "authors": [
        "Ruben Sharma",
        "Sebastijan Dumančić",
        "Ross D. King",
        "Andrew Cropper"
      ],
      "abstract": "Unifying probabilistic and logical learning is a key challenge in AI. We\nintroduce a Bayesian inductive logic programming approach that learns minimum\nmessage length programs from noisy data. Our approach balances hypothesis\ncomplexity and data fit through priors, which explicitly favour more general\nprograms, and a likelihood that favours accurate programs. Our experiments on\nseveral domains, including game playing and drug design, show that our method\nsignificantly outperforms previous methods, notably those that learn minimum\ndescription length programs. Our results also show that our approach is\ndata-efficient and insensitive to example balance, including the ability to\nlearn from exclusively positive examples.",
      "pdf_url": "http://arxiv.org/pdf/2508.06230v1",
      "published": "2025-08-08T11:23:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06230v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines",
      "authors": [
        "Yumeng Fu",
        "Jiayin Zhu",
        "Lingling Zhang",
        "Bo Zhao",
        "Shaoxuan Ma",
        "Yushun Zhang",
        "Yanrui Wu",
        "Wenjun Wu"
      ],
      "abstract": "Geometry problem solving (GPS) requires models to master diagram\ncomprehension, logical reasoning, knowledge application, numerical computation,\nand auxiliary line construction. This presents a significant challenge for\nMultimodal Large Language Models (MLLMs). However, existing benchmarks for\nevaluating MLLM geometry skills overlook auxiliary line construction and lack\nfine-grained process evaluation, making them insufficient for assessing MLLMs'\nlong-step reasoning abilities. To bridge these gaps, we present the GeoLaux\nbenchmark, comprising 2,186 geometry problems, incorporating both calculation\nand proving questions. Notably, the problems require an average of 6.51\nreasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary\nline construction. Building on the dataset, we design a novel five-dimensional\nevaluation strategy assessing answer correctness, process correctness, process\nquality, auxiliary line impact, and error causes. Extensive experiments on 13\nleading MLLMs (including thinking models and non-thinking models) yield three\npivotal findings: First, models exhibit substantial performance degradation in\nextended reasoning steps (nine models demonstrate over 50% performance drop).\nSecond, compared to calculation problems, MLLMs tend to take shortcuts when\nsolving proving problems. Third, models lack auxiliary line awareness, and\nenhancing this capability proves particularly beneficial for overall geometry\nreasoning improvement. These findings establish GeoLaux as both a benchmark for\nevaluating MLLMs' long-step geometric reasoning with auxiliary lines and a\nguide for capability advancement. Our dataset and code are included in\nsupplementary materials and will be released.",
      "pdf_url": "http://arxiv.org/pdf/2508.06226v1",
      "published": "2025-08-08T11:11:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06226v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution",
      "authors": [
        "Zailong Tian",
        "Zhuoheng Han",
        "Yanzhe Chen",
        "Haozhe Xu",
        "Xi Yang",
        "richeng xuan",
        "Hongfeng Wang",
        "Lizi Liao"
      ],
      "abstract": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines.",
      "pdf_url": "http://arxiv.org/pdf/2508.06225v1",
      "published": "2025-08-08T11:11:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06225v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?",
      "authors": [
        "Keummin Ka",
        "Junhyeong Park",
        "Jahyun Jeon",
        "Youngjae Yu"
      ],
      "abstract": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.06220v1",
      "published": "2025-08-08T11:03:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06220v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Reparameterization Proximal Policy Optimization",
      "authors": [
        "Hai Zhong",
        "Xun Wang",
        "Zhuoran Li",
        "Longbo Huang"
      ],
      "abstract": "Reparameterization policy gradient (RPG) is promising for improving sample\nefficiency by leveraging differentiable dynamics. However, a critical barrier\nis its training instability, where high-variance gradients can destabilize the\nlearning process. To address this, we draw inspiration from Proximal Policy\nOptimization (PPO), which uses a surrogate objective to enable stable sample\nreuse in the model-free setting. We first establish a connection between this\nsurrogate objective and RPG, which has been largely unexplored and is\nnon-trivial. Then, we bridge this gap by demonstrating that the\nreparameterization gradient of a PPO-like surrogate objective can be computed\nefficiently using backpropagation through time. Based on this key insight, we\npropose Reparameterization Proximal Policy Optimization (RPO), a stable and\nsample-efficient RPG-based method. RPO enables multiple epochs of stable sample\nreuse by optimizing a clipped surrogate objective tailored for RPG, while being\nfurther stabilized by Kullback-Leibler (KL) divergence regularization and\nremaining fully compatible with existing variance reduction methods. We\nevaluate RPO on a suite of challenging locomotion and manipulation tasks, where\nexperiments demonstrate that our method achieves superior sample efficiency and\nstrong performance.",
      "pdf_url": "http://arxiv.org/pdf/2508.06214v1",
      "published": "2025-08-08T10:50:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06214v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Graph Federated Learning for Personalized Privacy Recommendation",
      "authors": [
        "Ce Na",
        "Kai Yang",
        "Dengzhao Fang",
        "Yu Li",
        "Jingtong Gao",
        "Chengcheng Zhu",
        "Jiale Zhang",
        "Xiaobing Sun",
        "Yi Chang"
      ],
      "abstract": "Federated recommendation systems (FedRecs) have gained significant attention\nfor providing privacy-preserving recommendation services. However, existing\nFedRecs assume that all users have the same requirements for privacy\nprotection, i.e., they do not upload any data to the server. The approaches\noverlook the potential to enhance the recommendation service by utilizing\npublicly available user data. In real-world applications, users can choose to\nbe private or public. Private users' interaction data is not shared, while\npublic users' interaction data can be shared. Inspired by the issue, this paper\nproposes a novel Graph Federated Learning for Personalized Privacy\nRecommendation (GFed-PP) that adapts to different privacy requirements while\nimproving recommendation performance. GFed-PP incorporates the interaction data\nof public users to build a user-item interaction graph, which is then used to\nform a user relationship graph. A lightweight graph convolutional network (GCN)\nis employed to learn each user's user-specific personalized item embedding. To\nprotect user privacy, each client learns the user embedding and the scoring\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\nrecommendation framework through the initialization of item embedding on\nclients and the aggregation of the user relationship graph on the server.\nExperimental results demonstrate that GFed-PP significantly outperforms\nexisting methods for five datasets, offering superior recommendation accuracy\nwithout compromising privacy. This framework provides a practical solution for\naccommodating varying privacy preferences in federated recommendation systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.06208v1",
      "published": "2025-08-08T10:44:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06208v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Classification is a RAG problem: A case study on hate speech detection",
      "authors": [
        "Richard Willats",
        "Josh Pennington",
        "Aravind Mohan",
        "Bertie Vidgen"
      ],
      "abstract": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems.",
      "pdf_url": "http://arxiv.org/pdf/2508.06204v1",
      "published": "2025-08-08T10:35:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06204v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning",
      "authors": [
        "Chang Che",
        "Ziqi Wang",
        "Pengwan Yang",
        "Qi Wang",
        "Hui Ma",
        "Zenglin Shi"
      ],
      "abstract": "Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language\nModels (MLLMs) to incrementally learn new tasks over time. However, this\nprocess is challenged by catastrophic forgetting, where performance on\npreviously learned tasks deteriorates as the model adapts to new ones. A common\napproach to mitigate forgetting is architecture expansion, which introduces\ntask-specific modules to prevent interference. Yet, existing methods often\nexpand entire layers for each task, leading to significant parameter overhead\nand poor scalability. To overcome these issues, we introduce LoRA in LoRA\n(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in\nMLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,\napplies an additional low-rank decomposition to matrix B to minimize\ntask-specific parameters, and incorporates a cosine-regularized stability loss\nto preserve consistency in shared representations over time. Extensive\nexperiments on a diverse CVIT benchmark show that LiLoRA consistently achieves\nsuperior performance in sequential task learning while significantly improving\nparameter efficiency compared to existing approaches.",
      "pdf_url": "http://arxiv.org/pdf/2508.06202v1",
      "published": "2025-08-08T10:32:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06202v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning",
      "authors": [
        "Mateusz Praski",
        "Jakub Adamczyk",
        "Wojciech Czech"
      ],
      "abstract": "Pretrained neural networks have attracted significant interest in chemistry\nand small molecule drug design. Embeddings from these models are widely used\nfor molecular property prediction, virtual screening, and small data learning\nin molecular chemistry. This study presents the most extensive comparison of\nsuch models to date, evaluating 25 models across 25 datasets. Under a fair\ncomparison framework, we assess models spanning various modalities,\narchitectures, and pretraining strategies. Using a dedicated hierarchical\nBayesian statistical testing model, we arrive at a surprising result: nearly\nall neural models show negligible or no improvement over the baseline ECFP\nmolecular fingerprint. Only the CLAMP model, which is also based on molecular\nfingerprints, performs statistically significantly better than the\nalternatives. These findings raise concerns about the evaluation rigor in\nexisting studies. We discuss potential causes, propose solutions, and offer\npractical recommendations.",
      "pdf_url": "http://arxiv.org/pdf/2508.06199v1",
      "published": "2025-08-08T10:29:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.06199v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}