{
  "last_updated": "2025-03-27T00:47:11.301791",
  "papers": [
    {
      "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning",
      "authors": [
        "Hao Yu",
        "Zhuokai Zhao",
        "Shen Yan",
        "Lukasz Korycki",
        "Jianyu Wang",
        "Baosheng He",
        "Jiayi Liu",
        "Lizhu Zhang",
        "Xiangjun Fan",
        "Hanchao Yu"
      ],
      "abstract": "The rapid advancement of large vision-language models (LVLMs) has driven\nsignificant progress in multimodal tasks, enabling models to interpret, reason,\nand generate outputs across both visual and textual domains. While excelling in\ngenerative tasks, existing LVLMs often face limitations in tasks requiring\nhigh-fidelity representation learning, such as generating image or text\nembeddings for retrieval. Recent work has proposed finetuning LVLMs for\nrepresentational learning, but the fine-tuned model often loses its generative\ncapabilities due to the representational learning training paradigm. To address\nthis trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning\nframework that enhances LVLMs for both representation and generative tasks. By\nintegrating a contrastive objective with autoregressive language modeling, our\napproach unifies these traditionally separate tasks, achieving state-of-the-art\nresults in both multimodal retrieval and multimodal generative benchmarks,\nincluding object hallucination (OH) mitigation. CAFe establishes a novel\nframework that synergizes embedding and generative functionalities in a single\nmodel, setting a foundation for future multimodal models that excel in both\nretrieval precision and coherent output generation.",
      "pdf_url": "http://arxiv.org/pdf/2503.19900v1",
      "published": "2025-03-25T17:57:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19900v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A proposal for an incident regime that tracks and counters threats to national security posed by AI systems",
      "authors": [
        "Alejandro Ortega"
      ],
      "abstract": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2503.19887v1",
      "published": "2025-03-25T17:51:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19887v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Dynamics of Structured Complex-Valued Hopfield Neural Networks",
      "authors": [
        "Rama Murthy Garimella",
        "Marcos Eduardo Valle",
        "Guilherme Vieira",
        "Anil Rayala",
        "Dileep Munugoti"
      ],
      "abstract": "In this paper, we explore the dynamics of structured complex-valued Hopfield\nneural networks (CvHNNs), which arise when the synaptic weight matrix possesses\nspecific structural properties. We begin by analyzing CvHNNs with a Hermitian\nsynaptic weight matrix and establish the existence of four-cycle dynamics in\nCvHNNs with skew-Hermitian weight matrices operating synchronously.\nFurthermore, we introduce two new classes of complex-valued matrices: braided\nHermitian and braided skew-Hermitian matrices. We demonstrate that CvHNNs\nutilizing these matrix types exhibit cycles of length eight when operating in\nfull parallel update mode. Finally, we conduct extensive computational\nexperiments on synchronous CvHNNs, exploring other synaptic weight matrix\nstructures. The findings provide a comprehensive overview of the dynamics of\nstructured CvHNNs, offering insights that may contribute to developing improved\nassociative memory models when integrated with suitable learning rules.",
      "pdf_url": "http://arxiv.org/pdf/2503.19885v1",
      "published": "2025-03-25T17:49:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19885v1",
      "categories": [
        "cs.NE",
        "cs.AI"
      ]
    },
    {
      "title": "Geometric Meta-Learning via Coupled Ricci Flow: Unifying Knowledge Representation and Quantum Entanglement",
      "authors": [
        "Ming Lei",
        "Christophe Baehr"
      ],
      "abstract": "This paper establishes a unified framework integrating geometric flows with\ndeep learning through three fundamental innovations. First, we propose a\nthermodynamically coupled Ricci flow that dynamically adapts parameter space\ngeometry to loss landscape topology, formally proved to preserve isometric\nknowledge embedding (Theorem~\\ref{thm:isometric}). Second, we derive explicit\nphase transition thresholds and critical learning rates\n(Theorem~\\ref{thm:critical}) through curvature blowup analysis, enabling\nautomated singularity resolution via geometric surgery\n(Lemma~\\ref{lem:surgery}). Third, we establish an AdS/CFT-type holographic\nduality (Theorem~\\ref{thm:ads}) between neural networks and conformal field\ntheories, providing entanglement entropy bounds for regularization design.\nExperiments demonstrate 2.1$\\times$ convergence acceleration and 63\\%\ntopological simplification while maintaining $\\mathcal{O}(N\\log N)$ complexity,\noutperforming Riemannian baselines by 15.2\\% in few-shot accuracy.\nTheoretically, we prove exponential stability (Theorem~\\ref{thm:converge})\nthrough a new Lyapunov function combining Perelman entropy with Wasserstein\ngradient flows, fundamentally advancing geometric deep learning.",
      "pdf_url": "http://arxiv.org/pdf/2503.19867v1",
      "published": "2025-03-25T17:32:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19867v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "math.GT",
        "quant-ph",
        "68T05, 68T07, 68T27, 81V99, 37F40,",
        "I.2; K.3.2; F.4.1"
      ]
    },
    {
      "title": "GENIUS: A Generative Framework for Universal Multimodal Search",
      "authors": [
        "Sungyeon Kim",
        "Xinliang Zhu",
        "Xiaofan Lin",
        "Muhammet Bastan",
        "Douglas Gray",
        "Suha Kwak"
      ],
      "abstract": "Generative retrieval is an emerging approach in information retrieval that\ngenerates identifiers (IDs) of target data based on a query, providing an\nefficient alternative to traditional embedding-based retrieval methods.\nHowever, existing models are task-specific and fall short of embedding-based\nretrieval in performance. This paper proposes GENIUS, a universal generative\nretrieval framework supporting diverse tasks across multiple modalities and\ndomains. At its core, GENIUS introduces modality-decoupled semantic\nquantization, transforming multimodal data into discrete IDs encoding both\nmodality and semantics. Moreover, to enhance generalization, we propose a query\naugmentation that interpolates between a query and its target, allowing GENIUS\nto adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses\nprior generative methods by a clear margin. Unlike embedding-based retrieval,\nGENIUS consistently maintains high retrieval speed across database size, with\ncompetitive performance across multiple benchmarks. With additional re-ranking,\nGENIUS often achieves results close to those of embedding-based methods while\npreserving efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2503.19868v1",
      "published": "2025-03-25T17:32:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19868v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Guarding against artificial intelligence--hallucinated citations: the case for full-text reference deposit",
      "authors": [
        "Alex Glynn"
      ],
      "abstract": "The tendency of generative artificial intelligence (AI) systems to\n\"hallucinate\" false information is well-known; AI-generated citations to\nnon-existent sources have made their way into the reference lists of\npeer-reviewed publications. Here, I propose a solution to this problem, taking\ninspiration from the Transparency and Openness Promotion (TOP) data sharing\nguidelines, the clash of generative AI with the American judiciary, and the\nprecedent set by submissions of prior art to the United States Patent and\nTrademark Office. Journals should require authors to submit the full text of\neach cited source along with their manuscripts, thereby preventing authors from\nciting any material whose full text they cannot produce. This solution requires\nlimited additional work on the part of authors or editors while effectively\nimmunizing journals against hallucinated references.",
      "pdf_url": "http://arxiv.org/pdf/2503.19848v1",
      "published": "2025-03-25T17:12:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19848v1",
      "categories": [
        "cs.DL",
        "cs.AI",
        "I.2.0; K.4.1"
      ]
    },
    {
      "title": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950",
      "authors": [
        "Zhao Fang",
        "Liang-Chun Wu",
        "Xuening Kong",
        "Spencer Dean Stewart"
      ],
      "abstract": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data.",
      "pdf_url": "http://arxiv.org/pdf/2503.19844v1",
      "published": "2025-03-25T17:07:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19844v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity Optimization",
      "authors": [
        "Yan Zhuang",
        "Minheng Chen",
        "Chao Cao",
        "Tong Chen",
        "Jing Zhang",
        "Xiaowei Yu",
        "Yanjun Lyu",
        "Lu Zhang",
        "Tianming Liu",
        "Dajiang Zhu"
      ],
      "abstract": "Understanding the structural and functional organization of the human brain\nrequires a detailed examination of cortical folding patterns, among which the\nthree-hinge gyrus (3HG) has been identified as a key structural landmark.\nGyralNet, a network representation of cortical folding, models 3HGs as nodes\nand gyral crests as edges, highlighting their role as critical hubs in\ncortico-cortical connectivity. However, existing methods for analyzing 3HGs\nface significant challenges, including the sub-voxel scale of 3HGs at typical\nneuroimaging resolutions, the computational complexity of establishing\ncross-subject correspondences, and the oversimplification of treating 3HGs as\nindependent nodes without considering their community-level relationships. To\naddress these limitations, we propose a fully differentiable subnetwork\npartitioning framework that employs a spectral modularity maximization\noptimization strategy to modularize the organization of 3HGs within GyralNet.\nBy incorporating topological structural similarity and DTI-derived connectivity\npatterns as attribute features, our approach provides a biologically meaningful\nrepresentation of cortical organization. Extensive experiments on the Human\nConnectome Project (HCP) dataset demonstrate that our method effectively\npartitions GyralNet at the individual level while preserving the\ncommunity-level consistency of 3HGs across subjects, offering a robust\nfoundation for understanding brain connectivity.",
      "pdf_url": "http://arxiv.org/pdf/2503.19823v1",
      "published": "2025-03-25T16:33:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19823v1",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Bitstream Collisions in Neural Image Compression via Adversarial Perturbations",
      "authors": [
        "Jordan Madden",
        "Lhamo Dorje",
        "Xiaohua Li"
      ],
      "abstract": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented.",
      "pdf_url": "http://arxiv.org/pdf/2503.19817v1",
      "published": "2025-03-25T16:29:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19817v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Thinking agents for zero-shot generalization to qualitatively novel tasks",
      "authors": [
        "Thomas Miconi",
        "Kevin McKee",
        "Yicong Zheng",
        "Jed McCaleb"
      ],
      "abstract": "Intelligent organisms can solve truly novel problems which they have never\nencountered before, either in their lifetime or their evolution. An important\ncomponent of this capacity is the ability to ``think'', that is, to mentally\nmanipulate objects, concepts and behaviors in order to plan and evaluate\npossible solutions to novel problems, even without environment interaction. To\ngenerate problems that are truly qualitatively novel, while still solvable\nzero-shot (by mental simulation), we use the combinatorial nature of\nenvironments: we train the agent while withholding a specific combination of\nthe environment's elements. The novel test task, based on this combination, is\nthus guaranteed to be truly novel, while still mentally simulable since the\nagent has been exposed to each individual element (and their pairwise\ninteractions) during training. We propose a method to train agents endowed with\nworld models to make use their mental simulation abilities, by selecting tasks\nbased on the difference between the agent's pre-thinking and post-thinking\nperformance. When tested on the novel, withheld problem, the resulting agent\nsuccessfully simulated alternative scenarios and used the resulting information\nto guide its behavior in the actual environment, solving the novel task in a\nsingle real-environment trial (zero-shot).",
      "pdf_url": "http://arxiv.org/pdf/2503.19815v1",
      "published": "2025-03-25T16:26:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19815v1",
      "categories": [
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Guidelines For The Choice Of The Baseline in XAI Attribution Methods",
      "authors": [
        "Cristian Morasso",
        "Giorgio Dolci",
        "Ilaria Boscolo Galazzo",
        "Sergey M. Plis",
        "Gloria Menegaz"
      ],
      "abstract": "Given the broad adoption of artificial intelligence, it is essential to\nprovide evidence that AI models are reliable, trustable, and fair. To this end,\nthe emerging field of eXplainable AI develops techniques to probe such\nrequirements, counterbalancing the hype pushing the pervasiveness of this\ntechnology. Among the many facets of this issue, this paper focuses on baseline\nattribution methods, aiming at deriving a feature attribution map at the\nnetwork input relying on a \"neutral\" stimulus usually called \"baseline\". The\nchoice of the baseline is crucial as it determines the explanation of the\nnetwork behavior. In this framework, this paper has the twofold goal of\nshedding light on the implications of the choice of the baseline and providing\na simple yet effective method for identifying the best baseline for the task.\nTo achieve this, we propose a decision boundary sampling method, since the\nbaseline, by definition, lies on the decision boundary, which naturally becomes\nthe search domain. Experiments are performed on synthetic examples and\nvalidated relying on state-of-the-art methods. Despite being limited to the\nexperimental scope, this contribution is relevant as it offers clear guidelines\nand a simple proxy for baseline selection, reducing ambiguity and enhancing\ndeep models' reliability and trust.",
      "pdf_url": "http://arxiv.org/pdf/2503.19813v1",
      "published": "2025-03-25T16:25:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19813v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Simulating Tracking Data to Advance Sports Analytics Research",
      "authors": [
        "David Radke",
        "Kyle Tilbury"
      ],
      "abstract": "Advanced analytics have transformed how sports teams operate, particularly in\nepisodic sports like baseball. Their impact on continuous invasion sports, such\nas soccer and ice hockey, has been limited due to increased game complexity and\nrestricted access to high-resolution game tracking data. In this demo, we\npresent a method to collect and utilize simulated soccer tracking data from the\nGoogle Research Football environment to support the development of models\ndesigned for continuous tracking data. The data is stored in a schema that is\nrepresentative of real tracking data and we provide processes that extract\nhigh-level features and events. We include examples of established tracking\ndata models to showcase the efficacy of the simulated data. We address the\nscarcity of publicly available tracking data, providing support for research at\nthe intersection of artificial intelligence and sports analytics.",
      "pdf_url": "http://arxiv.org/pdf/2503.19809v1",
      "published": "2025-03-25T16:18:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19809v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset",
      "authors": [
        "Manjushree Aithal",
        "Rosaura G. VidalMata",
        "Manikandtan Kartha",
        "Gong Chen",
        "Eashan Adhikarla",
        "Lucas N. Kirsten",
        "Zhicheng Fu",
        "Nikhil A. Madhusudhana",
        "Joe Nasti"
      ],
      "abstract": "Low-light image enhancement is crucial for a myriad of applications, from\nnight vision and surveillance, to autonomous driving. However, due to the\ninherent limitations that come in hand with capturing images in\nlow-illumination environments, the task of enhancing such scenes still presents\na formidable challenge. To advance research in this field, we introduce our Low\nExposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure\nbenchmark dataset for low-light image enhancement comprising of over 230K\nframes showcasing 24K real-world indoor and outdoor, with-and without human,\nscenes. Captured using 3 different camera sensors, LENVIZ offers a wide range\nof lighting conditions, noise levels, and scene complexities, making it the\nlargest publicly available up-to 4K resolution benchmark in the field. LENVIZ\nincludes high quality human-generated ground truth, for which each\nmulti-exposure low-light scene has been meticulously curated and edited by\nexpert photographers to ensure optimal image quality. Furthermore, we also\nconduct a comprehensive analysis of current state-of-the-art low-light image\nenhancement techniques on our dataset and highlight potential areas of\nimprovement.",
      "pdf_url": "http://arxiv.org/pdf/2503.19804v1",
      "published": "2025-03-25T16:12:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19804v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SeLIP: Similarity Enhanced Contrastive Language Image Pretraining for Multi-modal Head MRI",
      "authors": [
        "Zhiyang Liu",
        "Dong Yang",
        "Minghao Zhang",
        "Hanyu Sun",
        "Hong Wu",
        "Huiying Wang",
        "Wen Shen",
        "Chao Chai",
        "Shuang Xia"
      ],
      "abstract": "Despite that deep learning (DL) methods have presented tremendous potential\nin many medical image analysis tasks, the practical applications of medical DL\nmodels are limited due to the lack of enough data samples with manual\nannotations. By noting that the clinical radiology examinations are associated\nwith radiology reports that describe the images, we propose to develop a\nfoundation model for multi-model head MRI by using contrastive learning on the\nimages and the corresponding radiology findings. In particular, a contrastive\nlearning framework is proposed, where a mixed syntax and semantic similarity\nmatching metric is integrated to reduce the thirst of extreme large dataset in\nconventional contrastive learning framework. Our proposed similarity enhanced\ncontrastive language image pretraining (SeLIP) is able to effectively extract\nmore useful features. Experiments revealed that our proposed SeLIP performs\nwell in many downstream tasks including image-text retrieval task,\nclassification task, and image segmentation, which highlights the importance of\nconsidering the similarities among texts describing different images in\ndeveloping medical image foundation models.",
      "pdf_url": "http://arxiv.org/pdf/2503.19801v1",
      "published": "2025-03-25T16:09:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19801v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "PAVE: Patching and Adapting Video Large Language Models",
      "authors": [
        "Zhuoming Liu",
        "Yiquan Li",
        "Khoi Duc Nguyen",
        "Yiwu Zhong",
        "Yin Li"
      ],
      "abstract": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.",
      "pdf_url": "http://arxiv.org/pdf/2503.19794v1",
      "published": "2025-03-25T16:02:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19794v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Gemma 3 Technical Report",
      "authors": [
        "Gemma Team",
        "Aishwarya Kamath",
        "Johan Ferret",
        "Shreya Pathak",
        "Nino Vieillard",
        "Ramona Merhej",
        "Sarah Perrin",
        "Tatiana Matejovicova",
        "Alexandre Ramé",
        "Morgane Rivière",
        "Louis Rouillard",
        "Thomas Mesnard",
        "Geoffrey Cideron",
        "Jean-bastien Grill",
        "Sabela Ramos",
        "Edouard Yvinec",
        "Michelle Casbon",
        "Etienne Pot",
        "Ivo Penchev",
        "Gaël Liu",
        "Francesco Visin",
        "Kathleen Kenealy",
        "Lucas Beyer",
        "Xiaohai Zhai",
        "Anton Tsitsulin",
        "Robert Busa-Fekete",
        "Alex Feng",
        "Noveen Sachdeva",
        "Benjamin Coleman",
        "Yi Gao",
        "Basil Mustafa",
        "Iain Barr",
        "Emilio Parisotto",
        "David Tian",
        "Matan Eyal",
        "Colin Cherry",
        "Jan-Thorsten Peter",
        "Danila Sinopalnikov",
        "Surya Bhupatiraju",
        "Rishabh Agarwal",
        "Mehran Kazemi",
        "Dan Malkin",
        "Ravin Kumar",
        "David Vilar",
        "Idan Brusilovsky",
        "Jiaming Luo",
        "Andreas Steiner",
        "Abe Friesen",
        "Abhanshu Sharma",
        "Abheesht Sharma",
        "Adi Mayrav Gilady",
        "Adrian Goedeckemeyer",
        "Alaa Saade",
        "Alex Feng",
        "Alexander Kolesnikov",
        "Alexei Bendebury",
        "Alvin Abdagic",
        "Amit Vadi",
        "András György",
        "André Susano Pinto",
        "Anil Das",
        "Ankur Bapna",
        "Antoine Miech",
        "Antoine Yang",
        "Antonia Paterson",
        "Ashish Shenoy",
        "Ayan Chakrabarti",
        "Bilal Piot",
        "Bo Wu",
        "Bobak Shahriari",
        "Bryce Petrini",
        "Charlie Chen",
        "Charline Le Lan",
        "Christopher A. Choquette-Choo",
        "CJ Carey",
        "Cormac Brick",
        "Daniel Deutsch",
        "Danielle Eisenbud",
        "Dee Cattle",
        "Derek Cheng",
        "Dimitris Paparas",
        "Divyashree Shivakumar Sreepathihalli",
        "Doug Reid",
        "Dustin Tran",
        "Dustin Zelle",
        "Eric Noland",
        "Erwin Huizenga",
        "Eugene Kharitonov",
        "Frederick Liu",
        "Gagik Amirkhanyan",
        "Glenn Cameron",
        "Hadi Hashemi",
        "Hanna Klimczak-Plucińska",
        "Harman Singh",
        "Harsh Mehta",
        "Harshal Tushar Lehri",
        "Hussein Hazimeh",
        "Ian Ballantyne",
        "Idan Szpektor",
        "Ivan Nardini",
        "Jean Pouget-Abadie",
        "Jetha Chan",
        "Joe Stanton",
        "John Wieting",
        "Jonathan Lai",
        "Jordi Orbay",
        "Joseph Fernandez",
        "Josh Newlan",
        "Ju-yeong Ji",
        "Jyotinder Singh",
        "Kat Black",
        "Kathy Yu",
        "Kevin Hui",
        "Kiran Vodrahalli",
        "Klaus Greff",
        "Linhai Qiu",
        "Marcella Valentine",
        "Marina Coelho",
        "Marvin Ritter",
        "Matt Hoffman",
        "Matthew Watson",
        "Mayank Chaturvedi",
        "Michael Moynihan",
        "Min Ma",
        "Nabila Babar",
        "Natasha Noy",
        "Nathan Byrd",
        "Nick Roy",
        "Nikola Momchev",
        "Nilay Chauhan",
        "Noveen Sachdeva",
        "Oskar Bunyan",
        "Pankil Botarda",
        "Paul Caron",
        "Paul Kishan Rubenstein",
        "Phil Culliton",
        "Philipp Schmid",
        "Pier Giuseppe Sessa",
        "Pingmei Xu",
        "Piotr Stanczyk",
        "Pouya Tafti",
        "Rakesh Shivanna",
        "Renjie Wu",
        "Renke Pan",
        "Reza Rokni",
        "Rob Willoughby",
        "Rohith Vallu",
        "Ryan Mullins",
        "Sammy Jerome",
        "Sara Smoot",
        "Sertan Girgin",
        "Shariq Iqbal",
        "Shashir Reddy",
        "Shruti Sheth",
        "Siim Põder",
        "Sijal Bhatnagar",
        "Sindhu Raghuram Panyam",
        "Sivan Eiger",
        "Susan Zhang",
        "Tianqi Liu",
        "Trevor Yacovone",
        "Tyler Liechty",
        "Uday Kalra",
        "Utku Evci",
        "Vedant Misra",
        "Vincent Roseberry",
        "Vlad Feinberg",
        "Vlad Kolesnikov",
        "Woohyun Han",
        "Woosuk Kwon",
        "Xi Chen",
        "Yinlam Chow",
        "Yuvein Zhu",
        "Zichuan Wei",
        "Zoltan Egyed",
        "Victor Cotruta",
        "Minh Giang",
        "Phoebe Kirk",
        "Anand Rao",
        "Kat Black",
        "Nabila Babar",
        "Jessica Lo",
        "Erica Moreira",
        "Luiz Gustavo Martins",
        "Omar Sanseviero",
        "Lucas Gonzalez",
        "Zach Gleicher",
        "Tris Warkentin",
        "Vahab Mirrokni",
        "Evan Senter",
        "Eli Collins",
        "Joelle Barral",
        "Zoubin Ghahramani",
        "Raia Hadsell",
        "Yossi Matias",
        "D. Sculley",
        "Slav Petrov",
        "Noah Fiedel",
        "Noam Shazeer",
        "Oriol Vinyals",
        "Jeff Dean",
        "Demis Hassabis",
        "Koray Kavukcuoglu",
        "Clement Farabet",
        "Elena Buchatskaya",
        "Jean-Baptiste Alayrac",
        "Rohan Anil",
        "Dmitry",
        "Lepikhin",
        "Sebastian Borgeaud",
        "Olivier Bachem",
        "Armand Joulin",
        "Alek Andreev",
        "Cassidy Hardin",
        "Robert Dadashi",
        "Léonard Hussenot"
      ],
      "abstract": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
      "pdf_url": "http://arxiv.org/pdf/2503.19786v1",
      "published": "2025-03-25T15:52:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19786v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Splitting Answer Set Programs with respect to Intensionality Statements (Extended Version)",
      "authors": [
        "Jorge Fandinno",
        "Yuliya Lierler"
      ],
      "abstract": "Splitting a logic program allows us to reduce the task of computing its\nstable models to similar tasks for its subprograms. This can be used to\nincrease solving performance and prove program correctness. We generalize the\nconditions under which this technique is applicable, by considering not only\ndependencies between predicates but also their arguments and context. This\nallows splitting programs commonly used in practice to which previous results\nwere not applicable.",
      "pdf_url": "http://arxiv.org/pdf/2503.19762v1",
      "published": "2025-03-25T15:27:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19762v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "A Survey on Event-driven 3D Reconstruction: Development under Different Categories",
      "authors": [
        "Chuanzhi Xu",
        "Haoxian Zhou",
        "Haodong Chen",
        "Vera Chung",
        "Qiang Qu"
      ],
      "abstract": "Event cameras have gained increasing attention for 3D reconstruction due to\ntheir high temporal resolution, low latency, and high dynamic range. They\ncapture per-pixel brightness changes asynchronously, allowing accurate\nreconstruction under fast motion and challenging lighting conditions. In this\nsurvey, we provide a comprehensive review of event-driven 3D reconstruction\nmethods, including stereo, monocular, and multimodal systems. We further\ncategorize recent developments based on geometric, learning-based, and hybrid\napproaches. Emerging trends, such as neural radiance fields and 3D Gaussian\nsplatting with event data, are also covered. The related works are structured\nchronologically to illustrate the innovations and progression within the field.\nTo support future research, we also highlight key research gaps and future\nresearch directions in dataset, experiment, evaluation, event representation,\netc.",
      "pdf_url": "http://arxiv.org/pdf/2503.19753v2",
      "published": "2025-03-25T15:16:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19753v2",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect on Human-Like Agenda Generation",
      "authors": [
        "Lewis Newsham",
        "Ryan Hyland",
        "Daniel Prince"
      ],
      "abstract": "This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies.",
      "pdf_url": "http://arxiv.org/pdf/2503.19752v1",
      "published": "2025-03-25T15:16:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19752v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "CamSAM2: Segment Anything Accurately in Camouflaged Videos",
      "authors": [
        "Yuli Zhou",
        "Guolei Sun",
        "Yawei Li",
        "Yuqian Fu",
        "Luca Benini",
        "Ender Konukoglu"
      ],
      "abstract": "Video camouflaged object segmentation (VCOS), aiming at segmenting\ncamouflaged objects that seamlessly blend into their environment, is a\nfundamental vision task with various real-world applications. With the release\nof SAM2, video segmentation has witnessed significant progress. However, SAM2's\ncapability of segmenting camouflaged videos is suboptimal, especially when\ngiven simple prompts such as point and box. To address the problem, we propose\nCamouflaged SAM2 (CamSAM2), which enhances SAM2's ability to handle camouflaged\nscenes without modifying SAM2's parameters. Specifically, we introduce a\ndecamouflaged token to provide the flexibility of feature adjustment for VCOS.\nTo make full use of fine-grained and high-resolution features from the current\nframe and previous frames, we propose implicit object-aware fusion (IOF) and\nexplicit object-aware fusion (EOF) modules, respectively. Object prototype\ngeneration (OPG) is introduced to abstract and memorize object prototypes with\ninformative details using high-quality features from previous frames. Extensive\nexperiments are conducted to validate the effectiveness of our approach. While\nCamSAM2 only adds negligible learnable parameters to SAM2, it substantially\noutperforms SAM2 on three VCOS datasets, especially achieving 12.2 mDice gains\nwith click prompt on MoCA-Mask and 19.6 mDice gains with mask prompt on\nSUN-SEG-Hard, with Hiera-T as the backbone. The code will be available at\nhttps://github.com/zhoustan/CamSAM2.",
      "pdf_url": "http://arxiv.org/pdf/2503.19730v2",
      "published": "2025-03-25T14:58:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19730v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?",
      "authors": [
        "Francisco Mena",
        "Diego Arenas",
        "Miro Miranda",
        "Andreas Dengel"
      ],
      "abstract": "In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications.",
      "pdf_url": "http://arxiv.org/pdf/2503.19719v1",
      "published": "2025-03-25T14:45:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19719v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Invertible Koopman neural operator for data-driven modeling of partial differential equations",
      "authors": [
        "Yuhong Jin",
        "Andong Cong",
        "Lei Hou",
        "Qiang Gao",
        "Xiangdong Ge",
        "Chonglong Zhu",
        "Yongzhi Feng",
        "Jun Li"
      ],
      "abstract": "Koopman operator theory is a popular candidate for data-driven modeling\nbecause it provides a global linearization representation for nonlinear\ndynamical systems. However, existing Koopman operator-based methods suffer from\nshortcomings in constructing the well-behaved observable function and its\ninverse and are inefficient enough when dealing with partial differential\nequations (PDEs). To address these issues, this paper proposes the Invertible\nKoopman Neural Operator (IKNO), a novel data-driven modeling approach inspired\nby the Koopman operator theory and neural operator. IKNO leverages an\nInvertible Neural Network to parameterize observable function and its inverse\nsimultaneously under the same learnable parameters, explicitly guaranteeing the\nreconstruction relation, thus eliminating the dependency on the reconstruction\nloss, which is an essential improvement over the original Koopman Neural\nOperator (KNO). The structured linear matrix inspired by the Koopman operator\ntheory is parameterized to learn the evolution of observables' low-frequency\nmodes in the frequency space rather than directly in the observable space,\nsustaining IKNO is resolution-invariant like other neural operators. Moreover,\nwith preprocessing such as interpolation and dimension expansion, IKNO can be\nextended to operator learning tasks defined on non-Cartesian domains. We fully\nsupport the above claims based on rich numerical and real-world examples and\ndemonstrate the effectiveness of IKNO and superiority over other neural\noperators.",
      "pdf_url": "http://arxiv.org/pdf/2503.19717v1",
      "published": "2025-03-25T14:43:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19717v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions",
      "authors": [
        "Sanghyuk Kim",
        "Minsik Seo",
        "Namwoo Kang"
      ],
      "abstract": "This study proposes a neural framework that predicts 3D vehicle collision\ndynamics by independently modeling global rigid-body motion and local\nstructural deformation. Unlike approaches directly predicting absolute\ndisplacement, this method explicitly separates the vehicle's overall\ntranslation and rotation from its structural deformation. Two specialized\nnetworks form the core of the framework: a quaternion-based Rigid Net for rigid\nmotion and a coordinate-based Deformation Net for local deformation. By\nindependently handling fundamentally distinct physical phenomena, the proposed\narchitecture achieves accurate predictions without requiring separate\nsupervision for each component. The model, trained on only 10% of available\nsimulation data, significantly outperforms baseline models, including single\nmulti-layer perceptron (MLP) and deep operator networks (DeepONet), with\nprediction errors reduced by up to 83%. Extensive validation demonstrates\nstrong generalization to collision conditions outside the training range,\naccurately predicting responses even under severe impacts involving extreme\nvelocities and large impact angles. Furthermore, the framework successfully\nreconstructs high-resolution deformation details from low-resolution inputs\nwithout increased computational effort. Consequently, the proposed approach\nprovides an effective, computationally efficient method for rapid and reliable\nassessment of vehicle safety across complex collision scenarios, substantially\nreducing the required simulation data and time while preserving prediction\nfidelity.",
      "pdf_url": "http://arxiv.org/pdf/2503.19712v1",
      "published": "2025-03-25T14:38:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19712v1",
      "categories": [
        "cs.CE",
        "cs.AI"
      ]
    },
    {
      "title": "Writing as a testbed for open ended agents",
      "authors": [
        "Sian Gooding",
        "Lucia Lopez-Rivilla",
        "Edward Grefenstette"
      ],
      "abstract": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.",
      "pdf_url": "http://arxiv.org/pdf/2503.19711v1",
      "published": "2025-03-25T14:38:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19711v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations",
      "authors": [
        "Jungin Park",
        "Jiyoung Lee",
        "Kwanghoon Sohn"
      ],
      "abstract": "View-invariant representation learning from egocentric (first-person, ego)\nand exocentric (third-person, exo) videos is a promising approach toward\ngeneralizing video understanding systems across multiple viewpoints. However,\nthis area has been underexplored due to the substantial differences in\nperspective, motion patterns, and context between ego and exo views. In this\npaper, we propose a novel masked ego-exo modeling that promotes both causal\ntemporal dynamics and cross-view alignment, called Bootstrap Your Own Views\n(BYOV), for fine-grained view-invariant video representation learning from\nunpaired ego-exo videos. We highlight the importance of capturing the\ncompositional nature of human actions as a basis for robust cross-view\nunderstanding. Specifically, self-view masking and cross-view masking\npredictions are designed to learn view-invariant and powerful representations\nconcurrently. Experimental results demonstrate that our BYOV significantly\nsurpasses existing approaches with notable gains across all metrics in four\ndownstream ego-exo video tasks. The code is available at\nhttps://github.com/park-jungin/byov.",
      "pdf_url": "http://arxiv.org/pdf/2503.19706v1",
      "published": "2025-03-25T14:33:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19706v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control",
      "authors": [
        "Muhammad Al-Zafar Khan",
        "Jamal Al-Karaki"
      ],
      "abstract": "In this study, we formulate the drone delivery problem as a control problem\nand solve it using Model Predictive Control. Two experiments are performed: The\nfirst is on a less challenging grid world environment with lower\ndimensionality, and the second is with a higher dimensionality and added\ncomplexity. The MPC method was benchmarked against three popular Multi-Agent\nReinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action\nLearners (JAL), and Value-Decomposition Networks (VDN). It was shown that the\nMPC method solved the problem quicker and required fewer optimal numbers of\ndrones to achieve a minimized cost and navigate the optimal path.",
      "pdf_url": "http://arxiv.org/pdf/2503.19699v1",
      "published": "2025-03-25T14:27:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19699v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing Mel Spectrograms",
      "authors": [
        "Niketa Penumajji"
      ],
      "abstract": "This paper explores the application of Convolutional Neural Networks CNNs for\nclassifying emotions in speech through Mel Spectrogram representations of audio\nfiles. Traditional methods such as Gaussian Mixture Models and Hidden Markov\nModels have proven insufficient for practical deployment, prompting a shift\ntowards deep learning techniques. By transforming audio data into a visual\nformat, the CNN model autonomously learns to identify intricate patterns,\nenhancing classification accuracy. The developed model is integrated into a\nuser-friendly graphical interface, facilitating realtime predictions and\npotential applications in educational environments. The study aims to advance\nthe understanding of deep learning in speech emotion recognition, assess the\nmodels feasibility, and contribute to the integration of technology in learning\ncontexts",
      "pdf_url": "http://arxiv.org/pdf/2503.19677v1",
      "published": "2025-03-25T14:02:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19677v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata Extraction",
      "authors": [
        "Jan Kohút",
        "Martin Dočekal",
        "Michal Hradiš",
        "Marek Vaško"
      ],
      "abstract": "Manual digitization of bibliographic metadata is time consuming and labor\nintensive, especially for historical and real-world archives with highly\nvariable formatting across documents. Despite advances in machine learning, the\nabsence of dedicated datasets for metadata extraction hinders automation. To\naddress this gap, we introduce BiblioPage, a dataset of scanned title pages\nannotated with structured bibliographic metadata. The dataset consists of\napproximately 2,000 monograph title pages collected from 14 Czech libraries,\nspanning a wide range of publication periods, typographic styles, and layout\nstructures. Each title page is annotated with 16 bibliographic attributes,\nincluding title, contributors, and publication metadata, along with precise\npositional information in the form of bounding boxes. To extract structured\ninformation from this dataset, we valuated object detection models such as YOLO\nand DETR combined with transformer-based OCR, achieving a maximum mAP of 52 and\nan F1 score of 59. Additionally, we assess the performance of various visual\nlarge language models, including LlamA 3.2-Vision and GPT-4o, with the best\nmodel reaching an F1 score of 67. BiblioPage serves as a real-world benchmark\nfor bibliographic metadata extraction, contributing to document understanding,\ndocument question answering, and document information extraction. Dataset and\nevaluation scripts are availible at: https://github.com/DCGM/biblio-dataset",
      "pdf_url": "http://arxiv.org/pdf/2503.19658v1",
      "published": "2025-03-25T13:46:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19658v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Towards Reliable Time Series Forecasting under Future Uncertainty: Ambiguity and Novelty Rejection Mechanisms",
      "authors": [
        "Ninghui Feng",
        "Songning Lai",
        "Xin Zhou",
        "Jiayu Yang",
        "Kunlong Feng",
        "Zhenxiao Yin",
        "Fobao Zhou",
        "Zhangyi Hu",
        "Yutao Yue",
        "Yuxuan Liang",
        "Boyu Wang",
        "Hang Zhao"
      ],
      "abstract": "In real-world time series forecasting, uncertainty and lack of reliable\nevaluation pose significant challenges. Notably, forecasting errors often arise\nfrom underfitting in-distribution data and failing to handle\nout-of-distribution inputs. To enhance model reliability, we introduce a dual\nrejection mechanism combining ambiguity and novelty rejection. Ambiguity\nrejection, using prediction error variance, allows the model to abstain under\nlow confidence, assessed through historical error variance analysis without\nfuture ground truth. Novelty rejection, employing Variational Autoencoders and\nMahalanobis distance, detects deviations from training data. This dual approach\nimproves forecasting reliability in dynamic environments by reducing errors and\nadapting to data changes, advancing reliability in complex scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2503.19656v1",
      "published": "2025-03-25T13:44:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19656v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models",
      "authors": [
        "Mehdi Moshtaghi",
        "Siavash H. Khajavi",
        "Joni Pajarinen"
      ],
      "abstract": "We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2503.19654v1",
      "published": "2025-03-25T13:43:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19654v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "OpenSDI: Spotting Diffusion-Generated Images in the Open World",
      "authors": [
        "Yabin Wang",
        "Zhiwu Huang",
        "Xiaopeng Hong"
      ],
      "abstract": "This paper identifies OpenSDI, a challenge for spotting diffusion-generated\nimages in open-world settings. In response to this challenge, we define a new\nbenchmark, the OpenSDI dataset (OpenSDID), which stands out from existing\ndatasets due to its diverse use of large vision-language models that simulate\nopen-world diffusion-based manipulations. Another outstanding feature of\nOpenSDID is its inclusion of both detection and localization tasks for images\nmanipulated globally and locally by diffusion models. To address the OpenSDI\nchallenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up\na mixture of foundation models. This approach exploits a collaboration\nmechanism with multiple pretrained foundation models to enhance generalization\nin the OpenSDI context, moving beyond traditional training by synergizing\nmultiple pretrained models through prompting and attending strategies. Building\non this scheme, we introduce MaskCLIP, an SPM-based model that aligns\nContrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE).\nExtensive evaluations on OpenSDID show that MaskCLIP significantly outperforms\ncurrent state-of-the-art methods for the OpenSDI challenge, achieving\nremarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in\naccuracy (2.38% in F1) compared to the second-best model in localization and\ndetection tasks, respectively. Our dataset and code are available at\nhttps://github.com/iamwangyabin/OpenSDI.",
      "pdf_url": "http://arxiv.org/pdf/2503.19653v1",
      "published": "2025-03-25T13:43:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19653v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection",
      "authors": [
        "Maryam Bala",
        "Amina Imam Abubakar",
        "Abdulhamid Abubakar",
        "Abdulkadir Shehu Bichi",
        "Hafsa Kabir Ahmad",
        "Sani Abdullahi Sani",
        "Idris Abdulmumin",
        "Shamsuddeen Hassan Muhamad",
        "Ibrahim Said Ahmad"
      ],
      "abstract": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable.",
      "pdf_url": "http://arxiv.org/pdf/2503.19650v1",
      "published": "2025-03-25T13:40:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19650v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Recover from Horcrux: A Spectrogram Augmentation Method for Cardiac Feature Monitoring from Radar Signal Components",
      "authors": [
        "Yuanyuan Zhang",
        "Sijie Xiong",
        "Rui Yang",
        "EngGee Lim",
        "Yutao Yue"
      ],
      "abstract": "Radar-based wellness monitoring is becoming an effective measurement to\nprovide accurate vital signs in a contactless manner, but data scarcity retards\nthe related research on deep-learning-based methods. Data augmentation is\ncommonly used to enrich the dataset by modifying the existing data, but most\naugmentation techniques can only couple with classification tasks. To enable\nthe augmentation for regression tasks, this research proposes a spectrogram\naugmentation method, Horcrux, for radar-based cardiac feature monitoring (e.g.,\nheartbeat detection, electrocardiogram reconstruction) with both classification\nand regression tasks involved. The proposed method is designed to increase the\ndiversity of input samples while the augmented spectrogram is still faithful to\nthe original ground truth vital sign. In addition, Horcrux proposes to inject\nzero values in specific areas to enhance the awareness of the deep learning\nmodel on subtle cardiac features, improving the performance for the limited\ndataset. Experimental result shows that Horcrux achieves an overall improvement\nof 16.20% in cardiac monitoring and has the potential to be extended to other\nspectrogram-based tasks. The code will be released upon publication.",
      "pdf_url": "http://arxiv.org/pdf/2503.19649v1",
      "published": "2025-03-25T13:40:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19649v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation",
      "authors": [
        "Niccolo Avogaro",
        "Thomas Frick",
        "Mattia Rigotti",
        "Andrea Bartezzaghi",
        "Filip Janicki",
        "Cristiano Malossi",
        "Konrad Schindler",
        "Roy Assaf"
      ],
      "abstract": "Large Vision-Language Models (VLMs) are increasingly being regarded as\nfoundation models that can be instructed to solve diverse tasks by prompting,\nwithout task-specific training. We examine the seemingly obvious question: how\nto effectively prompt VLMs for semantic segmentation. To that end, we\nsystematically evaluate the segmentation performance of several recent models\nguided by either text or visual prompts on the out-of-distribution MESS dataset\ncollection. We introduce a scalable prompting scheme, few-shot prompted\nsemantic segmentation, inspired by open-vocabulary segmentation and few-shot\nlearning. It turns out that VLMs lag far behind specialist models trained for a\nspecific segmentation task, by about 30% on average on the\nIntersection-over-Union metric. Moreover, we find that text prompts and visual\nprompts are complementary: each one of the two modes fails on many examples\nthat the other one can solve. Our analysis suggests that being able to\nanticipate the most effective prompt modality can lead to a 11% improvement in\nperformance. Motivated by our findings, we propose PromptMatcher, a remarkably\nsimple training-free baseline that combines both text and visual prompts,\nachieving state-of-the-art results outperforming the best text-prompted VLM by\n2.5%, and the top visual-prompted VLM by 3.5% on few-shot prompted semantic\nsegmentation.",
      "pdf_url": "http://arxiv.org/pdf/2503.19647v1",
      "published": "2025-03-25T13:36:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19647v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation",
      "authors": [
        "Max W. Y. Lam",
        "Yijin Xing",
        "Weiya You",
        "Jingcheng Wu",
        "Zongyu Yin",
        "Fuqiang Jiang",
        "Hangyu Liu",
        "Feng Liu",
        "Xingda Li",
        "Wei-Tsung Lu",
        "Hanyu Chen",
        "Tong Feng",
        "Tianwei Zhao",
        "Chien-Hung Liu",
        "Xuchen Song",
        "Yang Li",
        "Yahui Zhou"
      ],
      "abstract": "Autoregressive (AR) models have demonstrated impressive capabilities in\ngenerating high-fidelity music. However, the conventional next-token prediction\nparadigm in AR models does not align with the human creative process in music\ncomposition, potentially compromising the musicality of generated samples. To\novercome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT)\nprompting technique tailored for music generation. MusiCoT empowers the AR\nmodel to first outline an overall music structure before generating audio\ntokens, thereby enhancing the coherence and creativity of the resulting\ncompositions. By leveraging the contrastive language-audio pretraining (CLAP)\nmodel, we establish a chain of \"musical thoughts\", making MusiCoT scalable and\nindependent of human-labeled data, in contrast to conventional CoT methods.\nMoreover, MusiCoT allows for in-depth analysis of music structure, such as\ninstrumental arrangements, and supports music referencing -- accepting\nvariable-length audio inputs as optional style references. This innovative\napproach effectively addresses copying issues, positioning MusiCoT as a vital\npractical method for music prompting. Our experimental results indicate that\nMusiCoT consistently achieves superior performance across both objective and\nsubjective metrics, producing music quality that rivals state-of-the-art\ngeneration models.\n  Our samples are available at https://MusiCoT.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2503.19611v1",
      "published": "2025-03-25T12:51:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19611v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "eess.SP"
      ]
    },
    {
      "title": "Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review",
      "authors": [
        "Edward Gu",
        "Ho Chit Siu",
        "Melanie Platt",
        "Isabelle Hurley",
        "Jaime Peña",
        "Rohan Paleja"
      ],
      "abstract": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details.",
      "pdf_url": "http://arxiv.org/pdf/2503.19607v1",
      "published": "2025-03-25T12:43:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19607v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking",
      "authors": [
        "Yuyao Ge",
        "Shenghua Liu",
        "Yiwei Wang",
        "Lingrui Mei",
        "Lizhe Chen",
        "Baolong Bi",
        "Xueqi Cheng"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies.",
      "pdf_url": "http://arxiv.org/pdf/2503.19602v1",
      "published": "2025-03-25T12:37:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19602v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "HoarePrompt: Structural Reasoning About Program Correctness in Natural Language",
      "authors": [
        "Dimitrios Stamatios Bouras",
        "Yihan Dai",
        "Tairan Wang",
        "Yingfei Xiong",
        "Sergey Mechtaev"
      ],
      "abstract": "While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops.",
      "pdf_url": "http://arxiv.org/pdf/2503.19599v1",
      "published": "2025-03-25T12:30:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19599v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-agent Application System in Office Collaboration Scenarios",
      "authors": [
        "Songtao Sun",
        "Jingyi Li",
        "Yuanfei Dong",
        "Haoguang Liu",
        "Chenxin Xu",
        "Fuyang Li",
        "Qiang Liu"
      ],
      "abstract": "This paper introduces a multi-agent application system designed to enhance\noffice collaboration efficiency and work quality. The system integrates\nartificial intelligence, machine learning, and natural language processing\ntechnologies, achieving functionalities such as task allocation, progress\nmonitoring, and information sharing. The agents within the system are capable\nof providing personalized collaboration support based on team members' needs\nand incorporate data analysis tools to improve decision-making quality. The\npaper also proposes an intelligent agent architecture that separates Plan and\nSolver, and through techniques such as multi-turn query rewriting and business\ntool retrieval, it enhances the agent's multi-intent and multi-turn dialogue\ncapabilities. Furthermore, the paper details the design of tools and multi-turn\ndialogue in the context of office collaboration scenarios, and validates the\nsystem's effectiveness through experiments and evaluations. Ultimately, the\nsystem has demonstrated outstanding performance in real business applications,\nparticularly in query understanding, task planning, and tool calling. Looking\nforward, the system is expected to play a more significant role in addressing\ncomplex interaction issues within dynamic environments and large-scale\nmulti-agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2503.19584v2",
      "published": "2025-03-25T12:07:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19584v2",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ]
    },
    {
      "title": "FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments",
      "authors": [
        "Sree Bhargavi Balija"
      ],
      "abstract": "As artificial intelligence systems increasingly operate in Real-world\nenvironments, the integration of multi-modal data sources such as vision,\nlanguage, and audio presents both unprecedented opportunities and critical\nchallenges for achieving trustworthy intelligence. In this paper, we propose a\nnovel framework that unifies federated learning with explainable multi-modal\nreasoning to ensure trustworthiness in decentralized, dynamic settings. Our\napproach, called FedMM-X (Federated Multi-Modal Explainable Intelligence),\nleverages cross-modal consistency checks, client-level interpretability\nmechanisms, and dynamic trust calibration to address challenges posed by data\nheterogeneity, modality imbalance, and out-of-distribution generalization.\nThrough rigorous evaluation across federated multi-modal benchmarks involving\nvision-language tasks, we demonstrate improved performance in both accuracy and\ninterpretability while reducing vulnerabilities to adversarial and spurious\ncorrelations. Further, we introduce a novel trust score aggregation method to\nquantify global model reliability under dynamic client participation. Our\nfindings pave the way toward developing robust, interpretable, and socially\nresponsible AI systems in Real-world environments.",
      "pdf_url": "http://arxiv.org/pdf/2503.19564v1",
      "published": "2025-03-25T11:28:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19564v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Scaling Laws of Synthetic Data for Language Models",
      "authors": [
        "Zeyu Qin",
        "Qingxiu Dong",
        "Xingxing Zhang",
        "Li Dong",
        "Xiaolong Huang",
        "Ziyi Yang",
        "Mahmoud Khademi",
        "Dongdong Zhang",
        "Hany Hassan Awadalla",
        "Yi R. Fung",
        "Weizhu Chen",
        "Minhao Cheng",
        "Furu Wei"
      ],
      "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance.",
      "pdf_url": "http://arxiv.org/pdf/2503.19551v2",
      "published": "2025-03-25T11:07:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19551v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
      "authors": [
        "Dahyun Jung",
        "Seungyoon Lee",
        "Hyeonseok Moon",
        "Chanjun Park",
        "Heuiseok Lim"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.",
      "pdf_url": "http://arxiv.org/pdf/2503.19540v1",
      "published": "2025-03-25T10:48:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19540v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained Foundation Models",
      "authors": [
        "Suhas G Hegde",
        "Shilpy Kaur",
        "Aruna Tiwari"
      ],
      "abstract": "Popular PEFT methods achieve parameter efficiency by assuming that\nincremental weight updates are inherently low-rank, which often leads to a\nperformance gap compared to full fine-tuning. While recent methods have\nattempted to address this limitation, they typically lack sufficient parameter\nand memory efficiency. We propose VectorFit, an effective and easily deployable\napproach that adaptively trains the singular vectors and biases of pre-trained\nweight matrices. We demonstrate that the utilization of structural and\ntransformational characteristics of pre-trained weights enables high-rank\nupdates comparable to those of full fine-tuning. As a result, VectorFit\nachieves superior performance with 9X less trainable parameters compared to\nstate-of-the-art PEFT methods. Through extensive experiments over 17 datasets\nspanning diverse language and vision tasks such as natural language\nunderstanding and generation, question answering, image classification, and\nimage generation, we exhibit that VectorFit consistently outperforms baselines,\neven in extremely low-budget scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2503.19530v1",
      "published": "2025-03-25T10:36:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19530v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation",
      "authors": [
        "Sheng Wang"
      ],
      "abstract": "As robotic technologies advancing towards more complex multimodal\ninteractions and manipulation tasks, the integration of advanced\nVision-Language Models (VLMs) has become a key driver in the field. Despite\nprogress with current methods, challenges persist in fusing depth and RGB\ninformation within 3D environments and executing tasks guided by linguistic\ninstructions. In response to these challenges, we have enhanced the existing\nRoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates\ndepth data into VLMs to significantly improve robotic manipulation performance.\nOur research achieves a nuanced fusion of RGB and depth information by\nintegrating a pre-trained Vision Transformer (ViT) with a resampling technique,\nclosely aligning this combined data with linguistic cues for superior\nmultimodal understanding. The novelty of RoboFlamingo-Plus lies in its\nadaptation of inputs for depth data processing, leveraging a pre-trained\nresampler for depth feature extraction, and employing cross-attention\nmechanisms for optimal feature integration. These improvements allow\nRoboFlamingo-Plus to not only deeply understand 3D environments but also easily\nperform complex, language-guided tasks in challenging settings. Experimental\nresults show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over\ncurrent methods, marking a significant advancement. Codes and model weights are\npublic at RoboFlamingo-Plus.",
      "pdf_url": "http://arxiv.org/pdf/2503.19510v1",
      "published": "2025-03-25T10:01:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19510v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model",
      "authors": [
        "Qi Chen",
        "Yinghao Cui",
        "Guobin Hong",
        "Karumuri Ashok",
        "Yuchun Pu",
        "Xiaogu Zheng",
        "Xuanze Zhang",
        "Wei Zhong",
        "Peng Zhan",
        "Zhonglei Wang"
      ],
      "abstract": "El Ni\\~no-Southern Oscillation (ENSO) is a prominent mode of interannual\nclimate variability with far-reaching global impacts. Its evolution is governed\nby intricate air-sea interactions, posing significant challenges for long-term\nprediction. In this study, we introduce CTEFNet, a multivariate deep learning\nmodel that synergizes convolutional neural networks and transformers to enhance\nENSO forecasting. By integrating multiple oceanic and atmospheric predictors,\nCTEFNet extends the effective forecast lead time to 20 months while mitigating\nthe impact of the spring predictability barrier, outperforming both dynamical\nmodels and state-of-the-art deep learning approaches. Furthermore, CTEFNet\noffers physically meaningful and statistically significant insights through\ngradient-based sensitivity analysis, revealing the key precursor signals that\ngovern ENSO dynamics, which align with well-established theories and reveal new\ninsights about inter-basin interactions among the Pacific, Atlantic, and Indian\nOceans. The CTEFNet's superior predictive skill and interpretable sensitivity\nassessments underscore its potential for advancing climate prediction. Our\nfindings highlight the importance of multivariate coupling in ENSO evolution\nand demonstrate the promise of deep learning in capturing complex climate\ndynamics with enhanced interpretability.",
      "pdf_url": "http://arxiv.org/pdf/2503.19502v1",
      "published": "2025-03-25T09:50:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19502v1",
      "categories": [
        "physics.geo-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs",
      "authors": [
        "Vinayak Mali",
        "Saurabh Jaiswal"
      ],
      "abstract": "Falls among elderly residents in assisted living homes pose significant\nhealth risks, often leading to injuries and a decreased quality of life.\nCurrent fall detection solutions typically rely on sensor-based systems that\nrequire dedicated hardware, or on video-based models that demand high\ncomputational resources and GPUs for real-time processing. In contrast, this\npaper presents a robust fall detection system that does not require any\nadditional sensors or high-powered hardware. The system uses pose estimation\ntechniques, combined with threshold-based analysis and a voting mechanism, to\neffectively distinguish between fall and non-fall activities. For pose\ndetection, we leverage MediaPipe, a lightweight and efficient framework that\nenables real-time processing on standard CPUs with minimal computational\noverhead. By analyzing motion, body position, and key pose points, the system\nprocesses pose features with a 20-frame buffer, minimizing false positives and\nmaintaining high accuracy even in real-world settings. This unobtrusive,\nresource-efficient approach provides a practical solution for enhancing\nresident safety in old age homes, without the need for expensive sensors or\nhigh-end computational resources.",
      "pdf_url": "http://arxiv.org/pdf/2503.19501v1",
      "published": "2025-03-25T09:49:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19501v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables Design Exploration",
      "authors": [
        "Mohammad Daffa Robani",
        "Paul Saves",
        "Pramudita Satria Palar",
        "Lavi Rizki Zuhal",
        "oseph Morlier"
      ],
      "abstract": "Surrogate models are of high interest for many engineering applications,\nserving as cheap-to-evaluate time-efficient approximations of black-box\nfunctions to help engineers and practitioners make decisions and understand\ncomplex systems. As such, the need for explainability methods is rising and\nmany studies have been performed to facilitate knowledge discovery from\nsurrogate models. To respond to these enquiries, this paper introduces SMT-EX,\nan enhancement of the open-source Python Surrogate Modeling Toolbox (SMT) that\nintegrates explainability techniques into a state-of-the-art surrogate\nmodelling framework. More precisely, SMT-EX includes three key explainability\nmethods: Shapley Additive Explanations, Partial Dependence Plot, and Individual\nConditional Expectations. A peculiar explainability dependency of SMT has been\ndeveloped for such purpose that can be easily activated once the surrogate\nmodel is built, offering a user-friendly and efficient tool for swift insight\nextraction. The effectiveness of SMT-EX is showcased through two test cases.\nThe first case is a 10-variable wing weight problem with purely continuous\nvariables and the second one is a 3-variable mixed-categorical cantilever beam\nbending problem. Relying on SMT-EX analyses for these problems, we demonstrate\nits versatility in addressing a diverse range of problem characteristics.\nSMT-Explainability is freely available on Github:\nhttps://github.com/SMTorg/smt-explainability .",
      "pdf_url": "http://arxiv.org/pdf/2503.19496v1",
      "published": "2025-03-25T09:38:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19496v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "A-MESS: Anchor based Multimodal Embedding with Semantic Synchronization for Multimodal Intent Recognition",
      "authors": [
        "Yaomin Shen",
        "Xiaojian Lin",
        "Wei Fan"
      ],
      "abstract": "In the domain of multimodal intent recognition (MIR), the objective is to\nrecognize human intent by integrating a variety of modalities, such as language\ntext, body gestures, and tones. However, existing approaches face difficulties\nadequately capturing the intrinsic connections between the modalities and\noverlooking the corresponding semantic representations of intent. To address\nthese limitations, we present the Anchor-based Mul- timodal Embedding with\nSemantic Synchronization (A-MESS) framework. We first design an Anchor-based\nMultimodal Embed- ding (A-ME) module that employs an anchor-based embedding\nfusion mechanism to integrate multimodal inputs. Furthermore, we develop a\nSemantic Synchronization (SS) strategy with the Triplet Contrastive Learning\npipeline, which optimizes the pro- cess by synchronizing multimodal\nrepresentation with label de- scriptions produced by the large language model.\nComprehensive experiments indicate that our A-MESS achieves state-of-the-art\nand provides substantial insight into multimodal representation and downstream\ntasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.19474v1",
      "published": "2025-03-25T09:09:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19474v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning",
      "authors": [
        "Mingyang Chen",
        "Tianpeng Li",
        "Haoze Sun",
        "Yijie Zhou",
        "Chenzheng Zhu",
        "Fan Yang",
        "Zenan Zhou",
        "Weipeng Chen",
        "Haofen Wang",
        "Jeff Z. Pan",
        "Wen Zhang",
        "Huajun Chen"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
      "pdf_url": "http://arxiv.org/pdf/2503.19470v1",
      "published": "2025-03-25T09:00:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19470v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning",
      "authors": [
        "Fred Philippy",
        "Siwen Guo",
        "Cedric Lothritz",
        "Jacques Klein",
        "Tegawendé F. Bissyandé"
      ],
      "abstract": "In NLP, Zero-Shot Classification (ZSC) has become essential for enabling\nmodels to classify text into categories unseen during training, particularly in\nlow-resource languages and domains where labeled data is scarce. While\npretrained language models (PLMs) have shown promise in ZSC, they often rely on\nlarge training datasets or external knowledge, limiting their applicability in\nmultilingual and low-resource scenarios. Recent approaches leveraging natural\nlanguage prompts reduce the dependence on large training datasets but struggle\nto effectively incorporate available labeled data from related classification\ntasks, especially when these datasets originate from different languages or\ndistributions. Moreover, existing prompt-based methods typically rely on\nmanually crafted prompts in a specific language, limiting their adaptability\nand effectiveness in cross-lingual settings. To address these challenges, we\nintroduce RoSPrompt, a lightweight and data-efficient approach for training\nsoft prompts that enhance cross-lingual ZSC while ensuring robust\ngeneralization across data distribution shifts. RoSPrompt is designed for small\nmultilingual PLMs, enabling them to leverage high-resource languages to improve\nperformance in low-resource settings without requiring extensive fine-tuning or\nhigh computational costs. We evaluate our approach on multiple multilingual\nPLMs across datasets covering 106 languages, demonstrating strong cross-lingual\ntransfer performance and robust generalization capabilities over unseen\nclasses.",
      "pdf_url": "http://arxiv.org/pdf/2503.19469v1",
      "published": "2025-03-25T09:00:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.19469v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}