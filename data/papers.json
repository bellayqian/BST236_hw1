{
  "last_updated": "2025-10-24T00:45:43.957888",
  "papers": [
    {
      "title": "Semantic World Models",
      "authors": [
        "Jacob Berg",
        "Chuning Zhu",
        "Yanda Bao",
        "Ishan Durugkar",
        "Abhishek Gupta"
      ],
      "abstract": "Planning with world models offers a powerful paradigm for robotic control.\nConventional approaches train a model to predict future frames conditioned on\ncurrent frames and actions, which can then be used for planning. However, the\nobjective of predicting future pixels is often at odds with the actual planning\nobjective; strong pixel reconstruction does not always correlate with good\nplanning decisions. This paper posits that instead of reconstructing future\nframes as pixels, world models only need to predict task-relevant semantic\ninformation about the future. For such prediction the paper poses world\nmodeling as a visual question answering problem about semantic information in\nfuture frames. This perspective allows world modeling to be approached with the\nsame tools underlying vision language models. Thus vision language models can\nbe trained as \"semantic\" world models through a supervised finetuning process\non image-action-text data, enabling planning for decision-making while\ninheriting many of the generalization and robustness properties from the\npretrained vision-language models. The paper demonstrates how such a semantic\nworld model can be used for policy improvement on open-ended robotics tasks,\nleading to significant generalization improvements over typical paradigms of\nreconstruction-based action-conditional world modeling. Website available at\nhttps://weirdlabuw.github.io/swm.",
      "pdf_url": "http://arxiv.org/pdf/2510.19818v1",
      "published": "2025-10-22T17:53:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19818v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning",
      "authors": [
        "Xichen Zhang",
        "Sitong Wu",
        "Yinghao Zhu",
        "Haoru Tan",
        "Shaozuo Yu",
        "Ziyi He",
        "Jiaya Jia"
      ],
      "abstract": "Reinforcement learning from verifiable rewards has emerged as a powerful\ntechnique for enhancing the complex reasoning abilities of Large Language\nModels (LLMs). However, these methods are fundamentally constrained by the\n''learning cliff'' phenomenon: when faced with problems far beyond their\ncurrent capabilities, models consistently fail, yielding a persistent\nzero-reward signal. In policy optimization algorithms like GRPO, this collapses\nthe advantage calculation to zero, rendering these difficult problems invisible\nto the learning gradient and stalling progress. To overcome this, we introduce\nScaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive\ntraining framework that strategically provides minimal guidance only when a\nmodel's independent learning has plateaued. The framework first diagnoses\nlearning stagnation and then intervenes by injecting tiered in-prompt hints,\nranging from abstract concepts to concrete steps, enabling the model to\nconstruct a valid solution by itself. Extensive experiments on challenging\nmathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the\npass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative\n44.3% over a vanilla GRPO baseline. This result demonstrates our framework\nprovides a robust and effective methodology for unlocking a model's ability to\nsolve problems previously beyond its reach, a critical step towards extending\nthe frontier of autonomous reasoning in LLM.",
      "pdf_url": "http://arxiv.org/pdf/2510.19807v1",
      "published": "2025-10-22T17:41:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19807v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation",
      "authors": [
        "Ji Ma",
        "Albert Casella"
      ],
      "abstract": "Public and nonprofit organizations often hesitate to adopt AI tools because\nmost models are opaque even though standard approaches typically analyze\naggregate patterns rather than offering actionable, case-level guidance. This\nstudy tests a practitioner-in-the-loop workflow that pairs transparent\ndecision-tree models with large language models (LLMs) to improve predictive\naccuracy, interpretability, and the generation of practical insights. Using\ndata from an ongoing college-success program, we build interpretable decision\ntrees to surface key predictors. We then provide each tree's structure to an\nLLM, enabling it to reproduce case-level predictions grounded in the\ntransparent models. Practitioners participate throughout feature engineering,\nmodel design, explanation review, and usability assessment, ensuring that field\nexpertise informs the analysis at every stage. Results show that integrating\ntransparent models, LLMs, and practitioner input yields accurate, trustworthy,\nand actionable case-level evaluations, offering a viable pathway for\nresponsible AI adoption in the public and nonprofit sectors.",
      "pdf_url": "http://arxiv.org/pdf/2510.19799v1",
      "published": "2025-10-22T17:35:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19799v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.SE",
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "On Controlled Change: Generative AI's Impact on Professional Authority in Journalism",
      "authors": [
        "Tomás Dodds",
        "Wang Ngai Yeung",
        "Claudia Mellado",
        "Mathias-Felipe de Lima-Santos"
      ],
      "abstract": "Using (generative) artificial intelligence tools and systems in journalism is\nexpected to increase journalists' production rates, transform newsrooms'\neconomic models, and further personalize the audience's news consumption\npractices. Since its release in 2022, OpenAI's ChatGPT and other large language\nmodels have raised the alarms inside news organizations, not only for bringing\nnew challenges to news reporting and fact-checking but also for what these\ntechnologies would mean for journalists' professional authority in journalism.\nThis paper examines how journalists in Dutch media manage the integration of AI\ntechnologies into their daily routines. Drawing from 13 interviews with\neditors, journalists, and innovation managers in different news outlets and\nmedia companies, we propose the concept of controlled change. as a heuristic to\nexplain how journalists are proactively setting guidelines, experimenting with\nAI tools, and identifying their limitations and capabilities. Using\nprofessional authority as a theoretical framework, we argue that journalists\nanticipate and integrate AI technologies in a supervised manner and identify\nthree primary mechanisms through which journalists manage this integration: (1)\ndeveloping adaptive guidelines that align AI use with ethical codes, (2)\nexperimenting with AI technologies to determine their necessity and fit, and\n(3) critically assessing the capabilities and limitations of AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2510.19792v1",
      "published": "2025-10-22T17:27:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19792v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "K.4"
      ]
    },
    {
      "title": "Benchmarking World-Model Learning",
      "authors": [
        "Archana Warrier",
        "Dat Nyugen",
        "Michelangelo Naim",
        "Moksh Jain",
        "Yichao Liang",
        "Karen Schroeder",
        "Cambridge Yang",
        "Joshua B. Tenenbaum",
        "Sebastian Vollmer",
        "Kevin Ellis",
        "Zenna Tavares"
      ],
      "abstract": "Model-learning agents should gather information to learn world models that\nsupport many downstream tasks and inferences, such as predicting unobserved\nstates, estimating near- and far-term consequences of actions, planning action\nsequences, and detecting changes in dynamics. Current methods for learning and\nevaluating world models diverge from this goal: training and evaluation are\nanchored to next-frame prediction, and success is scored by reward maximization\nin the same environment. We propose WorldTest, a protocol to evaluate\nmodel-learning agents that separates reward-free interaction from a scored test\nphase in a different but related environment. WorldTest is\nopen-ended$\\unicode{x2014}$models should support many different tasks unknown\nahead of time$\\unicode{x2014}$and agnostic to model representation, allowing\ncomparison across approaches. We instantiated WorldTest with AutumnBench, a\nsuite of 43 interactive grid-world environments and 129 tasks across three\nfamilies: masked-frame prediction, planning, and predicting changes to the\ncausal dynamics. We compared 517 human participants and three frontier models\non AutumnBench. We found that humans outperform the models, and scaling compute\nimproves performance only in some environments but not others. WorldTest\nprovides a novel template$\\unicode{x2014}$reward-free exploration, derived\ntests, and behavior-based scoring$\\unicode{x2014}$to evaluate what agents learn\nabout environment dynamics, and AutumnBench exposes significant headroom in\nworld-model learning.",
      "pdf_url": "http://arxiv.org/pdf/2510.19788v1",
      "published": "2025-10-22T17:23:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19788v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders",
      "authors": [
        "Yuezhou Hu",
        "Jiaxin Guo",
        "Xinyu Feng",
        "Tuo Zhao"
      ],
      "abstract": "Speculative Decoding (SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced by Knowledge Distillation\n(KD). However, conventional KD methods aim to minimize the KL divergence\nbetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporates selective token filtering into the KD process. AdaSPEC utilizes a\nreference model to identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overall token acceptance rate\nwithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, including arithmetic reasoning, instruction-following, coding, and\nsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec.",
      "pdf_url": "http://arxiv.org/pdf/2510.19779v1",
      "published": "2025-10-22T17:13:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19779v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents",
      "authors": [
        "Gil Pasternak",
        "Dheeraj Rajagopal",
        "Julia White",
        "Dhruv Atreja",
        "Matthew Thomas",
        "George Hurn-Maloney",
        "Ash Lewis"
      ],
      "abstract": "LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions.",
      "pdf_url": "http://arxiv.org/pdf/2510.19771v1",
      "published": "2025-10-22T17:00:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19771v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration",
      "authors": [
        "Xichen Zhang",
        "Sitong Wu",
        "Haoru Tan",
        "Shaozuo Yu",
        "Yinghao Zhu",
        "Ziyi He",
        "Jiaya Jia"
      ],
      "abstract": "The long chain-of-thought (LongCoT) capability is central to the recent\nbreakthroughs achieved by large language models in complex reasoning tasks.\nHowever, the accompanying issue of ''underthinking'', where models exhibit\nshallow reasoning by frequently switching thoughts without sufficient\nexploration, limits both performance and token efficiency. To address this\nproblem, we propose a simple yet effective reasoning strategy: the SmartSwitch\ninference framework. This framework can be easily integrated into any large\nlanguage model as a plug-and-play solution, continuously monitoring the model's\nreasoning process to detect underthinking and guide it toward deeper\nexploration of promising but overlooked thoughts. Specifically, the perception\nmodule identifies points where thoughts switch and evaluates the potential of\nthe preceding thought using an off-the-shelf process reward model (PRM). If a\nhigh-potential thought is found to be prematurely abandoned, the intervention\nmodule interrupts the ongoing inference, backtracks to the point before the\nswitch, and inserts a \"deepening prompt\" to encourage further exploration along\nthat promising path. Extensive experiments on challenging mathematical\nreasoning benchmarks demonstrate that our method significantly enhances the\nperformance of various large language models of different sizes.",
      "pdf_url": "http://arxiv.org/pdf/2510.19767v1",
      "published": "2025-10-22T16:56:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19767v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation",
      "authors": [
        "Jiacheng Liu",
        "Xinyu Wang",
        "Yuqi Lin",
        "Zhikai Wang",
        "Peiru Wang",
        "Peiliang Cai",
        "Qinming Zhou",
        "Zhengan Yan",
        "Zexuan Yan",
        "Zhengyi Shi",
        "Chang Zou",
        "Yue Ma",
        "Linfeng Zhang"
      ],
      "abstract": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
      "pdf_url": "http://arxiv.org/pdf/2510.19755v2",
      "published": "2025-10-22T16:46:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19755v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models",
      "authors": [
        "Ameesh Shah",
        "William Chen",
        "Adwait Godbole",
        "Federico Mora",
        "Sanjit A. Seshia",
        "Sergey Levine"
      ],
      "abstract": "Solving complex real-world control tasks often takes multiple tries: if we\nfail at first, we reflect on what went wrong, and change our strategy\naccordingly to avoid making the same mistake. In robotics,\nVision-Language-Action models (VLAs) offer a promising path towards solving\ncomplex control tasks, but lack the ability to contextually and dynamically\nreadjust behavior when they fail to accomplish a task. In this work, we\nintroduce Learning from Inference-Time Execution (LITEN), which connects a VLA\nlow-level policy to a high-level VLM that conditions on past experiences by\nincluding them in-context, allowing it to learn the affordances and\ncapabilities of the low-level VLA. Our approach iterates between a reasoning\nphase that generates and executes plans for the low-level VLA, and an\nassessment phase that reflects on the resulting execution and draws useful\nconclusions to be included in future reasoning contexts. Unlike similar\napproaches to self-refinement in non-robotics domains, LITEN must reflect on\nunstructured real-world robot trajectories (e.g., raw videos), which requires\nstructured guiderails during assessment. Our experimental results demonstrate\nLITEN is able to effectively learn from past experience to generate plans that\nuse high-affordance instructions to accomplish long-horizon tasks.",
      "pdf_url": "http://arxiv.org/pdf/2510.19752v1",
      "published": "2025-10-22T16:43:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19752v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "68T40",
        "I.2.9; I.2.8"
      ]
    },
    {
      "title": "Misalignment Bounty: Crowdsourcing AI Agent Misbehavior",
      "authors": [
        "Rustem Turtayev",
        "Natalia Fedorova",
        "Oleg Serikov",
        "Sergey Koldyba",
        "Lev Avagyan",
        "Dmitrii Volkov"
      ],
      "abstract": "Advanced AI systems sometimes act in ways that differ from human intent. To\ngather clear, reproducible examples, we ran the Misalignment Bounty: a\ncrowdsourced project that collected cases of agents pursuing unintended or\nunsafe goals. The bounty received 295 submissions, of which nine were awarded.\n  This report explains the program's motivation and evaluation criteria, and\nwalks through the nine winning submissions step by step.",
      "pdf_url": "http://arxiv.org/pdf/2510.19738v1",
      "published": "2025-10-22T16:28:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19738v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning",
      "authors": [
        "Gunshi Gupta",
        "Karmesh Yadav",
        "Zsolt Kira",
        "Yarin Gal",
        "Rahaf Aljundi"
      ],
      "abstract": "To enable embodied agents to operate effectively over extended timeframes, it\nis crucial to develop models that form and access memories to stay\ncontextualized in their environment. In the current paradigm of training\ntransformer-based policies for embodied sequential decision-making tasks,\nvisual inputs often overwhelm the context limits of transformers, while humans\ncan maintain and utilize a lifetime of experience compressed as memories.\nSignificant compression is possible in principle, as much of the input is\nirrelevant and can be abstracted. However, existing approaches predominantly\nfocus on either recurrent models with fixed-size memory or transformers with\nfull-context reliance. In this work, we propose Memo, a transformer-based\narchitecture and training recipe for reinforcement learning (RL) on\nmemory-intensive, long-horizon tasks. Memo incorporates the creation and\nretrieval of memory by interleaving periodic summarization tokens with the\ninputs of a model during training. We demonstrate Memo's effectiveness on a\ngridworld meta-RL benchmark and a multi-object navigation task in\nphoto-realistic indoor settings. Memo outperforms naive long-context\ntransformer baselines while being more compute and storage efficient.\nAdditionally, Memo generalizes better to longer contexts at inference time and\nremains robust in streaming settings, where historical context must be\ntruncated to fit inference constraints.",
      "pdf_url": "http://arxiv.org/pdf/2510.19732v1",
      "published": "2025-10-22T16:24:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19732v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series",
      "authors": [
        "Mahmoud Ibrahim",
        "Bart Elen",
        "Chang Sun",
        "Gökhan Ertaylan",
        "Michel Dumontier"
      ],
      "abstract": "We present a novel framework for leveraging synthetic ICU time-series data\nnot only to train but also to rigorously and trustworthily evaluate predictive\nmodels, both at the population level and within fine-grained demographic\nsubgroups. Building on prior diffusion and VAE-based generators (TimeDiff,\nHealthGen, TimeAutoDiff), we introduce \\textit{Enhanced TimeAutoDiff}, which\naugments the latent diffusion objective with distribution-alignment penalties.\nWe extensively benchmark all models on MIMIC-III and eICU, on 24-hour mortality\nand binary length-of-stay tasks. Our results show that Enhanced TimeAutoDiff\nreduces the gap between real-on-synthetic and real-on-real evaluation (``TRTS\ngap'') by over 70\\%, achieving $\\Delta_{TRTS} \\leq 0.014$ AUROC, while\npreserving training utility ($\\Delta_{TSTR} \\approx 0.01$). Crucially, for 32\nintersectional subgroups, large synthetic cohorts cut subgroup-level AUROC\nestimation error by up to 50\\% relative to small real test sets, and outperform\nthem in 72--84\\% of subgroups. This work provides a practical,\nprivacy-preserving roadmap for trustworthy, granular model evaluation in\ncritical care, enabling robust and reliable performance analysis across diverse\npatient populations without exposing sensitive EHR data, contributing to the\noverall trustworthiness of Medical AI.",
      "pdf_url": "http://arxiv.org/pdf/2510.19728v1",
      "published": "2025-10-22T16:17:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19728v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models",
      "authors": [
        "Yang Yang",
        "Hua XU",
        "Zhangyi Hu",
        "Yutao Yue"
      ],
      "abstract": "Large Language Models (LLMs) can propose rules in natural language,\nsidestepping the need for a predefined predicate space in traditional rule\nlearning. Yet many LLM-based approaches ignore interactions among rules, and\nthe opportunity to couple LLMs with probabilistic rule learning for robust\ninference remains underexplored. We present RLIE, a unified framework that\nintegrates LLMs with probabilistic modeling to learn a set of weighted rules.\nRLIE has four stages: (1) Rule generation, where an LLM proposes and filters\ncandidates; (2) Logistic regression, which learns probabilistic weights for\nglobal selection and calibration; (3) Iterative refinement, which updates the\nrule set using prediction errors; and (4) Evaluation, which compares the\nweighted rule set as a direct classifier with methods that inject rules into an\nLLM. We evaluate multiple inference strategies on real-world datasets. Applying\nrules directly with their learned weights yields superior performance, whereas\nprompting LLMs with the rules, weights, and logistic-model outputs surprisingly\ndegrades accuracy. This supports the view that LLMs excel at semantic\ngeneration and interpretation but are less reliable for precise probabilistic\nintegration. RLIE clarifies the potential and limitations of LLMs for inductive\nreasoning and couples them with classic probabilistic rule combination methods\nto enable more reliable neuro-symbolic reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2510.19698v1",
      "published": "2025-10-22T15:50:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19698v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings",
      "authors": [
        "Cesar Gonzalez-Gutierrez",
        "Dirk Hovy"
      ],
      "abstract": "Prompting is a common approach for leveraging LMs in zero-shot settings.\nHowever, the underlying mechanisms that enable LMs to perform diverse tasks\nwithout task-specific supervision remain poorly understood. Studying the\nrelationship between prompting and the quality of internal representations can\nshed light on how pre-trained embeddings may support in-context task solving.\nIn this empirical study, we conduct a series of probing experiments on prompt\nembeddings, analyzing various combinations of prompt templates for zero-shot\nclassification. Our findings show that while prompting affects the quality of\nrepresentations, these changes do not consistently correlate with the relevance\nof the prompts to the target task. This result challenges the assumption that\nmore relevant prompts necessarily lead to better representations. We further\nanalyze potential factors that may contribute to this unexpected behavior.",
      "pdf_url": "http://arxiv.org/pdf/2510.19694v1",
      "published": "2025-10-22T15:43:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19694v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary",
      "authors": [
        "Rashina Hoda"
      ],
      "abstract": "Agentic AI is poised to usher in a seismic paradigm shift in Software\nEngineering (SE). As technologists rush head-along to make agentic AI a\nreality, SE researchers are driven to establish agentic SE as a research area.\nWhile early visions of agentic SE are primarily focused on code-related\nactivities, early empirical evidence calls for a consideration of a range of\nsocio-technical concerns to make it work in practice. This paper contributes to\nthe emerging community vision by: (a) recommending an expansion of its scope\nbeyond code, toward a 'whole of process' vision, grounding it in SE foundations\nand evolution and emerging agentic SE frameworks, (b) proposing a preliminary\nset of values and principles to guide efforts, and (c) sharing guidance on\ndesigning/using well-defined vocabulary for agentic SE. It is hoped that these\nideas will encourage community collaborations and steer the SE community\ntowards laying strong foundations of agentic SE so its not only inevitable but\nalso deliberate and desirable in the long run.",
      "pdf_url": "http://arxiv.org/pdf/2510.19692v1",
      "published": "2025-10-22T15:39:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19692v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation",
      "authors": [
        "Guilin Zhang",
        "Wulan Guo",
        "Ziqi Tan",
        "Srinivas Vippagunta",
        "Suchitra Raman",
        "Shreeshankar Chatterjee",
        "Ju Lin",
        "Shang Liu",
        "Mary Schladenhauffen",
        "Jeffrey Luo",
        "Hailong Jiang"
      ],
      "abstract": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings.",
      "pdf_url": "http://arxiv.org/pdf/2510.19689v1",
      "published": "2025-10-22T15:37:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19689v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "C.2.4; H.3.4; I.2.6"
      ]
    },
    {
      "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
      "authors": [
        "Addison J. Wu",
        "Ryan Liu",
        "Kerem Oktar",
        "Theodore R. Sumers",
        "Thomas L. Griffiths"
      ],
      "abstract": "Human communication is motivated: people speak, write, and create content\nwith a particular communicative intent in mind. As a result, information that\nlarge language models (LLMs) and AI agents process is inherently framed by\nhumans' intentions and incentives. People are adept at navigating such nuanced\ninformation: we routinely identify benevolent or self-serving motives in order\nto decide what statements to trust. For LLMs to be effective in the real world,\nthey too must critically evaluate content by factoring in the motivations of\nthe source -- for instance, weighing the credibility of claims made in a sales\npitch. In this paper, we undertake a comprehensive study of whether LLMs have\nthis capacity for motivational vigilance. We first employ controlled\nexperiments from cognitive science to verify that LLMs' behavior is consistent\nwith rational models of learning from motivated testimony, and find they\nsuccessfully discount information from biased sources in a human-like manner.\nWe then extend our evaluation to sponsored online adverts, a more naturalistic\nreflection of LLM agents' information ecosystems. In these settings, we find\nthat LLMs' inferences do not track the rational models' predictions nearly as\nclosely -- partly due to additional information that distracts them from\nvigilance-relevant considerations. However, a simple steering intervention that\nboosts the salience of intentions and incentives substantially increases the\ncorrespondence between LLMs and the rational model. These results suggest that\nLLMs possess a basic sensitivity to the motivations of others, but generalizing\nto novel real-world settings will require further improvements to these models.",
      "pdf_url": "http://arxiv.org/pdf/2510.19687v1",
      "published": "2025-10-22T15:35:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19687v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback Types on Student Engagement, Confidence, and Outcomes",
      "authors": [
        "Omar Alsaiari",
        "Nilufar Baghaei",
        "Jason M. Lodge",
        "Omid Noroozi",
        "Dragan Gašević",
        "Marie Boden",
        "Hassan Khosravi"
      ],
      "abstract": "Feedback is one of the most powerful influences on student learning, with\nextensive research examining how best to implement it in educational settings.\nIncreasingly, feedback is being generated by artificial intelligence (AI),\noffering scalable and adaptive responses. Two widely studied approaches are\ndirective feedback, which gives explicit explanations and reduces cognitive\nload to speed up learning, and metacognitive feedback which prompts learners to\nreflect, track their progress, and develop self-regulated learning (SRL)\nskills. While both approaches have clear theoretical advantages, their\ncomparative effects on engagement, confidence, and quality of work remain\nunderexplored. This study presents a semester-long randomised controlled trial\nwith 329 students in an introductory design and programming course using an\nadaptive educational platform. Participants were assigned to receive directive,\nmetacognitive, or hybrid AI-generated feedback that blended elements of both\ndirective and metacognitive feedback. Results showed that revision behaviour\ndiffered across feedback conditions, with Hybrid prompting the most revisions\ncompared to Directive and Metacognitive. Confidence ratings were uniformly\nhigh, and resource quality outcomes were comparable across conditions. These\nfindings highlight the promise of AI in delivering feedback that balances\nclarity with reflection. Hybrid approaches, in particular, show potential to\ncombine actionable guidance for immediate improvement with opportunities for\nself-reflection and metacognitive growth.",
      "pdf_url": "http://arxiv.org/pdf/2510.19685v1",
      "published": "2025-10-22T15:31:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19685v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs",
      "authors": [
        "John Burden",
        "Jonathan Prunty",
        "Ben Slater",
        "Matthieu Tehenan",
        "Greg Davis",
        "Lucy Cheke"
      ],
      "abstract": "Multimodal large language models (MLLMs) achieve strong performance on\nvision-language tasks, yet their visual processing is opaque. Most black-box\nevaluations measure task accuracy, but reveal little about underlying\nmechanisms. Drawing on cognitive psychology, we adapt classic visual search\nparadigms -- originally developed to study human perception -- to test whether\nMLLMs exhibit the ``pop-out'' effect, where salient visual features are\ndetected independently of distractor set size. Using controlled experiments\ntargeting colour, size and lighting features, we find that advanced MLLMs\nexhibit human-like pop-out effects in colour or size-based disjunctive (single\nfeature) search, as well as capacity limits for conjunctive (multiple feature)\nsearch. We also find evidence to suggest that MLLMs, like humans, incorporate\nnatural scene priors such as lighting direction into object representations. We\nreinforce our findings using targeted fine-tuning and mechanistic\ninterpretability analyses. Our work shows how visual search can serve as a\ncognitively grounded diagnostic tool for evaluating perceptual capabilities in\nMLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.19678v1",
      "published": "2025-10-22T15:24:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19678v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Study of Training Dynamics for Memory-Constrained Fine-Tuning",
      "authors": [
        "Aël Quélennec",
        "Nour Hezbri",
        "Pavlo Mozharovskyi",
        "Van-Tam Nguyen",
        "Enzo Tartaglione"
      ],
      "abstract": "Memory-efficient training of deep neural networks has become increasingly\nimportant as models grow larger while deployment environments impose strict\nresource constraints. We propose TraDy, a novel transfer learning scheme\nleveraging two key insights: layer importance for updates is\narchitecture-dependent and determinable a priori, while dynamic stochastic\nchannel selection provides superior gradient approximation compared to static\napproaches. We introduce a dynamic channel selection approach that\nstochastically resamples channels between epochs within preselected layers.\nExtensive experiments demonstrate TraDy achieves state-of-the-art performance\nacross various downstream tasks and architectures while maintaining strict\nmemory constraints, achieving up to 99% activation sparsity, 95% weight\nderivative sparsity, and 97% reduction in FLOPs for weight derivative\ncomputation.",
      "pdf_url": "http://arxiv.org/pdf/2510.19675v1",
      "published": "2025-10-22T15:21:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19675v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Explainable e-sports win prediction through Machine Learning classification in streaming",
      "authors": [
        "Silvia García-Méndez",
        "Francisco de Arriba-Pérez"
      ],
      "abstract": "The increasing number of spectators and players in e-sports, along with the\ndevelopment of optimized communication solutions and cloud computing\ntechnology, has motivated the constant growth of the online game industry. Even\nthough Artificial Intelligence-based solutions for e-sports analytics are\ntraditionally defined as extracting meaningful patterns from related data and\nvisualizing them to enhance decision-making, most of the effort in professional\nwinning prediction has been focused on the classification aspect from a batch\nperspective, also leaving aside the visualization techniques. Consequently,\nthis work contributes to an explainable win prediction classification solution\nin streaming in which input data is controlled over several sliding windows to\nreflect relevant game changes. Experimental results attained an accuracy higher\nthan 90 %, surpassing the performance of competing solutions in the literature.\nUltimately, our system can be leveraged by ranking and recommender systems for\ninformed decision-making, thanks to the explainability module, which fosters\ntrust in the outcome predictions.",
      "pdf_url": "http://arxiv.org/pdf/2510.19671v1",
      "published": "2025-10-22T15:18:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19671v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Unraveling Emotions with Pre-Trained Models",
      "authors": [
        "Alejandro Pajón-Sanmartín",
        "Francisco De Arriba-Pérez",
        "Silvia García-Méndez",
        "Fátima Leal",
        "Benedita Malheiro",
        "Juan Carlos Burguillo-Rial"
      ],
      "abstract": "Transformer models have significantly advanced the field of emotion\nrecognition. However, there are still open challenges when exploring open-ended\nqueries for Large Language Models (LLMs). Although current models offer good\nresults, automatic emotion analysis in open texts presents significant\nchallenges, such as contextual ambiguity, linguistic variability, and\ndifficulty interpreting complex emotional expressions. These limitations make\nthe direct application of generalist models difficult. Accordingly, this work\ncompares the effectiveness of fine-tuning and prompt engineering in emotion\ndetection in three distinct scenarios: (i) performance of fine-tuned\npre-trained models and general-purpose LLMs using simple prompts; (ii)\neffectiveness of different emotion prompt designs with LLMs; and (iii) impact\nof emotion grouping techniques on these models. Experimental tests attain\nmetrics above 70% with a fine-tuned pre-trained model for emotion recognition.\nMoreover, the findings highlight that LLMs require structured prompt\nengineering and emotion grouping to enhance their performance. These\nadvancements improve sentiment analysis, human-computer interaction, and\nunderstanding of user behavior across various domains.",
      "pdf_url": "http://arxiv.org/pdf/2510.19668v1",
      "published": "2025-10-22T15:13:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19668v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Graph Engine for Guitar Chord-Tone Soloing Education",
      "authors": [
        "Matthew Keating",
        "Michael Casey"
      ],
      "abstract": "We present a graph-based engine for computing chord tone soloing suggestions\nfor guitar students. Chord tone soloing is a fundamental practice for\nimprovising over a chord progression, where the instrumentalist uses only the\nnotes contained in the current chord. This practice is a building block for all\nadvanced jazz guitar theory but is difficult to learn and practice. First, we\ndiscuss methods for generating chord-tone arpeggios. Next, we construct a\nweighted graph where each node represents a chord tone arpeggio for a chord in\nthe progression. Then, we calculate the edge weight between each consecutive\nchord's nodes in terms of optimal transition tones. We then find the shortest\npath through this graph and reconstruct a chord-tone soloing line. Finally, we\ndiscuss a user-friendly system to handle input and output to this engine for\nguitar students to practice chord tone soloing.",
      "pdf_url": "http://arxiv.org/pdf/2510.19666v1",
      "published": "2025-10-22T15:13:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19666v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing",
      "authors": [
        "Xusen Guo",
        "Mingxing Peng",
        "Xixuan Hao",
        "Xingchen Zou",
        "Qiongyan Wang",
        "Sijie Ruan",
        "Yuxuan Liang"
      ],
      "abstract": "Web-based participatory urban sensing has emerged as a vital approach for\nmodern urban management by leveraging mobile individuals as distributed\nsensors. However, existing urban sensing systems struggle with limited\ngeneralization across diverse urban scenarios and poor interpretability in\ndecision-making. In this work, we introduce AgentSense, a hybrid, training-free\nframework that integrates large language models (LLMs) into participatory urban\nsensing through a multi-agent evolution system. AgentSense initially employs\nclassical planner to generate baseline solutions and then iteratively refines\nthem to adapt sensing task assignments to dynamic urban conditions and\nheterogeneous worker preferences, while producing natural language explanations\nthat enhance transparency and trust. Extensive experiments across two\nlarge-scale mobility datasets and seven types of dynamic disturbances\ndemonstrate that AgentSense offers distinct advantages in adaptivity and\nexplainability over traditional methods. Furthermore, compared to single-agent\nLLM baselines, our approach outperforms in both performance and robustness,\nwhile delivering more reasonable and transparent explanations. These results\nposition AgentSense as a significant advancement towards deploying adaptive and\nexplainable urban sensing systems on the web.",
      "pdf_url": "http://arxiv.org/pdf/2510.19661v1",
      "published": "2025-10-22T15:06:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19661v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction",
      "authors": [
        "Zhida Zhao",
        "Talas Fu",
        "Yifan Wang",
        "Lijun Wang",
        "Huchuan Lu"
      ],
      "abstract": "Despite remarkable progress in driving world models, their potential for\nautonomous systems remains largely untapped: the world models are mostly\nlearned for world simulation and decoupled from trajectory planning. While\nrecent efforts aim to unify world modeling and planning in a single framework,\nthe synergistic facilitation mechanism of world modeling for planning still\nrequires further exploration. In this work, we introduce a new driving paradigm\nnamed Policy World Model (PWM), which not only integrates world modeling and\ntrajectory planning within a unified architecture, but is also able to benefit\nplanning using the learned world knowledge through the proposed action-free\nfuture state forecasting scheme. Through collaborative state-action prediction,\nPWM can mimic the human-like anticipatory perception, yielding more reliable\nplanning performance. To facilitate the efficiency of video forecasting, we\nfurther introduce a dynamically enhanced parallel token generation mechanism,\nequipped with a context-guided tokenizer and an adaptive dynamic focal loss.\nDespite utilizing only front camera input, our method matches or exceeds\nstate-of-the-art approaches that rely on multi-view and multi-modal inputs.\nCode and model weights will be released at\nhttps://github.com/6550Zhao/Policy-World-Model.",
      "pdf_url": "http://arxiv.org/pdf/2510.19654v1",
      "published": "2025-10-22T14:57:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19654v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ]
    },
    {
      "title": "Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent",
      "authors": [
        "Yangshijie Zhang",
        "Xinda Wang",
        "Jialin Liu",
        "Wenqiang Wang",
        "Zhicong Ma",
        "Xingxing Jia"
      ],
      "abstract": "With social media growth, users employ stylistic fonts and font-like emoji to\nexpress individuality, creating visually appealing text that remains\nhuman-readable. However, these fonts introduce hidden vulnerabilities in NLP\nmodels: while humans easily read stylistic text, models process these\ncharacters as distinct tokens, causing interference. We identify this\nhuman-model perception gap and propose a style-based attack, Style Attack\nDisguise (SAD). We design two sizes: light for query efficiency and strong for\nsuperior attack performance. Experiments on sentiment classification and\nmachine translation across traditional models, LLMs, and commercial services\ndemonstrate SAD's strong attack performance. We also show SAD's potential\nthreats to multimodal tasks including text-to-image and text-to-speech\ngeneration.",
      "pdf_url": "http://arxiv.org/pdf/2510.19641v1",
      "published": "2025-10-22T14:40:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19641v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application",
      "authors": [
        "Yiqian Yang",
        "Tian Lan",
        "Qianghuai Jia",
        "Li Zhu",
        "Hui Jiang",
        "Hang Zhu",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "abstract": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further.",
      "pdf_url": "http://arxiv.org/pdf/2510.19631v1",
      "published": "2025-10-22T14:28:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19631v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ]
    },
    {
      "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
      "authors": [
        "Qianli Ma",
        "Siyu Wang",
        "Yilin Chen",
        "Yinhao Tang",
        "Yixiang Yang",
        "Chang Guo",
        "Bingjie Gao",
        "Zhening Xing",
        "Yanan Sun",
        "Zhipeng Zhang"
      ],
      "abstract": "In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\n$\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct $\\textbf{PageBench}$, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\$0.1. Code and dataset will be released at\n$\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.",
      "pdf_url": "http://arxiv.org/pdf/2510.19600v1",
      "published": "2025-10-22T13:53:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19600v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography",
      "authors": [
        "Haozhe Luo",
        "Shelley Zixin Shu",
        "Ziyu Zhou",
        "Sebastian Otalora",
        "Mauricio Reyes"
      ],
      "abstract": "Vision-language models (VLMs) have recently shown remarkable zero-shot\nperformance in medical image understanding, yet their grounding ability, the\nextent to which textual concepts align with visual evidence, remains\nunderexplored. In the medical domain, however, reliable grounding is essential\nfor interpretability and clinical adoption. In this work, we present the first\nsystematic benchmark for evaluating cross-modal interpretability in chest\nX-rays across seven CLIP-style VLM variants. We generate visual explanations\nusing cross-attention and similarity-based localization maps, and\nquantitatively assess their alignment with radiologist-annotated regions across\nmultiple pathologies. Our analysis reveals that: (1) while all VLM variants\ndemonstrate reasonable localization for large and well-defined pathologies,\ntheir performance substantially degrades for small or diffuse lesions; (2)\nmodels that are pretrained on chest X-ray-specific datasets exhibit improved\nalignment compared to those trained on general-domain data. (3) The overall\nrecognition ability and grounding ability of the model are strongly correlated.\nThese findings underscore that current VLMs, despite their strong recognition\nability, still fall short in clinically reliable grounding, highlighting the\nneed for targeted interpretability benchmarks before deployment in medical\npractice. XBench code is available at\nhttps://github.com/Roypic/Benchmarkingattention",
      "pdf_url": "http://arxiv.org/pdf/2510.19599v1",
      "published": "2025-10-22T13:52:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19599v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Goal-Driven Survey on Root Cause Analysis",
      "authors": [
        "Aoyang Fang",
        "Haowen Yang",
        "Haoze Dong",
        "Qisheng Lu",
        "Junjielong Xu",
        "Pinjia He"
      ],
      "abstract": "Root Cause Analysis (RCA) is a crucial aspect of incident management in\nlarge-scale cloud services. While the term root cause analysis or RCA has been\nwidely used, different studies formulate the task differently. This is because\nthe term \"RCA\" implicitly covers tasks with distinct underlying goals. For\ninstance, the goal of localizing a faulty service for rapid triage is\nfundamentally different from identifying a specific functional bug for a\ndefinitive fix. However, previous surveys have largely overlooked these\ngoal-based distinctions, conventionally categorizing papers by input data types\n(e.g., metric-based vs. trace-based methods). This leads to the grouping of\nworks with disparate objectives, thereby obscuring the true progress and gaps\nin the field. Meanwhile, the typical audience of an RCA survey is either laymen\nwho want to know the goals and big picture of the task or RCA researchers who\nwant to figure out past research under the same task formulation. Thus, an RCA\nsurvey that organizes the related papers according to their goals is in high\ndemand. To this end, this paper presents a goal-driven framework that\neffectively categorizes and integrates 135 papers on RCA in the context of\ncloud incident management based on their diverse goals, spanning the period\nfrom 2014 to 2025. In addition to the goal-driven categorization, it discusses\nthe ultimate goal of all RCA papers as an umbrella covering different RCA\nformulations. Moreover, the paper discusses open challenges and future\ndirections in RCA.",
      "pdf_url": "http://arxiv.org/pdf/2510.19593v1",
      "published": "2025-10-22T13:43:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19593v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark",
      "authors": [
        "Yu Wu",
        "Ke Shu",
        "Jonas Fischer",
        "Lidia Pivovarova",
        "David Rosson",
        "Eetu Mäkelä",
        "Mikko Tolonen"
      ],
      "abstract": "This paper presents a novel task of extracting Latin fragments from\nmixed-language historical documents with varied layouts. We benchmark and\nevaluate the performance of large foundation models against a multimodal\ndataset of 724 annotated pages. The results demonstrate that reliable Latin\ndetection with contemporary models is achievable. Our study provides the first\ncomprehensive analysis of these models' capabilities and limits for this task.",
      "pdf_url": "http://arxiv.org/pdf/2510.19585v1",
      "published": "2025-10-22T13:37:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19585v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.DL"
      ]
    },
    {
      "title": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration",
      "authors": [
        "Francisco Mena",
        "Dino Ienco",
        "Cassio F. Dantas",
        "Roberto Interdonato",
        "Andreas Dengel"
      ],
      "abstract": "Multi-modal co-learning is emerging as an effective paradigm in machine\nlearning, enabling models to collaboratively learn from different modalities to\nenhance single-modality predictions. Earth Observation (EO) represents a\nquintessential domain for multi-modal data analysis, wherein diverse remote\nsensors collect data to sense our planet. This unprecedented volume of data\nintroduces novel challenges. Specifically, the access to the same sensor\nmodalities at both training and inference stages becomes increasingly complex\nbased on real-world constraints affecting remote sensing platforms. In this\ncontext, multi-modal co-learning presents a promising strategy to leverage the\nvast amount of sensor-derived data available at the training stage to improve\nsingle-modality models for inference-time deployment. Most current research\nefforts focus on designing customized solutions for either particular\ndownstream tasks or specific modalities available at the inference stage. To\naddress this, we propose a novel multi-modal co-learning framework capable of\ngeneralizing across various tasks without targeting a specific modality for\ninference. Our approach combines contrastive and modality discriminative\nlearning together to guide single-modality models to structure the internal\nmodel manifold into modality-shared and modality-specific information. We\nevaluate our framework on four EO benchmarks spanning classification and\nregression tasks across different sensor modalities, where only one of the\nmodalities available during training is accessible at inference time. Our\nresults demonstrate consistent predictive improvements over state-of-the-art\napproaches from the recent machine learning and computer vision literature, as\nwell as EO-specific methods. The obtained findings validate our framework in\nthe single-modality inference scenarios across a diverse range of EO\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2510.19579v1",
      "published": "2025-10-22T13:29:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19579v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning",
      "authors": [
        "Runpeng Xie",
        "Quanwei Wang",
        "Hao Hu",
        "Zherui Zhou",
        "Ni Mu",
        "Xiyun Li",
        "Yiqin Yang",
        "Shuang Xu",
        "Qianchuan Zhao",
        "Bo XU"
      ],
      "abstract": "Comprehending natural language and following human instructions are critical\ncapabilities for intelligent agents. However, the flexibility of linguistic\ninstructions induces substantial ambiguity across language-conditioned tasks,\nseverely degrading algorithmic performance. To address these limitations, we\npresent a novel method named DAIL (Distributional Aligned Learning), featuring\ntwo key components: distributional policy and semantic alignment. Specifically,\nwe provide theoretical results that the value distribution estimation mechanism\nenhances task differentiability. Meanwhile, the semantic alignment module\ncaptures the correspondence between trajectories and linguistic instructions.\nExtensive experimental results on both structured and visual observation\nbenchmarks demonstrate that DAIL effectively resolves instruction ambiguities,\nachieving superior performance to baseline methods. Our implementation is\navailable at https://github.com/RunpengXie/Distributional-Aligned-Learning.",
      "pdf_url": "http://arxiv.org/pdf/2510.19562v2",
      "published": "2025-10-22T13:16:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19562v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A Matter of Time: Revealing the Structure of Time in Vision-Language Models",
      "authors": [
        "Nidham Tekaya",
        "Manuela Waldner",
        "Matthias Zeppelzauer"
      ],
      "abstract": "Large-scale vision-language models (VLMs) such as CLIP have gained popularity\nfor their generalizable and expressive multimodal representations. By\nleveraging large-scale training data with diverse textual metadata, VLMs\nacquire open-vocabulary capabilities, solving tasks beyond their training\nscope. This paper investigates the temporal awareness of VLMs, assessing their\nability to position visual content in time. We introduce TIME10k, a benchmark\ndataset of over 10,000 images with temporal ground truth, and evaluate the\ntime-awareness of 37 VLMs by a novel methodology. Our investigation reveals\nthat temporal information is structured along a low-dimensional, non-linear\nmanifold in the VLM embedding space. Based on this insight, we propose methods\nto derive an explicit ``timeline'' representation from the embedding space.\nThese representations model time and its chronological progression and thereby\nfacilitate temporal reasoning tasks. Our timeline approaches achieve\ncompetitive to superior accuracy compared to a prompt-based baseline while\nbeing computationally efficient. All code and data are available at\nhttps://tekayanidham.github.io/timeline-page/.",
      "pdf_url": "http://arxiv.org/pdf/2510.19559v1",
      "published": "2025-10-22T13:14:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19559v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.MM"
      ]
    },
    {
      "title": "Demonstrating Real Advantage of Machine-Learning-Enhanced Monte Carlo for Combinatorial Optimization",
      "authors": [
        "Luca Maria Del Bono",
        "Federico Ricci-Tersenghi",
        "Francesco Zamponi"
      ],
      "abstract": "Combinatorial optimization problems are central to both practical\napplications and the development of optimization methods. While classical and\nquantum algorithms have been refined over decades, machine learning-assisted\napproaches are comparatively recent and have not yet consistently outperformed\nsimple, state-of-the-art classical methods. Here, we focus on a class of\nQuadratic Unconstrained Binary Optimization (QUBO) problems, specifically the\nchallenge of finding minimum energy configurations in three-dimensional Ising\nspin glasses. We use a Global Annealing Monte Carlo algorithm that integrates\nstandard local moves with global moves proposed via machine learning. We show\nthat local moves play a crucial role in achieving optimal performance.\nBenchmarking against Simulated Annealing and Population Annealing, we\ndemonstrate that Global Annealing not only surpasses the performance of\nSimulated Annealing but also exhibits greater robustness than Population\nAnnealing, maintaining effectiveness across problem hardness and system size\nwithout hyperparameter tuning. These results provide, to our knowledge, the\nfirst clear and robust evidence that a machine learning-assisted optimization\nmethod can exceed the capabilities of classical state-of-the-art techniques in\na combinatorial optimization setting.",
      "pdf_url": "http://arxiv.org/pdf/2510.19544v1",
      "published": "2025-10-22T12:50:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19544v1",
      "categories": [
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ]
    },
    {
      "title": "Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data",
      "authors": [
        "Markus Bujotzek",
        "Evelyn Trautmann",
        "Calum Hand",
        "Ian Hales"
      ],
      "abstract": "AI methods are increasingly shaping pharmaceutical drug discovery. However,\ntheir translation to industrial applications remains limited due to their\nreliance on public datasets, lacking scale and diversity of proprietary\npharmaceutical data. Federated learning (FL) offers a promising approach to\nintegrate private data into privacy-preserving, collaborative model training\nacross data silos. This federated data access complicates important\ndata-centric tasks such as estimating dataset diversity, performing informed\ndata splits, and understanding the structure of the combined chemical space. To\naddress this gap, we investigate how well federated clustering methods can\ndisentangle and represent distributed molecular data. We benchmark three\napproaches, Federated kMeans (Fed-kMeans), Federated Principal Component\nAnalysis combined with Fed-kMeans (Fed-PCA+Fed-kMeans), and Federated\nLocality-Sensitive Hashing (Fed-LSH), against their centralized counterparts on\neight diverse molecular datasets. Our evaluation utilizes both, standard\nmathematical and a chemistry-informed evaluation metrics, SF-ICF, that we\nintroduce in this work. The large-scale benchmarking combined with an in-depth\nexplainability analysis shows the importance of incorporating domain knowledge\nthrough chemistry-informed metrics, and on-client explainability analyses for\nfederated diversity analysis on molecular data.",
      "pdf_url": "http://arxiv.org/pdf/2510.19535v1",
      "published": "2025-10-22T12:41:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19535v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning",
      "authors": [
        "Ruiyao Miao",
        "Junren Xiao",
        "Shiya Tsang",
        "Hui Xiong",
        "Yingnian Wu"
      ],
      "abstract": "Existing Bayesian Optimization (BO) methods typically balance exploration and\nexploitation to optimize costly objective functions. However, these methods\noften suffer from a significant one-step bias, which may lead to convergence\ntowards local optima and poor performance in complex or high-dimensional tasks.\nRecently, Black-Box Optimization (BBO) has achieved success across various\nscientific and engineering domains, particularly when function evaluations are\ncostly and gradients are unavailable. Motivated by this, we propose the\nReinforced Energy-Based Model for Bayesian Optimization (REBMBO), which\nintegrates Gaussian Processes (GP) for local guidance with an Energy-Based\nModel (EBM) to capture global structural information. Notably, we define each\nBayesian Optimization iteration as a Markov Decision Process (MDP) and use\nProximal Policy Optimization (PPO) for adaptive multi-step lookahead,\ndynamically adjusting the depth and direction of exploration to effectively\novercome the limitations of traditional BO methods. We conduct extensive\nexperiments on synthetic and real-world benchmarks, confirming the superior\nperformance of REBMBO. Additional analyses across various GP configurations\nfurther highlight its adaptability and robustness.",
      "pdf_url": "http://arxiv.org/pdf/2510.19530v1",
      "published": "2025-10-22T12:36:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19530v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification",
      "authors": [
        "Maciej Mozolewski",
        "Betül Bayrak",
        "Kerstin Bach",
        "Grzegorz J. Nalepa"
      ],
      "abstract": "In eXplainable Artificial Intelligence (XAI), instance-based explanations for\ntime series have gained increasing attention due to their potential for\nactionable and interpretable insights in domains such as healthcare. Addressing\nthe challenges of explainability of state-of-the-art models, we propose a\nprototype-driven framework for generating sparse counterfactual explanations\ntailored to 12-lead ECG classification models. Our method employs SHAP-based\nthresholds to identify critical signal segments and convert them into interval\nrules, uses Dynamic Time Warping (DTW) and medoid clustering to extract\nrepresentative prototypes, and aligns these prototypes to query R-peaks for\ncoherence with the sample being explained. The framework generates\ncounterfactuals that modify only 78% of the original signal while maintaining\n81.3% validity across all classes and achieving 43% improvement in temporal\nstability. We evaluate three variants of our approach, Original, Sparse, and\nAligned Sparse, with class-specific performance ranging from 98.9% validity for\nmyocardial infarction (MI) to challenges with hypertrophy (HYP) detection\n(13.2%). This approach supports near realtime generation (< 1 second) of\nclinically valid counterfactuals and provides a foundation for interactive\nexplanation platforms. Our findings establish design principles for\nphysiologically-aware counterfactual explanations in AI-based diagnosis systems\nand outline pathways toward user-controlled explanation interfaces for clinical\ndeployment.",
      "pdf_url": "http://arxiv.org/pdf/2510.19514v1",
      "published": "2025-10-22T12:09:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19514v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse",
      "authors": [
        "Trung-Dung Vu",
        "Benoit Gaudou",
        "Kamaldeep Singh Oberoi"
      ],
      "abstract": "Modeling realistic human behaviour to understand people's mode choices in\norder to propose personalised mobility solutions remains challenging. This\npaper presents an architecture for modeling realistic human mobility behavior\nin complex multimodal transport systems, demonstrated through a case study in\nToulouse, France. We apply Large Language Models (LLMs) within an agent-based\nsimulation to capture decision-making in a real urban setting. The framework\nintegrates the GAMA simulation platform with an LLM-based generative agent,\nalong with General Transit Feed Specification (GTFS) data for public transport,\nand OpenTripPlanner for multimodal routing. GAMA platform models the\ninteractive transport environment, providing visualization and dynamic agent\ninteractions while eliminating the need to construct the simulation environment\nfrom scratch. This design enables a stronger focus on developing generative\nagents and evaluating their performance in transport decision-making processes.\nOver a simulated month, results show that agents not only make context-aware\ntransport decisions but also form habits over time. We conclude that combining\nLLMs with agent-based simulation offers a promising direction for advancing\nintelligent transportation systems and personalised multimodal mobility\nsolutions. We also discuss some limitations of this approach and outline future\nwork on scaling to larger regions, integrating real-time data, and refining\nmemory models.",
      "pdf_url": "http://arxiv.org/pdf/2510.19497v1",
      "published": "2025-10-22T11:45:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19497v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "CARES: Context-Aware Resolution Selector for VLMs",
      "authors": [
        "Moshe Kimhi",
        "Nimrod Shabtay",
        "Raja Giryes",
        "Chaim Baskin",
        "Eli Schwartz"
      ],
      "abstract": "Large vision-language models (VLMs) commonly process images at native or high\nresolution to remain effective across tasks. This inflates visual tokens ofter\nto 97-99% of total tokens, resulting in high compute and latency, even when\nlow-resolution images would suffice. We introduce \\emph{CARES}-a\n\\textbf{C}ontext-\\textbf{A}ware \\textbf{R}esolution \\textbf{S}elector, a\nlightweight preprocessing module that, given an image-query pair, predicts the\n\\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to\nextract features and predict when a target pretrained VLM's response converges\nto its peak ability to answer correctly. Though trained as a discrete\nclassifier over a set of optional resolutions, CARES interpolates continuous\nresolutions at inference for fine-grained control. Across five multimodal\nbenchmarks spanning documents and natural images, as well as diverse target\nVLMs, CARES preserves task performance while reducing compute by up to 80%.",
      "pdf_url": "http://arxiv.org/pdf/2510.19496v1",
      "published": "2025-10-22T11:44:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19496v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning",
      "authors": [
        "Kevin Huang",
        "Rosario Scalise",
        "Cleah Winston",
        "Ayush Agrawal",
        "Yunchu Zhang",
        "Rohan Baijal",
        "Markus Grotz",
        "Byron Boots",
        "Benjamin Burchfiel",
        "Hongkai Dai",
        "Masha Itkina",
        "Paarth Shah",
        "Abhishek Gupta"
      ],
      "abstract": "Imitation learning has proven effective for training robots to perform\ncomplex tasks from expert human demonstrations. However, it remains limited by\nits reliance on high-quality, task-specific data, restricting adaptability to\nthe diverse range of real-world object configurations and scenarios. In\ncontrast, non-expert data -- such as play data, suboptimal demonstrations,\npartial task completions, or rollouts from suboptimal policies -- can offer\nbroader coverage and lower collection costs. However, conventional imitation\nlearning approaches fail to utilize this data effectively. To address these\nchallenges, we posit that with right design decisions, offline reinforcement\nlearning can be used as a tool to harness non-expert data to enhance the\nperformance of imitation learning policies. We show that while standard offline\nRL approaches can be ineffective at actually leveraging non-expert data under\nthe sparse data coverage settings typically encountered in the real world,\nsimple algorithmic modifications can allow for the utilization of this data,\nwithout significant additional assumptions. Our approach shows that broadening\nthe support of the policy distribution can allow imitation algorithms augmented\nby offline RL to solve tasks robustly, showing considerably enhanced recovery\nand generalization behavior. In manipulation tasks, these innovations\nsignificantly increase the range of initial conditions where learned policies\nare successful when non-expert data is incorporated. Moreover, we show that\nthese methods are able to leverage all collected data, including partial or\nsuboptimal demonstrations, to bolster task-directed policy performance. This\nunderscores the importance of algorithmic techniques for using non-expert data\nfor robust policy learning in robotics.",
      "pdf_url": "http://arxiv.org/pdf/2510.19495v1",
      "published": "2025-10-22T11:43:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19495v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
      "authors": [
        "Dunjie Lu",
        "Yiheng Xu",
        "Junli Wang",
        "Haoyuan Wu",
        "Xinyuan Wang",
        "Zekun Wang",
        "Junlin Yang",
        "Hongjin Su",
        "Jixuan Chen",
        "Junda Chen",
        "Yuchen Mao",
        "Jingren Zhou",
        "Junyang Lin",
        "Binyuan Hui",
        "Tao Yu"
      ],
      "abstract": "Training computer-use agents requires massive amounts of GUI interaction\ndata, but manually annotating action trajectories at scale is prohibitively\nexpensive. We present VideoAgentTrek, a scalable pipeline that automatically\nmines training data from publicly available screen-recorded videos at web\nscale, eliminating the need for manual annotation. Our approach addresses a key\nchallenge: raw videos contain implicit demonstrations but lack explicit action\nlabels. To solve this, we develop Video2Action, an inverse dynamics module\n(IDM) with two components: (1) a video grounding model that detects and\nlocalizes GUI actions with precise temporal boundaries and context, and (2) an\naction-content recognizer that extracts structured parameters like click\ncoordinates and typed text with high fidelity. Applied to 39,000 YouTube\ntutorial videos, our pipeline generates 1.52 million interaction steps\nautomatically. We leverage this data through continued pretraining followed by\nsupervised fine-tuning. On OSWorld-Verified, our approach improves task success\nrates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On\nAgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results\ndemonstrate that passive internet videos can be transformed into high-quality\nsupervision for computer-use agents, providing a scalable alternative to\nexpensive manual annotation.",
      "pdf_url": "http://arxiv.org/pdf/2510.19488v1",
      "published": "2025-10-22T11:25:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19488v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge",
      "authors": [
        "Zaifei Yang",
        "Hong Chang",
        "Ruibing Hou",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "abstract": "The molecular large language models have garnered widespread attention due to\ntheir promising potential on molecular applications. However, current molecular\nlarge language models face significant limitations in understanding molecules\ndue to inadequate textual descriptions and suboptimal molecular representation\nstrategies during pretraining. To address these challenges, we introduce\nKnowMol-100K, a large-scale dataset with 100K fine-grained molecular\nannotations across multiple levels, bridging the gap between molecules and\ntextual descriptions. Additionally, we propose chemically-informative molecular\nrepresentation, effectively addressing limitations in existing molecular\nrepresentation strategies. Building upon these innovations, we develop KnowMol,\na state-of-the-art multi-modal molecular large language model. Extensive\nexperiments demonstrate that KnowMol achieves superior performance across\nmolecular understanding and generation tasks.\n  GitHub: https://github.com/yzf-code/KnowMol\n  Huggingface: https://hf.co/datasets/yzf1102/KnowMol-100K",
      "pdf_url": "http://arxiv.org/pdf/2510.19484v1",
      "published": "2025-10-22T11:23:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19484v1",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Graph Unlearning Meets Influence-aware Negative Preference Optimization",
      "authors": [
        "Qiang Chen",
        "Zhongze Wu",
        "Ang He",
        "Xi Lin",
        "Shuo Jiang",
        "Shan You",
        "Chang Xu",
        "Yi Chen",
        "Xiu Su"
      ],
      "abstract": "Recent advancements in graph unlearning models have enhanced model utility by\npreserving the node representation essentially invariant, while using gradient\nascent on the forget set to achieve unlearning. However, this approach causes a\ndrastic degradation in model utility during the unlearning process due to the\nrapid divergence speed of gradient ascent. In this paper, we introduce\n\\textbf{INPO}, an \\textbf{I}nfluence-aware \\textbf{N}egative\n\\textbf{P}reference \\textbf{O}ptimization framework that focuses on slowing the\ndivergence speed and improving the robustness of the model utility to the\nunlearning process. Specifically, we first analyze that NPO has slower\ndivergence speed and theoretically propose that unlearning high-influence edges\ncan reduce impact of unlearning. We design an influence-aware message function\nto amplify the influence of unlearned edges and mitigate the tight topological\ncoupling between the forget set and the retain set. The influence of each edge\nis quickly estimated by a removal-based method. Additionally, we propose a\ntopological entropy loss from the perspective of topology to avoid excessive\ninformation loss in the local structure during unlearning. Extensive\nexperiments conducted on five real-world datasets demonstrate that INPO-based\nmodel achieves state-of-the-art performance on all forget quality metrics while\nmaintaining the model's utility. Codes are available at\n\\href{https://github.com/sh-qiangchen/INPO}{https://github.com/sh-qiangchen/INPO}.",
      "pdf_url": "http://arxiv.org/pdf/2510.19479v1",
      "published": "2025-10-22T11:18:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19479v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring",
      "authors": [
        "Julian Schulz"
      ],
      "abstract": "As AI systems approach dangerous capability levels where inability safety\ncases become insufficient, we need alternative approaches to ensure safety.\nThis paper presents a roadmap for constructing safety cases based on\nchain-of-thought (CoT) monitoring in reasoning models and outlines our research\nagenda. We argue that CoT monitoring might support both control and\ntrustworthiness safety cases. We propose a two-part safety case: (1)\nestablishing that models lack dangerous capabilities when operating without\ntheir CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT\nare detectable by CoT monitoring. We systematically examine two threats to\nmonitorability: neuralese and encoded reasoning, which we categorize into three\nforms (linguistic drift, steganography, and alien reasoning) and analyze their\npotential drivers. We evaluate existing and novel techniques for maintaining\nCoT faithfulness. For cases where models produce non-monitorable reasoning, we\nexplore the possibility of extracting a monitorable CoT from a non-monitorable\nCoT. To assess the viability of CoT monitoring safety cases, we establish\nprediction markets to aggregate forecasts on key technical milestones\ninfluencing their feasibility.",
      "pdf_url": "http://arxiv.org/pdf/2510.19476v1",
      "published": "2025-10-22T11:13:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19476v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission",
      "authors": [
        "Weihao Yang",
        "Hao Huang",
        "Donglei Wu",
        "Ningke Li",
        "Yanqi Pan",
        "Qiyang Zheng",
        "Wen Xia",
        "Shiyi Li",
        "Qiang Wang"
      ],
      "abstract": "Mixture-of-Experts (MoE) has become a popular architecture for scaling large\nmodels. However, the rapidly growing scale outpaces model training on a single\nDC, driving a shift toward a more flexible, cross-DC training paradigm. Under\nthis, Expert Parallelism (EP) of MoE faces significant scalability issues due\nto the limited cross-DC bandwidth. Specifically, existing EP optimizations\nattempt to overlap data communication and computation, which has little benefit\nin low-bandwidth scenarios due to a much longer data communication time.\nTherefore, the trends of cross-DC EP scaling is fast becoming a critical\nroadblock to the continued growth of MoE models.\n  To address this, we propose HybridEP, a modeling-guided framework to optimize\nEP under constrained bandwidth. Our key idea is to dynamically transform the\nspatial placement of experts to reduce data communication traffic and\nfrequency, thereby minimizing EP's communication overheads. However, it is\nnon-trivial to find the optimal solution because it complicates the original\ncommunication pattern by mixing data and expert communication. We therefore\nbuild a stream-based model to determine the optimal transmission ratio. Guided\nby this, we incorporate two techniques: (1) domain-based partition to construct\nthe mapping between hybrid patterns and specific communication topology at GPU\nlevel, and (2) parameter-efficient migration to further refine this topology by\nreducing expert transmission overhead and enlarging the domain size. Combining\nall these designs, HybridEP can be considered as a more general EP with better\nscalability. Experimental results show that HybridEP outperforms existing\nstate-of-the-art MoE training systems by up to 5.6x under constrained\nbandwidth. We further compare HybridEP and EP on large-scale simulations.\nHybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.",
      "pdf_url": "http://arxiv.org/pdf/2510.19470v1",
      "published": "2025-10-22T11:05:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19470v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Universal Quantitative Abstraction: Categorical Duality and Logical Completeness for Probabilistic Systems",
      "authors": [
        "Nivar Anwer"
      ],
      "abstract": "A unified theory of quantitative abstraction is presented for probabilistic\nsystems that links category theory, optimal transport, and quantitative modal\nlogic. At its core is a canonical $ \\varepsilon $-quotient endowed with a\nuniversal property: among all $ \\varepsilon $-abstractions, it is the most\ninformative one that respects a prescribed bound on value loss. This\nconstruction induces an adjunction between abstraction and realization functors\n$ (Q_{\\varepsilon} \\dashv R_{\\varepsilon}) $, established via the Special\nAdjoint Functor Theorem, revealing a categorical duality between metric\nstructure and logical semantics. A behavioral pseudometric is characterized as\nthe unique fixed point of a Bellman-style operator, with contraction and\nLipschitz properties proved in a coalgebraic setting. A quantitative modal $\n\\mu $-calculus is introduced and shown to be expressively complete for\nlogically representable systems, so that behavioral distance coincides with\nmaximal logical deviation. Compositionality under interface refinement is\nanalyzed, clarifying how abstractions interact across system boundaries. An\nexact validation suite on finite Markov decision processes corroborates the\ncontraction property, value-loss bounds, stability under perturbation,\nadversarial distinguishability, and scalability, demonstrating both robustness\nand computational feasibility. The resulting framework provides principled\ntargets for state aggregation and representation learning, with mathematically\nprecise guarantees for value-function approximation in stochastic domains.",
      "pdf_url": "http://arxiv.org/pdf/2510.19444v1",
      "published": "2025-10-22T10:16:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19444v1",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning",
      "authors": [
        "Wonje Choi",
        "Jooyoung Kim",
        "Honguk Woo"
      ],
      "abstract": "We address the challenge of adopting language models (LMs) for embodied tasks\nin dynamic environments, where online access to large-scale inference engines\nor symbolic planners is constrained due to latency, connectivity, and resource\nlimitations. To this end, we present NeSyPr, a novel embodied reasoning\nframework that compiles knowledge via neurosymbolic proceduralization, thereby\nequipping LM-based agents with structured, adaptive, and timely reasoning\ncapabilities. In NeSyPr, task-specific plans are first explicitly generated by\na symbolic tool leveraging its declarative knowledge. These plans are then\ntransformed into composable procedural representations that encode the plans'\nimplicit production rules, enabling the resulting composed procedures to be\nseamlessly integrated into the LM's inference process. This neurosymbolic\nproceduralization abstracts and generalizes multi-step symbolic structured\npath-finding and reasoning into single-step LM inference, akin to human\nknowledge compilation. It supports efficient test-time inference without\nrelying on external symbolic guidance, making it well suited for deployment in\nlatency-sensitive and resource-constrained physical systems. We evaluate NeSyPr\non the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating\nits efficient reasoning capabilities over large-scale reasoning models and a\nsymbolic planner, while using more compact LMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.19429v1",
      "published": "2025-10-22T09:57:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19429v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Neural Variational Dropout Processes",
      "authors": [
        "Insu Jeon",
        "Youngjin Park",
        "Gunhee Kim"
      ],
      "abstract": "Learning to infer the conditional posterior model is a key step for robust\nmeta-learning. This paper presents a new Bayesian meta-learning approach called\nNeural Variational Dropout Processes (NVDPs). NVDPs model the conditional\nposterior distribution based on a task-specific dropout; a low-rank product of\nBernoulli experts meta-model is utilized for a memory-efficient mapping of\ndropout rates from a few observed contexts. It allows for a quick\nreconfiguration of a globally learned and shared neural network for new tasks\nin multi-task few-shot learning. In addition, NVDPs utilize a novel prior\nconditioned on the whole task data to optimize the conditional \\textit{dropout}\nposterior in the amortized variational inference. Surprisingly, this enables\nthe robust approximation of task-specific dropout rates that can deal with a\nwide range of functional ambiguities and uncertainties. We compared the\nproposed method with other meta-learning approaches in the few-shot learning\ntasks such as 1D stochastic regression, image inpainting, and classification.\nThe results show the excellent performance of NVDPs.",
      "pdf_url": "http://arxiv.org/pdf/2510.19425v1",
      "published": "2025-10-22T09:45:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.19425v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T07 (Artificial neural networks), 62F15 (Bayesian inference)"
      ]
    }
  ]
}