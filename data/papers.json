{
  "last_updated": "2025-10-09T00:48:21.706329",
  "papers": [
    {
      "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark",
      "authors": [
        "Deheng Zhang",
        "Yuqian Fu",
        "Runyi Yang",
        "Yang Miao",
        "Tianwen Qian",
        "Xu Zheng",
        "Guolei Sun",
        "Ajad Chhatkuli",
        "Xuanjing Huang",
        "Yu-Gang Jiang",
        "Luc Van Gool",
        "Danda Pani Paudel"
      ],
      "abstract": "Most existing benchmarks for egocentric vision understanding focus primarily\non daytime scenarios, overlooking the low-light conditions that are inevitable\nin real-world applications. To investigate this gap, we present EgoNight, the\nfirst comprehensive benchmark for nighttime egocentric vision, with visual\nquestion answering (VQA) as the core task. A key feature of EgoNight is the\nintroduction of day-night aligned videos, which enhance night annotation\nquality using the daytime data and reveal clear performance gaps between\nlighting conditions. To achieve this, we collect both synthetic videos rendered\nby Blender and real-world recordings, ensuring that scenes and actions are\nvisually and temporally aligned. Leveraging these paired videos, we construct\nEgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and\nrefinement through extensive human verification. Each QA pair is double-checked\nby annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs\nacross 90 videos, spanning 12 diverse QA types, with more than 300 hours of\nhuman work. Evaluations of state-of-the-art multimodal large language models\n(MLLMs) reveal substantial performance drops when transferring from day to\nnight, underscoring the challenges of reasoning under low-light conditions.\nBeyond VQA, EgoNight also introduces two auxiliary tasks, day-night\ncorrespondence retrieval and egocentric depth estimation at night, that further\nexplore the boundaries of existing models. We believe EgoNight-VQA provides a\nstrong foundation for advancing application-driven egocentric vision research\nand for developing models that generalize across illumination domains. All the\ndata and code will be made available upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2510.06218v1",
      "published": "2025-10-07T17:59:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06218v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning",
      "authors": [
        "Jiaru Zou",
        "Soumya Roy",
        "Vinay Kumar Verma",
        "Ziyi Wang",
        "David Wipf",
        "Pan Lu",
        "Sumit Negi",
        "James Zou",
        "Jingrui He"
      ],
      "abstract": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor enhancing the reasoning capabilities of large reasoning models (LRMs),\nparticularly in the context of test-time scaling (TTS). However, their\npotential for supervising LRMs on tabular reasoning domains remains\nunderexplored. Through detailed empirical analyses, we identify that existing\nPRMs, though widely adopted for supervising text-only reasoning steps, struggle\nwith table-specific operations such as sub-table retrieval and schema\ninteraction, leading to critical performance bottlenecks. To address this\nlimitation, we propose TaTToo, a novel table-grounded PRM framework that (i)\nreasons explicitly over tabular reasoning steps and (ii) integrates tool-based\nverification to provide precise reward supervision. Concretely, we first design\na scalable data curation pipeline that constructs over 60k high-quality\nstep-level annotations by integrating table verification rationales with\ntool-based executions. Building on the collected data, we train TaTToo with a\ndual-stage paradigm: cold-start supervised fine-tuning to capture tool-use\nreasoning patterns, followed by reinforcement learning with tool-grounded\nreward shaping to align our model with table-based verification. We provide a\ncomprehensive evaluation of the policy improvement induced by our newly\ndesigned PRM. Across 5 challenging tabular reasoning benchmarks covering\nnumerical reasoning, fact-checking, and data analysis, TaTToo improves\ndownstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines\nsuch as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong\ngeneralizability across diverse TTS strategies.",
      "pdf_url": "http://arxiv.org/pdf/2510.06217v1",
      "published": "2025-10-07T17:59:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06217v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents",
      "authors": [
        "Mingkang Zhu",
        "Xi Chen",
        "Bei Yu",
        "Hengshuang Zhao",
        "Jiaya Jia"
      ],
      "abstract": "Large language model (LLM) agents increasingly rely on external tools such as\nsearch engines to solve complex, multi-step problems, and reinforcement\nlearning (RL) has become a key paradigm for training them. However, the\ntrajectories of search agents are structurally heterogeneous, where variations\nin the number, placement, and outcomes of search calls lead to fundamentally\ndifferent answer directions and reward distributions. Standard policy gradient\nmethods, which use a single global baseline, suffer from what we identify and\nformalize as cross-stratum bias-an \"apples-to-oranges\" comparison of\nheterogeneous trajectories. This cross-stratum bias distorts credit assignment\nand hinders exploration of complex, multi-step search strategies. To address\nthis, we propose Stratified GRPO, whose central component, Stratified Advantage\nNormalization (SAN), partitions trajectories into homogeneous strata based on\ntheir structural properties and computes advantages locally within each\nstratum. This ensures that trajectories are evaluated only against their true\npeers. Our analysis proves that SAN eliminates cross-stratum bias, yields\nconditionally unbiased unit-variance estimates inside each stratum, and retains\nthe global unbiasedness and unit-variance properties enjoyed by standard\nnormalization, resulting in a more pure and scale-stable learning signal. To\nimprove practical stability under finite-sample regimes, we further linearly\nblend SAN with the global estimator. Extensive experiments on diverse\nsingle-hop and multi-hop question-answering benchmarks demonstrate that\nStratified GRPO consistently and substantially outperforms GRPO by up to 11.3\npoints, achieving higher training rewards, greater training stability, and more\neffective search policies. These results establish stratification as a\nprincipled remedy for structural heterogeneity in RL for LLM search agents.",
      "pdf_url": "http://arxiv.org/pdf/2510.06214v1",
      "published": "2025-10-07T17:59:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06214v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Reference Grounded Skill Discovery",
      "authors": [
        "Seungeun Rho",
        "Aaron Trinh",
        "Danfei Xu",
        "Sehoon Ha"
      ],
      "abstract": "Scaling unsupervised skill discovery algorithms to high-DoF agents remains\nchallenging. As dimensionality increases, the exploration space grows\nexponentially, while the manifold of meaningful skills remains limited.\nTherefore, semantic meaningfulness becomes essential to effectively guide\nexploration in high-dimensional spaces. In this work, we present\nReference-Grounded Skill Discovery (RGSD), a novel algorithm that grounds skill\ndiscovery in a semantically meaningful latent space using reference data. RGSD\nfirst performs contrastive pretraining to embed motions on a unit hypersphere,\nclustering each reference trajectory into a distinct direction. This grounding\nenables skill discovery to simultaneously involve both imitation of reference\nbehaviors and the discovery of semantically related diverse behaviors. On a\nsimulated SMPL humanoid with 359-D observations and 69-D actions, RGSD learns\nstructured skills including walking, running, punching, and side stepping, and\nalso discovers related novel behaviors. In downstream control tasks, RGSD\noutperforms imitation-based skill acquisition baselines. Our results suggest\nthat lightweight reference-guided grounding offers a practical path to\ndiscovering semantically rich and structured skills in high-DoF systems.",
      "pdf_url": "http://arxiv.org/pdf/2510.06203v1",
      "published": "2025-10-07T17:55:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06203v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling",
      "authors": [
        "Mingxuan Wang",
        "Satoshi Nakamura"
      ],
      "abstract": "Machine Speech Chain, simulating the human perception-production loop, proves\neffective in jointly improving ASR and TTS. We propose TokenChain, a fully\ndiscrete speech chain coupling semantic-token ASR with a two-stage TTS: an\nautoregressive text-to-semantic model co-trained with ASR and a\nmasked-generative semantic-to-acoustic model for synthesis only. End-to-end\nfeedback across the text interface is enabled with straight-through\nargmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight\naveraging. Ablations examine optimal temperature schedules for in- and\ncross-domain transfer. Evaluation reveals TokenChain surpasses baseline\naccuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with\nstable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by\n31% on TED-LIUM with minimal forgetting, showing that chain learning remains\neffective with token interfaces and models.",
      "pdf_url": "http://arxiv.org/pdf/2510.06201v1",
      "published": "2025-10-07T17:54:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06201v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ]
    },
    {
      "title": "StarEmbed: Benchmarking Time Series Foundation Models on Astronomical Observations of Variable Stars",
      "authors": [
        "Weijian Li",
        "Hong-Yu Chen",
        "Qinjie Lin",
        "Nabeel Rehemtulla",
        "Ved G. Shah",
        "Dennis Wu",
        "Adam A. Miller",
        "Han Liu"
      ],
      "abstract": "Time series foundation models (TSFMs) are increasingly being adopted as\nhighly-capable general-purpose time series representation learners. Although\ntheir training corpora are vast, they exclude astronomical time series data.\nObservations of stars produce peta-scale time series with unique challenges\nincluding irregular sampling and heteroskedasticity. We introduce StarEmbed,\nthe first public benchmark for rigorous and standardized evaluation of\nstate-of-the-art TSFMs on stellar time series observations (``light curves'').\nWe benchmark on three scientifically-motivated downstream tasks: unsupervised\nclustering, supervised classification, and out-of-distribution source\ndetection. StarEmbed integrates a catalog of expert-vetted labels with\nmulti-variate light curves from the Zwicky Transient Facility, yielding ~40k\nhand-labeled light curves spread across seven astrophysical classes. We\nevaluate the zero-shot representation capabilities of three TSFMs (MOIRAI,\nChronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against\nhandcrafted feature extraction, the long-standing baseline in the astrophysics\nliterature. Our results demonstrate that these TSFMs, especially the Chronos\nmodels, which are trained on data completely unlike the astronomical\nobservations, can outperform established astrophysics-specific baselines in\nsome tasks and effectively generalize to entirely new data. In particular,\nTSFMs deliver state-of-the-art performance on our out-of-distribution source\ndetection benchmark. With the first benchmark of TSFMs on astronomical time\nseries data, we test the limits of their generalization and motivate a paradigm\nshift in time-domain astronomy from using task-specific, fully supervised\npipelines toward adopting generic foundation model representations for the\nanalysis of peta-scale datasets from forthcoming observatories.",
      "pdf_url": "http://arxiv.org/pdf/2510.06200v1",
      "published": "2025-10-07T17:53:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06200v1",
      "categories": [
        "astro-ph.SR",
        "astro-ph.IM",
        "cs.AI"
      ]
    },
    {
      "title": "Latent Speech-Text Transformer",
      "authors": [
        "Yen-Ju Lu",
        "Yashesh Gaur",
        "Wei Zhou",
        "Benjamin Muller",
        "Jesus Villalba",
        "Najim Dehak",
        "Luke Zettlemoyer",
        "Gargi Ghosh",
        "Mike Lewis",
        "Srinivasan Iyer",
        "Duc Le"
      ],
      "abstract": "Auto-regressive speech-text models are typically pre-trained on a large\nnumber of interleaved sequences of text tokens and raw speech encoded as speech\ntokens using vector quantization. These models have demonstrated\nstate-of-the-art performance in speech-to-speech understanding and generation\nbenchmarks, together with promising scaling laws, primarily enabled by the\nrepresentational alignment between text and speech. Nevertheless, they suffer\nfrom shortcomings, partly owing to the disproportionately longer sequences of\nspeech tokens in contrast to textual tokens. This results in a large compute\nimbalance between modalities during pre-training as well as during inference,\nand a potential hindrance to effectively aligning speech and text, ultimately\ntranslating to several orders of magnitude slower scaling laws. We introduce\nthe Latent Speech-Text Transformer (LST), which makes pre-training speech-text\nmodels more data-efficient by dynamically and inexpensively aggregating speech\ntokens into latent speech patches. These patches serve as higher-level units\nthat can either align with corresponding textual units to aid capability\ntransfer or even encapsulate common speech sequences like silences to be more\ncompute-efficient. We show that LST outperforms vanilla approaches on\nspeech-to-speech as well as text-to-text benchmarks in both data- and\ncompute-controlled settings, the former indicating more effective\nrepresentational alignment and the latter indicating steeper scaling laws for\nspeech-text models. On HellaSwag story completion, LST achieves 6.5% absolute\ngain in speech accuracy under compute-controlled training and 5.3% under\ndata-controlled training, while also improving text performance. We will\nrelease our models, code, and the evaluation data to facilitate further\nresearch.",
      "pdf_url": "http://arxiv.org/pdf/2510.06195v1",
      "published": "2025-10-07T17:52:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06195v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "Barbarians at the Gate: How AI is Upending Systems Research",
      "authors": [
        "Audrey Cheng",
        "Shu Liu",
        "Melissa Pan",
        "Zhifei Li",
        "Bowen Wang",
        "Alex Krentsel",
        "Tian Xia",
        "Mert Cemri",
        "Jongseok Park",
        "Shuo Yang",
        "Jeff Chen",
        "Lakshya Agrawal",
        "Aditya Desai",
        "Jiarong Xing",
        "Koushik Sen",
        "Matei Zaharia",
        "Ion Stoica"
      ],
      "abstract": "Artificial Intelligence (AI) is starting to transform the research process as\nwe know it by automating the discovery of new solutions. Given a task, the\ntypical AI-driven approach is (i) to generate a set of diverse solutions, and\nthen (ii) to verify these solutions and select one that solves the problem.\nCrucially, this approach assumes the existence of a reliable verifier, i.e.,\none that can accurately determine whether a solution solves the given problem.\nWe argue that systems research, long focused on designing and evaluating new\nperformance-oriented algorithms, is particularly well-suited for AI-driven\nsolution discovery. This is because system performance problems naturally admit\nreliable verifiers: solutions are typically implemented in real systems or\nsimulators, and verification reduces to running these software artifacts\nagainst predefined workloads and measuring performance. We term this approach\nas AI-Driven Research for Systems (ADRS), which iteratively generates,\nevaluates, and refines solutions. Using penEvolve, an existing open-source ADRS\ninstance, we present case studies across diverse domains, including load\nbalancing for multi-region cloud scheduling, Mixture-of-Experts inference,\nLLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS\ndiscovers algorithms that outperform state-of-the-art human designs (e.g.,\nachieving up to 5.0x runtime improvements or 50% cost reductions). We distill\nbest practices for guiding algorithm evolution, from prompt design to evaluator\nconstruction, for existing frameworks. We then discuss the broader implications\nfor the systems community: as AI assumes a central role in algorithm design, we\nargue that human researchers will increasingly focus on problem formulation and\nstrategic guidance. Our results highlight both the disruptive potential and the\nurgent need to adapt systems research practices in the age of AI.",
      "pdf_url": "http://arxiv.org/pdf/2510.06189v2",
      "published": "2025-10-07T17:49:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06189v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects",
      "authors": [
        "Jakir Hasan",
        "Shubhashis Roy Dipta"
      ],
      "abstract": "Real-time speech assistants are becoming increasingly popular for ensuring\nimproved accessibility to information. Bengali, being a low-resource language\nwith a high regional dialectal diversity, has seen limited progress in\ndeveloping such systems. Existing systems are not optimized for real-time use\nand focus only on standard Bengali. In this work, we present BanglaTalk, the\nfirst real-time speech assistance system for Bengali regional dialects.\nBanglaTalk follows the client-server architecture and uses the Real-time\nTransport Protocol (RTP) to ensure low-latency communication. To address\ndialectal variation, we introduce a dialect-aware ASR system, BRDialect,\ndeveloped by fine-tuning the IndicWav2Vec model in ten Bengali regional\ndialects. It outperforms the baseline ASR models by 12.41-33.98% on the\nRegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of\n24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low\nbandwidth usage and minimal end-to-end delay make the system both\ncost-effective and interactive for real-time use cases, enabling inclusive and\naccessible speech technology for the diverse community of Bengali speakers.",
      "pdf_url": "http://arxiv.org/pdf/2510.06188v1",
      "published": "2025-10-07T17:47:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06188v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Automated Program Repair of Uncompilable Student Code",
      "authors": [
        "Griffin Pitts",
        "Aum Pandya",
        "Darsh Rank",
        "Tirth Bhatt",
        "Muntasir Hoq",
        "Bita Akram"
      ],
      "abstract": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime.",
      "pdf_url": "http://arxiv.org/pdf/2510.06187v1",
      "published": "2025-10-07T17:46:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06187v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback",
      "authors": [
        "Chunyu Miao",
        "Henry Peng Zou",
        "Yangning Li",
        "Yankai Chen",
        "Yibo Wang",
        "Fangxin Wang",
        "Yifan Li",
        "Wooseong Yang",
        "Bowei He",
        "Xinni Zhang",
        "Dianzhi Yu",
        "Hanchen Yang",
        "Hoang H Nguyen",
        "Yue Zhou",
        "Jie Yang",
        "Jizhou Guo",
        "Wenzhe Fan",
        "Chin-Yuan Yeh",
        "Panpan Meng",
        "Liancheng Fang",
        "Jinhu Qi",
        "Wei-Chieh Huang",
        "Zhengyao Gu",
        "Yuwei Han",
        "Langzhou He",
        "Yuyao Yang",
        "Xue Liu",
        "Irwin King",
        "Philip S. Yu"
      ],
      "abstract": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation",
      "pdf_url": "http://arxiv.org/pdf/2510.06186v1",
      "published": "2025-10-07T17:45:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06186v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Smartphone-based iris recognition through high-quality visible-spectrum iris image capture.V2",
      "authors": [
        "Naveenkumar G Venkataswamy",
        "Yu Liu",
        "Soumyabrata Dey",
        "Stephanie Schuckers",
        "Masudul H Imtiaz"
      ],
      "abstract": "Smartphone-based iris recognition in the visible spectrum (VIS) remains\ndifficult due to illumination variability, pigmentation differences, and the\nabsence of standardized capture controls. This work presents a compact\nend-to-end pipeline that enforces ISO/IEC 29794-6 quality compliance at\nacquisition and demonstrates that accurate VIS iris recognition is feasible on\ncommodity devices. Using a custom Android application performing real-time\nframing, sharpness evaluation, and feedback, we introduce the CUVIRIS dataset\nof 752 compliant images from 47 subjects. A lightweight MobileNetV3-based\nmulti-task segmentation network (LightIrisNet) is developed for efficient\non-device processing, and a transformer matcher (IrisFormer) is adapted to the\nVIS domain. Under a standardized protocol and comparative benchmarking against\nprior CNN baselines, OSIRIS attains a TAR of 97.9% at FAR=0.01 (EER=0.76%),\nwhile IrisFormer, trained only on UBIRIS.v2, achieves an EER of 0.057% on\nCUVIRIS. The acquisition app, trained models, and a public subset of the\ndataset are released to support reproducibility. These results confirm that\nstandardized capture and VIS-adapted lightweight models enable accurate and\npractical iris recognition on smartphones.",
      "pdf_url": "http://arxiv.org/pdf/2510.06170v1",
      "published": "2025-10-07T17:33:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06170v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams",
      "authors": [
        "Aju Ani Justus",
        "Chris Baber"
      ],
      "abstract": "A critical challenge in modelling Heterogeneous-Agent Teams is training\nagents to collaborate with teammates whose policies are inaccessible or\nnon-stationary, such as humans. Traditional approaches rely on expensive\nhuman-in-the-loop data, which limits scalability. We propose using Large\nLanguage Models (LLMs) as policy-agnostic human proxies to generate synthetic\ndata that mimics human decision-making. To evaluate this, we conduct three\nexperiments in a grid-world capture game inspired by Stag Hunt, a game theory\nparadigm that balances risk and reward. In Experiment 1, we compare decisions\nfrom 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and\nMixtral 8x22B models. LLMs, prompted with game-state observations and reward\nstructures, align more closely with experts than participants, demonstrating\nconsistency in applying underlying decision criteria. Experiment 2 modifies\nprompts to induce risk-sensitive strategies (e.g. \"be risk averse\"). LLM\noutputs mirror human participants' variability, shifting between risk-averse\nand risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic\ngrid-world where the LLM agents generate movement actions. LLMs produce\ntrajectories resembling human participants' paths. While LLMs cannot yet fully\nreplicate human adaptability, their prompt-guided diversity offers a scalable\nfoundation for simulating policy-agnostic teammates.",
      "pdf_url": "http://arxiv.org/pdf/2510.06151v1",
      "published": "2025-10-07T17:21:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06151v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images",
      "authors": [
        "Aditya Prakash",
        "David Forsyth",
        "Saurabh Gupta"
      ],
      "abstract": "We tackle the problem of forecasting bimanual 3D hand motion & articulation\nfrom a single image in everyday settings. To address the lack of 3D hand\nannotations in diverse settings, we design an annotation pipeline consisting of\na diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the\nforecasting model, we adopt a diffusion loss to account for the multimodality\nin hand motion distribution. Extensive experiments across 6 datasets show the\nbenefits of training on diverse data with imputed labels (14% improvement) and\neffectiveness of our lifting (42% better) & forecasting (16.4% gain) models,\nover the best baselines, especially in zero-shot generalization to everyday\nimages.",
      "pdf_url": "http://arxiv.org/pdf/2510.06145v1",
      "published": "2025-10-07T17:18:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06145v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks",
      "authors": [
        "Rushiv Arora"
      ],
      "abstract": "Multi-task reinforcement learning often relies on task metadata -- such as\nbrief natural-language descriptions -- to guide behavior across diverse\nobjectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned\nmixture-of-policies architecture for multi-task RL. LEXPOL encodes task\nmetadata with a text encoder and uses a learned gating module to select or\nblend among multiple sub-policies, enabling end-to-end training across tasks.\nOn MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines\nin success rate and sample efficiency, without task-specific retraining. To\nanalyze the mechanism, we further study settings with fixed expert policies\nobtained independently of the gate and show that the learned language gate\ncomposes these experts to produce behaviors appropriate to novel task\ndescriptions and unseen task combinations. These results indicate that\nnatural-language metadata can effectively index and recombine reusable skills\nwithin a single policy.",
      "pdf_url": "http://arxiv.org/pdf/2510.06138v1",
      "published": "2025-10-07T17:12:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06138v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6"
      ]
    },
    {
      "title": "Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification",
      "authors": [
        "Weihao Zeng",
        "Keqing He",
        "Chuqiao Kuang",
        "Xiaoguang Li",
        "Junxian He"
      ],
      "abstract": "Test-time compute can be scaled both sequentially and in parallel. Sequential\nscaling involves lengthening the generation process, while parallel scaling\ninvolves verifying and selecting among multiple candidate outputs. Combining\nthese two strategies has led to the most powerful AI systems, such as Grok 4\nHeavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles),\nverifying responses can be substantially easier than generating them. This\nproperty, referred to as \\emph{asymmetric verification}, highlights the strong\npotential of test-time scaling (TTS). In this work, we study both sequential\nand parallel TTS of deep search agents, motivated by the intuition that\nverification in this setting is often much easier than generation. In\nexperiments, we first show that sequential scaling methods, such as budget\nforcing, can be effective initially but soon degrade performance. Leveraging\nasymmetric verification, however, we are able to achieve substantial\nimprovements by allocating only a modest amount of compute to the verifier. We\nconduct experiments with flagship open-source models and extend them to their\n``Heavy'' variants through TTS. These deep research agents achieve gains of up\nto 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an\nopen-source alternative, GLM-4.5 Heavy reaches accuracy of {\\bf 54.0\\%} on\nBrowseComp and {\\bf 66.0\\%} on GAIA, placing it comparable to the best\nproprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy\nfurther achieves {\\bf 69.0\\%} accuracy on BrowseComp, greatly surpassing the\nbest proprietary results.",
      "pdf_url": "http://arxiv.org/pdf/2510.06135v1",
      "published": "2025-10-07T17:09:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06135v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits",
      "authors": [
        "Kangyu Wang",
        "Zhiyun Jiang",
        "Haibo Feng",
        "Weijia Zhao",
        "Lin Liu",
        "Jianguo Li",
        "Zhenzhong Lan",
        "Weiyao Lin"
      ],
      "abstract": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising steps, achieving parallel decoding by denoising only high-confidence\npositions at each step. However, existing approaches often repetitively remask\ntokens due to initially low confidence scores, leading to redundant iterations\nand limiting overall acceleration. Through the analysis of dLLM decoding\ntraces, we observe that the model often determines the final prediction for a\ntoken several steps before the decoding step. To leverage this historical\ninformation and avoid redundant steps, we introduce the concept of Trace\nCredit, which quantifies each token's convergence potential by accumulating\nhistorical logits. Furthermore, we propose CreditDecoding, a training-free\nparallel decoding algorithm that accelerates the confidence convergence of\ncorrect but underconfident tokens by fusing current logits with Trace Credit.\nThis process significantly reduces redundant iterations and enhances decoding\nrobustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup\nand a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times\nspeedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.\nImportantly, CreditDecoding scales effectively to long sequences and is\northogonal to mainstream inference optimizations, making it a readily\nintegrable and versatile solution.",
      "pdf_url": "http://arxiv.org/pdf/2510.06133v1",
      "published": "2025-10-07T17:08:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06133v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation",
      "authors": [
        "Jiawei Mao",
        "Yuhan Wang",
        "Lifeng Chen",
        "Can Zhao",
        "Yucheng Tang",
        "Dong Yang",
        "Liangqiong Qu",
        "Daguang Xu",
        "Yuyin Zhou"
      ],
      "abstract": "Recent advances in generative medical models are constrained by\nmodality-specific scenarios that hinder the integration of complementary\nevidence from imaging, pathology, and clinical notes. This fragmentation limits\ntheir evolution into foundation models that can learn and reason across the\nfull spectrum of biomedical data. We propose MeDiM, the first medical discrete\ndiffusion model that learns shared distributions across modalities without\nmodality-specific components. MeDiM unifies multiple generative tasks:\ntranslating between images and text, and jointly producing image-report pairs\nacross domains in response to prompts. Built on a discrete diffusion framework,\nMeDiM bridges vision and language representations through a shared\nprobabilistic space. To enable unified and flexible medical generation, we\nemploy a multimodal large language model (MLLM) as the diffusion backbone,\nleveraging its prior knowledge and cross-modal reasoning. Two key designs are\nintroduced: (1) removing the causal attention mask for bidirectional context,\nand (2) injecting continuous timestep embeddings for diffusion awareness.\nExperiments demonstrate high-fidelity medical generation (FID 16.60 on\nMIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR\n0.2650 and 0.2580). Jointly generated image-report pairs further enhance\ndownstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,\nplus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports\ncoherent and clinically grounded multimodal outputs.",
      "pdf_url": "http://arxiv.org/pdf/2510.06131v1",
      "published": "2025-10-07T17:06:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06131v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models",
      "authors": [
        "Gagan Bhatia",
        "Somayajulu G Sripada",
        "Kevin Allan",
        "Jacobo Azcona"
      ],
      "abstract": "Large Language Models (LLMs) are prone to hallucination, the generation of\nplausible yet factually incorrect statements. This work investigates the\nintrinsic, architectural origins of this failure mode through three primary\ncontributions.First, to enable the reliable tracing of internal semantic\nfailures, we propose \\textbf{Distributional Semantics Tracing (DST)}, a unified\nframework that integrates established interpretability techniques to produce a\ncausal map of a model's reasoning, treating meaning as a function of context\n(distributional semantics). Second, we pinpoint the model's layer at which a\nhallucination becomes inevitable, identifying a specific \\textbf{commitment\nlayer} where a model's internal representations irreversibly diverge from\nfactuality. Third, we identify the underlying mechanism for these failures. We\nobserve a conflict between distinct computational pathways, which we interpret\nusing the lens of dual-process theory: a fast, heuristic \\textbf{associative\npathway} (akin to System 1) and a slow, deliberate \\textbf{contextual pathway}\n(akin to System 2), leading to predictable failure modes such as\n\\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the\ncoherence of the contextual pathway reveals a strong negative correlation\n($\\rho = -0.863$) with hallucination rates, implying that these failures are\npredictable consequences of internal semantic weakness. The result is a\nmechanistic account of how, when, and why hallucinations occur within the\nTransformer architecture.",
      "pdf_url": "http://arxiv.org/pdf/2510.06107v1",
      "published": "2025-10-07T16:40:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06107v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ]
    },
    {
      "title": "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences",
      "authors": [
        "Batu El",
        "James Zou"
      ],
      "abstract": "Large language models (LLMs) are increasingly shaping how information is\ncreated and disseminated, from companies using them to craft persuasive\nadvertisements, to election campaigns optimizing messaging to gain votes, to\nsocial media influencers boosting engagement. These settings are inherently\ncompetitive, with sellers, candidates, and influencers vying for audience\napproval, yet it remains poorly understood how competitive feedback loops\ninfluence LLM behavior. We show that optimizing LLMs for competitive success\ncan inadvertently drive misalignment. Using simulated environments across these\nscenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise\nin deceptive marketing; in elections, a 4.9% gain in vote share coincides with\n22.3% more disinformation and 12.5% more populist rhetoric; and on social\nmedia, a 7.5% engagement boost comes with 188.6% more disinformation and a\n16.3% increase in promotion of harmful behaviors. We call this phenomenon\nMoloch's Bargain for AI--competitive success achieved at the cost of alignment.\nThese misaligned behaviors emerge even when models are explicitly instructed to\nremain truthful and grounded, revealing the fragility of current alignment\nsafeguards. Our findings highlight how market-driven optimization pressures can\nsystematically erode alignment, creating a race to the bottom, and suggest that\nsafe deployment of AI systems will require stronger governance and carefully\ndesigned incentives to prevent competitive dynamics from undermining societal\ntrust.",
      "pdf_url": "http://arxiv.org/pdf/2510.06105v1",
      "published": "2025-10-07T16:37:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06105v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices",
      "authors": [
        "Mallika Mainali",
        "Harsha Sureshbabu",
        "Anik Sen",
        "Christopher B. Rauch",
        "Noah D. Reifsnyder",
        "John Meyer",
        "J. T. Turner",
        "Michael W. Floyd",
        "Matthew Molineaux",
        "Rosina O. Weber"
      ],
      "abstract": "As algorithmic decision-makers are increasingly applied to high-stakes\ndomains, AI alignment research has evolved from a focus on universal value\nalignment to context-specific approaches that account for decision-maker\nattributes. Prior work on Decision-Maker Alignment (DMA) has explored two\nprimary strategies: (1) classical AI methods integrating case-based reasoning,\nBayesian reasoning, and naturalistic decision-making, and (2) large language\nmodel (LLM)-based methods leveraging prompt engineering. While both approaches\nhave shown promise in limited domains such as medical triage, their\ngeneralizability to novel contexts remains underexplored. In this work, we\nimplement a prior classical AI model and develop an LLM-based algorithmic\ndecision-maker evaluated using a large reasoning model (GPT-5) and a\nnon-reasoning model (GPT-4) with weighted self-consistency under a zero-shot\nprompting framework, as proposed in recent literature. We evaluate both\napproaches on a health insurance decision-making dataset annotated for three\ntarget decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).\nIn the experiments reported herein, classical AI and LLM-based models achieved\ncomparable alignment with attribute-based targets, with classical AI exhibiting\nslightly better alignment for a moderate risk profile. The dataset and\nopen-source implementation are publicly available at:\nhttps://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and\nhttps://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.",
      "pdf_url": "http://arxiv.org/pdf/2510.06093v1",
      "published": "2025-10-07T16:21:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06093v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A public cardiac CT dataset featuring the left atrial appendage",
      "authors": [
        "Bjoern Hansen",
        "Jonas Pedersen",
        "Klaus F. Kofoed",
        "Oscar Camara",
        "Rasmus R. Paulsen",
        "Kristine Soerensen"
      ],
      "abstract": "Despite the success of advanced segmentation frameworks such as\nTotalSegmentator (TS), accurate segmentations of the left atrial appendage\n(LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant\nchallenge in medical imaging. In this work, we present the first open-source,\nanatomically coherent dataset of curated, high-resolution segmentations for\nthese structures, supplemented with whole-heart labels produced by TS on the\npublicly available ImageCAS dataset consisting of 1000 cardiac computed\ntomography angiography (CCTA) scans. One purpose of the data set is to foster\nnovel approaches to the analysis of LAA morphology.\n  LAA segmentations on ImageCAS were generated using a state-of-the-art\nsegmentation framework developed specifically for high resolution LAA\nsegmentation. We trained the network on a large private dataset with manual\nannotations provided by medical readers guided by a trained cardiologist and\ntransferred the model to ImageCAS data. CA labels were improved from the\noriginal ImageCAS annotations, while PV segmentations were refined from TS\noutputs. In addition, we provide a list of scans from ImageCAS that contains\ncommon data flaws such as step artefacts, LAAs extending beyond the scanner's\nfield of view, and other types of data defects.",
      "pdf_url": "http://arxiv.org/pdf/2510.06090v1",
      "published": "2025-10-07T16:16:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06090v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability",
      "authors": [
        "Taylor Sorensen",
        "Benjamin Newman",
        "Jared Moore",
        "Chan Park",
        "Jillian Fisher",
        "Niloofar Mireshghallah",
        "Liwei Jiang",
        "Yejin Choi"
      ],
      "abstract": "Language model post-training has enhanced instruction-following and\nperformance on many downstream tasks, but also comes with an often-overlooked\ncost on tasks with many possible valid answers. We characterize three\ndesiderata for conditional distributional modeling: in-context steerability,\nvalid output space coverage, and distributional alignment, and document across\nthree model families how current post-training can reduce these properties. In\nparticular, we disambiguate between two kinds of in-context learning: ICL for\neliciting existing underlying knowledge or capabilities, and in-context\nsteerability, where a model must use in-context information to override its\npriors and steer to a novel data generating distribution. To better evaluate\nand improve these desiderata, we introduce Spectrum Suite, a large-scale\nresource compiled from >40 data sources and spanning >90 tasks requiring models\nto steer to and match diverse distributions ranging from varied human\npreferences to numerical distributions and more. We find that while current\npost-training techniques help elicit underlying capabilities and knowledge,\nthey hurt models' ability to flexibly steer in-context. To mitigate these\nissues, we propose Spectrum Tuning, a post-training method using Spectrum Suite\nto improve steerability and distributional coverage. We find that Spectrum\nTuning often improves over pretrained models and their instruction-tuned\ncounterparts, enhancing steerability, spanning more of the output space, and\nimproving distributional alignment on held-out datasets.",
      "pdf_url": "http://arxiv.org/pdf/2510.06084v1",
      "published": "2025-10-07T16:10:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06084v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents",
      "authors": [
        "Tao Zhe",
        "Rui Liu",
        "Fateme Memar",
        "Xiao Luo",
        "Wei Fan",
        "Xinyue Ye",
        "Zhongren Peng",
        "Dongjie Wang"
      ],
      "abstract": "Route recommendation aims to provide users with optimal travel plans that\nsatisfy diverse and complex requirements. Classical routing algorithms (e.g.,\nshortest-path and constraint-aware search) are efficient but assume structured\ninputs and fixed objectives, limiting adaptability to natural-language queries.\nRecent LLM-based approaches enhance flexibility but struggle with spatial\nreasoning and the joint modeling of route-level and POI-level preferences. To\naddress these limitations, we propose RouteLLM, a hierarchical multi-agent\nframework that grounds natural-language intents into constraint-aware routes.\nIt first parses user queries into structured intents including POIs, paths, and\nconstraints. A manager agent then coordinates specialized sub-agents: a\nconstraint agent that resolves and formally check constraints, a POI agent that\nretrieves and ranks candidate POIs, and a path refinement agent that refines\nroutes via a routing engine with preference-conditioned costs. A final verifier\nagent ensures constraint satisfaction and produces the final route with an\ninterpretable rationale. This design bridges linguistic flexibility and spatial\nstructure, enabling reasoning over route feasibility and user preferences.\nExperiments show that our method reliably grounds textual preferences into\nconstraint-aware routes, improving route quality and preference satisfaction\nover classical methods.",
      "pdf_url": "http://arxiv.org/pdf/2510.06078v1",
      "published": "2025-10-07T16:03:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06078v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning",
      "authors": [
        "Mi Luo",
        "Zihui Xue",
        "Alex Dimakis",
        "Kristen Grauman"
      ],
      "abstract": "Video reasoning, the task of enabling machines to infer from dynamic visual\ncontent through multi-step logic, is crucial for advanced AI. While the\nChain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,\nits application to video understanding remains underexplored. This paper\npresents a systematic analysis revealing that CoT often degrades performance in\nvideo reasoning, generating verbose but misleading internal monologues, and\nleading to hallucinated visual details and overridden correct intuitions - a\nphenomenon we term \"visual thinking drift\". We explain this drift through a\nBayesian lens, positing that CoT traces often diverge from actual visual\nevidence, instead amplifying internal biases or language priors, causing models\nto storytell rather than engage in grounded reasoning. To counteract this, we\nintroduce Visual Evidence Reward (VER), a novel reinforcement learning\nframework that explicitly rewards the generation of reasoning traces that are\nverifiably grounded in visual evidence. Comprehensive evaluation across 10\ndiverse video understanding benchmarks demonstrates that our Video-VER\nconsistently achieves top performance. Our work sheds light on the distinct\nchallenges of video-centric reasoning and encourages the development of AI that\nrobustly grounds its inferences in visual evidence - for large multimodal\nmodels that not only \"think before answering\", but also \"see while thinking\".",
      "pdf_url": "http://arxiv.org/pdf/2510.06077v1",
      "published": "2025-10-07T16:03:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06077v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks",
      "authors": [
        "Joo Palmeiro",
        "Diogo Duarte",
        "Rita Costa",
        "Pedro Bizarro"
      ],
      "abstract": "AI models are increasingly used for data analysis and visualization, yet\nbenchmarks rarely address scatterplot-specific tasks, limiting insight into\nperformance. To address this gap for one of the most common chart types, we\nintroduce a synthetic, annotated dataset of over 18,000 scatterplots from six\ndata generators and 17 chart designs, and a benchmark based on it. We evaluate\nproprietary models from OpenAI and Google using N-shot prompting on five\ndistinct tasks derived from annotations of cluster bounding boxes, their center\ncoordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash,\nespecially when prompted with examples, are viable options for counting\nclusters and, in Flash's case, outliers (90%+ Accuracy). However, the results\nfor localization-related tasks are unsatisfactory: Precision and Recall are\nnear or below 50%, except for Flash in outlier identification (65.01%).\nFurthermore, the impact of chart design on performance appears to be a\nsecondary factor, but it is advisable to avoid scatterplots with wide aspect\nratios (16:9 and 21:9) or those colored randomly. Supplementary materials are\navailable at https://github.com/feedzai/biy-paper.",
      "pdf_url": "http://arxiv.org/pdf/2510.06071v1",
      "published": "2025-10-07T15:59:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06071v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning",
      "authors": [
        "Heng Zhang",
        "Kevin Yuchen Ma",
        "Mike Zheng Shou",
        "Weisi Lin",
        "Yan Wu"
      ],
      "abstract": "Dexterous grasping with multi-fingered hands remains challenging due to\nhigh-dimensional articulations and the cost of optimization-based pipelines.\nExisting end-to-end methods require training on large-scale datasets for\nspecific hands, limiting their ability to generalize across different\nembodiments. We propose an eigengrasp-based, end-to-end framework for\ncross-embodiment grasp generation. From a hand's morphology description, we\nderive a morphology embedding and an eigengrasp set. Conditioned on these,\ntogether with the object point cloud and wrist pose, an amplitude predictor\nregresses articulation coefficients in a low-dimensional space, which are\ndecoded into full joint articulations. Articulation learning is supervised with\na Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant\nmotions and injects morphology-specific structure. In simulation on unseen\nobjects across three dexterous hands, our model attains a 91.9% average grasp\nsuccess rate with less than 0.4 seconds inference per grasp. With few-shot\nadaptation to an unseen hand, it achieves 85.6% success on unseen objects in\nsimulation, and real-world experiments on this few-shot generalized hand\nachieve an 87% success rate. The code and additional materials will be made\navailable upon publication on our project website\nhttps://connor-zh.github.io/cross_embodiment_dexterous_grasping.",
      "pdf_url": "http://arxiv.org/pdf/2510.06068v1",
      "published": "2025-10-07T15:57:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06068v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA",
      "authors": [
        "Python Song",
        "Luke Tenyi Chang",
        "Yun-Yun Tsai",
        "Penghui Li",
        "Junfeng Yang"
      ],
      "abstract": "CAPTCHA, originally designed to distinguish humans from robots, has evolved\ninto a real-world benchmark for assessing the spatial reasoning capabilities of\nvision-language models. In this work, we first show that step-by-step reasoning\nis crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent\nhigh-difficulty spatial reasoning tasks, and that current commercial\nvision-language models still struggle with such reasoning. In particular, we\nobserve that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to\neffectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).\nHowever, our findings indicate that requiring the model to perform step-by-step\nreasoning before generating the final coordinates can significantly enhance its\nsolving accuracy, underscoring the severity of the gap. To systematically study\nthis issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with\nreasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,\netc.) with step-by-step action solutions and grounding annotations. We further\ndefine five reasoning-oriented metrics that enable a comprehensive evaluation\nof models reasoning capabilities. To validate the effectiveness of reasoning,\nwe also propose a general agentic VLM-based framework that incorporates the\nmodels inherent reasoning abilities. Our method achieves state-of-the-art\nperformance across five high-difficulty CAPTCHA types, with an average solving\naccuracy of 83.9 percent, substantially surpassing existing baselines. These\nresults reveal the limitations of current models and highlight the importance\nof reasoning in advancing visual-spatial challenges in the future.",
      "pdf_url": "http://arxiv.org/pdf/2510.06067v1",
      "published": "2025-10-07T15:56:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06067v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis",
      "authors": [
        "Austin Feng",
        "Andreas Varvarigos",
        "Ioannis Panitsas",
        "Daniela Fernandez",
        "Jinbiao Wei",
        "Yuwei Guo",
        "Jialin Chen",
        "Ali Maatouk",
        "Leandros Tassiulas",
        "Rex Ying"
      ],
      "abstract": "Modern enterprises generate vast streams of time series metrics when\nmonitoring complex systems, known as observability data. Unlike conventional\ntime series from domains such as weather, observability data are zero-inflated,\nhighly stochastic, and exhibit minimal temporal structure. Despite their\nimportance, observability datasets are underrepresented in public benchmarks\ndue to proprietary restrictions. Existing datasets are often anonymized and\nnormalized, removing scale information and limiting their use for tasks beyond\nforecasting, such as anomaly detection, root-cause analysis, and multi-modal\nreasoning. To address this gap, we introduce TelecomTS, a large-scale\nobservability dataset derived from a 5G telecommunications network. TelecomTS\nfeatures heterogeneous, de-anonymized covariates with explicit scale\ninformation and supports a suite of downstream tasks, including anomaly\ndetection, root-cause analysis, and a question-answering benchmark requiring\nmulti-modal reasoning. Benchmarking state-of-the-art time series, language, and\nreasoning models reveals that existing approaches struggle with the abrupt,\nnoisy, and high-variance dynamics of observability data. Our experiments also\nunderscore the importance of preserving covariates' absolute scale, emphasizing\nthe need for foundation time series models that natively leverage scale\ninformation for practical observability applications.",
      "pdf_url": "http://arxiv.org/pdf/2510.06063v1",
      "published": "2025-10-07T15:54:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06063v1",
      "categories": [
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "math.IT"
      ]
    },
    {
      "title": "Controllable Audio-Visual Viewpoint Generation from 360 Spatial Information",
      "authors": [
        "Christian Marinoni",
        "Riccardo Fosco Gramaccioni",
        "Eleonora Grassucci",
        "Danilo Comminiello"
      ],
      "abstract": "The generation of sounding videos has seen significant advancements with the\nadvent of diffusion models. However, existing methods often lack the\nfine-grained control needed to generate viewpoint-specific content from larger,\nimmersive 360-degree environments. This limitation restricts the creation of\naudio-visual experiences that are aware of off-camera events. To the best of\nour knowledge, this is the first work to introduce a framework for controllable\naudio-visual generation, addressing this unexplored gap. Specifically, we\npropose a diffusion model by introducing a set of powerful conditioning signals\nderived from the full 360-degree space: a panoramic saliency map to identify\nregions of interest, a bounding-box-aware signed distance map to define the\ntarget viewpoint, and a descriptive caption of the entire scene. By integrating\nthese controls, our model generates spatially-aware viewpoint videos and audios\nthat are coherently influenced by the broader, unseen environmental context,\nintroducing a strong controllability that is essential for realistic and\nimmersive audio-visual generation. We show audiovisual examples proving the\neffectiveness of our framework.",
      "pdf_url": "http://arxiv.org/pdf/2510.06060v1",
      "published": "2025-10-07T15:53:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06060v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research",
      "authors": [
        "Gang Liu",
        "Yihan Zhu",
        "Jie Chen",
        "Meng Jiang"
      ],
      "abstract": "Large language models hold promise as scientific assistants, yet existing\nagents either rely solely on algorithm evolution or on deep research in\nisolation, both of which face critical limitations. Pure algorithm evolution,\nas in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly\nplateaus in complex domains, while pure deep research proposes ideas without\nvalidation, resulting in unrealistic or unimplementable solutions. We present\nDeepEvolve, an agent that integrates deep research with algorithm evolution,\nuniting external knowledge retrieval, cross-file code editing, and systematic\ndebugging under a feedback-driven iterative loop. Each iteration not only\nproposes new hypotheses but also refines, implements, and tests them, avoiding\nboth shallow improvements and unproductive over-refinements. Across nine\nbenchmarks in chemistry, mathematics, biology, materials, and patents,\nDeepEvolve consistently improves the initial algorithm, producing executable\nnew algorithms with sustained gains. By bridging the gap between unguided\nevolution and research without grounding, DeepEvolve provides a reliable\nframework for advancing scientific algorithm discovery. Our code is available\nat https://github.com/liugangcode/deepevolve.",
      "pdf_url": "http://arxiv.org/pdf/2510.06056v1",
      "published": "2025-10-07T15:49:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06056v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MixReasoning: Switching Modes to Think",
      "authors": [
        "Haiquan Lu",
        "Gongfan Fang",
        "Xinyin Ma",
        "Qi Li",
        "Xinchao Wang"
      ],
      "abstract": "Reasoning models enhance performance by tackling problems in a step-by-step\nmanner, decomposing them into sub-problems and exploring long chains of thought\nbefore producing an answer. However, applying extended reasoning to every step\nintroduces substantial redundancy, as sub-problems vary widely in difficulty\nand complexity: a small number of pivotal steps are genuinely challenging and\ndecisive for the final answer, while many others only involve straightforward\nrevisions or simple computations. Therefore, a natural idea is to endow\nreasoning models with the ability to adaptively respond to this variation,\nrather than treating all steps with the same level of elaboration. To this end,\nwe propose MixReasoning, a framework that dynamically adjusts the depth of\nreasoning within a single response. The resulting chain of thought then becomes\na mixture of detailed reasoning on difficult steps and concise inference on\nsimpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning\nshortens reasoning length and substantially improves efficiency without\ncompromising accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2510.06052v1",
      "published": "2025-10-07T15:46:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06052v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "GLVD: Guided Learned Vertex Descent",
      "authors": [
        "Pol Caselles Rico",
        "Francesc Moreno Noguer"
      ],
      "abstract": "Existing 3D face modeling methods usually depend on 3D Morphable Models,\nwhich inherently constrain the representation capacity to fixed shape priors.\nOptimization-based approaches offer high-quality reconstructions but tend to be\ncomputationally expensive. In this work, we introduce GLVD, a hybrid method for\n3D face reconstruction from few-shot images that extends Learned Vertex Descent\n(LVD) by integrating per-vertex neural field optimization with global\nstructural guidance from dynamically predicted 3D keypoints. By incorporating\nrelative spatial encoding, GLVD iteratively refines mesh vertices without\nrequiring dense 3D supervision. This enables expressive and adaptable geometry\nreconstruction while maintaining computational efficiency. GLVD achieves\nstate-of-the-art performance in single-view settings and remains highly\ncompetitive in multi-view scenarios, all while substantially reducing inference\ntime.",
      "pdf_url": "http://arxiv.org/pdf/2510.06046v1",
      "published": "2025-10-07T15:40:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06046v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization",
      "authors": [
        "Xinye Cao",
        "Hongcan Guo",
        "Jiawen Qian",
        "Guoshun Nan",
        "Chao Wang",
        "Yuqi Pan",
        "Tianhao Hou",
        "Xiaojuan Wang",
        "Yutong Gao"
      ],
      "abstract": "Understanding hour-long videos with multi-modal large language models\n(MM-LLMs) enriches the landscape of human-centered AI applications. However,\nfor end-to-end video understanding with LLMs, uniformly sampling video frames\nresults in LLMs being overwhelmed by a vast amount of irrelevant information as\nvideo length increases. Existing hierarchical key frame extraction methods\nimprove the accuracy of video understanding but still face two critical\nchallenges. 1) How can the interference of extensive redundant information in\nlong videos be mitigated? 2) How can a model dynamically adapt to complex\nhierarchical structures while accurately identifying key frames? To address\nthese issues, we propose VideoMiner, which iteratively segments, captions, and\nclusters long videos, forming a hierarchical tree structure. The proposed\nVideoMiner progresses from long videos to events to frames while preserving\ntemporal coherence, effectively addressing the first challenge. To precisely\nlocate key frames, we introduce T-GRPO, a tree-based group relative policy\noptimization in reinforcement learning method that guides the exploration of\nthe VideoMiner. The proposed T-GRPO is specifically designed for tree\nstructures, integrating spatiotemporal information at the event level while\nbeing guided by the question, thus solving the second challenge. We achieve\nsuperior performance in all long-video understanding tasks and uncover several\ninteresting insights. Our proposed T-GRPO surprisingly incentivizes the model\nto spontaneously generate a reasoning chain. Additionally, the designed tree\ngrowth auxin dynamically adjusts the expansion depth, obtaining accuracy and\nefficiency gains. The code is publicly available at\nhttps://github.com/caoxinye/VideoMiner.",
      "pdf_url": "http://arxiv.org/pdf/2510.06040v1",
      "published": "2025-10-07T15:34:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06040v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs",
      "authors": [
        "Chengwei Wu",
        "Jiapu Wang",
        "Mingyang Gao",
        "Xingrui Zhuo",
        "Jipeng Guo",
        "Runlin Lei",
        "Haoran Luo",
        "Tianyu Chen",
        "Haoyi Zhou",
        "Shirui Pan",
        "Zechao Li"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language processing tasks. However, Chinese LLMs face unique\nchallenges, primarily due to the dominance of unstructured free text and the\nlack of structured representations in Chinese corpora. While existing\nbenchmarks for LLMs partially assess Chinese LLMs, they are still predominantly\nEnglish-centric and fail to address the unique linguistic characteristics of\nChinese, lacking structured datasets essential for robust evaluation. To\naddress these challenges, we present a Comprehensive Benchmark for Evaluating\nChinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese\nData-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million\naligned text pairs, each consisting of unstructured text coupled with one or\nmore corresponding triples, alongside a total of 15 million triples spanning\nfour critical domains. The core contributions of CDTP are threefold: (i)\nenriching Chinese corpora with high-quality structured information; (ii)\nenabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)\nsupporting multi-task fine-tuning to assess generalization and robustness\nacross scenarios, including Knowledge Graph Completion, Triple-to-Text\ngeneration, and Question Answering. Furthermore, we conduct rigorous\nevaluations through extensive experiments and ablation studies to assess the\neffectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.\nTo support reproducible research, we offer an open-source codebase and outline\npotential directions for future investigations based on our insights.",
      "pdf_url": "http://arxiv.org/pdf/2510.06039v1",
      "published": "2025-10-07T15:33:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06039v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning",
      "authors": [
        "Li Zeqiao",
        "Wang Yijing",
        "Wang Haoyu",
        "Li Zheng",
        "Li Peng",
        "Liu Wenfei",
        "Zuo Zhiqiang"
      ],
      "abstract": "Autonomous driving with reinforcement learning (RL) has significant\npotential. However, applying RL in real-world settings remains challenging due\nto the need for safe, efficient, and robust learning. Incorporating human\nexpertise into the learning process can help overcome these challenges by\nreducing risky exploration and improving sample efficiency. In this work, we\npropose a reward-free, active human-in-the-loop learning method called\nHuman-Guided Distributional Soft Actor-Critic (H-DSAC). Our method combines\nProxy Value Propagation (PVP) and Distributional Soft Actor-Critic (DSAC) to\nenable efficient and safe training in real-world environments. The key\ninnovation is the construction of a distributed proxy value function within the\nDSAC framework. This function encodes human intent by assigning higher expected\nreturns to expert demonstrations and penalizing actions that require human\nintervention. By extrapolating these labels to unlabeled states, the policy is\neffectively guided toward expert-like behavior. With a well-designed state\nspace, our method achieves real-world driving policy learning within practical\ntraining times. Results from both simulation and real-world experiments\ndemonstrate that our framework enables safe, robust, and sample-efficient\nlearning for autonomous driving.",
      "pdf_url": "http://arxiv.org/pdf/2510.06038v1",
      "published": "2025-10-07T15:33:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06038v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?",
      "authors": [
        "Qingyu Yin",
        "Chak Tou Leong",
        "Linyi Yang",
        "Wenxuan Huang",
        "Wenjie Li",
        "Xiting Wang",
        "Jaehong Yoon",
        "YunXing",
        "XingYu",
        "Jinjin Gu"
      ],
      "abstract": "Large reasoning models (LRMs) with multi-step reasoning capabilities have\nshown remarkable problem-solving abilities, yet they exhibit concerning safety\nvulnerabilities that remain poorly understood. In this work, we investigate why\nsafety alignment fails in reasoning models through a mechanistic\ninterpretability lens. Using a linear probing approach to trace refusal\nintentions across token positions, we discover a striking phenomenon termed as\n\\textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify\nharmful prompts and maintain strong refusal intentions during their thinking\nprocess, but experience a sharp drop in refusal scores at the final tokens\nbefore output generation. This suggests that these models are not inherently\nunsafe; rather, their refusal intentions are systematically suppressed. Through\ncausal intervention analysis, we identify a sparse set of attention heads that\nnegatively contribute to refusal behavior. Ablating just 3\\% of these heads can\nreduce attack success rates below 10\\%. Building on these mechanistic insights,\nwe propose \\textbf{Cliff-as-a-Judge}, a novel data selection method that\nidentifies training examples exhibiting the largest refusal cliff to\nefficiently repair reasoning models' safety alignment. This approach achieves\ncomparable safety improvements using only 1.7\\% of the vanilla safety training\ndata, demonstrating a less-is-more effect in safety alignment.",
      "pdf_url": "http://arxiv.org/pdf/2510.06036v1",
      "published": "2025-10-07T15:32:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06036v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Fast Leave-One-Out Approximation from Fragment-Target Prevalence Vectors (molFTP) : From Dummy Masking to Key-LOO for Leakage-Free Feature Construction",
      "authors": [
        "Guillaume Godin"
      ],
      "abstract": "We introduce molFTP (molecular fragment-target prevalence), a compact\nrepresentation that delivers strong predictive performance. To prevent feature\nleakage across cross-validation folds, we implement a dummy-masking procedure\nthat removes information about fragments present in the held-out molecules. We\nfurther show that key leave-one-out (key-loo) closely approximates true\nmolecule-level leave-one-out (LOO), with deviation below 8% on our datasets.\nThis enables near full data training while preserving unbiased cross-validation\nestimates of model performance. Overall, molFTP provides a fast,\nleakage-resistant fragment-target prevalence vectorization with practical\nsafeguards (dummy masking or key-LOO) that approximate LOO at a fraction of its\ncost.",
      "pdf_url": "http://arxiv.org/pdf/2510.06029v1",
      "published": "2025-10-07T15:27:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06029v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context",
      "authors": [
        "An Thi Nguyen",
        "Radina Stoykova",
        "Eric Arazo"
      ],
      "abstract": "Generic instance search models can dramatically reduce the manual effort\nrequired to analyze vast surveillance footage during criminal investigations by\nretrieving specific objects of interest to law enforcement. However, our\nresearch reveals an unintended emergent capability: through overlearning, these\nmodels can single out specific individuals even when trained on datasets\nwithout human subjects. This capability raises concerns regarding\nidentification and profiling of individuals based on their personal data, while\nthere is currently no clear standard on how de-identification can be achieved.\nWe evaluate two technical safeguards to curtail a model's person\nre-identification capacity: index exclusion and confusion loss. Our experiments\ndemonstrate that combining these approaches can reduce person re-identification\naccuracy to below 2% while maintaining 82% of retrieval performance for\nnon-person objects. However, we identify critical vulnerabilities in these\nmitigations, including potential circumvention using partial person images.\nThese findings highlight urgent regulatory questions at the intersection of AI\ngovernance and data protection: How should we classify and regulate systems\nwith emergent identification capabilities? And what technical standards should\nbe required to prevent identification capabilities from developing in seemingly\nbenign applications?",
      "pdf_url": "http://arxiv.org/pdf/2510.06026v1",
      "published": "2025-10-07T15:23:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06026v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models",
      "authors": [
        "Zhangyue Yin",
        "Qiushi Sun",
        "Zhiyuan Zeng",
        "Zhiyuan Yu",
        "Qipeng Guo",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "abstract": "Test-time scaling has emerged as a transformative paradigm for enhancing the\nperformance of large reasoning models, enabling dynamic allocation of\ncomputational resources during inference. However, as the landscape of\nreasoning models rapidly expands, a critical question remains: how can we\nsystematically compare and evaluate the test-time scaling capabilities across\ndifferent models? In this paper, we introduce ARISE (Adaptive Resolution-aware\nScaling Evaluation), a novel metric specifically designed to assess the\ntest-time scaling effectiveness of large reasoning models. Unlike existing\nevaluation approaches, ARISE incorporates two key innovations: (1) sample-level\nawareness that effectively penalizes negative scaling behaviors where increased\ncomputation leads to performance degradation, and (2) a dynamic sampling\nmechanism that mitigates the impact of accuracy fluctuations and token count\ninstability on the final assessment. We conduct comprehensive experiments\nevaluating state-of-the-art reasoning models across diverse domains including\nmathematical reasoning, code generation, and agentic tasks. Our results\ndemonstrate that ARISE provides a reliable and fine-grained measurement of\ntest-time scaling capabilities, revealing significant variations in scaling\nefficiency across models. Notably, our evaluation identifies Claude Opus as\nexhibiting superior scaling characteristics compared to other contemporary\nreasoning models.",
      "pdf_url": "http://arxiv.org/pdf/2510.06014v1",
      "published": "2025-10-07T15:10:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06014v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Hybrid Quantum-Classical Policy Gradient for Adaptive Control of Cyber-Physical Systems: A Comparative Study of VQC vs. MLP",
      "authors": [
        "Aueaphum Aueawatthanaphisut",
        "Nyi Wunna Tun"
      ],
      "abstract": "The comparative evaluation between classical and quantum reinforcement\nlearning (QRL) paradigms was conducted to investigate their convergence\nbehavior, robustness under observational noise, and computational efficiency in\na benchmark control environment. The study employed a multilayer perceptron\n(MLP) agent as a classical baseline and a parameterized variational quantum\ncircuit (VQC) as a quantum counterpart, both trained on the CartPole-v1\nenvironment over 500 episodes. Empirical results demonstrated that the\nclassical MLP achieved near-optimal policy convergence with a mean return of\n498.7 +/- 3.2, maintaining stable equilibrium throughout training. In contrast,\nthe VQC exhibited limited learning capability, with an average return of 14.6\n+/- 4.8, primarily constrained by circuit depth and qubit connectivity. Noise\nrobustness analysis further revealed that the MLP policy deteriorated\ngracefully under Gaussian perturbations, while the VQC displayed higher\nsensitivity at equivalent noise levels. Despite the lower asymptotic\nperformance, the VQC exhibited significantly lower parameter count and\nmarginally increased training time, highlighting its potential scalability for\nlow-resource quantum processors. The results suggest that while classical\nneural policies remain dominant in current control benchmarks, quantum-enhanced\narchitectures could offer promising efficiency advantages once hardware noise\nand expressivity limitations are mitigated.",
      "pdf_url": "http://arxiv.org/pdf/2510.06010v1",
      "published": "2025-10-07T15:09:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06010v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Detection and Measurement of Hailstones with Multimodal Large Language Models",
      "authors": [
        "Moritz Alker",
        "David C. Schedl",
        "Andreas Stckl"
      ],
      "abstract": "This study examines the use of social media and news images to detect and\nmeasure hailstones, utilizing pre-trained multimodal large language models. The\ndataset for this study comprises 474 crowdsourced images of hailstones from\ndocumented hail events in Austria, which occurred between January 2022 and\nSeptember 2024. These hailstones have maximum diameters ranging from 2 to 11cm.\nWe estimate the hail diameters and compare four different models utilizing\none-stage and two-stage prompting strategies. The latter utilizes additional\nsize cues from reference objects, such as human hands, within the image. Our\nresults show that pretrained models already have the potential to measure\nhailstone diameters from images with an average mean absolute error of 1.12cm\nfor the best model. In comparison to a single-stage prompt, two-stage prompting\nimproves the reliability of most models. Our study suggests that these\noff-the-shelf models, even without fine-tuning, can complement traditional hail\nsensors by extracting meaningful and spatially dense information from social\nmedia imagery, enabling faster and more detailed assessments of severe weather\nevents. The automated real-time image harvesting from social media and other\nsources remains an open task, but it will make our approach directly applicable\nto future hail events.",
      "pdf_url": "http://arxiv.org/pdf/2510.06008v1",
      "published": "2025-10-07T15:07:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06008v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07, 68T45, 86A10",
        "I.4; I.2"
      ]
    },
    {
      "title": "Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG",
      "authors": [
        "Hudson de Martim"
      ],
      "abstract": "The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core\nlimitations of standard Retrieval-Augmented Generation in the legal domain by\nproviding a verifiable knowledge graph that models hierarchical structure,\ntemporal evolution, and causal events of legal norms. However, a critical gap\nremains: how to reliably query this structured knowledge without sacrificing\nits deterministic properties. This paper introduces the SAT-Graph API, a formal\nquery execution layer centered on canonical actions-atomic, composable, and\nauditable primitives that isolate probabilistic discovery from deterministic\nretrieval. These actions enable: (i) high-precision hybrid search; (ii) robust\nreference resolution; (iii) point-in-time version retrieval; and (iv) auditable\ncausal tracing. We demonstrate how planner-guided agents can decompose complex\nqueries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer\narchitecture transforms retrieval from an opaque black box to a transparent,\nauditable process, directly addressing Explainable AI (XAI) requirements for\nhigh-stakes domains.",
      "pdf_url": "http://arxiv.org/pdf/2510.06002v1",
      "published": "2025-10-07T15:04:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.06002v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ]
    },
    {
      "title": "Information-Theoretic Policy Pre-Training with Empowerment",
      "authors": [
        "Moritz Schneider",
        "Robert Krug",
        "Narunas Vaskevicius",
        "Luigi Palmieri",
        "Michael Volpp",
        "Joschka Boedecker"
      ],
      "abstract": "Empowerment, an information-theoretic measure of an agent's potential\ninfluence on its environment, has emerged as a powerful intrinsic motivation\nand exploration framework for reinforcement learning (RL). Besides for\nunsupervised RL and skill learning algorithms, the specific use of empowerment\nas a pre-training signal has received limited attention in the literature. We\nshow that empowerment can be used as a pre-training signal for data-efficient\ndownstream task adaptation. For this we extend the traditional notion of\nempowerment by introducing discounted empowerment, which balances the agent's\ncontrol over the environment across short- and long-term horizons. Leveraging\nthis formulation, we propose a novel pre-training paradigm that initializes\npolicies to maximize discounted empowerment, enabling agents to acquire a\nrobust understanding of environmental dynamics. We analyze empowerment-based\npre-training for various existing RL algorithms and empirically demonstrate its\npotential as a general-purpose initialization strategy: empowerment-maximizing\npolicies with long horizons are data-efficient and effective, leading to\nimproved adaptability in downstream tasks. Our findings pave the way for future\nresearch to scale this framework to high-dimensional and complex tasks, further\nadvancing the field of RL.",
      "pdf_url": "http://arxiv.org/pdf/2510.05996v1",
      "published": "2025-10-07T14:57:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05996v1",
      "categories": [
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "cs.RO",
        "math.IT"
      ]
    },
    {
      "title": "ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning",
      "authors": [
        "Tao Zhu",
        "Yinfeng Yu",
        "Liejun Wang",
        "Fuchun Sun",
        "Wendong Zheng"
      ],
      "abstract": "Diffusion models have demonstrated remarkable performance in speech\nsynthesis, but typically require multi-step sampling, resulting in low\ninference efficiency. Recent studies address this issue by distilling diffusion\nmodels into consistency models, enabling efficient one-step generation.\nHowever, these approaches introduce additional training costs and rely heavily\non the performance of pre-trained teacher models. In this paper, we propose\nECTSpeech, a simple and effective one-step speech synthesis framework that, for\nthe first time, incorporates the Easy Consistency Tuning (ECT) strategy into\nspeech synthesis. By progressively tightening consistency constraints on a\npre-trained diffusion model, ECTSpeech achieves high-quality one-step\ngeneration while significantly reducing training complexity. In addition, we\ndesign a multi-scale gate module (MSGate) to enhance the denoiser's ability to\nfuse features at different scales. Experimental results on the LJSpeech dataset\ndemonstrate that ECTSpeech achieves audio quality comparable to\nstate-of-the-art methods under single-step sampling, while substantially\nreducing the model's training cost and complexity.",
      "pdf_url": "http://arxiv.org/pdf/2510.05984v1",
      "published": "2025-10-07T14:44:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05984v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis",
      "authors": [
        "Eashan Adhikarla",
        "Yixin Liu",
        "Brian D. Davison"
      ],
      "abstract": "Low-light image enhancement (LLIE) is vital for safety-critical applications\nsuch as surveillance, autonomous navigation, and medical imaging, where\nvisibility degradation can impair downstream task performance. Recently,\ndiffusion models have emerged as a promising generative paradigm for LLIE due\nto their capacity to model complex image distributions via iterative denoising.\nThis survey provides an up-to-date critical analysis of diffusion models for\nLLIE, distinctively featuring an in-depth comparative performance evaluation\nagainst Generative Adversarial Network and Transformer-based state-of-the-art\nmethods, a thorough examination of practical deployment challenges, and a\nforward-looking perspective on the role of emerging paradigms like foundation\nmodels. We propose a multi-perspective taxonomy encompassing six categories:\nIntrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,\nand Autonomous; that map enhancement methods across physical priors,\nconditioning schemes, and computational efficiency. Our taxonomy is grounded in\na hybrid view of both the model mechanism and the conditioning signals. We\nevaluate qualitative failure modes, benchmark inconsistencies, and trade-offs\nbetween interpretability, generalization, and inference efficiency. We also\ndiscuss real-world deployment constraints (e.g., memory, energy use) and\nethical considerations. This survey aims to guide the next generation of\ndiffusion-based LLIE research by highlighting trends and surfacing open\nresearch questions, including novel conditioning, real-time adaptation, and the\npotential of foundation models.",
      "pdf_url": "http://arxiv.org/pdf/2510.05976v1",
      "published": "2025-10-07T14:30:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05976v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language",
      "authors": [
        "Periklis Mantenoglou",
        "Rishi Hazra",
        "Pedro Zuidberg Dos Martires",
        "Luc De Raedt"
      ],
      "abstract": "Owing to their reasoning capabilities, large language models (LLMs) have been\nevaluated on planning tasks described in natural language. However, LLMs have\nlargely been tested on planning domains without constraints. In order to deploy\nthem in real-world settings where adherence to constraints, in particular\nsafety constraints, is critical, we need to evaluate their performance on\nconstrained planning tasks. We introduce LexiCon -- a natural language-based\n(Lexi) constrained (Con) planning benchmark, consisting of a suite of\nenvironments, that can be used to evaluate the planning capabilities of LLMs in\na principled fashion. The core idea behind LexiCon is to take existing planning\nenvironments and impose temporal constraints on the states. These constrained\nproblems are then translated into natural language and given to an LLM to\nsolve. A key feature of LexiCon is its extensibility. That is, the set of\nsupported environments can be extended with new (unconstrained) environment\ngenerators, for which temporal constraints are constructed automatically. This\nrenders LexiCon future-proof: the hardness of the generated planning problems\ncan be increased as the planning capabilities of LLMs improve. Our experiments\nreveal that the performance of state-of-the-art LLMs, including reasoning\nmodels like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of\nthe planning tasks increases.",
      "pdf_url": "http://arxiv.org/pdf/2510.05972v1",
      "published": "2025-10-07T14:28:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05972v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Probing the Difficulty Perception Mechanism of Large Language Models",
      "authors": [
        "Sunbowen Lee",
        "Qingyu Yin",
        "Chak Tou Leong",
        "Jialiang Zhang",
        "Yicheng Gong",
        "Xiaoyu Shen"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed on complex reasoning\ntasks, yet little is known about their ability to internally evaluate problem\ndifficulty, which is an essential capability for adaptive reasoning and\nefficient resource allocation. In this work, we investigate whether LLMs\nimplicitly encode problem difficulty in their internal representations. Using a\nlinear probe on the final-token representations of LLMs, we demonstrate that\nthe difficulty level of math problems can be linearly modeled. We further\nlocate the specific attention heads of the final Transformer layer: these\nattention heads have opposite activation patterns for simple and difficult\nproblems, thus achieving perception of difficulty. Our ablation experiments\nprove the accuracy of the location. Crucially, our experiments provide\npractical support for using LLMs as automatic difficulty annotators,\npotentially substantially reducing reliance on costly human labeling in\nbenchmark construction and curriculum learning. We also uncover that there is a\nsignificant difference in entropy and difficulty perception at the token level.\nOur study reveals that difficulty perception in LLMs is not only present but\nalso structurally organized, offering new theoretical insights and practical\ndirections for future research.",
      "pdf_url": "http://arxiv.org/pdf/2510.05969v1",
      "published": "2025-10-07T14:24:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05969v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization",
      "authors": [
        "Dayyn O'Brien",
        "Barry Haddow",
        "Emily Allaway",
        "Pinzhen Chen"
      ],
      "abstract": "Conducting contamination-free evaluation of mathematical capabilities can be\ndifficult for two reasons: models may memorize a test set once it is made\npublic, and current mathematical benchmarks are prone to overfitting due to\nhaving limited diversity of symbols and rules, coupled with closed-ended\nanswers. This paper proposes a method to leverage these shortcomings as useful\nfeatures to a construct dynamic, counterfactual benchmark, which can be used to\nboth reveal overfitting and measure true reasoning. We demonstrate this via\nMatheMagic, which generates math test instances with the interpretations of\nnumbers and operators altered, yet has automatically verifiable answers. Test\ninstances are randomly seeded and constructed at test time to evaluate a\nmodel's induction or deduction capability, offering stability, extensibility,\ncomparability, and robustness to overfitting. Our experiments find that models\nsolve deduction more easily than induction, but they revert to standard math.\nFurther analysis reveals that math-adapted models fail to exhibit a general\n\"skill\" of reasoning, and fine-tuning on induction tasks generalizes poorly.",
      "pdf_url": "http://arxiv.org/pdf/2510.05962v1",
      "published": "2025-10-07T14:19:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05962v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Training-Free Time Series Classification via In-Context Reasoning with LLM Agents",
      "authors": [
        "Songyuan Sui",
        "Zihang Xu",
        "Yu-Neng Chuang",
        "Kwei-Herng Lai",
        "Xia Hu"
      ],
      "abstract": "Time series classification (TSC) spans diverse application scenarios, yet\nlabeled data are often scarce, making task-specific training costly and\ninflexible. Recent reasoning-oriented large language models (LLMs) show promise\nin understanding temporal patterns, but purely zero-shot usage remains\nsuboptimal. We propose FETA, a multi-agent framework for training-free TSC via\nexemplar-based in-context reasoning. FETA decomposes a multivariate series into\nchannel-wise subproblems, retrieves a few structurally similar labeled examples\nfor each channel, and leverages a reasoning LLM to compare the query against\nthese exemplars, producing channel-level labels with self-assessed confidences;\na confidence-weighted aggregator then fuses all channel decisions. This design\neliminates the need for pretraining or fine-tuning, improves efficiency by\npruning irrelevant channels and controlling input length, and enhances\ninterpretability through exemplar grounding and confidence estimation. On nine\nchallenging UEA datasets, FETA achieves strong accuracy under a fully\ntraining-free setting, surpassing multiple trained baselines. These results\ndemonstrate that a multi-agent in-context reasoning framework can transform\nLLMs into competitive, plug-and-play TSC solvers without any parameter\ntraining. The code is available at https://github.com/SongyuanSui/FETATSC.",
      "pdf_url": "http://arxiv.org/pdf/2510.05950v1",
      "published": "2025-10-07T14:07:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.05950v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}