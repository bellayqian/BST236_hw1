{
  "last_updated": "2025-11-21T00:51:44.904451",
  "papers": [
    {
      "title": "In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data",
      "authors": [
        "Xiongyi Cai",
        "Ri-Zhao Qiu",
        "Geng Chen",
        "Lai Wei",
        "Isabella Liu",
        "Tianshu Huang",
        "Xuxin Cheng",
        "Xiaolong Wang"
      ],
      "abstract": "Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/",
      "pdf_url": "https://arxiv.org/pdf/2511.15704v1",
      "published": "2025-11-19T18:59:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15704v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC",
      "authors": [
        "Beichen Zhang",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Haodong Duan",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.",
      "pdf_url": "https://arxiv.org/pdf/2511.15703v1",
      "published": "2025-11-19T18:59:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15703v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Joint Semantic-Channel Coding and Modulation for Token Communications",
      "authors": [
        "Jingkai Ying",
        "Zhijin Qin",
        "Yulong Feng",
        "Liejun Wang",
        "Xiaoming Tao"
      ],
      "abstract": "In recent years, the Transformer architecture has achieved outstanding performance across a wide range of tasks and modalities. Token is the unified input and output representation in Transformer-based models, which has become a fundamental information unit. In this work, we consider the problem of token communication, studying how to transmit tokens efficiently and reliably. Point cloud, a prevailing three-dimensional format which exhibits a more complex spatial structure compared to image or video, is chosen to be the information source. We utilize the set abstraction method to obtain point tokens. Subsequently, to get a more informative and transmission-friendly representation based on tokens, we propose a joint semantic-channel and modulation (JSCCM) scheme for the token encoder, mapping point tokens to standard digital constellation points (modulated tokens). Specifically, the JSCCM consists of two parallel Point Transformer-based encoders and a differential modulator which combines the Gumel-softmax and soft quantization methods. Besides, the rate allocator and channel adapter are developed, facilitating adaptive generation of high-quality modulated tokens conditioned on both semantic information and channel conditions. Extensive simulations demonstrate that the proposed method outperforms both joint semantic-channel coding and traditional separate coding, achieving over 1dB gain in reconstruction and more than 6x compression ratio in modulated symbols.",
      "pdf_url": "https://arxiv.org/pdf/2511.15699v1",
      "published": "2025-11-19T18:56:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15699v1",
      "categories": [
        "eess.SP",
        "cs.AI"
      ]
    },
    {
      "title": "Walrus: A Cross-Domain Foundation Model for Continuum Dynamics",
      "authors": [
        "Michael McCabe",
        "Payel Mukhopadhyay",
        "Tanya Marwah",
        "Bruno Regaldo-Saint Blancard",
        "Francois Rozet",
        "Cristiana Diaconu",
        "Lucas Meyer",
        "Kaze W. K. Wong",
        "Hadi Sotoudeh",
        "Alberto Bietti",
        "Irina Espejo",
        "Rio Fear",
        "Siavash Golkar",
        "Tom Hehir",
        "Keiya Hirashima",
        "Geraud Krawezik",
        "Francois Lanusse",
        "Rudy Morel",
        "Ruben Ohana",
        "Liam Parker",
        "Mariel Pettee",
        "Jeff Shen",
        "Kyunghyun Cho",
        "Miles Cranmer",
        "Shirley Ho"
      ],
      "abstract": "Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.",
      "pdf_url": "https://arxiv.org/pdf/2511.15684v1",
      "published": "2025-11-19T18:36:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15684v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ]
    },
    {
      "title": "MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features",
      "authors": [
        "Sejuti Rahman",
        "Swakshar Deb",
        "MD. Sameer Iqbal Chowdhury",
        "MD. Jubair Ahmed Sourov",
        "Mohammad Shamsuddin"
      ],
      "abstract": "Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.",
      "pdf_url": "https://arxiv.org/pdf/2511.15675v1",
      "published": "2025-11-19T18:18:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15675v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
      "authors": [
        "Yicheng He",
        "Chengsong Huang",
        "Zongxia Li",
        "Jiaxin Huang",
        "Yonghui Yang"
      ],
      "abstract": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
      "pdf_url": "https://arxiv.org/pdf/2511.15661v1",
      "published": "2025-11-19T17:55:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15661v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI",
      "authors": [
        "Naomi Simumba",
        "Nils Lehmann",
        "Paolo Fraccaro",
        "Hamed Alemohammad",
        "Geeth De Mel",
        "Salman Khan",
        "Manil Maskey",
        "Nicolas Longepe",
        "Xiao Xiang Zhu",
        "Hannah Kerner",
        "Juan Bernabe-Moreno",
        "Alexander Lacoste"
      ],
      "abstract": "Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.\n  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.",
      "pdf_url": "https://arxiv.org/pdf/2511.15658v1",
      "published": "2025-11-19T17:45:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15658v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges",
      "authors": [
        "Kim N. Nolle",
        "Ivana Dusparic",
        "Rhodri Cusack",
        "Vinny Cahill"
      ],
      "abstract": "Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.\n  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.\n  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.",
      "pdf_url": "https://arxiv.org/pdf/2511.15652v1",
      "published": "2025-11-19T17:40:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15652v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Sufficient Explanations in Databases and their Connections to Necessary Explanations and Repairs",
      "authors": [
        "Leopoldo Bertossi",
        "Nina Pardal"
      ],
      "abstract": "The notion of cause, as formalized by Halpern and Pearl, has been recently applied to relational databases, to characterize and compute causal explanations for query answers. In this work we consider the alternative notion of sufficient explanation. We investigate its connections with database repairs as used for dealing with inconsistent databases, and with causality-based necessary explanations. We also obtain some computational results.",
      "pdf_url": "https://arxiv.org/pdf/2511.15623v1",
      "published": "2025-11-19T17:07:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15623v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification",
      "authors": [
        "Dante Francisco Wasmuht",
        "Otto Brookes",
        "Maximillian Schall",
        "Pablo Palencia",
        "Chris Beirne",
        "Tilo Burghardt",
        "Majid Mirmehdi",
        "Hjalmar Kühl",
        "Mimi Arandjelovic",
        "Sam Pottie",
        "Peter Bermant",
        "Brandon Asheim",
        "Yi Jin Toh",
        "Adam Elzinga",
        "Jason Holmberg",
        "Andrew Whitworth",
        "Eleanor Flatt",
        "Laura Gustafson",
        "Chaitanya Ryali",
        "Yuan-Ting Hu",
        "Baishan Guo",
        "Andrew Westbury",
        "Kate Saenko",
        "Didac Suris"
      ],
      "abstract": "Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at $\\href{https://www.conservationxlabs.com/sa-fari}{\\text{conservationxlabs.com/SA-FARI}}$.",
      "pdf_url": "https://arxiv.org/pdf/2511.15622v1",
      "published": "2025-11-19T17:07:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15622v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Optimus-Q: Utilizing Federated Learning in Adaptive Robots for Intelligent Nuclear Power Plant Operations through Quantum Cryptography",
      "authors": [
        "Sai Puppala",
        "Ismail Hossain",
        "Jahangir Alam",
        "Sajedul Talukder"
      ],
      "abstract": "The integration of advanced robotics in nuclear power plants (NPPs) presents a transformative opportunity to enhance safety, efficiency, and environmental monitoring in high-stakes environments. Our paper introduces the Optimus-Q robot, a sophisticated system designed to autonomously monitor air quality and detect contamination while leveraging adaptive learning techniques and secure quantum communication. Equipped with advanced infrared sensors, the Optimus-Q robot continuously streams real-time environmental data to predict hazardous gas emissions, including carbon dioxide (CO$_2$), carbon monoxide (CO), and methane (CH$_4$). Utilizing a federated learning approach, the robot collaborates with other systems across various NPPs to improve its predictive capabilities without compromising data privacy. Additionally, the implementation of Quantum Key Distribution (QKD) ensures secure data transmission, safeguarding sensitive operational information. Our methodology combines systematic navigation patterns with machine learning algorithms to facilitate efficient coverage of designated areas, thereby optimizing contamination monitoring processes. Through simulations and real-world experiments, we demonstrate the effectiveness of the Optimus-Q robot in enhancing operational safety and responsiveness in nuclear facilities. This research underscores the potential of integrating robotics, machine learning, and quantum technologies to revolutionize monitoring systems in hazardous environments.",
      "pdf_url": "https://arxiv.org/pdf/2511.15614v1",
      "published": "2025-11-19T17:01:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15614v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity",
      "authors": [
        "Alexis Audran-Reiss",
        "Jordi Armengol Estapé",
        "Karen Hambardzumyan",
        "Amar Budhiraja",
        "Martin Josifoski",
        "Edan Toledo",
        "Rishi Hazra",
        "Despoina Magka",
        "Michael Shvartsman",
        "Parth Pathak",
        "Justine T Kao",
        "Lucia Cipolina-Kun",
        "Bhavul Gauri",
        "Jean-Christophe Gagnon-Audet",
        "Emanuel Tewolde",
        "Jenny Zhang",
        "Taco Cohen",
        "Yossi Adi",
        "Tatiana Shavrina",
        "Yoram Bachrach"
      ],
      "abstract": "AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.",
      "pdf_url": "https://arxiv.org/pdf/2511.15593v1",
      "published": "2025-11-19T16:32:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15593v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking",
      "authors": [
        "Sifan Zhou",
        "Yichao Cao",
        "Jiahao Nie",
        "Yuqian Fu",
        "Ziyu Zhao",
        "Xiaobo Lu",
        "Shuo Wang"
      ],
      "abstract": "3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.",
      "pdf_url": "https://arxiv.org/pdf/2511.15580v1",
      "published": "2025-11-19T16:12:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15580v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning",
      "authors": [
        "Qihao Yang",
        "Xuelin Wang",
        "Jiale Chen",
        "Xuelian Dong",
        "Yuxin Hao",
        "Tianyong Hao"
      ],
      "abstract": "Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.",
      "pdf_url": "https://arxiv.org/pdf/2511.15574v1",
      "published": "2025-11-19T16:06:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15574v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index",
      "authors": [
        "Selim Furkan Tekin",
        "Rajesh Bordawekar"
      ],
      "abstract": "Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.",
      "pdf_url": "https://arxiv.org/pdf/2511.15557v1",
      "published": "2025-11-19T15:50:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15557v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.DS"
      ]
    },
    {
      "title": "Multimodal Evaluation of Russian-language Architectures",
      "authors": [
        "Artem Chervyakov",
        "Ulyana Isaeva",
        "Anton Emelyanov",
        "Artem Safin",
        "Maria Tikhonova",
        "Alexander Kharitonov",
        "Yulia Lyakh",
        "Petr Surovtsev",
        "Denis Shevelev Vildan Saburov",
        "Vasily Konovalov",
        "Elisei Rykov",
        "Ivan Sviridov",
        "Amina Miftakhova",
        "Ilseyar Alimova",
        "Alexander Panchenko",
        "Alexander Kapitanov",
        "Alena Fenogenova"
      ],
      "abstract": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
      "pdf_url": "https://arxiv.org/pdf/2511.15552v1",
      "published": "2025-11-19T15:43:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15552v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Exploring the use of AI authors and reviewers at Agents4Science",
      "authors": [
        "Federico Bianchi",
        "Owen Queen",
        "Nitya Thakkar",
        "Eric Sun",
        "James Zou"
      ],
      "abstract": "There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.",
      "pdf_url": "https://arxiv.org/pdf/2511.15534v1",
      "published": "2025-11-19T15:32:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15534v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies",
      "authors": [
        "Gabriel Lauzier",
        "Alexandre Girard",
        "François Ferland"
      ],
      "abstract": "Diffusion Policy has shown great performance in robotic manipulation tasks under stochastic perturbations, due to its ability to model multimodal action distributions. Nonetheless, its reliance on a computationally expensive reverse-time diffusion (denoising) process, for action inference, makes it challenging to use for real-time applications where quick decision-making is mandatory. This work studies the possibility of conducting the denoising process only partially before executing an action, allowing the plant to evolve according to its dynamics in parallel to the reverse-time diffusion dynamics ongoing on the computer. In a classical diffusion policy setting, the plant dynamics are usually slow and the two dynamical processes are uncoupled. Here, we investigate theoretical bounds on the stability of closed-loop systems using diffusion policies when the plant dynamics and the denoising dynamics are coupled. The contribution of this work gives a framework for faster imitation learning and a metric that yields if a controller will be stable based on the variance of the demonstrations.",
      "pdf_url": "https://arxiv.org/pdf/2511.15520v1",
      "published": "2025-11-19T15:13:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15520v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels",
      "authors": [
        "Maria Pilligua",
        "David Serrano-Lozano",
        "Pai Peng",
        "Ramon Baldrich",
        "Michael S. Brown",
        "Javier Vazquez-Corral"
      ],
      "abstract": "Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.",
      "pdf_url": "https://arxiv.org/pdf/2511.15496v1",
      "published": "2025-11-19T14:52:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15496v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection",
      "authors": [
        "Rashid Iqbal",
        "Saddam Hussain Khan"
      ],
      "abstract": "This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.",
      "pdf_url": "https://arxiv.org/pdf/2511.15476v1",
      "published": "2025-11-19T14:32:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15476v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Insights from the ICLR Peer Review and Rebuttal Process",
      "authors": [
        "Amir Hossein Kargaran",
        "Nafiseh Nikeghbal",
        "Jing Yang",
        "Nedjma Ousidhoum"
      ],
      "abstract": "Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.",
      "pdf_url": "https://arxiv.org/pdf/2511.15462v1",
      "published": "2025-11-19T14:21:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15462v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining",
      "authors": [
        "Qian'ang Mao",
        "Yuxuan Zhang",
        "Jiaman Chen",
        "Wenjun Zhou",
        "Jiaqi Yan"
      ],
      "abstract": "As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.",
      "pdf_url": "https://arxiv.org/pdf/2511.15456v1",
      "published": "2025-11-19T14:15:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15456v1",
      "categories": [
        "cs.AI",
        "q-fin.GN"
      ]
    },
    {
      "title": "TSFM in-context learning for time-series classification of bearing-health status",
      "authors": [
        "Michel Tokic",
        "Slobodan Djukanović",
        "Anja von Beuningen",
        "Cheng Feng"
      ],
      "abstract": "This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.15447v1",
      "published": "2025-11-19T14:01:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15447v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation",
      "authors": [
        "Linyin Luo",
        "Yujuan Ding",
        "Yunshan Ma",
        "Wenqi Fan",
        "Hanjiang Lai"
      ],
      "abstract": "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.",
      "pdf_url": "https://arxiv.org/pdf/2511.15435v1",
      "published": "2025-11-19T13:45:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15435v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs",
      "authors": [
        "Georg Goldenits",
        "Philip Koenig",
        "Sebastian Raubitzek",
        "Andreas Ekelhart"
      ],
      "abstract": "Phishing websites pose a major cybersecurity threat, exploiting unsuspecting users and causing significant financial and organisational harm. Traditional machine learning approaches for phishing detection often require extensive feature engineering, continuous retraining, and costly infrastructure maintenance. At the same time, proprietary large language models (LLMs) have demonstrated strong performance in phishing-related classification tasks, but their operational costs and reliance on external providers limit their practical adoption in many business environments. This paper investigates the feasibility of small language models (SLMs) for detecting phishing websites using only their raw HTML code. A key advantage of these models is that they can be deployed on local infrastructure, providing organisations with greater control over data and operations. We systematically evaluate 15 commonly used Small Language Models (SLMs), ranging from 1 billion to 70 billion parameters, benchmarking their classification accuracy, computational requirements, and cost-efficiency. Our results highlight the trade-offs between detection performance and resource consumption, demonstrating that while SLMs underperform compared to state-of-the-art proprietary LLMs, they can still provide a viable and scalable alternative to external LLM services. By presenting a comparative analysis of costs and benefits, this work lays the foundation for future research on the adaptation, fine-tuning, and deployment of SLMs in phishing detection systems, aiming to balance security effectiveness and economic practicality.",
      "pdf_url": "https://arxiv.org/pdf/2511.15434v1",
      "published": "2025-11-19T13:45:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15434v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Understanding Layer Contributions in Tabular In-Context Learning Models",
      "authors": [
        "Amir Rezaei Balef",
        "Mykhailo Koshil",
        "Katharina Eggensperger"
      ],
      "abstract": "Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the \"layers as painters\" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.",
      "pdf_url": "https://arxiv.org/pdf/2511.15432v1",
      "published": "2025-11-19T13:39:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15432v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Building Robust and Scalable Multilingual ASR for Indian Languages",
      "authors": [
        "Arjun Gangwar",
        "Kaousheik Jayakumar",
        "S. Umesh"
      ],
      "abstract": "This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).",
      "pdf_url": "https://arxiv.org/pdf/2511.15418v1",
      "published": "2025-11-19T13:17:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15418v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer",
      "authors": [
        "Mingyang Feng",
        "Shaoyuan Li",
        "Xiang Yin"
      ],
      "abstract": "We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \\emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.",
      "pdf_url": "https://arxiv.org/pdf/2511.15414v1",
      "published": "2025-11-19T13:14:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15414v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework",
      "authors": [
        "Shanlin Zhou",
        "Xinpeng Wang",
        "Jianxun Lian",
        "Zhenghao Liu",
        "Laks V. S. Lakshmanan",
        "Xiaoyuan Yi",
        "Yongtao Hao"
      ],
      "abstract": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",
      "pdf_url": "https://arxiv.org/pdf/2511.15408v1",
      "published": "2025-11-19T13:05:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15408v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.MA",
        "cs.NE"
      ]
    },
    {
      "title": "IPR-1: Interactive Physical Reasoner",
      "authors": [
        "Mingyu Zhang",
        "Lifeng Zhuo",
        "Tianxi Tan",
        "Guocan Xie",
        "Xian Nie",
        "Yan Li",
        "Renjie Zhao",
        "Zizhu He",
        "Ziyu Wang",
        "Jiting Cai",
        "Yong-Lu Li"
      ],
      "abstract": "Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2511.15407v1",
      "published": "2025-11-19T13:04:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15407v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents",
      "authors": [
        "Sirui Chen",
        "Mengshi Zhao",
        "Lei Xu",
        "Yuying Zhao",
        "Beier Zhu",
        "Hanwang Zhang",
        "Shengjie Zhao",
        "Chaochao Lu"
      ],
      "abstract": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.",
      "pdf_url": "https://arxiv.org/pdf/2511.15392v1",
      "published": "2025-11-19T12:38:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15392v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Compliance-Preserving Retrieval System for Aircraft MRO Task Search",
      "authors": [
        "Byungho Jo"
      ],
      "abstract": "Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",
      "pdf_url": "https://arxiv.org/pdf/2511.15383v1",
      "published": "2025-11-19T12:25:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15383v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.IR"
      ]
    },
    {
      "title": "Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents",
      "authors": [
        "Trevor McInroe"
      ],
      "abstract": "We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.",
      "pdf_url": "https://arxiv.org/pdf/2511.15378v1",
      "published": "2025-11-19T12:10:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15378v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Parameter Importance-Driven Continual Learning for Foundation Models",
      "authors": [
        "Lingxiang Wang",
        "Hainan Zhang",
        "Zhiming Zheng"
      ],
      "abstract": "Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.",
      "pdf_url": "https://arxiv.org/pdf/2511.15375v1",
      "published": "2025-11-19T12:07:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15375v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The Empowerment of Science of Science by Large Language Models: New Tools and Methods",
      "authors": [
        "Guoqiang Liang",
        "Jingqian Gong",
        "Mengxuan Li",
        "Gege Lin",
        "Shuo Zhang"
      ],
      "abstract": "Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2511.15370v1",
      "published": "2025-11-19T11:57:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15370v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers",
      "authors": [
        "Gihwan Kim",
        "Jemin Lee",
        "Hyungshin Kim"
      ],
      "abstract": "Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\\%p (avg. 1.78\\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.",
      "pdf_url": "https://arxiv.org/pdf/2511.15369v1",
      "published": "2025-11-19T11:56:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15369v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration",
      "authors": [
        "Yifu Guo",
        "Zishan Xu",
        "Zhiyuan Yao",
        "Yuquan Lu",
        "Jiaye Lin",
        "Sen Hu",
        "Zhenheng Tang",
        "Yingchao Li",
        "Huacan Wang",
        "Ronghao Chen"
      ],
      "abstract": "Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2511.15351v1",
      "published": "2025-11-19T11:22:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15351v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Reflexive Evidence-Based Multimodal Learning for Clean Energy Transitions: Causal Insights on Cooking Fuel Access, Urbanization, and Carbon Emissions",
      "authors": [
        "Shan Shan"
      ],
      "abstract": "Achieving Sustainable Development Goal 7 (Affordable and Clean Energy) requires not only technological innovation but also a deeper understanding of the socioeconomic factors influencing energy access and carbon emissions. While these factors are gaining attention, critical questions remain, particularly regarding how to quantify their impacts on energy systems, model their cross-domain interactions, and capture feedback dynamics in the broader context of energy transitions. To address these gaps, this study introduces ClimateAgents, an AI-based framework that combines large language models with domain-specialized agents to support hypothesis generation and scenario exploration. Leveraging 20 years of socioeconomic and emissions data from 265 economies, countries and regions, and 98 indicators drawn from the World Bank database, the framework applies a machine learning based causal inference approach to identify key determinants of carbon emissions in an evidence-based, data driven manner. The analysis highlights three primary drivers: access to clean cooking fuels in rural areas, access to clean cooking fuels in urban areas, and the percentage of population living in urban areas. These findings underscore the critical role of clean cooking technologies and urbanization patterns in shaping emission outcomes. In line with growing calls for evidence-based AI policy, ClimateAgents offers a modular and reflexive learning system that supports the generation of credible and actionable insights for policy. By integrating heterogeneous data modalities, including structured indicators, policy documents, and semantic reasoning, the framework contributes to adaptive policymaking infrastructures that can evolve with complex socio-technical challenges. This approach aims to support a shift from siloed modeling to reflexive, modular systems designed for dynamic, context-aware climate action.",
      "pdf_url": "https://arxiv.org/pdf/2511.15342v1",
      "published": "2025-11-19T11:02:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15342v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection",
      "authors": [
        "Kadir-Kaan Özer",
        "René Ebeling",
        "Markus Enzweiler"
      ],
      "abstract": "Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.\n  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.\n  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.",
      "pdf_url": "https://arxiv.org/pdf/2511.15339v1",
      "published": "2025-11-19T10:58:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15339v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models",
      "authors": [
        "Piercosma Bisconti",
        "Matteo Prandi",
        "Federico Pierucci",
        "Francesco Giarrusso",
        "Marcantonio Bracale",
        "Marcello Galisai",
        "Vincenzo Suriani",
        "Olga Sorokoletova",
        "Federico Sartore",
        "Daniele Nardi"
      ],
      "abstract": "We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.",
      "pdf_url": "https://arxiv.org/pdf/2511.15304v1",
      "published": "2025-11-19T10:14:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15304v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Jonas De Maeyer",
        "Hossein Yarahmadi",
        "Moharram Challenger"
      ],
      "abstract": "Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.",
      "pdf_url": "https://arxiv.org/pdf/2511.15284v1",
      "published": "2025-11-19T09:48:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15284v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research",
      "authors": [
        "Ninell Oldenburg",
        "Ruchira Dhar",
        "Anders Søgaard"
      ],
      "abstract": "In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.",
      "pdf_url": "https://arxiv.org/pdf/2511.15282v1",
      "published": "2025-11-19T09:48:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15282v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Behavior Trees vs Executable Ontologies: a Comparative Analysis of Robot Control Paradigms",
      "authors": [
        "Alexander Boldachev"
      ],
      "abstract": "This paper compares two distinct approaches to modeling robotic behavior: imperative Behavior Trees (BTs) and declarative Executable Ontologies (EO), implemented through the boldsea framework. BTs structure behavior hierarchically using control-flow, whereas EO represents the domain as a temporal, event-based semantic graph driven by dataflow rules. We demonstrate that EO achieves comparable reactivity and modularity to BTs through a fundamentally different architecture: replacing polling-based tick execution with event-driven state propagation. We propose that EO offers an alternative framework, moving from procedural programming to semantic domain modeling, to address the semantic-process gap in traditional robotic control. EO supports runtime model modification, full temporal traceability, and a unified representation of data, logic, and interface - features that are difficult or sometimes impossible to achieve with BTs, although BTs excel in established, predictable scenarios. The comparison is grounded in a practical mobile manipulation task. This comparison highlights the respective operational strengths of each approach in dynamic, evolving robotic systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.15274v1",
      "published": "2025-11-19T09:38:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15274v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.FL"
      ]
    },
    {
      "title": "Efficiency Will Not Lead to Sustainable Reasoning AI",
      "authors": [
        "Philipp Wiesner",
        "Daniel W. O'Neill",
        "Francesca Larosa",
        "Odej Kao"
      ],
      "abstract": "AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.15259v1",
      "published": "2025-11-19T09:23:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15259v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "PresentCoach: Dual-Agent Presentation Coaching through Exemplars and Interactive Feedback",
      "authors": [
        "Sirui Chen",
        "Jinsong Zhou",
        "Xinli Xu",
        "Xiaoyu Yang",
        "Litao Guo",
        "Ying-Cong Chen"
      ],
      "abstract": "Effective presentation skills are essential in education, professional communication, and public speaking, yet learners often lack access to high-quality exemplars or personalized coaching. Existing AI tools typically provide isolated functionalities such as speech scoring or script generation without integrating reference modeling and interactive feedback into a cohesive learning experience. We introduce a dual-agent system that supports presentation practice through two complementary roles: the Ideal Presentation Agent and the Coach Agent. The Ideal Presentation Agent converts user-provided slides into model presentation videos by combining slide processing, visual-language analysis, narration script generation, personalized voice synthesis, and synchronized video assembly. The Coach Agent then evaluates user-recorded presentations against these exemplars, conducting multimodal speech analysis and delivering structured feedback in an Observation-Impact-Suggestion (OIS) format. To enhance the authenticity of the learning experience, the Coach Agent incorporates an Audience Agent, which simulates the perspective of a human listener and provides humanized feedback reflecting audience reactions and engagement. Together, these agents form a closed loop of observation, practice, and feedback. Implemented on a robust backend with multi-model integration, voice cloning, and error handling mechanisms, the system demonstrates how AI-driven agents can provide engaging, human-centered, and scalable support for presentation skill development in both educational and professional contexts.",
      "pdf_url": "https://arxiv.org/pdf/2511.15253v1",
      "published": "2025-11-19T09:15:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15253v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
      "authors": [
        "Kai Yang",
        "Xin Xu",
        "Yangkun Chen",
        "Weijie Liu",
        "Jiafei Lyu",
        "Zichuan Lin",
        "Deheng Ye",
        "Saiyong Yang"
      ],
      "abstract": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2511.15248v1",
      "published": "2025-11-19T09:06:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15248v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition",
      "authors": [
        "Xinli Tao",
        "Xin Dong",
        "Xuezhong Zhou"
      ],
      "abstract": "Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.",
      "pdf_url": "https://arxiv.org/pdf/2511.15211v1",
      "published": "2025-11-19T08:02:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15211v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
      "authors": [
        "Vladislav Pedashenko",
        "Laida Kushnareva",
        "Yana Khassan Nibal",
        "Eduard Tulchinskii",
        "Kristian Kuznetsov",
        "Vladislav Zharchinskii",
        "Yury Maximov",
        "Irina Piontkovskaya"
      ],
      "abstract": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",
      "pdf_url": "https://arxiv.org/pdf/2511.15210v1",
      "published": "2025-11-19T08:00:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15210v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Physics-Based Benchmarking Metrics for Multimodal Synthetic Images",
      "authors": [
        "Kishor Datta Gupta",
        "Marufa Kamal",
        "Md. Mahfuzur Rahman",
        "Fahad Rahman",
        "Mohd Ariful Haque",
        "Sunzida Siddique"
      ],
      "abstract": "Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.",
      "pdf_url": "https://arxiv.org/pdf/2511.15204v1",
      "published": "2025-11-19T07:52:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15204v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks",
      "authors": [
        "Zimo Ji",
        "Xunguang Wang",
        "Zongjie Li",
        "Pingchuan Ma",
        "Yudong Gao",
        "Daoyuan Wu",
        "Xincheng Yan",
        "Tian Tian",
        "Shuai Wang"
      ],
      "abstract": "Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.",
      "pdf_url": "https://arxiv.org/pdf/2511.15203v1",
      "published": "2025-11-19T07:47:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.15203v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    }
  ]
}