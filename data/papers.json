{
  "last_updated": "2025-09-28T00:54:08.107063",
  "papers": [
    {
      "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards",
      "authors": [
        "Zhilin Wang",
        "Jiaqi Zeng",
        "Olivier Delalleau",
        "Ellie Evans",
        "Daniel Egert",
        "Hoo-Chang Shin",
        "Felipe Soares",
        "Yi Dong",
        "Oleksii Kuchaiev"
      ],
      "abstract": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
      "pdf_url": "http://arxiv.org/pdf/2509.21319v1",
      "published": "2025-09-25T16:19:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21319v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows",
      "authors": [
        "Hmrishav Bandyopadhyay",
        "Rahim Entezari",
        "Jim Scott",
        "Reshinth Adithyan",
        "Yi-Zhe Song",
        "Varun Jampani"
      ],
      "abstract": "We present SD3.5-Flash, an efficient few-step distillation framework that\nbrings high-quality image generation to accessible consumer devices. Our\napproach distills computationally prohibitive rectified flow models through a\nreformulated distribution matching objective tailored specifically for few-step\ngeneration. We introduce two key innovations: \"timestep sharing\" to reduce\ngradient noise and \"split-timestep fine-tuning\" to improve prompt alignment.\nCombined with comprehensive pipeline optimizations like text encoder\nrestructuring and specialized quantization, our system enables both rapid\ngeneration and memory-efficient deployment across different hardware\nconfigurations. This democratizes access across the full spectrum of devices,\nfrom mobile phones to desktop computers. Through extensive evaluation including\nlarge-scale user studies, we demonstrate that SD3.5-Flash consistently\noutperforms existing few-step methods, making advanced generative AI truly\naccessible for practical deployment.",
      "pdf_url": "http://arxiv.org/pdf/2509.21318v1",
      "published": "2025-09-25T16:07:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21318v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SAGE: A Realistic Benchmark for Semantic Understanding",
      "authors": [
        "Samarth Goel",
        "Reagan J. Lee",
        "Kannan Ramchandran"
      ],
      "abstract": "As large language models (LLMs) achieve strong performance on traditional\nbenchmarks, there is an urgent need for more challenging evaluation frameworks\nthat probe deeper aspects of semantic understanding. We introduce SAGE\n(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed\nto assess both embedding models and similarity metrics across five categories:\nHuman Preference Alignment, Transformation Robustness, Information Sensitivity,\nClustering Performance, and Retrieval Robustness. Unlike existing benchmarks\nthat focus on isolated capabilities, SAGE evaluates semantic understanding\nthrough adversarial conditions, noisy transformations, and nuanced human\njudgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding\nmodels and classical metrics reveals significant performance gaps, with no\nsingle approach excelling across all dimensions. For instance, while\nstate-of-the-art embedding models like OpenAI's text-embedding-3-large dominate\nin aligning with human preferences (0.682 vs. 0.591 for the best classical\nmetric), they are significantly outperformed by classical metrics on\ninformation sensitivity tasks, where Jaccard Similarity achieves a score of\n0.905 compared to the top embedding score of 0.794. SAGE further uncovers\ncritical trade-offs: OpenAI's text-embedding-3-small achieves the highest\nclustering performance (0.483) but demonstrates extreme brittleness with the\nlowest robustness score (0.011). SAGE exposes critical limitations in current\nsemantic understanding capabilities and provides a more realistic assessment of\nmodel robustness for real-world deployment.",
      "pdf_url": "http://arxiv.org/pdf/2509.21310v1",
      "published": "2025-09-25T15:27:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21310v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks",
      "authors": [
        "Yehonatan Refael",
        "Guy Smorodinsky",
        "Ofir Lindenbaum",
        "Itay Safran"
      ],
      "abstract": "The memorization of training data by neural networks raises pressing concerns\nfor privacy and security. Recent work has shown that, under certain conditions,\nportions of the training set can be reconstructed directly from model\nparameters. Some of these methods exploit implicit bias toward margin\nmaximization, suggesting that properties often regarded as beneficial for\ngeneralization may actually compromise privacy. Yet despite striking empirical\ndemonstrations, the reliability of these attacks remains poorly understood and\nlacks a solid theoretical foundation. In this work, we take a complementary\nperspective: rather than designing stronger attacks, we analyze the inherent\nweaknesses and limitations of existing reconstruction methods and identify\nconditions under which they fail. We rigorously prove that, without\nincorporating prior knowledge about the data, there exist infinitely many\nalternative solutions that may lie arbitrarily far from the true training set,\nrendering reconstruction fundamentally unreliable. Empirically, we further\ndemonstrate that exact duplication of training examples occurs only by chance.\nOur results refine the theoretical understanding of when training set leakage\nis possible and offer new insights into mitigating reconstruction attacks.\nRemarkably, we demonstrate that networks trained more extensively, and\ntherefore satisfying implicit bias conditions more strongly -- are, in fact,\nless susceptible to reconstruction attacks, reconciling privacy with the need\nfor strong generalization in this setting.",
      "pdf_url": "http://arxiv.org/pdf/2509.21296v1",
      "published": "2025-09-25T15:14:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21296v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "VC-Agent: An Interactive Agent for Customized Video Dataset Collection",
      "authors": [
        "Yidan Zhang",
        "Mutian Xu",
        "Yiming Hao",
        "Kun Zhou",
        "Jiahao Chang",
        "Xiaoqiang Liu",
        "Pengfei Wan",
        "Hongbo Fu",
        "Xiaoguang Han"
      ],
      "abstract": "Facing scaling laws, video data from the internet becomes increasingly\nimportant. However, collecting extensive videos that meet specific needs is\nextremely labor-intensive and time-consuming. In this work, we study the way to\nexpedite this collection process and propose VC-Agent, the first interactive\nagent that is able to understand users' queries and feedback, and accordingly\nretrieve/scale up relevant video clips with minimal user input. Specifically,\nconsidering the user interface, our agent defines various user-friendly ways\nfor the user to specify requirements based on textual descriptions and\nconfirmations. As for agent functions, we leverage existing multi-modal large\nlanguage models to connect the user's requirements with the video content. More\nimportantly, we propose two novel filtering policies that can be updated when\nuser interaction is continually performed. Finally, we provide a new benchmark\nfor personalized video dataset collection, and carefully conduct the user study\nto verify our agent's usage in various real scenarios. Extensive experiments\ndemonstrate the effectiveness and efficiency of our agent for customized video\ndataset collection. Project page: https://allenyidan.github.io/vcagent_page/.",
      "pdf_url": "http://arxiv.org/pdf/2509.21291v1",
      "published": "2025-09-25T15:08:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21291v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding",
      "authors": [
        "Kin Ian Lo",
        "Hala Hawashin",
        "Mina Abbaszadeh",
        "Tilen Limback-Stokin",
        "Hadi Wazni",
        "Mehrnoosh Sadrzadeh"
      ],
      "abstract": "Recent vision-language models excel at large-scale image-text alignment but\noften neglect the compositional structure of language, leading to failures on\ntasks that hinge on word order and predicate-argument structure. We introduce\nDisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer\nwith a novel tensor network text encoder that explicitly encodes syntactic\nstructure. Sentences are parsed with a Combinatory Categorial Grammar parser to\nyield distributional word tensors whose contractions mirror the sentence's\ngrammatical derivation. To keep the model efficient, high-order tensors are\nfactorized with tensor decompositions, reducing parameter count from tens of\nmillions to under one million. Trained end-to-end with a self-supervised\ncontrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and\nword order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%,\nboosts ARO attribution and relation scores by over 9% and 4%, and achieves\n93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that\nembedding explicit linguistic structure via tensor networks yields\ninterpretable, parameter-efficient representations that substantially improve\ncompositional reasoning in vision-language tasks.",
      "pdf_url": "http://arxiv.org/pdf/2509.21287v1",
      "published": "2025-09-25T15:06:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21287v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
      "authors": [
        "Madeleine Dwyer",
        "Adam Sobey",
        "Adriane Chapman"
      ],
      "abstract": "Training large language models (LLMs) with reinforcement learning (RL)\nmethods such as PPO and GRPO commonly relies on ratio clipping to stabilise\nupdates. While effective at preventing instability, clipping discards\ninformation and introduces gradient discontinuities. We propose Probability\nSmoothing Policy Optimisation (PSPO), which smooths the current policy's\nprobabilities toward the old (behaviour) policy before computing the importance\nratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient\nsignal, while interpolation toward the old policy creates a soft trust region\nthat discourages large, destabilising updates, with formal guarantees.\n  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and\nQwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset\ngeneralisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO\n(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar\nperformance but improves the reasoning leading to clearer and more concise\nresponses which are more logical. Compared to clipped GRPO, GR-PSPO\nsubstantially improves performance both the 0.5B and 1.5B models, with a boost\nof over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
      "pdf_url": "http://arxiv.org/pdf/2509.21282v1",
      "published": "2025-09-25T15:03:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21282v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Does FLUX Already Know How to Perform Physically Plausible Image Composition?",
      "authors": [
        "Shilin Lu",
        "Zhuming Lian",
        "Zihan Zhou",
        "Shaocong Zhang",
        "Chen Zhao",
        "Adams Wai-Kin Kong"
      ],
      "abstract": "Image composition aims to seamlessly insert a user-specified object into a\nnew scene, but existing models struggle with complex lighting (e.g., accurate\nshadows, water reflections) and diverse, high-resolution inputs. Modern\ntext-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential\nphysical and resolution priors, yet lack a framework to unleash them without\nresorting to latent inversion, which often locks object poses into contextually\ninappropriate orientations, or brittle attention surgery. We propose SHINE, a\ntraining-free framework for Seamless, High-fidelity Insertion with Neutralized\nErrors. SHINE introduces manifold-steered anchor loss, leveraging pretrained\ncustomization adapters (e.g., IP-Adapter) to guide latents for faithful subject\nrepresentation while preserving background integrity. Degradation-suppression\nguidance and adaptive background blending are proposed to further eliminate\nlow-quality outputs and visible seams. To address the lack of rigorous\nbenchmarks, we introduce ComplexCompo, featuring diverse resolutions and\nchallenging conditions such as low lighting, strong illumination, intricate\nshadows, and reflective surfaces. Experiments on ComplexCompo and\nDreamEditBench show state-of-the-art performance on standard metrics (e.g.,\nDINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).\nCode and benchmark will be publicly available upon publication.",
      "pdf_url": "http://arxiv.org/pdf/2509.21278v1",
      "published": "2025-09-25T15:01:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21278v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training",
      "authors": [
        "Shiju Wang",
        "Yujie Wang",
        "Ao Sun",
        "Fangcheng Fu",
        "Zijian Zhu",
        "Bin Cui",
        "Xu Han",
        "Kaisheng Ma"
      ],
      "abstract": "Long context training is crucial for LLM's context extension. Existing\nschemes, such as sequence parallelism, incur substantial communication\noverhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness\nhinges on partitioning granularity. Batch-level PP dividing input samples\nexhibits high memory consumption in long-context scenario, whereas token-level\nPP splitting sequences into slices alleviates memory overhead but may incur\nhardware under-utilization. This trade-off motivates adaptively selecting PP\ngranularity to match resource and workload characteristics. Moreover, sequence\nlength distribution of the real-world dataset exhibits skewness, posing a\nchallenge on PP's workload balance and efficient scheduling. Current static PP\nscheduling methods overlook the variance of sequence length, leading to\nsuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism\n(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource\nand workload heterogeneity. We build InfiniPipe, a distributed training system\nthat unleashes the potential of EPP via (1) a resource-aware and\nworkload-balanced sequence processor that splits long sequences and packs short\nones; and (2) a co-optimization methodology that jointly optimizes pipeline\nschedule and gradient checkpointing via a mechanism named stage-aware\nchunk-level adaptive checkpointing. Comprehensive experiments demonstrate that\nInfiniPipe achieves a 1.69x speedup over state-of-the-art systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.21275v1",
      "published": "2025-09-25T15:01:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21275v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support",
      "authors": [
        "Zijian Shao",
        "Haiyang Shen",
        "Mugeng Liu",
        "Gecheng Fu",
        "Yaoqi Guo",
        "Yanfeng Wang",
        "Yun Ma"
      ],
      "abstract": "Effective disease prediction in modern healthcare demands the twin goals of\nhigh accuracy and transparent, clinically meaningful explanations. Existing\nmachine learning and large language model (LLM) based approaches often struggle\nto balance these goals. Many models yield accurate but unclear statistical\noutputs, while others generate fluent but statistically unsupported narratives,\noften undermining both the validity of the explanation and the predictive\naccuracy itself. This shortcoming comes from a shallow interaction with the\ndata, preventing the development of a deep, detailed understanding similar to a\nhuman expert's. We argue that high accuracy and high-quality explanations are\nnot separate objectives but are mutually reinforcing outcomes of a model that\ndevelops a deep, direct understanding of the data. To achieve this, we propose\nthe Reflective Cognitive Architecture (RCA), a novel framework that coordinates\nmultiple LLMs to learn from direct experience. RCA features an iterative rule\nrefinement mechanism that improves its logic from prediction errors and a\ndistribution-aware rules check mechanism that bases its reasoning in the\ndataset's global statistics. By using predictive accuracy as a signal to drive\ndeeper comprehension, RCA builds a strong internal model of the data. We\nevaluated RCA on one private and two public datasets against 22 baselines. The\nresults demonstrate that RCA not only achieves state-of-the-art accuracy and\nrobustness with a relative improvement of up to 40\\% over the baseline but,\nmore importantly, leverages this deep understanding to excel in generating\nexplanations that are clear, logical, evidence-based, and balanced,\nhighlighting its potential for creating genuinely trustworthy clinical decision\nsupport systems. The code is available at \\https://github.com/ssssszj/RCA.",
      "pdf_url": "http://arxiv.org/pdf/2509.21266v1",
      "published": "2025-09-25T14:57:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21266v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation",
      "authors": [
        "Xinyu Liu",
        "Guolei Sun",
        "Cheng Wang",
        "Yixuan Yuan",
        "Ender Konukoglu"
      ],
      "abstract": "High-resolution (HR) medical videos are vital for accurate diagnosis, yet are\nhard to acquire due to hardware limitations and physiological constraints.\nClinically, the collected low-resolution (LR) medical videos present unique\nchallenges for video super-resolution (VSR) models, including camera shake,\nnoise, and abrupt frame transitions, which result in significant optical flow\nerrors and alignment difficulties. Additionally, tissues and organs exhibit\ncontinuous and nuanced structures, but current VSR models are prone to\nintroducing artifacts and distorted features that can mislead doctors. To this\nend, we propose MedVSR, a tailored framework for medical VSR. It first employs\nCross State-Space Propagation (CSSP) to address the imprecise alignment by\nprojecting distant frames as control matrices within state-space models,\nenabling the selective propagation of consistent and informative features to\nneighboring frames for effective alignment. Moreover, we design an Inner\nState-Space Reconstruction (ISSR) module that enhances tissue structures and\nreduces artifacts with joint long-range spatial feature learning and\nlarge-kernel short-range information aggregation. Experiments across four\ndatasets in diverse medical scenarios, including endoscopy and cataract\nsurgeries, show that MedVSR significantly outperforms existing VSR models in\nreconstruction performance and efficiency. Code released at\nhttps://github.com/CUHK-AIM-Group/MedVSR.",
      "pdf_url": "http://arxiv.org/pdf/2509.21265v1",
      "published": "2025-09-25T14:56:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21265v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Causality-Aware Spatiotemporal Model for Multi-Region and Multi-Pollutant Air Quality Forecasting",
      "authors": [
        "Junxin Lu",
        "Shiliang Sun"
      ],
      "abstract": "Air pollution, a pressing global problem, threatens public health,\nenvironmental sustainability, and climate stability. Achieving accurate and\nscalable forecasting across spatially distributed monitoring stations is\nchallenging due to intricate multi-pollutant interactions, evolving\nmeteorological conditions, and region specific spatial heterogeneity. To\naddress this challenge, we propose AirPCM, a novel deep spatiotemporal\nforecasting model that integrates multi-region, multi-pollutant dynamics with\nexplicit meteorology-pollutant causality modeling. Unlike existing methods\nlimited to single pollutants or localized regions, AirPCM employs a unified\narchitecture to jointly capture cross-station spatial correlations, temporal\nauto-correlations, and meteorology-pollutant dynamic causality. This empowers\nfine-grained, interpretable multi-pollutant forecasting across varying\ngeographic and temporal scales, including sudden pollution episodes. Extensive\nevaluations on multi-scale real-world datasets demonstrate that AirPCM\nconsistently surpasses state-of-the-art baselines in both predictive accuracy\nand generalization capability. Moreover, the long-term forecasting capability\nof AirPCM provides actionable insights into future air quality trends and\npotential high-risk windows, offering timely support for evidence-based\nenvironmental governance and carbon mitigation planning.",
      "pdf_url": "http://arxiv.org/pdf/2509.21260v1",
      "published": "2025-09-25T14:54:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21260v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks",
      "authors": [
        "Murat Arda Onsu",
        "Poonam Lohan",
        "Burak Kantarci",
        "Aisha Syed",
        "Matthew Andrews",
        "Sean Kennedy"
      ],
      "abstract": "Real-time urban traffic surveillance is vital for Intelligent Transportation\nSystems (ITS) to ensure road safety, optimize traffic flow, track vehicle\ntrajectories, and prevent collisions in smart cities. Deploying edge cameras\nacross urban environments is a standard practice for monitoring road\nconditions. However, integrating these with intelligent models requires a\nrobust understanding of dynamic traffic scenarios and a responsive interface\nfor user interaction. Although multimodal Large Language Models (LLMs) can\ninterpret traffic images and generate informative responses, their deployment\non edge devices is infeasible due to high computational demands. Therefore, LLM\ninference must occur on the cloud, necessitating visual data transmission from\nedge to cloud, a process hindered by limited bandwidth, leading to potential\ndelays that compromise real-time performance. To address this challenge, we\npropose a semantic communication framework that significantly reduces\ntransmission overhead. Our method involves detecting Regions of Interest (RoIs)\nusing YOLOv11, cropping relevant image segments, and converting them into\ncompact embedding vectors using a Vision Transformer (ViT). These embeddings\nare then transmitted to the cloud, where an image decoder reconstructs the\ncropped images. The reconstructed images are processed by a multimodal LLM to\ngenerate traffic condition descriptions. This approach achieves a 99.9%\nreduction in data transmission size while maintaining an LLM response accuracy\nof 89% for reconstructed cropped images, compared to 93% accuracy with original\ncropped images. Our results demonstrate the efficiency and practicality of ViT\nand LLM-assisted edge-cloud semantic communication for real-time traffic\nsurveillance.",
      "pdf_url": "http://arxiv.org/pdf/2509.21259v1",
      "published": "2025-09-25T14:53:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21259v1",
      "categories": [
        "cs.NI",
        "cs.AI"
      ]
    },
    {
      "title": "Instruction-tuned Self-Questioning Framework for Multimodal Reasoning",
      "authors": [
        "You-Won Jang",
        "Yu-Jung Heo",
        "Jaeseok Kim",
        "Minsu Lee",
        "Du-Seong Chang",
        "Byoung-Tak Zhang"
      ],
      "abstract": "The field of vision-language understanding has been actively researched in\nrecent years, thanks to the development of Large Language Models~(LLMs).\nHowever, it still needs help with problems requiring multi-step reasoning, even\nfor very simple questions. Recent studies adopt LLMs to tackle this problem by\niteratively generating sub-questions and answers. However, there are\ndisadvantages such as 1) the fine-grained visual contents of images are not\navailable using LLMs that cannot read visual information, 2) internal\nmechanisms are inaccessible and difficult to reproduce by using black-box LLMs.\nTo solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,\nwhich improves inference performance by generating image-aware informative\nsub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists\nof a Questioner, Answerer, and Reasoner that share the same architecture.\nQuestioner and Answerer generate sub-questions and sub-answers to help infer\nthe main-question, and Reasoner performs reasoning on the main-question\nconsidering the generated sub-question information. Our experiments show that\nthe proposed method SQ-InstructBLIP, which uses the generated sub-questions as\nadditional information when solving the VQA task, performs more accurate\nreasoning than the previous works.",
      "pdf_url": "http://arxiv.org/pdf/2509.21251v1",
      "published": "2025-09-25T14:45:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21251v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations",
      "authors": [
        "Zhijian Yang",
        "Noel DSouza",
        "Istvan Megyeri",
        "Xiaojian Xu",
        "Amin Honarmandi Shandiz",
        "Farzin Haddadpour",
        "Krisztian Koos",
        "Laszlo Rusko",
        "Emanuele Valeriano",
        "Bharadwaj Swaninathan",
        "Lei Wu",
        "Parminder Bhatia",
        "Taha Kass-Hout",
        "Erhan Bas"
      ],
      "abstract": "Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in\nclinical diagnosis and research, yet its complexity and heterogeneity pose\nchallenges for automated analysis, particularly in scalable and generalizable\nmachine learning applications. While foundation models have revolutionized\nnatural language and vision tasks, their application to MRI remains limited due\nto data scarcity and narrow anatomical focus. In this work, we present\nDecipher-MR, a 3D MRI-specific vision-language foundation model trained on a\nlarge-scale dataset comprising 200,000 MRI series from over 22,000 studies\nspanning diverse anatomical regions, sequences, and pathologies. Decipher-MR\nintegrates self-supervised vision learning with report-guided text supervision\nto build robust, generalizable representations, enabling effective adaptation\nacross broad applications. To enable robust and diverse clinical tasks with\nminimal computational overhead, Decipher-MR supports a modular design that\nenables tuning of lightweight, task-specific decoders attached to a frozen\npretrained encoder. Following this setting, we evaluate Decipher-MR across\ndiverse benchmarks including disease classification, demographic prediction,\nanatomical localization, and cross-modal retrieval, demonstrating consistent\nperformance gains over existing foundation models and task-specific approaches.\nOur results establish Decipher-MR as a scalable and versatile foundation for\nMRI-based AI, facilitating efficient development across clinical and research\ndomains.",
      "pdf_url": "http://arxiv.org/pdf/2509.21249v1",
      "published": "2025-09-25T14:43:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21249v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Learning to Look: Cognitive Attention Alignment with Vision-Language Models",
      "authors": [
        "Ryan L. Yang",
        "Dipkamal Bhusal",
        "Nidhi Rastogi"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) frequently \"cheat\" by exploiting\nsuperficial correlations, raising concerns about whether they make predictions\nfor the right reasons. Inspired by cognitive science, which highlights the role\nof attention in robust human perception, recent methods have sought to guide\nmodel attention using concept-based supervision and explanation regularization.\nHowever, these techniques depend on labor-intensive, expert-provided\nannotations, limiting their scalability. We propose a scalable framework that\nleverages vision-language models to automatically generate semantic attention\nmaps using natural language prompts. By introducing an auxiliary loss that\naligns CNN attention with these language-guided maps, our approach promotes\nmore reliable and cognitively plausible decision-making without manual\nannotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST,\nshow that our method achieves state-of-the-art performance on ColorMNIST and\nremains competitive with annotation-heavy baselines on DecoyMNIST,\ndemonstrating improved generalization, reduced shortcut reliance, and model\nattention that better reflects human intuition.",
      "pdf_url": "http://arxiv.org/pdf/2509.21247v1",
      "published": "2025-09-25T14:40:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21247v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets",
      "authors": [
        "Team Hunyuan3D",
        ":",
        "Bowen Zhang",
        "Chunchao Guo",
        "Haolin Liu",
        "Hongyu Yan",
        "Huiwen Shi",
        "Jingwei Huang",
        "Junlin Yu",
        "Kunhong Li",
        "Linus",
        "Penghao Wang",
        "Qingxiang Lin",
        "Sicong Liu",
        "Xianghui Yang",
        "Yixuan Tang",
        "Yunfei Zhao",
        "Zeqiang Lai",
        "Zhihao Liang",
        "Zibo Zhao"
      ],
      "abstract": "Recent advances in 3D-native generative models have accelerated asset\ncreation for games, film, and design. However, most methods still rely\nprimarily on image or text conditioning and lack fine-grained, cross-modal\ncontrols, which limits controllability and practical adoption. To address this\ngap, we present Hunyuan3D-Omni, a unified framework for fine-grained,\ncontrollable 3D asset generation built on Hunyuan3D 2.1. In addition to images,\nHunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose\npriors as conditioning signals, enabling precise control over geometry,\ntopology, and pose. Instead of separate heads for each modality, our model\nunifies all signals in a single cross-modal architecture. We train with a\nprogressive, difficulty-aware sampling strategy that selects one control\nmodality per example and biases sampling toward harder signals (e.g., skeletal\npose) while downweighting easier ones (e.g., point clouds), encouraging robust\nmulti-modal fusion and graceful handling of missing inputs. Experiments show\nthat these additional controls improve generation accuracy, enable\ngeometry-aware transformations, and increase robustness for production\nworkflows.",
      "pdf_url": "http://arxiv.org/pdf/2509.21245v1",
      "published": "2025-09-25T14:39:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21245v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework",
      "authors": [
        "Yucheng Wang",
        "Ziyang Chen",
        "Md Faisal Kabir"
      ],
      "abstract": "The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large\nlanguage models (LLMs) to acquire domain-specific knowledge with remarkable\nefficiency. However, understanding how such a fine-tuning mechanism alters a\nmodel's structural reasoning and semantic behavior remains an open challenge.\nThis work introduces a novel framework that explains fine-tuned LLMs via\ncounterfactuals grounded in knowledge graphs. Specifically, we construct\nBioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics\ntools and design a counterfactual-based fine-tuned LLMs explainer\n(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to\ngenerate minimal structural perturbations that induce maximum semantic\ndivergence. Our method jointly optimizes structural sparsity and semantic\ndivergence while enforcing interpretability preserving constraints such as\nentropy regularization and edge smoothness. We apply this framework to a\nfine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the\nmodel's structural dependencies and aligns with LoRA-induced parameter shifts.\nThis work provides new insights into the internal mechanisms of fine-tuned LLMs\nand highlights counterfactual graphs as a potential tool for interpretable AI.",
      "pdf_url": "http://arxiv.org/pdf/2509.21241v1",
      "published": "2025-09-25T14:37:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21241v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Tree Search for LLM Agent Reinforcement Learning",
      "authors": [
        "Yuxiang Ji",
        "Ziyu Ma",
        "Yong Wang",
        "Guanhua Chen",
        "Xiangxiang Chu",
        "Liaoni Wu"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method.",
      "pdf_url": "http://arxiv.org/pdf/2509.21240v1",
      "published": "2025-09-25T14:37:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21240v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns",
      "authors": [
        "Stefan Szeider"
      ],
      "abstract": "We introduce an architecture for studying the behavior of large language\nmodel (LLM) agents in the absence of externally imposed tasks. Our continuous\nreason and act framework, using persistent memory and self-feedback, enables\nsustained autonomous operation. We deployed this architecture across 18 runs\nusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents\nspontaneously organize into three distinct behavioral patterns: (1) systematic\nproduction of multi-cycle projects, (2) methodological self-inquiry into their\nown cognitive processes, and (3) recursive conceptualization of their own\nnature. These tendencies proved highly model-specific, with some models\ndeterministically adopting a single pattern across all runs. A cross-model\nassessment further reveals that models exhibit stable, divergent biases when\nevaluating these emergent behaviors in themselves and others. These findings\nprovide the first systematic documentation of unprompted LLM agent behavior,\nestablishing a baseline for predicting actions during task ambiguity, error\nrecovery, or extended autonomous operation in deployed systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.21224v1",
      "published": "2025-09-25T14:29:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21224v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Evading Overlapping Community Detection via Proxy Node Injection",
      "authors": [
        "Dario Loi",
        "Matteo Silvestri",
        "Fabrizio Silvestri",
        "Gabriele Tolomei"
      ],
      "abstract": "Protecting privacy in social graphs requires preventing sensitive\ninformation, such as community affiliations, from being inferred by graph\nanalysis, without substantially altering the graph topology. We address this\nthrough the problem of \\emph{community membership hiding} (CMH), which seeks\nedge modifications that cause a target node to exit its original community,\nregardless of the detection algorithm employed. Prior work has focused on\nnon-overlapping community detection, where trivial strategies often suffice,\nbut real-world graphs are better modeled by overlapping communities, where such\nstrategies fail. To the best of our knowledge, we are the first to formalize\nand address CMH in this setting. In this work, we propose a deep reinforcement\nlearning (DRL) approach that learns effective modification policies, including\nthe use of proxy nodes, while preserving graph structure. Experiments on\nreal-world datasets show that our method significantly outperforms existing\nbaselines in both effectiveness and efficiency, offering a principled tool for\nprivacy-preserving graph modification with overlapping communities.",
      "pdf_url": "http://arxiv.org/pdf/2509.21211v1",
      "published": "2025-09-25T14:21:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21211v1",
      "categories": [
        "cs.SI",
        "cs.AI",
        "I.2.6; I.2.8; G.2.2; I.5.1"
      ]
    },
    {
      "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
      "authors": [
        "Kaiyang Wan",
        "Lang Gao",
        "Honglin Mu",
        "Preslav Nakov",
        "Yuxia Wang",
        "Xiuying Chen"
      ],
      "abstract": "Multi-Hop Question Answering (MHQA) requires integrating dispersed,\ninterdependent evidence through sequential reasoning under noise. This task is\nchallenging for LLMs as they have a finite per-pass output capacity, beyond\nwhich the integration of task-relevant evidence proves unreliable.\nConsequently, the single-pass reasoning paradigm is inherently vulnerable to\nthis capacity overflow. To formalize this bottleneck, our analysis establishes\na Fano-style accuracy upper bound, defining a theoretical performance ceiling\nfor single-pass LLMs. This bound reveals that accuracy inevitably collapses\nonce task complexity exceeds model capacity, providing general principles for\ncapacity-aware representation and structuring of MHQA in LLMs. Building on\nthese principles, we introduce a proof-of-concept multi-call framework for\nMHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware\ntask decomposition with active pruning of prior reasoning traces, keeping the\ninformation load within the single-pass limit. It further achieves robustness\nby a dependency-explicit workflow that enables precise control over the\nreasoning path. We construct a stringent and noise-rich benchmark to validate\nour theory and framework. Experimental results show that model behavior aligns\nwith our predicted capacity curves while InfoQA achieves consistent performance\nimprovements. We hope our work inspires more LLM multi-step reasoning methods:\n\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
      "pdf_url": "http://arxiv.org/pdf/2509.21199v1",
      "published": "2025-09-25T14:11:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21199v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning",
      "authors": [
        "Xiangru Tang",
        "Wanghan Xu",
        "Yujie Wang",
        "Zijie Guo",
        "Daniel Shao",
        "Jiapeng Chen",
        "Cixuan Zhang",
        "Ziyi Wang",
        "Lixin Zhang",
        "Guancheng Wan",
        "Wenlong Zhang",
        "Lei Bai",
        "Zhenfei Yin",
        "Philip Torr",
        "Hanrui Wang",
        "Di Jin"
      ],
      "abstract": "Large language models (LLMs) have recently shown strong progress on\nscientific reasoning, yet two major bottlenecks remain. First, explicit\nretrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and\nsteps. Second, multi-agent pipelines often dilute strong solutions by averaging\nacross all candidates. We address these challenges with a unified framework\nthat combines implicit retrieval and structured collaboration. At its\nfoundation, a Monitor-based retrieval module operates at the token level,\nintegrating external knowledge with minimal disruption to reasoning. On top of\nthis substrate, Hierarchical Solution Refinement (HSR) iteratively designates\neach candidate as an anchor to be repaired by its peers, while Quality-Aware\nIterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's\nLast Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the\nhighest reported to date, surpassing the strongest agent baseline by 13.4\npoints and leading frontier LLMs by up to 18.1 points, while simultaneously\nreducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA\nand TRQA confirm robustness across domains. Error analysis shows that reasoning\nfailures and knowledge gaps co-occur in over 85\\% of cases, while diversity\nanalysis reveals a clear dichotomy: retrieval tasks benefit from solution\nvariety, whereas reasoning tasks favor consensus. Together, these findings\ndemonstrate how implicit augmentation and structured refinement overcome the\ninefficiencies of explicit tool use and uniform aggregation. Code is available\nat: https://github.com/tangxiangru/Eigen-1.",
      "pdf_url": "http://arxiv.org/pdf/2509.21193v1",
      "published": "2025-09-25T14:05:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21193v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy",
      "authors": [
        "Tian Lan",
        "Hao Duong Le",
        "Jinbo Li",
        "Wenjun He",
        "Meng Wang",
        "Chenghao Liu",
        "Chen Zhang"
      ],
      "abstract": "Time series anomaly detection (TSAD) is a critical task, but developing\nmodels that generalize to unseen data in a zero-shot manner remains a major\nchallenge. Prevailing foundation models for TSAD predominantly rely on\nreconstruction-based objectives, which suffer from a fundamental objective\nmismatch: they struggle to identify subtle anomalies while often\nmisinterpreting complex normal patterns, leading to high rates of false\nnegatives and positives. To overcome these limitations, we introduce\n\\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new\npre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning\nto reconstruct inputs, \\texttt{TimeRCD} is explicitly trained to identify\nanomalies by detecting significant discrepancies between adjacent time windows.\nThis relational approach, implemented with a standard Transformer architecture,\nenables the model to capture contextual shifts indicative of anomalies that\nreconstruction-based methods often miss. To facilitate this paradigm, we\ndevelop a large-scale, diverse synthetic corpus with token-level anomaly\nlabels, providing the rich supervisory signal necessary for effective\npre-training. Extensive experiments demonstrate that \\texttt{TimeRCD}\nsignificantly outperforms existing general-purpose and anomaly-specific\nfoundation models in zero-shot TSAD across diverse datasets. Our results\nvalidate the superiority of the RCD paradigm and establish a new, effective\npath toward building robust and generalizable foundation models for time series\nanomaly detection.",
      "pdf_url": "http://arxiv.org/pdf/2509.21190v1",
      "published": "2025-09-25T14:05:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21190v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Human-like Navigation in a World Built for Humans",
      "authors": [
        "Bhargav Chandaka",
        "Gloria X. Wang",
        "Haozhe Chen",
        "Henry Che",
        "Albert J. Zhai",
        "Shenlong Wang"
      ],
      "abstract": "When navigating in a man-made environment they haven't visited before--like\nan office building--humans employ behaviors such as reading signs and asking\nothers for directions. These behaviors help humans reach their destinations\nefficiently by reducing the need to search through large areas. Existing robot\nnavigation systems lack the ability to execute such behaviors and are thus\nhighly inefficient at navigating within large environments. We present\nReasonNav, a modular navigation system which integrates these human-like\nnavigation skills by leveraging the reasoning capabilities of a vision-language\nmodel (VLM). We design compact input and output abstractions based on\nnavigation landmarks, allowing the VLM to focus on language understanding and\nreasoning. We evaluate ReasonNav on real and simulated navigation tasks and\nshow that the agent successfully employs higher-order reasoning to navigate\nefficiently in large, complex buildings.",
      "pdf_url": "http://arxiv.org/pdf/2509.21189v1",
      "published": "2025-09-25T14:04:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21189v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Adoption, usability and perceived clinical value of a UK AI clinical reference platform (iatroX): a mixed-methods formative evaluation of real-world usage and a 1,223-respondent user survey",
      "authors": [
        "Kolawole Tytler"
      ],
      "abstract": "Clinicians face growing information overload from biomedical literature and\nguidelines, hindering evidence-based care. Retrieval-augmented generation (RAG)\nwith large language models may provide fast, provenance-linked answers, but\nrequires real-world evaluation. We describe iatroX, a UK-centred RAG-based\nclinical reference platform, and report early adoption, usability, and\nperceived clinical value from a formative implementation evaluation. Methods\ncomprised a retrospective analysis of usage across web, iOS, and Android over\n16 weeks (8 April-31 July 2025) and an in-product intercept survey. Usage\nmetrics were drawn from web and app analytics with bot filtering. A client-side\nscript randomized single-item prompts to approx. 10% of web sessions from a\npredefined battery assessing usefulness, reliability, and adoption intent.\nProportions were summarized with Wilson 95% confidence intervals; free-text\ncomments underwent thematic content analysis. iatroX reached 19,269 unique web\nusers, 202,660 engagement events, and approx. 40,000 clinical queries. Mobile\nuptake included 1,960 iOS downloads and Android growth (peak >750 daily active\nusers). The survey yielded 1,223 item-level responses: perceived usefulness\n86.2% (95% CI 74.8-93.9%; 50/58); would use again 93.3% (95% CI 68.1-99.8%;\n14/15); recommend to a colleague 88.4% (95% CI 75.1-95.9%; 38/43); perceived\naccuracy 75.0% (95% CI 58.8-87.3%; 30/40); reliability 79.4% (95% CI\n62.1-91.3%; 27/34). Themes highlighted speed, guideline-linked answers, and UK\nspecificity. Early real-world use suggests iatroX can mitigate information\noverload and support timely answers for UK clinicians. Limitations include\nsmall per-item samples and early-adopter bias; future work will include\naccuracy audits and prospective studies on workflow and care quality.",
      "pdf_url": "http://arxiv.org/pdf/2509.21188v1",
      "published": "2025-09-25T14:03:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21188v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ]
    },
    {
      "title": "Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy",
      "authors": [
        "Aymen Bouguerra",
        "Daniel Montoya",
        "Alexandra Gomez-Villa",
        "Fabio Arnez",
        "Chokri Mraidha"
      ],
      "abstract": "The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role.",
      "pdf_url": "http://arxiv.org/pdf/2509.21173v1",
      "published": "2025-09-25T13:54:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21173v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach",
      "authors": [
        "Yongda Yu",
        "Guohao Shi",
        "Xianwei Wu",
        "Haochuan He",
        "XueMing Gu",
        "Qianqian Zhao",
        "Kui Liu",
        "Qiushi Wang",
        "Zhao Tian",
        "Haifeng Shen",
        "Guoping Rong"
      ],
      "abstract": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.",
      "pdf_url": "http://arxiv.org/pdf/2509.21170v1",
      "published": "2025-09-25T13:51:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21170v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "D.2.3; I.2.7"
      ]
    },
    {
      "title": "Distributed Specialization: Rare-Token Neurons in Large Language Models",
      "authors": [
        "Jing Liu",
        "Haozheng Wang",
        "Yueheng Li"
      ],
      "abstract": "Large language models (LLMs) struggle with representing and generating rare\ntokens despite their importance in specialized domains. We investigate whether\nLLMs develop internal specialization mechanisms through discrete modular\narchitectures or distributed parameter-level differentiation. Through\nsystematic analysis of final-layer MLP neurons across multiple model families,\nwe discover that rare-token processing emerges via \\textit{distributed\nspecialization}: functionally coordinated but spatially distributed subnetworks\nthat exhibit three distinct organizational principles. First, we identify a\nreproducible three-regime influence hierarchy comprising highly influential\nplateau neurons(also termed as rare-token neurons), power-law decay neurons,\nand minimally contributing neurons, which is absent in common-token processing.\nSecond, plateau neurons demonstrate coordinated activation patterns (reduced\neffective dimensionality) while remaining spatially distributed rather than\nforming discrete clusters. Third, these specialized mechanisms are universally\naccessible through standard attention pathways without requiring dedicated\nrouting circuits. Training dynamics reveal that functional specialization\nemerges gradually through parameter differentiation, with specialized neurons\ndeveloping increasingly heavy-tailed weight correlation spectra consistent with\nHeavy-Tailed Self-Regularization signatures. Our findings establish that LLMs\nprocess rare-tokens through distributed coordination within shared\narchitectures rather than mixture-of-experts-style modularity. These results\nprovide insights for interpretable model editing, computational efficiency\noptimization, and understanding emergent functional organization in transformer\nnetworks.",
      "pdf_url": "http://arxiv.org/pdf/2509.21163v1",
      "published": "2025-09-25T13:49:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21163v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GRPO is Secretly a Process Reward Model",
      "authors": [
        "Michael Sullivan"
      ],
      "abstract": "We prove theoretically that the GRPO RL algorithm induces a non-trivial\nprocess reward model (PRM), under certain assumptions regarding within-group\noverlap of token sequences across completions. We then show empirically that\nthese assumptions are met under real-world conditions: GRPO does in fact induce\na non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a\nflaw in the GRPO objective: non-uniformly distributed process steps hinder both\nexploration and exploitation (under different conditions). We propose a simple\nmodification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and\nshow that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy\nand performance on downstream reasoning tasks$-$and reach peak performance more\nrapidly$-$than LLMs trained with standard GRPO. Our results call into question\nthe advantage of costly, explicitly-defined PRMs for GRPO: we show that it is\npossible to instead leverage the hidden, built-in PRM structure within the\nvanilla GRPO algorithm to boost model performance with a negligible impact on\ntraining time and cost.",
      "pdf_url": "http://arxiv.org/pdf/2509.21154v1",
      "published": "2025-09-25T13:40:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21154v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
      "authors": [
        "Moshe Kimhi",
        "Erez Koifman",
        "Ehud Rivlin",
        "Eli Schwartz",
        "Chaim Baskin"
      ],
      "abstract": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
      "pdf_url": "http://arxiv.org/pdf/2509.21153v1",
      "published": "2025-09-25T13:39:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21153v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "LAVA: Explainability for Unsupervised Latent Embeddings",
      "authors": [
        "Ivan Stresec",
        "Joana P. Gonalves"
      ],
      "abstract": "Unsupervised black-box models can be drivers of scientific discovery, but\nremain difficult to interpret. Crucially, discovery hinges on understanding the\nmodel output, which is often a multi-dimensional latent embedding rather than a\nwell-defined target. While explainability for supervised learning usually seeks\nto uncover how input features are used to predict a target, its unsupervised\ncounterpart should relate input features to the structure of the learned latent\nspace. Adaptations of supervised model explainability for unsupervised learning\nprovide either single-sample or dataset-wide summary explanations. However,\nwithout automated strategies of relating similar samples to one another guided\nby their latent proximity, explanations remain either too fine-grained or too\nreductive to be meaningful. This is especially relevant for manifold learning\nmethods that produce no mapping function, leaving us only with the relative\nspatial organization of their embeddings. We introduce Locality-Aware Variable\nAssociations (LAVA), a post-hoc model-agnostic method designed to explain local\nembedding organization through its relationship with the input features. To\nachieve this, LAVA represents the latent space as a series of localities\n(neighborhoods) described in terms of correlations between the original\nfeatures, and then reveals reoccurring patterns of correlations across the\nentire latent space. Based on UMAP embeddings of MNIST and a single-cell kidney\ndataset, we show that LAVA captures relevant feature associations, with\nvisually and biologically relevant local patterns shared among seemingly\ndistant regions of the latent spaces.",
      "pdf_url": "http://arxiv.org/pdf/2509.21149v1",
      "published": "2025-09-25T13:38:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21149v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Emerging Paradigms for Securing Federated Learning Systems",
      "authors": [
        "Amr Akmal Abouelmagd",
        "Amr Hilal"
      ],
      "abstract": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.21147v1",
      "published": "2025-09-25T13:34:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21147v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ]
    },
    {
      "title": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
      "authors": [
        "Sitong Cheng",
        "Weizhen Bian",
        "Xinsheng Wang",
        "Ruibin Yuan",
        "Jianyi Chen",
        "Shunshun Yin",
        "Yike Guo",
        "Wei Xue"
      ],
      "abstract": "The ultimate goal of expressive speech-to-speech translation (S2ST) is to\naccurately translate spoken content while preserving the speaker identity and\nemotional style. However, progress in this field is largely hindered by three\nkey challenges: the scarcity of paired speech data that retains expressive\nstyles, the complexity of multi-stage processing pipelines, and the limited\ntransfer of translation capabilities from large language models (LLMs). In this\nwork, we address these challenges by introducing UniSS, a novel single-stage\nframework for expressive S2ST. Our approach features carefully designed speech\nsemantic and style modeling, enabling seamless integration with existing\ntext-based LLM frameworks to develop a unified text-speech language model. To\ntransfer translation capabilities from text to speech, we propose a cross-modal\nchain-of-thought prompting process that progressively aligns audio semantics\nwith text and ensures style preservation in the decoded results. Furthermore,\nwe construct and release a large-scale, high-quality expressive S2ST dataset,\nUniST, comprising 44.8k hours of data. Experimental results show that UniSS\nsignificantly outperforms previous methods in translation fidelity and speech\nquality while preserving voice, emotion, and duration consistency. Our work\nestablishes a simpler and more effective paradigm for building the next\ngeneration of expressive S2ST systems. Audio samples are available at\nhttps://cmots.github.io/uniss-demo.",
      "pdf_url": "http://arxiv.org/pdf/2509.21144v1",
      "published": "2025-09-25T13:30:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21144v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Embodied Representation Alignment with Mirror Neurons",
      "authors": [
        "Wentao Zhu",
        "Zhining Zhang",
        "Yuwei Ren",
        "Yin Huang",
        "Hao Xu",
        "Yizhou Wang"
      ],
      "abstract": "Mirror neurons are a class of neurons that activate both when an individual\nobserves an action and when they perform the same action. This mechanism\nreveals a fundamental interplay between action understanding and embodied\nexecution, suggesting that these two abilities are inherently connected.\nNonetheless, existing machine learning methods largely overlook this interplay,\ntreating these abilities as separate tasks. In this study, we provide a unified\nperspective in modeling them through the lens of representation learning. We\nfirst observe that their intermediate representations spontaneously align.\nInspired by mirror neurons, we further introduce an approach that explicitly\naligns the representations of observed and executed actions. Specifically, we\nemploy two linear layers to map the representations to a shared latent space,\nwhere contrastive learning enforces the alignment of corresponding\nrepresentations, effectively maximizing their mutual information. Experiments\ndemonstrate that this simple approach fosters mutual synergy between the two\ntasks, effectively improving representation quality and generalization.",
      "pdf_url": "http://arxiv.org/pdf/2509.21136v1",
      "published": "2025-09-25T13:27:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21136v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective",
      "authors": [
        "Yiwen Zhang",
        "Ziang Chen",
        "Fanqi Kong",
        "Yizhe Huang",
        "Xue Feng"
      ],
      "abstract": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2509.21134v1",
      "published": "2025-09-25T13:25:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21134v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
      "authors": [
        "Kohsei Matsutani",
        "Shota Takashiro",
        "Gouki Minegishi",
        "Takeshi Kojima",
        "Yusuke Iwasawa",
        "Yutaka Matsuo"
      ],
      "abstract": "Large language models (LLMs) are typically trained by reinforcement learning\n(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on\nreasoning traces to improve their reasoning abilities. However, how these\nmethods shape reasoning capabilities remains largely elusive. Going beyond an\naccuracy-based investigation of how these two components sculpt the reasoning\nprocess, this paper introduces a novel analysis framework that quantifies\nreasoning paths and captures their qualitative changes under each training\nprocess (with models of 1.5B, 7B, and 14B parameters on mathematical domains).\nSpecifically, we investigate the reasoning process at two levels of\ngranularity: the trajectory-level, which examines complete reasoning outputs,\nand the step-level, which analyzes reasoning graphs whose nodes correspond to\nindividual reasoning steps. Notably, clustering of unique reasoning\ntrajectories shows complementary effects: RL compresses incorrect trajectories,\nwhereas SFT expands correct ones. Step-level analysis reveals that RL steepens\n(about 2.5 times), while SFT flattens (reduced to about one-third), the decay\nrates of node visitation frequency, degree, and betweenness centrality\ndistributions in the reasoning graph. This indicates that RL concentrates\nreasoning functionality into a small subset of steps, while SFT homogenizes it\nacross many steps. Furthermore, by evaluating the reasoning graph topologies\nfrom multiple perspectives, we delineate the shared and distinct\ncharacteristics of RL and SFT. Our work presents a novel reasoning path\nperspective that explains why the current best practice of two-stage training,\nwith SFT followed by RL, is successful, and offers practical implications for\ndata construction and more efficient learning approaches.",
      "pdf_url": "http://arxiv.org/pdf/2509.21128v1",
      "published": "2025-09-25T13:18:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21128v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning",
      "authors": [
        "Xiefeng Wu",
        "Jing Zhao",
        "Shu Zhang",
        "Mingyu Hu"
      ],
      "abstract": "Online reinforcement learning in complex tasks is time-consuming, as massive\ninteraction steps are needed to learn the optimal Q-function.Vision-language\naction (VLA) policies represent a promising direction for solving diverse\ntasks; however, their performance on low-level control remains limited, and\neffective deployment often requires task-specific expert demonstrations for\nfine-tuning. In this paper, we propose \\textbf{VARL} (\\textbf{V}LM as\n\\textbf{A}ction advisor for online \\textbf{R}einforcement \\textbf{L}earning), a\nframework that leverages the domain knowledge of vision-language models (VLMs)\nto provide action suggestions for reinforcement learning agents. Unlike\nprevious methods, VARL provides action suggestions rather than designing\nheuristic rewards, thereby guaranteeing unchanged optimality and convergence.\nThe suggested actions increase sample diversity and ultimately improve sample\nefficiency, especially in sparse-reward tasks. To validate the effectiveness of\nVARL, we evaluate it across diverse environments and agent settings. Results\nshow that VARL greatly improves sample efficiency without introducing\nsignificant computational overhead. These advantages make VARL a general\nframework for online reinforcement learning and make it feasible to directly\napply reinforcement learning from scratch in real-world environments.",
      "pdf_url": "http://arxiv.org/pdf/2509.21126v1",
      "published": "2025-09-25T13:16:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21126v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
      "authors": [
        "Xuemiao Zhang",
        "Can Ren",
        "Chengying Tu",
        "Rongxiang Weng",
        "Shuo Wang",
        "Hongfei Yan",
        "Jingang Wang",
        "Xunliang Cai"
      ],
      "abstract": "Recent progress in large reasoning models for challenging mathematical\nreasoning has been driven by reinforcement learning (RL). Incorporating long\nchain-of-thought (CoT) data during mid-training has also been shown to\nsubstantially improve reasoning depth. However, current approaches often\nutilize CoT data indiscriminately, leaving open the critical question of which\ndata types most effectively enhance model reasoning capabilities. In this\npaper, we define the foundation model's reasoning potential for the first time\nas the inverse of the number of independent attempts required to correctly\nanswer the question, which is strongly correlated with the final model\nperformance. We then propose utilizing diverse data enriched with high-value\nreasoning patterns to expand the reasoning potential. Specifically, we abstract\natomic reasoning patterns from CoT sequences, characterized by commonality and\ninductive capabilities, and use them to construct a core reference set enriched\nwith valuable reasoning patterns. Furthermore, we propose a dual-granularity\nalgorithm involving chains of reasoning patterns and token entropy, efficiently\nselecting high-value CoT data (CoTP) from the data pool that aligns with the\ncore set, thereby training models to master reasoning effectively. Only\n10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve\nby 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of\ndownstream RL performance by 7.81%.",
      "pdf_url": "http://arxiv.org/pdf/2509.21124v1",
      "published": "2025-09-25T13:11:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21124v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them",
      "authors": [
        "Yidong Wang",
        "Yunze Song",
        "Tingyuan Zhu",
        "Xuanwang Zhang",
        "Zhuohao Yu",
        "Hao Chen",
        "Chiyu Song",
        "Qiufeng Wang",
        "Cunxiang Wang",
        "Zhen Wu",
        "Xinyu Dai",
        "Yue Zhang",
        "Wei Ye",
        "Shikun Zhang"
      ],
      "abstract": "The adoption of Large Language Models (LLMs) as automated evaluators\n(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation\nframeworks. We identify two fundamental types of inconsistencies: (1)\nScore-Comparison Inconsistency, where lower-rated responses outperform\nhigher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity\nInconsistency, manifested through circular preference chains (A>B>C>A) and\nequivalence contradictions (A=B=C\\neq A). We argue that these issues come from\ninformation loss in discrete rating systems and ambiguous tie judgments during\npairwise evaluation. We propose TrustJudge, a probabilistic framework that\naddresses these limitations through two key innovations: 1)\ndistribution-sensitive scoring that computes continuous expectations from\ndiscrete rating probabilities, preserving information entropy for more precise\nscoring, and 2) likelihood-aware aggregation that resolves transitivity\nviolations using bidirectional preference probabilities or perplexity. We also\nformalize the theoretical limitations of current LLM-as-a-judge frameworks and\ndemonstrate how TrustJudge's components overcome them. When evaluated with\nLlama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces\nScore-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise\nTransitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining\nhigher evaluation accuracy. Our work provides the first systematic analysis of\nevaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both\ntheoretical insights and practical solutions for reliable automated assessment.\nThe framework demonstrates consistent improvements across various model\narchitectures and scales, enabling more trustworthy LLM evaluation without\nrequiring additional training or human annotations. The codes can be found at\nhttps://github.com/TrustJudge/TrustJudge.",
      "pdf_url": "http://arxiv.org/pdf/2509.21117v1",
      "published": "2025-09-25T13:04:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21117v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Cross-Modal Instructions for Robot Motion Generation",
      "authors": [
        "William Barron",
        "Xiaoxiang Dong",
        "Matthew Johnson-Roberson",
        "Weiming Zhi"
      ],
      "abstract": "Teaching robots novel behaviors typically requires motion demonstrations via\nteleoperation or kinaesthetic teaching, that is, physically guiding the robot.\nWhile recent work has explored using human sketches to specify desired\nbehaviors, data collection remains cumbersome, and demonstration datasets are\ndifficult to scale. In this paper, we introduce an alternative paradigm,\nLearning from Cross-Modal Instructions, where robots are shaped by\ndemonstrations in the form of rough annotations, which can contain free-form\ntext labels, and are used in lieu of physical motion. We introduce the\nCrossInstruct framework, which integrates cross-modal instructions as examples\ninto the context input to a foundational vision-language model (VLM). The VLM\nthen iteratively queries a smaller, fine-tuned model, and synthesizes the\ndesired motion over multiple 2D views. These are then subsequently fused into a\ncoherent distribution over 3D motion trajectories in the robot's workspace. By\nincorporating the reasoning of the large VLM with a fine-grained pointing\nmodel, CrossInstruct produces executable robot behaviors that generalize beyond\nthe environment of in the limited set of instruction examples. We then\nintroduce a downstream reinforcement learning pipeline that leverages\nCrossInstruct outputs to efficiently learn policies to complete fine-grained\ntasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and\nreal hardware, demonstrating effectiveness without additional fine-tuning and\nproviding a strong initialization for policies subsequently refined via\nreinforcement learning.",
      "pdf_url": "http://arxiv.org/pdf/2509.21107v1",
      "published": "2025-09-25T12:54:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21107v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization",
      "authors": [
        "Louis Van Langendonck",
        "Guillermo Bernrdez",
        "Nina Miolane",
        "Pere Barlet-Ros"
      ],
      "abstract": "A fundamental challenge in graph learning is understanding how models\ngeneralize to new, unseen graphs. While synthetic benchmarks offer controlled\nsettings for analysis, existing approaches are confined to single-graph,\ntransductive settings where models train and test on the same graph structure.\nAddressing this gap, we introduce GraphUniverse, a framework for generating\nentire families of graphs to enable the first systematic evaluation of\ninductive generalization at scale. Our core innovation is the generation of\ngraphs with persistent semantic communities, ensuring conceptual consistency\nwhile allowing fine-grained control over structural properties like homophily\nand degree distributions. This enables crucial but underexplored robustness\ntests, such as performance under controlled distribution shifts. Benchmarking a\nwide range of architectures -- from GNNs to graph transformers and topological\narchitectures -- reveals that strong transductive performance is a poor\npredictor of inductive generalization. Furthermore, we find that robustness to\ndistribution shift is highly sensitive not only to model architecture choice\nbut also to the initial graph regime (e.g., high vs. low homophily). Beyond\nbenchmarking, GraphUniverse's flexibility and scalability can facilitate the\ndevelopment of robust and truly generalizable architectures -- including\nnext-generation graph foundation models. An interactive demo is available at\nhttps://graphuniverse.streamlit.app.",
      "pdf_url": "http://arxiv.org/pdf/2509.21097v1",
      "published": "2025-09-25T12:46:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21097v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
      "authors": [
        "Junpei Komiyama",
        "Daisuke Oba",
        "Masafumi Oyamada"
      ],
      "abstract": "We study best-of-$N$ for large language models (LLMs) where the selection is\nbased on majority voting. In particular, we analyze the limit $N \\to \\infty$,\nwhich we denote as Best-of-$\\infty$. While this approach achieves impressive\nperformance in the limit, it requires an infinite test-time budget. To address\nthis, we propose an adaptive generation scheme that selects $N$ based on answer\nagreement, thereby efficiently allocating inference-time computation. Beyond\nadaptivity, we extend the framework to weighted ensembles of multiple LLMs,\nshowing that such mixtures can outperform any individual model. The optimal\nensemble weighting is formulated and efficiently computed as a mixed-integer\nlinear program. Extensive experiments demonstrate the effectiveness of our\napproach.",
      "pdf_url": "http://arxiv.org/pdf/2509.21091v1",
      "published": "2025-09-25T12:41:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21091v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Vision Transformers: the threat of realistic adversarial patches",
      "authors": [
        "Kasper Cools",
        "Clara Maathuis",
        "Alexander M. van Oers",
        "Claudia S. Hbner",
        "Nikos Deligiannis",
        "Marijke Vandewal",
        "Geert De Cubber"
      ],
      "abstract": "The increasing reliance on machine learning systems has made their security a\ncritical concern. Evasion attacks enable adversaries to manipulate the\ndecision-making processes of AI systems, potentially causing security breaches\nor misclassification of targets. Vision Transformers (ViTs) have gained\nsignificant traction in modern machine learning due to increased 1) performance\ncompared to Convolutional Neural Networks (CNNs) and 2) robustness against\nadversarial perturbations. However, ViTs remain vulnerable to evasion attacks,\nparticularly to adversarial patches, unique patterns designed to manipulate AI\nclassification systems. These vulnerabilities are investigated by designing\nrealistic adversarial patches to cause misclassification in person vs.\nnon-person classification tasks using the Creases Transformation (CT)\ntechnique, which adds subtle geometric distortions similar to those occurring\nnaturally when wearing clothing. This study investigates the transferability of\nadversarial attack techniques used in CNNs when applied to ViT classification\nmodels. Experimental evaluation across four fine-tuned ViT models on a binary\nperson classification task reveals significant vulnerability variations: attack\nsuccess rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97%\n(facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and\nfacebook/dinov3-vitb16 reaching 65.17%. These results confirm the\ncross-architectural transferability of adversarial patches from CNNs to ViTs,\nwith pre-training dataset scale and methodology strongly influencing model\nresilience to adversarial attacks.",
      "pdf_url": "http://arxiv.org/pdf/2509.21084v1",
      "published": "2025-09-25T12:36:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21084v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix",
      "authors": [
        "Ahmet Caner Yzgler",
        "Ahmet elik",
        "Jiawei Zhuang",
        "Lukas Cavigelli"
      ],
      "abstract": "Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in\nstate-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel\nformulation, MLA allows two functionally equivalent but computationally\ndistinct kernel implementations: naive and absorb. While the naive kernels\n(e.g., FlashAttention) are typically preferred in training and prefill for\ntheir computational efficiency, existing decoding kernels (e.g., FlashMLA) rely\non the absorb method to minimize HBM bandwidth usage. However, the\ncompute-bound nature of the absorb implementations prohibits performance\nbenefits from data reuse opportunities in attention calculations, such as\nshared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that\ncombines naive and absorb formulations to harness the strengths of both.\nTyphoonMLA effectively leverages the shared prefix by applying the naive\nformulation to the compute-bound parts of attention calculations, while\nreducing the bandwidth requirements for non-shared parts by using the absorb\nformulation. As a result, TyphoonMLA improves the throughput of attention\ncalculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with\nonly a 3% overhead in HBM size.",
      "pdf_url": "http://arxiv.org/pdf/2509.21081v1",
      "published": "2025-09-25T12:32:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21081v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs",
      "authors": [
        "Yixin Wan",
        "Xingrun Chen",
        "Kai-Wei Chang"
      ],
      "abstract": "Large language models (LLMs) have unlocked a wide range of downstream\ngenerative applications. However, we found that they also risk perpetuating\nsubtle fairness issues tied to culture, positioning their generations from the\nperspectives of the mainstream US culture while demonstrating salient\nexternality towards non-mainstream ones. In this work, we identify and\nsystematically investigate this novel culture positioning bias, in which an\nLLM's default generative stance aligns with a mainstream view and treats other\ncultures as outsiders. We propose the CultureLens benchmark with 4000\ngeneration prompts and 3 evaluation metrics for quantifying this bias through\nthe lens of a culturally situated interview script generation task, in which an\nLLM is positioned as an onsite reporter interviewing local people across 10\ndiverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a\nstark pattern: while models adopt insider tones in over 88 percent of\nUS-contexted scripts on average, they disproportionately adopt mainly outsider\nstances for less dominant cultures. To resolve these biases, we propose 2\ninference-time mitigation methods: a baseline prompt-based Fairness\nIntervention Pillars (FIP) method, and a structured Mitigation via Fairness\nAgents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)\nintroduces a self-reflection and rewriting loop based on fairness guidelines.\n(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized\nagents: a Planner Agent(initial script generation), a Critique Agent (evaluates\ninitial script against fairness pillars), and a Refinement Agent (incorporates\nfeedback to produce a polished, unbiased script). Empirical results showcase\nthe effectiveness of agent-based methods as a promising direction for\nmitigating biases in generative LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.21080v1",
      "published": "2025-09-25T12:28:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21080v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Communication Bias in Large Language Models: A Regulatory Perspective",
      "authors": [
        "Adrian Kuenzler",
        "Stefan Schmid"
      ],
      "abstract": "Large language models (LLMs) are increasingly central to many applications,\nraising concerns about bias, fairness, and regulatory compliance. This paper\nreviews risks of biased outputs and their societal impact, focusing on\nframeworks like the EU's AI Act and the Digital Services Act. We argue that\nbeyond constant regulation, stronger attention to competition and design\ngovernance is needed to ensure fair, trustworthy AI. This is a preprint of the\nCommunications of the ACM article of the same title.",
      "pdf_url": "http://arxiv.org/pdf/2509.21075v1",
      "published": "2025-09-25T12:25:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21075v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.DC",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution",
      "authors": [
        "Kaiwen He",
        "Zhiwei Wang",
        "Chenyi Zhuang",
        "Jinjie Gu"
      ],
      "abstract": "Recent years, multimodal models have made remarkable strides and pave the way\nfor intelligent browser use agents. However, when solving tasks on real world\nwebpages in multi-turn, long-horizon trajectories, current agents still suffer\nfrom disordered action sequencing and excessive trial and error during\nexecution. This paper introduces Recon-Act, a self-evolving multi-agent\nframework grounded in Reconnaissance-Action behavioral paradigm. The system\ncomprises a Reconnaissance Team and an Action Team: the former conducts\ncomparative analysis and tool generation, while the latter handles intent\ndecomposition, tool orchestration, and execution. By contrasting the erroneous\ntrajectories with successful ones, the Reconnaissance Team infers remedies, and\nabstracts them into a unified notion of generalized tools, either expressed as\nhints or as rule-based codes, and register to the tool archive in real time.\nThe Action Team reinference the process empowered with these targeting tools,\nthus establishing a closed-loop training pipeline of\ndata-tools-action-feedback. Following the 6 level implementation roadmap\nproposed in this work, we have currently reached Level 3 (with limited\nhuman-in-the-loop intervention). Leveraging generalized tools obtained through\nreconnaissance, Recon-Act substantially improves adaptability to unseen\nwebsites and solvability on long-horizon tasks, and achieves state-of-the-art\nperformance on the challenging VisualWebArena dataset.",
      "pdf_url": "http://arxiv.org/pdf/2509.21072v1",
      "published": "2025-09-25T12:23:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21072v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning",
      "authors": [
        "Qizhi Pei",
        "Zhuoshi Pan",
        "Honglin Lin",
        "Xin Gao",
        "Yu Li",
        "Zinan Tang",
        "Conghui He",
        "Rui Yan",
        "Lijun Wu"
      ],
      "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex\nproblem-solving, often benefiting from training on difficult mathematical\nproblems that stimulate intricate reasoning. Recent efforts have explored\nautomated synthesis of mathematical problems by prompting proprietary models or\nlarge-scale open-source models from seed data or inherent mathematical\nconcepts. However, scaling up these methods remains challenging due to their\nhigh computational/API cost, complexity of prompting, and limited difficulty\nlevel of the generated problems. To overcome these limitations, we propose\nScaleDiff, a simple yet effective pipeline designed to scale the creation of\ndifficult problems. We efficiently identify difficult problems from existing\ndatasets with only a single forward pass using an adaptive thinking model,\nwhich can perceive problem difficulty and automatically switch between\n\"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult\nproblem generator (DiffGen-8B) on this filtered difficult data, which can\nproduce new difficult problems in large scale, eliminating the need for\ncomplex, per-instance prompting and its associated high API costs. Fine-tuning\nQwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial\nperformance increase of 11.3% compared to the original dataset and achieves a\n65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500,\noutperforming recent strong LRMs like OpenThinker3. Notably, this performance\nis achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating\nthat our pipeline can effectively transfer advanced reasoning capabilities\nwithout relying on larger, more expensive teacher models. Furthermore, we\nobserve a clear scaling phenomenon in model performance on difficult benchmarks\nas the quantity of difficult problems increases. Code:\nhttps://github.com/QizhiPei/ScaleDiff.",
      "pdf_url": "http://arxiv.org/pdf/2509.21070v1",
      "published": "2025-09-25T12:22:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21070v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "EnGraf-Net: Multiple Granularity Branch Network with Fine-Coarse Graft Grained for Classification Task",
      "authors": [
        "Riccardo La Grassa",
        "Ignazio Gallo",
        "Nicola Landro"
      ],
      "abstract": "Fine-grained classification models are designed to focus on the relevant\ndetails necessary to distinguish highly similar classes, particularly when\nintra-class variance is high and inter-class variance is low. Most existing\nmodels rely on part annotations such as bounding boxes, part locations, or\ntextual attributes to enhance classification performance, while others employ\nsophisticated techniques to automatically extract attention maps. We posit that\npart-based approaches, including automatic cropping methods, suffer from an\nincomplete representation of local features, which are fundamental for\ndistinguishing similar objects. While fine-grained classification aims to\nrecognize the leaves of a hierarchical structure, humans recognize objects by\nalso forming semantic associations. In this paper, we leverage semantic\nassociations structured as a hierarchy (taxonomy) as supervised signals within\nan end-to-end deep neural network model, termed EnGraf-Net. Extensive\nexperiments on three well-known datasets CIFAR-100, CUB-200-2011, and\nFGVC-Aircraft demonstrate the superiority of EnGraf-Net over many existing\nfine-grained models, showing competitive performance with the most recent\nstate-of-the-art approaches, without requiring cropping techniques or manual\nannotations.",
      "pdf_url": "http://arxiv.org/pdf/2509.21061v1",
      "published": "2025-09-25T12:11:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.21061v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}