{
  "last_updated": "2025-03-20T00:45:45.341337",
  "papers": [
    {
      "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
      "authors": [
        "Susung Hong",
        "Ira Kemelmacher-Shlizerman",
        "Brian Curless",
        "Steven M. Seitz"
      ],
      "abstract": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
      "pdf_url": "http://arxiv.org/pdf/2503.14505v1",
      "published": "2025-03-18T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14505v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The Power of Context: How Multimodality Improves Image Super-Resolution",
      "authors": [
        "Kangfu Mei",
        "Hossein Talebi",
        "Mojtaba Ardakani",
        "Vishal M. Patel",
        "Peyman Milanfar",
        "Mauricio Delbracio"
      ],
      "abstract": "Single-image super-resolution (SISR) remains challenging due to the inherent\ndifficulty of recovering fine-grained details and preserving perceptual quality\nfrom low-resolution inputs. Existing methods often rely on limited image\npriors, leading to suboptimal results. We propose a novel approach that\nleverages the rich contextual information available in multiple modalities --\nincluding depth, segmentation, edges, and text prompts -- to learn a powerful\ngenerative prior for SISR within a diffusion model framework. We introduce a\nflexible network architecture that effectively fuses multimodal information,\naccommodating an arbitrary number of input modalities without requiring\nsignificant modifications to the diffusion process. Crucially, we mitigate\nhallucinations, often introduced by text prompts, by using spatial information\nfrom other modalities to guide regional text-based conditioning. Each\nmodality's guidance strength can also be controlled independently, allowing\nsteering outputs toward different directions, such as increasing bokeh through\ndepth or adjusting object prominence via segmentation. Extensive experiments\ndemonstrate that our model surpasses state-of-the-art generative SISR methods,\nachieving superior visual quality and fidelity. See project page at\nhttps://mmsr.kfmei.com/.",
      "pdf_url": "http://arxiv.org/pdf/2503.14503v1",
      "published": "2025-03-18T17:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14503v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Measuring AI Ability to Complete Long Tasks",
      "authors": [
        "Thomas Kwa",
        "Ben West",
        "Joel Becker",
        "Amy Deng",
        "Katharyn Garcia",
        "Max Hasin",
        "Sami Jawhar",
        "Megan Kinniment",
        "Nate Rush",
        "Sydney Von Arx",
        "Ryan Bloom",
        "Thomas Broadley",
        "Haoxing Du",
        "Brian Goodrich",
        "Nikola Jurkovic",
        "Luke Harold Miles",
        "Seraphina Nix",
        "Tao Lin",
        "Neev Parikh",
        "David Rein",
        "Lucas Jun Koba Sato",
        "Hjalmar Wijk",
        "Daniel M. Ziegler",
        "Elizabeth Barnes",
        "Lawrence Chan"
      ],
      "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
      "pdf_url": "http://arxiv.org/pdf/2503.14499v1",
      "published": "2025-03-18T17:59:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14499v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "State Space Model Meets Transformer: A New Paradigm for 3D Object Detection",
      "authors": [
        "Chuxin Wang",
        "Wenfei Yang",
        "Xiang Liu",
        "Tianzhu Zhang"
      ],
      "abstract": "DETR-based methods, which use multi-layer transformer decoders to refine\nobject queries iteratively, have shown promising performance in 3D indoor\nobject detection. However, the scene point features in the transformer decoder\nremain fixed, leading to minimal contributions from later decoder layers,\nthereby limiting performance improvement. Recently, State Space Models (SSM)\nhave shown efficient context modeling ability with linear complexity through\niterative interactions between system states and inputs. Inspired by SSMs, we\npropose a new 3D object DEtection paradigm with an interactive STate space\nmodel (DEST). In the interactive SSM, we design a novel state-dependent SSM\nparameterization method that enables system states to effectively serve as\nqueries in 3D indoor detection tasks. In addition, we introduce four key\ndesigns tailored to the characteristics of point cloud and SSM: The\nserialization and bidirectional scanning strategies enable bidirectional\nfeature interaction among scene points within the SSM. The inter-state\nattention mechanism models the relationships between state points, while the\ngated feed-forward network enhances inter-channel correlations. To the best of\nour knowledge, this is the first method to model queries as system states and\nscene points as system inputs, which can simultaneously update scene point\nfeatures and query features with linear complexity. Extensive experiments on\ntwo challenging datasets demonstrate the effectiveness of our DEST-based\nmethod. Our method improves the GroupFree baseline in terms of AP50 on ScanNet\nV2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our\nmethod sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.",
      "pdf_url": "http://arxiv.org/pdf/2503.14493v1",
      "published": "2025-03-18T17:58:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14493v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control",
      "authors": [
        "NVIDIA",
        ":",
        "Hassan Abu Alhaija",
        "Jose Alvarez",
        "Maciej Bala",
        "Tiffany Cai",
        "Tianshi Cao",
        "Liz Cha",
        "Joshua Chen",
        "Mike Chen",
        "Francesco Ferroni",
        "Sanja Fidler",
        "Dieter Fox",
        "Yunhao Ge",
        "Jinwei Gu",
        "Ali Hassani",
        "Michael Isaev",
        "Pooya Jannaty",
        "Shiyi Lan",
        "Tobias Lasser",
        "Huan Ling",
        "Ming-Yu Liu",
        "Xian Liu",
        "Yifan Lu",
        "Alice Luo",
        "Qianli Ma",
        "Hanzi Mao",
        "Fabio Ramos",
        "Xuanchi Ren",
        "Tianchang Shen",
        "Shitao Tang",
        "Ting-Chun Wang",
        "Jay Wu",
        "Jiashu Xu",
        "Stella Xu",
        "Kevin Xie",
        "Yuchong Ye",
        "Xiaodong Yang",
        "Xiaohui Zeng",
        "Yu Zeng"
      ],
      "abstract": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
      "pdf_url": "http://arxiv.org/pdf/2503.14492v1",
      "published": "2025-03-18T17:57:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14492v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Engineering Scientific Assistants using Interactive Structured Induction of Programs",
      "authors": [
        "Shraddha Surana",
        "Ashwin Srinivasan"
      ],
      "abstract": "We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants.",
      "pdf_url": "http://arxiv.org/pdf/2503.14488v1",
      "published": "2025-03-18T17:57:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14488v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
      "authors": [
        "Minglei Shi",
        "Ziyang Yuan",
        "Haotian Yang",
        "Xintao Wang",
        "Mingwu Zheng",
        "Xin Tao",
        "Wenliang Zhao",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai"
      ],
      "abstract": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/",
      "pdf_url": "http://arxiv.org/pdf/2503.14487v1",
      "published": "2025-03-18T17:57:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14487v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Gricean Norms as a Basis for Effective Collaboration",
      "authors": [
        "Fardin Saad",
        "Pradeep K. Murukannaiah",
        "Munindar P. Singh"
      ],
      "abstract": "Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents.",
      "pdf_url": "http://arxiv.org/pdf/2503.14484v1",
      "published": "2025-03-18T17:54:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14484v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Attribution Score Alignment in Explainable Data Management",
      "authors": [
        "Felipe Azua",
        "Leopoldo Bertossi"
      ],
      "abstract": "Different attribution-scores have been proposed to quantify the relevance of\ndatabase tuples for a query answer from a database. Among them, we find Causal\nResponsibility, the Shapley Value, the Banzhaf Power-Index, and the Causal\nEffect. They have been analyzed in isolation, mainly in terms of computational\nproperties. In this work, we start an investigation into the alignment of these\nscores on the basis of the queries at hand; that is, on whether they induce\ncompatible rankings of tuples. We are able to identify vast classes of queries\nfor which some pairs of scores are always aligned, and others for which they\nare not. It turns out that the presence of exogenous tuples makes a crucial\ndifference in this regard.",
      "pdf_url": "http://arxiv.org/pdf/2503.14469v1",
      "published": "2025-03-18T17:45:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14469v1",
      "categories": [
        "cs.DB",
        "cs.AI"
      ]
    },
    {
      "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
      "authors": [
        "Bo Peng",
        "Ruichong Zhang",
        "Daniel Goldstein",
        "Eric Alcaide",
        "Haowen Hou",
        "Janna Lu",
        "William Merrill",
        "Guangyu Song",
        "Kaifeng Tan",
        "Saiteja Utpala",
        "Nathan Wilce",
        "Johan S. Wind",
        "Tianyi Wu",
        "Daniel Wuttke",
        "Christian Zhou-Zheng"
      ],
      "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
      "pdf_url": "http://arxiv.org/pdf/2503.14456v1",
      "published": "2025-03-18T17:31:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14456v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.0; I.2.7"
      ]
    },
    {
      "title": "Pauli Network Circuit Synthesis with Reinforcement Learning",
      "authors": [
        "Ayushi Dubal",
        "David Kremer",
        "Simon Martiel",
        "Victor Villar",
        "Derek Wang",
        "Juan Cruz-Benito"
      ],
      "abstract": "We introduce a Reinforcement Learning (RL)-based method for re-synthesis of\nquantum circuits containing arbitrary Pauli rotations alongside Clifford\noperations. By collapsing each sub-block to a compact representation and then\nsynthesizing it step-by-step through a learned heuristic, we obtain circuits\nthat are both shorter and compliant with hardware connectivity constraints. We\nfind that the method is fast enough and good enough to work as an optimization\nprocedure: in direct comparisons on 6-qubit random Pauli Networks against\nstate-of-the-art heuristic methods, our RL approach yields over 2x reduction in\ntwo-qubit gate count, while executing in under 10 milliseconds per circuit. We\nfurther integrate the method into a collect-and-re-synthesize pipeline, applied\nas a Qiskit transpiler pass, where we observe average improvements of 20% in\ntwo-qubit gate count and depth, reaching up to 60% for many instances, across\nthe Benchpress benchmark. These results highlight the potential of RL-driven\nsynthesis to significantly improve circuit quality in realistic, large-scale\nquantum transpilation workloads.",
      "pdf_url": "http://arxiv.org/pdf/2503.14448v1",
      "published": "2025-03-18T17:27:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14448v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers",
      "authors": [
        "Nikhil Abhyankar",
        "Parshin Shojaee",
        "Chandan K. Reddy"
      ],
      "abstract": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2503.14434v1",
      "published": "2025-03-18T17:11:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14434v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ]
    },
    {
      "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play",
      "authors": [
        "Wei Fang",
        "Yang Zhang",
        "Kaizhi Qian",
        "James Glass",
        "Yada Zhu"
      ],
      "abstract": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.",
      "pdf_url": "http://arxiv.org/pdf/2503.14432v1",
      "published": "2025-03-18T17:09:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14432v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation",
      "authors": [
        "Hongyu Zhang",
        "Yufan Deng",
        "Shenghai Yuan",
        "Peng Jin",
        "Zesen Cheng",
        "Yian Zhao",
        "Chang Liu",
        "Jie Chen"
      ],
      "abstract": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
      "pdf_url": "http://arxiv.org/pdf/2503.14428v1",
      "published": "2025-03-18T17:02:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14428v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms",
      "authors": [
        "Seungwon Lim",
        "Sungwoong Kim",
        "Jihwan Yu",
        "Sungjae Lee",
        "Jiwan Chung",
        "Youngjae Yu"
      ],
      "abstract": "Escape rooms present a unique cognitive challenge that demands\nexploration-driven planning: players should actively search their environment,\ncontinuously update their knowledge based on new discoveries, and connect\ndisparate clues to determine which elements are relevant to their objectives.\nMotivated by this, we introduce VisEscape, a benchmark of 20 virtual escape\nrooms specifically designed to evaluate AI models under these challenging\nconditions, where success depends not only on solving isolated puzzles but also\non iteratively constructing and refining spatial-temporal knowledge of a\ndynamically changing environment. On VisEscape, we observed that even\nstate-of-the-art multimodal models generally fail to escape the rooms, showing\nconsiderable variation in their levels of progress and trajectories. To address\nthis issue, we propose VisEscaper, which effectively integrates Memory,\nFeedback, and ReAct modules, demonstrating significant improvements by\nperforming 3.7 times more effectively and 5.0 times more efficiently on\naverage.",
      "pdf_url": "http://arxiv.org/pdf/2503.14427v1",
      "published": "2025-03-18T16:59:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14427v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ExDDV: A New Dataset for Explainable Deepfake Detection in Video",
      "authors": [
        "Vlad Hondru",
        "Eduard Hogea",
        "Darian Onchis",
        "Radu Tudor Ionescu"
      ],
      "abstract": "The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV.",
      "pdf_url": "http://arxiv.org/pdf/2503.14421v1",
      "published": "2025-03-18T16:55:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14421v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "Iffy-Or-Not: Extending the Web to Support the Critical Evaluation of Fallacious Texts",
      "authors": [
        "Gionnieve Lim",
        "Juho Kim",
        "Simon T. Perrault"
      ],
      "abstract": "Social platforms have expanded opportunities for deliberation with the\ncomments being used to inform one's opinion. However, using such information to\nform opinions is challenged by unsubstantiated or false content. To enhance the\nquality of opinion formation and potentially confer resistance to\nmisinformation, we developed Iffy-Or-Not (ION), a browser extension that seeks\nto invoke critical thinking when reading texts. With three features guided by\nargumentation theory, ION highlights fallacious content, suggests diverse\nqueries to probe them with, and offers deeper questions to consider and chat\nwith others about. From a user study (N=18), we found that ION encourages users\nto be more attentive to the content, suggests queries that align with or are\npreferable to their own, and poses thought-provoking questions that expands\ntheir perspectives. However, some participants expressed aversion to ION due to\nmisalignments with their information goals and thinking predispositions.\nPotential backfiring effects with ION are discussed.",
      "pdf_url": "http://arxiv.org/pdf/2503.14412v1",
      "published": "2025-03-18T16:50:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14412v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models",
      "authors": [
        "Siwei Zhang",
        "Yun Xiong",
        "Yateng Tang",
        "Xi Chen",
        "Zian Jia",
        "Zehao Gu",
        "Jiarong Xu",
        "Jiawei Zhang"
      ],
      "abstract": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
      "pdf_url": "http://arxiv.org/pdf/2503.14411v1",
      "published": "2025-03-18T16:50:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14411v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels",
      "authors": [
        "Maximilian Beck",
        "Korbinian Pöppel",
        "Phillip Lippe",
        "Sepp Hochreiter"
      ],
      "abstract": "Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels.\nLeveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear\nAttention (FLA) shows that linear RNN kernels are faster than Flash Attention,\nby parallelizing over chunks of the input sequence. However, since the chunk\nsize of FLA is limited, many intermediate states must be materialized in GPU\nmemory. This leads to low arithmetic intensity and causes high memory\nconsumption and IO cost, especially for long-context pre-training. In this\nwork, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm\nfor linear RNNs, that enables arbitrary large chunk sizes by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM. Second, we propose an mLSTM\nvariant with sigmoid input gate and reduced computation for even faster kernel\nruntimes at equal language modeling performance. In our speed benchmarks, we\nshow that our new mLSTM kernels based on TFLA outperform highly optimized Flash\nAttention, Linear Attention and Mamba kernels, setting a new state of the art\nfor efficient long-context sequence modeling primitives.",
      "pdf_url": "http://arxiv.org/pdf/2503.14376v1",
      "published": "2025-03-18T16:09:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14376v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Retrospective: A CORDIC Based Configurable Activation Function for NN Applications",
      "authors": [
        "Omkar Kokane",
        "Gopal Raut",
        "Salim Ullah",
        "Mukul Lokhande",
        "Adam Teman",
        "Akash Kumar",
        "Santosh Kumar Vishvakarma"
      ],
      "abstract": "A CORDIC-based configuration for the design of Activation Functions (AF) was\npreviously suggested to accelerate ASIC hardware design for\nresource-constrained systems by providing functional reconfigurability. Since\nits introduction, this new approach for neural network acceleration has gained\nwidespread popularity, influencing numerous designs for activation functions in\nboth academic and commercial AI processors. In this retrospective analysis, we\nexplore the foundational aspects of this initiative, summarize key developments\nover recent years, and introduce the DA-VINCI AF tailored for the evolving\nneeds of AI applications. This new generation of dynamically configurable and\nprecision-adjustable activation function cores promise greater adaptability for\na range of activation functions in AI workloads, including Swish, SoftMax,\nSeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously\npresented design has been optimized for MAC, Sigmoid, and Tanh functionalities\nand incorporated into ReLU AFs, culminating in an accumulative NEURIC compute\nunit. These enhancements position NEURIC as a fundamental component in the\nresource-efficient vector engine for the realization of AI accelerators that\nfocus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results\n(QoR) of 98.5%.",
      "pdf_url": "http://arxiv.org/pdf/2503.14354v1",
      "published": "2025-03-18T15:38:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14354v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "eess.IV"
      ]
    },
    {
      "title": "VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation",
      "authors": [
        "Shoubin Yu",
        "Difan Liu",
        "Ziqiao Ma",
        "Yicong Hong",
        "Yang Zhou",
        "Hao Tan",
        "Joyce Chai",
        "Mohit Bansal"
      ],
      "abstract": "Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.",
      "pdf_url": "http://arxiv.org/pdf/2503.14350v1",
      "published": "2025-03-18T15:31:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14350v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MoonCast: High-Quality Zero-Shot Podcast Generation",
      "authors": [
        "Zeqian Ju",
        "Dongchao Yang",
        "Jianwei Yu",
        "Kai Shen",
        "Yichong Leng",
        "Zhengtao Wang",
        "Xu Tan",
        "Xinyu Zhou",
        "Tao Qin",
        "Xiangyang Li"
      ],
      "abstract": "Recent advances in text-to-speech synthesis have achieved notable success in\ngenerating high-quality short utterances for individual speakers. However,\nthese systems still face challenges when extending their capabilities to long,\nmulti-speaker, and spontaneous dialogues, typical of real-world scenarios such\nas podcasts. These limitations arise from two primary challenges: 1) long\nspeech: podcasts typically span several minutes, exceeding the upper limit of\nmost existing work; 2) spontaneity: podcasts are marked by their spontaneous,\noral nature, which sharply contrasts with formal, written contexts; existing\nworks often fall short in capturing this spontaneity. In this paper, we propose\nMoonCast, a solution for high-quality zero-shot podcast generation, aiming to\nsynthesize natural podcast-style speech from text-only sources (e.g., stories,\ntechnical reports, news in TXT, PDF, or Web URL formats) using the voices of\nunseen speakers. To generate long audio, we adopt a long-context language\nmodel-based audio modeling approach utilizing large-scale long-context speech\ndata. To enhance spontaneity, we utilize a podcast generation module to\ngenerate scripts with spontaneous details, which have been empirically shown to\nbe as crucial as the text-to-speech modeling itself. Experiments demonstrate\nthat MoonCast outperforms baselines, with particularly notable improvements in\nspontaneity and coherence.",
      "pdf_url": "http://arxiv.org/pdf/2503.14345v2",
      "published": "2025-03-18T15:25:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14345v2",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ]
    },
    {
      "title": "Spatio-Temporal Graph Neural Networks for Infant Language Acquisition Prediction",
      "authors": [
        "Andrew Roxburgh",
        "Floriana Grasso",
        "Terry R. Payne"
      ],
      "abstract": "Predicting the words that a child is going to learn next can be useful for\nboosting language acquisition, and such predictions have been shown to be\npossible with both neural network techniques (looking at changes in the\nvocabulary state over time) and graph model (looking at data pertaining to the\nrelationships between words). However, these models do not fully capture the\ncomplexity of the language learning process of an infant when used in\nisolation. In this paper, we examine how a model of language acquisition for\ninfants and young children can be constructed and adapted for use in a\nSpatio-Temporal Graph Convolutional Network (STGCN), taking into account the\ndifferent types of linguistic relationships that occur during child language\nlearning. We introduce a novel approach for predicting child vocabulary\nacquisition, and evaluate the efficacy of such a model with respect to the\ndifferent types of linguistic relationships that occur during language\nacquisition, resulting in insightful observations on model calibration and norm\nselection. An evaluation of this model found that the mean accuracy of models\nfor predicting new words when using sensorimotor relationships (0.733) and\nsemantic relationships (0.729) were found to be superior to that observed with\na 2-layer Feed-forward neural network. Furthermore, the high recall for some\nrelationships suggested that some relationships (e.g. visual) were superior in\nidentifying a larger proportion of relevant words that a child should\nsubsequently learn than others (such as auditory).",
      "pdf_url": "http://arxiv.org/pdf/2503.14341v1",
      "published": "2025-03-18T15:21:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14341v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Revealing higher-order neural representations with generative artificial intelligence",
      "authors": [
        "Hojjat Azimi Asrari",
        "Megan A. K. Peters"
      ],
      "abstract": "Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.",
      "pdf_url": "http://arxiv.org/pdf/2503.14333v1",
      "published": "2025-03-18T15:08:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14333v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ]
    },
    {
      "title": "COPA: Comparing the Incomparable to Explore the Pareto Front",
      "authors": [
        "Adrián Javaloy",
        "Antonio Vergari",
        "Isabel Valera"
      ],
      "abstract": "In machine learning (ML), it is common to account for multiple objectives\nwhen, e.g., selecting a model to deploy. However, it is often unclear how one\nshould compare, aggregate and, ultimately, trade-off these objectives, as they\nmight be measured in different units or scales. For example, when deploying\nlarge language models (LLMs), we might not only care about their performance,\nbut also their CO2 consumption. In this work, we investigate how objectives can\nbe sensibly compared and aggregated to navigate their Pareto front. To do so,\nwe propose to make incomparable objectives comparable via their CDFs,\napproximated by their relative rankings. This allows us to aggregate them while\nmatching user-specific preferences, allowing practitioners to meaningfully\nnavigate and search for models in the Pareto front. We demonstrate the\npotential impact of our methodology in diverse areas such as LLM selection,\ndomain generalization, and AutoML benchmarking, where classical ways to\naggregate and normalize objectives fail.",
      "pdf_url": "http://arxiv.org/pdf/2503.14321v1",
      "published": "2025-03-18T14:51:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14321v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation",
      "authors": [
        "Baiqin Wang",
        "Xiangyu Zhu",
        "Fan Shen",
        "Hao Xu",
        "Zhen Lei"
      ],
      "abstract": "Recent advancements in audio-driven talking face generation have made great\nprogress in lip synchronization. However, current methods often lack sufficient\ncontrol over facial animation such as speaking style and emotional expression,\nresulting in uniform outputs. In this paper, we focus on improving two key\nfactors: lip-audio alignment and emotion control, to enhance the diversity and\nuser-friendliness of talking videos. Lip-audio alignment control focuses on\nelements like speaking style and the scale of lip movements, whereas emotion\ncontrol is centered on generating realistic emotional expressions, allowing for\nmodifications in multiple attributes such as intensity. To achieve precise\ncontrol of facial animation, we propose a novel framework, PC-Talk, which\nenables lip-audio alignment and emotion control through implicit keypoint\ndeformations. First, our lip-audio alignment control module facilitates precise\nediting of speaking styles at the word level and adjusts lip movement scales to\nsimulate varying vocal loudness levels, maintaining lip synchronization with\nthe audio. Second, our emotion control module generates vivid emotional facial\nfeatures with pure emotional deformation. This module also enables the fine\nmodification of intensity and the combination of multiple emotions across\ndifferent facial regions. Our method demonstrates outstanding control\ncapabilities and achieves state-of-the-art performance on both HDTF and MEAD\ndatasets in extensive experiments.",
      "pdf_url": "http://arxiv.org/pdf/2503.14295v1",
      "published": "2025-03-18T14:35:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14295v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Ensemble Knowledge Distillation for Machine Learning Interatomic Potentials",
      "authors": [
        "Sakib Matin",
        "Emily Shinkle",
        "Yulia Pimonova",
        "Galen T. Craven",
        "Ying Wai Li",
        "Kipton Barros",
        "Nicholas Lubbers"
      ],
      "abstract": "Machine learning interatomic potentials (MLIPs) are a promising tool to\naccelerate atomistic simulations and molecular property prediction. The quality\nof MLIPs strongly depends on the quantity of available training data as well as\nthe quantum chemistry (QC) level of theory used to generate that data. Datasets\ngenerated with high-fidelity QC methods, such as coupled cluster, are typically\nrestricted to small molecules and may be missing energy gradients. With this\nlimited quantity of data, it is often difficult to train good MLIP models. We\npresent an ensemble knowledge distillation (EKD) method to improve MLIP\naccuracy when trained to energy-only datasets. In our EKD approach, first,\nmultiple teacher models are trained to QC energies and then used to generate\natomic forces for all configurations in the dataset. Next, a student MLIP is\ntrained to both QC energies and to ensemble-averaged forces generated by the\nteacher models. We apply this workflow on the ANI-1ccx dataset which consists\nof organic molecules with configuration energies computed at the coupled\ncluster level of theory. The resulting student MLIPs achieve new\nstate-of-the-art accuracy on the out-of-sample COMP6 benchmark and improved\nstability for molecular dynamics simulations. The EKD approach for MLIP is\nbroadly applicable for chemical, biomolecular and materials science\nsimulations.",
      "pdf_url": "http://arxiv.org/pdf/2503.14293v1",
      "published": "2025-03-18T14:32:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14293v1",
      "categories": [
        "physics.chem-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Manual Labelling Artificially Inflates Deep Learning-Based Segmentation Performance on Closed Canopy: Validation Using TLS",
      "authors": [
        "Matthew J. Allen",
        "Harry J. F. Owen",
        "Stuart W. D. Grieve",
        "Emily R. Lines"
      ],
      "abstract": "Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.",
      "pdf_url": "http://arxiv.org/pdf/2503.14273v1",
      "published": "2025-03-18T14:09:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14273v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4; I.4.6; I.4.8; I.4.9; I.5; I.5.4"
      ]
    },
    {
      "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System",
      "authors": [
        "Weihang Su",
        "Baoqing Yue",
        "Qingyao Ai",
        "Yiran Hu",
        "Jiaqi Li",
        "Changyue Wang",
        "Kaiyuan Zhang",
        "Yueyue Wu",
        "Yiqun Liu"
      ],
      "abstract": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
      "pdf_url": "http://arxiv.org/pdf/2503.14258v1",
      "published": "2025-03-18T13:48:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14258v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented Robot Exploration",
      "authors": [
        "Chunyu Yang",
        "Shengben Bi",
        "Yihui Xu",
        "Xin Zhang"
      ],
      "abstract": "With the increasing demand for efficient and flexible robotic exploration\nsolutions, Reinforcement Learning (RL) is becoming a promising approach in the\nfield of autonomous robotic exploration. However, current RL-based exploration\nalgorithms often face limited environmental reasoning capabilities, slow\nconvergence rates, and substantial challenges in Sim-To-Real (S2R) transfer. To\naddress these issues, we propose a Curriculum Learning-based Transformer\nReinforcement Learning Algorithm (CTSAC) aimed at improving both exploration\nefficiency and transfer performance. To enhance the robot's reasoning ability,\na Transformer is integrated into the perception network of the Soft\nActor-Critic (SAC) framework, leveraging historical information to improve the\nfarsightedness of the strategy. A periodic review-based curriculum learning is\nproposed, which enhances training efficiency while mitigating catastrophic\nforgetting during curriculum transitions. Training is conducted on the\nROS-Gazebo continuous robotic simulation platform, with LiDAR clustering\noptimization to further reduce the S2R gap. Experimental results demonstrate\nthe CTSAC algorithm outperforms the state-of-the-art non-learning and\nlearning-based algorithms in terms of success rate and success rate-weighted\nexploration time. Moreover, real-world experiments validate the strong S2R\ntransfer capabilities of CTSAC.",
      "pdf_url": "http://arxiv.org/pdf/2503.14254v1",
      "published": "2025-03-18T13:44:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14254v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A Parallel Hybrid Action Space Reinforcement Learning Model for Real-world Adaptive Traffic Signal Control",
      "authors": [
        "Yuxuan Wang",
        "Meng Long",
        "Qiang Wu",
        "Wei Liu",
        "Jiatian Pi",
        "Xinmin Yang"
      ],
      "abstract": "Adaptive traffic signal control (ATSC) can effectively reduce vehicle travel\ntimes by dynamically adjusting signal timings but poses a critical challenge in\nreal-world scenarios due to the complexity of real-time decision-making in\ndynamic and uncertain traffic conditions. The burgeoning field of intelligent\ntransportation systems, bolstered by artificial intelligence techniques and\nextensive data availability, offers new prospects for the implementation of\nATSC. In this study, we introduce a parallel hybrid action space reinforcement\nlearning model (PH-DDPG) that optimizes traffic signal phase and duration of\ntraffic signals simultaneously, eliminating the need for sequential\ndecision-making seen in traditional two-stage models. Our model features a\ntask-specific parallel hybrid action space tailored for adaptive traffic\ncontrol, which directly outputs discrete phase selections and their associated\ncontinuous duration parameters concurrently, thereby inherently addressing\ndynamic traffic adaptation through unified parametric optimization. %Our model\nfeatures a unique parallel hybrid action space that allows for the simultaneous\noutput of each action and its optimal parameters, streamlining the\ndecision-making process. Furthermore, to ascertain the robustness and\neffectiveness of this approach, we executed ablation studies focusing on the\nutilization of a random action parameter mask within the critic network, which\ndecouples the parameter space for individual actions, facilitating the use of\npreferable parameters for each action. The results from these studies confirm\nthe efficacy of this method, distinctly enhancing real-world applicability",
      "pdf_url": "http://arxiv.org/pdf/2503.14250v1",
      "published": "2025-03-18T13:38:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14250v1",
      "categories": [
        "cs.AI",
        "I.2.6; I.2.8"
      ]
    },
    {
      "title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics",
      "authors": [
        "Tingyang Xiao",
        "Xiaolin Zhou",
        "Liu Liu",
        "Wei Sui",
        "Wei Feng",
        "Jiaxiong Qiu",
        "Xinjie Wang",
        "Zhizhong Su"
      ],
      "abstract": "This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robots operating in highly dynamic\nenvironments.By integrating geometric consistency, legged odometry constraints,\nand dual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/NSN-Hello/GeoFlow-SLAM",
      "pdf_url": "http://arxiv.org/pdf/2503.14247v1",
      "published": "2025-03-18T13:35:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14247v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Trading-off Accuracy and Communication Cost in Federated Learning",
      "authors": [
        "Mattia Jacopo Villani",
        "Emanuele Natale",
        "Frederik Mallmann-Trenn"
      ],
      "abstract": "Leveraging the training-by-pruning paradigm introduced by Zhou et al. and\nIsik et al. introduced a federated learning protocol that achieves a 34-fold\nreduction in communication cost. We achieve a compression improvements of\norders of orders of magnitude over the state-of-the-art. The central idea of\nour framework is to encode the network weights $\\vec w$ by a the vector of\ntrainable parameters $\\vec p$, such that $\\vec w = Q\\cdot \\vec p$ where $Q$ is\na carefully-generate sparse random matrix (that remains fixed throughout\ntraining). In such framework, the previous work of Zhou et al. [NeurIPS'19] is\nretrieved when $Q$ is diagonal and $\\vec p$ has the same dimension of $\\vec w$.\nWe instead show that $\\vec p$ can effectively be chosen much smaller than $\\vec\nw$, while retaining the same accuracy at the price of a decrease of the\nsparsity of $Q$. Since server and clients only need to share $\\vec p$, such a\ntrade-off leads to a substantial improvement in communication cost. Moreover,\nwe provide theoretical insight into our framework and establish a novel link\nbetween training-by-sampling and random convex geometry.",
      "pdf_url": "http://arxiv.org/pdf/2503.14246v1",
      "published": "2025-03-18T13:35:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14246v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented Generation Framework for Temporal Reasoning",
      "authors": [
        "Ruiyi Yang",
        "Hao Xue",
        "Imran Razzak",
        "Hakim Hacid",
        "Flora D. Salim"
      ],
      "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
      "pdf_url": "http://arxiv.org/pdf/2503.14234v2",
      "published": "2025-03-18T13:11:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14234v2",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models",
      "authors": [
        "Yuyang Xue",
        "Edward Moroshko",
        "Feng Chen",
        "Steven McDonagh",
        "Sotirios A. Tsaftaris"
      ],
      "abstract": "Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure techniques. However, existing methods struggle\nwith under-erasure, leaving residual traces of targeted concepts, or\nover-erasure, mistakenly eliminating unrelated but visually similar concepts.\nTo address these limitations, we introduce CRCE, a novel concept erasure\nframework that leverages Large Language Models to identify both semantically\nrelated concepts that should be erased alongside the target and distinct\nconcepts that should be preserved. By explicitly modeling coreferential and\nretained concepts semantically, CRCE enables more precise concept removal,\nwithout unintended erasure. Experiments demonstrate that CRCE outperforms\nexisting methods on diverse erasure tasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.14232v1",
      "published": "2025-03-18T13:09:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14232v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard",
      "authors": [
        "Yifei Dong",
        "Fengyi Wu",
        "Qi He",
        "Heng Li",
        "Minghan Li",
        "Zebang Cheng",
        "Yuxuan Zhou",
        "Jingdong Sun",
        "Qi Dai",
        "Zhi-Qi Cheng",
        "Alexander G Hauptmann"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) systems often focus on either discrete\n(panoramic) or continuous (free-motion) paradigms alone, overlooking the\ncomplexities of human-populated, dynamic environments. We introduce a unified\nHuman-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit\nsocial-awareness constraints. Our contributions include: 1. A standardized task\ndefinition that balances discrete-continuous navigation with personal-space\nrequirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded\nsimulators capturing realistic multi-human interactions, outdoor contexts, and\nrefined motion-language alignment; 3. Extensive benchmarking on 16,844\nhuman-centric instructions, revealing how multi-human dynamics and partial\nobservability pose substantial challenges for leading VLN agents; 4. Real-world\nrobot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A\npublic leaderboard supporting transparent comparisons across discrete and\ncontinuous tasks. Empirical results show improved navigation success and fewer\ncollisions when social context is integrated, underscoring the need for\nhuman-centric design. By releasing all datasets, simulators, agent code, and\nevaluation tools, we aim to advance safer, more capable, and socially\nresponsible VLN research.",
      "pdf_url": "http://arxiv.org/pdf/2503.14229v1",
      "published": "2025-03-18T13:05:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14229v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "Panoramic Distortion-Aware Tokenization for Person Detection and Localization Using Transformers in Overhead Fisheye Images",
      "authors": [
        "Nobuhiko Wakai",
        "Satoshi Sato",
        "Yasunori Ishii",
        "Takayoshi Yamashita"
      ],
      "abstract": "Person detection methods are used widely in applications including visual\nsurveillance, pedestrian detection, and robotics. However, accurate detection\nof persons from overhead fisheye images remains an open challenge because of\nfactors including person rotation and small-sized persons. To address the\nperson rotation problem, we convert the fisheye images into panoramic images.\nFor smaller people, we focused on the geometry of the panoramas. Conventional\ndetection methods tend to focus on larger people because these larger people\nyield large significant areas for feature maps. In equirectangular panoramic\nimages, we find that a person's height decreases linearly near the top of the\nimages. Using this finding, we leverage the significance values and aggregate\ntokens that are sorted based on these values to balance the significant areas.\nIn this leveraging process, we introduce panoramic distortion-aware\ntokenization. This tokenization procedure divides a panoramic image using\nself-similarity figures that enable determination of optimal divisions without\ngaps, and we leverage the maximum significant values in each tile of token\ngroups to preserve the significant areas of smaller people. To achieve higher\ndetection accuracy, we propose a person detection and localization method that\ncombines panoramic-image remapping and the tokenization procedure. Extensive\nexperiments demonstrated that our method outperforms conventional methods when\napplied to large-scale datasets.",
      "pdf_url": "http://arxiv.org/pdf/2503.14228v1",
      "published": "2025-03-18T13:05:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14228v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Stochastic Trajectory Prediction under Unstructured Constraints",
      "authors": [
        "Hao Ma",
        "Zhiqiang Pu",
        "Shijie Wang",
        "Boyin Liu",
        "Huimu Wang",
        "Yanyan Liang",
        "Jianqiang Yi"
      ],
      "abstract": "Trajectory prediction facilitates effective planning and decision-making,\nwhile constrained trajectory prediction integrates regulation into prediction.\nRecent advances in constrained trajectory prediction focus on structured\nconstraints by constructing optimization objectives. However, handling\nunstructured constraints is challenging due to the lack of differentiable\nformal definitions. To address this, we propose a novel method for constrained\ntrajectory prediction using a conditional generative paradigm, named\nControllable Trajectory Diffusion (CTD). The key idea is that any trajectory\ncorresponds to a degree of conformity to a constraint. By quantifying this\ndegree and treating it as a condition, a model can implicitly learn to predict\ntrajectories under unstructured constraints. CTD employs a pre-trained scoring\nmodel to predict the degree of conformity (i.e., a score), and uses this score\nas a condition for a conditional diffusion model to generate trajectories.\nExperimental results demonstrate that CTD achieves high accuracy on the ETH/UCY\nand SDD benchmarks. Qualitative analysis confirms that CTD ensures adherence to\nunstructured constraints and can predict trajectories that satisfy\ncombinatorial constraints.",
      "pdf_url": "http://arxiv.org/pdf/2503.14203v1",
      "published": "2025-03-18T12:27:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14203v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Driving behavior recognition via self-discovery learning",
      "authors": [
        "Yilin Wang"
      ],
      "abstract": "Autonomous driving systems require a deep understanding of human driving\nbehaviors to achieve higher intelligence and safety.Despite advancements in\ndeep learning, challenges such as long-tail distribution due to scarce samples\nand confusion from similar behaviors hinder effective driving behavior\ndetection.Existing methods often fail to address sample confusion adequately,\nas datasets frequently contain ambiguous samples that obscure unique semantic\ninformation.",
      "pdf_url": "http://arxiv.org/pdf/2503.14194v1",
      "published": "2025-03-18T12:13:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14194v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Strategic White Paper on AI Infrastructure for Particle, Nuclear, and Astroparticle Physics: Insights from JENA and EuCAIF",
      "authors": [
        "Sascha Caron",
        "Andreas Ipp",
        "Gert Aarts",
        "Gábor Bíró",
        "Daniele Bonacorsi",
        "Elena Cuoco",
        "Caterina Doglioni",
        "Tommaso Dorigo",
        "Julián García Pardiñas",
        "Stefano Giagu",
        "Tobias Golling",
        "Lukas Heinrich",
        "Ik Siong Heng",
        "Paula Gina Isar",
        "Karolos Potamianos",
        "Liliana Teodorescu",
        "John Veitch",
        "Pietro Vischia",
        "Christoph Weniger"
      ],
      "abstract": "Artificial intelligence (AI) is transforming scientific research, with deep\nlearning methods playing a central role in data analysis, simulations, and\nsignal detection across particle, nuclear, and astroparticle physics. Within\nthe JENA communities-ECFA, NuPECC, and APPEC-and as part of the EuCAIF\ninitiative, AI integration is advancing steadily. However, broader adoption\nremains constrained by challenges such as limited computational resources, a\nlack of expertise, and difficulties in transitioning from research and\ndevelopment (R&D) to production. This white paper provides a strategic roadmap,\ninformed by a community survey, to address these barriers. It outlines critical\ninfrastructure requirements, prioritizes training initiatives, and proposes\nfunding strategies to scale AI capabilities across fundamental physics over the\nnext five years.",
      "pdf_url": "http://arxiv.org/pdf/2503.14192v1",
      "published": "2025-03-18T12:11:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14192v1",
      "categories": [
        "astro-ph.IM",
        "astro-ph.HE",
        "cs.AI",
        "cs.LG",
        "hep-ex",
        "hep-ph",
        "nucl-th"
      ]
    },
    {
      "title": "Inferring Event Descriptions from Time Series with Language Models",
      "authors": [
        "Mingtian Tan",
        "Mike A. Merrill",
        "Zack Gottesman",
        "Tim Althoff",
        "David Evans",
        "Tom Hartvigsen"
      ],
      "abstract": "Time series data measure how environments change over time and drive\ndecision-making in critical domains like finance and healthcare. When analyzing\ntime series, we often seek to understand the underlying events occurring in the\nmeasured environment. For example, one might ask: What caused a sharp drop in\nthe stock price? Events are often described with natural language, so we\nconduct the first study of whether Large Language Models (LLMs) can infer\nnatural language events from time series. We curate a new benchmark featuring\nwin probabilities collected from 4,200 basketball and American football games,\nfeaturing 1.7M timesteps with real value data and corresponding natural\nlanguage events. Building on the recent wave of using LLMs on time series, we\nevaluate 16 LLMs and find that they demonstrate promising abilities to infer\nevents from time series data. The open-weights DeepSeek-R1 32B model\noutperforms proprietary models like GPT-4o. Despite this impressive initial\nperformance, we also find clear avenues to improve recent models, as we\nidentify failures when altering the provided context, event sequence lengths,\nand evaluation strategy. (All resources needed to reproduce our work are\navailable: https://github.com/BennyTMT/GAMETime)",
      "pdf_url": "http://arxiv.org/pdf/2503.14190v1",
      "published": "2025-03-18T12:07:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14190v1",
      "categories": [
        "cs.AI",
        "62M10, 68T07,",
        "I.2.6; I.2.7"
      ]
    },
    {
      "title": "Variable Time-Step MPC for Agile Multi-Rotor UAV Interception of Dynamic Targets",
      "authors": [
        "Atharva Ghotavadekar",
        "František Nekovář",
        "Martin Saska",
        "Jan Faigl"
      ],
      "abstract": "Agile trajectory planning can improve the efficiency of multi-rotor Uncrewed\nAerial Vehicles (UAVs) in scenarios with combined task-oriented and kinematic\ntrajectory planning, such as monitoring spatio-temporal phenomena or\nintercepting dynamic targets. Agile planning using existing non-linear model\npredictive control methods is limited by the number of planning steps as it\nbecomes increasingly computationally demanding. That reduces the prediction\nhorizon length, leading to a decrease in solution quality. Besides, the fixed\ntime-step length limits the utilization of the available UAV dynamics in the\ntarget neighborhood. In this paper, we propose to address these limitations by\nintroducing variable time steps and coupling them with the prediction horizon\nlength. A simplified point-mass motion primitive is used to leverage the\ndifferential flatness of quadrotor dynamics and the generation of feasible\ntrajectories in the flat output space. Based on the presented evaluation\nresults and experimentally validated deployment, the proposed method increases\nthe solution quality by enabling planning for long flight segments but allowing\ntightly sampled maneuvering.",
      "pdf_url": "http://arxiv.org/pdf/2503.14184v1",
      "published": "2025-03-18T11:59:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14184v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Can LLMs Enable Verification in Mainstream Programming?",
      "authors": [
        "Aleksandr Shefer",
        "Igor Engel",
        "Stanislav Alekseev",
        "Daniil Berezun",
        "Ekaterina Verbitskaia",
        "Anton Podkopaev"
      ],
      "abstract": "Although formal methods are capable of producing reliable software, they have\nseen minimal adoption in everyday programming. Automatic code generation using\nlarge language models is becoming increasingly widespread, but it rarely\nconsiders producing strong correctness guarantees. In this study, we explore\nthe ability of LLMs to produce verified code in three verification languages\n(Dafny, Nagini, and Verus). To do so, we use manually curated datasets derived\nfrom the state-ofthe-art Python benchmark, HumanEval. We also assess what types\nof information are sufficient to achieve good-quality results.",
      "pdf_url": "http://arxiv.org/pdf/2503.14183v1",
      "published": "2025-03-18T11:58:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14183v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ]
    },
    {
      "title": "EIAD: Explainable Industrial Anomaly Detection Via Multi-Modal Large Language Models",
      "authors": [
        "Zongyun Zhang",
        "Jiacheng Ruan",
        "Xian Gao",
        "Ting Liu",
        "Yuzhuo Fu"
      ],
      "abstract": "Industrial Anomaly Detection (IAD) is critical to ensure product quality\nduring manufacturing. Although existing zero-shot defect segmentation and\ndetection methods have shown effectiveness, they cannot provide detailed\ndescriptions of the defects. Furthermore, the application of large multi-modal\nmodels in IAD remains in its infancy, facing challenges in balancing\nquestion-answering (QA) performance and mask-based grounding capabilities,\noften owing to overfitting during the fine-tuning process. To address these\nchallenges, we propose a novel approach that introduces a dedicated multi-modal\ndefect localization module to decouple the dialog functionality from the core\nfeature extraction. This decoupling is achieved through independent\noptimization objectives and tailored learning strategies. Additionally, we\ncontribute to the first multi-modal industrial anomaly detection training\ndataset, named Defect Detection Question Answering (DDQA), encompassing a wide\nrange of defect types and industrial scenarios. Unlike conventional datasets\nthat rely on GPT-generated data, DDQA ensures authenticity and reliability and\noffers a robust foundation for model training. Experimental results demonstrate\nthat our proposed method, Explainable Industrial Anomaly Detection Assistant\n(EIAD), achieves outstanding performance in defect detection and localization\ntasks. It not only significantly enhances accuracy but also improves\ninterpretability. These advancements highlight the potential of EIAD for\npractical applications in industrial settings.",
      "pdf_url": "http://arxiv.org/pdf/2503.14162v1",
      "published": "2025-03-18T11:33:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14162v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
      "authors": [
        "Yong Zhong",
        "Zhuoyi Yang",
        "Jiayan Teng",
        "Xiaotao Gu",
        "Chongxuan Li"
      ],
      "abstract": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
      "pdf_url": "http://arxiv.org/pdf/2503.14151v1",
      "published": "2025-03-18T11:17:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14151v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The Role of Datasets, Architectures, and Loss Functions",
      "authors": [
        "Siddharth D Jaiswal",
        "Sagnik Basu",
        "Sandipan Sikdar",
        "Animesh Mukherjee"
      ],
      "abstract": "Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment.",
      "pdf_url": "http://arxiv.org/pdf/2503.14138v1",
      "published": "2025-03-18T11:04:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14138v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "CARE: A QLoRA-Fine Tuned Multi-Domain Chatbot With Fast Learning On Minimal Hardware",
      "authors": [
        "Ankit Dutta",
        "Nabarup Ghosh",
        "Ankush Chatterjee"
      ],
      "abstract": "Large Language models have demonstrated excellent domain-specific\nquestion-answering capabilities when finetuned with a particular dataset of\nthat specific domain. However, fine-tuning the models requires a significant\namount of training time and a considerable amount of hardware. In this work, we\npropose CARE (Customer Assistance and Response Engine), a lightweight model\nmade by fine-tuning Phi3.5-mini on very minimal hardware and data, designed to\nhandle queries primarily across three domains: telecommunications support,\nmedical support, and banking support. For telecommunications and banking, the\nchatbot addresses issues and problems faced by customers regularly in the\nabove-mentioned domains. In the medical domain, CARE provides preliminary\nsupport by offering basic diagnoses and medical suggestions that a user might\ntake before consulting a healthcare professional. Since CARE is built on\nPhi3.5-mini, it can be used even on mobile devices, increasing its usability.\nOur research also shows that CARE performs relatively well on various medical\nbenchmarks, indicating that it can be used to make basic medical suggestions.",
      "pdf_url": "http://arxiv.org/pdf/2503.14136v1",
      "published": "2025-03-18T10:58:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14136v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Inference-Time Intervention in Large Language Models for Reliable Requirement Verification",
      "authors": [
        "Paul Darm",
        "James Xie",
        "Annalisa Riccardi"
      ],
      "abstract": "Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set.",
      "pdf_url": "http://arxiv.org/pdf/2503.14130v1",
      "published": "2025-03-18T10:49:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14130v1",
      "categories": [
        "cs.AI",
        "cs.SE",
        "H.4.2; I.2.1; I.2.7"
      ]
    },
    {
      "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
      "authors": [
        "Defa Zhu",
        "Hongzhi Huang",
        "Jundong Zhou",
        "Zihao Huang",
        "Yutao Zeng",
        "Banggu Wu",
        "Qiyang Min",
        "Xun Zhou"
      ],
      "abstract": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
      "pdf_url": "http://arxiv.org/pdf/2503.14125v1",
      "published": "2025-03-18T10:37:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14125v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Operational Change Detection for Geographical Information: Overview and Challenges",
      "authors": [
        "Nicolas Gonthier"
      ],
      "abstract": "Rapid evolution of territories due to climate change and human impact\nrequires prompt and effective updates to geospatial databases maintained by the\nNational Mapping Agency. This paper presents a comprehensive overview of change\ndetection methods tailored for the operational updating of large-scale\ngeographic databases. This review first outlines the fundamental definition of\nchange, emphasizing its multifaceted nature, from temporal to semantic\ncharacterization. It categorizes automatic change detection methods into four\nmain families: rule-based, statistical, machine learning, and simulation\nmethods. The strengths, limitations, and applicability of every family are\ndiscussed in the context of various input data. Then, key applications for\nNational Mapping Agencies are identified, particularly the optimization of\ngeospatial database updating, change-based phenomena, and dynamics monitoring.\nFinally, the paper highlights the current challenges for leveraging change\ndetection such as the variability of change definition, the missing of relevant\nlarge-scale datasets, the diversity of input data, the unstudied no-change\ndetection, the human in the loop integration and the operational constraints.\nThe discussion underscores the necessity for ongoing innovation in change\ndetection techniques to address the future needs of geographic information\nsystems for national mapping agencies.",
      "pdf_url": "http://arxiv.org/pdf/2503.14109v1",
      "published": "2025-03-18T10:25:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.14109v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}