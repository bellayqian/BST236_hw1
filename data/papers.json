{
  "last_updated": "2025-07-01T01:00:21.452330",
  "papers": [
    {
      "title": "CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings",
      "authors": [
        "Randeep Bhatia",
        "Nikos Papadis",
        "Murali Kodialam",
        "TV Lakshman",
        "Sayak Chakrabarty"
      ],
      "abstract": "We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm\nfor Clustered Federated Learning (CFL). In CFL, clients are naturally grouped\ninto clusters based on their data distribution. However, identifying these\nclusters is challenging, as client assignments are unknown. CLoVE utilizes\nclient embeddings derived from model losses on client data, and leverages the\ninsight that clients in the same cluster share similar loss values, while those\nin different clusters exhibit distinct loss patterns. Based on these\nembeddings, CLoVE is able to iteratively identify and separate clients from\ndifferent clusters and optimize cluster-specific models through federated\naggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its\nsimplicity, (2) its applicability to both supervised and unsupervised settings,\nand (3) the fact that it eliminates the need for near-optimal model\ninitialization, which makes it more robust and better suited for real-world\napplications. We establish theoretical convergence bounds, showing that CLoVE\ncan recover clusters accurately with high probability in a single round and\nconverges exponentially fast to optimal models in a linear setting. Our\ncomprehensive experiments comparing with a variety of both CFL and generic\nPersonalized Federated Learning (PFL) algorithms on different types of datasets\nand an extensive array of non-IID settings demonstrate that CLoVE achieves\nhighly accurate cluster recovery in just a few rounds of training, along with\nstate-of-the-art model accuracy, across a variety of both supervised and\nunsupervised PFL tasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.22427v1",
      "published": "2025-06-27T17:52:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22427v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
      "authors": [
        "Bingchen Zhao",
        "Despoina Magka",
        "Minqi Jiang",
        "Xian Li",
        "Roberta Raileanu",
        "Tatiana Shavrina",
        "Jean-Christophe Gagnon-Audet",
        "Kelvin Niu",
        "Shagun Sodhani",
        "Michael Shvartsman",
        "Andrei Lupu",
        "Alisia Lupidi",
        "Edan Toledo",
        "Karen Hambardzumyan",
        "Martin Josifoski",
        "Thomas Foster",
        "Lucia Cipolina-Kun",
        "Abhishek Charnalia",
        "Derek Dunfield",
        "Alexander H. Miller",
        "Oisin Mac Aodha",
        "Jakob Foerster",
        "Yoram Bachrach"
      ],
      "abstract": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
      "pdf_url": "http://arxiv.org/pdf/2506.22419v1",
      "published": "2025-06-27T17:44:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22419v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "HyperCLOVA X THINK Technical Report",
      "authors": [
        "NAVER Cloud HyperCLOVA X Team"
      ],
      "abstract": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community.",
      "pdf_url": "http://arxiv.org/pdf/2506.22403v1",
      "published": "2025-06-27T17:23:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22403v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism",
      "authors": [
        "Anirban Ray",
        "Ashesh",
        "Florian Jug"
      ],
      "abstract": "Fluorescence microscopy is a major driver of scientific progress in the life\nsciences. Although high-end confocal microscopes are capable of filtering\nout-of-focus light, cheaper and more accessible microscopy modalities, such as\nwidefield microscopy, can not, which consequently leads to hazy image data.\nComputational dehazing is trying to combine the best of both worlds, leading to\ncheap microscopy but crisp-looking images. The perception-distortion trade-off\ntells us that we can optimize either for data fidelity, e.g. low MSE or high\nPSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.\nExisting methods either prioritize fidelity at the expense of realism, or\nproduce perceptually convincing results that lack quantitative accuracy. In\nthis work, we propose HazeMatching, a novel iterative method for dehazing light\nmicroscopy images, which effectively balances these objectives. Our goal was to\nfind a balanced trade-off between the fidelity of the dehazing results and the\nrealism of individual predictions (samples). We achieve this by adapting the\nconditional flow matching framework by guiding the generative process with a\nhazy observation in the conditional velocity field. We evaluate HazeMatching on\n5 datasets, covering both synthetic and real data, assessing both distortion\nand perceptual quality. Our method is compared against 7 baselines, achieving a\nconsistent balance between fidelity and realism on average. Additionally, with\ncalibration analysis, we show that HazeMatching produces well-calibrated\npredictions. Note that our method does not need an explicit degradation\noperator to exist, making it easily applicable on real microscopy data. All\ndata used for training and evaluation and our code will be publicly available\nunder a permissive license.",
      "pdf_url": "http://arxiv.org/pdf/2506.22397v1",
      "published": "2025-06-27T17:10:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22397v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
      "authors": [
        "Danush Khanna",
        "Aditya Kumar Guru",
        "Srivarshinee Sridhar",
        "Zidan Ahmed",
        "Rubhav Bahirwani",
        "Meetu Malhotra",
        "Vinija Jain",
        "Aman Chadha",
        "Amitava Das",
        "Kripabandhu Ghosh"
      ],
      "abstract": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
      "pdf_url": "http://arxiv.org/pdf/2506.22396v1",
      "published": "2025-06-27T17:10:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22396v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.0; I.2.7"
      ]
    },
    {
      "title": "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis",
      "authors": [
        "YongKyung Oh",
        "Alex Bui"
      ],
      "abstract": "Adapting machine learning models to medical time series across different\ndomains remains a challenge due to complex temporal dependencies and dynamic\ndistribution shifts. Current approaches often focus on isolated feature\nrepresentations, limiting their ability to fully capture the intricate temporal\ndynamics necessary for robust domain adaptation. In this work, we propose a\nnovel framework leveraging multi-view contrastive learning to integrate\ntemporal patterns, derivative-based dynamics, and frequency-domain features.\nOur method employs independent encoders and a hierarchical fusion mechanism to\nlearn feature-invariant representations that are transferable across domains\nwhile preserving temporal coherence. Extensive experiments on diverse medical\ndatasets, including electroencephalogram (EEG), electrocardiogram (ECG), and\nelectromyography (EMG) demonstrate that our approach significantly outperforms\nstate-of-the-art methods in transfer learning tasks. By advancing the\nrobustness and generalizability of machine learning models, our framework\noffers a practical pathway for deploying reliable AI systems in diverse\nhealthcare settings.",
      "pdf_url": "http://arxiv.org/pdf/2506.22393v1",
      "published": "2025-06-27T17:06:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22393v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Distributed Neural Architectures",
      "authors": [
        "Aditya Cowsik",
        "Tianyu He",
        "Andrey Gromov"
      ],
      "abstract": "We introduce and train distributed neural architectures (DNA) in vision and\nlanguage domains. DNAs are initialized with a proto-architecture that consists\nof (transformer, MLP, attention, etc.) modules and routers. Any token (or\npatch) can traverse any series of modules in any order. DNAs are a natural\ngeneralization of the sparse methods such as Mixture-of-Experts,\nMixture-of-Depths, parameter sharing, etc. Computation and communication\npatterns of DNA modules are learnt end-to-end during training and depend on the\ncontent and context of each token (or patch). These patterns can be shaped by\nfurther requirements added to the optimization objective such as compute/memory\nefficiency or load balancing. We empirically show that (i) trained DNAs are\ncompetitive with the dense baselines in both domains and (ii) compute\nefficiency/parameter sharing can be learnt from data. Next, we analyze the\nemergent connectivity and computation patterns in the trained DNAs. We find\nthat the paths that tokens take through the models are themselves distributed\naccording to a power-law. We show that some paths (or, equivalently, groups of\nmodules) show emergent specialization. Finally, we demonstrate that models\nlearn to allocate compute and active parameters in an interpretable way.",
      "pdf_url": "http://arxiv.org/pdf/2506.22389v1",
      "published": "2025-06-27T16:57:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22389v1",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI"
      ]
    },
    {
      "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment",
      "authors": [
        "Yue Zhang",
        "Jilei Sun",
        "Yunhui Guo",
        "Vibhav Gogate"
      ],
      "abstract": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.",
      "pdf_url": "http://arxiv.org/pdf/2506.22385v1",
      "published": "2025-06-27T16:51:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22385v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Probabilistic Optimality for Inference-time Scaling",
      "authors": [
        "Youkang Wang",
        "Jian Wang",
        "Rubing Chen",
        "Xiao-Yong Wei",
        "Qing Li"
      ],
      "abstract": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2506.22376v1",
      "published": "2025-06-27T16:44:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22376v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems",
      "authors": [
        "Abdulmomen Ghalkha",
        "Zhuojun Tian",
        "Chaouki Ben Issaid",
        "Mehdi Bennis"
      ],
      "abstract": "In large-scale communication systems, increasingly complex scenarios require\nmore intelligent collaboration among edge devices collecting various multimodal\nsensory data to achieve a more comprehensive understanding of the environment\nand improve decision-making accuracy. However, conventional federated learning\n(FL) algorithms typically consider unimodal datasets, require identical model\narchitectures, and fail to leverage the rich information embedded in multimodal\ndata, limiting their applicability to real-world scenarios with diverse\nmodalities and varying client capabilities. To address this issue, we propose\nSheaf-DMFL, a novel decentralized multimodal learning framework leveraging\nsheaf theory to enhance collaboration among devices with diverse modalities.\nSpecifically, each client has a set of local feature encoders for its different\nmodalities, whose outputs are concatenated before passing through a\ntask-specific layer. While encoders for the same modality are trained\ncollaboratively across clients, we capture the intrinsic correlations among\nclients' task-specific layers using a sheaf-based structure. To further enhance\nlearning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,\nwhich tailors the attention mechanism within each client to capture\ncorrelations among different modalities. A rigorous convergence analysis of\nSheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive\nsimulations are conducted on real-world link blockage prediction and mmWave\nbeamforming scenarios, demonstrate the superiority of the proposed algorithms\nin such heterogeneous wireless communication systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.22374v1",
      "published": "2025-06-27T16:41:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22374v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications",
      "authors": [
        "Nouf Almesafri",
        "Hector Figueiredo",
        "Miguel Arana-Catania"
      ],
      "abstract": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.22360v1",
      "published": "2025-06-27T16:21:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22360v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models",
      "authors": [
        "Viswanath Kumarskandpriya",
        "Abdulhalim Dandoush",
        "Abbas Bradai",
        "Ali Belgacem"
      ],
      "abstract": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management.",
      "pdf_url": "http://arxiv.org/pdf/2506.22359v1",
      "published": "2025-06-27T16:20:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22359v1",
      "categories": [
        "cs.NI",
        "cs.AI"
      ]
    },
    {
      "title": "AI Model Passport: Data and System Traceability Framework for Transparent AI in Health",
      "authors": [
        "Varvara Kalokyri",
        "Nikolaos S. Tachos",
        "Charalampos N. Kalantzopoulos",
        "Stelios Sfakianakis",
        "Haridimos Kondylakis",
        "Dimitrios I. Zaridis",
        "Sara Colantonio",
        "Daniele Regge",
        "Nikolaos Papanikolaou",
        "The ProCAncer-I consortium",
        "Konstantinos Marias",
        "Dimitrios I. Fotiadis",
        "Manolis Tsiknakis"
      ],
      "abstract": "The increasing integration of Artificial Intelligence (AI) into health and\nbiomedical systems necessitates robust frameworks for transparency,\naccountability, and ethical compliance. Existing frameworks often rely on\nhuman-readable, manual documentation which limits scalability, comparability,\nand machine interpretability across projects and platforms. They also fail to\nprovide a unique, verifiable identity for AI models to ensure their provenance\nand authenticity across systems and use cases, limiting reproducibility and\nstakeholder trust. This paper introduces the concept of the AI Model Passport,\na structured and standardized documentation framework that acts as a digital\nidentity and verification tool for AI models. It captures essential metadata to\nuniquely identify, verify, trace and monitor AI models across their lifecycle -\nfrom data acquisition and preprocessing to model design, development and\ndeployment. In addition, an implementation of this framework is presented\nthrough AIPassport, an MLOps tool developed within the ProCAncer-I EU project\nfor medical imaging applications. AIPassport automates metadata collection,\nensures proper versioning, decouples results from source scripts, and\nintegrates with various development environments. Its effectiveness is\nshowcased through a lesion segmentation use case using data from the\nProCAncer-I dataset, illustrating how the AI Model Passport enhances\ntransparency, reproducibility, and regulatory readiness while reducing manual\neffort. This approach aims to set a new standard for fostering trust and\naccountability in AI-driven healthcare solutions, aspiring to serve as the\nbasis for developing transparent and regulation compliant AI systems across\ndomains.",
      "pdf_url": "http://arxiv.org/pdf/2506.22358v1",
      "published": "2025-06-27T16:16:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22358v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Embodied AI Agents: Modeling the World",
      "authors": [
        "Pascale Fung",
        "Yoram Bachrach",
        "Asli Celikyilmaz",
        "Kamalika Chaudhuri",
        "Delong Chen",
        "Willy Chung",
        "Emmanuel Dupoux",
        "Hervé Jégou",
        "Alessandro Lazaric",
        "Arjun Majumdar",
        "Andrea Madotto",
        "Franziska Meier",
        "Florian Metze",
        "Théo Moutakanni",
        "Juan Pino",
        "Basile Terver",
        "Joseph Tighe",
        "Jitendra Malik"
      ],
      "abstract": "This paper describes our research on AI agents embodied in visual, virtual or\nphysical forms, enabling them to interact with both users and their\nenvironments. These agents, which include virtual avatars, wearable devices,\nand robots, are designed to perceive, learn and act within their surroundings,\nwhich makes them more similar to how humans learn and interact with the\nenvironments as compared to disembodied agents. We propose that the development\nof world models is central to reasoning and planning of embodied AI agents,\nallowing these agents to understand and predict their environment, to\nunderstand user intentions and social contexts, thereby enhancing their ability\nto perform complex tasks autonomously. World modeling encompasses the\nintegration of multimodal perception, planning through reasoning for action and\ncontrol, and memory to create a comprehensive understanding of the physical\nworld. Beyond the physical world, we also propose to learn the mental world\nmodel of users to enable better human-agent collaboration.",
      "pdf_url": "http://arxiv.org/pdf/2506.22355v1",
      "published": "2025-06-27T16:05:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22355v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A Framework for Multi-source Privacy Preserving Epidemic Analysis",
      "authors": [
        "Zihan Guan",
        "Zhiyuan Zhao",
        "Fengwei Tian",
        "Dung Nguyen",
        "Payel Bhattacharjee",
        "Ravi Tandon",
        "B. Aditya Prakash",
        "Anil Vullikanti"
      ],
      "abstract": "It is now well understood that diverse datasets provide a lot of value in key\nepidemiology and public health analyses, such as forecasting and nowcasting,\ndevelopment of epidemic models, evaluation and design of interventions and\nresource allocation. Some of these datasets are often sensitive, and need\nadequate privacy protections. There are many models of privacy, but\nDifferential Privacy (DP) has become a de facto standard because of its strong\nguarantees, without making models about adversaries. In this paper, we develop\na framework the integrates deep learning and epidemic models to simultaneously\nperform epidemic forecasting and learning a mechanistic model of epidemic\nspread, while incorporating multiple datasets for these analyses, including\nsome with DP guarantees. We demonstrate our framework using a realistic but\nsynthetic financial dataset with DP; such a dataset has not been used in such\nepidemic analyses. We show that this dataset provides significant value in\nforecasting and learning an epidemic model, even when used with DP guarantees.",
      "pdf_url": "http://arxiv.org/pdf/2506.22342v1",
      "published": "2025-06-27T15:52:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22342v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake",
      "authors": [
        "Luigi Russo",
        "Deodato Tapete",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building damage identification shortly after a disaster is crucial for\nguiding emergency response and recovery efforts. Although optical satellite\nimagery is commonly used for disaster mapping, its effectiveness is often\nhampered by cloud cover or the absence of pre-event acquisitions. To overcome\nthese challenges, we introduce a novel multimodal deep learning (DL) framework\nfor detecting building damage using single-date very high resolution (VHR)\nSynthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)\nCOSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.\nOur method integrates SAR image patches, OpenStreetMap (OSM) building\nfootprints, digital surface model (DSM) data, and structural and exposure\nattributes from the Global Earthquake Model (GEM) to improve detection accuracy\nand contextual interpretation. Unlike existing approaches that depend on pre\nand post event imagery, our model utilizes only post event data, facilitating\nrapid deployment in critical scenarios. The framework effectiveness is\ndemonstrated using a new dataset from the 2023 earthquake in Turkey, covering\nmultiple cities with diverse urban settings. Results highlight that\nincorporating geospatial features significantly enhances detection performance\nand generalizability to previously unseen areas. By combining SAR imagery with\ndetailed vulnerability and exposure information, our approach provides reliable\nand rapid building damage assessments without the dependency from available\npre-event data. Moreover, the automated and scalable data generation process\nensures the framework's applicability across diverse disaster-affected regions,\nunderscoring its potential to support effective disaster management and\nrecovery efforts. Code and data will be made available upon acceptance of the\npaper.",
      "pdf_url": "http://arxiv.org/pdf/2506.22338v1",
      "published": "2025-06-27T15:49:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22338v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Less Greedy Equivalence Search",
      "authors": [
        "Adiba Ejaz",
        "Elias Bareinboim"
      ],
      "abstract": "Greedy Equivalence Search (GES) is a classic score-based algorithm for causal\ndiscovery from observational data. In the sample limit, it recovers the Markov\nequivalence class of graphs that describe the data. Still, it faces two\nchallenges in practice: computational cost and finite-sample accuracy. In this\npaper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that\nretains its theoretical guarantees while partially addressing these\nlimitations. LGES modifies the greedy step: rather than always applying the\nhighest-scoring insertion, it avoids edge insertions between variables for\nwhich the score implies some conditional independence. This more targeted\nsearch yields up to a \\(10\\)-fold speed-up and a substantial reduction in\nstructural error relative to GES. Moreover, LGES can guide the search using\nprior assumptions, while correcting these assumptions when contradicted by the\ndata. Finally, LGES can exploit interventional data to refine the learned\nobservational equivalence class. We prove that LGES recovers the true\nequivalence class in the sample limit from observational and interventional\ndata, even with misspecified prior assumptions. Experiments demonstrate that\nLGES outperforms GES and other baselines in speed, accuracy, and robustness to\nmisspecified assumptions. Our code is available at\nhttps://github.com/CausalAILab/lges.",
      "pdf_url": "http://arxiv.org/pdf/2506.22331v1",
      "published": "2025-06-27T15:39:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22331v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension",
      "authors": [
        "Tarikul Islam Tamiti",
        "Anomadarshi Barua"
      ],
      "abstract": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB.",
      "pdf_url": "http://arxiv.org/pdf/2506.22321v1",
      "published": "2025-06-27T15:35:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22321v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Conceptual Topic Aggregation",
      "authors": [
        "Klara M. Gutekunst",
        "Dominik Dürrschnabel",
        "Johannes Hirth",
        "Gerd Stumme"
      ],
      "abstract": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.",
      "pdf_url": "http://arxiv.org/pdf/2506.22309v1",
      "published": "2025-06-27T15:19:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22309v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DM",
        "cs.LG",
        "06B99",
        "I.2.4; I.2.7"
      ]
    },
    {
      "title": "CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks",
      "authors": [
        "Tao Liu",
        "Longlong Lin",
        "Yunfeng Yu",
        "Xi Ou",
        "Youan Zhang",
        "Zhiqiu Ye",
        "Tao Jia"
      ],
      "abstract": "Graph Neural Networks (GNNs) have garnered substantial attention due to their\nremarkable capability in learning graph representations. However, real-world\ngraphs often exhibit substantial noise and incompleteness, which severely\ndegrades the performance of GNNs. Existing methods typically address this issue\nthrough single-dimensional augmentation, focusing either on refining topology\nstructures or perturbing node attributes, thereby overlooking the deeper\ninterplays between the two. To bridge this gap, this paper presents CoATA, a\ndual-channel GNN framework specifically designed for the Co-Augmentation of\nTopology and Attribute. Specifically, CoATA first propagates structural signals\nto enrich and denoise node attributes. Then, it projects the enhanced attribute\nspace into a node-attribute bipartite graph for further refinement or\nreconstruction of the underlying structure. Subsequently, CoATA introduces\ncontrastive learning, leveraging prototype alignment and consistency\nconstraints, to facilitate mutual corrections between the augmented and\noriginal graphs. Finally, extensive experiments on seven benchmark datasets\ndemonstrate that the proposed CoATA outperforms eleven state-of-the-art\nbaseline methods, showcasing its effectiveness in capturing the synergistic\nrelationship between topology and attributes.",
      "pdf_url": "http://arxiv.org/pdf/2506.22299v1",
      "published": "2025-06-27T15:11:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22299v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2"
      ]
    },
    {
      "title": "RoomCraft: Controllable and Complete 3D Indoor Scene Generation",
      "authors": [
        "Mengqi Zhou",
        "Xipeng Wang",
        "Yuxi Wang",
        "Zhaoxiang Zhang"
      ],
      "abstract": "Generating realistic 3D indoor scenes from user inputs remains a challenging\nproblem in computer vision and graphics, requiring careful balance of geometric\nconsistency, spatial relationships, and visual realism. While neural generation\nmethods often produce repetitive elements due to limited global spatial\nreasoning, procedural approaches can leverage constraints for controllable\ngeneration but struggle with multi-constraint scenarios. When constraints\nbecome numerous, object collisions frequently occur, forcing the removal of\nfurniture items and compromising layout completeness.\n  To address these limitations, we propose RoomCraft, a multi-stage pipeline\nthat converts real images, sketches, or text descriptions into coherent 3D\nindoor scenes. Our approach combines a scene generation pipeline with a\nconstraint-driven optimization framework. The pipeline first extracts\nhigh-level scene information from user inputs and organizes it into a\nstructured format containing room type, furniture items, and spatial relations.\nIt then constructs a spatial relationship network to represent furniture\narrangements and generates an optimized placement sequence using a\nheuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.\nTo handle complex multi-constraint scenarios, we introduce a unified constraint\nrepresentation that processes both formal specifications and natural language\ninputs, enabling flexible constraint-oriented adjustments through a\ncomprehensive action space design. Additionally, we propose a Conflict-Aware\nPositioning Strategy (CAPS) that dynamically adjusts placement weights to\nminimize furniture collisions and ensure layout completeness.\n  Extensive experiments demonstrate that RoomCraft significantly outperforms\nexisting methods in generating realistic, semantically coherent, and visually\nappealing room layouts across diverse input modalities.",
      "pdf_url": "http://arxiv.org/pdf/2506.22291v1",
      "published": "2025-06-27T15:03:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22291v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates",
      "authors": [
        "Reuth Mirsky"
      ],
      "abstract": "Artificial intelligence has made remarkable strides in recent years,\nachieving superhuman performance across a wide range of tasks. Yet despite\nthese advances, most cooperative AI systems remain rigidly obedient, designed\nto follow human instructions without question and conform to user expectations,\neven when doing so may be counterproductive or unsafe. This paper argues for\nexpanding the agency of AI teammates to include \\textit{intelligent\ndisobedience}, empowering them to make meaningful and autonomous contributions\nwithin human-AI teams. It introduces a scale of AI agency levels and uses\nrepresentative examples to highlight the importance and growing necessity of\ntreating AI autonomy as an independent research focus in cooperative settings.\nThe paper then explores how intelligent disobedience manifests across different\nautonomy levels and concludes by proposing initial boundaries and\nconsiderations for studying disobedience as a core capability of artificial\nagents.",
      "pdf_url": "http://arxiv.org/pdf/2506.22276v1",
      "published": "2025-06-27T14:45:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22276v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Breaking Rank Bottlenecks in Knowledge Graph Completion",
      "authors": [
        "Samy Badreddine",
        "Emile van Krieken",
        "Luciano Serafini"
      ],
      "abstract": "Many Knowledge Graph Completion (KGC) models, despite using powerful\nencoders, rely on a simple vector-matrix multiplication to score queries\nagainst candidate object entities. When the number of entities is larger than\nthe model's embedding dimension, which in practical scenarios is often by\nseveral orders of magnitude, we have a linear output layer with a rank\nbottleneck. Such bottlenecked layers limit model expressivity. We investigate\nboth theoretically and empirically how rank bottlenecks affect KGC models. We\nfind that, by limiting the set of feasible predictions, rank bottlenecks hurt\nranking accuracy and the distribution fidelity of scores. Inspired by the\nlanguage modelling literature, we propose KGE-MoS, a mixture-based output layer\nto break rank bottlenecks in many KGC models. Our experiments on four datasets\nshow that KGE-MoS improves performance and probabilistic fit of KGC models for\na low parameter cost.",
      "pdf_url": "http://arxiv.org/pdf/2506.22271v1",
      "published": "2025-06-27T14:41:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22271v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression",
      "authors": [
        "Maciej Stefaniak",
        "Michał Krutul",
        "Jan Małaśnicki",
        "Maciej Pióro",
        "Jakub Krajewski",
        "Sebastian Jaszczur",
        "Marek Cygan",
        "Kamil Adamczewski",
        "Jan Ludziejewski"
      ],
      "abstract": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.",
      "pdf_url": "http://arxiv.org/pdf/2506.22255v1",
      "published": "2025-06-27T14:24:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22255v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education",
      "authors": [
        "Russell Beale"
      ],
      "abstract": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.",
      "pdf_url": "http://arxiv.org/pdf/2506.22231v1",
      "published": "2025-06-27T13:49:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22231v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "K.3.1; K.3.2; K.6.0"
      ]
    },
    {
      "title": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework",
      "authors": [
        "Chen Wang",
        "Lai Wei",
        "Yanzhi Zhang",
        "Chenyang Shao",
        "Zedong Dan",
        "Weiran Huang",
        "Yue Wang",
        "Yuzhi Zhang"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.",
      "pdf_url": "http://arxiv.org/pdf/2506.22200v1",
      "published": "2025-06-27T13:09:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22200v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration",
      "authors": [
        "Matteo Esposito",
        "Alexander Bakhtin",
        "Noman Ahmad",
        "Mikel Robredo",
        "Ruoyu Su",
        "Valentina Lenarduzzi",
        "Davide Taibi"
      ],
      "abstract": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.",
      "pdf_url": "http://arxiv.org/pdf/2506.22185v1",
      "published": "2025-06-27T12:46:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22185v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.DC",
        "cs.NI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety",
      "authors": [
        "Camille François",
        "Ludovic Péran",
        "Ayah Bdeir",
        "Nouha Dziri",
        "Will Hawkins",
        "Yacine Jernite",
        "Sayash Kapoor",
        "Juliet Shen",
        "Heidy Khlaaf",
        "Kevin Klyman",
        "Nik Marda",
        "Marie Pellat",
        "Deb Raji",
        "Divya Siddarth",
        "Aviya Skowron",
        "Joseph Spisak",
        "Madhulika Srikumar",
        "Victor Storchan",
        "Audrey Tang",
        "Jen Weedon"
      ],
      "abstract": "The rapid rise of open-weight and open-source foundation models is\nintensifying the obligation and reshaping the opportunity to make AI systems\nsafe. This paper reports outcomes from the Columbia Convening on AI Openness\nand Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme\ninvolving more than forty-five researchers, engineers, and policy leaders from\nacademia, industry, civil society, and government. Using a participatory,\nsolutions-oriented process, the working groups produced (i) a research agenda\nat the intersection of safety and open source AI; (ii) a mapping of existing\nand needed technical interventions and open source tools to safely and\nresponsibly deploy open foundation models across the AI development workflow;\nand (iii) a mapping of the content safety filter ecosystem with a proposed\nroadmap for future research and development. We find that openness --\nunderstood as transparent weights, interoperable tooling, and public governance\n-- can enhance safety by enabling independent scrutiny, decentralized\nmitigation, and culturally plural oversight. However, significant gaps persist:\nscarce multimodal and multilingual benchmarks, limited defenses against\nprompt-injection and compositional attacks in agentic systems, and insufficient\nparticipatory mechanisms for communities most affected by AI harms. The paper\nconcludes with a roadmap of five priority research directions, emphasizing\nparticipatory inputs, future-proof content filters, ecosystem-wide safety\ninfrastructure, rigorous agentic safeguards, and expanded harm taxonomies.\nThese recommendations informed the February 2025 French AI Action Summit and\nlay groundwork for an open, plural, and accountable AI safety discipline.",
      "pdf_url": "http://arxiv.org/pdf/2506.22183v1",
      "published": "2025-06-27T12:45:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22183v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition",
      "authors": [
        "Wenhan Wu",
        "Zhishuai Guo",
        "Chen Chen",
        "Hongfei Xue",
        "Aidong Lu"
      ],
      "abstract": "Zero-shot skeleton-based action recognition aims to develop models capable of\nidentifying actions beyond the categories encountered during training. Previous\napproaches have primarily focused on aligning visual and semantic\nrepresentations but often overlooked the importance of fine-grained action\npatterns in the semantic space (e.g., the hand movements in drinking water and\nbrushing teeth). To address these limitations, we propose a Frequency-Semantic\nEnhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic\nrepresentation learning with frequency decomposition. FS-VAE consists of three\nkey components: 1) a frequency-based enhancement module with high- and\nlow-frequency adjustments to enrich the skeletal semantics learning and improve\nthe robustness of zero-shot action recognition; 2) a semantic-based action\ndescription with multilevel alignment to capture both local details and global\ncorrespondence, effectively bridging the semantic gap and compensating for the\ninherent loss of information in skeleton sequences; 3) a calibrated\ncross-alignment loss that enables valid skeleton-text pairs to counterbalance\nambiguous ones, mitigating discrepancies and ambiguities in skeleton and text\nfeatures, thereby ensuring robust alignment. Evaluations on the benchmarks\ndemonstrate the effectiveness of our approach, validating that\nfrequency-enhanced semantic features enable robust differentiation of visually\nand semantically similar action clusters, improving zero-shot action\nrecognition.",
      "pdf_url": "http://arxiv.org/pdf/2506.22179v1",
      "published": "2025-06-27T12:44:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22179v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs",
      "authors": [
        "Amirmohammad Izadi",
        "Mohammad Ali Banayeeanzade",
        "Fatemeh Askari",
        "Ali Rahimiakbar",
        "Mohammad Mahdi Vahedi",
        "Hosein Hasani",
        "Mahdieh Soleymani Baghshah"
      ],
      "abstract": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.22146v1",
      "published": "2025-06-27T11:44:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22146v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Learning to Solve Multi-Objective Routing Problems on Multigraphs",
      "authors": [
        "Filip Rydin",
        "Attila Lischka",
        "Jiaming Wu",
        "Morteza Haghir Chehreghani",
        "Balázs Kulcsár"
      ],
      "abstract": "Learning-based methods for routing have gained significant attention in\nrecent years, both in single-objective and multi-objective contexts. However,\nthe multigraph setting, where multiple paths with distinct attributes can exist\nbetween destinations, has largely been overlooked, despite its high practical\nrelevancy. In this paper, we introduce two neural approaches to address\nmulti-objective routing on multigraphs. Our first approach works directly on\nthe multigraph, by autoregressively selecting edges until a tour is completed.\nOn the other hand, our second model first prunes the multigraph into a simple\ngraph and then builds routes. We validate both models experimentally and find\nthat they demonstrate strong performance across a variety of problems,\nincluding the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP).",
      "pdf_url": "http://arxiv.org/pdf/2506.22095v1",
      "published": "2025-06-27T10:25:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22095v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Transformers are Graph Neural Networks",
      "authors": [
        "Chaitanya K. Joshi"
      ],
      "abstract": "We establish connections between the Transformer architecture, originally\nintroduced for natural language processing, and Graph Neural Networks (GNNs)\nfor representation learning on graphs. We show how Transformers can be viewed\nas message passing GNNs operating on fully connected graphs of tokens, where\nthe self-attention mechanism capture the relative importance of all tokens\nw.r.t. each-other, and positional encodings provide hints about sequential\nordering or structure. Thus, Transformers are expressive set processing\nnetworks that learn relationships among input elements without being\nconstrained by apriori graphs. Despite this mathematical connection to GNNs,\nTransformers are implemented via dense matrix operations that are significantly\nmore efficient on modern hardware than sparse message passing. This leads to\nthe perspective that Transformers are GNNs currently winning the hardware\nlottery.",
      "pdf_url": "http://arxiv.org/pdf/2506.22084v1",
      "published": "2025-06-27T10:15:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22084v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios",
      "authors": [
        "Shengyue Yao",
        "Runqing Guo",
        "Yangyang Qin",
        "Miangbing Meng",
        "Jipeng Cao",
        "Yilun Lin",
        "Yisheng Lv",
        "Fei-Yue Wang"
      ],
      "abstract": "With the deep penetration of Artificial Intelligence (AI) in the\ntransportation sector, intelligent cockpits, autonomous driving, and\nintelligent road networks are developing at an unprecedented pace. However, the\ndata ecosystems of these three key areas are increasingly fragmented and\nincompatible. Especially, existing testing methods rely on data stacking, fail\nto cover all edge cases, and lack flexibility. To address this issue, this\npaper introduces the concept of \"Query as Test\" (QaT). This concept shifts the\nfocus from rigid, prescripted test cases to flexible, on-demand logical queries\nagainst a unified data representation. Specifically, we identify the need for a\nfundamental improvement in data storage and representation, leading to our\nproposal of \"Extensible Scenarios Notations\" (ESN). ESN is a novel declarative\ndata framework based on Answer Set Programming (ASP), which uniformly\nrepresents heterogeneous multimodal data from the cockpit, vehicle, and road as\na collection of logical facts and rules. This approach not only achieves deep\nsemantic fusion of data, but also brings three core advantages: (1) supports\ncomplex and flexible semantic querying through logical reasoning; (2) provides\nnatural interpretability for decision-making processes; (3) allows for\non-demand data abstraction through logical rules, enabling fine-grained privacy\nprotection. We further elaborate on the QaT paradigm, transforming the\nfunctional validation and safety compliance checks of autonomous driving\nsystems into logical queries against the ESN database, significantly enhancing\nthe expressiveness and formal rigor of the testing. Finally, we introduce the\nconcept of \"Validation-Driven Development\" (VDD), which suggests to guide\ndevelopments by logical validation rather than quantitative testing in the era\nof Large Language Models, in order to accelerating the iteration and\ndevelopment process.",
      "pdf_url": "http://arxiv.org/pdf/2506.22068v1",
      "published": "2025-06-27T09:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22068v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Universal Retrieval for Multimodal Trajectory Modeling",
      "authors": [
        "Xuan Zhang",
        "Ziyan Jiang",
        "Rui Meng",
        "Yifei Leng",
        "Zhenbang Xiao",
        "Zora Zhiruo Wang",
        "Yanyi Shang",
        "Dehan Kong"
      ],
      "abstract": "Trajectory data, capturing human actions and environmental states across\nvarious modalities, holds significant potential for enhancing AI agent\ncapabilities, particularly in GUI environments. However, how to model the\nrepresentation of trajectory-level data presents a significant challenge that\nhas not been systematically addressed amid explosive trajectory data growth. In\nthis work, we introduce Multimodal Trajectory Retrieval, bridging the gap\nbetween universal retrieval and agent-centric trajectory modeling. We construct\nthe Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and\nstates across diverse real-world scenarios. Based on this, we present\nGAE-Bench, a benchmark containing a large number of trajectory-based retrieval\npairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework\nthat adopts vision-language models and incorporates optimized contrastive\nlearning through a token selection and the GradCache mechanism. Comprehensive\nevaluations across multiple datasets show that GAE-Retriever consistently\noutperforms strong baselines in retrieval recall, highlighting its\neffectiveness in advancing multimodal trajectory retrieval.",
      "pdf_url": "http://arxiv.org/pdf/2506.22056v1",
      "published": "2025-06-27T09:50:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22056v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting",
      "authors": [
        "Lu Han",
        "Yu Liu",
        "Qiwen Deng",
        "Jian Jiang",
        "Yinbo Sun",
        "Zhe Yu",
        "Binfeng Wang",
        "Xingyu Lu",
        "Lintao Ma",
        "Han-Jia Ye",
        "De-Chuan Zhan"
      ],
      "abstract": "Time Series Foundation Models (TSFMs) have achieved remarkable success\nthrough large-scale pretraining. However, their design primarily targets\nreal-valued series, limiting their ability to handle general forecasting tasks\ninvolving diverse and often heterogeneous covariates--such as categorical\nvariables and multimodal data (e.g., images, text)--which are typically\ntask-specific and difficult to leverage during pretraining. To address this\ngap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge\nTSFMs with general covariate-aware forecasting. UniCA first performs covariate\nhomogenization to transform heterogeneous covariates into high-level\nhomogeneous series representations and then fuses them via a unified\nattention-based fusion mechanism. UniCA is compatible and universal for\nadaptation with both homogeneous and heterogeneous covariates, incorporating\nextra covariate information while preserving the generalization ability of\nTSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware\nforecasting benchmarks demonstrate the superiority of UniCA, highlighting the\npromise of covariate-aware TSFM adaptation in real-world forecasting scenarios.\nCodes are released on https://github.com/hanlu-nju/UniCA.",
      "pdf_url": "http://arxiv.org/pdf/2506.22039v1",
      "published": "2025-06-27T09:35:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22039v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
      "authors": [
        "Simra Shahid",
        "Marissa Radensky",
        "Raymond Fok",
        "Pao Siangliulue",
        "Daniel S. Weld",
        "Tom Hope"
      ],
      "abstract": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.",
      "pdf_url": "http://arxiv.org/pdf/2506.22026v1",
      "published": "2025-06-27T08:47:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22026v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "I.2; H.3"
      ]
    },
    {
      "title": "TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning",
      "authors": [
        "Alessandro Sestini",
        "Joakim Bergdahl",
        "Konrad Tollmar",
        "Andrew D. Bagdanov",
        "Linus Gisslén"
      ],
      "abstract": "In offline reinforcement learning, agents are trained using only a fixed set\nof stored transitions derived from a source policy. However, this requires that\nthe dataset be labeled by a reward function. In applied settings such as video\ngame development, the availability of the reward function is not always\nguaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement\nlearning (TROFI), a novel approach to effectively learn a policy offline\nwithout a pre-defined reward function. TROFI first learns a reward function\nfrom human preferences, which it then uses to label the original dataset making\nit usable for training the policy. In contrast to other approaches, our method\ndoes not require optimal trajectories. Through experiments on the D4RL\nbenchmark we demonstrate that TROFI consistently outperforms baselines and\nperforms comparably to using the ground truth reward to learn policies.\nAdditionally, we validate the efficacy of our method in a 3D game environment.\nOur studies of the reward model highlight the importance of the reward function\nin this setting: we show that to ensure the alignment of a value function to\nthe actual future discounted reward, it is fundamental to have a\nwell-engineered and easy-to-learn reward function.",
      "pdf_url": "http://arxiv.org/pdf/2506.22008v1",
      "published": "2025-06-27T08:22:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22008v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving",
      "authors": [
        "Naoto Onda",
        "Kazumi Kasaura",
        "Yuta Oriike",
        "Masaya Taniguchi",
        "Akiyoshi Sannai",
        "Sho Sonoda"
      ],
      "abstract": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults.",
      "pdf_url": "http://arxiv.org/pdf/2506.22005v1",
      "published": "2025-06-27T08:17:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.22005v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Binned semiparametric Bayesian networks",
      "authors": [
        "Rafael Sojo",
        "Javier Díaz-Rozo",
        "Concha Bielza",
        "Pedro Larrañaga"
      ],
      "abstract": "This paper introduces a new type of probabilistic semiparametric model that\ntakes advantage of data binning to reduce the computational cost of kernel\ndensity estimation in nonparametric distributions. Two new conditional\nprobability distributions are developed for the new binned semiparametric\nBayesian networks, the sparse binned kernel density estimation and the Fourier\nkernel density estimation. These two probability distributions address the\ncurse of dimensionality, which typically impacts binned models, by using sparse\ntensors and restricting the number of parent nodes in conditional probability\ncalculations. To evaluate the proposal, we perform a complexity analysis and\nconduct several comparative experiments using synthetic data and datasets from\nthe UCI Machine Learning repository. The experiments include different binning\nrules, parent restrictions, grid sizes, and number of instances to get a\nholistic view of the model's behavior. As a result, our binned semiparametric\nBayesian networks achieve structural learning and log-likelihood estimations\nwith no statistically significant differences compared to the semiparametric\nBayesian networks, but at a much higher speed. Thus, the new binned\nsemiparametric Bayesian networks prove to be a reliable and more efficient\nalternative to their non-binned counterparts.",
      "pdf_url": "http://arxiv.org/pdf/2506.21997v1",
      "published": "2025-06-27T08:07:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21997v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6; I.5.1; G.3"
      ]
    },
    {
      "title": "AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms",
      "authors": [
        "Raphaël Boige",
        "Amine Boumaza",
        "Bruno Scherrer"
      ],
      "abstract": "Deterministic game-solving algorithms are conventionally analyzed in the\nlight of their average-case complexity against a distribution of random\ngame-trees, where leaf values are independently sampled from a fixed\ndistribution. This simplified model enables uncluttered mathematical analysis,\nrevealing two key properties: root value distributions asymptotically collapse\nto a single fixed value for finite-valued trees, and all reasonable algorithms\nachieve global optimality. However, these findings are artifacts of the model's\ndesign-its long criticized independence assumption strips games of structural\ncomplexity, producing trivial instances where no algorithm faces meaningful\nchallenges. To address this limitation, we introduce a new probabilistic model\nthat incrementally constructs game-trees using a fixed level-wise conditional\ndistribution. By enforcing ancestor dependency, a critical structural feature\nof real-world games, our framework generates problems with adjustable\ndifficulty while retaining some form of analytical tractability. For several\nalgorithms, including AlphaBeta and Scout, we derive recursive formulas\ncharacterizing their average-case complexities under this model. These allow us\nto rigorously compare algorithms on deep game-trees, where Monte-Carlo\nsimulations are no longer feasible. While asymptotically, all algorithms seem\nto converge to identical branching factor (a result analogous to those of\nindependence-based models), deep finite trees reveal stark differences:\nAlphaBeta incurs a significantly larger constant multiplicative factor compared\nto algorithms like Scout, leading to a substantial practical slowdown. Our\nframework sheds new light on classical game-solving algorithms, offering\nrigorous evidence and analytical tools to advance the understanding of these\nmethods under a more realistic, challenging, and yet tractable model.",
      "pdf_url": "http://arxiv.org/pdf/2506.21996v1",
      "published": "2025-06-27T08:07:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21996v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit",
      "authors": [
        "Kartheek Kumar Reddy Nareddy",
        "Sarah Ternus",
        "Julia Niebling"
      ],
      "abstract": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).",
      "pdf_url": "http://arxiv.org/pdf/2506.21990v1",
      "published": "2025-06-27T07:57:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21990v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ]
    },
    {
      "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model",
      "authors": [
        "Shuhan Tan",
        "John Lambert",
        "Hong Jeon",
        "Sakshum Kulshrestha",
        "Yijing Bai",
        "Jing Luo",
        "Dragomir Anguelov",
        "Mingxing Tan",
        "Chiyu Max Jiang"
      ],
      "abstract": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.",
      "pdf_url": "http://arxiv.org/pdf/2506.21976v1",
      "published": "2025-06-27T07:35:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21976v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MA",
        "cs.RO"
      ]
    },
    {
      "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses",
      "authors": [
        "Mohamed Ahmed",
        "Mohamed Abdelmouty",
        "Mingyu Kim",
        "Gunvanth Kandula",
        "Alex Park",
        "James C. Davis"
      ],
      "abstract": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.",
      "pdf_url": "http://arxiv.org/pdf/2506.21972v1",
      "published": "2025-06-27T07:26:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21972v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics",
      "authors": [
        "Michael A. Riegler",
        "Kristoffer Herland Hellton",
        "Vajira Thambawita",
        "Hugo L. Hammer"
      ],
      "abstract": "Selecting prior distributions in Bayesian statistics is challenging,\nresource-intensive, and subjective. We analyze using large-language models\n(LLMs) to suggest suitable, knowledge-based informative priors. We developed an\nextensive prompt asking LLMs not only to suggest priors but also to verify and\nreflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real\ndatasets: heart disease risk and concrete strength. All LLMs correctly\nidentified the direction for all associations (e.g., that heart disease risk is\nhigher for males). The quality of suggested priors was measured by their\nKullback-Leibler divergence from the maximum likelihood estimator's\ndistribution.\n  The LLMs suggested both moderately and weakly informative priors. The\nmoderate priors were often overconfident, resulting in distributions misaligned\nwith the data. In our experiments, Claude and Gemini provided better priors\nthan ChatGPT. For weakly informative priors, a key performance difference\nemerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0,\nwhile Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great\npotential as an efficient, objective method for developing informative priors.\nHowever, the primary challenge remains in calibrating the width of these priors\nto avoid over- and under-confidence.",
      "pdf_url": "http://arxiv.org/pdf/2506.21964v1",
      "published": "2025-06-27T07:11:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21964v1",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images",
      "authors": [
        "Naftaly Wambugu",
        "Ruisheng Wang",
        "Bo Guo",
        "Tianshu Yu",
        "Sheng Xu",
        "Mohammed Elhassan"
      ],
      "abstract": "Land cover maps generated from semantic segmentation of high-resolution\nremotely sensed images have drawn mucon in the photogrammetry and remote\nsensing research community. Currently, massive fine-resolution remotely sensed\n(FRRS) images acquired by improving sensing and imaging technologies become\navailable. However, accurate semantic segmentation of such FRRS images is\ngreatly affected by substantial class disparities, the invisibility of key\nground objects due to occlusion, and object size variation. Despite the\nextraordinary potential in deep convolutional neural networks (DCNNs) in image\nfeature learning and representation, extracting sufficient features from FRRS\nimages for accurate semantic segmentation is still challenging. These\nchallenges demand the deep learning models to learn robust features and\ngenerate sufficient feature descriptors. Specifically, learning\nmulti-contextual features to guarantee adequate coverage of varied object sizes\nfrom the ground scene and harnessing global-local contexts to overcome class\ndisparities challenge even profound networks. Deeper networks significantly\nlose spatial details due to gradual downsampling processes resulting in poor\nsegmentation results and coarse boundaries. This article presents a stacked\ndeep residual network (SDRNet) for semantic segmentation from FRRS images. The\nproposed framework utilizes two stacked encoder-decoder networks to harness\nlong-range semantics yet preserve spatial information and dilated residual\nblocks (DRB) between each encoder and decoder network to capture sufficient\nglobal dependencies thus improving segmentation performance. Our experimental\nresults obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate\nthat the SDRNet performs effectively and competitively against current DCNNs in\nsemantic segmentation.",
      "pdf_url": "http://arxiv.org/pdf/2506.21945v1",
      "published": "2025-06-27T06:40:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21945v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation",
      "authors": [
        "Reza Yousefi Maragheh",
        "Pratheek Vadla",
        "Priyank Gupta",
        "Kai Zhao",
        "Aysenur Inan",
        "Kehui Yao",
        "Jianpeng Xu",
        "Praveen Kanumala",
        "Jason Cho",
        "Sushant Kumar"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.",
      "pdf_url": "http://arxiv.org/pdf/2506.21931v1",
      "published": "2025-06-27T05:45:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21931v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "I.2.11; I.2.7; H.3.3"
      ]
    },
    {
      "title": "SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation",
      "authors": [
        "Adam Goodge",
        "Xun Xu",
        "Bryan Hooi",
        "Wee Siong Ng",
        "Jingyi Liao",
        "Yongyi Su",
        "Xulei Yang"
      ],
      "abstract": "As point cloud data increases in prevalence in a variety of applications, the\nability to detect out-of-distribution (OOD) point cloud objects becomes\ncritical for ensuring model safety and reliability. However, this problem\nremains under-explored in existing research. Inspired by success in the image\ndomain, we propose to exploit advances in 3D vision-language models (3D VLMs)\nfor OOD detection in point cloud objects. However, a major challenge is that\npoint cloud datasets used to pre-train 3D VLMs are drastically smaller in size\nand object diversity than their image-based counterparts. Critically, they\noften contain exclusively computer-designed synthetic objects. This leads to a\nsubstantial domain shift when the model is transferred to practical tasks\ninvolving real objects scanned from the physical environment. In this paper,\nour empirical experiments show that synthetic-to-real domain shift\nsignificantly degrades the alignment of point cloud with their associated text\nembeddings in the 3D VLM latent space, hindering downstream performance. To\naddress this, we propose a novel methodology called SODA which improves the\ndetection of OOD point clouds through a neighborhood-based score propagation\nscheme. SODA is inference-based, requires no additional model training, and\nachieves state-of-the-art performance over existing approaches across datasets\nand problem settings.",
      "pdf_url": "http://arxiv.org/pdf/2506.21892v1",
      "published": "2025-06-27T04:05:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21892v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds",
      "authors": [
        "Edward Chen",
        "Sang T. Truong",
        "Natalie Dullerud",
        "Sanmi Koyejo",
        "Carlos Guestrin"
      ],
      "abstract": "High-stakes decision-making involves navigating multiple competing objectives\nwith expensive evaluations. For instance, in brachytherapy, clinicians must\nbalance maximizing tumor coverage (e.g., an aspirational target or soft bound\nof >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard\nbound of <601 cGy to the bladder), with each plan evaluation being\nresource-intensive. Selecting Pareto-optimal solutions that match implicit\npreferences is challenging, as exhaustive Pareto frontier exploration is\ncomputationally and cognitively prohibitive, necessitating interactive\nframeworks to guide users. While decision-makers (DMs) often possess domain\nknowledge to narrow the search via such soft-hard bounds, current methods often\nlack systematic approaches to iteratively refine these multi-faceted preference\nstructures. Critically, DMs must trust their final decision, confident they\nhaven't missed superior alternatives; this trust is paramount in\nhigh-consequence scenarios. We present Active-MoSH, an interactive local-global\nframework designed for this process. Its local component integrates soft-hard\nbounds with probabilistic preference learning, maintaining distributions over\nDM preferences and bounds for adaptive Pareto subset refinement. This is guided\nby an active sampling strategy optimizing exploration-exploitation while\nminimizing cognitive burden. To build DM trust, Active-MoSH's global component,\nT-MoSH, leverages multi-objective sensitivity analysis to identify potentially\noverlooked, high-value points beyond immediate feedback. We demonstrate\nActive-MoSH's performance benefits through diverse synthetic and real-world\napplications. A user study on AI-generated image selection further validates\nour hypotheses regarding the framework's ability to improve convergence,\nenhance DM trust, and provide expressive preference articulation, enabling more\neffective DMs.",
      "pdf_url": "http://arxiv.org/pdf/2506.21887v1",
      "published": "2025-06-27T03:44:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21887v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields",
      "authors": [
        "Fabian Perez",
        "Sara Rojas",
        "Carlos Hinojosa",
        "Hoover Rueda-Chacón",
        "Bernard Ghanem"
      ],
      "abstract": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.",
      "pdf_url": "http://arxiv.org/pdf/2506.21884v1",
      "published": "2025-06-27T03:42:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21884v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation",
      "authors": [
        "Qiyue Gao",
        "Xinyu Pi",
        "Kevin Liu",
        "Junrong Chen",
        "Ruolan Yang",
        "Xinqi Huang",
        "Xinyu Fang",
        "Lu Sun",
        "Gautham Kishore",
        "Bo Ai",
        "Stone Tao",
        "Mengyang Liu",
        "Jiaxi Yang",
        "Chao-Jung Lai",
        "Chuanyang Jin",
        "Jiannan Xiang",
        "Benhao Huang",
        "Zeming Chen",
        "David Danks",
        "Hao Su",
        "Tianmin Shu",
        "Ziqiao Ma",
        "Lianhui Qin",
        "Zhiting Hu"
      ],
      "abstract": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling.",
      "pdf_url": "http://arxiv.org/pdf/2506.21876v1",
      "published": "2025-06-27T03:24:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21876v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    }
  ]
}