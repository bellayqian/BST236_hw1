{
  "last_updated": "2025-05-25T00:57:12.030177",
  "papers": [
    {
      "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning",
      "authors": [
        "Chengqi Duan",
        "Rongyao Fang",
        "Yuqing Wang",
        "Kun Wang",
        "Linjiang Huang",
        "Xingyu Zeng",
        "Hongsheng Li",
        "Xihui Liu"
      ],
      "abstract": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.",
      "pdf_url": "http://arxiv.org/pdf/2505.17022v1",
      "published": "2025-05-22T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17022v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework",
      "authors": [
        "Chenhao Zhang",
        "Yazhe Niu"
      ],
      "abstract": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
      "pdf_url": "http://arxiv.org/pdf/2505.17019v1",
      "published": "2025-05-22T17:59:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17019v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
      "authors": [
        "Chengzhuo Tong",
        "Ziyu Guo",
        "Renrui Zhang",
        "Wenyu Shan",
        "Xinyu Wei",
        "Zhenghao Xing",
        "Hongsheng Li",
        "Pheng-Ann Heng"
      ],
      "abstract": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT",
      "pdf_url": "http://arxiv.org/pdf/2505.17017v1",
      "published": "2025-05-22T17:59:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17017v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Interactive Post-Training for Vision-Language-Action Models",
      "authors": [
        "Shuhan Tan",
        "Kairan Dou",
        "Yue Zhao",
        "Philipp Krähenbühl"
      ],
      "abstract": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
      "pdf_url": "http://arxiv.org/pdf/2505.17016v1",
      "published": "2025-05-22T17:59:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17016v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding",
      "authors": [
        "Haoning Wu",
        "Xiao Huang",
        "Yaohui Chen",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2505.17012v1",
      "published": "2025-05-22T17:59:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17012v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning",
      "authors": [
        "Tim Genewein",
        "Kevin Wenliang Li",
        "Jordi Grau-Moya",
        "Anian Ruoss",
        "Laurent Orseau",
        "Marcus Hutter"
      ],
      "abstract": "Prompting is one of the main ways to adapt a pretrained model to target\ntasks. Besides manually constructing prompts, many prompt optimization methods\nhave been proposed in the literature. Method development is mainly empirically\ndriven, with less emphasis on a conceptual understanding of prompting. In this\npaper we discuss how optimal prompting can be understood through a Bayesian\nview, which also implies some fundamental limitations of prompting that can\nonly be overcome by tuning weights. The paper explains in detail how\nmeta-trained neural networks behave as Bayesian predictors over the pretraining\ndistribution, whose hallmark feature is rapid in-context adaptation. Optimal\nprompting can be studied formally as conditioning these Bayesian predictors,\nyielding criteria for target tasks where optimal prompting is and is not\npossible. We support the theory with educational experiments on LSTMs and\nTransformers, where we compare different versions of prefix-tuning and\ndifferent weight-tuning methods. We also confirm that soft prefixes, which are\nsequences of real-valued vectors outside the token alphabet, can lead to very\neffective prompts for trained and even untrained networks by manipulating\nactivations in ways that are not achievable by hard tokens. This adds an\nimportant mechanistic aspect beyond the conceptual Bayesian theory.",
      "pdf_url": "http://arxiv.org/pdf/2505.17010v1",
      "published": "2025-05-22T17:58:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17010v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning",
      "authors": [
        "Huatong Song",
        "Jinhao Jiang",
        "Wenqing Tian",
        "Zhipeng Chen",
        "Yuhuan Wu",
        "Jiahao Zhao",
        "Yingqian Min",
        "Wayne Xin Zhao",
        "Lei Fang",
        "Ji-Rong Wen"
      ],
      "abstract": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.",
      "pdf_url": "http://arxiv.org/pdf/2505.17005v1",
      "published": "2025-05-22T17:58:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17005v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs",
      "authors": [
        "Jiachen Yao",
        "Abbas Mammadov",
        "Julius Berner",
        "Gavin Kerrigan",
        "Jong Chul Ye",
        "Kamyar Azizzadenesheli",
        "Anima Anandkumar"
      ],
      "abstract": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS",
      "pdf_url": "http://arxiv.org/pdf/2505.17004v1",
      "published": "2025-05-22T17:58:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17004v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA",
        "stat.ML"
      ]
    },
    {
      "title": "PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association",
      "authors": [
        "Abdul Hannan",
        "Muhammad Arslan Manzoor",
        "Shah Nawaz",
        "Muhammad Irzam Liaqat",
        "Markus Schedl",
        "Mubashir Noman"
      ],
      "abstract": "We study the task of learning association between faces and voices, which is\ngaining interest in the multimodal community lately. These methods suffer from\nthe deliberate crafting of negative mining procedures as well as the reliance\non the distant margin parameter. These issues are addressed by learning a joint\nembedding space in which orthogonality constraints are applied to the fused\nembeddings of faces and voices. However, embedding spaces of faces and voices\npossess different characteristics and require spaces to be aligned before\nfusing them. To this end, we propose a method that accurately aligns the\nembedding spaces and fuses them with an enhanced gated fusion thereby improving\nthe performance of face-voice association. Extensive experiments on the\nVoxCeleb dataset reveals the merits of the proposed approach.",
      "pdf_url": "http://arxiv.org/pdf/2505.17002v1",
      "published": "2025-05-22T17:57:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.17002v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?",
      "authors": [
        "Jin Jiang",
        "Jianing Wang",
        "Yuchen Yan",
        "Yang Liu",
        "Jianhua Zhu",
        "Mengdi Zhang",
        "Xunliang Cai",
        "Liangcai Gao"
      ],
      "abstract": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.",
      "pdf_url": "http://arxiv.org/pdf/2505.16998v1",
      "published": "2025-05-22T17:57:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16998v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs",
      "authors": [
        "Rui Ye",
        "Xiangrui Liu",
        "Qimin Wu",
        "Xianghe Pang",
        "Zhenfei Yin",
        "Lei Bai",
        "Siheng Chen"
      ],
      "abstract": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.16997v1",
      "published": "2025-05-22T17:56:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16997v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ]
    },
    {
      "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning",
      "authors": [
        "Runyang You",
        "Yongqi Li",
        "Xinyu Lin",
        "Xin Zhang",
        "Wenjie Wang",
        "Wenjie Li",
        "Liqiang Nie"
      ],
      "abstract": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
      "pdf_url": "http://arxiv.org/pdf/2505.16994v1",
      "published": "2025-05-22T17:55:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16994v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems",
      "authors": [
        "Rui Ye",
        "Keduan Huang",
        "Qimin Wu",
        "Yuzhu Cai",
        "Tian Jin",
        "Xianghe Pang",
        "Xiangrui Liu",
        "Jiaqi Su",
        "Chen Qian",
        "Bohan Tang",
        "Kaiqu Liang",
        "Jiaao Chen",
        "Yue Hu",
        "Zhenfei Yin",
        "Rongye Shi",
        "Bo An",
        "Yang Gao",
        "Wenjun Wu",
        "Lei Bai",
        "Siheng Chen"
      ],
      "abstract": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.",
      "pdf_url": "http://arxiv.org/pdf/2505.16988v1",
      "published": "2025-05-22T17:54:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16988v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning",
      "authors": [
        "Amartya Chakraborty",
        "Paresh Dashore",
        "Nadia Bathaee",
        "Anmol Jain",
        "Anirban Das",
        "Shi-Xiong Zhang",
        "Sambit Sahu",
        "Milind Naphade",
        "Genta Indra Winata"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2505.16986v1",
      "published": "2025-05-22T17:54:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16986v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation",
      "authors": [
        "Moru Liu",
        "Hao Dong",
        "Jessica Kelly",
        "Olga Fink",
        "Mario Trapp"
      ],
      "abstract": "Out-of-distribution (OOD) detection and segmentation are crucial for\ndeploying machine learning models in safety-critical applications such as\nautonomous driving and robot-assisted surgery. While prior research has\nprimarily focused on unimodal image data, real-world applications are\ninherently multimodal, requiring the integration of multiple modalities for\nimproved OOD detection. A key challenge is the lack of supervision signals from\nunknown data, leading to overconfident predictions on OOD samples. To address\nthis challenge, we propose Feature Mixing, an extremely simple and fast method\nfor multimodal outlier synthesis with theoretical support, which can be further\noptimized to help the model better distinguish between in-distribution (ID) and\nOOD data. Feature Mixing is modality-agnostic and applicable to various\nmodality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal\ndataset for OOD segmentation, featuring synthetic OOD objects across diverse\nscenes and weather conditions. Extensive experiments on SemanticKITTI,\nnuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that\nFeature Mixing achieves state-of-the-art performance with a $10 \\times$ to $370\n\\times$ speedup. Our source code and dataset will be available at\nhttps://github.com/mona4399/FeatureMixing.",
      "pdf_url": "http://arxiv.org/pdf/2505.16985v1",
      "published": "2025-05-22T17:54:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16985v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine",
      "authors": [
        "Adib Bazgir",
        "Amir Habibdoust Lafmajani",
        "Yuwen Zhang"
      ],
      "abstract": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.",
      "pdf_url": "http://arxiv.org/pdf/2505.16982v1",
      "published": "2025-05-22T17:52:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16982v1",
      "categories": [
        "cs.AI",
        "physics.med-ph"
      ]
    },
    {
      "title": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design",
      "authors": [
        "Zhenkun Li",
        "Lingyao Li",
        "Shuhang Lin",
        "Yongfeng Zhang"
      ],
      "abstract": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle\ndomain transfer. Conventional multi-agent fixes soften those edges yet expose\nfresh pains: ill-posed decompositions, fuzzy contracts, and verification\noverhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a\nframework that converts domain priors into an algorithmic blueprint hierarchy,\nin which tasks are recursively split into typed, controller-mediated subtasks,\neach solved zero-shot or with the lightest viable boost (e.g.,\nchain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch\ntheorem, KtR trades the chase for a universal prompt for disciplined\ndecomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents\nraise accuracy from 3% zero-shot to 95% on size-5 instances after patching a\nsingle bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a\nsix-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,\nversus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation\nthus turns modest models into reliable collaborators--no ever-larger monoliths\nrequired.",
      "pdf_url": "http://arxiv.org/pdf/2505.16979v1",
      "published": "2025-05-22T17:52:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16979v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation",
      "authors": [
        "Weizhi Tang",
        "Yixuan Li",
        "Chris Sypherd",
        "Elizabeth Polgreen",
        "Vaishak Belle"
      ],
      "abstract": "Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2505.16978v1",
      "published": "2025-05-22T17:52:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16978v1",
      "categories": [
        "cs.AI",
        "cs.PL"
      ]
    },
    {
      "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
      "authors": [
        "Ahmed Heakl",
        "Sarim Hashmi",
        "Gustavo Bertolo Stahl",
        "Seung Hun Eddie Han",
        "Salman Khan",
        "Abdulrahman Mahmoud"
      ],
      "abstract": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}.",
      "pdf_url": "http://arxiv.org/pdf/2505.16968v1",
      "published": "2025-05-22T17:48:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16968v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.PL"
      ]
    },
    {
      "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval",
      "authors": [
        "Nandan Thakur",
        "Crystina Zhang",
        "Xueguang Ma",
        "Jimmy Lin"
      ],
      "abstract": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.",
      "pdf_url": "http://arxiv.org/pdf/2505.16967v1",
      "published": "2025-05-22T17:47:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16967v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation",
      "authors": [
        "Fengyi Li",
        "Kayhan Behdin",
        "Natesh Pillai",
        "Xiaofeng Wang",
        "Zhipeng Wang",
        "Ercan Yildiz"
      ],
      "abstract": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches.",
      "pdf_url": "http://arxiv.org/pdf/2505.16965v1",
      "published": "2025-05-22T17:46:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16965v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models",
      "authors": [
        "Junjie Xiong",
        "Changjia Zhu",
        "Shuhang Lin",
        "Chong Zhang",
        "Yongfeng Zhang",
        "Yao Liu",
        "Lingyao Li"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content.",
      "pdf_url": "http://arxiv.org/pdf/2505.16957v1",
      "published": "2025-05-22T17:36:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16957v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning",
      "authors": [
        "Adnan Oomerjee",
        "Zafeirios Fountas",
        "Zhongwei Yu",
        "Haitham Bou-Ammar",
        "Jun Wang"
      ],
      "abstract": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
      "pdf_url": "http://arxiv.org/pdf/2505.16950v1",
      "published": "2025-05-22T17:33:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16950v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs",
      "authors": [
        "Csaba Dékány",
        "Stefan Balauca",
        "Robin Staab",
        "Dimitar I. Dimitrov",
        "Martin Vechev"
      ],
      "abstract": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT.",
      "pdf_url": "http://arxiv.org/pdf/2505.16947v1",
      "published": "2025-05-22T17:32:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16947v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.7; K.4.1"
      ]
    },
    {
      "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios",
      "authors": [
        "Yunjia Qi",
        "Hao Peng",
        "Xiaozhi Wang",
        "Amy Xin",
        "Youfeng Liu",
        "Bin Xu",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.",
      "pdf_url": "http://arxiv.org/pdf/2505.16944v1",
      "published": "2025-05-22T17:31:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16944v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records",
      "authors": [
        "Chao Pang",
        "Vincent Jeanselme",
        "Young Sang Choi",
        "Xinzhuo Jiang",
        "Zilin Jing",
        "Aparajita Kashyap",
        "Yuta Kobayashi",
        "Yanwei Li",
        "Florent Pollet",
        "Karthik Natarajan",
        "Shalmali Joshi"
      ],
      "abstract": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.",
      "pdf_url": "http://arxiv.org/pdf/2505.16941v1",
      "published": "2025-05-22T17:29:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16941v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification",
      "authors": [
        "NovelSeek Team",
        "Bo Zhang",
        "Shiyang Feng",
        "Xiangchao Yan",
        "Jiakang Yuan",
        "Zhiyin Yu",
        "Xiaohan He",
        "Songtao Huang",
        "Shaowei Hou",
        "Zheng Nie",
        "Zhilong Wang",
        "Jinyao Liu",
        "Runmin Ma",
        "Tianshuo Peng",
        "Peng Ye",
        "Dongzhan Zhou",
        "Shufei Zhang",
        "Xiaosong Wang",
        "Yilan Zhang",
        "Meng Li",
        "Zhongying Tu",
        "Xiangyu Yue",
        "Wangli Ouyang",
        "Bowen Zhou",
        "Lei Bai"
      ],
      "abstract": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.",
      "pdf_url": "http://arxiv.org/pdf/2505.16938v1",
      "published": "2025-05-22T17:27:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16938v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm",
      "authors": [
        "Noah Amsel",
        "David Persson",
        "Christopher Musco",
        "Robert Gower"
      ],
      "abstract": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates.",
      "pdf_url": "http://arxiv.org/pdf/2505.16932v1",
      "published": "2025-05-22T17:23:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16932v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NA",
        "math.NA",
        "math.OC"
      ]
    },
    {
      "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning",
      "authors": [
        "Bosung Kim",
        "Prithviraj Ammanabrolu"
      ],
      "abstract": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.",
      "pdf_url": "http://arxiv.org/pdf/2505.16928v1",
      "published": "2025-05-22T17:20:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16928v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Latent Principle Discovery for Language Model Self-Improvement",
      "authors": [
        "Keshav Ramji",
        "Tahira Naseem",
        "Ramón Fernandez Astudillo"
      ],
      "abstract": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.",
      "pdf_url": "http://arxiv.org/pdf/2505.16927v1",
      "published": "2025-05-22T17:20:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16927v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?",
      "authors": [
        "Qirui Jiao",
        "Daoyuan Chen",
        "Yilun Huang",
        "Xika Lin",
        "Ying Shen",
        "Yaliang Li"
      ],
      "abstract": "While recent text-to-image (T2I) models show impressive capabilities in\nsynthesizing images from brief descriptions, their performance significantly\ndegrades when confronted with long, detail-intensive prompts required in\nprofessional applications. We present DetailMaster, the first comprehensive\nbenchmark specifically designed to evaluate T2I models' systematical abilities\nto handle extended textual inputs that contain complex compositional\nrequirements. Our benchmark introduces four critical evaluation dimensions:\nCharacter Attributes, Structured Character Locations, Multi-Dimensional Scene\nAttributes, and Explicit Spatial/Interactive Relationships. The benchmark\ncomprises long and detail-rich prompts averaging 284.89 tokens, with high\nquality validated by expert annotators. Evaluation on 7 general-purpose and 5\nlong-prompt-optimized T2I models reveals critical performance limitations:\nstate-of-the-art models achieve merely ~50% accuracy in key dimensions like\nattribute binding and spatial reasoning, while all models showing progressive\nperformance degradation as prompt length increases. Our analysis highlights\nsystemic failures in structural comprehension and detail overload handling,\nmotivating future research into architectures with enhanced compositional\nreasoning. We open-source the dataset, data curation code, and evaluation tools\nto advance detail-rich T2I generation and enable broad applications that would\notherwise be infeasible due to the lack of a dedicated benchmark.",
      "pdf_url": "http://arxiv.org/pdf/2505.16915v1",
      "published": "2025-05-22T17:11:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16915v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation",
      "authors": [
        "Ofir Yaish",
        "Yehuda Mishaly",
        "Eliya Nachmani"
      ],
      "abstract": "We introduce a new paradigm for active sound modification: Active Speech\nEnhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on\nsuppressing external interference, ASE goes further by actively shaping the\nspeech signal -- both attenuating unwanted noise components and amplifying\nspeech-relevant frequencies -- to improve intelligibility and perceptual\nquality. To enable this, we propose a novel Transformer-Mamba-based\narchitecture, along with a task-specific loss function designed to jointly\noptimize interference suppression and signal enrichment. Our method outperforms\nexisting baselines across multiple speech processing tasks -- including\ndenoising, dereverberation, and declipping -- demonstrating the effectiveness\nof active, targeted modulation in challenging acoustic environments.",
      "pdf_url": "http://arxiv.org/pdf/2505.16911v1",
      "published": "2025-05-22T17:10:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16911v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships",
      "authors": [
        "Kerem Oktar",
        "Katherine M. Collins",
        "Jose Hernandez-Orallo",
        "Diane Coyle",
        "Stephen Cave",
        "Adrian Weller",
        "Ilia Sucholutsky"
      ],
      "abstract": "Artificial Intelligence (AI) systems have historically been used as tools\nthat execute narrowly defined tasks. Yet recent advances in AI have unlocked\npossibilities for a new class of models that genuinely collaborate with humans\nin complex reasoning, from conceptualizing problems to brainstorming solutions.\nSuch AI thought partners enable novel forms of collaboration and extended\ncognition, yet they also pose major risks-including and beyond risks of typical\nAI tools and agents. In this commentary, we systematically identify risks of AI\nthought partners through a novel framework that identifies risks at multiple\nlevels of analysis, including Real-time, Individual, and Societal risks arising\nfrom collaborative cognition (RISc). We leverage this framework to propose\nconcrete metrics for risk evaluation, and finally suggest specific mitigation\nstrategies for developers and policymakers. As AI thought partners continue to\nproliferate, these strategies can help prevent major harms and ensure that\nhumans actively benefit from productive thought partnerships.",
      "pdf_url": "http://arxiv.org/pdf/2505.16899v1",
      "published": "2025-05-22T16:58:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16899v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Structure-Aligned Protein Language Model",
      "authors": [
        "Can Chen",
        "David Heurtel-Depeiges",
        "Robert M. Vernon",
        "Christopher James Langmead",
        "Yoshua Bengio",
        "Quentin Fournier"
      ],
      "abstract": "Protein language models (pLMs) pre-trained on vast protein sequence databases\nexcel at various downstream tasks but lack the structural knowledge essential\nfor many biological applications. To address this, we integrate structural\ninsights from pre-trained protein graph neural networks (pGNNs) into pLMs\nthrough a latent-level contrastive learning task. This task aligns residue\nrepresentations from pLMs with those from pGNNs across multiple proteins,\nenriching pLMs with inter-protein structural knowledge. Additionally, we\nincorporate a physical-level task that infuses intra-protein structural\nknowledge by optimizing pLMs to predict structural tokens. The proposed\ndual-task framework effectively incorporates both inter-protein and\nintra-protein structural knowledge into pLMs. Given the variability in the\nquality of protein structures in PDB, we further introduce a residue loss\nselection module, which uses a small model trained on high-quality structures\nto select reliable yet challenging residue losses for the pLM to learn.\nApplying our structure alignment method to the state-of-the-art ESM2 and\nAMPLIFY results in notable performance gains across a wide range of tasks,\nincluding a 12.7% increase in ESM2 contact prediction. The data, code, and\nresulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.",
      "pdf_url": "http://arxiv.org/pdf/2505.16896v1",
      "published": "2025-05-22T16:56:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16896v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework",
      "authors": [
        "Viet Pham",
        "Thai Le"
      ],
      "abstract": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2505.16888v1",
      "published": "2025-05-22T16:47:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16888v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?",
      "authors": [
        "Nour Jedidi",
        "Yung-Sung Chuang",
        "James Glass",
        "Jimmy Lin"
      ],
      "abstract": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.",
      "pdf_url": "http://arxiv.org/pdf/2505.16886v1",
      "published": "2025-05-22T16:41:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16886v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models",
      "authors": [
        "Daniel F. Perez-Ramirez",
        "Dejan Kostic",
        "Magnus Boman"
      ],
      "abstract": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.16881v1",
      "published": "2025-05-22T16:35:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16881v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings",
      "authors": [
        "Yuqicheng Zhu",
        "Daniel Hernández",
        "Yuan He",
        "Zifeng Ding",
        "Bo Xiong",
        "Evgeny Kharlamov",
        "Steffen Staab"
      ],
      "abstract": "Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is\ncrucial for ensuring the reliability of downstream applications. A recent work\napplies conformal prediction to KGE methods, providing uncertainty estimates by\ngenerating a set of answers that is guaranteed to include the true answer with\na predefined confidence level. However, existing methods provide probabilistic\nguarantees averaged over a reference set of queries and answers (marginal\ncoverage guarantee). In high-stakes applications such as medical diagnosis, a\nstronger guarantee is often required: the predicted sets must provide\nconsistent coverage per query (conditional coverage guarantee). We propose\nCondKGCP, a novel method that approximates predicate-conditional coverage\nguarantees while maintaining compact prediction sets. CondKGCP merges\npredicates with similar vector representations and augments calibration with\nrank information. We prove the theoretical guarantees and demonstrate empirical\neffectiveness of CondKGCP by comprehensive evaluations.",
      "pdf_url": "http://arxiv.org/pdf/2505.16877v1",
      "published": "2025-05-22T16:33:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16877v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "T2I-ConBench: Text-to-Image Benchmark for Continual Post-training",
      "authors": [
        "Zhehao Huang",
        "Yuhang Liu",
        "Yixin Lou",
        "Zhengbao He",
        "Mingzhen He",
        "Wenxing Zhou",
        "Tao Li",
        "Kehan Li",
        "Zeyi Huang",
        "Xiaolin Huang"
      ],
      "abstract": "Continual post-training adapts a single text-to-image diffusion model to\nlearn new tasks without incurring the cost of separate models, but naive\npost-training causes forgetting of pretrained knowledge and undermines\nzero-shot compositionality. We observe that the absence of a standardized\nevaluation protocol hampers related research for continual post-training. To\naddress this, we introduce T2I-ConBench, a unified benchmark for continual\npost-training of text-to-image models. T2I-ConBench focuses on two practical\nscenarios, item customization and domain enhancement, and analyzes four\ndimensions: (1) retention of generality, (2) target-task performance, (3)\ncatastrophic forgetting, and (4) cross-task generalization. It combines\nautomated metrics, human-preference modeling, and vision-language QA for\ncomprehensive assessment. We benchmark ten representative methods across three\nrealistic task sequences and find that no approach excels on all fronts. Even\njoint \"oracle\" training does not succeed for every task, and cross-task\ngeneralization remains unsolved. We release all datasets, code, and evaluation\ntools to accelerate research in continual post-training for text-to-image\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2505.16875v1",
      "published": "2025-05-22T16:31:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16875v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "GCAL: Adapting Graph Models to Evolving Domain Shifts",
      "authors": [
        "Ziyue Qiao",
        "Qianyi Cai",
        "Hao Dong",
        "Jiawei Gu",
        "Pengyang Wang",
        "Meng Xiao",
        "Xiao Luo",
        "Hui Xiong"
      ],
      "abstract": "This paper addresses the challenge of graph domain adaptation on evolving,\nmultiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation\nmethods are confined to single-step adaptation, making them ineffective in\nhandling continuous domain shifts and prone to catastrophic forgetting. This\npaper introduces the Graph Continual Adaptive Learning (GCAL) method, designed\nto enhance model sustainability and adaptability across various graph domains.\nGCAL employs a bilevel optimization strategy. The \"adapt\" phase uses an\ninformation maximization approach to fine-tune the model with new graph domains\nwhile re-adapting past memories to mitigate forgetting. Concurrently, the\n\"generate memory\" phase, guided by a theoretical lower bound derived from\ninformation bottleneck theory, involves a variational memory graph generation\nmodule to condense original graphs into memories. Extensive experimental\nevaluations demonstrate that GCAL substantially outperforms existing methods in\nterms of adaptability and knowledge retention.",
      "pdf_url": "http://arxiv.org/pdf/2505.16860v1",
      "published": "2025-05-22T16:19:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16860v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only",
      "authors": [
        "Wei Xiao",
        "Jiacheng Liu",
        "Zifeng Zhuang",
        "Runze Suo",
        "Shangke Lyu",
        "Donglin Wang"
      ],
      "abstract": "Improving the performance of pre-trained policies through online\nreinforcement learning (RL) is a critical yet challenging topic. Existing\nonline RL fine-tuning methods require continued training with offline\npretrained Q-functions for stability and performance. However, these offline\npretrained Q-functions commonly underestimate state-action pairs beyond the\noffline dataset due to the conservatism in most offline RL methods, which\nhinders further exploration when transitioning from the offline to the online\nsetting. Additionally, this requirement limits their applicability in scenarios\nwhere only pre-trained policies are available but pre-trained Q-functions are\nabsent, such as in imitation learning (IL) pre-training. To address these\nchallenges, we propose a method for efficient online RL fine-tuning using\nsolely the offline pre-trained policy, eliminating reliance on pre-trained\nQ-functions. We introduce PORL (Policy-Only Reinforcement Learning\nFine-Tuning), which rapidly initializes the Q-function from scratch during the\nonline phase to avoid detrimental pessimism. Our method not only achieves\ncompetitive performance with advanced offline-to-online RL algorithms and\nonline RL approaches that leverage data or policies prior, but also pioneers a\nnew path for directly fine-tuning behavior cloning (BC) policies.",
      "pdf_url": "http://arxiv.org/pdf/2505.16856v1",
      "published": "2025-05-22T16:14:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16856v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models",
      "authors": [
        "Jiaqi Wang",
        "Kevin Qinghong Lin",
        "James Cheng",
        "Mike Zheng Shou"
      ],
      "abstract": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.",
      "pdf_url": "http://arxiv.org/pdf/2505.16854v1",
      "published": "2025-05-22T16:13:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16854v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
      "authors": [
        "Hanglei Zhang",
        "Yiwei Guo",
        "Zhihan Li",
        "Xiang Hao",
        "Xie Chen",
        "Kai Yu"
      ],
      "abstract": "Most neural speech codecs achieve bitrate adjustment through intra-frame\nmechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However,\nspeech segments inherently have time-varying information density (e.g., silent\nintervals versus voiced regions). This property makes CFR not optimal in terms\nof bitrate and token sequence length, hindering efficiency in real-time\napplications. In this work, we propose a Temporally Flexible Coding (TFC)\ntechnique, introducing variable frame rate (VFR) into neural speech codecs for\nthe first time. TFC enables seamlessly tunable average frame rates and\ndynamically allocates frame rates based on temporal entropy. Experimental\nresults show that a codec with TFC achieves optimal reconstruction quality with\nhigh flexibility, and maintains competitive performance even at lower frame\nrates. Our approach is promising for the integration with other efforts to\ndevelop low-frame-rate neural speech codecs for more efficient downstream\ntasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.16845v1",
      "published": "2025-05-22T16:10:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16845v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "title": "Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning",
      "authors": [
        "Fanrui Zhang",
        "Dian Li",
        "Qiang Zhang",
        "Chenjun",
        "sinbadliu",
        "Junxiong Lin",
        "Jiahong Yan",
        "Jiawei Liu",
        "Zheng-Jun Zha"
      ],
      "abstract": "The rapid spread of multimodal misinformation on social media has raised\ngrowing concerns, while research on video misinformation detection remains\nlimited due to the lack of large-scale, diverse datasets. Existing methods\noften overfit to rigid templates and lack deep reasoning over deceptive\ncontent. To address these challenges, we introduce FakeVV, a large-scale\nbenchmark comprising over 100,000 video-text pairs with fine-grained,\ninterpretable annotations. In addition, we further propose Fact-R1, a novel\nframework that integrates deep reasoning with collaborative rule-based\nreinforcement learning. Fact-R1 is trained through a three-stage process: (1)\nmisinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference\nalignment via Direct Preference Optimization (DPO), and (3) Group Relative\nPolicy Optimization (GRPO) using a novel verifiable reward function. This\nenables Fact-R1 to exhibit emergent reasoning behaviors comparable to those\nobserved in advanced text-based reinforcement learning systems, but in the more\ncomplex multimodal misinformation setting. Our work establishes a new paradigm\nfor misinformation detection, bridging large-scale video understanding,\nreasoning-guided alignment, and interpretable verification.",
      "pdf_url": "http://arxiv.org/pdf/2505.16836v1",
      "published": "2025-05-22T16:05:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16836v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis",
      "authors": [
        "Shuang Sun",
        "Huatong Song",
        "Yuhao Wang",
        "Ruiyang Ren",
        "Jinhao Jiang",
        "Junjie Zhang",
        "Fei Bai",
        "Jia Deng",
        "Wayne Xin Zhao",
        "Zheng Liu",
        "Lei Fang",
        "Zhongyuan Wang",
        "Ji-Rong Wen"
      ],
      "abstract": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.",
      "pdf_url": "http://arxiv.org/pdf/2505.16834v1",
      "published": "2025-05-22T16:05:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16834v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization",
      "authors": [
        "Haonian Ji",
        "Shi Qiu",
        "Siyang Xin",
        "Siwei Han",
        "Zhaorun Chen",
        "Hongyi Wang",
        "Dake Zhang",
        "Huaxiu Yao"
      ],
      "abstract": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.",
      "pdf_url": "http://arxiv.org/pdf/2505.16832v1",
      "published": "2025-05-22T16:02:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16832v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs",
      "authors": [
        "Xiaoyu Xu",
        "Xiang Yue",
        "Yang Liu",
        "Qingqing Ye",
        "Haibo Hu",
        "Minxin Du"
      ],
      "abstract": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.",
      "pdf_url": "http://arxiv.org/pdf/2505.16831v1",
      "published": "2025-05-22T16:02:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16831v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent",
      "authors": [
        "Bin Xie",
        "Rui Shao",
        "Gongwei Chen",
        "Kaiwen Zhou",
        "Yinchuan Li",
        "Jie Liu",
        "Min Zhang",
        "Liqiang Nie"
      ],
      "abstract": "GUI automation faces critical challenges in dynamic environments. MLLMs\nsuffer from two key issues: misinterpreting UI components and outdated\nknowledge. Traditional fine-tuning methods are costly for app-specific\nknowledge updates. We propose GUI-explorer, a training-free GUI agent that\nincorporates two fundamental mechanisms: (1) Autonomous Exploration of\nFunction-aware Trajectory. To comprehensively cover all application\nfunctionalities, we design a Function-aware Task Goal Generator that\nautomatically constructs exploration goals by analyzing GUI structural\ninformation (e.g., screenshots and activity hierarchies). This enables\nsystematic exploration to collect diverse trajectories. (2) Unsupervised Mining\nof Transition-aware Knowledge. To establish precise screen-operation logic, we\ndevelop a Transition-aware Knowledge Extractor that extracts effective\nscreen-operation logic through unsupervised analysis the state transition of\nstructured interaction triples (observation, action, outcome). This eliminates\nthe need for human involvement in knowledge extraction. With a task success\nrate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows\nsignificant improvements over SOTA agents. It requires no parameter updates for\nnew apps. GUI-explorer is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/GUI-explorer.",
      "pdf_url": "http://arxiv.org/pdf/2505.16827v1",
      "published": "2025-05-22T16:01:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16827v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning",
      "authors": [
        "Wei Sun",
        "Wen Yang",
        "Pu Jian",
        "Qianlong Du",
        "Fuwei Cui",
        "Shuo Ren",
        "Jiajun Zhang"
      ],
      "abstract": "Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel.",
      "pdf_url": "http://arxiv.org/pdf/2505.16826v1",
      "published": "2025-05-22T16:00:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16826v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Dynamic Reservoir Computing with Physical Neuromorphic Networks",
      "authors": [
        "Yinhao Xu",
        "Georg A. Gottwald",
        "Zdenka Kuncic"
      ],
      "abstract": "Reservoir Computing (RC) with physical systems requires an understanding of\nthe underlying structure and internal dynamics of the specific physical\nreservoir. In this study, physical nano-electronic networks with neuromorphic\ndynamics are investigated for their use as physical reservoirs in an RC\nframework. These neuromorphic networks operate as dynamic reservoirs, with node\nactivities in general coupled to the edge dynamics through nonlinear\nnano-electronic circuit elements, and the reservoir outputs influenced by the\nunderlying network connectivity structure. This study finds that networks with\nvarying degrees of sparsity generate more useful nonlinear temporal outputs for\ndynamic RC compared to dense networks. Dynamic RC is also tested on an\nautonomous multivariate chaotic time series prediction task with networks of\nvarying densities, which revealed the importance of network sparsity in\nmaintaining network activity and overall dynamics, that in turn enabled the\nlearning of the chaotic Lorenz63 system's attractor behavior.",
      "pdf_url": "http://arxiv.org/pdf/2505.16813v1",
      "published": "2025-05-22T15:50:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.16813v1",
      "categories": [
        "cs.ET",
        "cond-mat.dis-nn",
        "cs.AI"
      ]
    }
  ]
}