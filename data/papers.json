{
  "last_updated": "2025-04-09T00:48:09.083844",
  "papers": [
    {
      "title": "URECA: Unique Region Caption Anything",
      "authors": [
        "Sangbeom Lim",
        "Junwan Kim",
        "Heeji Yoon",
        "Jaewoo Jung",
        "Seungryong Kim"
      ],
      "abstract": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2504.05305v1",
      "published": "2025-04-07T17:59:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05305v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SmolVLM: Redefining small and efficient multimodal models",
      "authors": [
        "Andrés Marafioti",
        "Orr Zohar",
        "Miquel Farré",
        "Merve Noyan",
        "Elie Bakouch",
        "Pedro Cuenca",
        "Cyril Zakka",
        "Loubna Ben Allal",
        "Anton Lozhkov",
        "Nouamane Tazi",
        "Vaibhav Srivastav",
        "Joshua Lochner",
        "Hugo Larcher",
        "Mathieu Morlon",
        "Lewis Tunstall",
        "Leandro von Werra",
        "Thomas Wolf"
      ],
      "abstract": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
      "pdf_url": "http://arxiv.org/pdf/2504.05299v1",
      "published": "2025-04-07T17:58:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05299v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Dion: A Communication-Efficient Optimizer for Large Models",
      "authors": [
        "Kwangjun Ahn",
        "Byron Xu"
      ],
      "abstract": "Training large AI models efficiently requires distributing computation across\nmultiple accelerators, but this often incurs significant communication overhead\n-- especially during gradient synchronization. We introduce Dion, a\ncommunication-efficient optimizer that retains the synchronous semantics of\nstandard distributed training (e.g., DDP, FSDP) while substantially reducing\nI/O costs. Unlike conventional optimizers that synchronize full gradient\nmatrices, Dion leverages orthonormalized updates with device-local momentum\nbuffers, eliminating the need for full gradient exchange. It further supports\nan efficient sharding strategy that avoids reconstructing large matrices during\ntraining.",
      "pdf_url": "http://arxiv.org/pdf/2504.05295v1",
      "published": "2025-04-07T17:49:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05295v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ]
    },
    {
      "title": "The challenge of uncertainty quantification of large language models in medicine",
      "authors": [
        "Zahra Atf",
        "Seyed Amir Ahmad Safavi-Naini",
        "Peter R. Lewis",
        "Aref Mahjoubfar",
        "Nariman Naderi",
        "Thomas R. Savage",
        "Ali Soroush"
      ],
      "abstract": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge.",
      "pdf_url": "http://arxiv.org/pdf/2504.05278v1",
      "published": "2025-04-07T17:24:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05278v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "How to evaluate control measures for LLM agents? A trajectory from today to superintelligence",
      "authors": [
        "Tomek Korbak",
        "Mikita Balesni",
        "Buck Shlegeris",
        "Geoffrey Irving"
      ],
      "abstract": "As LLM agents grow more capable of causing harm autonomously, AI developers\nwill rely on increasingly sophisticated control measures to prevent possibly\nmisaligned agents from causing harm. AI developers could demonstrate that their\ncontrol measures are sufficient by running control evaluations: testing\nexercises in which a red team produces agents that try to subvert control\nmeasures. To ensure control evaluations accurately capture misalignment risks,\nthe affordances granted to this red team should be adapted to the capability\nprofiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of\nred teams to advancing AI capabilities. Rather than assuming that agents will\nalways execute the best attack strategies known to humans, we demonstrate how\nknowledge of an agents's actual capability profile can inform proportional\ncontrol evaluations, resulting in more practical and cost-effective control\nmeasures. We illustrate our framework by considering a sequence of five\nfictional models (M1-M5) with progressively advanced capabilities, defining\nfive distinct AI control levels (ACLs). For each ACL, we provide example rules\nfor control evaluation, control measures, and safety cases that could be\nappropriate. Finally, we show why constructing a compelling AI control safety\ncase for superintelligent LLM agents will require research breakthroughs,\nhighlighting that we might eventually need alternative approaches to mitigating\nmisalignment risk.",
      "pdf_url": "http://arxiv.org/pdf/2504.05259v1",
      "published": "2025-04-07T16:52:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05259v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models",
      "authors": [
        "Adrián Bazaga",
        "Rexhina Blloshmi",
        "Bill Byrne",
        "Adrià de Gispert"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.",
      "pdf_url": "http://arxiv.org/pdf/2504.05258v1",
      "published": "2025-04-07T16:51:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05258v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Explaining Low Perception Model Competency with High-Competency Counterfactuals",
      "authors": [
        "Sara Pohland",
        "Claire Tomlin"
      ],
      "abstract": "There exist many methods to explain how an image classification model\ngenerates its decision, but very little work has explored methods to explain\nwhy a classifier might lack confidence in its prediction. As there are various\nreasons the classifier might lose confidence, it would be valuable for this\nmodel to not only indicate its level of uncertainty but also explain why it is\nuncertain. Counterfactual images have been used to visualize changes that could\nbe made to an image to generate a different classification decision. In this\nwork, we explore the use of counterfactuals to offer an explanation for low\nmodel competency--a generalized form of predictive uncertainty that measures\nconfidence. Toward this end, we develop five novel methods to generate\nhigh-competency counterfactual images, namely Image Gradient Descent (IGD),\nFeature Gradient Descent (FGD), Autoencoder Reconstruction (Reco), Latent\nGradient Descent (LGD), and Latent Nearest Neighbors (LNN). We evaluate these\nmethods across two unique datasets containing images with six known causes for\nlow model competency and find Reco, LGD, and LNN to be the most promising\nmethods for counterfactual generation. We further evaluate how these three\nmethods can be utilized by pre-trained Multimodal Large Language Models (MLLMs)\nto generate language explanations for low model competency. We find that the\ninclusion of a counterfactual image in the language model query greatly\nincreases the ability of the model to generate an accurate explanation for the\ncause of low model competency, thus demonstrating the utility of counterfactual\nimages in explaining low perception model competency.",
      "pdf_url": "http://arxiv.org/pdf/2504.05254v1",
      "published": "2025-04-07T16:46:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05254v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Adversarial KA",
      "authors": [
        "Sviatoslav Dzhenzher",
        "Michael H. Freedman"
      ],
      "abstract": "Regarding the representation theorem of Kolmogorov and Arnold (KA) as an\nalgorithm for representing or {\\guillemotleft}expressing{\\guillemotright}\nfunctions, we test its robustness by analyzing its ability to withstand\nadversarial attacks. We find KA to be robust to countable collections of\ncontinuous adversaries, but unearth a question about the equi-continuity of the\nouter functions that, so far, obstructs taking limits and defeating continuous\ngroups of adversaries. This question on the regularity of the outer functions\nis relevant to the debate over the applicability of KA to the general theory of\nNNs.",
      "pdf_url": "http://arxiv.org/pdf/2504.05255v1",
      "published": "2025-04-07T16:46:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05255v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.FA"
      ]
    },
    {
      "title": "PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks",
      "authors": [
        "Marius Almanstötter",
        "Roman Vetter",
        "Dagmar Iber"
      ],
      "abstract": "Parameter estimation for differential equations from measured data is an\ninverse problem prevalent across quantitative sciences. Physics-Informed Neural\nNetworks (PINNs) have emerged as effective tools for solving such problems,\nespecially with sparse measurements and incomplete system information. However,\nPINNs face convergence issues, stability problems, overfitting, and complex\nloss function design. Here we introduce PINNverse, a training paradigm that\naddresses these limitations by reformulating the learning process as a\nconstrained differential optimization problem. This approach achieves a dynamic\nbalance between data loss and differential equation residual loss during\ntraining while preventing overfitting. PINNverse combines the advantages of\nPINNs with the Modified Differential Method of Multipliers to enable\nconvergence on any point on the Pareto front. We demonstrate robust and\naccurate parameter estimation from noisy data in four classical ODE and PDE\nmodels from physics and biology. Our method enables accurate parameter\ninference also when the forward problem is expensive to solve.",
      "pdf_url": "http://arxiv.org/pdf/2504.05248v1",
      "published": "2025-04-07T16:34:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05248v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ]
    },
    {
      "title": "Mapping biodiversity at very-high resolution in Europe",
      "authors": [
        "César Leblanc",
        "Lukas Picek",
        "Benjamin Deneu",
        "Pierre Bonnet",
        "Maximilien Servajean",
        "Rémi Palard",
        "Alexis Joly"
      ],
      "abstract": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.",
      "pdf_url": "http://arxiv.org/pdf/2504.05231v1",
      "published": "2025-04-07T16:15:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05231v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
      "authors": [
        "Islam Eldifrawi",
        "Shengrui Wang",
        "Amine Trabelsi"
      ],
      "abstract": "The field of explainable Automatic Fact-Checking (AFC) aims to enhance the\ntransparency and trustworthiness of automated fact-verification systems by\nproviding clear and comprehensible explanations. However, the effectiveness of\nthese explanations depends on their actionability --their ability to empower\nusers to make informed decisions and mitigate misinformation. Despite\nactionability being a critical property of high-quality explanations, no prior\nresearch has proposed a dedicated method to evaluate it. This paper introduces\nFinGrAct, a fine-grained evaluation framework that can access the web, and it\nis designed to assess actionability in AFC explanations through well-defined\ncriteria and an evaluation dataset. FinGrAct surpasses state-of-the-art (SOTA)\nevaluators, achieving the highest Pearson and Kendall correlation with human\njudgments while demonstrating the lowest ego-centric bias, making it a more\nrobust evaluation approach for actionability evaluation in AFC.",
      "pdf_url": "http://arxiv.org/pdf/2504.05229v1",
      "published": "2025-04-07T16:14:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05229v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG",
      "authors": [
        "Hengran Zhang",
        "Minghao Tang",
        "Keping Bi",
        "Jiafeng Guo",
        "Shihao Liu",
        "Daiting Shi",
        "Dawei Yin",
        "Xueqi Cheng"
      ],
      "abstract": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.",
      "pdf_url": "http://arxiv.org/pdf/2504.05220v2",
      "published": "2025-04-07T16:05:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05220v2",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
      "authors": [
        "Hengran Zhang",
        "Keping Bi",
        "Jiafeng Guo",
        "Xiaojie Sun",
        "Shihao Liu",
        "Daiting Shi",
        "Dawei Yin",
        "Xueqi Cheng"
      ],
      "abstract": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin.",
      "pdf_url": "http://arxiv.org/pdf/2504.05216v1",
      "published": "2025-04-07T16:03:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05216v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A moving target in AI-assisted decision-making: Dataset shift, model updating, and the problem of update opacity",
      "authors": [
        "Joshua Hatherley"
      ],
      "abstract": "Machine learning (ML) systems are vulnerable to performance decline over time\ndue to dataset shift. To address this problem, experts often suggest that ML\nsystems should be regularly updated to ensure ongoing performance stability.\nSome scholarly literature has begun to address the epistemic and ethical\nchallenges associated with different updating methodologies. Thus far, however,\nlittle attention has been paid to the impact of model updating on the\nML-assisted decision-making process itself, particularly in the AI ethics and\nAI epistemology literatures. This article aims to address this gap in the\nliterature. It argues that model updating introduces a new sub-type of opacity\ninto ML-assisted decision-making -- update opacity -- that occurs when users\ncannot understand how or why an update has changed the reasoning or behaviour\nof an ML system. This type of opacity presents a variety of distinctive\nepistemic and safety concerns that available solutions to the black box problem\nin ML are largely ill-equipped to address. A variety of alternative strategies\nmay be developed or pursued to address the problem of update opacity more\ndirectly, including bi-factual explanations, dynamic model reporting, and\nupdate compatibility. However, each of these strategies presents its own risks\nor carries significant limitations. Further research will be needed to address\nthe epistemic and safety concerns associated with model updating and update\nopacity going forward.",
      "pdf_url": "http://arxiv.org/pdf/2504.05210v1",
      "published": "2025-04-07T15:58:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05210v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Correcting Class Imbalances with Self-Training for Improved Universal Lesion Detection and Tagging",
      "authors": [
        "Alexander Shieh",
        "Tejas Sudharshan Mathai",
        "Jianfei Liu",
        "Angshuman Paul",
        "Ronald M. Summers"
      ],
      "abstract": "Universal lesion detection and tagging (ULDT) in CT studies is critical for\ntumor burden assessment and tracking the progression of lesion status\n(growth/shrinkage) over time. However, a lack of fully annotated data hinders\nthe development of effective ULDT approaches. Prior work used the DeepLesion\ndataset (4,427 patients, 10,594 studies, 32,120 CT slices, 32,735 lesions, 8\nbody part labels) for algorithmic development, but this dataset is not\ncompletely annotated and contains class imbalances. To address these issues, in\nthis work, we developed a self-training pipeline for ULDT. A VFNet model was\ntrained on a limited 11.5\\% subset of DeepLesion (bounding boxes + tags) to\ndetect and classify lesions in CT studies. Then, it identified and incorporated\nnovel lesion candidates from a larger unseen data subset into its training set,\nand self-trained itself over multiple rounds. Multiple self-training\nexperiments were conducted with different threshold policies to select\npredicted lesions with higher quality and cover the class imbalances. We\ndiscovered that direct self-training improved the sensitivities of\nover-represented lesion classes at the expense of under-represented classes.\nHowever, upsampling the lesions mined during self-training along with a\nvariable threshold policy yielded a 6.5\\% increase in sensitivity at 4 FP in\ncontrast to self-training without class balancing (72\\% vs 78.5\\%) and a 11.7\\%\nincrease compared to the same self-training policy without upsampling (66.8\\%\nvs 78.5\\%). Furthermore, we show that our results either improved or maintained\nthe sensitivity at 4FP for all 8 lesion classes.",
      "pdf_url": "http://arxiv.org/pdf/2504.05207v1",
      "published": "2025-04-07T15:57:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05207v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "3D Universal Lesion Detection and Tagging in CT with Self-Training",
      "authors": [
        "Jared Frazier",
        "Tejas Sudharshan Mathai",
        "Jianfei Liu",
        "Angshuman Paul",
        "Ronald M. Summers"
      ],
      "abstract": "Radiologists routinely perform the tedious task of lesion localization,\nclassification, and size measurement in computed tomography (CT) studies.\nUniversal lesion detection and tagging (ULDT) can simultaneously help alleviate\nthe cumbersome nature of lesion measurement and enable tumor burden assessment.\nPrevious ULDT approaches utilize the publicly available DeepLesion dataset,\nhowever it does not provide the full volumetric (3D) extent of lesions and also\ndisplays a severe class imbalance. In this work, we propose a self-training\npipeline to detect 3D lesions and tag them according to the body part they\noccur in. We used a significantly limited 30\\% subset of DeepLesion to train a\nVFNet model for 2D lesion detection and tagging. Next, the 2D lesion context\nwas expanded into 3D, and the mined 3D lesion proposals were integrated back\ninto the baseline training data in order to retrain the model over multiple\nrounds. Through the self-training procedure, our VFNet model learned from its\nown predictions, detected lesions in 3D, and tagged them. Our results indicated\nthat our VFNet model achieved an average sensitivity of 46.9\\% at [0.125:8]\nfalse positives (FP) with a limited 30\\% data subset in comparison to the\n46.8\\% of an existing approach that used the entire DeepLesion dataset. To our\nknowledge, we are the first to jointly detect lesions in 3D and tag them\naccording to the body part label.",
      "pdf_url": "http://arxiv.org/pdf/2504.05201v1",
      "published": "2025-04-07T15:50:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05201v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Universal Lymph Node Detection in Multiparametric MRI with Selective Augmentation",
      "authors": [
        "Tejas Sudharshan Mathai",
        "Sungwon Lee",
        "Thomas C. Shen",
        "Zhiyong Lu",
        "Ronald M. Summers"
      ],
      "abstract": "Robust localization of lymph nodes (LNs) in multiparametric MRI (mpMRI) is\ncritical for the assessment of lymphadenopathy. Radiologists routinely measure\nthe size of LN to distinguish benign from malignant nodes, which would require\nsubsequent cancer staging. Sizing is a cumbersome task compounded by the\ndiverse appearances of LNs in mpMRI, which renders their measurement difficult.\nFurthermore, smaller and potentially metastatic LNs could be missed during a\nbusy clinical day. To alleviate these imaging and workflow problems, we propose\na pipeline to universally detect both benign and metastatic nodes in the body\nfor their ensuing measurement. The recently proposed VFNet neural network was\nemployed to identify LN in T2 fat suppressed and diffusion weighted imaging\n(DWI) sequences acquired by various scanners with a variety of exam protocols.\nWe also use a selective augmentation technique known as Intra-Label LISA (ILL)\nto diversify the input data samples the model sees during training, such that\nit improves its robustness during the evaluation phase. We achieved a\nsensitivity of $\\sim$83\\% with ILL vs. $\\sim$80\\% without ILL at 4 FP/vol.\nCompared with current LN detection approaches evaluated on mpMRI, we show a\nsensitivity improvement of $\\sim$9\\% at 4 FP/vol.",
      "pdf_url": "http://arxiv.org/pdf/2504.05196v1",
      "published": "2025-04-07T15:46:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05196v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Resource-Efficient Beam Prediction in mmWave Communications with Multimodal Realistic Simulation Framework",
      "authors": [
        "Yu Min Park",
        "Yan Kyaw Tun",
        "Walid Saad",
        "Choong Seon Hong"
      ],
      "abstract": "Beamforming is a key technology in millimeter-wave (mmWave) communications\nthat improves signal transmission by optimizing directionality and intensity.\nHowever, conventional channel estimation methods, such as pilot signals or beam\nsweeping, often fail to adapt to rapidly changing communication environments.\nTo address this limitation, multimodal sensing-aided beam prediction has gained\nsignificant attention, using various sensing data from devices such as LiDAR,\nradar, GPS, and RGB images to predict user locations or network conditions.\nDespite its promising potential, the adoption of multimodal sensing-aided beam\nprediction is hindered by high computational complexity, high costs, and\nlimited datasets. Thus, in this paper, a resource-efficient learning approach\nis proposed to transfer knowledge from a multimodal network to a monomodal\n(radar-only) network based on cross-modal relational knowledge distillation\n(CRKD), while reducing computational overhead and preserving predictive\naccuracy. To enable multimodal learning with realistic data, a novel multimodal\nsimulation framework is developed while integrating sensor data generated from\nthe autonomous driving simulator CARLA with MATLAB-based mmWave channel\nmodeling, and reflecting real-world conditions. The proposed CRKD achieves its\nobjective by distilling relational information across different feature spaces,\nwhich enhances beam prediction performance without relying on expensive sensor\ndata. Simulation results demonstrate that CRKD efficiently distills multimodal\nknowledge, allowing a radar-only model to achieve $94.62\\%$ of the teacher\nperformance. In particular, this is achieved with just $10\\%$ of the teacher\nnetwork's parameters, thereby significantly reducing computational complexity\nand dependence on multimodal sensor data.",
      "pdf_url": "http://arxiv.org/pdf/2504.05187v1",
      "published": "2025-04-07T15:38:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05187v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval",
      "authors": [
        "Kidist Amde Mekonnen",
        "Yubao Tang",
        "Maarten de Rijke"
      ],
      "abstract": "Generative information retrieval (GenIR) is a promising neural retrieval\nparadigm that formulates document retrieval as a document identifier (docid)\ngeneration task, allowing for end-to-end optimization toward a unified global\nretrieval objective. However, existing GenIR models suffer from token-level\nmisalignment, where models trained to predict the next token often fail to\ncapture document-level relevance effectively. While reinforcement\nlearning-based methods, such as reinforcement learning from relevance feedback\n(RLRF), aim to address this misalignment through reward modeling, they\nintroduce significant complexity, requiring the optimization of an auxiliary\nreward function followed by reinforcement fine-tuning, which is computationally\nexpensive and often unstable. To address these challenges, we propose direct\ndocument relevance optimization (DDRO), which aligns token-level docid\ngeneration with document-level relevance estimation through direct optimization\nvia pairwise ranking, eliminating the need for explicit reward modeling and\nreinforcement learning. Experimental results on benchmark datasets, including\nMS MARCO document and Natural Questions, show that DDRO outperforms\nreinforcement learning-based methods, achieving a 7.4% improvement in MRR@10\nfor MS MARCO and a 19.9% improvement for Natural Questions. These findings\nhighlight DDRO's potential to enhance retrieval effectiveness with a simplified\noptimization approach. By framing alignment as a direct optimization problem,\nDDRO simplifies the ranking optimization pipeline of GenIR models while\noffering a viable alternative to reinforcement learning-based methods.",
      "pdf_url": "http://arxiv.org/pdf/2504.05181v1",
      "published": "2025-04-07T15:27:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05181v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DL",
        "cs.LG",
        "H.3.3"
      ]
    },
    {
      "title": "BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks",
      "authors": [
        "Wei Li",
        "Yang Zou",
        "Christopher Ellis",
        "Ruben Purdy",
        "Shawn Blanton",
        "José M. F. Moura"
      ],
      "abstract": "While many EDA tasks already involve graph-based data, existing LLMs in EDA\nprimarily either represent graphs as sequential text, or simply ignore\ngraph-structured data that might be beneficial like dataflow graphs of RTL\ncode. Recent studies have found that LLM performance suffers when graphs are\nrepresented as sequential text, and using additional graph information\nsignificantly boosts performance. To address these challenges, we introduce\nBRIDGES, a framework designed to incorporate graph modality into LLMs for EDA\ntasks. BRIDGES integrates an automated data generation workflow, a solution\nthat combines graph modality with LLM, and a comprehensive evaluation suite.\nFirst, we establish an LLM-driven workflow to generate RTL and netlist-level\ndata, converting them into dataflow and netlist graphs with function\ndescriptions. This workflow yields a large-scale dataset comprising over\n500,000 graph instances and more than 1.5 billion tokens. Second, we propose a\nlightweight cross-modal projector that encodes graph representations into\ntext-compatible prompts, enabling LLMs to effectively utilize graph data\nwithout architectural modifications. Experimental results demonstrate 2x to 10x\nimprovements across multiple tasks compared to text-only baselines, including\naccuracy in design retrieval, type prediction and perplexity in function\ndescription, with negligible computational overhead (<1% model weights increase\nand <30% additional runtime overhead). Even without additional LLM finetuning,\nour results outperform text-only by a large margin. We plan to release BRIDGES,\nincluding the dataset, models, and training flow.",
      "pdf_url": "http://arxiv.org/pdf/2504.05180v1",
      "published": "2025-04-07T15:27:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05180v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Attention-Based Multi-Scale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes",
      "authors": [
        "Guangqiang Li",
        "M. Amine Atoui",
        "Xiangshun Li"
      ],
      "abstract": "Fault diagnosis in multimode processes plays a critical role in ensuring the\nsafe operation of industrial systems across multiple modes. It faces a great\nchallenge yet to be addressed - that is, the significant distributional\ndifferences among monitoring data from multiple modes make it difficult for the\nmodels to extract shared feature representations related to system health\nconditions. In response to this problem, this paper introduces a novel method\ncalled attention-based multi-scale temporal fusion network. The multi-scale\ndepthwise convolution and gated recurrent unit are employed to extract\nmulti-scale contextual local features and long-short-term features. A temporal\nattention mechanism is designed to focus on critical time points with higher\ncross-mode shared information, thereby enhancing the accuracy of fault\ndiagnosis. The proposed model is applied to Tennessee Eastman process dataset\nand three-phase flow facility dataset. The experiments demonstrate that the\nproposed model achieves superior diagnostic performance and maintains a small\nmodel size.",
      "pdf_url": "http://arxiv.org/pdf/2504.05172v1",
      "published": "2025-04-07T15:16:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05172v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection",
      "authors": [
        "Bonan Ding",
        "Jin Xie",
        "Jing Nie",
        "Jiale Cao"
      ],
      "abstract": "Multimodal 3D object detection based on deep neural networks has indeed made\nsignificant progress. However, it still faces challenges due to the\nmisalignment of scale and spatial information between features extracted from\n2D images and those derived from 3D point clouds. Existing methods usually\naggregate multimodal features at a single stage. However, leveraging\nmulti-stage cross-modal features is crucial for detecting objects of various\nscales. Therefore, these methods often struggle to integrate features across\ndifferent scales and modalities effectively, thereby restricting the accuracy\nof detection. Additionally, the time-consuming Query-Key-Value-based\n(QKV-based) cross-attention operations often utilized in existing methods aid\nin reasoning the location and existence of objects by capturing non-local\ncontexts. However, this approach tends to increase computational complexity. To\naddress these challenges, we present SSLFusion, a novel Scale & Space Aligned\nLatent Fusion Model, consisting of a scale-aligned fusion strategy (SAF), a\n3D-to-2D space alignment module (SAM), and a latent cross-modal fusion module\n(LFM). SAF mitigates scale misalignment between modalities by aggregating\nfeatures from both images and point clouds across multiple levels. SAM is\ndesigned to reduce the inter-modal gap between features from images and point\nclouds by incorporating 3D coordinate information into 2D image features.\nAdditionally, LFM captures cross-modal non-local contexts in the latent space\nwithout utilizing the QKV-based attention operations, thus mitigating\ncomputational complexity. Experiments on the KITTI and DENSE datasets\ndemonstrate that our SSLFusion outperforms state-of-the-art methods. Our\napproach obtains an absolute gain of 2.15% in 3D AP, compared with the\nstate-of-art method GraphAlign on the moderate level of the KITTI test set.",
      "pdf_url": "http://arxiv.org/pdf/2504.05170v1",
      "published": "2025-04-07T15:15:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05170v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "RLBayes: a Bayesian Network Structure Learning Algorithm via Reinforcement Learning-Based Search Strategy",
      "authors": [
        "Mingcan Wang",
        "Junchang Xin",
        "Luxuan Qu",
        "Qi Chen",
        "Zhiqiong Wang"
      ],
      "abstract": "The score-based structure learning of Bayesian network (BN) is an effective\nway to learn BN models, which are regarded as some of the most compelling\nprobabilistic graphical models in the field of representation and reasoning\nunder uncertainty. However, the search space of structure learning grows\nsuper-exponentially as the number of variables increases, which makes BN\nstructure learning an NP-hard problem, as well as a combination optimization\nproblem (COP). Despite the successes of many heuristic methods on it, the\nresults of the structure learning of BN are usually unsatisfactory. Inspired by\nQ-learning, in this paper, a Bayesian network structure learning algorithm via\nreinforcement learning-based (RL-based) search strategy is proposed, namely\nRLBayes. The method borrows the idea of RL and tends to record and guide the\nlearning process by a dynamically maintained Q-table. By creating and\nmaintaining the dynamic Q-table, RLBayes achieve storing the unlimited search\nspace within limited space, thereby achieving the structure learning of BN via\nQ-learning. Not only is it theoretically proved that RLBayes can converge to\nthe global optimal BN structure, but also it is experimentally proved that\nRLBayes has a better effect than almost all other heuristic search algorithms.",
      "pdf_url": "http://arxiv.org/pdf/2504.05167v1",
      "published": "2025-04-07T15:11:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05167v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods under Knowledge Incompleteness",
      "authors": [
        "Dongzhuoran Zhou",
        "Yuqicheng Zhu",
        "Yuan He",
        "Jiaoyan Chen",
        "Evgeny Kharlamov",
        "Steffen Staab"
      ],
      "abstract": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings.",
      "pdf_url": "http://arxiv.org/pdf/2504.05163v1",
      "published": "2025-04-07T15:08:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05163v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Leveraging Label Potential for Enhanced Multimodal Emotion Recognition",
      "authors": [
        "Xuechun Shao",
        "Yinfeng Yu",
        "Liejun Wang"
      ],
      "abstract": "Multimodal emotion recognition (MER) seeks to integrate various modalities to\npredict emotional states accurately. However, most current research focuses\nsolely on the fusion of audio and text features, overlooking the valuable\ninformation in emotion labels. This oversight could potentially hinder the\nperformance of existing methods, as emotion labels harbor rich, insightful\ninformation that could significantly aid MER. We introduce a novel model called\nLabel Signal-Guided Multimodal Emotion Recognition (LSGMER) to overcome this\nlimitation. This model aims to fully harness the power of emotion label\ninformation to boost the classification accuracy and stability of MER.\nSpecifically, LSGMER employs a Label Signal Enhancement module that optimizes\nthe representation of modality features by interacting with audio and text\nfeatures through label embeddings, enabling it to capture the nuances of\nemotions precisely. Furthermore, we propose a Joint Objective Optimization(JOO)\napproach to enhance classification accuracy by introducing the\nAttribution-Prediction Consistency Constraint (APC), which strengthens the\nalignment between fused features and emotion categories. Extensive experiments\nconducted on the IEMOCAP and MELD datasets have demonstrated the effectiveness\nof our proposed LSGMER model.",
      "pdf_url": "http://arxiv.org/pdf/2504.05158v1",
      "published": "2025-04-07T15:00:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05158v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "A Reinforcement Learning Method for Environments with Stochastic Variables: Post-Decision Proximal Policy Optimization with Dual Critic Networks",
      "authors": [
        "Leonardo Kanashiro Felizardo",
        "Edoardo Fadda",
        "Paolo Brandimarte",
        "Emilio Del-Moral-Hernandez",
        "Mariá Cristina Vasconcelos Nascimento"
      ],
      "abstract": "This paper presents Post-Decision Proximal Policy Optimization (PDPPO), a\nnovel variation of the leading deep reinforcement learning method, Proximal\nPolicy Optimization (PPO). The PDPPO state transition process is divided into\ntwo steps: a deterministic step resulting in the post-decision state and a\nstochastic step leading to the next state. Our approach incorporates\npost-decision states and dual critics to reduce the problem's dimensionality\nand enhance the accuracy of value function estimation. Lot-sizing is a mixed\ninteger programming problem for which we exemplify such dynamics. The objective\nof lot-sizing is to optimize production, delivery fulfillment, and inventory\nlevels in uncertain demand and cost parameters. This paper evaluates the\nperformance of PDPPO across various environments and configurations. Notably,\nPDPPO with a dual critic architecture achieves nearly double the maximum reward\nof vanilla PPO in specific scenarios, requiring fewer episode iterations and\ndemonstrating faster and more consistent learning across different\ninitializations. On average, PDPPO outperforms PPO in environments with a\nstochastic component in the state transition. These results support the\nbenefits of using a post-decision state. Integrating this post-decision state\nin the value function approximation leads to more informed and efficient\nlearning in high-dimensional and stochastic environments.",
      "pdf_url": "http://arxiv.org/pdf/2504.05150v1",
      "published": "2025-04-07T14:56:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05150v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6; G.1.6"
      ]
    },
    {
      "title": "EffOWT: Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively",
      "authors": [
        "Bingyang Wang",
        "Kaer Huang",
        "Bin Li",
        "Yiqiang Yan",
        "Lihe Zhang",
        "Huchuan Lu",
        "You He"
      ],
      "abstract": "Open-World Tracking (OWT) aims to track every object of any category, which\nrequires the model to have strong generalization capabilities. Trackers can\nimprove their generalization ability by leveraging Visual Language Models\n(VLMs). However, challenges arise with the fine-tuning strategies when VLMs are\ntransferred to OWT: full fine-tuning results in excessive parameter and memory\ncosts, while the zero-shot strategy leads to sub-optimal performance. To solve\nthe problem, EffOWT is proposed for efficiently transferring VLMs to OWT.\nSpecifically, we build a small and independent learnable side network outside\nthe VLM backbone. By freezing the backbone and only executing backpropagation\non the side network, the model's efficiency requirements can be met. In\naddition, EffOWT enhances the side network by proposing a hybrid structure of\nTransformer and CNN to improve the model's performance in the OWT field.\nFinally, we implement sparse interactions on the MLP, thus reducing parameter\nupdates and memory costs significantly. Thanks to the proposed methods, EffOWT\nachieves an absolute gain of 5.5% on the tracking metric OWTA for unknown\ncategories, while only updating 1.3% of the parameters compared to full\nfine-tuning, with a 36.4% memory saving. Other metrics also demonstrate obvious\nimprovement.",
      "pdf_url": "http://arxiv.org/pdf/2504.05141v1",
      "published": "2025-04-07T14:47:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05141v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Interpretable Style Takagi-Sugeno-Kang Fuzzy Clustering",
      "authors": [
        "Suhang Gu",
        "Ye Wang",
        "Yongxin Chou",
        "Jinliang Cong",
        "Mingli Lu",
        "Zhuqing Jiao"
      ],
      "abstract": "Clustering is an efficient and essential technique for exploring latent\nknowledge of data. However, limited attention has been given to the\ninterpretability of the clusters detected by most clustering algorithms. In\naddition, due to the homogeneity of data, different groups of data have their\nown homogeneous styles. In this paper, the above two aspects are considered,\nand an interpretable style Takagi-Sugeno-Kang (TSK) fuzzy clustering\n(IS-TSK-FC) algorithm is proposed. The clustering behavior of IS-TSK-FC is\nfully guided by the TSK fuzzy inference on fuzzy rules. In particular, samples\nare grouped into clusters represented by the corresponding consequent vectors\nof all fuzzy rules learned in an unsupervised manner. This can explain how the\nclusters are generated in detail, thus making the underlying decision-making\nprocess of the IS-TSK-FC interpretable. Moreover, a series of style matrices\nare introduced to facilitate the consequents of fuzzy rules in IS-TSK-FC by\ncapturing the styles of clusters as well as the nuances between different\nstyles. Consequently, all the fuzzy rules in IS-TSK-FC have powerful data\nrepresentation capability. After determining the antecedents of all the fuzzy\nrules, the optimization problem of IS-TSK-FC can be iteratively solved in an\nalternation manner. The effectiveness of IS-TSK-FC as an interpretable\nclustering tool is validated through extensive experiments on benchmark\ndatasets with unknown implicit/explicit styles. Specially, the superior\nclustering performance of IS-TSK-FC is demonstrated on case studies where\ndifferent groups of data present explicit styles. The source code of IS-TSK-FC\ncan be downloaded from https://github.com/gusuhang10/IS-TSK-FC.",
      "pdf_url": "http://arxiv.org/pdf/2504.05125v1",
      "published": "2025-04-07T14:28:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05125v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection",
      "authors": [
        "Jon Gutiérrez Zaballa",
        "Koldo Basterretxea",
        "Javier Echanobe"
      ],
      "abstract": "Machine learning-based embedded systems for safety-critical applications,\nsuch as aerospace and autonomous driving, must be robust to perturbations\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\nmodern electronic devices become more susceptible to background radiation,\nincreasing the concern about failures produced by soft errors. The resilience\nof deep neural networks (DNNs) to these errors depends not only on target\ndevice technology but also on model structure and the numerical representation\nand arithmetic precision of their parameters. Compression techniques like\npruning and quantization, used to reduce memory footprint and computational\ncomplexity, alter both model structure and representation, affecting soft error\nrobustness. In this regard, although often overlooked, the choice of activation\nfunctions (AFs) impacts not only accuracy and trainability but also\ncompressibility and error resilience. This paper explores the use of bounded\nAFs to enhance robustness against parameter perturbations, while evaluating\ntheir effects on model accuracy, compressibility, and computational load with a\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\ndeveloped for semantic segmentation of hyperspectral images with application to\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\nSoM.",
      "pdf_url": "http://arxiv.org/pdf/2504.05119v1",
      "published": "2025-04-07T14:21:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05119v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.CV",
        "eess.IV"
      ]
    },
    {
      "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks",
      "authors": [
        "Yu Yue",
        "Yufeng Yuan",
        "Qiying Yu",
        "Xiaochen Zuo",
        "Ruofei Zhu",
        "Wenyuan Xu",
        "Jiaze Chen",
        "Chengyi Wang",
        "TianTian Fan",
        "Zhengyin Du",
        "Xiangpeng Wei",
        "Xiangyu Yu",
        "Gaohong Liu",
        "Juncai Liu",
        "Lingjun Liu",
        "Haibin Lin",
        "Zhiqi Lin",
        "Bole Ma",
        "Chi Zhang",
        "Mofan Zhang",
        "Wang Zhang",
        "Hang Zhu",
        "Ru Zhang",
        "Xin Liu",
        "Mingxuan Wang",
        "Yonghui Wu",
        "Lin Yan"
      ],
      "abstract": "We present VAPO, Value-based Augmented Proximal Policy Optimization framework\nfor reasoning models., a novel framework tailored for reasoning models within\nthe value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the\nQwen 32B pre-trained model, attains a state-of-the-art score of\n$\\mathbf{60.4}$. In direct comparison under identical experimental settings,\nVAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B\nand DAPO by more than 10 points. The training process of VAPO stands out for\nits stability and efficiency. It reaches state-of-the-art performance within a\nmere 5,000 steps. Moreover, across multiple independent runs, no training\ncrashes occur, underscoring its reliability. This research delves into long\nchain-of-thought (long-CoT) reasoning using a value-based reinforcement\nlearning framework. We pinpoint three key challenges that plague value-based\nmethods: value model bias, the presence of heterogeneous sequence lengths, and\nthe sparsity of reward signals. Through systematic design, VAPO offers an\nintegrated solution that effectively alleviates these challenges, enabling\nenhanced performance in long-CoT reasoning tasks.",
      "pdf_url": "http://arxiv.org/pdf/2504.05118v2",
      "published": "2025-04-07T14:21:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05118v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
      "authors": [
        "Anja Surina",
        "Amin Mansouri",
        "Lars Quaedvlieg",
        "Amal Seddas",
        "Maryna Viazovska",
        "Emmanuel Abbe",
        "Caglar Gulcehre"
      ],
      "abstract": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non three combinatorial optimization tasks - bin packing, traveling salesman,\nand the flatpack problem - show that combining RL and evolutionary search\nimproves discovery efficiency of improved algorithms, showcasing the potential\nof RL-enhanced evolutionary strategies to assist computer scientists and\nmathematicians for more efficient algorithm design.",
      "pdf_url": "http://arxiv.org/pdf/2504.05108v1",
      "published": "2025-04-07T14:14:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05108v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ]
    },
    {
      "title": "SpeakEasy: Enhancing Text-to-Speech Interactions for Expressive Content Creation",
      "authors": [
        "Stephen Brade",
        "Sam Anderson",
        "Rithesh Kumar",
        "Zeyu Jin",
        "Anh Truong"
      ],
      "abstract": "Novice content creators often invest significant time recording expressive\nspeech for social media videos. While recent advancements in text-to-speech\n(TTS) technology can generate highly realistic speech in various languages and\naccents, many struggle with unintuitive or overly granular TTS interfaces. We\npropose simplifying TTS generation by allowing users to specify high-level\ncontext alongside their script. Our Wizard-of-Oz system, SpeakEasy, leverages\nuser-provided context to inform and influence TTS output, enabling iterative\nrefinement with high-level feedback. This approach was informed by two\n8-subject formative studies: one examining content creators' experiences with\nTTS, and the other drawing on effective strategies from voice actors. Our\nevaluation shows that participants using SpeakEasy were more successful in\ngenerating performances matching their personal standards, without requiring\nsignificantly more effort than leading industry interfaces.",
      "pdf_url": "http://arxiv.org/pdf/2504.05106v1",
      "published": "2025-04-07T14:13:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05106v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models",
      "authors": [
        "Jiawei Lian",
        "Jianhong Pan",
        "Lefan Wang",
        "Yi Wang",
        "Shaohui Mei",
        "Lap-Pui Chau"
      ],
      "abstract": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.",
      "pdf_url": "http://arxiv.org/pdf/2504.05050v1",
      "published": "2025-04-07T13:20:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05050v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning",
      "authors": [
        "Sugyeong Eo",
        "Hyeonseok Moon",
        "Evelyn Hayoon Zi",
        "Chanjun Park",
        "Heuiseok Lim"
      ],
      "abstract": "Multiagent collaboration has emerged as a promising framework for enhancing\nthe reasoning capabilities of large language models (LLMs). While this approach\nimproves reasoning capability, it incurs substantial computational overhead due\nto iterative agent interactions. Furthermore, engaging in debates for queries\nthat do not necessitate collaboration amplifies the risk of error generation.\nTo address these challenges, we propose Debate Only When Necessary (DOWN), an\nadaptive multiagent debate framework that selectively activates the debate\nprocess based on the confidence score of the agent's initial response. For\nqueries where debate is triggered, agents refine their outputs using responses\nfrom participating agents and their confidence scores. Experimental results\ndemonstrate that this mechanism significantly improves efficiency while\nmaintaining or even surpassing the performance of existing multiagent debate\nsystems. We also find that confidence-guided debate mitigates error propagation\nand enhances the selective incorporation of reliable responses. These results\nestablish DOWN as an optimization strategy for efficient and effective\nmultiagent reasoning, facilitating the practical deployment of LLM-based\ncollaboration.",
      "pdf_url": "http://arxiv.org/pdf/2504.05047v1",
      "published": "2025-04-07T13:17:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05047v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Graph-based Diffusion Model for Collaborative Filtering",
      "authors": [
        "Xuan Zhang",
        "Xiang Deng",
        "Hongxing Yuan",
        "Chunyu Wei",
        "Yushun Fan"
      ],
      "abstract": "Recently, diffusion-based recommendation methods have achieved impressive\nresults. However, existing approaches predominantly treat each user's\nhistorical interactions as independent training samples, overlooking the\npotential of higher-order collaborative signals between users and items. Such\nsignals, which encapsulate richer and more nuanced relationships, can be\nnaturally captured using graph-based data structures. To address this\nlimitation, we extend diffusion-based recommendation methods to the graph\ndomain by directly modeling user-item bipartite graphs with diffusion models.\nThis enables better modeling of the higher-order connectivity inherent in\ncomplex interaction dynamics. However, this extension introduces two primary\nchallenges: (1) Noise Heterogeneity, where interactions are influenced by\nvarious forms of continuous and discrete noise, and (2) Relation Explosion,\nreferring to the high computational costs of processing large-scale graphs. To\ntackle these challenges, we propose a Graph-based Diffusion Model for\nCollaborative Filtering (GDMCF). To address noise heterogeneity, we introduce a\nmulti-level noise corruption mechanism that integrates both continuous and\ndiscrete noise, effectively simulating real-world interaction complexities. To\nmitigate relation explosion, we design a user-active guided diffusion process\nthat selectively focuses on the most meaningful edges and active users,\nreducing inference costs while preserving the graph's topological integrity.\nExtensive experiments on three benchmark datasets demonstrate that GDMCF\nconsistently outperforms state-of-the-art methods, highlighting its\neffectiveness in capturing higher-order collaborative signals and improving\nrecommendation performance.",
      "pdf_url": "http://arxiv.org/pdf/2504.05029v1",
      "published": "2025-04-07T12:51:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05029v1",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Batch Aggregation: An Approach to Enhance Text Classification with Correlated Augmented Data",
      "authors": [
        "Charco Hui",
        "Yalu Wen"
      ],
      "abstract": "Natural language processing models often face challenges due to limited\nlabeled data, especially in domain specific areas, e.g., clinical trials. To\novercome this, text augmentation techniques are commonly used to increases\nsample size by transforming the original input data into artificial ones with\nthe label preserved. However, traditional text classification methods ignores\nthe relationship between augmented texts and treats them as independent samples\nwhich may introduce classification error. Therefore, we propose a novel\napproach called 'Batch Aggregation' (BAGG) which explicitly models the\ndependence of text inputs generated through augmentation by incorporating an\nadditional layer that aggregates results from correlated texts. Through\nstudying multiple benchmark data sets across different domains, we found that\nBAGG can improve classification accuracy. We also found that the increase of\nperformance with BAGG is more obvious in domain specific data sets, with\naccuracy improvements of up to 10-29%. Through the analysis of benchmark data,\nthe proposed method addresses limitations of traditional techniques and\nimproves robustness in text classification tasks. Our result demonstrates that\nBAGG offers more robust results and outperforms traditional approaches when\ntraining data is limited.",
      "pdf_url": "http://arxiv.org/pdf/2504.05020v1",
      "published": "2025-04-07T12:46:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05020v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Measuring the right thing: justifying metrics in AI impact assessments",
      "authors": [
        "Stefan Buijsman",
        "Herman Veluwenkamp"
      ],
      "abstract": "AI Impact Assessments are only as good as the measures used to assess the\nimpact of these systems. It is therefore paramount that we can justify our\nchoice of metrics in these assessments, especially for difficult to quantify\nethical and social values. We present a two-step approach to ensure metrics are\nproperly motivated. First, a conception needs to be spelled out (e.g. Rawlsian\nfairness or fairness as solidarity) and then a metric can be fitted to that\nconception. Both steps require separate justifications, as conceptions can be\njudged on how well they fit with the function of, for example, fairness. We\nargue that conceptual engineering offers helpful tools for this step. Second,\nmetrics need to be fitted to a conception. We illustrate this process through\nan examination of competing fairness metrics to illustrate that here the\nadditional content that a conception offers helps us justify the choice for a\nspecific metric. We thus advocate that impact assessments are not only clear on\ntheir metrics, but also on the conceptions that motivate those metrics.",
      "pdf_url": "http://arxiv.org/pdf/2504.05007v1",
      "published": "2025-04-07T12:32:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.05007v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "SurvSurf: a partially monotonic neural network for first-hitting time prediction of intermittently observed discrete and continuous sequential events",
      "authors": [
        "Yichen Kelly Chen",
        "Sören Dittmer",
        "Kinga Bernatowicz",
        "Josep Arús-Pous",
        "Kamen Bliznashki",
        "John Aston",
        "James H. F. Rudd",
        "Carola-Bibiane Schönlieb",
        "James Jones",
        "Michael Roberts"
      ],
      "abstract": "We propose a neural-network based survival model (SurvSurf) specifically\ndesigned for direct and simultaneous probabilistic prediction of the first\nhitting time of sequential events from baseline. Unlike existing models,\nSurvSurf is theoretically guaranteed to never violate the monotonic\nrelationship between the cumulative incidence functions of sequential events,\nwhile allowing nonlinear influence from predictors. It also incorporates\nimplicit truths for unobserved intermediate events in model fitting, and\nsupports both discrete and continuous time and events. We also identified a\nvariant of the Integrated Brier Score (IBS) that showed robust correlation with\nthe mean squared error (MSE) between the true and predicted probabilities by\naccounting for implied truths about the missing intermediate events. We\ndemonstrated the superiority of SurvSurf compared to modern and traditional\npredictive survival models in two simulated datasets and two real-world\ndatasets, using MSE, the more robust IBS and by measuring the extent of\nmonotonicity violation.",
      "pdf_url": "http://arxiv.org/pdf/2504.04997v1",
      "published": "2025-04-07T12:24:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04997v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.AP",
        "stat.TH",
        "62N01"
      ]
    },
    {
      "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs",
      "authors": [
        "Ling Hu",
        "Yuemei Xu",
        "Xiaoyang Gu",
        "Letao Han"
      ],
      "abstract": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.",
      "pdf_url": "http://arxiv.org/pdf/2504.04994v1",
      "published": "2025-04-07T12:23:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04994v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model",
      "authors": [
        "Congcong Wen",
        "Yiting Lin",
        "Xiaokang Qu",
        "Nan Li",
        "Yong Liao",
        "Hui Lin",
        "Xiang Li"
      ],
      "abstract": "Recent progress in VLMs has demonstrated impressive capabilities across a\nvariety of tasks in the natural image domain. Motivated by these advancements,\nthe remote sensing community has begun to adopt VLMs for remote sensing\nvision-language tasks, including scene understanding, image captioning, and\nvisual question answering. However, existing remote sensing VLMs typically rely\non closed-set scene understanding and focus on generic scene descriptions, yet\nlack the ability to incorporate external knowledge. This limitation hinders\ntheir capacity for semantic reasoning over complex or context-dependent queries\nthat involve domain-specific or world knowledge. To address these challenges,\nwe first introduced a multimodal Remote Sensing World Knowledge (RSWK) dataset,\nwhich comprises high-resolution satellite imagery and detailed textual\ndescriptions for 14,141 well-known landmarks from 175 countries, integrating\nboth remote sensing domain knowledge and broader world knowledge. Building upon\nthis dataset, we proposed a novel Remote Sensing Retrieval-Augmented Generation\n(RS-RAG) framework, which consists of two key components. The Multi-Modal\nKnowledge Vector Database Construction module encodes remote sensing imagery\nand associated textual knowledge into a unified vector space. The Knowledge\nRetrieval and Response Generation module retrieves and re-ranks relevant\nknowledge based on image and/or text queries, and incorporates the retrieved\ncontent into a knowledge-augmented prompt to guide the VLM in producing\ncontextually grounded responses. We validated the effectiveness of our approach\non three representative vision-language tasks, including image captioning,\nimage classification, and visual question answering, where RS-RAG significantly\noutperformed state-of-the-art baselines.",
      "pdf_url": "http://arxiv.org/pdf/2504.04988v1",
      "published": "2025-04-07T12:13:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04988v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Transforming Future Data Center Operations and Management via Physical AI",
      "authors": [
        "Zhiwei Cao",
        "Minghao Li",
        "Feng Lin",
        "Qiang Fu",
        "Jimin Jia",
        "Yonggang Wen",
        "Jianxiong Yin",
        "Simon See"
      ],
      "abstract": "Data centers (DCs) as mission-critical infrastructures are pivotal in\npowering the growth of artificial intelligence (AI) and the digital economy.\nThe evolution from Internet DC to AI DC has introduced new challenges in\noperating and managing data centers for improved business resilience and\nreduced total cost of ownership. As a result, new paradigms, beyond the\ntraditional approaches based on best practices, must be in order for future\ndata centers. In this research, we propose and develop a novel Physical AI\n(PhyAI) framework for advancing DC operations and management. Our system\nleverages the emerging capabilities of state-of-the-art industrial products and\nour in-house research and development. Specifically, it presents three core\nmodules, namely: 1) an industry-grade in-house simulation engine to simulate DC\noperations in a highly accurate manner, 2) an AI engine built upon NVIDIA\nPhysicsNemo for the training and evaluation of physics-informed machine\nlearning (PIML) models, and 3) a digital twin platform built upon NVIDIA\nOmniverse for our proposed 5-tier digital twin framework. This system presents\na scalable and adaptable solution to digitalize, optimize, and automate future\ndata center operations and management, by enabling real-time digital twins for\nfuture data centers. To illustrate its effectiveness, we present a compelling\ncase study on building a surrogate model for predicting the thermal and airflow\nprofiles of a large-scale DC in a real-time manner. Our results demonstrate its\nsuperior performance over traditional time-consuming Computational Fluid\nDynamics/Heat Transfer (CFD/HT) simulation, with a median absolute temperature\nprediction error of 0.18 {\\deg}C. This emerging approach would open doors to\nseveral potential research directions for advancing Physical AI in future DC\noperations.",
      "pdf_url": "http://arxiv.org/pdf/2504.04982v1",
      "published": "2025-04-07T12:09:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04982v1",
      "categories": [
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "DiCoTTA: Domain-invariant Learning for Continual Test-time Adaptation",
      "authors": [
        "Sohyun Lee",
        "Nayeong Kim",
        "Juwon Kang",
        "Seong Joon Oh",
        "Suha Kwak"
      ],
      "abstract": "This paper studies continual test-time adaptation (CTTA), the task of\nadapting a model to constantly changing unseen domains in testing while\npreserving previously learned knowledge. Existing CTTA methods mostly focus on\nadaptation to the current test domain only, overlooking generalization to\narbitrary test domains a model may face in the future. To tackle this\nlimitation, we present a novel online domain-invariant learning framework for\nCTTA, dubbed DiCoTTA. DiCoTTA aims to learn feature representation to be\ninvariant to both current and previous test domains on the fly during testing.\nTo this end, we propose a new model architecture and a test-time adaptation\nstrategy dedicated to learning domain-invariant features without corrupting\nsemantic contents, along with a new data structure and optimization algorithm\nfor effectively managing information from previous test domains. DiCoTTA\nachieved state-of-the-art performance on four public CTTA benchmarks. Moreover,\nit showed superior generalization to unseen test domains.",
      "pdf_url": "http://arxiv.org/pdf/2504.04981v1",
      "published": "2025-04-07T12:09:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04981v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
      "authors": [
        "Ming Li",
        "Ruiyi Zhang",
        "Jian Chen",
        "Jiuxiang Gu",
        "Yufan Zhou",
        "Franck Dernoncourt",
        "Wanrong Zhu",
        "Tianyi Zhou",
        "Tong Sun"
      ],
      "abstract": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2504.04974v1",
      "published": "2025-04-07T12:01:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04974v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds",
      "authors": [
        "Qian Zuo",
        "Fengxiang He"
      ],
      "abstract": "This paper studies constrained Markov decision processes (CMDPs) with\nconstraints against stochastic thresholds, aiming at safety of reinforcement\nlearning in unknown and uncertain environments. We leverage a Growing-Window\nestimator sampling from interactions with the uncertain and dynamic environment\nto estimate the thresholds, based on which we design Stochastic\nPessimistic-Optimistic Thresholding (SPOT), a novel model-based primal-dual\nalgorithm for multiple constraints against stochastic thresholds. SPOT enables\nreinforcement learning under both pessimistic and optimistic threshold\nsettings. We prove that our algorithm achieves sublinear regret and constraint\nviolation; i.e., a reward regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ while\nallowing an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ constraint violation over $T$\nepisodes. The theoretical guarantees show that our algorithm achieves\nperformance comparable to that of an approach relying on fixed and clear\nthresholds. To the best of our knowledge, SPOT is the first reinforcement\nlearning algorithm that realises theoretical guaranteed performance in an\nuncertain environment where even thresholds are unknown.",
      "pdf_url": "http://arxiv.org/pdf/2504.04973v1",
      "published": "2025-04-07T11:58:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04973v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "A High-Force Gripper with Embedded Multimodal Sensing for Powerful and Perception Driven Grasping",
      "authors": [
        "Edoardo Del Bianco",
        "Davide Torielli",
        "Federico Rollo",
        "Damiano Gasperini",
        "Arturo Laurenzi",
        "Lorenzo Baccelliere",
        "Luca Muratore",
        "Marco Roveri",
        "Nikos G. Tsagarakis"
      ],
      "abstract": "Modern humanoid robots have shown their promising potential for executing\nvarious tasks involving the grasping and manipulation of objects using their\nend-effectors. Nevertheless, in the most of the cases, the grasping and\nmanipulation actions involve low to moderate payload and interaction forces.\nThis is due to limitations often presented by the end-effectors, which can not\nmatch their arm-reachable payload, and hence limit the payload that can be\ngrasped and manipulated. In addition, grippers usually do not embed adequate\nperception in their hardware, and grasping actions are mainly driven by\nperception sensors installed in the rest of the robot body, frequently affected\nby occlusions due to the arm motions during the execution of the grasping and\nmanipulation tasks. To address the above, we developed a modular high grasping\nforce gripper equipped with embedded multi-modal perception functionalities.\nThe proposed gripper can generate a grasping force of 110 N in a compact\nimplementation. The high grasping force capability is combined with embedded\nmulti-modal sensing, which includes an eye-in-hand camera, a Time-of-Flight\n(ToF) distance sensor, an Inertial Measurement Unit (IMU) and an\nomnidirectional microphone, permitting the implementation of perception-driven\ngrasping functionalities.\n  We extensively evaluated the grasping force capacity of the gripper by\nintroducing novel payload evaluation metrics that are a function of the robot\narm's dynamic motion and gripper thermal states. We also evaluated the embedded\nmulti-modal sensing by performing perception-guided enhanced grasping\noperations.",
      "pdf_url": "http://arxiv.org/pdf/2504.04970v1",
      "published": "2025-04-07T11:57:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04970v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection",
      "authors": [
        "Jiayang Huang",
        "Lingjie Li",
        "Kang Zhang",
        "David Yip"
      ],
      "abstract": "This paper introduces the art project The Dream Within Huang Long Cave, an\nAI-driven interactive and immersive narrative experience. The project offers\nnew insights into AI technology, artistic practice, and psychoanalysis.\nInspired by actual geographical landscapes and familial archetypes, the work\ncombines psychoanalytic theory and computational technology, providing an\nartistic response to the concept of the non-existence of the Big Other. The\nnarrative is driven by a combination of a large language model (LLM) and a\nrealistic digital character, forming a virtual agent named YELL. Through\ndialogue and exploration within a cave automatic virtual environment (CAVE),\nthe audience is invited to unravel the language puzzles presented by YELL and\nhelp him overcome his life challenges. YELL is a fictional embodiment of the\nBig Other, modeled after the artist's real father. Through a cross-temporal\ninteraction with this digital father, the project seeks to deconstruct complex\nfamilial relationships. By demonstrating the non-existence of the Big Other, we\naim to underscore the authenticity of interpersonal emotions, positioning art\nas a bridge for emotional connection and understanding within family dynamics.",
      "pdf_url": "http://arxiv.org/pdf/2504.04968v1",
      "published": "2025-04-07T11:54:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04968v1",
      "categories": [
        "cs.MM",
        "cs.AI"
      ]
    },
    {
      "title": "GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision",
      "authors": [
        "Aditya Hemant Shahane",
        "Prathosh A. P",
        "Sandeep Kumar"
      ],
      "abstract": "Graphs are growing rapidly, along with the number of distinct label\ncategories associated with them. Applications like e-commerce, healthcare,\nrecommendation systems, and various social media platforms are rapidly moving\ntowards graph representation of data due to their ability to capture both\nstructural and attribute information. One crucial task in graph analysis is\nnode classification, where unlabeled nodes are categorized into predefined\nclasses. In practice, novel classes appear incrementally sometimes with just a\nfew labels (seen classes) or even without any labels (unseen classes), either\nbecause they are new or haven't been explored much. Traditional methods assume\nabundant labeled data for training, which isn't always feasible. We investigate\na broader objective: \\emph{Graph Class Incremental Learning under Weak\nSupervision (GCL)}, addressing this challenge by meta-training on base classes\nwith limited labeled instances. During the incremental streams, novel classes\ncan have few-shot or zero-shot representation. Our proposed framework GOTHAM\nefficiently accommodates these unlabeled nodes by finding the closest prototype\nrepresentation, serving as class representatives in the attribute space. For\nText-Attributed Graphs (TAGs), our framework additionally incorporates semantic\ninformation to enhance the representation. By employing teacher-student\nknowledge distillation to mitigate forgetting, GOTHAM achieves promising\nresults across various tasks. Experiments on datasets such as Cora-ML, Amazon,\nand OBGN-Arxiv showcase the effectiveness of our approach in handling evolving\ngraph data under limited supervision. The repository is available here:\n\\href{https://github.com/adityashahane10/GOTHAM--Graph-based-Class-Incremental-Learning-Framework-under-Weak-Supervision}{\\small\n\\textcolor{blue}{Code}}",
      "pdf_url": "http://arxiv.org/pdf/2504.04954v1",
      "published": "2025-04-07T11:39:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04954v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges",
      "authors": [
        "José Pombal",
        "Dongkeun Yoon",
        "Patrick Fernandes",
        "Ian Wu",
        "Seungone Kim",
        "Ricardo Rei",
        "Graham Neubig",
        "André F. T. Martins"
      ],
      "abstract": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.",
      "pdf_url": "http://arxiv.org/pdf/2504.04953v1",
      "published": "2025-04-07T11:37:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04953v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "One Quantizer is Enough: Toward a Lightweight Audio Codec",
      "authors": [
        "Linwei Zhai",
        "Han Ding",
        "Cui Zhao",
        "fei wang",
        "Ge Wang",
        "Wang Zhi",
        "Wei Xi"
      ],
      "abstract": "Neural audio codecs have recently gained traction for their ability to\ncompress high-fidelity audio and generate discrete tokens that can be utilized\nin downstream generative modeling tasks. However, leading approaches often rely\non resource-intensive models and multi-quantizer architectures, resulting in\nconsiderable computational overhead and constrained real-world applicability.\nIn this paper, we present SQCodec, a lightweight neural audio codec that\nleverages a single quantizer to address these limitations. SQCodec explores\nstreamlined convolutional networks and local Transformer modules, alongside\nTConv, a novel mechanism designed to capture acoustic variations across\nmultiple temporal scales, thereby enhancing reconstruction fidelity while\nreducing model complexity. Extensive experiments across diverse datasets show\nthat SQCodec achieves audio quality comparable to multi-quantizer baselines,\nwhile its single-quantizer design offers enhanced adaptability and its\nlightweight architecture reduces resource consumption by an order of magnitude.\nThe source code is publicly available at https://github.com/zhai-lw/SQCodec.",
      "pdf_url": "http://arxiv.org/pdf/2504.04949v1",
      "published": "2025-04-07T11:34:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04949v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "68T07",
        "I.2.m"
      ]
    },
    {
      "title": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam",
      "authors": [
        "Rean Fernandes",
        "André Biedenkapp",
        "Frank Hutter",
        "Noor Awad"
      ],
      "abstract": "Legal reasoning tasks present unique challenges for large language models\n(LLMs) due to the complexity of domain-specific knowledge and reasoning\nprocesses. This paper investigates how effectively smaller language models\n(Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514\nMulti-state Bar Examination (MBE) questions to improve legal question answering\naccuracy. We evaluate these models on the 2022 MBE questions licensed from JD\nAdvising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our\nmethodology involves collecting approximately 200 questions per legal domain\nacross 7 domains. We distill the dataset using Llama 3 (70B) to transform\nexplanations into a structured IRAC (Issue, Rule, Application, Conclusion)\nformat as a guided reasoning process to see if it results in better performance\nover the non-distilled dataset. We compare the non-fine-tuned models against\ntheir supervised fine-tuned (SFT) counterparts, trained for different sample\nsizes per domain, to study the effect on accuracy and prompt adherence. We also\nanalyse option selection biases and their mitigation following SFT. In\naddition, we consolidate the performance across multiple variables: prompt type\n(few-shot vs zero-shot), answer ordering (chosen-option first vs\ngenerated-explanation first), response format (Numbered list vs Markdown vs\nJSON), and different decoding temperatures. Our findings show that\ndomain-specific SFT helps some model configurations achieve close to human\nbaseline performance, despite limited computational resources and a relatively\nsmall dataset. We release both the gathered SFT dataset and the family of\nSupervised Fine-tuned (SFT) adapters optimised for MBE performance. This\nestablishes a practical lower bound on resources needed towards achieving\neffective legal question answering in smaller LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2504.04945v1",
      "published": "2025-04-07T11:31:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.04945v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "I.2.7; I.2.1"
      ]
    }
  ]
}