{
  "last_updated": "2025-08-27T00:49:59.587219",
  "papers": [
    {
      "title": "SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation",
      "authors": [
        "Haoyuan Deng",
        "Wenkai Guo",
        "Qianzhun Wang",
        "Zhenyu Wu",
        "Ziwei Wang"
      ],
      "abstract": "Bimanual manipulation has been widely applied in household services and\nmanufacturing, which enables the complex task completion with coordination\nrequirements. Recent diffusion-based policy learning approaches have achieved\npromising performance in modeling action distributions for bimanual\nmanipulation. However, they ignored the physical safety constraints of bimanual\nmanipulation, which leads to the dangerous behaviors with damage to robots and\nobjects. To this end, we propose a test-time trajectory optimization framework\nnamed SafeBimanual for any pre-trained diffusion-based bimanual manipulation\npolicies, which imposes the safety constraints on bimanual actions to avoid\ndangerous robot behaviors with improved success rate. Specifically, we design\ndiverse cost functions for safety constraints in different dual-arm cooperation\npatterns including avoidance of tearing objects and collision between arms and\nobjects, which optimizes the manipulator trajectories with guided sampling of\ndiffusion denoising process. Moreover, we employ a vision-language model (VLM)\nto schedule the cost functions by specifying keypoints and corresponding\npairwise relationship, so that the optimal safety constraint is dynamically\ngenerated in the entire bimanual manipulation process. SafeBimanual\ndemonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase\nin success rate and a 18.8% reduction in unsafe interactions over\nstate-of-the-art diffusion-based methods. Extensive experiments on 4 real-world\ntasks further verify its practical value by improving the success rate by\n32.5%.",
      "pdf_url": "http://arxiv.org/pdf/2508.18268v1",
      "published": "2025-08-25T17:59:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18268v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "ANO : Faster is Better in Noisy Landscape",
      "authors": [
        "Adrien Kegreisz"
      ],
      "abstract": "Stochastic optimizers are central to deep learning, yet widely used methods\nsuch as Adam and Adan can degrade in non-stationary or noisy environments,\npartly due to their reliance on momentum-based magnitude estimates. We\nintroduce Ano, a novel optimizer that decouples direction and magnitude:\nmomentum is used for directional smoothing, while instantaneous gradient\nmagnitudes determine step size. This design improves robustness to gradient\nnoise while retaining the simplicity and efficiency of first-order methods. We\nfurther propose Anolog, which removes sensitivity to the momentum coefficient\nby expanding its window over time via a logarithmic schedule. We establish\nnon-convex convergence guarantees with a convergence rate similar to other\nsign-based methods, and empirically show that Ano provides substantial gains in\nnoisy and non-stationary regimes such as reinforcement learning, while\nremaining competitive on low-noise tasks such as standard computer vision\nbenchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2508.18258v1",
      "published": "2025-08-25T17:51:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18258v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Hermes 4 Technical Report",
      "authors": [
        "Ryan Teknium",
        "Roger Jin",
        "Jai Suphavadeeprasit",
        "Dakota Mahan",
        "Jeffrey Quesnelle",
        "Joe Li",
        "Chen Guang",
        "Shannon Sands",
        "Karan Malhotra"
      ],
      "abstract": "We present Hermes 4, a family of hybrid reasoning models that combine\nstructured, multi-turn reasoning with broad instruction-following ability. We\ndescribe the challenges encountered during data curation, synthesis, training,\nand evaluation, and outline the solutions employed to address these challenges\nat scale. We comprehensively evaluate across mathematical reasoning, coding,\nknowledge, comprehension, and alignment benchmarks, and we report both\nquantitative performance and qualitative behavioral analysis. To support open\nresearch, all model weights are published publicly at\nhttps://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728",
      "pdf_url": "http://arxiv.org/pdf/2508.18255v1",
      "published": "2025-08-25T17:45:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18255v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Efficient Computation of Blackwell Optimal Policies using Rational Functions",
      "authors": [
        "Dibyangshu Mukherjee",
        "Shivaram Kalyanakrishnan"
      ],
      "abstract": "Markov Decision Problems (MDPs) provide a foundational framework for\nmodelling sequential decision-making across diverse domains, guided by\noptimality criteria such as discounted and average rewards. However, these\ncriteria have inherent limitations: discounted optimality may overly prioritise\nshort-term rewards, while average optimality relies on strong structural\nassumptions. Blackwell optimality addresses these challenges, offering a robust\nand comprehensive criterion that ensures optimality under both discounted and\naverage reward frameworks. Despite its theoretical appeal, existing algorithms\nfor computing Blackwell Optimal (BO) policies are computationally expensive or\nhard to implement.\n  In this paper we describe procedures for computing BO policies using an\nordering of rational functions in the vicinity of $1$. We adapt\nstate-of-the-art algorithms for deterministic and general MDPs, replacing\nnumerical evaluations with symbolic operations on rational functions to derive\nbounds independent of bit complexity. For deterministic MDPs, we give the first\nstrongly polynomial-time algorithms for computing BO policies, and for general\nMDPs we obtain the first subexponential-time algorithm. We further generalise\nseveral policy iteration algorithms, extending the best known upper bounds from\nthe discounted to the Blackwell criterion.",
      "pdf_url": "http://arxiv.org/pdf/2508.18252v1",
      "published": "2025-08-25T17:41:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18252v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data",
      "authors": [
        "Chu-Cheng Lin",
        "Daiyi Peng",
        "Yifeng Lu",
        "Ming Zhang",
        "Eugene Ie"
      ],
      "abstract": "Reliably composing Large Language Models (LLMs) for complex, multi-step\nworkflows remains a significant challenge. The dominant paradigm-optimizing\ndiscrete prompts in a pipeline-is notoriously brittle and struggles to enforce\nthe formal compliance required for structured tasks. We introduce\nType-Compliant Adaptation Cascades (TACs), a framework that recasts workflow\nadaptation as learning typed probabilistic programs. TACs treats the entire\nworkflow, which is composed of parameter-efficiently adapted LLMs and\ndeterministic logic, as an unnormalized joint distribution. This enables\nprincipled, gradient-based training even with latent intermediate structures.\nWe provide theoretical justification for our tractable optimization objective,\nproving that the optimization bias vanishes as the model learns type\ncompliance. Empirically, TACs significantly outperforms state-of-the-art\nprompt-optimization baselines. Gains are particularly pronounced on structured\ntasks, improving MGSM-SymPy from $57.1\\%$ to $75.9\\%$ for a 27B model, MGSM\nfrom $1.6\\%$ to $27.3\\%$ for a 7B model. TACs offers a robust and theoretically\ngrounded paradigm for developing reliable, task-compliant LLM systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.18244v1",
      "published": "2025-08-25T17:36:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18244v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Disentangling the Factors of Convergence between Brains and Computer Vision Models",
      "authors": [
        "Joséphine Raugel",
        "Marc Szafraniec",
        "Huy V. Vo",
        "Camille Couprie",
        "Patrick Labatut",
        "Piotr Bojanowski",
        "Valentin Wyart",
        "Jean-Rémi King"
      ],
      "abstract": "Many AI models trained on natural images develop representations that\nresemble those of the human brain. However, the factors that drive this\nbrain-model similarity remain poorly understood. To disentangle how the model,\ntraining and data independently lead a neural network to develop brain-like\nrepresentations, we trained a family of self-supervised vision transformers\n(DINOv3) that systematically varied these different factors. We compare their\nrepresentations of images to those of the human brain recorded with both fMRI\nand MEG, providing high resolution in spatial and temporal analyses. We assess\nthe brain-model similarity with three complementary metrics focusing on overall\nrepresentational similarity, topographical organization, and temporal dynamics.\nWe show that all three factors - model size, training amount, and image type -\nindependently and interactively impact each of these brain similarity metrics.\nIn particular, the largest DINOv3 models trained with the most human-centric\nimages reach the highest brain-similarity. This emergence of brain-like\nrepresentations in AI models follows a specific chronology during training:\nmodels first align with the early representations of the sensory cortices, and\nonly align with the late and prefrontal representations of the brain with\nconsiderably more training. Finally, this developmental trajectory is indexed\nby both structural and functional properties of the human cortex: the\nrepresentations that are acquired last by the models specifically align with\nthe cortical areas with the largest developmental expansion, thickness, least\nmyelination, and slowest timescales. Overall, these findings disentangle the\ninterplay between architecture and experience in shaping how artificial neural\nnetworks come to see the world as humans do, thus offering a promising\nframework to understand how the human brain comes to represent its visual\nworld.",
      "pdf_url": "http://arxiv.org/pdf/2508.18226v1",
      "published": "2025-08-25T17:23:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18226v1",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ]
    },
    {
      "title": "Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center Dialogue Generation",
      "authors": [
        "Rishikesh Devanathan",
        "Varun Nathan",
        "Ayush Kumar"
      ],
      "abstract": "Synthetic transcript generation is critical in contact center domains, where\nprivacy and data scarcity limit model training and evaluation. Unlike prior\nsynthetic dialogue generation work on open-domain or medical dialogues, contact\ncenter conversations are goal-oriented, role-asymmetric, and behaviorally\ncomplex, featuring disfluencies, ASR noise, and compliance-driven agent\nactions. In deployments where transcripts are unavailable, standard pipelines\nstill yield derived call attributes such as Intent Summaries, Topic Flow, and\nQA Evaluation Forms. We leverage these as supervision signals to guide\ngeneration. To assess the quality of such outputs, we introduce a diagnostic\nframework of 18 linguistically and behaviorally grounded metrics for comparing\nreal and synthetic transcripts. We benchmark four language-agnostic generation\nstrategies, from simple prompting to characteristic-aware multi-stage\napproaches, alongside reference-free baselines. Results reveal persistent\nchallenges: no method excels across all traits, with notable deficits in\ndisfluency, sentiment, and behavioral realism. Our diagnostic tool exposes\nthese gaps, enabling fine-grained evaluation and stress testing of synthetic\ndialogue across languages.",
      "pdf_url": "http://arxiv.org/pdf/2508.18210v1",
      "published": "2025-08-25T17:10:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18210v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Unraveling the cognitive patterns of Large Language Models through module communities",
      "authors": [
        "Kushal Raj Bhandari",
        "Pin-Yu Chen",
        "Jianxi Gao"
      ],
      "abstract": "Large Language Models (LLMs) have reshaped our world with significant\nadvancements in science, engineering, and society through applications ranging\nfrom scientific discoveries and medical diagnostics to Chatbots. Despite their\nubiquity and utility, the underlying mechanisms of LLM remain concealed within\nbillions of parameters and complex structures, making their inner architecture\nand cognitive processes challenging to comprehend. We address this gap by\nadopting approaches to understanding emerging cognition in biology and\ndeveloping a network-based framework that links cognitive skills, LLM\narchitectures, and datasets, ushering in a paradigm shift in foundation model\nanalysis. The skill distribution in the module communities demonstrates that\nwhile LLMs do not strictly parallel the focalized specialization observed in\nspecific biological systems, they exhibit unique communities of modules whose\nemergent skill patterns partially mirror the distributed yet interconnected\ncognitive organization seen in avian and small mammalian brains. Our numerical\nresults highlight a key divergence from biological systems to LLMs, where skill\nacquisition benefits substantially from dynamic, cross-regional interactions\nand neural plasticity. By integrating cognitive science principles with machine\nlearning, our framework provides new insights into LLM interpretability and\nsuggests that effective fine-tuning strategies should leverage distributed\nlearning dynamics rather than rigid modular interventions.",
      "pdf_url": "http://arxiv.org/pdf/2508.18192v1",
      "published": "2025-08-25T16:49:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18192v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
      "authors": [
        "Zirui Tang",
        "Boyu Niu",
        "Xuanhe Zhou",
        "Boxiu Li",
        "Wei Zhou",
        "Jiannan Wang",
        "Guoliang Li",
        "Xinyi Zhang",
        "Fan Wu"
      ],
      "abstract": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
      "pdf_url": "http://arxiv.org/pdf/2508.18190v2",
      "published": "2025-08-25T16:48:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18190v2",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ]
    },
    {
      "title": "Explain and Monitor Deep Learning Models for Computer Vision using Obz AI",
      "authors": [
        "Neo Christopher Chung",
        "Jakub Binda"
      ],
      "abstract": "Deep learning has transformed computer vision (CV), achieving outstanding\nperformance in classification, segmentation, and related tasks. Such AI-based\nCV systems are becoming prevalent, with applications spanning from medical\nimaging to surveillance. State of the art models such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs) are often regarded as ``black\nboxes,'' offering limited transparency into their decision-making processes.\nDespite a recent advancement in explainable AI (XAI), explainability remains\nunderutilized in practical CV deployments. A primary obstacle is the absence of\nintegrated software solutions that connect XAI techniques with robust knowledge\nmanagement and monitoring frameworks. To close this gap, we have developed Obz\nAI, a comprehensive software ecosystem designed to facilitate state-of-the-art\nexplainability and observability for vision AI systems. Obz AI provides a\nseamless integration pipeline, from a Python client library to a full-stack\nanalytics dashboard. With Obz AI, a machine learning engineer can easily\nincorporate advanced XAI methodologies, extract and analyze features for\noutlier detection, and continuously monitor AI models in real time. By making\nthe decision-making mechanisms of deep models interpretable, Obz AI promotes\nobservability and responsible deployment of computer vision systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.18188v1",
      "published": "2025-08-25T16:46:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18188v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.SE"
      ]
    },
    {
      "title": "BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding",
      "authors": [
        "Xuan-Bac Nguyen",
        "Thanh-Dat Truong",
        "Pawan Sinha",
        "Khoa Luu"
      ],
      "abstract": "Memory decay makes it harder for the human brain to recognize visual objects\nand retain details. Consequently, recorded brain signals become weaker,\nuncertain, and contain poor visual context over time. This paper presents one\nof the first vision-learning approaches to address this problem. First, we\nstatistically and experimentally demonstrate the existence of inconsistency in\nbrain signals and its impact on the Vision-Brain Understanding (VBU) model. Our\nfindings show that brain signal representations shift over recording sessions,\nleading to compounding bias, which poses challenges for model learning and\ndegrades performance. Then, we propose a new Bias-Mitigation Continual Learning\n(BRAIN) approach to address these limitations. In this approach, the model is\ntrained in a continual learning setup and mitigates the growing bias from each\nlearning step. A new loss function named De-bias Contrastive Learning is also\nintroduced to address the bias problem. In addition, to prevent catastrophic\nforgetting, where the model loses knowledge from previous sessions, the new\nAngular-based Forgetting Mitigation approach is introduced to preserve learned\nknowledge in the model. Finally, the empirical experiments demonstrate that our\napproach achieves State-of-the-Art (SOTA) performance across various\nbenchmarks, surpassing prior and non-continual learning methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.18187v1",
      "published": "2025-08-25T16:44:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18187v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios",
      "authors": [
        "Luana Bulla",
        "Gabriele Tuccio",
        "Misael Mongiovì",
        "Aldo Gangemi"
      ],
      "abstract": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities.",
      "pdf_url": "http://arxiv.org/pdf/2508.18183v1",
      "published": "2025-08-25T16:36:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18183v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "I.2; I.2.7"
      ]
    },
    {
      "title": "AdLoCo: adaptive batching significantly improves communications efficiency and convergence for Large Language Models",
      "authors": [
        "Nikolay Kutuzov",
        "Makar Baderko",
        "Stepan Kulibaba",
        "Artem Dzhalilov",
        "Daniel Bobrov",
        "Maxim Mashtaler",
        "Alexander Gasnikov"
      ],
      "abstract": "Scaling distributed training of Large Language Models (LLMs) requires not\nonly algorithmic advances but also efficient utilization of heterogeneous\nhardware resources. While existing methods such as DiLoCo have demonstrated\npromising results, they often fail to fully exploit computational clusters\nunder dynamic workloads. To address this limitation, we propose a three-stage\nmethod that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo,\nand switch mode mechanism. MIT allows individual nodes to run multiple\nlightweight training streams with different model instances in parallel and\nmerge them to combine knowledge, increasing throughput and reducing idle time.\nAdaptive Batched DiLoCo dynamically adjusts local batch sizes to balance\ncomputation and communication, substantially lowering synchronization delays.\nSwitch mode further stabilizes training by seamlessly introducing gradient\naccumulation once adaptive batch sizes grow beyond hardware-friendly limits.\nTogether, these innovations improve both convergence speed and system\nefficiency. We also provide a theoretical estimate of the number of\ncommunications required for the full convergence of a model trained using our\nmethod.",
      "pdf_url": "http://arxiv.org/pdf/2508.18182v1",
      "published": "2025-08-25T16:35:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18182v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ]
    },
    {
      "title": "SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models",
      "authors": [
        "Zhenwei Tang",
        "Difan Jiao",
        "Blair Yang",
        "Ashton Anderson"
      ],
      "abstract": "Evaluating whether vision-language models (VLMs) reason consistently across\nrepresentations is challenging because modality comparisons are typically\nconfounded by task differences and asymmetric information. We introduce SEAM, a\nbenchmark that pairs semantically equivalent inputs across four domains that\nhave existing standardized textual and visual notations. By employing distinct\nnotation systems across modalities, in contrast to OCR-based image-text\npairing, SEAM provides a rigorous comparative assessment of the\ntextual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21\ncontemporary models, we observe systematic modality imbalance: vision\nfrequently lags language in overall performance, despite the problems\ncontaining semantically equivalent information, and cross-modal agreement is\nrelatively low. Our error analysis reveals two main drivers: textual perception\nfailures from tokenization in domain notation and visual perception failures\nthat induce hallucinations. We also show that our results are largely robust to\nvisual transformations. SEAM establishes a controlled, semantically equivalent\nsetting for measuring and improving modality-agnostic reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2508.18179v1",
      "published": "2025-08-25T16:33:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18179v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Amortized Sampling with Transferable Normalizing Flows",
      "authors": [
        "Charlie B. Tan",
        "Majdi Hassan",
        "Leon Klein",
        "Saifuddin Syed",
        "Dominique Beaini",
        "Michael M. Bronstein",
        "Alexander Tong",
        "Kirill Neklyudov"
      ],
      "abstract": "Efficient equilibrium sampling of molecular conformations remains a core\nchallenge in computational chemistry and statistical inference. Classical\napproaches such as molecular dynamics or Markov chain Monte Carlo inherently\nlack amortization; the computational cost of sampling must be paid in-full for\neach system of interest. The widespread success of generative models has\ninspired interest into overcoming this limitation through learning sampling\nalgorithms. Despite performing on par with conventional methods when trained on\na single system, learned samplers have so far demonstrated limited ability to\ntransfer across systems. We prove that deep learning enables the design of\nscalable and transferable samplers by introducing Prose, a 280 million\nparameter all-atom transferable normalizing flow trained on a corpus of peptide\nmolecular dynamics trajectories up to 8 residues in length. Prose draws\nzero-shot uncorrelated proposal samples for arbitrary peptide systems,\nachieving the previously intractable transferability across sequence length,\nwhilst retaining the efficient likelihood evaluation of normalizing flows.\nThrough extensive empirical evaluation we demonstrate the efficacy of Prose as\na proposal for a variety of sampling algorithms, finding a simple importance\nsampling-based finetuning procedure to achieve superior performance to\nestablished methods such as sequential Monte Carlo on unseen tetrapeptides. We\nopen-source the Prose codebase, model weights, and training dataset, to further\nstimulate research into amortized sampling methods and finetuning objectives.",
      "pdf_url": "http://arxiv.org/pdf/2508.18175v1",
      "published": "2025-08-25T16:28:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18175v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The Computational Complexity of Satisfiability in State Space Models",
      "authors": [
        "Eric Alsmann",
        "Martin Lange"
      ],
      "abstract": "We analyse the complexity of the satisfiability problem ssmSAT for State\nSpace Models (SSM), which asks whether an input sequence can lead the model to\nan accepting configuration. We find that ssmSAT is undecidable in general,\nreflecting the computational power of SSM. Motivated by practical settings, we\nidentify two natural restrictions under which ssmSAT becomes decidable and\nestablish corresponding complexity bounds. First, for SSM with bounded context\nlength, ssmSAT is NP-complete when the input length is given in unary and in\nNEXPTIME (and PSPACE-hard) when the input length is given in binary. Second,\nfor quantised SSM operating over fixed-width arithmetic, ssmSAT is\nPSPACE-complete resp. in EXPSPACE depending on the bit-width encoding. While\nthese results hold for diagonal gated SSM we also establish complexity bounds\nfor time-invariant SSM. Our results establish a first complexity landscape for\nformal reasoning in SSM and highlight fundamental limits and opportunities for\nthe verification of SSM-based language models.",
      "pdf_url": "http://arxiv.org/pdf/2508.18162v1",
      "published": "2025-08-25T16:12:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18162v1",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.CC",
        "cs.LG"
      ]
    },
    {
      "title": "Assessing the Noise Robustness of Class Activation Maps: A Framework for Reliable Model Interpretability",
      "authors": [
        "Syamantak Sarkar",
        "Revoti P. Bora",
        "Bhupender Kaushal",
        "Sudhish N George",
        "Kiran Raja"
      ],
      "abstract": "Class Activation Maps (CAMs) are one of the important methods for visualizing\nregions used by deep learning models. Yet their robustness to different noise\nremains underexplored. In this work, we evaluate and report the resilience of\nvarious CAM methods for different noise perturbations across multiple\narchitectures and datasets. By analyzing the influence of different noise types\non CAM explanations, we assess the susceptibility to noise and the extent to\nwhich dataset characteristics may impact explanation stability. The findings\nhighlight considerable variability in noise sensitivity for various CAMs. We\npropose a robustness metric for CAMs that captures two key properties:\nconsistency and responsiveness. Consistency reflects the ability of CAMs to\nremain stable under input perturbations that do not alter the predicted class,\nwhile responsiveness measures the sensitivity of CAMs to changes in the\nprediction caused by such perturbations. The metric is evaluated empirically\nacross models, different perturbations, and datasets along with complementary\nstatistical tests to exemplify the applicability of our proposed approach.",
      "pdf_url": "http://arxiv.org/pdf/2508.18154v1",
      "published": "2025-08-25T15:59:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18154v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation",
      "authors": [
        "Haijian Ma",
        "Daizong Liu",
        "Xiaowen Cai",
        "Pan Zhou",
        "Yulai Xie"
      ],
      "abstract": "Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats.",
      "pdf_url": "http://arxiv.org/pdf/2508.18148v1",
      "published": "2025-08-25T15:55:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18148v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations",
      "authors": [
        "Hung-Chun Hsu",
        "Yuan-Ching Kuo",
        "Chao-Han Huck Yang",
        "Szu-Wei Fu",
        "Hanrong Ye",
        "Hongxu Yin",
        "Yu-Chiang Frank Wang",
        "Ming-Feng Tsai",
        "Chuan-Ju Wang"
      ],
      "abstract": "The rapid evolution of e-commerce has exposed the limitations of traditional\nproduct retrieval systems in managing complex, multi-turn user interactions.\nRecent advances in multimodal generative retrieval -- particularly those\nleveraging multimodal large language models (MLLMs) as retrievers -- have shown\npromise. However, most existing methods are tailored to single-turn scenarios\nand struggle to model the evolving intent and iterative nature of multi-turn\ndialogues when applied naively. Concurrently, test-time scaling has emerged as\na powerful paradigm for improving large language model (LLM) performance\nthrough iterative inference-time refinement. Yet, its effectiveness typically\nrelies on two conditions: (1) a well-defined problem space (e.g., mathematical\nreasoning), and (2) the model's ability to self-correct -- conditions that are\nrarely met in conversational product search. In this setting, user queries are\noften ambiguous and evolving, and MLLMs alone have difficulty grounding\nresponses in a fixed product corpus. Motivated by these challenges, we propose\na novel framework that introduces test-time scaling into conversational\nmultimodal product retrieval. Our approach builds on a generative retriever,\nfurther augmented with a test-time reranking (TTR) mechanism that improves\nretrieval accuracy and better aligns results with evolving user intent\nthroughout the dialogue. Experiments across multiple benchmarks show consistent\nimprovements, with average gains of 14.5 points in MRR and 10.6 points in\nnDCG@1.",
      "pdf_url": "http://arxiv.org/pdf/2508.18132v1",
      "published": "2025-08-25T15:38:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18132v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics",
      "authors": [
        "Weida Wang",
        "Dongchen Huang",
        "Jiatong Li",
        "Tengchao Yang",
        "Ziyang Zheng",
        "Di Zhang",
        "Dong Han",
        "Benteng Chen",
        "Binzhao Luo",
        "Zhiyu Liu",
        "Kunling Liu",
        "Zhiyuan Gao",
        "Shiqi Geng",
        "Wei Ma",
        "Jiaming Su",
        "Xin Li",
        "Shuchen Pu",
        "Yuhan Shui",
        "Qianjia Cheng",
        "Zhihao Dou",
        "Dongfei Cui",
        "Changyong He",
        "Jin Zeng",
        "Zeke Xie",
        "Mao Su",
        "Dongzhan Zhou",
        "Yuqiang Li",
        "Wanli Ouyang",
        "Yunqi Cai",
        "Xi Dai",
        "Shufei Zhang",
        "Lei Bai",
        "Jinguang Cheng",
        "Zhong Fang",
        "Hongming Weng"
      ],
      "abstract": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench.",
      "pdf_url": "http://arxiv.org/pdf/2508.18124v2",
      "published": "2025-08-25T15:32:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18124v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The AI Data Scientist",
      "authors": [
        "Farkhad Akimov",
        "Munachiso Samuel Nwadike",
        "Zangir Iklassov",
        "Martin Takáč"
      ],
      "abstract": "Imagine decision-makers uploading data and, within minutes, receiving clear,\nactionable insights delivered straight to their fingertips. That is the promise\nof the AI Data Scientist, an autonomous Agent powered by large language models\n(LLMs) that closes the gap between evidence and action. Rather than simply\nwriting code or responding to prompts, it reasons through questions, tests\nideas, and delivers end-to-end insights at a pace far beyond traditional\nworkflows. Guided by the scientific tenet of the hypothesis, this Agent\nuncovers explanatory patterns in data, evaluates their statistical\nsignificance, and uses them to inform predictive modeling. It then translates\nthese results into recommendations that are both rigorous and accessible. At\nthe core of the AI Data Scientist is a team of specialized LLM Subagents, each\nresponsible for a distinct task such as data cleaning, statistical testing,\nvalidation, and plain-language communication. These Subagents write their own\ncode, reason about causality, and identify when additional data is needed to\nsupport sound conclusions. Together, they achieve in minutes what might\notherwise take days or weeks, enabling a new kind of interaction that makes\ndeep data science both accessible and actionable.",
      "pdf_url": "http://arxiv.org/pdf/2508.18113v1",
      "published": "2025-08-25T15:21:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18113v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code",
      "authors": [
        "Keke Lian",
        "Bin Wang",
        "Lei Zhang",
        "Libo Chen",
        "Junjie Wang",
        "Ziming Zhao",
        "Yujiu Yang",
        "Haotong Duan",
        "Haoran Zhao",
        "Shuang Liao",
        "Mingda Guo",
        "Jiazheng Quan",
        "Yilu Zhong",
        "Chenhao He",
        "Zichuan Chen",
        "Jie Wu",
        "Haoling Li",
        "Zhaoxuan Li",
        "Jiongchi Yu",
        "Hui Li",
        "Dong Zhang"
      ],
      "abstract": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.",
      "pdf_url": "http://arxiv.org/pdf/2508.18106v1",
      "published": "2025-08-25T15:11:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18106v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization",
      "authors": [
        "Mohammad J. Abdel-Rahman",
        "Yasmeen Alslman",
        "Dania Refai",
        "Amro Saleh",
        "Malik A. Abu Loha",
        "Mohammad Yahya Hamed"
      ],
      "abstract": "This paper investigates the capabilities of large language models (LLMs) in\nformulating and solving decision-making problems using mathematical\nprogramming. We first conduct a systematic review and meta-analysis of recent\nliterature to assess how well LLMs understand, structure, and solve\noptimization problems across domains. The analysis is guided by critical review\nquestions focusing on learning approaches, dataset designs, evaluation metrics,\nand prompting strategies. Our systematic evidence is complemented by targeted\nexperiments designed to evaluate the performance of state-of-the-art LLMs in\nautomatically generating optimization models for problems in computer networks.\nUsing a newly constructed dataset, we apply three prompting strategies:\nAct-as-expert, chain-of-thought, and self-consistency, and evaluate the\nobtained outputs based on optimality gap, token-level F1 score, and compilation\naccuracy. Results show promising progress in LLMs' ability to parse natural\nlanguage and represent symbolic formulations, but also reveal key limitations\nin accuracy, scalability, and interpretability. These empirical gaps motivate\nseveral future research directions, including structured datasets,\ndomain-specific fine-tuning, hybrid neuro-symbolic approaches, modular\nmulti-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper\ncontributes a structured roadmap for advancing LLM capabilities in mathematical\nprogramming.",
      "pdf_url": "http://arxiv.org/pdf/2508.18091v1",
      "published": "2025-08-25T14:52:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18091v1",
      "categories": [
        "cs.AI",
        "math.OC"
      ]
    },
    {
      "title": "Named Entity Recognition of Historical Text via Large Language Model",
      "authors": [
        "Shibingfeng Zhang",
        "Giovanni Colavizza"
      ],
      "abstract": "Large language models have demonstrated remarkable versatility across a wide\nrange of natural language processing tasks and domains. One such task is Named\nEntity Recognition (NER), which involves identifying and classifying proper\nnames in text, such as people, organizations, locations, dates, and other\nspecific entities. NER plays a crucial role in extracting information from\nunstructured textual data, enabling downstream applications such as information\nretrieval from unstructured text.\n  Traditionally, NER is addressed using supervised machine learning approaches,\nwhich require large amounts of annotated training data. However, historical\ntexts present a unique challenge, as the annotated datasets are often scarce or\nnonexistent, due to the high cost and expertise required for manual labeling.\nIn addition, the variability and noise inherent in historical language, such as\ninconsistent spelling and archaic vocabulary, further complicate the\ndevelopment of reliable NER systems for these sources.\n  In this study, we explore the feasibility of applying LLMs to NER in\nhistorical documents using zero-shot and few-shot prompting strategies, which\nrequire little to no task-specific training data. Our experiments, conducted on\nthe HIPE-2022 (Identifying Historical People, Places and other Entities)\ndataset, show that LLMs can achieve reasonably strong performance on NER tasks\nin this setting. While their performance falls short of fully supervised models\ntrained on domain-specific annotations, the results are nevertheless promising.\nThese findings suggest that LLMs offer a viable and efficient alternative for\ninformation extraction in low-resource or historically significant corpora,\nwhere traditional supervised methods are infeasible.",
      "pdf_url": "http://arxiv.org/pdf/2508.18090v1",
      "published": "2025-08-25T14:52:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18090v1",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Arnold: a generalist muscle transformer policy",
      "authors": [
        "Alberto Silvio Chiappa",
        "Boshi An",
        "Merkourios Simos",
        "Chengkun Li",
        "Alexander Mathis"
      ],
      "abstract": "Controlling high-dimensional and nonlinear musculoskeletal models of the\nhuman body is a foundational scientific challenge. Recent machine learning\nbreakthroughs have heralded policies that master individual skills like\nreaching, object manipulation and locomotion in musculoskeletal systems with\nmany degrees of freedom. However, these agents are merely \"specialists\",\nachieving high performance for a single skill. In this work, we develop Arnold,\na generalist policy that masters multiple tasks and embodiments. Arnold\ncombines behavior cloning and fine-tuning with PPO to achieve expert or\nsuper-expert performance in 14 challenging control tasks from dexterous object\nmanipulation to locomotion. A key innovation is Arnold's sensorimotor\nvocabulary, a compositional representation of the semantics of heterogeneous\nsensory modalities, objectives, and actuators. Arnold leverages this vocabulary\nvia a transformer architecture to deal with the variable observation and action\nspaces of each task. This framework supports efficient multi-task,\nmulti-embodiment learning and facilitates rapid adaptation to novel tasks.\nFinally, we analyze Arnold to provide insights into biological motor control,\ncorroborating recent findings on the limited transferability of muscle\nsynergies across tasks.",
      "pdf_url": "http://arxiv.org/pdf/2508.18066v1",
      "published": "2025-08-25T14:22:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18066v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ]
    },
    {
      "title": "Dynamic Fusion Multimodal Network for SpeechWellness Detection",
      "authors": [
        "Wenqiang Sun",
        "Han Yin",
        "Jisheng Bai",
        "Jianfeng Chen"
      ],
      "abstract": "Suicide is one of the leading causes of death among adolescents. Previous\nsuicide risk prediction studies have primarily focused on either textual or\nacoustic information in isolation, the integration of multimodal signals, such\nas speech and text, offers a more comprehensive understanding of an\nindividual's mental state. Motivated by this, and in the context of the 1st\nSpeechWellness detection challenge, we explore a lightweight multi-branch\nmultimodal system based on a dynamic fusion mechanism for speechwellness\ndetection. To address the limitation of prior approaches that rely on\ntime-domain waveforms for acoustic analysis, our system incorporates both\ntime-domain and time-frequency (TF) domain acoustic features, as well as\nsemantic representations. In addition, we introduce a dynamic fusion block to\nadaptively integrate information from different modalities. Specifically, it\napplies learnable weights to each modality during the fusion process, enabling\nthe model to adjust the contribution of each modality. To enhance computational\nefficiency, we design a lightweight structure by simplifying the original\nbaseline model. Experimental results demonstrate that the proposed system\nexhibits superior performance compared to the challenge baseline, achieving a\n78% reduction in model parameters and a 5% improvement in accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.18057v1",
      "published": "2025-08-25T14:18:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18057v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data",
      "authors": [
        "Jiyoon Myung",
        "Jihyeon Park",
        "Joohyung Han"
      ],
      "abstract": "User queries in real-world recommendation systems often combine structured\nconstraints (e.g., category, attributes) with unstructured preferences (e.g.,\nproduct descriptions or reviews). We introduce HyST (Hybrid retrieval over\nSemi-structured Tabular data), a hybrid retrieval framework that combines\nLLM-powered structured filtering with semantic embedding search to support\ncomplex information needs over semi-structured tabular data. HyST extracts\nattribute-level constraints from natural language using large language models\n(LLMs) and applies them as metadata filters, while processing the remaining\nunstructured query components via embedding-based retrieval. Experiments on a\nsemi-structured benchmark show that HyST consistently outperforms tradtional\nbaselines, highlighting the importance of structured filtering in improving\nretrieval precision, offering a scalable and accurate solution for real-world\nuser queries.",
      "pdf_url": "http://arxiv.org/pdf/2508.18048v1",
      "published": "2025-08-25T14:06:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18048v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration",
      "authors": [
        "Xin Wang",
        "Zhiyao Cui",
        "Hao Li",
        "Ya Zeng",
        "Chenxu Wang",
        "Ruiqi Song",
        "Yihang Chen",
        "Kun Shao",
        "Qiaosheng Zhang",
        "Jinzhuo Liu",
        "Siyue Ren",
        "Shuyue Hu",
        "Zhen Wang"
      ],
      "abstract": "Vision language model (VLM)-based mobile agents show great potential for\nassisting users in performing instruction-driven tasks. However, these agents\ntypically struggle with personalized instructions -- those containing\nambiguous, user-specific context -- a challenge that has been largely\noverlooked in previous research. In this paper, we define personalized\ninstructions and introduce PerInstruct, a novel human-annotated dataset\ncovering diverse personalized instructions across various mobile scenarios.\nFurthermore, given the limited personalization capabilities of existing mobile\nagents, we propose PerPilot, a plug-and-play framework powered by large\nlanguage models (LLMs) that enables mobile agents to autonomously perceive,\nunderstand, and execute personalized user instructions. PerPilot identifies\npersonalized elements and autonomously completes instructions via two\ncomplementary approaches: memory-based retrieval and reasoning-based\nexploration. Experimental results demonstrate that PerPilot effectively handles\npersonalized tasks with minimal user intervention and progressively improves\nits performance with continued use, underscoring the importance of\npersonalization-aware reasoning for next-generation mobile agents. The dataset\nand code are available at: https://github.com/xinwang-nwpu/PerPilot",
      "pdf_url": "http://arxiv.org/pdf/2508.18040v1",
      "published": "2025-08-25T13:57:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18040v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration",
      "authors": [
        "Aditri Paul",
        "Archan Paul"
      ],
      "abstract": "Autonomous planetary exploration missions are critically dependent on\nreal-time, accurate environmental perception for navigation and hazard\navoidance. However, deploying deep learning models on the resource-constrained\ncomputational hardware of planetary exploration platforms remains a significant\nchallenge. This paper introduces the Adaptive Quantized Planetary Crater\nDetection System (AQ-PCDSys), a novel framework specifically engineered for\nreal-time, onboard deployment in the computationally constrained environments\nof space exploration missions. AQ-PCDSys synergistically integrates a Quantized\nNeural Network (QNN) architecture, trained using Quantization-Aware Training\n(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture\nsignificantly optimizes model size and inference latency suitable for real-time\nonboard deployment in space exploration missions, while preserving high\naccuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and\nDigital Elevation Models (DEMs) at the feature level, utilizing an Adaptive\nWeighting Mechanism (AWM) to dynamically prioritize the most relevant and\nreliable sensor modality based on planetary ambient conditions. This approach\nenhances detection robustness across diverse planetary landscapes. Paired with\nMulti-Scale Detection Heads specifically designed for robust and efficient\ndetection of craters across a wide range of sizes, AQ-PCDSys provides a\ncomputationally efficient, reliable and accurate solution for planetary crater\ndetection, a critical capability for enabling the next generation of autonomous\nplanetary landing, navigation, and scientific exploration.",
      "pdf_url": "http://arxiv.org/pdf/2508.18025v1",
      "published": "2025-08-25T13:44:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18025v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "cs.SY",
        "eess.SY",
        "68T07(2020), 68T45(2020), 68T10(2020), 90C90(2020)",
        "I.2.10; I.2.6; I.2.9; J.2"
      ]
    },
    {
      "title": "Towards Continual Visual Anomaly Detection in the Medical Domain",
      "authors": [
        "Manuel Barusco",
        "Francesco Borsatti",
        "Nicola Beda",
        "Davide Dalle Pezze",
        "Gian Antonio Susto"
      ],
      "abstract": "Visual Anomaly Detection (VAD) seeks to identify abnormal images and\nprecisely localize the corresponding anomalous regions, relying solely on\nnormal data during training. This approach has proven essential in domains such\nas manufacturing and, more recently, in the medical field, where accurate and\nexplainable detection is critical. Despite its importance, the impact of\nevolving input data distributions over time has received limited attention,\neven though such changes can significantly degrade model performance. In\nparticular, given the dynamic and evolving nature of medical imaging data,\nContinual Learning (CL) provides a natural and effective framework to\nincrementally adapt models while preserving previously acquired knowledge. This\nstudy explores for the first time the application of VAD models in a CL\nscenario for the medical field. In this work, we utilize a CL version of the\nwell-established PatchCore model, called PatchCoreCL, and evaluate its\nperformance using BMAD, a real-world medical imaging dataset with both\nimage-level and pixel-level annotations. Our results demonstrate that\nPatchCoreCL is an effective solution, achieving performance comparable to the\ntask-specific models, with a forgetting value less than a 1%, highlighting the\nfeasibility and potential of CL for adaptive VAD in medical imaging.",
      "pdf_url": "http://arxiv.org/pdf/2508.18013v1",
      "published": "2025-08-25T13:28:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18013v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Previously on... Automating Code Review",
      "authors": [
        "Robert Heumüller",
        "Frank Ortmeier"
      ],
      "abstract": "Modern Code Review (MCR) is a standard practice in software engineering, yet\nit demands substantial time and resource investments. Recent research has\nincreasingly explored automating core review tasks using machine learning (ML)\nand deep learning (DL). As a result, there is substantial variability in task\ndefinitions, datasets, and evaluation procedures. This study provides the first\ncomprehensive analysis of MCR automation research, aiming to characterize the\nfield's evolution, formalize learning tasks, highlight methodological\nchallenges, and offer actionable recommendations to guide future research.\nFocusing on the primary code review tasks, we systematically surveyed 691\npublications and identified 24 relevant studies published between May 2015 and\nApril 2024. Each study was analyzed in terms of tasks, models, metrics,\nbaselines, results, validity concerns, and artifact availability. In\nparticular, our analysis reveals significant potential for standardization,\nincluding 48 task metric combinations, 22 of which were unique to their\noriginal paper, and limited dataset reuse. We highlight challenges and derive\nconcrete recommendations for examples such as the temporal bias threat, which\nare rarely addressed so far. Our work contributes to a clearer overview of the\nfield, supports the framing of new research, helps to avoid pitfalls, and\npromotes greater standardization in evaluation practices.",
      "pdf_url": "http://arxiv.org/pdf/2508.18003v1",
      "published": "2025-08-25T13:12:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.18003v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Automating Conflict-Aware ACL Configurations with Natural Language Intents",
      "authors": [
        "Wenlong Ding",
        "Jianqiang Li",
        "Zhixiong Niu",
        "Huangxun Chen",
        "Yongqiang Xiong",
        "Hong Xu"
      ],
      "abstract": "ACL configuration is essential for managing network flow reachability, yet\nits complexity grows significantly with topologies and pre-existing rules. To\ncarry out ACL configuration, the operator needs to (1) understand the new\nconfiguration policies or intents and translate them into concrete ACL rules,\n(2) check and resolve any conflicts between the new and existing rules, and (3)\ndeploy them across the network. Existing systems rely heavily on manual efforts\nfor these tasks, especially for the first two, which are tedious, error-prone,\nand impractical to scale.\n  We propose Xumi to tackle this problem. Leveraging LLMs with domain knowledge\nof the target network, Xumi automatically and accurately translates the natural\nlanguage intents into complete ACL rules to reduce operators' manual efforts.\nXumi then detects all potential conflicts between new and existing rules and\ngenerates resolved intents for deployment with operators' guidance, and finally\nidentifies the best deployment plan that minimizes the rule additions while\nsatisfying all intents. Evaluation shows that Xumi accelerates the entire\nconfiguration pipeline by over 10x compared to current practices, addresses\nO(100) conflicting ACLs and reduces rule additions by ~40% in modern cloud\nnetwork.",
      "pdf_url": "http://arxiv.org/pdf/2508.17990v1",
      "published": "2025-08-25T13:00:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17990v1",
      "categories": [
        "cs.NI",
        "cs.AI"
      ]
    },
    {
      "title": "Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding",
      "authors": [
        "Pu Feng",
        "Size Wang",
        "Yuhong Cao",
        "Junkang Liang",
        "Rongye Shi",
        "Wenjun Wu"
      ],
      "abstract": "The development and application of large language models (LLM) have\ndemonstrated that foundational models can be utilized to solve a wide array of\ntasks. However, their performance in multi-agent path finding (MAPF) tasks has\nbeen less than satisfactory, with only a few studies exploring this area. MAPF\nis a complex problem requiring both planning and multi-agent coordination. To\nimprove the performance of LLM in MAPF tasks, we propose a novel framework,\nLLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for\nMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained\ngraph neural network-based NAR, and a cross-attention mechanism. This is the\nfirst work to propose using a neural algorithmic reasoner to integrate GNNs\nwith the map information for MAPF, thereby guiding LLM to achieve superior\nperformance. LLM-NAR can be easily adapted to various LLM models. Both\nsimulation and real-world experiments demonstrate that our method significantly\noutperforms existing LLM-based approaches in solving MAPF problems.",
      "pdf_url": "http://arxiv.org/pdf/2508.17971v1",
      "published": "2025-08-25T12:38:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17971v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Language Models Coupled with Metacognition Can Outperform Reasoning Models",
      "authors": [
        "Vedant Khandelwal",
        "Francesca Rossi",
        "Keerthiram Murugesan",
        "Erik Miehling",
        "Murray Campbell",
        "Karthikeyan Natesan Ramamurthy",
        "Lior Horesh"
      ],
      "abstract": "Large language models (LLMs) excel in speed and adaptability across various\nreasoning tasks, but they often struggle when strict logic or constraint\nenforcement is required. In contrast, Large Reasoning Models (LRMs) are\nspecifically designed for complex, step-by-step reasoning, although they come\nwith significant computational costs and slower inference times. To address\nthese trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)\ncognitive architecture into SOFAI-LM, which coordinates a fast LLM with a\nslower but more powerful LRM through metacognition. The metacognitive module\nactively monitors the LLM's performance and provides targeted, iterative\nfeedback with relevant examples. This enables the LLM to progressively refine\nits solutions without requiring the need for additional model fine-tuning.\nExtensive experiments on graph coloring and code debugging problems demonstrate\nthat our feedback-driven approach significantly enhances the problem-solving\ncapabilities of the LLM. In many instances, it achieves performance levels that\nmatch or even exceed those of standalone LRMs while requiring considerably less\ntime. Additionally, when the LLM and feedback mechanism alone are insufficient,\nwe engage the LRM by providing appropriate information collected during the\nLLM's feedback loop, tailored to the specific characteristics of the problem\ndomain and leads to improved overall performance. Evaluations on two\ncontrasting domains: graph coloring, requiring globally consistent solutions,\nand code debugging, demanding localized fixes, demonstrate that SOFAI-LM\nenables LLMs to match or outperform standalone LRMs in accuracy while\nmaintaining significantly lower inference time.",
      "pdf_url": "http://arxiv.org/pdf/2508.17959v1",
      "published": "2025-08-25T12:19:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17959v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Understanding Subword Compositionality of Large Language Models",
      "authors": [
        "Qiwei Peng",
        "Yekun Chai",
        "Anders Søgaard"
      ],
      "abstract": "Large language models (LLMs) take sequences of subwords as input, requiring\nthem to effective compose subword representations into meaningful word-level\nrepresentations. In this paper, we present a comprehensive set of experiments\nto probe how LLMs compose subword information, focusing on three key aspects:\nstructural similarity, semantic decomposability, and form retention. Our\nanalysis of the experiments suggests that these five LLM families can be\nclassified into three distinct groups, likely reflecting difference in their\nunderlying composition strategies. Specifically, we observe (i) three distinct\npatterns in the evolution of structural similarity between subword compositions\nand whole-word representations across layers; (ii) great performance when\nprobing layer by layer their sensitivity to semantic decompositionality; and\n(iii) three distinct patterns when probing sensitivity to formal features,\ne.g., character sequence length. These findings provide valuable insights into\nthe compositional dynamics of LLMs and highlight different compositional\npattens in how LLMs encode and integrate subword information.",
      "pdf_url": "http://arxiv.org/pdf/2508.17953v1",
      "published": "2025-08-25T12:16:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17953v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Debiasing Multilingual LLMs in Cross-lingual Latent Space",
      "authors": [
        "Qiwei Peng",
        "Guimin Hu",
        "Yekun Chai",
        "Anders Søgaard"
      ],
      "abstract": "Debiasing techniques such as SentDebias aim to reduce bias in large language\nmodels (LLMs). Previous studies have evaluated their cross-lingual\ntransferability by directly applying these methods to LLM representations,\nrevealing their limited effectiveness across languages. In this work, we\ntherefore propose to perform debiasing in a joint latent space rather than\ndirectly on LLM representations. We construct a well-aligned cross-lingual\nlatent space using an autoencoder trained on parallel TED talk scripts. Our\nexperiments with Aya-expanse and two debiasing techniques across four languages\n(English, French, German, Dutch) demonstrate that a) autoencoders effectively\nconstruct a well-aligned cross-lingual latent space, and b) applying debiasing\ntechniques in the learned cross-lingual latent space significantly improves\nboth the overall debiasing performance and cross-lingual transferability.",
      "pdf_url": "http://arxiv.org/pdf/2508.17948v1",
      "published": "2025-08-25T12:13:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17948v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A Feminist Account of Intersectional Algorithmic Fairness",
      "authors": [
        "Marie Mirsch",
        "Laila Wegner",
        "Jonas Strube",
        "Carmen Leicht-Scholten"
      ],
      "abstract": "Intersectionality has profoundly influenced research and political action by\nrevealing how interconnected systems of privilege and oppression influence\nlived experiences, yet its integration into algorithmic fairness research\nremains limited. Existing approaches often rely on single-axis or formal\nsubgroup frameworks that risk oversimplifying social realities and neglecting\nstructural inequalities. We propose Substantive Intersectional Algorithmic\nFairness, extending Green's (2022) notion of substantive algorithmic fairness\nwith insights from intersectional feminist theory. Building on this foundation,\nwe introduce ten desiderata within the ROOF methodology to guide the design,\nassessment, and deployment of algorithmic systems in ways that address systemic\ninequities while mitigating harms to intersectionally marginalized communities.\nRather than prescribing fixed operationalizations, these desiderata encourage\nreflection on assumptions of neutrality, the use of protected attributes, the\ninclusion of multiply marginalized groups, and enhancing algorithmic systems'\npotential. Our approach emphasizes that fairness cannot be separated from\nsocial context, and that in some cases, principled non-deployment may be\nnecessary. By bridging computational and social science perspectives, we\nprovide actionable guidance for more equitable, inclusive, and\ncontext-sensitive intersectional algorithmic practices.",
      "pdf_url": "http://arxiv.org/pdf/2508.17944v1",
      "published": "2025-08-25T12:09:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17944v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops",
      "authors": [
        "Zixuan Dong",
        "Baoyun Peng",
        "Yufei Wang",
        "Lin Liu",
        "Xinxin Dong",
        "Yunlong Cao",
        "Xiaodong Wang"
      ],
      "abstract": "Human video comprehension demonstrates dynamic coordination between reasoning\nand visual attention, adaptively focusing on query-relevant details. However,\ncurrent long-form video question answering systems employ rigid pipelines that\ndecouple reasoning from perception, leading to either information loss through\npremature visual abstraction or computational inefficiency through exhaustive\nprocessing. The core limitation lies in the inability to adapt visual\nextraction to specific reasoning requirements, different queries demand\nfundamentally different visual evidence from the same video content. In this\nwork, we present CAVIA, a training-free framework that revolutionizes video\nunderstanding through reasoning, perception coordination. Unlike conventional\napproaches where visual processing operates independently of reasoning, CAVIA\ncreates a closed-loop system where reasoning continuously guides visual\nextraction based on identified information gaps. CAVIA introduces three\ninnovations: (1) hierarchical reasoning, guided localization to precise frames;\n(2) cross-modal semantic bridging for targeted extraction; (3)\nconfidence-driven iterative synthesis. CAVIA achieves state-of-the-art\nperformance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA\n(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamic\nreasoning-perception coordination provides a scalable paradigm for video\nunderstanding.",
      "pdf_url": "http://arxiv.org/pdf/2508.17932v1",
      "published": "2025-08-25T12:00:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17932v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T45, 68T05",
        "H.5.1; I.2.10; I.4.8; I.5.4"
      ]
    },
    {
      "title": "AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation",
      "authors": [
        "Henri Savigny",
        "Bruno Yun"
      ],
      "abstract": "Argument mining is a subfield of argumentation that aims to automatically\nextract argumentative structures and their relations from natural language\ntexts. This paper investigates how a single large language model can be\nleveraged to perform one or several argument mining tasks. Our contributions\nare two-fold. First, we construct a multi-task dataset by surveying and\nconverting 19 well-known argument mining datasets from the literature into a\nunified format. Second, we explore various training strategies using Meta AI's\nLlama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2)\nfine-tuning jointly on multiple tasks, and (3) merging models fine-tuned\nseparately on individual tasks. Our experiments show that task-specific\nfine-tuning significantly improves individual performance across all tasks.\nMoreover, multi-task fine-tuning maintains strong performance without\ndegradation, suggesting effective transfer learning across related tasks.\nFinally, we demonstrate that model merging offers a viable compromise: it\nyields competitive performance while mitigating the computational costs\nassociated with full multi-task fine-tuning.",
      "pdf_url": "http://arxiv.org/pdf/2508.17926v1",
      "published": "2025-08-25T11:51:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17926v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Riemannian Optimization for LoRA on the Stiefel Manifold",
      "authors": [
        "Juneyoung Park",
        "Minjae Kang",
        "Seongbae Lee",
        "Haegang Lee",
        "Seongwan Kim",
        "Jaeho Lee"
      ],
      "abstract": "While powerful, large language models (LLMs) present significant fine-tuning\nchallenges due to their size. Parameter-efficient fine-tuning (PEFT) methods\nlike LoRA provide solutions, yet suffer from critical optimizer inefficiencies;\nnotably basis redundancy in LoRA's $B$ matrix when using AdamW, which\nfundamentally limits performance. We address this by optimizing the $B$ matrix\non the Stiefel manifold, imposing explicit orthogonality constraints that\nachieve near-perfect orthogonality and full effective rank. This geometric\napproach dramatically enhances parameter efficiency and representational\ncapacity. Our Stiefel optimizer consistently outperforms AdamW across\nbenchmarks with both LoRA and DoRA, demonstrating that geometric constraints\nare the key to unlocking LoRA's full potential for effective LLM fine-tuning.",
      "pdf_url": "http://arxiv.org/pdf/2508.17901v1",
      "published": "2025-08-25T11:15:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17901v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Defect Classification Framework for AI-Based Software Systems (AI-ODC)",
      "authors": [
        "Mohammed O. Alannsary"
      ],
      "abstract": "Artificial Intelligence has gained a lot of attention recently, it has been\nutilized in several fields ranging from daily life activities, such as\nresponding to emails and scheduling appointments, to manufacturing and\nautomating work activities. Artificial Intelligence systems are mainly\nimplemented as software solutions, and it is essential to discover and remove\nsoftware defects to assure its quality using defect analysis which is one of\nthe major activities that contribute to software quality. Despite the\nproliferation of AI-based systems, current defect analysis models fail to\ncapture their unique attributes. This paper proposes a framework inspired by\nthe Orthogonal Defect Classification (ODC) paradigm and enables defect analysis\nof Artificial Intelligence systems while recognizing its special attributes and\ncharacteristics. This study demonstrated the feasibility of modifying ODC for\nAI systems to classify its defects. The ODC was adjusted to accommodate the\nData, Learning, and Thinking aspects of AI systems which are newly introduced\nclassification dimensions. This adjustment involved the introduction of an\nadditional attribute to the ODC attributes, the incorporation of a new severity\nlevel, and the substitution of impact areas with characteristics pertinent to\nAI systems. The framework was showcased by applying it to a publicly available\nMachine Learning bug dataset, with results analyzed through one-way and two-way\nanalysis. The case study indicated that defects occurring during the Learning\nphase were the most prevalent and were significantly linked to high-severity\nclassifications. In contrast, defects identified in the Thinking phase had a\ndisproportionate effect on trustworthiness and accuracy. These findings\nillustrate AIODC's capability to identify high-risk defect categories and\ninform focused quality assurance measures.",
      "pdf_url": "http://arxiv.org/pdf/2508.17900v1",
      "published": "2025-08-25T11:15:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17900v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "Designing Practical Models for Isolated Word Visual Speech Recognition",
      "authors": [
        "Iason Ioannis Panagos",
        "Giorgos Sfikas",
        "Christophoros Nikou"
      ],
      "abstract": "Visual speech recognition (VSR) systems decode spoken words from an input\nsequence using only the video data. Practical applications of such systems\ninclude medical assistance as well as human-machine interactions. A VSR system\nis typically employed in a complementary role in cases where the audio is\ncorrupt or not available. In order to accurately predict the spoken words,\nthese architectures often rely on deep neural networks in order to extract\nmeaningful representations from the input sequence. While deep architectures\nachieve impressive recognition performance, relying on such models incurs\nsignificant computation costs which translates into increased resource demands\nin terms of hardware requirements and results in limited applicability in\nreal-world scenarios where resources might be constrained. This factor prevents\nwider adoption and deployment of speech recognition systems in more practical\napplications. In this work, we aim to alleviate this issue by developing\narchitectures for VSR that have low hardware costs. Following the standard\ntwo-network design paradigm, where one network handles visual feature\nextraction and another one utilizes the extracted features to classify the\nentire sequence, we develop lightweight end-to-end architectures by first\nbenchmarking efficient models from the image classification literature, and\nthen adopting lightweight block designs in a temporal convolution network\nbackbone. We create several unified models with low resource requirements but\nstrong recognition performance. Experiments on the largest public database for\nEnglish words demonstrate the effectiveness and practicality of our developed\nmodels. Code and trained models will be made publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2508.17894v1",
      "published": "2025-08-25T11:04:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17894v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection",
      "authors": [
        "Dabbrata Das",
        "Mahshar Yahan",
        "Md Tareq Zaman",
        "Md Rishadul Bayesh"
      ],
      "abstract": "The rapid advancement of generative models has led to a growing prevalence of\nhighly realistic AI-generated images, posing significant challenges for digital\nforensics and content authentication. Conventional detection methods mainly\nrely on deep learning models that extract global features, which often overlook\nsubtle structural inconsistencies and demand substantial computational\nresources. To address these limitations, we propose a hybrid detection\nframework that combines a fine-tuned Vision Transformer (ViT) with a novel\nedge-based image processing module. The edge-based module computes variance\nfrom edge-difference maps generated before and after smoothing, exploiting the\nobservation that AI-generated images typically exhibit smoother textures,\nweaker edges, and reduced noise compared to real images. When applied as a\npost-processing step on ViT predictions, this module enhances sensitivity to\nfine-grained structural cues while maintaining computational efficiency.\nExtensive experiments on the CIFAKE, Artistic, and Custom Curated datasets\ndemonstrate that the proposed framework achieves superior detection performance\nacross all benchmarks, attaining 97.75% accuracy and a 97.77% F1-score on\nCIFAKE, surpassing widely adopted state-of-the-art models. These results\nestablish the proposed method as a lightweight, interpretable, and effective\nsolution for both still images and video frames, making it highly suitable for\nreal-world applications in automated content verification and digital\nforensics.",
      "pdf_url": "http://arxiv.org/pdf/2508.17877v1",
      "published": "2025-08-25T10:30:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17877v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Vocoder-Projected Feature Discriminator",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Yuto Kondo"
      ],
      "abstract": "In text-to-speech (TTS) and voice conversion (VC), acoustic features, such as\nmel spectrograms, are typically used as synthesis or conversion targets owing\nto their compactness and ease of learning. However, because the ultimate goal\nis to generate high-quality waveforms, employing a vocoder to convert these\nfeatures into waveforms and applying adversarial training in the time domain is\nreasonable. Nevertheless, upsampling the waveform introduces significant time\nand memory overheads. To address this issue, we propose a vocoder-projected\nfeature discriminator (VPFD), which uses vocoder features for adversarial\ntraining. Experiments on diffusion-based VC distillation demonstrated that a\npretrained and frozen vocoder feature extractor with a single upsampling step\nis necessary and sufficient to achieve a VC performance comparable to that of\nwaveform discriminators while reducing the training time and memory consumption\nby 9.6 and 11.4 times, respectively.",
      "pdf_url": "http://arxiv.org/pdf/2508.17874v1",
      "published": "2025-08-25T10:29:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17874v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "stat.ML"
      ]
    },
    {
      "title": "FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Yuto Kondo"
      ],
      "abstract": "A diffusion-based voice conversion (VC) model (e.g., VoiceGrad) can achieve\nhigh speech quality and speaker similarity; however, its conversion process is\nslow owing to iterative sampling. FastVoiceGrad overcomes this limitation by\ndistilling VoiceGrad into a one-step diffusion model. However, it still\nrequires a computationally intensive content encoder to disentangle the\nspeaker's identity and content, which slows conversion. Therefore, we propose\nFasterVoiceGrad, a novel one-step diffusion-based VC model obtained by\nsimultaneously distilling a diffusion model and content encoder using\nadversarial diffusion conversion distillation (ADCD), where distillation is\nperformed in the conversion process while leveraging adversarial and score\ndistillation training. Experimental evaluations of one-shot VC demonstrated\nthat FasterVoiceGrad achieves competitive VC performance compared to\nFastVoiceGrad, with 6.6-6.9 and 1.8 times faster speed on a GPU and CPU,\nrespectively.",
      "pdf_url": "http://arxiv.org/pdf/2508.17868v1",
      "published": "2025-08-25T10:23:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17868v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "stat.ML"
      ]
    },
    {
      "title": "Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networks",
      "authors": [
        "Dan Wang",
        "Feng Jiang",
        "Zhanquan Wang"
      ],
      "abstract": "Accurate air quality prediction is becoming increasingly important in the\nenvironmental field. To address issues such as low prediction accuracy and slow\nreal-time updates in existing models, which lead to lagging prediction results,\nwe propose a Transformer-based spatiotemporal data prediction method\n(Ada-TransGNN) that integrates global spatial semantics and temporal behavior.\nThe model constructs an efficient and collaborative spatiotemporal block set\ncomprising a multi-head attention mechanism and a graph convolutional network\nto extract dynamically changing spatiotemporal dependency features from complex\nair quality monitoring data. Considering the interaction relationships between\ndifferent monitoring points, we propose an adaptive graph structure learning\nmodule, which combines spatiotemporal dependency features in a data-driven\nmanner to learn the optimal graph structure, thereby more accurately capturing\nthe spatial relationships between monitoring points. Additionally, we design an\nauxiliary task learning module that enhances the decoding capability of\ntemporal relationships by integrating spatial context information into the\noptimal graph structure representation, effectively improving the accuracy of\nprediction results. We conducted comprehensive evaluations on a benchmark\ndataset and a novel dataset (Mete-air). The results demonstrate that our model\noutperforms existing state-of-the-art prediction models in short-term and\nlong-term predictions.",
      "pdf_url": "http://arxiv.org/pdf/2508.17867v2",
      "published": "2025-08-25T10:22:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17867v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering",
      "authors": [
        "Kang Zeng",
        "Guojin Zhong",
        "Jintao Cheng",
        "Jin Yuan",
        "Zhiyong Li"
      ],
      "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has driven\nsignificant progress in Visual Question Answering (VQA), evolving from Single\nto Multi Image VQA (MVQA). However, the increased number of images in MVQA\ninevitably introduces substantial visual redundancy that is irrelevant to\nquestion answering, negatively impacting both accuracy and efficiency. To\naddress this issue, existing methods lack flexibility in controlling the number\nof compressed visual tokens and tend to produce discrete visual fragments,\nwhich hinder MLLMs' ability to comprehend images holistically. In this paper,\nwe propose a straightforward yet universal Adaptive Visual Anchoring strategy,\nwhich can be seamlessly integrated into existing MLLMs, offering significant\naccuracy improvements through adaptive compression. Meanwhile, to balance the\nresults derived from both global and compressed visual input, we further\nintroduce a novel collaborative decoding mechanism, enabling optimal\nperformance. Extensive experiments validate the effectiveness of our method,\ndemonstrating consistent performance improvements across various MLLMs. The\ncode will be publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2508.17860v1",
      "published": "2025-08-25T10:10:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17860v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference",
      "authors": [
        "Pengfei Jiang",
        "Hanjun Li",
        "Linglan Zhao",
        "Fei Chao",
        "Ke Yan",
        "Shouhong Ding",
        "Rongrong Ji"
      ],
      "abstract": "In this study, we introduce a novel method called group-wise \\textbf{VI}sual\ntoken \\textbf{S}election and \\textbf{A}ggregation (VISA) to address the issue\nof inefficient inference stemming from excessive visual tokens in multimoal\nlarge language models (MLLMs). Compared with previous token pruning approaches,\nour method can preserve more visual information while compressing visual\ntokens. We first propose a graph-based visual token aggregation (VTA) module.\nVTA treats each visual token as a node, forming a graph based on semantic\nsimilarity among visual tokens. It then aggregates information from removed\ntokens into kept tokens based on this graph, producing a more compact visual\ntoken representation. Additionally, we introduce a group-wise token selection\nstrategy (GTS) to divide visual tokens into kept and removed ones, guided by\ntext tokens from the final layers of each group. This strategy progressively\naggregates visual information, enhancing the stability of the visual\ninformation extraction process. We conduct comprehensive experiments on\nLLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate\nthe efficacy of VISA. Our method consistently outperforms previous methods,\nachieving a superior trade-off between model performance and inference speed.\nThe code is available at https://github.com/mobiushy/VISA.",
      "pdf_url": "http://arxiv.org/pdf/2508.17857v1",
      "published": "2025-08-25T10:07:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17857v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs",
      "authors": [
        "Han Zhang",
        "Ruibin Zheng",
        "Zexuan Yi",
        "Hanyang Peng",
        "Hui Wang",
        "Yue Yu"
      ],
      "abstract": "As single-center computing approaches power constraints, decentralized\ntraining is becoming essential. Reinforcement Learning (RL) post-training\nenhances Large Language Models (LLMs) but faces challenges in heterogeneous\ndistributed environments due to its tightly-coupled sampling-learning\nalternation. We propose HeteroRL, an asynchronous RL architecture that\ndecouples rollout sampling from parameter learning, enabling robust deployment\nacross geographically distributed nodes under network delays. We identify that\nlatency-induced KL divergence causes importance sampling failure due to high\nvariance. To address this, we propose Group Expectation Policy Optimization\n(GEPO), which reduces importance weight variance through a refined sampling\nmechanism. Theoretically, GEPO achieves exponential variance reduction.\nExperiments show it maintains superior stability over methods like GRPO, with\nless than 3% performance degradation under 1800-second delays, demonstrating\nstrong potential for decentralized RL in heterogeneous networks.",
      "pdf_url": "http://arxiv.org/pdf/2508.17850v1",
      "published": "2025-08-25T09:57:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17850v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games",
      "authors": [
        "Bingkang Shi",
        "Jen-tse Huang",
        "Guoyi Li",
        "Xiaodan Zhang",
        "Zhongjiang Yao"
      ],
      "abstract": "Leveraging their advanced capabilities, Large Language Models (LLMs)\ndemonstrate vast application potential in video games--from dynamic scene\ngeneration and intelligent NPC interactions to adaptive opponents--replacing or\nenhancing traditional game mechanics. However, LLMs' trustworthiness in this\napplication has not been sufficiently explored. In this paper, we reveal that\nthe models' inherent social biases can directly damage game balance in\nreal-world gaming environments. To this end, we present FairGamer, the first\nbias evaluation Benchmark for LLMs in video game scenarios, featuring six tasks\nand a novel metrics ${D_lstd}$. It covers three key scenarios in games where\nLLMs' social biases are particularly likely to manifest: Serving as Non-Player\nCharacters, Interacting as Competitive Opponents, and Generating Game Scenes.\nFairGamer utilizes both reality-grounded and fully fictional game content,\ncovering a variety of video game genres. Experiments reveal: (1) Decision\nbiases directly cause game balance degradation, with Grok-3 (average ${D_lstd}$\nscore=0.431) exhibiting the most severe degradation; (2) LLMs demonstrate\nisomorphic social/cultural biases toward both real and virtual world content,\nsuggesting their biases nature may stem from inherent model characteristics.\nThese findings expose critical reliability gaps in LLMs' gaming applications.\nOur code and data are available at anonymous GitHub\nhttps://github.com/Anonymous999-xxx/FairGamer .",
      "pdf_url": "http://arxiv.org/pdf/2508.17825v1",
      "published": "2025-08-25T09:26:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.17825v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}