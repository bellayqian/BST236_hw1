{
  "last_updated": "2026-01-13T00:53:22.864685",
  "papers": [
    {
      "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs",
      "authors": [
        "Chengming Cui",
        "Tianxin Wei",
        "Ziyi Chen",
        "Ruizhong Qiu",
        "Zhichen Zeng",
        "Zhining Liu",
        "Xuying Ning",
        "Duo Zhou",
        "Jingrui He"
      ],
      "abstract": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.",
      "pdf_url": "https://arxiv.org/pdf/2601.06022v1",
      "published": "2026-01-09T18:58:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.06022v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
      "authors": [
        "Qiguang Chen",
        "Yantao Du",
        "Ziniu Li",
        "Jinhao Liu",
        "Songyao Duan",
        "Jiarui Guo",
        "Minghao Liu",
        "Jiaheng Liu",
        "Tong Yang",
        "Ge Zhang",
        "Libo Qin",
        "Wanxiang Che",
        "Wenhao Huang"
      ],
      "abstract": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
      "pdf_url": "https://arxiv.org/pdf/2601.06002v1",
      "published": "2026-01-09T18:39:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.06002v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Open-Vocabulary 3D Instruction Ambiguity Detection",
      "authors": [
        "Jiayu Ding",
        "Haoran Tang",
        "Ge Li"
      ],
      "abstract": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.",
      "pdf_url": "https://arxiv.org/pdf/2601.05991v1",
      "published": "2026-01-09T18:17:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05991v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
      "authors": [
        "Longbin Ji",
        "Xiaoxiong Liu",
        "Junyuan Shang",
        "Shuohuan Wang",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "abstract": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
      "pdf_url": "https://arxiv.org/pdf/2601.05966v1",
      "published": "2026-01-09T17:34:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05966v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Performance of a Deep Learning-Based Segmentation Model for Pancreatic Tumors on Public Endoscopic Ultrasound Datasets",
      "authors": [
        "Pankaj Gupta",
        "Priya Mudgil",
        "Niharika Dutta",
        "Kartik Bose",
        "Nitish Kumar",
        "Anupam Kumar",
        "Jimil Shah",
        "Vaneet Jearth",
        "Jayanta Samanta",
        "Vishal Sharma",
        "Harshal Mandavdhare",
        "Surinder Rana",
        "Saroj K Sinha",
        "Usha Dutta"
      ],
      "abstract": "Background: Pancreatic cancer is one of the most aggressive cancers, with poor survival rates. Endoscopic ultrasound (EUS) is a key diagnostic modality, but its effectiveness is constrained by operator subjectivity. This study evaluates a Vision Transformer-based deep learning segmentation model for pancreatic tumors. Methods: A segmentation model using the USFM framework with a Vision Transformer backbone was trained and validated with 17,367 EUS images (from two public datasets) in 5-fold cross-validation. The model was tested on an independent dataset of 350 EUS images from another public dataset, manually segmented by radiologists. Preprocessing included grayscale conversion, cropping, and resizing to 512x512 pixels. Metrics included Dice similarity coefficient (DSC), intersection over union (IoU), sensitivity, specificity, and accuracy. Results: In 5-fold cross-validation, the model achieved a mean DSC of 0.651 +/- 0.738, IoU of 0.579 +/- 0.658, sensitivity of 69.8%, specificity of 98.8%, and accuracy of 97.5%. For the external validation set, the model achieved a DSC of 0.657 (95% CI: 0.634-0.769), IoU of 0.614 (95% CI: 0.590-0.689), sensitivity of 71.8%, and specificity of 97.7%. Results were consistent, but 9.7% of cases exhibited erroneous multiple predictions. Conclusions: The Vision Transformer-based model demonstrated strong performance for pancreatic tumor segmentation in EUS images. However, dataset heterogeneity and limited external validation highlight the need for further refinement, standardization, and prospective studies.",
      "pdf_url": "https://arxiv.org/pdf/2601.05937v1",
      "published": "2026-01-09T16:48:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05937v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Can We Predict Before Executing Machine Learning Agents?",
      "authors": [
        "Jingsheng Zheng",
        "Jintian Zhang",
        "Yujie Luo",
        "Yuren Mao",
        "Yunjun Gao",
        "Lun Du",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
      "pdf_url": "https://arxiv.org/pdf/2601.05930v1",
      "published": "2026-01-09T16:44:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05930v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world",
      "authors": [
        "E. Middell",
        "L. Carlton",
        "S. Moradi",
        "T. Codina",
        "T. Fischer",
        "J. Cutler",
        "S. Kelley",
        "J. Behrendt",
        "T. Dissanayake",
        "N. Harmening",
        "M. A. Yücel",
        "D. A. Boas",
        "A. von Lühmann"
      ],
      "abstract": "Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.",
      "pdf_url": "https://arxiv.org/pdf/2601.05923v1",
      "published": "2026-01-09T16:37:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05923v1",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "eess.IV",
        "q-bio.QM"
      ]
    },
    {
      "title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
      "authors": [
        "Tianshi Li"
      ],
      "abstract": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
      "pdf_url": "https://arxiv.org/pdf/2601.05918v1",
      "published": "2026-01-09T16:32:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05918v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates",
      "authors": [
        "Ayoub Ajarra",
        "Debabrota Basu"
      ],
      "abstract": "As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain properties of interest, raising fundamental questions about what can be reliably audited under such shifts.\n  In this work, we study group fairness auditing under arbitrary updates. We consider general shifts that modify the pre-audit model class while maintaining invariance of the audited property. Our goals are two-fold: (i) to characterize the information complexity of allowable updates, by identifying which strategic changes preserve the property under audit; and (ii) to efficiently estimate auditing properties, such as group fairness, using a minimal number of labeled samples.\n  We propose a generic framework for PAC auditing based on an Empirical Property Optimization (EPO) oracle. For statistical parity, we establish distribution-free auditing bounds characterized by the SP dimension, a novel combinatorial measure that captures the complexity of admissible strategic updates. Finally, we demonstrate that our framework naturally extends to other auditing objectives, including prediction error and robust risk.",
      "pdf_url": "https://arxiv.org/pdf/2601.05909v1",
      "published": "2026-01-09T16:28:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05909v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ]
    },
    {
      "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
      "authors": [
        "Haoming Xu",
        "Ningyuan Zhao",
        "Yunzhi Yao",
        "Weihong Xu",
        "Hongru Wang",
        "Xinle Deng",
        "Shumin Deng",
        "Jeff Z. Pan",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
      "pdf_url": "https://arxiv.org/pdf/2601.05905v1",
      "published": "2026-01-09T16:23:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05905v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Can AI mediation improve democratic deliberation?",
      "authors": [
        "Michael Henry Tessler",
        "Georgina Evans",
        "Michiel A. Bakker",
        "Iason Gabriel",
        "Sophie Bridgers",
        "Rishub Jain",
        "Raphael Koster",
        "Verena Rieser",
        "Anca Dragan",
        "Matthew Botvinick",
        "Christopher Summerfield"
      ],
      "abstract": "The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this \"trilemma\" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.",
      "pdf_url": "https://arxiv.org/pdf/2601.05904v1",
      "published": "2026-01-09T16:22:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05904v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
      "authors": [
        "Dawei Wang",
        "Chengming Zhou",
        "Di Zhao",
        "Xinyuan Liu",
        "Marci Chi Ma",
        "Gary Ushaw",
        "Richard Davison"
      ],
      "abstract": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).",
      "pdf_url": "https://arxiv.org/pdf/2601.05899v1",
      "published": "2026-01-09T16:18:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05899v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management",
      "authors": [
        "Ruizhe Zhang",
        "Xinke Jiang",
        "Zhibang Yang",
        "Zhixin Zhang",
        "Jiaran Gao",
        "Yuzhen Xiao",
        "Hongbin Lai",
        "Xu Chu",
        "Junfeng Zhao",
        "Yasha Wang"
      ],
      "abstract": "Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.",
      "pdf_url": "https://arxiv.org/pdf/2601.05890v1",
      "published": "2026-01-09T16:09:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05890v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift",
      "authors": [
        "Constantinos Karouzos",
        "Xingwei Tan",
        "Nikolaos Aletras"
      ],
      "abstract": "Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation",
      "pdf_url": "https://arxiv.org/pdf/2601.05882v1",
      "published": "2026-01-09T15:56:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05882v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law",
      "authors": [
        "Jakub Harasta",
        "Matej Vasina",
        "Martin Kornel",
        "Tomas Foltynek"
      ],
      "abstract": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.",
      "pdf_url": "https://arxiv.org/pdf/2601.05879v1",
      "published": "2026-01-09T15:55:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05879v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Continual-learning for Modelling Low-Resource Languages from Large Language Models",
      "authors": [
        "Santosh Srinath K",
        "Mudit Somani",
        "Varun Reddy Padala",
        "Prajna Devi Upadhyay",
        "Abhijit Das"
      ],
      "abstract": "Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.",
      "pdf_url": "https://arxiv.org/pdf/2601.05874v1",
      "published": "2026-01-09T15:51:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05874v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
      "authors": [
        "Huilin Deng",
        "Hongchen Luo",
        "Yue Zhu",
        "Long Li",
        "Zhuoyue Chen",
        "Xinghao Zhao",
        "Ming Li",
        "Jihai Zhang",
        "Mengchang Wang",
        "Yang Cao",
        "Yu Kang"
      ],
      "abstract": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
      "pdf_url": "https://arxiv.org/pdf/2601.05870v1",
      "published": "2026-01-09T15:46:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05870v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning",
      "authors": [
        "Alexandra Dragomir",
        "Florin Brad",
        "Radu Tudor Ionescu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.",
      "pdf_url": "https://arxiv.org/pdf/2601.05858v1",
      "published": "2026-01-09T15:34:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05858v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting",
      "authors": [
        "Yinghan Xu",
        "John Dingliana"
      ],
      "abstract": "We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at https://github.com/RockyXu66/LayerGS",
      "pdf_url": "https://arxiv.org/pdf/2601.05853v1",
      "published": "2026-01-09T15:30:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05853v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
      "authors": [
        "Sandeep Mishra",
        "Devichand Budagam",
        "Anubhab Mandal",
        "Bishal Santra",
        "Pawan Goyal",
        "Manish Gupta"
      ],
      "abstract": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
      "pdf_url": "https://arxiv.org/pdf/2601.05851v1",
      "published": "2026-01-09T15:29:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05851v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
      "authors": [
        "Nate Gillman",
        "Yinghua Zhou",
        "Zitian Tang",
        "Evan Luo",
        "Arjan Chakravarthy",
        "Daksh Aggarwal",
        "Michael Freeman",
        "Charles Herrmann",
        "Chen Sun"
      ],
      "abstract": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
      "pdf_url": "https://arxiv.org/pdf/2601.05848v1",
      "published": "2026-01-09T15:23:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05848v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation",
      "authors": [
        "Yutong Liang",
        "Shiyi Xu",
        "Yulong Zhang",
        "Bowen Zhan",
        "He Zhang",
        "Libin Liu"
      ],
      "abstract": "Capturing fine-grained hand-object interactions is challenging due to severe self-occlusion from closely spaced fingers and the subtlety of in-hand manipulation motions. Existing optical motion capture systems rely on expensive camera setups and extensive manual post-processing, while low-cost vision-based methods often suffer from reduced accuracy and reliability under occlusion. To address these challenges, we present DexterCap, a low-cost optical capture system for dexterous in-hand manipulation. DexterCap uses dense, character-coded marker patches to achieve robust tracking under severe self-occlusion, together with an automated reconstruction pipeline that requires minimal manual effort. With DexterCap, we introduce DexterHand, a dataset of fine-grained hand-object interactions covering diverse manipulation behaviors and objects, from simple primitives to complex articulated objects such as a Rubik's Cube. We release the dataset and code to support future research on dexterous hand-object interaction.",
      "pdf_url": "https://arxiv.org/pdf/2601.05844v1",
      "published": "2026-01-09T15:16:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05844v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Intelligent Singularity Avoidance in UR10 Robotic Arm Path Planning Using Hybrid Fuzzy Logic and Reinforcement Learning",
      "authors": [
        "Sheng-Kai Chen",
        "Jyh-Horng Wu"
      ],
      "abstract": "This paper presents a comprehensive approach to singularity detection and avoidance in UR10 robotic arm path planning through the integration of fuzzy logic safety systems and reinforcement learning algorithms. The proposed system addresses critical challenges in robotic manipulation where singularities can cause loss of control and potential equipment damage. Our hybrid approach combines real-time singularity detection using manipulability measures, condition number analysis, and fuzzy logic decision-making with a stable reinforcement learning framework for adaptive path planning. Experimental results demonstrate a 90% success rate in reaching target positions while maintaining safe distances from singular configurations. The system integrates PyBullet simulation for training data collection and URSim connectivity for real-world deployment.",
      "pdf_url": "https://arxiv.org/pdf/2601.05836v1",
      "published": "2026-01-09T15:10:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05836v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Influence of Parallelism in Vector-Multiplication Units on Correlation Power Analysis",
      "authors": [
        "Manuel Brosch",
        "Matthias Probst",
        "Stefan Kögler",
        "Georg Sigl"
      ],
      "abstract": "The use of neural networks in edge devices is increasing, which introduces new security challenges related to the neural networks' confidentiality. As edge devices often offer physical access, attacks targeting the hardware, such as side-channel analysis, must be considered. To enhance the performance of neural network inference, hardware accelerators are commonly employed. This work investigates the influence of parallel processing within such accelerators on correlation-based side-channel attacks that exploit power consumption. The focus is on neurons that are part of the same fully-connected layer, which run parallel and simultaneously process the same input value. The theoretical impact of concurrent multiply-and-accumulate operations on overall power consumption is evaluated, as well as the success rate of correlation power analysis. Based on the observed behavior, equations are derived that describe how the correlation decreases with increasing levels of parallelism. The applicability of these equations is validated using a vector-multiplication unit implemented on an FPGA.",
      "pdf_url": "https://arxiv.org/pdf/2601.05828v1",
      "published": "2026-01-09T15:01:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05828v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI",
      "authors": [
        "Lucija Mihić Zidar",
        "Philipp Wicke",
        "Praneel Bhatia",
        "Rosa Lutz",
        "Marius Klug",
        "Thorsten O. Zander"
      ],
      "abstract": "Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2601.05825v1",
      "published": "2026-01-09T14:59:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05825v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "SceneFoundry: Generating Interactive Infinite 3D Worlds",
      "authors": [
        "ChunTeng Chen",
        "YiChen Hsu",
        "YiWen Liu",
        "WeiFang Sun",
        "TsaiChing Ni",
        "ChunYi Lee",
        "Min Sun",
        "YuanFu Yang"
      ],
      "abstract": "The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.",
      "pdf_url": "https://arxiv.org/pdf/2601.05810v1",
      "published": "2026-01-09T14:33:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05810v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
      "authors": [
        "Xiaoshuai Song",
        "Haofei Chang",
        "Guanting Dong",
        "Yutao Zhu",
        "Zhicheng Dou",
        "Ji-Rong Wen"
      ],
      "abstract": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
      "pdf_url": "https://arxiv.org/pdf/2601.05808v1",
      "published": "2026-01-09T14:32:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05808v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning",
      "authors": [
        "Manel Gil-Sorribes",
        "Júlia Vilalta-Mor",
        "Isaac Filella-Mercè",
        "Robert Soliva",
        "Álvaro Ciudad",
        "Víctor Guallar",
        "Alexis Molina"
      ],
      "abstract": "Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.",
      "pdf_url": "https://arxiv.org/pdf/2601.05792v1",
      "published": "2026-01-09T13:39:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05792v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ]
    },
    {
      "title": "SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces",
      "authors": [
        "Tianwang Jia",
        "Xiaoqing Chen",
        "Dongrui Wu"
      ],
      "abstract": "Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) are widely adopted due to their efficiency and portability; however, their decoding algorithms still face multiple challenges, including inadequate generalization, adversarial vulnerability, and privacy leakage. This paper proposes Secure and Accurate FEderated learning (SAFE), a federated learning-based approach that protects user privacy by keeping data local during model training. SAFE employs local batch-specific normalization to mitigate cross-subject feature distribution shifts and hence improves model generalization. It further enhances adversarial robustness by introducing perturbations in both the input space and the parameter space through federated adversarial training and adversarial weight perturbation. Experiments on five EEG datasets from motor imagery (MI) and event-related potential (ERP) BCI paradigms demonstrated that SAFE consistently outperformed 14 state-of-the-art approaches in both decoding accuracy and adversarial robustness, while ensuring privacy protection. Notably, it even outperformed centralized training approaches that do not consider privacy protection at all. To our knowledge, SAFE is the first algorithm to simultaneously achieve high decoding accuracy, strong adversarial robustness, and reliable privacy protection without using any calibration data from the target subject, making it highly desirable for real-world BCIs.",
      "pdf_url": "https://arxiv.org/pdf/2601.05789v1",
      "published": "2026-01-09T13:29:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05789v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation",
      "authors": [
        "Zezhou Wang",
        "Ziyun Zhang",
        "Xiaoyi Zhang",
        "Zhuzhong Qian",
        "Yan Lu"
      ],
      "abstract": "Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git",
      "pdf_url": "https://arxiv.org/pdf/2601.05787v1",
      "published": "2026-01-09T13:26:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05787v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Adaptive Disentangled Representation Learning for Incomplete Multi-View Multi-Label Classification",
      "authors": [
        "Quanjiang Li",
        "Zhiming Liu",
        "Tianxiang Xu",
        "Tingjin Luo",
        "Chenping Hou"
      ],
      "abstract": "Multi-view multi-label learning frequently suffers from simultaneous feature absence and incomplete annotations, due to challenges in data acquisition and cost-intensive supervision. To tackle the complex yet highly practical problem while overcoming the existing limitations of feature recovery, representation disentanglement, and label semantics modeling, we propose an Adaptive Disentangled Representation Learning method (ADRL). ADRL achieves robust view completion by propagating feature-level affinity across modalities with neighborhood awareness, and reinforces reconstruction effectiveness by leveraging a stochastic masking strategy. Through disseminating category-level association across label distributions, ADRL refines distribution parameters for capturing interdependent label prototypes. Besides, we formulate a mutual-information-based objective to promote consistency among shared representations and suppress information overlap between view-specific representation and other modalities. Theoretically, we derive the tractable bounds to train the dual-channel network. Moreover, ADRL performs prototype-specific feature selection by enabling independent interactions between label embeddings and view representations, accompanied by the generation of pseudo-labels for each category. The structural characteristics of the pseudo-label space are then exploited to guide a discriminative trade-off during view fusion. Finally, extensive experiments on public datasets and real-world applications demonstrate the superior performance of ADRL.",
      "pdf_url": "https://arxiv.org/pdf/2601.05785v1",
      "published": "2026-01-09T13:22:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05785v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms",
      "authors": [
        "Turkan Simge Ispak",
        "Salih Tileylioglu",
        "Erdem Akagunduz"
      ],
      "abstract": "Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce \"overgeneralization\", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.",
      "pdf_url": "https://arxiv.org/pdf/2601.05759v1",
      "published": "2026-01-09T12:28:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05759v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit",
      "authors": [
        "Junda Lin",
        "Zhaomeng Zhou",
        "Zhi Zheng",
        "Shuochen Liu",
        "Tong Xu",
        "Yong Chen",
        "Enhong Chen"
      ],
      "abstract": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility. Code is available at https://anonymous.4open.science/r/VIGIL-378B/.",
      "pdf_url": "https://arxiv.org/pdf/2601.05755v1",
      "published": "2026-01-09T12:19:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05755v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Analysing Differences in Persuasive Language in LLM-Generated Text: Uncovering Stereotypical Gender Patterns",
      "authors": [
        "Amalie Brogaard Pauli",
        "Maria Barrett",
        "Max Müller-Eberstein",
        "Isabelle Augenstein",
        "Ira Assent"
      ],
      "abstract": "Large language models (LLMs) are increasingly used for everyday communication tasks, including drafting interpersonal messages intended to influence and persuade. Prior work has shown that LLMs can successfully persuade humans and amplify persuasive language. It is therefore essential to understand how user instructions affect the generation of persuasive language, and to understand whether the generated persuasive language differs, for example, when targeting different groups. In this work, we propose a framework for evaluating how persuasive language generation is affected by recipient gender, sender intent, or output language. We evaluate 13 LLMs and 16 languages using pairwise prompt instructions. We evaluate model responses on 19 categories of persuasive language using an LLM-as-judge setup grounded in social psychology and communication science. Our results reveal significant gender differences in the persuasive language generated across all models. These patterns reflect biases consistent with gender-stereotypical linguistic tendencies documented in social psychology and sociolinguistics.",
      "pdf_url": "https://arxiv.org/pdf/2601.05751v1",
      "published": "2026-01-09T12:07:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05751v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation",
      "authors": [
        "Zhenghao Li",
        "Zhi Zheng",
        "Wei Chen",
        "Jielun Zhao",
        "Yong Chen",
        "Tong Xu",
        "Enhong Chen"
      ],
      "abstract": "Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.",
      "pdf_url": "https://arxiv.org/pdf/2601.05746v1",
      "published": "2026-01-09T12:01:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05746v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Echo Chamber Multi-Turn LLM Jailbreak",
      "authors": [
        "Ahmad Alobaid",
        "Martí Jordà Roca",
        "Carlos Castillo",
        "Joan Vendrell"
      ],
      "abstract": "The availability of Large Language Models (LLMs) has led to a new generation of powerful chatbots that can be developed at relatively low cost. As companies deploy these tools, security challenges need to be addressed to prevent financial loss and reputational damage. A key security challenge is jailbreaking, the malicious manipulation of prompts and inputs to bypass a chatbot's safety guardrails. Multi-turn attacks are a relatively new form of jailbreaking involving a carefully crafted chain of interactions with a chatbot. We introduce Echo Chamber, a new multi-turn attack using a gradual escalation method. We describe this attack in detail, compare it to other multi-turn attacks, and demonstrate its performance against multiple state-of-the-art models through extensive evaluation.",
      "pdf_url": "https://arxiv.org/pdf/2601.05742v1",
      "published": "2026-01-09T11:46:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05742v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility",
      "authors": [
        "G M Shahariar",
        "Zabir Al Nazi",
        "Md Olid Hasan Bhuiyan",
        "Zhouxing Shi"
      ],
      "abstract": "Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.",
      "pdf_url": "https://arxiv.org/pdf/2601.05739v1",
      "published": "2026-01-09T11:40:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05739v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.CV"
      ]
    },
    {
      "title": "mHC-lite: You Don't Need 20 Sinkhorn-Knopp Iterations",
      "authors": [
        "Yongyi Yang",
        "Jianyang Gao"
      ],
      "abstract": "Hyper-Connections (HC) generalizes residual connections by introducing dynamic residual matrices that mix information across multiple residual streams, accelerating convergence in deep neural networks. However, unconstrained residual matrices can compromise training stability. To address this, DeepSeek's Manifold-Constrained Hyper-Connections (mHC) approximately projects these matrices onto the Birkhoff polytope via iterative Sinkhorn--Knopp (SK) normalization. We identify two limitations of this approach: (i) finite SK iterations do not guarantee exact doubly stochasticity, leaving an approximation gap that can accumulate through network depth and undermine stability; (ii) efficient SK implementation requires highly specialized CUDA kernels, raising engineering barriers and reducing portability. Motivated by the Birkhoff--von Neumann theorem, we propose mHC-lite, a simple reparameterization that explicitly constructs doubly stochastic matrices as convex combinations of permutation matrices. This approach guarantees exact doubly stochasticity by construction and can be implemented using only native matrix operations. Extensive experiments demonstrate that mHC-lite matches or exceeds mHC in performance while achieving higher training throughput with a naive implementation and eliminating the residual instabilities observed in both HC and mHC. The code is publicly available at https://github.com/FFTYYY/mhc-lite.",
      "pdf_url": "https://arxiv.org/pdf/2601.05732v1",
      "published": "2026-01-09T11:19:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05732v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding",
      "authors": [
        "Yuxuan Zhou",
        "Fei Huang",
        "Heng Li",
        "Fengyi Wu",
        "Tianyu Wang",
        "Jianwei Zhang",
        "Junyang Lin",
        "Zhi-Qi Cheng"
      ],
      "abstract": "Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.",
      "pdf_url": "https://arxiv.org/pdf/2601.05724v1",
      "published": "2026-01-09T11:10:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05724v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging",
      "authors": [
        "Thomas Fabian"
      ],
      "abstract": "Understanding how large language models (LLMs) represent natural language is a central challenge in natural language processing (NLP) research. Many existing methods extract word embeddings from an LLM, visualise the embedding space via point-plots, and compare the relative positions of certain words. However, this approach only considers single words and not whole natural language expressions, thus disregards the context in which a word is used. Here we present a novel tool for analysing and visualising information flow in natural language expressions by applying diffusion tensor imaging (DTI) to word embeddings. We find that DTI reveals how information flows between word embeddings. Tracking information flows within the layers of an LLM allows for comparing different model structures and revealing opportunities for pruning an LLM's under-utilised layers. Furthermore, our model reveals differences in information flows for tasks like pronoun resolution and metaphor detection. Our results show that our model permits novel insights into how LLMs represent actual natural language expressions, extending the comparison of isolated word embeddings and improving the interpretability of NLP models.",
      "pdf_url": "https://arxiv.org/pdf/2601.05713v1",
      "published": "2026-01-09T10:58:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05713v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multimodal In-context Learning for ASR of Low-resource Languages",
      "authors": [
        "Zhaolin Li",
        "Jan Niehues"
      ],
      "abstract": "Automatic speech recognition (ASR) still covers only a small fraction of the world's languages, mainly due to supervised data scarcity. In-context learning (ICL) with large language models (LLMs) addresses this problem, but prior work largely focuses on high-resource languages covered during training and text-only settings. This paper investigates whether speech LLMs can learn unseen languages with multimodal ICL (MICL), and how this learning can be used to improve ASR. We conduct experiments with two speech LLMs, Phi-4 and Qwen3-Omni, on three diverse endangered languages. Firstly, we find that MICL is effective for unseen languages, leveraging both speech and text modalities. We further show that cross-lingual transfer learning improves MICL efficiency on target languages without training on them. Moreover, we analyze attention patterns to interpret MICL mechanisms, and we observe layer-dependent preferences between audio and text context, with an overall bias towards text. Finally, we show that prompt-based ASR with speech LLMs performs poorly on unseen languages, motivating a simple ASR system that combines a stronger acoustic model with a speech LLM via MICL-based selection of acoustic hypotheses. Results show that MICL consistently improves ASR performance, and that cross-lingual transfer learning matches or outperforms corpus-trained language models without using target-language data. Our code is publicly available.",
      "pdf_url": "https://arxiv.org/pdf/2601.05707v1",
      "published": "2026-01-09T10:52:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05707v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning",
      "authors": [
        "Ali Farjami",
        "Luca Redondi",
        "Marco Valentino"
      ],
      "abstract": "Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2601.05705v1",
      "published": "2026-01-09T10:47:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05705v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ]
    },
    {
      "title": "AIBoMGen: Generating an AI Bill of Materials for Secure, Transparent, and Compliant Model Training",
      "authors": [
        "Wiebe Vandendriessche",
        "Jordi Thijsman",
        "Laurens D'hooge",
        "Bruno Volckaert",
        "Merlijn Sebrechts"
      ],
      "abstract": "The rapid adoption of complex AI systems has outpaced the development of tools to ensure their transparency, security, and regulatory compliance. In this paper, the AI Bill of Materials (AIBOM), an extension of the Software Bill of Materials (SBOM), is introduced as a standardized, verifiable record of trained AI models and their environments. Our proof-of-concept platform, AIBoMGen, automates the generation of signed AIBOMs by capturing datasets, model metadata, and environment details during training. The training platform acts as a neutral, third-party observer and root of trust. It enforces verifiable AIBOM creation for every job. The system uses cryptographic hashing, digital signatures, and in-toto attestations to ensure integrity and protect against threats such as artifact tampering by dishonest model creators. Our evaluation demonstrates that AIBoMGen reliably detects unauthorized modifications to all artifacts and can generate AIBOMs with negligible performance overhead. These results highlight the potential of AIBoMGen as a foundational step toward building secure and transparent AI ecosystems, enabling compliance with regulatory frameworks like the EUs AI Act.",
      "pdf_url": "https://arxiv.org/pdf/2601.05703v1",
      "published": "2026-01-09T10:46:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05703v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models",
      "authors": [
        "Zenghao Duan",
        "Liang Pang",
        "Zihao Wei",
        "Wenbin Duan",
        "Yuxin Tian",
        "Shicheng Xu",
        "Jingcheng Deng",
        "Zhiyi Yin",
        "Xueqi Cheng"
      ],
      "abstract": "Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2601.05693v1",
      "published": "2026-01-09T10:23:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05693v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Joint Optimization of Neural Autoregressors via Scoring rules",
      "authors": [
        "Jonas Landsgesell"
      ],
      "abstract": "Non-parametric distributional regression has achieved significant milestones in recent years. Among these, the Tabular Prior-Data Fitted Network (TabPFN) has demonstrated state-of-the-art performance on various benchmarks. However, a challenge remains in extending these grid-based approaches to a truly multivariate setting. In a naive non-parametric discretization with $N$ bins per dimension, the complexity of an explicit joint grid scales exponentially and the paramer count of the neural networks rise sharply. This scaling is particularly detrimental in low-data regimes, as the final projection layer would require many parameters, leading to severe overfitting and intractability.",
      "pdf_url": "https://arxiv.org/pdf/2601.05683v1",
      "published": "2026-01-09T10:05:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05683v1",
      "categories": [
        "cond-mat.soft",
        "cs.AI"
      ]
    },
    {
      "title": "AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces",
      "authors": [
        "Yeonsang Shin",
        "Insoo Kim",
        "Bongkeun Kim",
        "Keonwoo Bae",
        "Bohyung Han"
      ],
      "abstract": "Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.",
      "pdf_url": "https://arxiv.org/pdf/2601.05680v1",
      "published": "2026-01-09T09:57:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05680v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "CHDP: Cooperative Hybrid Diffusion Policies for Reinforcement Learning in Parameterized Action Space",
      "authors": [
        "Bingyi Liu",
        "Jinbo He",
        "Haiyong Shi",
        "Enshu Wang",
        "Weizhen Han",
        "Jingxiang Hao",
        "Peixi Wang",
        "Zhuangzhuang Zhang"
      ],
      "abstract": "Hybrid action space, which combines discrete choices and continuous parameters, is prevalent in domains such as robot control and game AI. However, efficiently modeling and optimizing hybrid discrete-continuous action space remains a fundamental challenge, mainly due to limited policy expressiveness and poor scalability in high-dimensional settings. To address this challenge, we view the hybrid action space problem as a fully cooperative game and propose a \\textbf{Cooperative Hybrid Diffusion Policies (CHDP)} framework to solve it. CHDP employs two cooperative agents that leverage a discrete and a continuous diffusion policy, respectively. The continuous policy is conditioned on the discrete action's representation, explicitly modeling the dependency between them. This cooperative design allows the diffusion policies to leverage their expressiveness to capture complex distributions in their respective action spaces. To mitigate the update conflicts arising from simultaneous policy updates in this cooperative setting, we employ a sequential update scheme that fosters co-adaptation. Moreover, to improve scalability when learning in high-dimensional discrete action space, we construct a codebook that embeds the action space into a low-dimensional latent space. This mapping enables the discrete policy to learn in a compact, structured space. Finally, we design a Q-function-based guidance mechanism to align the codebook's embeddings with the discrete policy's representation during training. On challenging hybrid action benchmarks, CHDP outperforms the state-of-the-art method by up to $19.3\\%$ in success rate.",
      "pdf_url": "https://arxiv.org/pdf/2601.05675v1",
      "published": "2026-01-09T09:50:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05675v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Advancing credit mobility through stakeholder-informed AI design and adoption",
      "authors": [
        "Yerin Kwak",
        "Siddharth Adelkar",
        "Zachary A. Pardos"
      ],
      "abstract": "Transferring from a 2-year to a 4-year college is crucial for socioeconomic mobility, yet students often face challenges ensuring their credits are fully recognized, leading to delays in their academic progress and unexpected costs. Determining whether courses at different institutions are equivalent (i.e., articulation) is essential for successful credit transfer, as it minimizes unused credits and increases the likelihood of bachelor's degree completion. However, establishing articulation agreements remains time- and resource-intensive, as all candidate articulations are reviewed manually. Although recent efforts have explored the use of artificial intelligence to support this work, its use in articulation practice remains limited. Given these challenges and the need for scalable support, this study applies artificial intelligence to suggest articulations between institutions in collaboration with the State University of New York system, one of the largest systems of higher education in the US. To develop our methodology, we first surveyed articulation staff and faculty to assess adoption rates of baseline algorithmic recommendations and gather feedback on perceptions and concerns about these recommendations. Building on these insights, we developed a supervised alignment method that addresses superficial matching and institutional biases in catalog descriptions, achieving a 5.5-fold improvement in accuracy over previous methods. Based on articulation predictions of this method and a 61% average surveyed adoption rate among faculty and staff, these findings project a 12-fold increase in valid credit mobility opportunities that would otherwise remain unrealized. This study suggests that stakeholder-informed design of AI in higher education administration can expand student credit mobility and help reshape current institutional decision-making in course articulation.",
      "pdf_url": "https://arxiv.org/pdf/2601.05666v1",
      "published": "2026-01-09T09:39:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05666v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat",
      "authors": [
        "Hao Yang",
        "Hongyuan Lu",
        "Dingkang Yang",
        "Wenliang Yang",
        "Peng Sun",
        "Xiaochuan Zhang",
        "Jun Xiao",
        "Kefan He",
        "Wai Lam",
        "Yang Liu",
        "Xinhua Zeng"
      ],
      "abstract": "Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.",
      "pdf_url": "https://arxiv.org/pdf/2601.05657v1",
      "published": "2026-01-09T09:27:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05657v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation",
      "authors": [
        "Rongxin Chen",
        "Tianyu Wu",
        "Bingbing Xu",
        "Xiucheng Xu",
        "Huawei Shen"
      ],
      "abstract": "High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.",
      "pdf_url": "https://arxiv.org/pdf/2601.05656v1",
      "published": "2026-01-09T09:26:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2601.05656v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}