{
  "last_updated": "2025-08-10T01:01:27.608020",
  "papers": [
    {
      "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling",
      "authors": [
        "Jianpeng Yao",
        "Xiaopan Zhang",
        "Yu Xia",
        "Zejin Wang",
        "Amit K. Roy-Chowdhury",
        "Jiachen Li"
      ],
      "abstract": "Mobile robots navigating in crowds trained using reinforcement learning are\nknown to suffer performance degradation when faced with out-of-distribution\nscenarios. We propose that by properly accounting for the uncertainties of\npedestrians, a robot can learn safe navigation policies that are robust to\ndistribution shifts. Our method augments agent observations with prediction\nuncertainty estimates generated by adaptive conformal inference, and it uses\nthese estimates to guide the agent's behavior through constrained reinforcement\nlearning. The system helps regulate the agent's actions and enables it to adapt\nto distribution shifts. In the in-distribution setting, our approach achieves a\n96.93% success rate, which is over 8.80% higher than the previous\nstate-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times\nfewer intrusions into ground-truth human future trajectories. In three\nout-of-distribution scenarios, our method shows much stronger robustness when\nfacing distribution shifts in velocity variations, policy changes, and\ntransitions from individual to group dynamics. We deploy our method on a real\nrobot, and experiments show that the robot makes safe and robust decisions when\ninteracting with both sparse and dense crowds. Our code and videos are\navailable on https://gen-safe-nav.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2508.05634v1",
      "published": "2025-08-07T17:59:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05634v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation",
      "authors": [
        "Changle Qu",
        "Sunhao Dai",
        "Ke Guo",
        "Liqin Zhao",
        "Yanan Niu",
        "Xiao Zhang",
        "Jun Xu"
      ],
      "abstract": "Live streaming platforms have become a dominant form of online content\nconsumption, offering dynamically evolving content, real-time interactions, and\nhighly engaging user experiences. These unique characteristics introduce new\nchallenges that differentiate live streaming recommendation from traditional\nrecommendation settings and have garnered increasing attention from industry in\nrecent years. However, research progress in academia has been hindered by the\nlack of publicly available datasets that accurately reflect the dynamic nature\nof live streaming environments. To address this gap, we introduce KuaiLive, the\nfirst real-time, interactive dataset collected from Kuaishou, a leading live\nstreaming platform in China with over 400 million daily active users. The\ndataset records the interaction logs of 23,772 users and 452,621 streamers over\na 21-day period. Compared to existing datasets, KuaiLive offers several\nadvantages: it includes precise live room start and end timestamps, multiple\ntypes of real-time user interactions (click, comment, like, gift), and rich\nside information features for both users and streamers. These features enable\nmore realistic simulation of dynamic candidate items and better modeling of\nuser and streamer behaviors. We conduct a thorough analysis of KuaiLive from\nmultiple perspectives and evaluate several representative recommendation\nmethods on it, establishing a strong benchmark for future research. KuaiLive\ncan support a wide range of tasks in the live streaming domain, such as top-K\nrecommendation, click-through rate prediction, watch time prediction, and gift\nprice prediction. Moreover, its fine-grained behavioral data also enables\nresearch on multi-behavior modeling, multi-task learning, and fairness-aware\nrecommendation. The dataset and related resources are publicly available at\nhttps://imgkkk574.github.io/KuaiLive.",
      "pdf_url": "http://arxiv.org/pdf/2508.05633v1",
      "published": "2025-08-07T17:59:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05633v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages",
      "authors": [
        "Mehrdad Zakershahrak",
        "Samira Ghodratnama"
      ],
      "abstract": "Byte-level language models eliminate fragile tokenizers but face\ncomputational challenges in morphologically-rich languages (MRLs), where words\nspan many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that\nlearns linguistically-informed segmentation through end-to-end training. Key\ninnovations include: (1) a lightweight Transformer context-mixer (1.9M\nparameters) for cross-chunk attention, (2) a two-level latent hyper-prior for\ndocument-level consistency, (3) specialized handling of orthographic artifacts\n(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence\nlengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art\nresults: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better\ncompression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ\ncorruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks\nalign with Persian morphology without explicit supervision, demonstrating that\nhierarchical dynamic chunking provides an effective tokenizer-free solution for\nMRLs while maintaining computational efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2508.05628v1",
      "published": "2025-08-07T17:59:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05628v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations",
      "authors": [
        "Brandon Jaipersaud",
        "David Krueger",
        "Ekdeep Singh Lubana"
      ],
      "abstract": "Large Language Models (LLMs) have started to demonstrate the ability to\npersuade humans, yet our understanding of how this dynamic transpires is\nlimited. Recent work has used linear probes, lightweight tools for analyzing\nmodel representations, to study various LLM skills such as the ability to model\nuser sentiment and political perspective. Motivated by this, we apply probes to\nstudy persuasion dynamics in natural, multi-turn conversations. We leverage\ninsights from cognitive science to train probes on distinct aspects of\npersuasion: persuasion success, persuadee personality, and persuasion strategy.\nDespite their simplicity, we show that they capture various aspects of\npersuasion at both the sample and dataset levels. For instance, probes can\nidentify the point in a conversation where the persuadee was persuaded or where\npersuasive success generally occurs across the entire dataset. We also show\nthat in addition to being faster than expensive prompting-based approaches,\nprobes can do just as well and even outperform prompting in some settings, such\nas when uncovering persuasion strategy. This suggests probes as a plausible\navenue for studying other complex behaviours such as deception and\nmanipulation, especially in multi-turn settings and large-scale dataset\nanalysis where prompting-based methods would be computationally inefficient.",
      "pdf_url": "http://arxiv.org/pdf/2508.05625v1",
      "published": "2025-08-07T17:58:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05625v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents",
      "authors": [
        "Yu Yuan",
        "Lili Zhao",
        "Wei Chen",
        "Guangting Zheng",
        "Kai Zhang",
        "Mengdi Zhang",
        "Qi Liu"
      ],
      "abstract": "Capturing human learning behavior based on deep learning methods has become a\nmajor research focus in both psychology and intelligent systems. Recent\napproaches rely on controlled experiments or rule-based models to explore\ncognitive processes. However, they struggle to capture learning dynamics, track\nprogress over time, or provide explainability. To address these challenges, we\nintroduce LearnerAgent, a novel multi-agent framework based on Large Language\nModels (LLMs) to simulate a realistic teaching environment. To explore\nhuman-like learning dynamics, we construct learners with psychologically\ngrounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free\nGeneral Learner to inspect the base LLM's default behavior. Through weekly\nknowledge acquisition, monthly strategic choices, periodic tests, and peer\ninteraction, we can track the dynamic learning progress of individual learners\nover a full-year journey. Our findings are fourfold: 1) Longitudinal analysis\nreveals that only Deep Learner achieves sustained cognitive growth. Our\nspecially designed \"trap questions\" effectively diagnose Surface Learner's\nshallow knowledge. 2) The behavioral and cognitive patterns of distinct\nlearners align closely with their psychological profiles. 3) Learners'\nself-concept scores evolve realistically, with the General Learner developing\nsurprisingly high self-efficacy despite its cognitive limitations. 4)\nCritically, the default profile of base LLM is a \"diligent but brittle Surface\nLearner\"-an agent that mimics the behaviors of a good student but lacks true,\ngeneralizable understanding. Extensive simulation experiments demonstrate that\nLearnerAgent aligns well with real scenarios, yielding more insightful findings\nabout LLMs' behavior.",
      "pdf_url": "http://arxiv.org/pdf/2508.05622v1",
      "published": "2025-08-07T17:57:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05622v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Missing Reward: Active Inference in the Era of Experience",
      "authors": [
        "Bo Wen"
      ],
      "abstract": "This paper argues that Active Inference (AIF) provides a crucial foundation\nfor developing autonomous AI agents capable of learning from experience without\ncontinuous human reward engineering. As AI systems begin to exhaust\nhigh-quality training data and rely on increasingly large human workforces for\nreward design, the current paradigm faces significant scalability challenges\nthat could impede progress toward genuinely autonomous intelligence. The\nproposal for an ``Era of Experience,'' where agents learn from self-generated\ndata, is a promising step forward. However, this vision still depends on\nextensive human engineering of reward functions, effectively shifting the\nbottleneck from data curation to reward curation. This highlights what we\nidentify as the \\textbf{grounded-agency gap}: the inability of contemporary AI\nsystems to autonomously formulate, adapt, and pursue objectives in response to\nchanging circumstances. We propose that AIF can bridge this gap by replacing\nexternal reward signals with an intrinsic drive to minimize free energy,\nallowing agents to naturally balance exploration and exploitation through a\nunified Bayesian objective. By integrating Large Language Models as generative\nworld models with AIF's principled decision-making framework, we can create\nagents that learn efficiently from experience while remaining aligned with\nhuman values. This synthesis offers a compelling path toward AI systems that\ncan develop autonomously while adhering to both computational and physical\nconstraints.",
      "pdf_url": "http://arxiv.org/pdf/2508.05619v1",
      "published": "2025-08-07T17:57:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05619v1",
      "categories": [
        "cs.AI",
        "nlin.AO",
        "physics.bio-ph",
        "physics.comp-ph",
        "physics.hist-ph"
      ]
    },
    {
      "title": "TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution",
      "authors": [
        "Zhikai Zhao",
        "Chuanbo Hua",
        "Federico Berto",
        "Kanghoon Lee",
        "Zihan Ma",
        "Jiachen Li",
        "Jinkyoo Park"
      ],
      "abstract": "Trajectory prediction is a critical task in modeling human behavior,\nespecially in safety-critical domains such as social robotics and autonomous\nvehicle navigation. Traditional heuristics based on handcrafted rules often\nlack accuracy and generalizability. Although deep learning approaches offer\nimproved performance, they typically suffer from high computational cost,\nlimited explainability, and, importantly, poor generalization to\nout-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a\nframework that leverages Large Language Models (LLMs) to automatically design\ntrajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to\ngenerate and refine prediction heuristics from past trajectory data. We propose\ntwo key innovations: Cross-Generation Elite Sampling to encourage population\ndiversity, and a Statistics Feedback Loop that enables the LLM to analyze and\nimprove alternative predictions. Our evaluations demonstrate that TrajEvo\noutperforms existing heuristic methods across multiple real-world datasets, and\nnotably surpasses both heuristic and deep learning methods in generalizing to\nan unseen OOD real-world dataset. TrajEvo marks a promising step toward the\nautomated design of fast, explainable, and generalizable trajectory prediction\nheuristics. We release our source code to facilitate future research at\nhttps://github.com/ai4co/trajevo.",
      "pdf_url": "http://arxiv.org/pdf/2508.05616v1",
      "published": "2025-08-07T17:55:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05616v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.RO"
      ]
    },
    {
      "title": "Test-Time Reinforcement Learning for GUI Grounding via Region Consistency",
      "authors": [
        "Yong Du",
        "Yuchen Yan",
        "Fei Tang",
        "Zhengxi Lu",
        "Chang Zong",
        "Weiming Lu",
        "Shengpei Jiang",
        "Yongliang Shen"
      ],
      "abstract": "Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents.",
      "pdf_url": "http://arxiv.org/pdf/2508.05615v1",
      "published": "2025-08-07T17:54:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05615v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks",
      "authors": [
        "Zixuan Wang",
        "Dingming Li",
        "Hongxing Li",
        "Shuo Chen",
        "Yuchen Yan",
        "Wenqi Zhang",
        "Yongliang Shen",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "abstract": "Large language models excel at abstract reasoning but their capacity for\nembodied agent reasoning remains largely unexplored. We present OmniEAR, a\ncomprehensive framework for evaluating how language models reason about\nphysical interactions, tool usage, and multi-agent coordination in embodied\ntasks. Unlike existing benchmarks that provide predefined tool sets or explicit\ncollaboration directives, OmniEAR requires agents to dynamically acquire\ncapabilities and autonomously determine coordination strategies based on task\ndemands. Through text-based environment representation, we model continuous\nphysical properties and complex spatial relationships across 1,500 scenarios\nspanning household and industrial domains. Our systematic evaluation reveals\nsevere performance degradation when models must reason from constraints: while\nachieving 85-96% success with explicit instructions, performance drops to\n56-85% for tool reasoning and 63-85% for implicit collaboration, with compound\ntasks showing over 50% failure rates. Surprisingly, complete environmental\ninformation degrades coordination performance, indicating models cannot filter\ntask-relevant constraints. Fine-tuning improves single-agent tasks dramatically\n(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing\nfundamental architectural limitations. These findings demonstrate that embodied\nreasoning poses fundamentally different challenges than current models can\naddress, establishing OmniEAR as a rigorous benchmark for evaluating and\nadvancing embodied AI systems. Our code and data are included in the\nsupplementary materials and will be open-sourced upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2508.05614v1",
      "published": "2025-08-07T17:54:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05614v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models",
      "authors": [
        "Haitao Hong",
        "Yuchen Yan",
        "Xingyu Wu",
        "Guiyang Hou",
        "Wenqi Zhang",
        "Weiming Lu",
        "Yongliang Shen",
        "Jun Xiao"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.",
      "pdf_url": "http://arxiv.org/pdf/2508.05613v1",
      "published": "2025-08-07T17:53:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05613v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle",
      "authors": [
        "Linghao Zhu",
        "Yiran Guan",
        "Dingkang Liang",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Bin Qin",
        "Jian Luan",
        "Yuliang Liu",
        "Xiang Bai"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as an effective post-training\nparadigm for enhancing the reasoning capabilities of multimodal large language\nmodel (MLLM). However, current RL pipelines often suffer from training\ninefficiencies caused by two underexplored issues: Advantage Collapsing, where\nmost advantages in a batch concentrate near zero, and Rollout Silencing, where\nthe proportion of rollouts contributing non-zero gradients diminishes over\ntime. These issues lead to suboptimal gradient updates and hinder long-term\nlearning efficiency. To address these issues, we propose Shuffle-R1, a simple\nyet principled framework that improves RL fine-tuning efficiency by dynamically\nrestructuring trajectory sampling and batch composition. It introduces (1)\nPairwise Trajectory Sampling, which selects high-contrast trajectories with\nlarge advantages to improve gradient signal quality, and (2) Advantage-based\nTrajectory Shuffle, which increases exposure of valuable rollouts through\ninformed batch reshuffling. Experiments across multiple reasoning benchmarks\nshow that our framework consistently outperforms strong RL baselines with\nminimal overhead. These results highlight the importance of data-centric\nadaptations for more efficient RL training in MLLM.",
      "pdf_url": "http://arxiv.org/pdf/2508.05612v1",
      "published": "2025-08-07T17:53:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05612v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models",
      "authors": [
        "Guilherme Seidyo Imai Aldeia",
        "Daniel S. Herman",
        "William G. La Cava"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities for\nmedical question answering and programming, but their potential for generating\ninterpretable computable phenotypes (CPs) is under-explored. In this work, we\ninvestigate whether LLMs can generate accurate and concise CPs for six clinical\nphenotypes of varying complexity, which could be leveraged to enable scalable\nclinical decision support to improve care for patients with hypertension. In\naddition to evaluating zero-short performance, we propose and test a\nsynthesize, execute, debug, instruct strategy that uses LLMs to generate and\niteratively refine CPs using data-driven feedback. Our results show that LLMs,\ncoupled with iterative learning, can generate interpretable and reasonably\naccurate programs that approach the performance of state-of-the-art ML methods\nwhile requiring significantly fewer training examples.",
      "pdf_url": "http://arxiv.org/pdf/2508.05581v1",
      "published": "2025-08-07T17:15:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05581v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media",
      "authors": [
        "Rui Lu",
        "Jinhe Bi",
        "Yunpu Ma",
        "Feng Xiao",
        "Yuntao Du",
        "Yijun Tian"
      ],
      "abstract": "Social media has evolved into a complex multimodal environment where text,\nimages, and other signals interact to shape nuanced meanings, often concealing\nharmful intent. Identifying such intent, whether sarcasm, hate speech, or\nmisinformation, remains challenging due to cross-modal contradictions, rapid\ncultural shifts, and subtle pragmatic cues. To address these challenges, we\npropose MV-Debate, a multi-view agent debate framework with dynamic reflection\ngating for unified multimodal harmful content detection. MV-Debate assembles\nfour complementary debate agents, a surface analyst, a deep reasoner, a\nmodality contrast, and a social contextualist, to analyze content from diverse\ninterpretive perspectives. Through iterative debate and reflection, the agents\nrefine responses under a reflection-gain criterion, ensuring both accuracy and\nefficiency. Experiments on three benchmark datasets demonstrate that MV-Debate\nsignificantly outperforms strong single-model and existing multi-agent debate\nbaselines. This work highlights the promise of multi-agent debate in advancing\nreliable social intent detection in safety-critical online contexts.",
      "pdf_url": "http://arxiv.org/pdf/2508.05557v1",
      "published": "2025-08-07T16:38:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05557v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Adapting Vision-Language Models Without Labels: A Comprehensive Survey",
      "authors": [
        "Hao Dong",
        "Lijun Sheng",
        "Jian Liang",
        "Ran He",
        "Eleni Chatzi",
        "Olga Fink"
      ],
      "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable generalization\ncapabilities across a wide range of tasks. However, their performance often\nremains suboptimal when directly applied to specific downstream scenarios\nwithout task-specific adaptation. To enhance their utility while preserving\ndata efficiency, recent research has increasingly focused on unsupervised\nadaptation methods that do not rely on labeled data. Despite the growing\ninterest in this area, there remains a lack of a unified, task-oriented survey\ndedicated to unsupervised VLM adaptation. To bridge this gap, we present a\ncomprehensive and structured overview of the field. We propose a taxonomy based\non the availability and nature of unlabeled visual data, categorizing existing\napproaches into four key paradigms: Data-Free Transfer (no data), Unsupervised\nDomain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),\nand Online Test-Time Adaptation (streaming data). Within this framework, we\nanalyze core methodologies and adaptation strategies associated with each\nparadigm, aiming to establish a systematic understanding of the field.\nAdditionally, we review representative benchmarks across diverse applications\nand highlight open challenges and promising directions for future research. An\nactively maintained repository of relevant literature is available at\nhttps://github.com/tim-learn/Awesome-LabelFree-VLMs.",
      "pdf_url": "http://arxiv.org/pdf/2508.05547v1",
      "published": "2025-08-07T16:27:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05547v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees",
      "authors": [
        "Guang Yang",
        "Xinyang Liu"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable progress in\nmultiple-choice question answering (MCQA), but their inherent unreliability,\nsuch as hallucination and overconfidence, limits their application in high-risk\ndomains. To address this, we propose a frequency-based uncertainty\nquantification method under black-box settings, leveraging conformal prediction\n(CP) to ensure provable coverage guarantees. Our approach involves multiple\nindependent samplings of the model's output distribution for each input, with\nthe most frequent sample serving as a reference to calculate predictive entropy\n(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,\nMedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms\nlogit-based PE in distinguishing between correct and incorrect predictions, as\nmeasured by AUROC. Furthermore, the method effectively controls the empirical\nmiscoverage rate under user-specified risk levels, validating that sampling\nfrequency can serve as a viable substitute for logit-based probabilities in\nblack-box scenarios. This work provides a distribution-free model-agnostic\nframework for reliable uncertainty quantification in MCQA with guaranteed\ncoverage, enhancing the trustworthiness of LLMs in practical applications.",
      "pdf_url": "http://arxiv.org/pdf/2508.05544v1",
      "published": "2025-08-07T16:22:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05544v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Tractable Sharpness-Aware Learning of Probabilistic Circuits",
      "authors": [
        "Hrithik Suresh",
        "Sahil Sidheekh",
        "Vishnu Shreeram M. P",
        "Sriraam Natarajan",
        "Narayanan C. Krishnan"
      ],
      "abstract": "Probabilistic Circuits (PCs) are a class of generative models that allow\nexact and tractable inference for a wide range of queries. While recent\ndevelopments have enabled the learning of deep and expressive PCs, this\nincreased capacity can often lead to overfitting, especially when data is\nlimited. We analyze PC overfitting from a log-likelihood-landscape perspective\nand show that it is often caused by convergence to sharp optima that generalize\npoorly. Inspired by sharpness aware minimization in neural networks, we propose\na Hessian-based regularizer for training PCs. As a key contribution, we show\nthat the trace of the Hessian of the log-likelihood-a sharpness proxy that is\ntypically intractable in deep neural networks-can be computed efficiently for\nPCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer\nthat yields simple closed-form parameter updates for EM, and integrates\nseamlessly with gradient based learning methods. Experiments on synthetic and\nreal-world datasets demonstrate that our method consistently guides PCs toward\nflatter minima, improves generalization performance.",
      "pdf_url": "http://arxiv.org/pdf/2508.05537v1",
      "published": "2025-08-07T16:13:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05537v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities",
      "authors": [
        "Harsh Nishant Lalai",
        "Raj Sanjay Shah",
        "Jiaxin Pei",
        "Sashank Varma",
        "Yi-Chia Wang",
        "Ali Emami"
      ],
      "abstract": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home.",
      "pdf_url": "http://arxiv.org/pdf/2508.05525v1",
      "published": "2025-08-07T15:53:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05525v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program",
      "authors": [
        "Meryem Yilmaz Soylu",
        "Adrian Gallard",
        "Jeonghyun Lee",
        "Gayane Grigoryan",
        "Rushil Desai",
        "Stephen Harmon"
      ],
      "abstract": "Letters of recommendation (LORs) provide valuable insights into candidates'\ncapabilities and experiences beyond standardized test scores. However,\nreviewing these text-heavy materials is time-consuming and labor-intensive. To\naddress this challenge and support the admission committee in providing\nfeedback for students' professional growth, our study introduces LORI: LOR\nInsights, a novel AI-based detection tool for assessing leadership skills in\nLORs submitted by online master's program applicants. By employing natural\nlanguage processing and leveraging large language models using RoBERTa and\nLLAMA, we seek to identify leadership attributes such as teamwork,\ncommunication, and innovation. Our latest RoBERTa model achieves a weighted F1\nscore of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong\nlevel of consistency in our test data. With the growing importance of\nleadership skills in the STEM sector, integrating LORI into the graduate\nadmissions process is crucial for accurately assessing applicants' leadership\ncapabilities. This approach not only streamlines the admissions process but\nalso automates and ensures a more comprehensive evaluation of candidates'\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2508.05513v1",
      "published": "2025-08-07T15:46:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05513v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LAG: Logic-Augmented Generation from a Cartesian Perspective",
      "authors": [
        "Yilin Xiao",
        "Chuang Zhou",
        "Qinggang Zhang",
        "Su Dong",
        "Shengyuan Chen",
        "Xiao Huang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.05509v1",
      "published": "2025-08-07T15:42:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05509v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation",
      "authors": [
        "Roshita Bhonsle",
        "Rishav Dutta",
        "Sneha Vavilapalli",
        "Harsh Seth",
        "Abubakarr Jaye",
        "Yapei Chang",
        "Mukund Rungta",
        "Emmanuel Aboah Boateng",
        "Sadid Hasan",
        "Ehi Nosakhare",
        "Soundar Srinivasan"
      ],
      "abstract": "The increasing adoption of foundation models as agents across diverse domains\nnecessitates a robust evaluation framework. Current methods, such as\nLLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step\nreasoning that drives agentic decision-making. Meanwhile, existing\nAgent-as-a-Judge systems, where one agent evaluates another's task completion,\nare typically designed for narrow, domain-specific settings. To address this\ngap, we propose a generalizable, modular framework for evaluating agent task\ncompletion independent of the task domain. The framework emulates human-like\nevaluation by decomposing tasks into sub-tasks and validating each step using\navailable information, such as the agent's output and reasoning. Each module\ncontributes to a specific aspect of the evaluation process, and their outputs\nare aggregated to produce a final verdict on task completion. We validate our\nframework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA\nand BigCodeBench. Our Judge Agent predicts task success with closer agreement\nto human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,\nrespectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This\ndemonstrates the potential of our proposed general-purpose evaluation\nframework.",
      "pdf_url": "http://arxiv.org/pdf/2508.05508v1",
      "published": "2025-08-07T15:39:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05508v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning",
      "authors": [
        "Ge Chang",
        "Jinbo Su",
        "Jiacheng Liu",
        "Pengfei Yang",
        "Yuhao Shang",
        "Huiwen Zheng",
        "Hongli Ma",
        "Yan Liang",
        "Yuanchun Li",
        "Yunxin Liu"
      ],
      "abstract": "Large Language Models (LLMs) integrated with Retrieval-Augmented Generation\n(RAG) techniques have exhibited remarkable performance across a wide range of\ndomains. However, existing RAG approaches primarily operate on unstructured\ndata and demonstrate limited capability in handling structured knowledge such\nas knowledge graphs. Meanwhile, current graph retrieval methods fundamentally\nstruggle to capture holistic graph structures while simultaneously facing\nprecision control challenges that manifest as either critical information gaps\nor excessive redundant connections, collectively undermining reasoning\nperformance. To address this challenge, we propose GRAIL: Graph-Retrieval\nAugmented Interactive Learning, a framework designed to interact with\nlarge-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL\nintegrates LLM-guided random exploration with path filtering to establish a\ndata synthesis pipeline, where a fine-grained reasoning trajectory is\nautomatically generated for each task. Based on the synthesized data, we then\nemploy a two-stage training process to learn a policy that dynamically decides\nthe optimal actions at each reasoning step. The overall objective of\nprecision-conciseness balance in graph retrieval is decoupled into fine-grained\nprocess-supervised rewards to enhance data efficiency and training stability.\nIn practical deployment, GRAIL adopts an interactive retrieval paradigm,\nenabling the model to autonomously explore graph paths while dynamically\nbalancing retrieval breadth and precision. Extensive experiments have shown\nthat GRAIL achieves an average accuracy improvement of 21.01% and F1\nimprovement of 22.43% on three knowledge graph question-answering datasets. Our\nsource code and datasets is available at https://github.com/Changgeww/GRAIL.",
      "pdf_url": "http://arxiv.org/pdf/2508.05498v1",
      "published": "2025-08-07T15:34:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05498v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities",
      "authors": [
        "Shuo Cai",
        "Su Lu",
        "Qi Zhou",
        "Kejing Yang",
        "Zhijie Sang",
        "Congkai Xie",
        "Hongxia Yang"
      ],
      "abstract": "Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.",
      "pdf_url": "http://arxiv.org/pdf/2508.05496v1",
      "published": "2025-08-07T15:34:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05496v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling",
      "authors": [
        "Jifan Gao",
        "Mahmudur Rahman",
        "John Caskey",
        "Madeline Oguss",
        "Ann O'Rourke",
        "Randy Brown",
        "Anne Stey",
        "Anoop Mayampurath",
        "Matthew M. Churpek",
        "Guanhua Chen",
        "Majid Afshar"
      ],
      "abstract": "Multimodal electronic health record (EHR) data provide richer, complementary\ninsights into patient health compared to single-modality data. However,\neffectively integrating diverse data modalities for clinical prediction\nmodeling remains challenging due to the substantial data requirements. We\nintroduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed\nto leverage multiple large language model (LLM) agents for clinical prediction\ntasks using multimodal EHR data. MoMA employs specialized LLM agents\n(\"specialist agents\") to convert non-textual modalities, such as medical images\nand laboratory results, into structured textual summaries. These summaries,\ntogether with clinical notes, are combined by another LLM (\"aggregator agent\")\nto generate a unified multimodal summary, which is then used by a third LLM\n(\"predictor agent\") to produce clinical predictions. Evaluating MoMA on three\nprediction tasks using real-world datasets with different modality combinations\nand prediction settings, MoMA outperforms current state-of-the-art methods,\nhighlighting its enhanced accuracy and flexibility across various tasks.",
      "pdf_url": "http://arxiv.org/pdf/2508.05492v1",
      "published": "2025-08-07T15:28:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05492v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?",
      "authors": [
        "Burak Can Kaplan",
        "Hugo Cesar De Castro Carneiro",
        "Stefan Wermter"
      ],
      "abstract": "Emotion recognition in conversations (ERC) focuses on identifying emotion\nshifts within interactions, representing a significant step toward advancing\nmachine intelligence. However, ERC data remains scarce, and existing datasets\nface numerous challenges due to their highly biased sources and the inherent\nsubjectivity of soft labels. Even though Large Language Models (LLMs) have\ndemonstrated their quality in many affective tasks, they are typically\nexpensive to train, and their application to ERC tasks--particularly in data\ngeneration--remains limited. To address these challenges, we employ a small,\nresource-efficient, and general-purpose LLM to synthesize ERC datasets with\ndiverse properties, supplementing the three most widely used ERC benchmarks. We\ngenerate six novel datasets, with two tailored to enhance each benchmark. We\nevaluate the utility of these datasets to (1) supplement existing datasets for\nERC classification, and (2) analyze the effects of label imbalance in ERC. Our\nexperimental results indicate that ERC classifier models trained on the\ngenerated datasets exhibit strong robustness and consistently achieve\nstatistically significant performance improvements on existing ERC benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2508.05474v1",
      "published": "2025-08-07T15:13:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05474v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Embedding Alignment in Code Generation for Audio",
      "authors": [
        "Sam Kouteili",
        "Hiren Madhu",
        "George Typaldos",
        "Mark Santolucito"
      ],
      "abstract": "LLM-powered code generation has the potential to revolutionize creative\ncoding endeavors, such as live-coding, by enabling users to focus on structural\nmotifs over syntactic details. In such domains, when prompting an LLM, users\nmay benefit from considering multiple varied code candidates to better realize\ntheir musical intentions. Code generation models, however, struggle to present\nunique and diverse code candidates, with no direct insight into the code's\naudio output. To better establish a relationship between code candidates and\nproduced audio, we investigate the topology of the mapping between code and\naudio embedding spaces. We find that code and audio embeddings do not exhibit a\nsimple linear relationship, but supplement this with a constructed predictive\nmodel that shows an embedding alignment map could be learned. Supplementing the\naim for musically diverse output, we present a model that given code predicts\noutput audio embedding, constructing a code-audio embedding alignment map.",
      "pdf_url": "http://arxiv.org/pdf/2508.05473v1",
      "published": "2025-08-07T15:13:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05473v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?",
      "authors": [
        "Matteo Prandi",
        "Vincenzo Suriani",
        "Federico Pierucci",
        "Marcello Galisai",
        "Daniele Nardi",
        "Piercosma Bisconti"
      ],
      "abstract": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem is overwhelmingly focused on a narrow set of behavioral propensities,\nsuch as \"Tendency to hallucinate\" (53.7% of the corpus) and \"Discriminatory\nbias\" (28.9%), while critical functional capabilities are dangerously\nneglected. Crucially, capabilities central to loss-of-control scenarios,\nincluding evading human oversight, self-replication, and autonomous AI\ndevelopment, receive zero coverage in the entire benchmark corpus. This\ntranslates to a near-total evaluation gap for systemic risks like \"Loss of\nControl\" (0.4% coverage) and \"Cyber Offence\" (0.8% coverage). This study\nprovides the first comprehensive, quantitative analysis of this gap, offering\ncritical insights for policymakers to refine the CoP and for developers to\nbuild the next generation of evaluation tools, ultimately fostering safer and\nmore compliant AI.",
      "pdf_url": "http://arxiv.org/pdf/2508.05464v1",
      "published": "2025-08-07T15:03:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05464v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Task complexity shapes internal representations and robustness in neural networks",
      "authors": [
        "Robert Jankowski",
        "Filippo Radicchi",
        "M. ngeles Serrano",
        "Marin Bogu",
        "Santo Fortunato"
      ],
      "abstract": "Neural networks excel across a wide range of tasks, yet remain black boxes.\nIn particular, how their internal representations are shaped by the complexity\nof the input data and the problems they solve remains obscure. In this work, we\nintroduce a suite of five data-agnostic probes-pruning, binarization, noise\ninjection, sign flipping, and bipartite network randomization-to quantify how\ntask difficulty influences the topology and robustness of representations in\nmultilayer perceptrons (MLPs). MLPs are represented as signed, weighted\nbipartite graphs from a network science perspective. We contrast easy and hard\nclassification tasks on the MNIST and Fashion-MNIST datasets. We show that\nbinarizing weights in hard-task models collapses accuracy to chance, whereas\neasy-task models remain robust. We also find that pruning low-magnitude edges\nin binarized hard-task models reveals a sharp phase-transition in performance.\nMoreover, moderate noise injection can enhance accuracy, resembling a\nstochastic-resonance effect linked to optimal sign flips of small-magnitude\nweights. Finally, preserving only the sign structure-instead of precise weight\nmagnitudes-through bipartite network randomizations suffices to maintain high\naccuracy. These phenomena define a model- and modality-agnostic measure of task\ncomplexity: the performance gap between full-precision and binarized or\nshuffled neural network performance. Our findings highlight the crucial role of\nsigned bipartite topology in learned representations and suggest practical\nstrategies for model compression and interpretability that align with task\ncomplexity.",
      "pdf_url": "http://arxiv.org/pdf/2508.05463v1",
      "published": "2025-08-07T15:02:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05463v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting",
      "authors": [
        "Wei Li",
        "Zixin Wang",
        "Qizheng Sun",
        "Qixiang Gao",
        "Fenglei Yang"
      ],
      "abstract": "Accurate and reliable energy time series prediction is of great significance\nfor power generation planning and allocation. At present, deep learning time\nseries prediction has become the mainstream method. However, the multi-scale\ntime dynamics and the irregularity of real data lead to the limitations of the\nexisting methods. Therefore, we propose EnergyPatchTST, which is an extension\nof the Patch Time Series Transformer specially designed for energy forecasting.\nThe main innovations of our method are as follows: (1) multi-scale feature\nextraction mechanism to capture patterns with different time resolutions; (2)\nprobability prediction framework to estimate uncertainty through Monte Carlo\nelimination; (3) integration path of future known variables (such as\ntemperature and wind conditions); And (4) Pre-training and Fine-tuning examples\nto enhance the performance of limited energy data sets. A series of experiments\non common energy data sets show that EnergyPatchTST is superior to other\ncommonly used methods, the prediction error is reduced by 7-12%, and reliable\nuncertainty estimation is provided, which provides an important reference for\ntime series prediction in the energy field.",
      "pdf_url": "http://arxiv.org/pdf/2508.05454v1",
      "published": "2025-08-07T14:48:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05454v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees",
      "authors": [
        "Zuyuan Zhang",
        "Arnob Ghosh",
        "Tian Lan"
      ],
      "abstract": "Making decisions with respect to just the expected returns in Monte Carlo\nTree Search (MCTS) cannot account for the potential range of high-risk, adverse\noutcomes associated with a decision. To this end, safety-aware MCTS often\nconsider some constrained variants -- by introducing some form of mean risk\nmeasures or hard cost thresholds. These approaches fail to provide rigorous\ntail-safety guarantees with respect to extreme or high-risk outcomes (denoted\nas tail-risk), potentially resulting in serious consequence in high-stake\nscenarios. This paper addresses the problem by developing two novel solutions.\nWe first propose CVaR-MCTS, which embeds a coherent tail risk measure,\nConditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter\n$\\alpha$ achieves explicit tail-risk control over the expected loss in the\n\"worst $(1-\\alpha)\\%$ scenarios.\" Second, we further address the estimation\nbias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or\nW-MCTS) by introducing a first-order Wasserstein ambiguity set\n$\\mathcal{P}_{\\varepsilon_{s}}(s,a)$ with radius $\\varepsilon_{s}$ to\ncharacterize the uncertainty in tail-risk estimates. We prove PAC tail-safety\nguarantees for both CVaR-MCTS and W-MCTS and establish their regret.\nEvaluations on diverse simulated environments demonstrate that our proposed\nmethods outperform existing baselines, effectively achieving robust tail-risk\nguarantees with improved rewards and stability.",
      "pdf_url": "http://arxiv.org/pdf/2508.05441v1",
      "published": "2025-08-07T14:31:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05441v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI",
      "authors": [
        "Krzysztof Janowicz",
        "Zilong Liu",
        "Gengchen Mai",
        "Zhangyu Wang",
        "Ivan Majic",
        "Alexandra Fortacz",
        "Grant McKenzie",
        "Song Gao"
      ],
      "abstract": "AI (super) alignment describes the challenge of ensuring (future) AI systems\nbehave in accordance with societal norms and goals. While a quickly evolving\nliterature is addressing biases and inequalities, the geographic variability of\nalignment remains underexplored. Simply put, what is considered appropriate,\ntruthful, or legal can differ widely across regions due to cultural norms,\npolitical realities, and legislation. Alignment measures applied to AI/ML\nworkflows can sometimes produce outcomes that diverge from statistical\nrealities, such as text-to-image models depicting balanced gender ratios in\ncompany leadership despite existing imbalances. Crucially, some model outputs\nare globally acceptable, while others, e.g., questions about Kashmir, depend on\nknowing the user's location and their context. This geographic sensitivity is\nnot new. For instance, Google Maps renders Kashmir's borders differently based\non user location. What is new is the unprecedented scale and automation with\nwhich AI now mediates knowledge, expresses opinions, and represents geographic\nreality to millions of users worldwide, often with little transparency about\nhow context is managed. As we approach Agentic AI, the need for\nspatio-temporally aware alignment, rather than one-size-fits-all approaches, is\nincreasingly urgent. This paper reviews key geographic research problems,\nsuggests topics for future work, and outlines methods for assessing alignment\nsensitivity.",
      "pdf_url": "http://arxiv.org/pdf/2508.05432v1",
      "published": "2025-08-07T14:21:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05432v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions",
      "authors": [
        "Hubert Baniecki",
        "Maximilian Muschalik",
        "Fabian Fumagalli",
        "Barbara Hammer",
        "Eyke Hllermeier",
        "Przemyslaw Biecek"
      ],
      "abstract": "Language-image pre-training (LIP) enables the development of vision-language\nmodels capable of zero-shot classification, localization, multimodal retrieval,\nand semantic understanding. Various explanation methods have been proposed to\nvisualize the importance of input image-text pairs on the model's similarity\noutputs. However, popular saliency maps are limited by capturing only\nfirst-order attributions, overlooking the complex cross-modal interactions\nintrinsic to such encoders. We introduce faithful interaction explanations of\nLIP models (FIxLIP) as a unified approach to decomposing the similarity in\nvision-language encoders. FIxLIP is rooted in game theory, where we analyze how\nusing the weighted Banzhaf interaction index offers greater flexibility and\nimproves computational efficiency over the Shapley interaction quantification\nframework. From a practical perspective, we propose how to naturally extend\nexplanation evaluation metrics, like the pointing game and area between the\ninsertion/deletion curves, to second-order interaction explanations.\nExperiments on MS COCO and ImageNet-1k benchmarks validate that second-order\nmethods like FIxLIP outperform first-order attribution methods. Beyond\ndelivering high-quality explanations, we demonstrate the utility of FIxLIP in\ncomparing different models like CLIP vs. SigLIP-2 and ViT-B/32 vs. ViT-L/16.",
      "pdf_url": "http://arxiv.org/pdf/2508.05430v1",
      "published": "2025-08-07T14:18:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05430v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints",
      "authors": [
        "Zhong Ken Hew",
        "Jia Xin Low",
        "Sze Jue Yang",
        "Chee Seng chan"
      ],
      "abstract": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2508.05429v1",
      "published": "2025-08-07T14:17:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05429v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation",
      "authors": [
        "Kartar Kumar Lohana Tharwani",
        "Rajesh Kumar",
        "Sumita",
        "Numan Ahmed",
        "Yong Tang"
      ],
      "abstract": "Large language models (LLMs) are beginning to reshape how chemists plan and\nrun reactions in organic synthesis. Trained on millions of reported\ntransformations, these text-based models can propose synthetic routes, forecast\nreaction outcomes and even instruct robots that execute experiments without\nhuman supervision. Here we survey the milestones that turned LLMs from\nspeculative tools into practical lab partners. We show how coupling LLMs with\ngraph neural networks, quantum calculations and real-time spectroscopy shrinks\ndiscovery cycles and supports greener, data-driven chemistry. We discuss\nlimitations, including biased datasets, opaque reasoning and the need for\nsafety gates that prevent unintentional hazards. Finally, we outline community\ninitiatives open benchmarks, federated learning and explainable interfaces that\naim to democratize access while keeping humans firmly in control. These\nadvances chart a path towards rapid, reliable and inclusive molecular\ninnovation powered by artificial intelligence and automation.",
      "pdf_url": "http://arxiv.org/pdf/2508.05427v1",
      "published": "2025-08-07T14:17:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05427v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LLM-based Multi-Agent Copilot for Quantum Sensor",
      "authors": [
        "Rong Sha",
        "Binglin Wang",
        "Jun Yang",
        "Xiaoxiao Ma",
        "Chengkun Wu",
        "Liang Yan",
        "Chao Zhou",
        "Jixun Liu",
        "Guochao Wang",
        "Shuhua Yan",
        "Lingxiao Zhu"
      ],
      "abstract": "Large language models (LLM) exhibit broad utility but face limitations in\nquantum sensor development, stemming from interdisciplinary knowledge barriers\nand involving complex optimization processes. Here we present QCopilot, an\nLLM-based multi-agent framework integrating external knowledge access, active\nlearning, and uncertainty quantification for quantum sensor design and\ndiagnosis. Comprising commercial LLMs with few-shot prompt engineering and\nvector knowledge base, QCopilot employs specialized agents to adaptively select\noptimization methods, automate modeling analysis, and independently perform\nproblem diagnosis. Applying QCopilot to atom cooling experiments, we generated\n10${}^{\\rm{8}}$ sub-$\\rm{\\mu}$K atoms without any human intervention within a\nfew hours, representing $\\sim$100$\\times$ speedup over manual experimentation.\nNotably, by continuously accumulating prior knowledge and enabling dynamic\nmodeling, QCopilot can autonomously identify anomalous parameters in\nmulti-parameter experimental settings. Our work reduces barriers to large-scale\nquantum sensor deployment and readily extends to other quantum information\nsystems.",
      "pdf_url": "http://arxiv.org/pdf/2508.05421v1",
      "published": "2025-08-07T14:14:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05421v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "physics.atom-ph"
      ]
    },
    {
      "title": "DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning",
      "authors": [
        "Xinrun Xu",
        "Pi Bu",
        "Ye Wang",
        "Brje F. Karlsson",
        "Ziming Wang",
        "Tengtao Song",
        "Qi Zhu",
        "Jun Song",
        "Zhiming Ding",
        "Bo Zheng"
      ],
      "abstract": "Although Vision Language Models (VLMs) exhibit strong perceptual abilities\nand impressive visual reasoning, they struggle with attention to detail and\nprecise action planning in complex, dynamic environments, leading to subpar\nperformance. Real-world tasks typically require complex interactions, advanced\nspatial reasoning, long-term planning, and continuous strategy refinement,\nusually necessitating understanding the physics rules of the target scenario.\nHowever, evaluating these capabilities in real-world scenarios is often\nprohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel\nbenchmark framework designed to systematically evaluate VLMs' understanding and\nreasoning about fundamental physical principles through a series of challenging\nsimulated environments. DeepPHY integrates multiple physical reasoning\nenvironments of varying difficulty levels and incorporates fine-grained\nevaluation metrics. Our evaluation finds that even state-of-the-art VLMs\nstruggle to translate descriptive physical knowledge into precise, predictive\ncontrol.",
      "pdf_url": "http://arxiv.org/pdf/2508.05405v1",
      "published": "2025-08-07T13:58:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05405v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation",
      "authors": [
        "Wonjun Kang",
        "Byeongkeun Ahn",
        "Minjae Lee",
        "Kevin Galim",
        "Seunghyuk Oh",
        "Hyung Il Koo",
        "Nam Ik Cho"
      ],
      "abstract": "Text-to-image (T2I) generation has been actively studied using Diffusion\nModels and Autoregressive Models. Recently, Masked Generative Transformers have\ngained attention as an alternative to Autoregressive Models to overcome the\ninherent limitations of causal attention and autoregressive decoding through\nbidirectional attention and parallel decoding, enabling efficient and\nhigh-quality image generation. However, compositional T2I generation remains\nchallenging, as even state-of-the-art Diffusion Models often fail to accurately\nbind attributes and achieve proper text-image alignment. While Diffusion Models\nhave been extensively studied for this issue, Masked Generative Transformers\nexhibit similar limitations but have not been explored in this context. To\naddress this, we propose Unmasking with Contrastive Attention Guidance\n(UNCAGE), a novel training-free method that improves compositional fidelity by\nleveraging attention maps to prioritize the unmasking of tokens that clearly\nrepresent individual objects. UNCAGE consistently improves performance in both\nquantitative and qualitative evaluations across multiple benchmarks and\nmetrics, with negligible inference overhead. Our code is available at\nhttps://github.com/furiosa-ai/uncage.",
      "pdf_url": "http://arxiv.org/pdf/2508.05399v1",
      "published": "2025-08-07T13:51:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05399v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Real-Time Iteration Scheme for Diffusion Policy",
      "authors": [
        "Yufei Duan",
        "Hang Yin",
        "Danica Kragic"
      ],
      "abstract": "Diffusion Policies have demonstrated impressive performance in robotic\nmanipulation tasks. However, their long inference time, resulting from an\nextensive iterative denoising process, and the need to execute an action chunk\nbefore the next prediction to maintain consistent actions limit their\napplicability to latency-critical tasks or simple tasks with a short cycle\ntime. While recent methods explored distillation or alternative policy\nstructures to accelerate inference, these often demand additional training,\nwhich can be resource-intensive for large robotic models. In this paper, we\nintroduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a\nmethod from optimal control that accelerates optimization by leveraging\nsolutions from previous time steps as initial guesses for subsequent\niterations. We explore the application of this scheme in diffusion inference\nand propose a scaling-based method to effectively handle discrete actions, such\nas grasping, in robotic manipulation. The proposed scheme significantly reduces\nruntime computational costs without the need for distillation or policy\nredesign. This enables a seamless integration into many pre-trained\ndiffusion-based models, in particular, to resource-demanding large models. We\nalso provide theoretical conditions for the contractivity which could be useful\nfor estimating the initial denoising step. Quantitative results from extensive\nsimulation experiments show a substantial reduction in inference time, with\ncomparable overall performance compared with Diffusion Policy using full-step\ndenoising. Our project page with additional resources is available at:\nhttps://rti-dp.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2508.05396v1",
      "published": "2025-08-07T13:49:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05396v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "An Explainable Machine Learning Framework for Railway Predictive Maintenance using Data Streams from the Metro Operator of Portugal",
      "authors": [
        "Silvia Garca-Mndez",
        "Francisco de Arriba-Prez",
        "Ftima Leal",
        "Bruno Veloso",
        "Benedita Malheiro",
        "Juan Carlos Burguillo-Rial"
      ],
      "abstract": "This work contributes to a real-time data-driven predictive maintenance\nsolution for Intelligent Transportation Systems. The proposed method implements\na processing pipeline comprised of sample pre-processing, incremental\nclassification with Machine Learning models, and outcome explanation. This\nnovel online processing pipeline has two main highlights: (i) a dedicated\nsample pre-processing module, which builds statistical and frequency-related\nfeatures on the fly, and (ii) an explainability module. This work is the first\nto perform online fault prediction with natural language and visual\nexplainability. The experiments were performed with the MetroPT data set from\nthe metro operator of Porto, Portugal. The results are above 98 % for F-measure\nand 99 % for accuracy. In the context of railway predictive maintenance,\nachieving these high values is crucial due to the practical and operational\nimplications of accurate failure prediction. In the specific case of a high\nF-measure, this ensures that the system maintains an optimal balance between\ndetecting the highest possible number of real faults and minimizing false\nalarms, which is crucial for maximizing service availability. Furthermore, the\naccuracy obtained enables reliability, directly impacting cost reduction and\nincreased safety. The analysis demonstrates that the pipeline maintains high\nperformance even in the presence of class imbalance and noise, and its\nexplanations effectively reflect the decision-making process. These findings\nvalidate the methodological soundness of the approach and confirm its practical\napplicability for supporting proactive maintenance decisions in real-world\nrailway operations. Therefore, by identifying the early signs of failure, this\npipeline enables decision-makers to understand the underlying problems and act\naccordingly swiftly.",
      "pdf_url": "http://arxiv.org/pdf/2508.05388v1",
      "published": "2025-08-07T13:38:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05388v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms",
      "authors": [
        "Jie Xiao",
        "Shaoduo Gan",
        "Changyuan Fan",
        "Qingnan Ren",
        "Alfred Long",
        "Yuchen Zhang",
        "Rymon Yu",
        "Eric Yang",
        "Lynn Ai"
      ],
      "abstract": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\nsampler weights on every API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training three representative RL workloads with\nQwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,\nEcho matches a fully co-located Verl baseline in convergence speed and final\nreward while off-loading trajectory generation to commodity edge hardware.\nThese promising results demonstrate that large-scale RL for LLMs could achieve\ndatacentre-grade performance using decentralised, heterogeneous resources.",
      "pdf_url": "http://arxiv.org/pdf/2508.05387v1",
      "published": "2025-08-07T13:37:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05387v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models",
      "authors": [
        "Xiangxiang Zhang",
        "Jingxuan Wei",
        "Donghong Zhong",
        "Qi Chen",
        "Caijun Jia",
        "Cheng Tan",
        "Jinming Gu",
        "Xiaobo Qin",
        "Zhiping Liu",
        "Liang Hu",
        "Tong Sun",
        "Yuchen Wu",
        "Zewei Sun",
        "Chenwei Lou",
        "Hua Zheng",
        "Tianyang Zhan",
        "Changbao Wang",
        "Shuangzhi Wu",
        "Zefa Lin",
        "Chang Guo",
        "Sihang Yuan",
        "Riwei Chen",
        "Shixiong Zhao",
        "Yingping Zhang",
        "Gaowei Wu",
        "Bihui Yu",
        "Jiahui Wu",
        "Zhehui Zhao",
        "Qianqian Liu",
        "Ruofeng Tang",
        "Xingyue Huang",
        "Bing Zhao",
        "Mengyang Zhang",
        "Youqiang Zhou"
      ],
      "abstract": "Existing Vision-Language Models often struggle with complex, multi-question\nreasoning tasks where partial correctness is crucial for effective learning.\nTraditional reward mechanisms, which provide a single binary score for an\nentire response, are too coarse to guide models through intricate problems with\nmultiple sub-parts. To address this, we introduce StructVRM, a method that\naligns multimodal reasoning with Structured and Verifiable Reward Models. At\nits core is a model-based verifier trained to provide fine-grained,\nsub-question-level feedback, assessing semantic and mathematical equivalence\nrather than relying on rigid string matching. This allows for nuanced, partial\ncredit scoring in previously intractable problem formats. Extensive experiments\ndemonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM,\nachieves state-of-the-art performance on six out of twelve public multimodal\nbenchmarks and our newly curated, high-difficulty STEM-Bench. The success of\nStructVRM validates that training with structured, verifiable rewards is a\nhighly effective approach for advancing the capabilities of multimodal models\nin complex, real-world reasoning domains.",
      "pdf_url": "http://arxiv.org/pdf/2508.05383v1",
      "published": "2025-08-07T13:31:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05383v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Optimal Corpus Aware Training for Neural Machine Translation",
      "authors": [
        "Yi-Hsiu Liao",
        "Cheng Shen",
        "Brenda",
        "Yang"
      ],
      "abstract": "Corpus Aware Training (CAT) leverages valuable corpus metadata during\ntraining by injecting corpus information into each training example, and has\nbeen found effective in the literature, commonly known as the \"tagging\"\napproach. Models trained with CAT inherently learn the quality, domain and\nnuance between corpora directly from data, and can easily switch to different\ninference behavior. To achieve the best evaluation, CAT models pre-define a\ngroup of high quality data before training starts which can be error-prone and\ninefficient. In this work, we propose Optimal Corpus Aware Training (OCAT),\nwhich fine-tunes a CAT pre-trained model by freezing most of the model\nparameters and only tuning small set of corpus-related parameters. We show that\nOCAT is lightweight, resilient to overfitting, and effective in boosting model\naccuracy. We use WMT23 English to Chinese and English to German translation\ntasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively,\nover vanilla training. Furthermore, our approach is on-par or slightly better\nthan other state-of-the-art fine-tuning techniques while being less sensitive\nto hyperparameter settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.05364v1",
      "published": "2025-08-07T13:12:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05364v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Building Effective Safety Guardrails in AI Education Tools",
      "authors": [
        "Hannah-Beth Clark",
        "Laura Benton",
        "Emma Searle",
        "Margaux Dowland",
        "Matthew Gregory",
        "Will Gayne",
        "John Roberts"
      ],
      "abstract": "There has been rapid development in generative AI tools across the education\nsector, which in turn is leading to increased adoption by teachers. However,\nthis raises concerns regarding the safety and age-appropriateness of the\nAI-generated content that is being created for use in classrooms. This paper\nexplores Oak National Academy's approach to addressing these concerns within\nthe development of the UK Government's first publicly available generative AI\ntool - our AI-powered lesson planning assistant (Aila). Aila is intended to\nsupport teachers planning national curriculum-aligned lessons that are\nappropriate for pupils aged 5-16 years. To mitigate safety risks associated\nwith AI-generated content we have implemented four key safety guardrails - (1)\nprompt engineering to ensure AI outputs are generated within pedagogically\nsound and curriculum-aligned parameters, (2) input threat detection to mitigate\nattacks, (3) an Independent Asynchronous Content Moderation Agent (IACMA) to\nassess outputs against predefined safety categories, and (4) taking a\nhuman-in-the-loop approach, to encourage teachers to review generated content\nbefore it is used in the classroom. Through our on-going evaluation of these\nsafety guardrails we have identified several challenges and opportunities to\ntake into account when implementing and testing safety guardrails. This paper\nhighlights ways to build more effective safety guardrails in generative AI\neducation tools including the on-going iteration and refinement of guardrails,\nas well as enabling cross-sector collaboration through sharing both open-source\ncode, datasets and learnings.",
      "pdf_url": "http://arxiv.org/pdf/2508.05360v1",
      "published": "2025-08-07T13:09:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05360v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation",
      "authors": [
        "Kang Liu",
        "Zhuoqi Ma",
        "Zikang Fang",
        "Yunan Li",
        "Kun Xie",
        "Qiguang Miao"
      ],
      "abstract": "Chest X-ray report generation aims to reduce radiologists' workload by\nautomatically producing high-quality preliminary reports. A critical yet\nunderexplored aspect of this task is the effective use of patient-specific\nprior knowledge -- including clinical context (e.g., symptoms, medical history)\nand the most recent prior image -- which radiologists routinely rely on for\ndiagnostic reasoning. Most existing methods generate reports from single\nimages, neglecting this essential prior information and thus failing to capture\ndiagnostic intent or disease progression. To bridge this gap, we propose\nPriorRG, a novel chest X-ray report generation framework that emulates\nreal-world clinical workflows via a two-stage training pipeline. In Stage 1, we\nintroduce a prior-guided contrastive pre-training scheme that leverages\nclinical context to guide spatiotemporal feature extraction, allowing the model\nto align more closely with the intrinsic spatiotemporal semantics in radiology\nreports. In Stage 2, we present a prior-aware coarse-to-fine decoding for\nreport generation that progressively integrates patient-specific prior\nknowledge with the vision encoder's hidden states. This decoding allows the\nmodel to align with diagnostic focus and track disease progression, thereby\nenhancing the clinical accuracy and fluency of the generated reports. Extensive\nexperiments on MIMIC-CXR and MIMIC-ABN datasets demonstrate that PriorRG\noutperforms state-of-the-art methods, achieving a 3.6% BLEU-4 and 3.8% F1 score\nimprovement on MIMIC-CXR, and a 5.9% BLEU-1 gain on MIMIC-ABN. Code and\ncheckpoints will be released upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2508.05353v1",
      "published": "2025-08-07T13:02:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05353v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising",
      "authors": [
        "Xiaoxi Cui",
        "Weihai Lu",
        "Yu Tong",
        "Yiheng Li",
        "Zhejun Zhao"
      ],
      "abstract": "The sequential recommendation system utilizes historical user interactions to\npredict preferences. Effectively integrating diverse user behavior patterns\nwith rich multimodal information of items to enhance the accuracy of sequential\nrecommendations is an emerging and challenging research direction. This paper\nfocuses on the problem of multi-modal multi-behavior sequential recommendation,\naiming to address the following challenges: (1) the lack of effective\ncharacterization of modal preferences across different behaviors, as user\nattention to different item modalities varies depending on the behavior; (2)\nthe difficulty of effectively mitigating implicit noise in user behavior, such\nas unintended actions like accidental clicks; (3) the inability to handle\nmodality noise in multi-modal representations, which further impacts the\naccurate modeling of user preferences. To tackle these issues, we propose a\nnovel Multi-Modal Multi-Behavior Sequential Recommendation model (M$^3$BSR).\nThis model first removes noise in multi-modal representations using a\nConditional Diffusion Modality Denoising Layer. Subsequently, it utilizes deep\nbehavioral information to guide the denoising of shallow behavioral data,\nthereby alleviating the impact of noise in implicit feedback through\nConditional Diffusion Behavior Denoising. Finally, by introducing a\nMulti-Expert Interest Extraction Layer, M$^3$BSR explicitly models the common\nand specific interests across behaviors and modalities to enhance\nrecommendation performance. Experimental results indicate that M$^3$BSR\nsignificantly outperforms existing state-of-the-art methods on benchmark\ndatasets.",
      "pdf_url": "http://arxiv.org/pdf/2508.05352v1",
      "published": "2025-08-07T12:58:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05352v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Minimal Model Reasoning in Description Logics: Don't Try This at Home!",
      "authors": [
        "Federica Di Stefano",
        "Quentin Manire",
        "Magdalena Ortiz",
        "Mantas imkus"
      ],
      "abstract": "Reasoning with minimal models has always been at the core of many knowledge\nrepresentation techniques, but we still have only a limited understanding of\nthis problem in Description Logics (DLs). Minimization of some selected\npredicates, letting the remaining predicates vary or be fixed, as proposed in\ncircumscription, has been explored and exhibits high complexity. The case of\n`pure' minimal models, where the extension of all predicates must be minimal,\nhas remained largely uncharted. We address this problem in popular DLs and\nobtain surprisingly negative results: concept satisfiability in minimal models\nis undecidable already for $\\mathcal{EL}$. This undecidability also extends to\na very restricted fragment of tuple-generating dependencies. To regain\ndecidability, we impose acyclicity conditions on the TBox that bring the\nworst-case complexity below double exponential time and allow us to establish a\nconnection with the recently studied pointwise circumscription; we also derive\nresults in data complexity. We conclude with a brief excursion to the DL-Lite\nfamily, where a positive result was known for DL-Lite$_{\\text{core}}$, but our\ninvestigation establishes ExpSpace-hardness already for its extension\nDL-Lite$_{\\text{horn}}$.",
      "pdf_url": "http://arxiv.org/pdf/2508.05350v1",
      "published": "2025-08-07T12:56:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05350v1",
      "categories": [
        "cs.AI",
        "cs.CC",
        "cs.LO"
      ]
    },
    {
      "title": "NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making",
      "authors": [
        "Asutosh Hota",
        "Jussi P. P. Jokinen"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have extended their\ncapabilities from basic text processing to complex reasoning tasks, including\nlegal interpretation, argumentation, and strategic interaction. However,\nempirical understanding of LLM behavior in open-ended, multi-agent settings\nespecially those involving deliberation over legal and ethical dilemmas remains\nlimited. We introduce NomicLaw, a structured multi-agent simulation where LLMs\nengage in collaborative law-making, responding to complex legal vignettes by\nproposing rules, justifying them, and voting on peer proposals. We\nquantitatively measure trust and reciprocity via voting patterns and\nqualitatively assess how agents use strategic language to justify proposals and\ninfluence outcomes. Experiments involving homogeneous and heterogeneous LLM\ngroups demonstrate how agents spontaneously form alliances, betray trust, and\nadapt their rhetoric to shape collective decisions. Our results highlight the\nlatent social reasoning and persuasive capabilities of ten open-source LLMs and\nprovide insights into the design of future AI systems capable of autonomous\nnegotiation, coordination and drafting legislation in legal settings.",
      "pdf_url": "http://arxiv.org/pdf/2508.05344v1",
      "published": "2025-08-07T12:49:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05344v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control",
      "authors": [
        "Shunlei Li",
        "Longsen Gao",
        "Jin Wang",
        "Chang Che",
        "Xi Xiao",
        "Jiuwen Cao",
        "Yingbai Hu",
        "Hamid Reza Karimi"
      ],
      "abstract": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations.",
      "pdf_url": "http://arxiv.org/pdf/2508.05342v1",
      "published": "2025-08-07T12:48:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05342v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition",
      "authors": [
        "Brinnae Bent"
      ],
      "abstract": "The term 'agent' in artificial intelligence has long carried multiple\ninterpretations across different subfields. Recent developments in AI\ncapabilities, particularly in large language model systems, have amplified this\nambiguity, creating significant challenges in research communication, system\nevaluation and reproducibility, and policy development. This paper argues that\nthe term 'agent' requires redefinition. Drawing from historical analysis and\ncontemporary usage patterns, we propose a framework that defines clear minimum\nrequirements for a system to be considered an agent while characterizing\nsystems along a multidimensional spectrum of environmental interaction,\nlearning and adaptation, autonomy, goal complexity, and temporal coherence.\nThis approach provides precise vocabulary for system description while\npreserving the term's historically multifaceted nature. After examining\npotential counterarguments and implementation challenges, we provide specific\nrecommendations for moving forward as a field, including suggestions for\nterminology standardization and framework adoption. The proposed approach\noffers practical tools for improving research clarity and reproducibility while\nsupporting more effective policy development.",
      "pdf_url": "http://arxiv.org/pdf/2508.05338v1",
      "published": "2025-08-07T12:40:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05338v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression",
      "authors": [
        "Jiameng Huang",
        "Baijiong Lin",
        "Guhao Feng",
        "Jierun Chen",
        "Di He",
        "Lu Hou"
      ],
      "abstract": "Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought\nreasoning with complex reflection behaviors, typically signaled by specific\ntrigger words (e.g., \"Wait\" and \"Alternatively\") to enhance performance.\nHowever, these reflection behaviors can lead to the overthinking problem where\nthe generation of redundant reasoning steps that unnecessarily increase token\nusage, raise inference costs, and reduce practical utility. In this paper, we\npropose Certainty-Guided Reflection Suppression (CGRS), a novel method that\nmitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS\noperates by dynamically suppressing the model's generation of reflection\ntriggers when it exhibits high confidence in its current response, thereby\npreventing redundant reflection cycles without compromising output quality. Our\napproach is model-agnostic, requires no retraining or architectural\nmodifications, and can be integrated seamlessly with existing autoregressive\ngeneration pipelines. Extensive experiments across four reasoning benchmarks\n(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it\nreduces token usage by an average of 18.5% to 41.9% while preserving accuracy.\nIt also achieves the optimal balance between length reduction and performance\ncompared to state-of-the-art baselines. These results hold consistently across\nmodel architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3\nfamily) and scales (4B to 32B parameters), highlighting CGRS's practical value\nfor efficient reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2508.05337v1",
      "published": "2025-08-07T12:38:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05337v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering",
      "authors": [
        "Xu Yuan",
        "Liangbo Ning",
        "Wenqi Fan",
        "Qing Li"
      ],
      "abstract": "Recently, Retrieval-Augmented Generation (RAG) has been proposed to expand\ninternal knowledge of Multimodal Large Language Models (MLLMs) by incorporating\nexternal knowledge databases into the generation process, which is widely used\nfor knowledge-based Visual Question Answering (VQA) tasks. Despite impressive\nadvancements, vanilla RAG-based VQA methods that rely on unstructured documents\nand overlook the structural relationships among knowledge elements frequently\nintroduce irrelevant or misleading content, reducing answer accuracy and\nreliability. To overcome these challenges, a promising solution is to integrate\nmultimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the\ngeneration by introducing structured multimodal knowledge. Therefore, in this\npaper, we propose a novel multimodal knowledge-augmented generation framework\n(mKG-RAG) based on multimodal KGs for knowledge-intensive VQA tasks.\nSpecifically, our approach leverages MLLM-powered keyword extraction and\nvision-text matching to distill semantically consistent and modality-aligned\nentities/relationships from multimodal documents, constructing high-quality\nmultimodal KGs as structured knowledge representations. In addition, a\ndual-stage retrieval strategy equipped with a question-aware multimodal\nretriever is introduced to improve retrieval efficiency while refining\nprecision. Comprehensive experiments demonstrate that our approach\nsignificantly outperforms existing methods, setting a new state-of-the-art for\nknowledge-based VQA.",
      "pdf_url": "http://arxiv.org/pdf/2508.05318v1",
      "published": "2025-08-07T12:22:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.05318v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}