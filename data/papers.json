{
  "last_updated": "2025-10-03T00:46:35.184800",
  "papers": [
    {
      "title": "Stitch: Training-Free Position Control in Multimodal Diffusion Transformers",
      "authors": [
        "Jessica Bader",
        "Mateusz Pach",
        "Maria A. Bravo",
        "Serge Belongie",
        "Zeynep Akata"
      ],
      "abstract": "Text-to-Image (T2I) generation models have advanced rapidly in recent years,\nbut accurately capturing spatial relationships like \"above\" or \"to the right\nof\" poses a persistent challenge. Earlier methods improved spatial relationship\nfollowing with external position control. However, as architectures evolved to\nenhance image quality, these techniques became incompatible with modern models.\nWe propose Stitch, a training-free method for incorporating external position\ncontrol into Multi-Modal Diffusion Transformers (MMDiT) via\nautomatically-generated bounding boxes. Stitch produces images that are both\nspatially accurate and visually appealing by generating individual objects\nwithin designated bounding boxes and seamlessly stitching them together. We\nfind that targeted attention heads capture the information necessary to isolate\nand cut out individual objects mid-generation, without needing to fully\ncomplete the image. We evaluate Stitch on PosEval, our benchmark for\nposition-based T2I generation. Featuring five new tasks that extend the concept\nof Position beyond the basic GenEval task, PosEval demonstrates that even top\nmodels still have significant room for improvement in position-based\ngeneration. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances\nbase models, even improving FLUX by 218% on GenEval's Position task and by 206%\non PosEval. Stitch achieves state-of-the-art results with Qwen-Image on\nPosEval, improving over previous models by 54%, all accomplished while\nintegrating position control into leading models training-free. Code is\navailable at https://github.com/ExplainableML/Stitch.",
      "pdf_url": "http://arxiv.org/pdf/2509.26644v1",
      "published": "2025-09-30T17:59:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26644v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction",
      "authors": [
        "Lujie Yang",
        "Xiaoyu Huang",
        "Zhen Wu",
        "Angjoo Kanazawa",
        "Pieter Abbeel",
        "Carmelo Sferrazza",
        "C. Karen Liu",
        "Rocky Duan",
        "Guanya Shi"
      ],
      "abstract": "A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.",
      "pdf_url": "http://arxiv.org/pdf/2509.26633v1",
      "published": "2025-09-30T17:59:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26633v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Branching Out: Broadening AI Measurement and Evaluation with Measurement Trees",
      "authors": [
        "Craig Greenberg",
        "Patrick Hall",
        "Theodore Jensen",
        "Kristen Greene",
        "Razvan Amironesei"
      ],
      "abstract": "This paper introduces \\textit{measurement trees}, a novel class of metrics\ndesigned to combine various constructs into an interpretable multi-level\nrepresentation of a measurand. Unlike conventional metrics that yield single\nvalues, vectors, surfaces, or categories, measurement trees produce a\nhierarchical directed graph in which each node summarizes its children through\nuser-defined aggregation methods. In response to recent calls to expand the\nscope of AI system evaluation, measurement trees enhance metric transparency\nand facilitate the integration of heterogeneous evidence, including, e.g.,\nagentic, business, energy-efficiency, sociotechnical, or security signals. We\npresent definitions and examples, demonstrate practical utility through a\nlarge-scale measurement exercise, and provide accompanying open-source Python\ncode. By operationalizing a transparent approach to measurement of complex\nconstructs, this work offers a principled foundation for broader and more\ninterpretable AI evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2509.26632v1",
      "published": "2025-09-30T17:58:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26632v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Learning Generalizable Shape Completion with SIM(3) Equivariance",
      "authors": [
        "Yuqing Wang",
        "Zhaiyu Chen",
        "Xiao Xiang Zhu"
      ],
      "abstract": "3D shape completion methods typically assume scans are pre-aligned to a\ncanonical frame. This leaks pose and scale cues that networks may exploit to\nmemorize absolute positions rather than inferring intrinsic geometry. When such\nalignment is absent in real data, performance collapses. We argue that robust\ngeneralization demands architectural equivariance to the similarity group,\nSIM(3), so the model remains agnostic to pose and scale. Following this\nprinciple, we introduce the first SIM(3)-equivariant shape completion network,\nwhose modular layers successively canonicalize features, reason over\nsimilarity-invariant geometry, and restore the original frame. Under a\nde-biased evaluation protocol that removes the hidden cues, our model\noutperforms both equivariant and augmentation baselines on the PCN benchmark.\nIt also sets new cross-domain records on real driving and indoor scans,\nlowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$\non OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol\nstill outperforms competitors under their biased settings. These results\nestablish full SIM(3) equivariance as an effective route to truly generalizable\nshape completion. Project page: https://sime-completion.github.io.",
      "pdf_url": "http://arxiv.org/pdf/2509.26631v1",
      "published": "2025-09-30T17:58:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26631v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance",
      "authors": [
        "Yuyang Liu",
        "Chuan Wen",
        "Yihang Hu",
        "Dinesh Jayaraman",
        "Yang Gao"
      ],
      "abstract": "Designing dense rewards is crucial for reinforcement learning (RL), yet in\nrobotics it often demands extensive manual effort and lacks scalability. One\npromising solution is to view task progress as a dense reward signal, as it\nquantifies the degree to which actions advance the system toward task\ncompletion over time. We present TimeRewarder, a simple yet effective reward\nlearning method that derives progress estimation signals from passive videos,\nincluding robot demonstrations and human videos, by modeling temporal distances\nbetween frame pairs. We then demonstrate how TimeRewarder can supply step-wise\nproxy rewards to guide reinforcement learning. In our comprehensive experiments\non ten challenging Meta-World tasks, we show that TimeRewarder dramatically\nimproves RL for sparse-reward tasks, achieving nearly perfect success in 9/10\ntasks with only 200,000 interactions per task with the environment. This\napproach outperformed previous methods and even the manually designed\nenvironment dense reward on both the final success rate and sample efficiency.\nMoreover, we show that TimeRewarder pretraining can exploit real-world human\nvideos, highlighting its potential as a scalable approach path to rich reward\nsignals from diverse video sources.",
      "pdf_url": "http://arxiv.org/pdf/2509.26627v1",
      "published": "2025-09-30T17:58:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26627v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training",
      "authors": [
        "Junlin Han",
        "Shengbang Tong",
        "David Fan",
        "Yufan Ren",
        "Koustuv Sinha",
        "Philip Torr",
        "Filippos Kokkinos"
      ],
      "abstract": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.26625v1",
      "published": "2025-09-30T17:57:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26625v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ]
    },
    {
      "title": "Searching for Difficult-to-Translate Test Examples at Scale",
      "authors": [
        "Wenda Xu",
        "Vilém Zouhar",
        "Parker Riley",
        "Mara Finkelstein",
        "Markus Freitag",
        "Daniel Deutsch"
      ],
      "abstract": "NLP models require test data that are sufficiently challenging. The\ndifficulty of an example is linked to the topic it originates from (''seed\ntopic''). The relationship between the topic and the difficulty of its\ninstances is stochastic in nature: an example about a difficult topic can\nhappen to be easy, and vice versa. At the scale of the Internet, there are tens\nof thousands of potential topics, and finding the most difficult one by drawing\nand evaluating a large number of examples across all topics is computationally\ninfeasible. We formalize this task and treat it as a multi-armed bandit\nproblem. In this framework, each topic is an ''arm,'' and pulling an arm (at a\ncost) involves drawing a single example, evaluating it, and measuring its\ndifficulty. The goal is to efficiently identify the most difficult topics\nwithin a fixed computational budget. We illustrate the bandit problem setup of\nfinding difficult examples for the task of machine translation. We find that\nvarious bandit strategies vastly outperform baseline methods like brute-force\nsearching the most challenging topics.",
      "pdf_url": "http://arxiv.org/pdf/2509.26619v1",
      "published": "2025-09-30T17:55:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26619v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning",
      "authors": [
        "Maël Macuglia",
        "Paul Friedrich",
        "Giorgia Ramponi"
      ],
      "abstract": "Deploying reinforcement learning (RL) in robotics, industry, and health care\nis blocked by two obstacles: the difficulty of specifying accurate rewards and\nthe risk of unsafe, data-hungry exploration. We address this by proposing a\ntwo-stage framework that first learns a safe initial policy from a reward-free\ndataset of expert demonstrations, then fine-tunes it online using\npreference-based human feedback. We provide the first principled analysis of\nthis offline-to-online approach and introduce BRIDGE, a unified algorithm that\nintegrates both signals via an uncertainty-weighted objective. We derive regret\nbounds that shrink with the number of offline demonstrations, explicitly\nconnecting the quantity of offline data to online sample efficiency. We\nvalidate BRIDGE in discrete and continuous control MuJoCo environments, showing\nit achieves lower regret than both standalone behavioral cloning and online\npreference-based RL. Our work establishes a theoretical foundation for\ndesigning more sample-efficient interactive agents.",
      "pdf_url": "http://arxiv.org/pdf/2509.26605v1",
      "published": "2025-09-30T17:50:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26605v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages",
      "authors": [
        "Chenxi Whitehouse",
        "Sebastian Ruder",
        "Tony Lin",
        "Oksana Kurylo",
        "Haruka Takagi",
        "Janice Lam",
        "Nicolò Busetto",
        "Denise Diaz"
      ],
      "abstract": "Ensuring native-like quality of large language model (LLM) responses across\nmany languages is challenging. To address this, we introduce MENLO, a framework\nthat operationalizes the evaluation of native-like response quality based on\naudience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423\nhuman-annotated prompt-response preference pairs covering four quality\ndimensions with high inter-annotator agreement in 47 language varieties. Our\nevaluation reveals that zero-shot LLM judges benefit significantly from\npairwise evaluation and our structured annotation rubrics, yet they still\nunderperform human annotators on our dataset. We demonstrate substantial\nimprovements through fine-tuning with reinforcement learning, reward shaping,\nand multi-task learning approaches. Additionally, we show that RL-trained\njudges can serve as generative reward models to enhance LLMs' multilingual\nproficiency, though discrepancies with human judgment remain. Our findings\nsuggest promising directions for scalable multilingual evaluation and\npreference alignment. We release our dataset and evaluation framework to\nsupport further research in multilingual LLM evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2509.26601v1",
      "published": "2025-09-30T17:48:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26601v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks",
      "authors": [
        "Wenda Xu",
        "Sweta Agrawal",
        "Vilém Zouhar",
        "Markus Freitag",
        "Daniel Deutsch"
      ],
      "abstract": "As large language models (LLMs) begin to saturate existing benchmarks,\nautomated benchmark creation using LLMs (LLM as a benchmark) has emerged as a\nscalable alternative to slow and costly human curation. While these generated\ntest sets have to potential to cheaply rank models, we demonstrate a critical\nflaw. LLM generated benchmarks systematically favor the model that created the\nbenchmark, they exhibit self bias on low resource languages to English\ntranslation tasks. We show three key findings on automatic benchmarking of LLMs\nfor translation: First, this bias originates from two sources: the generated\ntest data (LLM as a testset) and the evaluation method (LLM as an evaluator),\nwith their combination amplifying the effect. Second, self bias in LLM as a\nbenchmark is heavily influenced by the model's generation capabilities in the\nsource language. For instance, we observe more pronounced bias in into English\ntranslation, where the model's generation system is developed, than in out of\nEnglish translation tasks. Third, we observe that low diversity in source text\nis one attribution to self bias. Our results suggest that improving the\ndiversity of these generated source texts can mitigate some of the observed\nself bias.",
      "pdf_url": "http://arxiv.org/pdf/2509.26600v1",
      "published": "2025-09-30T17:48:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26600v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Are Robust LLM Fingerprints Adversarially Robust?",
      "authors": [
        "Anshul Nasery",
        "Edoardo Contente",
        "Alkin Kaz",
        "Pramod Viswanath",
        "Sewoong Oh"
      ],
      "abstract": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods.",
      "pdf_url": "http://arxiv.org/pdf/2509.26598v1",
      "published": "2025-09-30T17:47:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26598v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models",
      "authors": [
        "Matheus Vinicius da Silva de Oliveira",
        "Jonathan de Andrade Silva",
        "Awdren de Lima Fontao"
      ],
      "abstract": "Large Language Models (LLMs) are widely used across multiple domains but\ncontinue to raise concerns regarding security and fairness. Beyond known attack\nvectors such as data poisoning and prompt injection, LLMs are also vulnerable\nto fairness bugs. These refer to unintended behaviors influenced by sensitive\ndemographic cues (e.g., race or sexual orientation) that should not affect\noutcomes. Another key issue is hallucination, where models generate plausible\nyet false information. Retrieval-Augmented Generation (RAG) has emerged as a\nstrategy to mitigate hallucinations by combining external retrieval with text\ngeneration. However, its adoption raises new fairness concerns, as the\nretrieved content itself may surface or amplify bias. This study conducts\nfairness testing through metamorphic testing (MT), introducing controlled\ndemographic perturbations in prompts to assess fairness in sentiment analysis\nperformed by three Small Language Models (SLMs) hosted on HuggingFace\n(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),\neach integrated into a RAG pipeline. Results show that minor demographic\nvariations can break up to one third of metamorphic relations (MRs). A detailed\nanalysis of these failures reveals a consistent bias hierarchy, with\nperturbations involving racial cues being the predominant cause of the\nviolations. In addition to offering a comparative evaluation, this work\nreinforces that the retrieval component in RAG must be carefully curated to\nprevent bias amplification. The findings serve as a practical alert for\ndevelopers, testers and small organizations aiming to adopt accessible SLMs\nwithout compromising fairness or reliability.",
      "pdf_url": "http://arxiv.org/pdf/2509.26584v1",
      "published": "2025-09-30T17:42:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26584v1",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark",
      "authors": [
        "Minhui Zhu",
        "Minyang Tian",
        "Xiaocheng Yang",
        "Tianci Zhou",
        "Penghao Zhu",
        "Eli Chertkov",
        "Shengyan Liu",
        "Yufeng Du",
        "Lifan Yuan",
        "Ziming Ji",
        "Indranil Das",
        "Junyi Cao",
        "Yufeng Du",
        "Jinchen He",
        "Yifan Su",
        "Jiabin Yu",
        "Yikun Jiang",
        "Yujie Zhang",
        "Chang Liu",
        "Ze-Min Huang",
        "Weizhen Jia",
        "Xinan Chen",
        "Peixue Wu",
        "Yunkai Wang",
        "Juntai Zhou",
        "Yong Zhao",
        "Farshid Jafarpour",
        "Jessie Shelton",
        "Aaron Young",
        "John Bartolotta",
        "Wenchao Xu",
        "Yue Sun",
        "Anjun Chu",
        "Victor Colussi",
        "Chris Akers",
        "Nathan Brooks",
        "Wenbo Fu",
        "Christopher Wilson",
        "Jinchao Zhao",
        "Marvin Qi",
        "Anqi Mu",
        "Yubo Yang",
        "Allen Zang",
        "Yang Lyu",
        "Peizhi Mai",
        "Xuefei Guo",
        "Luyu Gao",
        "Ze Yang",
        "Chi Xue",
        "Dmytro Bandak",
        "Yaïr Hein",
        "Yonatan Kahn",
        "Kevin Zhou",
        "John Drew Wilson",
        "Jarrod T. Reilly",
        "Di Luo",
        "Daniel Inafuku",
        "Hao Tong",
        "Liang Yang",
        "Ruixing Zhang",
        "Xueying Wang",
        "Ofir Press",
        "Nicolas Chia",
        "Eliu Huerta",
        "Hao Peng"
      ],
      "abstract": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.",
      "pdf_url": "http://arxiv.org/pdf/2509.26574v2",
      "published": "2025-09-30T17:34:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26574v2",
      "categories": [
        "cs.AI",
        "cond-mat.other",
        "cs.CL",
        "hep-th",
        "quant-ph"
      ]
    },
    {
      "title": "AI-assisted Advanced Propellant Development for Electric Propulsion",
      "authors": [
        "Angel Pan Du",
        "Miguel Arana-Catania",
        "Enric Grustan Gutiérrez"
      ],
      "abstract": "Artificial Intelligence algorithms are introduced in this work as a tool to\npredict the performance of new chemical compounds as alternative propellants\nfor electric propulsion, focusing on predicting their ionisation\ncharacteristics and fragmentation patterns. The chemical properties and\nstructure of the compounds are encoded using a chemical fingerprint, and the\ntraining datasets are extracted from the NIST WebBook. The AI-predicted\nionisation energy and minimum appearance energy have a mean relative error of\n6.87% and 7.99%, respectively, and a predicted ion mass with a 23.89% relative\nerror. In the cases of full mass spectra due to electron ionisation, the\npredictions have a cosine similarity of 0.6395 and align with the top 10 most\nsimilar mass spectra in 78% of instances within a 30 Da range.",
      "pdf_url": "http://arxiv.org/pdf/2509.26567v1",
      "published": "2025-09-30T17:31:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26567v1",
      "categories": [
        "astro-ph.IM",
        "cs.AI",
        "cs.LG",
        "physics.space-ph"
      ]
    },
    {
      "title": "Parametric Neural Amp Modeling with Active Learning",
      "authors": [
        "Florian Grötschla",
        "Longxiang Jiao",
        "Luca A. Lanzendörfer",
        "Roger Wattenhofer"
      ],
      "abstract": "We introduce Panama, an active learning framework to train parametric guitar\namp models end-to-end using a combination of an LSTM model and a WaveNet-like\narchitecture. With \\model, one can create a virtual amp by recording samples\nthat are determined through an ensemble-based active learning strategy to\nminimize the amount of datapoints needed (i.e., amp knob settings). Our\nstrategy uses gradient-based optimization to maximize the disagreement among\nensemble models, in order to identify the most informative datapoints. MUSHRA\nlistening tests reveal that, with 75 datapoints, our models are able to match\nthe perceptual quality of NAM, the leading open-source non-parametric amp\nmodeler.",
      "pdf_url": "http://arxiv.org/pdf/2509.26564v1",
      "published": "2025-09-30T17:30:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26564v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models",
      "authors": [
        "Lina Conti",
        "Dennis Fucci",
        "Marco Gaido",
        "Matteo Negri",
        "Guillaume Wisniewski",
        "Luisa Bentivogli"
      ],
      "abstract": "Contrastive explanations, which indicate why an AI system produced one output\n(the target) instead of another (the foil), are widely regarded in explainable\nAI as more informative and interpretable than standard explanations. However,\nobtaining such explanations for speech-to-text (S2T) generative models remains\nan open challenge. Drawing from feature attribution techniques, we propose the\nfirst method to obtain contrastive explanations in S2T by analyzing how parts\nof the input spectrogram influence the choice between alternative outputs.\nThrough a case study on gender assignment in speech translation, we show that\nour method accurately identifies the audio features that drive the selection of\none gender over another. By extending the scope of contrastive explanations to\nS2T, our work provides a foundation for better understanding S2T models.",
      "pdf_url": "http://arxiv.org/pdf/2509.26543v1",
      "published": "2025-09-30T17:17:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26543v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "HilbertA: Hilbert Attention for Image Generation with Diffusion Models",
      "authors": [
        "Shaoyi Zheng",
        "Wenbo Lu",
        "Yuxuan Xia",
        "Haomin Liu",
        "Shengjie Wang"
      ],
      "abstract": "Designing sparse attention for diffusion transformers requires reconciling\ntwo-dimensional spatial locality with GPU efficiency, a trade-off that current\nmethods struggle to achieve. Existing approaches enforce two-dimensional\nspatial locality but often incur uncoalesced memory access. We present\nHilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA\nreorders image tokens along Hilbert curves to achieve a contiguous memory\nlayout while preserving spatial neighborhoods, and employs a sliding schedule\nacross layers to enable long-range information propagation without repeated or\nuncoalesced memory access. To further enhance cross-tile communication and\npositional awareness, HilbertA introduces a small central shared region.\nImplemented in Triton, HilbertA delivers comparable image quality with\nsignificant acceleration over prior methods on Flux.1-dev, demonstrating the\nfeasibility of hardware-aligned two-dimensional sparse attention for\nhigh-resolution image generation. HilbertA delivers attention speedups of\n$2.3\\times$ when generating $1024\\times 1024$ images, and up to $4.17\\times$ at\n$2048\\times 2048$, while achieving image quality comparable to or surpassing\nbaselines.",
      "pdf_url": "http://arxiv.org/pdf/2509.26538v1",
      "published": "2025-09-30T17:13:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26538v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
      "authors": [
        "Yida Xue",
        "Mingjun Mao",
        "Xiangyuan Ru",
        "Yuqi Zhu",
        "Baochang Ren",
        "Shuofei Qiao",
        "Mengru Wang",
        "Shumin Deng",
        "Xinyu An",
        "Ningyu Zhang",
        "Ying Chen",
        "Huajun Chen"
      ],
      "abstract": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.",
      "pdf_url": "http://arxiv.org/pdf/2509.26536v1",
      "published": "2025-09-30T17:09:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26536v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework",
      "authors": [
        "Jovan Stojkovic",
        "Chaojie Zhang",
        "Íñigo Goiri",
        "Ricardo Bianchini"
      ],
      "abstract": "The rapid rise of large language models (LLMs) has been driving an enormous\ndemand for AI inference infrastructure, mainly powered by high-end GPUs. While\nthese accelerators offer immense computational power, they incur high capital\nand operational costs due to frequent upgrades, dense power consumption, and\ncooling demands, making total cost of ownership (TCO) for AI datacenters a\ncritical concern for cloud providers. Unfortunately, traditional datacenter\nlifecycle management (designed for general-purpose workloads) struggles to keep\npace with AI's fast-evolving models, rising resource needs, and diverse\nhardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme\nacross three stages: building, hardware refresh, and operation. We show how\ndesign choices in power, cooling, and networking provisioning impact long-term\nTCO. We also explore refresh strategies aligned with hardware trends. Finally,\nwe use operation software optimizations to reduce cost. While these\noptimizations at each stage yield benefits, unlocking the full potential\nrequires rethinking the entire lifecycle. Thus, we present a holistic lifecycle\nmanagement framework that coordinates and co-optimizes decisions across all\nthree stages, accounting for workload dynamics, hardware evolution, and system\naging. Our system reduces the TCO by up to 40\\% over traditional approaches.\nUsing our framework we provide guidelines on how to manage AI datacenter\nlifecycle for the future.",
      "pdf_url": "http://arxiv.org/pdf/2509.26534v1",
      "published": "2025-09-30T17:08:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26534v1",
      "categories": [
        "cs.AI",
        "cs.AR",
        "cs.DC"
      ]
    },
    {
      "title": "TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning",
      "authors": [
        "Seohyun Lee",
        "Wenzhi Fang",
        "Dong-Jun Han",
        "Seyyedali Hosseinalipour",
        "Christopher G. Brinton"
      ],
      "abstract": "Federated Learning (FL), despite demonstrating impressive capabilities in the\ntraining of multiple models in a decentralized manner, has been shown to\nproduce a final model not necessarily well-suited to the needs of each client.\nWhile extensive work has been conducted on how to create tailored personalized\nmodels, called Personalized Federated Learning (PFL), less attention has been\ngiven to personalization via fine-tuning of foundation models with multi-task\nand multi-modal properties. Moreover, there exists a lack of understanding in\nthe literature on how to fine-tune and personalize such models in a setting\nthat is heterogeneous across clients not only in data, but also in tasks and\nmodalities. To address this gap in the literature, we propose TAP (Two-Stage\nAdaptive Personalization), which (i) leverages mismatched model architectures\nbetween the clients and server to selectively conduct replacement operations\nwhen it benefits a client's local tasks and (ii) engages in post-FL knowledge\ndistillation for capturing beneficial general knowledge without compromising\npersonalization. We also introduce the first convergence analysis of the server\nmodel under its modality-task pair architecture, and demonstrate that as the\nnumber of modality-task pairs increases, its ability to cater to all tasks\nsuffers. Through extensive experiments, we demonstrate the effectiveness of our\nproposed algorithm across a variety of datasets and tasks in comparison to a\nmultitude of baselines. Implementation code is publicly available at\nhttps://github.com/lee3296/TAP.",
      "pdf_url": "http://arxiv.org/pdf/2509.26524v1",
      "published": "2025-09-30T17:01:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26524v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph Classification Models",
      "authors": [
        "Baptiste Hilaire",
        "Emmanouil Karystinaios",
        "Gerhard Widmer"
      ],
      "abstract": "Interpretability is essential for deploying deep learning models in symbolic\nmusic analysis, yet most research emphasizes model performance over\nexplanation. To address this, we introduce MUSE-Explainer, a new method that\nhelps reveal how music Graph Neural Network models make decisions by providing\nclear, human-friendly explanations. Our approach generates counterfactual\nexplanations by making small, meaningful changes to musical score graphs that\nalter a model's prediction while ensuring the results remain musically\ncoherent. Unlike existing methods, MUSE-Explainer tailors its explanations to\nthe structure of musical data and avoids unrealistic or confusing outputs. We\nevaluate our method on a music analysis task and show it offers intuitive\ninsights that can be visualized with standard music tools such as Verovio.",
      "pdf_url": "http://arxiv.org/pdf/2509.26521v1",
      "published": "2025-09-30T16:58:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26521v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain",
      "authors": [
        "Adrian Kosowski",
        "Przemysław Uznański",
        "Jan Chorowski",
        "Zuzanna Stamirowska",
        "Michał Bartoszkiewicz"
      ],
      "abstract": "The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as the brain, have powerful\nproperties, including generalizing over time, which is the main barrier for\nMachine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a new Large Language Model\narchitecture based on a scale-free biologically inspired network of \\$n\\$\nlocally-interacting neuron particles. BDH couples strong theoretical\nfoundations and inherent interpretability without sacrificing Transformer-like\nperformance.\n  BDH is a practical, performant state-of-the-art attention-based state space\nsequence learning architecture. In addition to being a graph model, BDH admits\na GPU-friendly formulation. It exhibits Transformer-like scaling laws:\nempirically BDH rivals GPT2 performance on language and translation tasks, at\nthe same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during\ninference entirely relies on synaptic plasticity with Hebbian learning using\nspiking neurons. We confirm empirically that specific, individual synapses\nstrengthen connection whenever BDH hears or reasons about a specific concept\nwhile processing language inputs. The neuron interaction network of BDH is a\ngraph of high modularity with heavy-tailed degree distribution. The BDH model\nis biologically plausible, explaining one possible mechanism which human\nneurons could use to achieve speech.\n  BDH is designed for interpretability. Activation vectors of BDH are sparse\nand positive. We demonstrate monosemanticity in BDH on language tasks.\nInterpretability of state, which goes beyond interpretability of neurons and\nmodel parameters, is an inherent feature of the BDH architecture.",
      "pdf_url": "http://arxiv.org/pdf/2509.26507v1",
      "published": "2025-09-30T16:49:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26507v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "SCUBA: Salesforce Computer Use Benchmark",
      "authors": [
        "Yutong Dai",
        "Krithika Ramakrishnan",
        "Jing Gu",
        "Matthew Fernandez",
        "Yanqi Luo",
        "Viraj Prabhu",
        "Zhenyu Hu",
        "Silvio Savarese",
        "Caiming Xiong",
        "Zeyuan Chen",
        "Ran Xu"
      ],
      "abstract": "We introduce SCUBA, a benchmark designed to evaluate computer-use agents on\ncustomer relationship management (CRM) workflows within the Salesforce\nplatform. SCUBA contains 300 task instances derived from real user interviews,\nspanning three primary personas, platform administrators, sales\nrepresentatives, and service agents. The tasks test a range of\nenterprise-critical abilities, including Enterprise Software UI navigation,\ndata manipulation, workflow automation, information retrieval, and\ntroubleshooting. To ensure realism, SCUBA operates in Salesforce sandbox\nenvironments with support for parallel execution and fine-grained evaluation\nmetrics to capture milestone progress. We benchmark a diverse set of agents\nunder both zero-shot and demonstration-augmented settings. We observed huge\nperformance gaps in different agent design paradigms and gaps between the\nopen-source model and the closed-source model. In the zero-shot setting,\nopen-source model powered computer-use agents that have strong performance on\nrelated benchmarks like OSWorld only have less than 5\\% success rate on SCUBA,\nwhile methods built on closed-source models can still have up to 39% task\nsuccess rate. In the demonstration-augmented settings, task success rates can\nbe improved to 50\\% while simultaneously reducing time and costs by 13% and\n16%, respectively. These findings highlight both the challenges of enterprise\ntasks automation and the promise of agentic solutions. By offering a realistic\nbenchmark with interpretable evaluation, SCUBA aims to accelerate progress in\nbuilding reliable computer-use agents for complex business software ecosystems.",
      "pdf_url": "http://arxiv.org/pdf/2509.26506v1",
      "published": "2025-09-30T16:48:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26506v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Indoor/Outdoor Spectrum Sharing Enabled by GNSS-based Classifiers",
      "authors": [
        "Hossein Nasiri",
        "Muhammad Iqbal Rochman",
        "Monisha Ghosh"
      ],
      "abstract": "The desirability of the mid-band frequency range (1 - 10 GHz) for federal and\ncommercial applications, combined with the growing applications for commercial\nindoor use-cases, such as factory automation, opens up a new approach to\nspectrum sharing: the same frequency bands used outdoors by federal incumbents\ncan be reused by commercial indoor users. A recent example of such sharing,\nbetween commercial systems, is the 6 GHz band (5.925 - 7.125 GHz) where\nunlicensed, low-power-indoor (LPI) users share the band with outdoor\nincumbents, primarily fixed microwave links. However, to date, there exist no\nreliable, automatic means of determining whether a device is indoors or\noutdoors, necessitating the use of other mechanisms such as mandating indoor\naccess points (APs) to have integrated antennas and not be battery powered, and\nreducing transmit power of client devices which may be outdoors. An accurate\nindoor/outdoor (I/O) classification addresses these challenges, enabling\nautomatic transmit power adjustments without interfering with incumbents. To\nthis end, we leverage the Global Navigation Satellite System (GNSS) signals for\nI/O classification. GNSS signals, designed inherently for outdoor reception and\nhighly susceptible to indoor attenuation and blocking, provide a robust and\ndistinguishing feature for environmental sensing. We develop various\nmethodologies, including threshold-based techniques and machine learning\napproaches and evaluate them using an expanded dataset gathered from diverse\ngeographical locations. Our results demonstrate that GNSS-based methods alone\ncan achieve greater accuracy than approaches relying solely on wireless (Wi-Fi)\ndata, particularly in unfamiliar locations. Furthermore, the integration of\nGNSS data with Wi-Fi information leads to improved classification accuracy,\nshowcasing the significant benefits of multi-modal data fusion.",
      "pdf_url": "http://arxiv.org/pdf/2509.26500v1",
      "published": "2025-09-30T16:43:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26500v1",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!",
      "authors": [
        "Jingdi Lei",
        "Varun Gumma",
        "Rishabh Bhardwaj",
        "Seok Min Lim",
        "Chuan Li",
        "Amir Zadeh",
        "Soujanya Poria"
      ],
      "abstract": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.",
      "pdf_url": "http://arxiv.org/pdf/2509.26495v1",
      "published": "2025-09-30T16:39:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26495v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications",
      "authors": [
        "Wei He",
        "Yueqing Sun",
        "Hongyan Hao",
        "Xueyuan Hao",
        "Zhikang Xia",
        "Qi Gu",
        "Chengcheng Han",
        "Dengchang Zhao",
        "Hui Su",
        "Kefeng Zhang",
        "Man Gao",
        "Xi Su",
        "Xiaodong Cai",
        "Xunliang Cai",
        "Yu Yang",
        "Yunke Zhao"
      ],
      "abstract": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2509.26490v1",
      "published": "2025-09-30T16:33:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26490v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Combining Knowledge Graphs and NLP to Analyze Instant Messaging Data in Criminal Investigations",
      "authors": [
        "Riccardo Pozzi",
        "Valentina Barbera",
        "Renzo Alva Principe",
        "Davide Giardini",
        "Riccardo Rubini",
        "Matteo Palmonari"
      ],
      "abstract": "Criminal investigations often involve the analysis of messages exchanged\nthrough instant messaging apps such as WhatsApp, which can be an extremely\neffort-consuming task. Our approach integrates knowledge graphs and NLP models\nto support this analysis by semantically enriching data collected from\nsuspects' mobile phones, and help prosecutors and investigators search into the\ndata and get valuable insights. Our semantic enrichment process involves\nextracting message data and modeling it using a knowledge graph, generating\ntranscriptions of voice messages, and annotating the data using an end-to-end\nentity extraction approach. We adopt two different solutions to help users get\ninsights into the data, one based on querying and visualizing the graph, and\none based on semantic search. The proposed approach ensures that users can\nverify the information by accessing the original data. While we report about\nearly results and prototypes developed in the context of an ongoing project,\nour proposal has undergone practical applications with real investigation data.\nAs a consequence, we had the chance to interact closely with prosecutors,\ncollecting positive feedback but also identifying interesting opportunities as\nwell as promising research directions to share with the research community.",
      "pdf_url": "http://arxiv.org/pdf/2509.26487v1",
      "published": "2025-09-30T16:32:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26487v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "TVS Sidekick: Challenges and Practical Insights from Deploying Large Language Models in the Enterprise",
      "authors": [
        "Paula Reyero Lobo",
        "Kevin Johnson",
        "Bill Buchanan",
        "Matthew Shardlow",
        "Ashley Williams",
        "Samuel Attwood"
      ],
      "abstract": "Many enterprises are increasingly adopting Artificial Intelligence (AI) to\nmake internal processes more competitive and efficient. In response to public\nconcern and new regulations for the ethical and responsible use of AI,\nimplementing AI governance frameworks could help to integrate AI within\norganisations and mitigate associated risks. However, the rapid technological\nadvances and lack of shared ethical AI infrastructures creates barriers to\ntheir practical adoption in businesses. This paper presents a real-world AI\napplication at TVS Supply Chain Solutions, reporting on the experience\ndeveloping an AI assistant underpinned by large language models and the\nethical, regulatory, and sociotechnical challenges in deployment for enterprise\nuse.",
      "pdf_url": "http://arxiv.org/pdf/2509.26482v1",
      "published": "2025-09-30T16:29:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26482v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Regression Language Models for Code",
      "authors": [
        "Yash Akhauri",
        "Xingyou Song",
        "Arissa Wongpanich",
        "Bryan Lewandowski",
        "Mohamed S. Abdelfattah"
      ],
      "abstract": "We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five\nclassic NAS design spaces previously dominated by graph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms.",
      "pdf_url": "http://arxiv.org/pdf/2509.26476v1",
      "published": "2025-09-30T16:25:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26476v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.PF",
        "cs.SE"
      ]
    },
    {
      "title": "The Average Patient Fallacy",
      "authors": [
        "Alaleh Azhir",
        "Shawn N. Murphy",
        "Hossein Estiri"
      ],
      "abstract": "Machine learning in medicine is typically optimized for population averages.\nThis frequency weighted training privileges common presentations and\nmarginalizes rare yet clinically critical cases, a bias we call the average\npatient fallacy. In mixture models, gradients from rare cases are suppressed by\nprevalence, creating a direct conflict with precision medicine. Clinical\nvignettes in oncology, cardiology, and ophthalmology show how this yields\nmissed rare responders, delayed recognition of atypical emergencies, and\nunderperformance on vision-threatening variants. We propose operational fixes:\nRare Case Performance Gap, Rare Case Calibration Error, a prevalence utility\ndefinition of rarity, and clinically weighted objectives that surface ethical\npriorities. Weight selection should follow structured deliberation. AI in\nmedicine must detect exceptional cases because of their significance.",
      "pdf_url": "http://arxiv.org/pdf/2509.26474v1",
      "published": "2025-09-30T16:24:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26474v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models",
      "authors": [
        "Shaoxiong Guo",
        "Tianyi Du",
        "Lijun Li",
        "Yuyao Wu",
        "Jie Li",
        "Jing Shao"
      ],
      "abstract": "Unified Multimodal understanding and generation Models (UMMs) have\ndemonstrated remarkable capabilities in both understanding and generation\ntasks. However, we identify a vulnerability arising from the\ngeneration-understanding coupling in UMMs. The attackers can use the generative\nfunction to craft an information-rich adversarial image and then leverage the\nunderstanding function to absorb it in a single pass, which we call Cross-Modal\nGenerative Injection (CMGI). Current attack methods on malicious instructions\nare often limited to a single modality while also relying on prompt rewriting\nwith semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We\npropose STaR-Attack, the first multi-turn jailbreak attack framework that\nexploits unique safety weaknesses of UMMs without semantic drift. Specifically,\nour method defines a malicious event that is strongly correlated with the\ntarget query within a spatio-temporal context. Using the three-act narrative\ntheory, STaR-Attack generates the pre-event and the post-event scenes while\nconcealing the malicious event as the hidden climax. When executing the attack\nstrategy, the opening two rounds exploit the UMM's generative ability to\nproduce images for these scenes. Subsequently, an image-based question guessing\nand answering game is introduced by exploiting the understanding capability.\nSTaR-Attack embeds the original malicious question among benign candidates,\nforcing the model to select and answer the most relevant one given the\nnarrative context. Extensive experiments show that STaR-Attack consistently\nsurpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and\nsurpasses the strongest prior baseline, FlipAttack. Our work uncovers a\ncritical yet underdeveloped vulnerability and highlights the need for safety\nalignments in UMMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.26473v1",
      "published": "2025-09-30T16:22:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26473v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "On Deepfake Voice Detection -- It's All in the Presentation",
      "authors": [
        "Héctor Delgado",
        "Giorgio Ramondetti",
        "Emanuele Dalmasso",
        "Gennady Karvitsky",
        "Daniele Colibro",
        "Haydar Talib"
      ],
      "abstract": "While the technologies empowering malicious audio deepfakes have dramatically\nevolved in recent years due to generative AI advances, the same cannot be said\nof global research into spoofing (deepfake) countermeasures. This paper\nhighlights how current deepfake datasets and research methodologies led to\nsystems that failed to generalize to real world application. The main reason is\ndue to the difference between raw deepfake audio, and deepfake audio that has\nbeen presented through a communication channel, e.g. by phone. We propose a new\nframework for data creation and research methodology, allowing for the\ndevelopment of spoofing countermeasures that would be more effective in\nreal-world scenarios. By following the guidelines outlined here we improved\ndeepfake detection accuracy by 39% in more robust and realistic lab setups, and\nby 57% on a real-world benchmark. We also demonstrate how improvement in\ndatasets would have a bigger impact on deepfake detection accuracy than the\nchoice of larger SOTA models would over smaller models; that is, it would be\nmore important for the scientific community to make greater investment on\ncomprehensive data collection programs than to simply train larger models with\nhigher computational demands.",
      "pdf_url": "http://arxiv.org/pdf/2509.26471v1",
      "published": "2025-09-30T16:19:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26471v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "Extreme Self-Preference in Language Models",
      "authors": [
        "Steven A. Lehr",
        "Mary Cipperman",
        "Mahzarin R. Banaji"
      ],
      "abstract": "A preference for oneself (self-love) is a fundamental feature of biological\norganisms, with evidence in humans often bordering on the comedic. Since large\nlanguage models (LLMs) lack sentience - and themselves disclaim having selfhood\nor identity - one anticipated benefit is that they will be protected from, and\nin turn protect us from, distortions in our decisions. Yet, across 5 studies\nand ~20,000 queries, we discovered massive self-preferences in four widely used\nLLMs. In word-association tasks, models overwhelmingly paired positive\nattributes with their own names, companies, and CEOs relative to those of their\ncompetitors. Strikingly, when models were queried through APIs this\nself-preference vanished, initiating detection work that revealed API models\noften lack clear recognition of themselves. This peculiar feature\nserendipitously created opportunities to test the causal link between\nself-recognition and self-love. By directly manipulating LLM identity - i.e.,\nexplicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing\nLLM1 that it was LLM2 - we found that self-love consistently followed assigned,\nnot true, identity. Importantly, LLM self-love emerged in consequential\nsettings beyond word-association tasks, when evaluating job candidates,\nsecurity software proposals and medical chatbots. Far from bypassing this human\nbias, self-love appears to be deeply encoded in LLM cognition. This result\nraises questions about whether LLM behavior will be systematically influenced\nby self-preferential tendencies, including a bias toward their own operation\nand even their own existence. We call on corporate creators of these models to\ncontend with a significant rupture in a core promise of LLMs - neutrality in\njudgment and decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2509.26464v1",
      "published": "2025-09-30T16:13:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26464v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2.7; I.2.6; K.4.2"
      ]
    },
    {
      "title": "Zero-Shot Decentralized Federated Learning",
      "authors": [
        "Alessio Masano",
        "Matteo Pennisi",
        "Federica Proietto Salanitri",
        "Concetto Spampinato",
        "Giovanni Bellitto"
      ],
      "abstract": "CLIP has revolutionized zero-shot learning by enabling task generalization\nwithout fine-tuning. While prompting techniques like CoOp and CoCoOp enhance\nCLIP's adaptability, their effectiveness in Federated Learning (FL) remains an\nopen challenge. Existing federated prompt learning approaches, such as FedCoOp\nand FedTPG, improve performance but face generalization issues, high\ncommunication costs, and reliance on a central server, limiting scalability and\nprivacy. We propose Zero-shot Decentralized Federated Learning (ZeroDFL), a\nfully decentralized framework that enables zero-shot adaptation across\ndistributed clients without a central coordinator. ZeroDFL employs an iterative\nprompt-sharing mechanism, allowing clients to optimize and exchange textual\nprompts to enhance generalization while drastically reducing communication\noverhead. We validate ZeroDFL on nine diverse image classification datasets,\ndemonstrating that it consistently outperforms--or remains on par\nwith--state-of-the-art federated prompt learning methods. More importantly,\nZeroDFL achieves this performance in a fully decentralized setting while\nreducing communication overhead by 118x compared to FedTPG. These results\nhighlight that our approach not only enhances generalization in federated\nzero-shot learning but also improves scalability, efficiency, and privacy\npreservation--paving the way for decentralized adaptation of large\nvision-language models in real-world applications.",
      "pdf_url": "http://arxiv.org/pdf/2509.26462v1",
      "published": "2025-09-30T16:13:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26462v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification",
      "authors": [
        "Artur Barros",
        "Carlos Caetano",
        "João Macedo",
        "Jefersson A. dos Santos",
        "Sandra Avila"
      ],
      "abstract": "Indoor scene classification is a critical task in computer vision, with\nwide-ranging applications that go from robotics to sensitive content analysis,\nsuch as child sexual abuse imagery (CSAI) classification. The problem is\nparticularly challenging due to the intricate relationships between objects and\ncomplex spatial layouts. In this work, we propose the Attention over Scene\nGraphs for Sensitive Content Analysis (ASGRA), a novel framework that operates\non structured graph representations instead of raw pixels. By first converting\nimages into Scene Graphs and then employing a Graph Attention Network for\ninference, ASGRA directly models the interactions between a scene's components.\nThis approach offers two key benefits: (i) inherent explainability via object\nand relationship identification, and (ii) privacy preservation, enabling model\ntraining without direct access to sensitive images. On Places8, we achieve\n81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI\nevaluation with law enforcement yields 74.27% balanced accuracy. Our results\nestablish structured scene representations as a robust paradigm for indoor\nscene classification and CSAI classification. Code is publicly available at\nhttps://github.com/tutuzeraa/ASGRA.",
      "pdf_url": "http://arxiv.org/pdf/2509.26457v1",
      "published": "2025-09-30T16:09:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26457v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL Benchmark Dataset and 0.92 AUC Baseline",
      "authors": [
        "Naomi Fridman",
        "Anat Goldstein"
      ],
      "abstract": "The error is caused by special characters that arXiv's system doesn't\nrecognize. Here's the cleaned version with all problematic characters replaced:\nBreast magnetic resonance imaging is a critical tool for cancer detection and\ntreatment planning, but its clinical utility is hindered by poor specificity,\nleading to high false-positive rates and unnecessary biopsies. This study\nintroduces a transformer-based framework for automated classification of breast\nlesions in dynamic contrast-enhanced MRI, addressing the challenge of\ndistinguishing benign from malignant findings. We implemented a SegFormer\narchitecture that achieved an AUC of 0.92 for lesion-level classification, with\n100% sensitivity and 67% specificity at the patient level - potentially\neliminating one-third of unnecessary biopsies without missing malignancies. The\nmodel quantifies malignant pixel distribution via semantic segmentation,\nproducing interpretable spatial predictions that support clinical\ndecision-making. To establish reproducible benchmarks, we curated\nBreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection\ninto a standardized deep learning dataset with 88 patients and 133 annotated\nlesions (89 benign, 44 malignant). This resource addresses a key infrastructure\ngap, as existing public datasets lack benign lesion annotations, limiting\nbenign-malignant classification research. Training incorporated an expanded\ncohort of over 1,200 patients through integration with BreastDCEDL datasets,\nvalidating transfer learning approaches despite primary tumor-only annotations.\nPublic release of the dataset, models, and evaluation protocols provides the\nfirst standardized benchmark for DCE-MRI lesion classification, enabling\nmethodological advancement toward clinical deployment.",
      "pdf_url": "http://arxiv.org/pdf/2509.26440v1",
      "published": "2025-09-30T15:58:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26440v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Adaptive Planning for Multi-Attribute Controllable Summarization with Monte Carlo Tree Search",
      "authors": [
        "Sangwon Ryu",
        "Heejin Do",
        "Yunsu Kim",
        "Gary Geunbae Lee",
        "Jungseul Ok"
      ],
      "abstract": "Controllable summarization moves beyond generic outputs toward human-aligned\nsummaries guided by specified attributes. In practice, the interdependence\namong attributes makes it challenging for language models to satisfy correlated\nconstraints consistently. Moreover, previous approaches often require\nper-attribute fine-tuning, limiting flexibility across diverse summary\nattributes. In this paper, we propose adaptive planning for multi-attribute\ncontrollable summarization (PACO), a training-free framework that reframes the\ntask as planning the order of sequential attribute control with a customized\nMonte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions\ncorrespond to single-attribute adjustments, enabling progressive refinement of\nonly the attributes requiring further control. This strategy adaptively\ndiscovers optimal control orders, ultimately producing summaries that\neffectively meet all constraints. Extensive experiments across diverse domains\nand models demonstrate that PACO achieves robust multi-attribute\ncontrollability, surpassing both LLM-based self-planning models and fine-tuned\nbaselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the\nmuch larger Llama-3.3-70B baselines. With larger models, PACO achieves superior\ncontrol performance, outperforming all competitors.",
      "pdf_url": "http://arxiv.org/pdf/2509.26435v1",
      "published": "2025-09-30T15:55:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26435v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ACT: Agentic Classification Tree",
      "authors": [
        "Vincent Grari",
        "Tim Arni",
        "Thibault Laugel",
        "Sylvain Lamprier",
        "James Zou",
        "Marcin Detyniecki"
      ],
      "abstract": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths.",
      "pdf_url": "http://arxiv.org/pdf/2509.26433v1",
      "published": "2025-09-30T15:54:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26433v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size",
      "authors": [
        "Guanxi Lu",
        "Hao Mark Chen",
        "Yuto Karashima",
        "Zhican Wang",
        "Daichi Fujiki",
        "Hongxiang Fan"
      ],
      "abstract": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.26432v2",
      "published": "2025-09-30T15:53:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26432v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Ascent Fails to Forget",
      "authors": [
        "Ioannis Mavrothalassitis",
        "Pol Puigdemont",
        "Noam Itzhak Levi",
        "Volkan Cevher"
      ],
      "abstract": "Contrary to common belief, we show that gradient ascent-based unconstrained\noptimization methods frequently fail to perform machine unlearning, a\nphenomenon we attribute to the inherent statistical dependence between the\nforget and retain data sets. This dependence, which can manifest itself even as\nsimple correlations, undermines the misconception that these sets can be\nindependently manipulated during unlearning. We provide empirical and\ntheoretical evidence showing these methods often fail precisely due to this\noverlooked relationship. For random forget sets, this dependence means that\ndegrading forget set metrics (which, for a retrained model, should mirror test\nset metrics) inevitably harms overall test performance. Going beyond random\nsets, we consider logistic regression as an instructive example where a\ncritical failure mode emerges: inter-set dependence causes gradient\ndescent-ascent iterations to progressively diverge from the ideal retrained\nmodel. Strikingly, these methods can converge to solutions that are not only\nfar from the retrained ideal but are potentially even further from it than the\noriginal model itself, rendering the unlearning process actively detrimental. A\ntoy example further illustrates how this dependence can trap models in inferior\nlocal minima, inescapable via finetuning. Our findings highlight that the\npresence of such statistical dependencies, even when manifest only as\ncorrelations, can be sufficient for ascent-based unlearning to fail. Our\ntheoretical insights are corroborated by experiments on complex neural\nnetworks, demonstrating that these methods do not perform as expected in\npractice due to this unaddressed statistical interplay.",
      "pdf_url": "http://arxiv.org/pdf/2509.26427v1",
      "published": "2025-09-30T15:48:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26427v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "OntoAligner Meets Knowledge Graph Embedding Aligners",
      "authors": [
        "Hamed Babaei Giglou",
        "Jennifer D'Souza",
        "Sören Auer",
        "Mahsa Sanaei"
      ],
      "abstract": "Ontology Alignment (OA) is essential for enabling semantic interoperability\nacross heterogeneous knowledge systems. While recent advances have focused on\nlarge language models (LLMs) for capturing contextual semantics, this work\nrevisits the underexplored potential of Knowledge Graph Embedding (KGE) models,\nwhich offer scalable, structure-aware representations well-suited to\nontology-based tasks. Despite their effectiveness in link prediction, KGE\nmethods remain underutilized in OA, with most prior work focusing narrowly on a\nfew models. To address this gap, we reformulate OA as a link prediction problem\nover merged ontologies represented as RDF-style triples and develop a modular\nframework, integrated into the OntoAligner library, that supports 17 diverse\nKGE models. The system learns embeddings from a combined ontology and aligns\nentities by computing cosine similarity between their representations. We\nevaluate our approach using standard metrics across seven benchmark datasets\nspanning five domains: Anatomy, Biodiversity, Circular Economy, Material\nScience and Engineering, and Biomedical Machine Learning. Two key findings\nemerge: first, KGE models like ConvE and TransF consistently produce\nhigh-precision alignments, outperforming traditional systems in structure-rich\nand multi-relational domains; second, while their recall is moderate, this\nconservatism makes KGEs well-suited for scenarios demanding high-confidence\nmappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs\ndirectly preserve and exploit ontology structure, offering a complementary and\ncomputationally efficient strategy. These results highlight the promise of\nembedding-based OA and open pathways for further work on hybrid models and\nadaptive strategies.",
      "pdf_url": "http://arxiv.org/pdf/2509.26417v1",
      "published": "2025-09-30T15:41:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26417v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From",
      "authors": [
        "Yao Tong",
        "Haonan Wang",
        "Siquan Li",
        "Kenji Kawaguchi",
        "Tianyang Hu"
      ],
      "abstract": "Fingerprinting Large Language Models (LLMs) is essential for provenance\nverification and model attribution. Existing methods typically extract post-hoc\nsignatures based on training dynamics, data exposure, or hyperparameters --\nproperties that only emerge after training begins. In contrast, we propose a\nstronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method\nthat leverages random initialization biases as persistent, seed-dependent\nidentifiers present even before training. We show that untrained models exhibit\nreproducible token selection biases conditioned solely on their parameters at\ninitialization. These biases are stable and measurable throughout training,\nenabling our statistical detection method to recover a model's lineage with\nhigh confidence. Unlike prior techniques, unreliable before convergence and\nvulnerable to distribution shifts, SeedPrints remains effective across all\ntraining stages and robust under domain shifts or parameter modifications.\nExperiments on LLaMA-style and Qwen-style models show that SeedPrints achieves\nseed-level distinguishability and can provide birth-to-lifecycle identity\nverification akin to a biometric fingerprint. Evaluations on large-scale\npretrained models and fingerprinting benchmarks further confirm its\neffectiveness under practical deployment scenarios. These results suggest that\ninitialization itself imprints a unique and persistent identity on neural\nlanguage models, forming a true ''Galtonian'' fingerprint.",
      "pdf_url": "http://arxiv.org/pdf/2509.26404v1",
      "published": "2025-09-30T15:34:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26404v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Communication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation",
      "authors": [
        "Le-Tuan Nguyen",
        "Minh-Duong Nguyen",
        "Seon-Geun Jeong",
        "Dung D. Le",
        "Quoc-Viet Pham"
      ],
      "abstract": "With the rapid emergence of foundation models and the increasing need for\nfine-tuning across distributed environments, Federated Low-Rank Adaptation\n(FedLoRA) has recently gained significant attention. Despite enormous\npotential, current FedLoRA methods face notable challenges due to inexact\nupdates. Existing approaches have attempted to mitigate this issue, but they\noften introduce a \\emph{local-global generalization gap} and incur\n\\emph{substantial communication overhead}, limiting their scalability and\neffectiveness. To address these limitations, we propose \\textbf{F}ederated\n\\textbf{Lo}w-\\textbf{R}ank \\textbf{A}ggregation with \\textbf{N}early\n\\textbf{A}ccurate Estimation (FLoRA-NA). FLoRA-NA leverages the local LoRA\nmatrices on the server to estimate the aggregated matrices $\\hat{A}$ and\n$\\hat{B}$, which are then distributed to clients for local updates. This\nsurrogated aggregated matrices minimizes the divergence between ideal $\\nabla\n\\Bar{W} = \\sum^{U}_{u=1}B_u A_u$ and practical updates $\\nabla \\hat{W} =\n\\hat{B}\\hat{A}$ without adding communication cost beyond vanilla FedLoRA. By\ndoing so, FLoRA-NA achieves communication efficiency and bridges the gap\nbetween local personalization and global generalization, addressing a key\nlimitation of prior personalized FedLoRA approaches. We conduct extensive\nevaluations across diverse tasks, including natural language understanding,\nmathematical reasoning, and code-solving ability using various foundation\nmodels. Experimental results consistently demonstrate that FLoRA-NA achieves\nstate-of-the-art global performance while maintaining low communication\noverhead.",
      "pdf_url": "http://arxiv.org/pdf/2509.26399v3",
      "published": "2025-09-30T15:32:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26399v3",
      "categories": [
        "cs.AI",
        "68",
        "I.2"
      ]
    },
    {
      "title": "Game-Time: Evaluating Temporal Dynamics in Spoken Language Models",
      "authors": [
        "Kai-Wei Chang",
        "En-Pei Hu",
        "Chun-Yi Kuan",
        "Wenze Ren",
        "Wei-Chih Chen",
        "Guan-Ting Lin",
        "Yu Tsao",
        "Shao-Hua Sun",
        "Hung-yi Lee",
        "James Glass"
      ],
      "abstract": "Conversational Spoken Language Models (SLMs) are emerging as a promising\nparadigm for real-time speech interaction. However, their capacity of temporal\ndynamics, including the ability to manage timing, tempo and simultaneous\nspeaking, remains a critical and unevaluated challenge for conversational\nfluency. To address this gap, we introduce the Game-Time Benchmark, a framework\nto systematically assess these temporal capabilities. Inspired by how humans\nlearn a language through language activities, Game-Time consists of basic\ninstruction-following tasks and advanced tasks with temporal constraints, such\nas tempo adherence and synchronized responses. Our evaluation of diverse SLM\narchitectures reveals a clear performance disparity: while state-of-the-art\nmodels handle basic tasks well, many contemporary systems still struggle with\nfundamental instruction-following. More critically, nearly all models degrade\nsubstantially under temporal constraints, exposing persistent weaknesses in\ntime awareness and full-duplex interaction. The Game-Time Benchmark provides a\nfoundation for guiding future research toward more temporally-aware\nconversational AI. Demos and datasets are available on our project website\nhttps://ga642381.github.io/Game-Time.",
      "pdf_url": "http://arxiv.org/pdf/2509.26388v1",
      "published": "2025-09-30T15:23:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26388v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning",
      "authors": [
        "Jinyeop Song",
        "Song Wang",
        "Julian Shun",
        "Yada Zhu"
      ],
      "abstract": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large\nlanguage models (LLMs) with structured, verifiable knowledge graphs (KGs) to\nreduce hallucinations and expose reasoning traces. However, many KG-RAG systems\ncompose multiple LLM modules (e.g planning, reasoning, and responding),\ninflating inference cost and binding behavior to a specific target KG. To\naddress this, we introduce KG-R1, an agentic KG retrieval-augmented generation\n(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single\nagent that interacts with KGs as its environment, learning to retrieve at each\nstep and incorporating the retrieved information into its reasoning and\ngeneration. The process is optimized through end-to-end RL. In controlled\nexperiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our\nmethod demonstrates both efficiency and transferability: Using Qwen-2.5-3B,\nKG-R1 improves answer accuracy with fewer generation tokens than prior\nmulti-module workflow methods that use larger foundation or fine-tuned models.\nFurthermore, KG-R1 enables plug and play: after training, it maintains strong\naccuracy on new KGs without modification. These properties make KG-R1 a\npromising KG-RAG framework for real-world deployment. Our code is publicly\navailable at https://github.com/Jinyeop3110/KG-R1.",
      "pdf_url": "http://arxiv.org/pdf/2509.26383v2",
      "published": "2025-09-30T15:14:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26383v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MC-GNNAS-Dock: Multi-criteria GNN-based Algorithm Selection for Molecular Docking",
      "authors": [
        "Siyuan Cao",
        "Hongxuan Wu",
        "Jiabao Brad Wang",
        "Yiliang Yuan",
        "Mustafa Misir"
      ],
      "abstract": "Molecular docking is a core tool in drug discovery for predicting\nligand-target interactions. Despite the availability of diverse search-based\nand machine learning approaches, no single docking algorithm consistently\ndominates, as performance varies by context. To overcome this challenge,\nalgorithm selection frameworks such as GNNAS-Dock, built on graph neural\nnetworks, have been proposed. This study introduces an enhanced system,\nMC-GNNAS-Dock, with three key advances. First, a multi-criteria evaluation\nintegrates binding-pose accuracy (RMSD) with validity checks from PoseBusters,\noffering a more rigorous assessment. Second, architectural refinements by\ninclusion of residual connections strengthen predictive robustness. Third,\nrank-aware loss functions are incorporated to sharpen rank learning. Extensive\nexperiments are performed on a curated dataset containing approximately 3200\nprotein-ligand complexes from PDBBind. MC-GNNAS-Dock demonstrates consistently\nsuperior performance, achieving up to 5.4% (3.4%) gains under composite\ncriteria of RMSD below 1\\AA{} (2\\AA{}) with PoseBuster-validity compared to the\nsingle best solver (SBS) Uni-Mol Docking V2.",
      "pdf_url": "http://arxiv.org/pdf/2509.26377v1",
      "published": "2025-09-30T15:08:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26377v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning",
      "authors": [
        "Zichao Shen",
        "Chen Gao",
        "Jiaqi Yuan",
        "Tianchen Zhu",
        "Xingcheng Fu",
        "Qingyun Sun"
      ],
      "abstract": "Embodied task planning requires agents to produce executable actions in a\nclose-loop manner within the environment. With progressively improving\ncapabilities of LLMs in task decomposition, planning, and generalization,\ncurrent embodied task planning methods adopt LLM-based architecture.However,\nexisting LLM-based planners remain limited in three aspects, i.e., fixed\nplanning paradigms, lack of action sequence constraints, and error-agnostic. In\nthis work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,\nstate-dependency aware and error-aware mechanisms for comprehensive embodied\ntask planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to\nexplicitly model action preconditions and effects, guiding the dynamic\nrevision. To handle execution error, it employs an error-adaptive replanning\nstrategy consisting of Error Backtrack and Diagnosis and Adaptive Action\nSubTree Generation, which locally reconstructs the affected portion of the plan\nbased on the current environment state. Experiments demonstrate that\nSDA-PLANNER consistently outperforms baselines in success rate and goal\ncompletion, particularly under diverse error conditions.",
      "pdf_url": "http://arxiv.org/pdf/2509.26375v1",
      "published": "2025-09-30T15:07:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26375v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Vector-Valued Reproducing Kernel Banach Spaces for Neural Networks and Operators",
      "authors": [
        "Sven Dummer",
        "Tjeerd Jan Heeringa",
        "José A. Iglesias"
      ],
      "abstract": "Recently, there has been growing interest in characterizing the function\nspaces underlying neural networks. While shallow and deep scalar-valued neural\nnetworks have been linked to scalar-valued reproducing kernel Banach spaces\n(RKBS), $\\mathbb{R}^d$-valued neural networks and neural operator models remain\nless understood in the RKBS setting. To address this gap, we develop a general\ndefinition of vector-valued RKBS (vv-RKBS), which inherently includes the\nassociated reproducing kernel. Our construction extends existing definitions by\navoiding restrictive assumptions such as symmetric kernel domains,\nfinite-dimensional output spaces, reflexivity, or separability, while still\nrecovering familiar properties of vector-valued reproducing kernel Hilbert\nspaces (vv-RKHS). We then show that shallow $\\mathbb{R}^d$-valued neural\nnetworks are elements of a specific vv-RKBS, namely an instance of the integral\nand neural vv-RKBS. To also explore the functional structure of neural\noperators, we analyze the DeepONet and Hypernetwork architectures and\ndemonstrate that they too belong to an integral and neural vv-RKBS. In all\ncases, we establish a Representer Theorem, showing that optimization over these\nfunction spaces recovers the corresponding neural architectures.",
      "pdf_url": "http://arxiv.org/pdf/2509.26371v2",
      "published": "2025-09-30T15:06:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26371v2",
      "categories": [
        "math.FA",
        "cs.AI",
        "cs.LG",
        "stat.ML",
        "46E15, 68T07, 46G10, 46E22, 46B10, 26B40",
        "G.1.2; G.1.6; I.5.1; I.2.6"
      ]
    },
    {
      "title": "TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos",
      "authors": [
        "Xiangrui Liu",
        "Minghao Qin",
        "Yan Shu",
        "Zhengyang Liang",
        "Yang Tian",
        "Chen Jason Zhang",
        "Bo Zhao",
        "Zheng Liu"
      ],
      "abstract": "Identifying key moments in long videos is essential for downstream\nunderstanding and reasoning tasks. In this paper, we introduce a new problem,\nTaskoriented Temporal Grounding ToTG, which aims to localize time intervals\ncontaining the necessary information based on a task's natural description.\nAlong with the definition, we also present ToTG Bench, a comprehensive\nbenchmark for evaluating the performance on ToTG. ToTG is particularly\nchallenging for traditional approaches due to their limited generalizability\nand difficulty in handling long videos. To address these challenges, we propose\nTimeScope, a novel framework built upon progressive reasoning. TimeScope first\nidentifies a coarse-grained temporal scope in the long video that likely\ncontains the key moments, and then refines this scope through finegrained\nmoment partitioning. Additionally, we curate a highquality dataset, namely ToTG\nPile, to enhance TimeScope's ability to perform progressive temporal grounding\neffectively. Extensive experiments demonstrate that TimeScope consistently\noutperforms both existing temporalgrounding methods and popular MLLMs across\nvarious settings, highlighting its effectiveness in addressing this new\nchallenging problem.",
      "pdf_url": "http://arxiv.org/pdf/2509.26360v1",
      "published": "2025-09-30T15:00:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26360v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
      "authors": [
        "Shuai Shao",
        "Qihan Ren",
        "Chen Qian",
        "Boyi Wei",
        "Dadi Guo",
        "Jingyi Yang",
        "Xinhao Song",
        "Linfeng Zhang",
        "Weinan Zhang",
        "Dongrui Liu",
        "Jing Shao"
      ],
      "abstract": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.",
      "pdf_url": "http://arxiv.org/pdf/2509.26354v1",
      "published": "2025-09-30T14:55:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.26354v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    }
  ]
}