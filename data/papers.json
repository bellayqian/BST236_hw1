{
  "last_updated": "2025-04-16T00:50:26.154636",
  "papers": [
    {
      "title": "Weight Ensembling Improves Reasoning in Language Models",
      "authors": [
        "Xingyu Dang",
        "Christina Baek",
        "Kaiyue Wen",
        "Zico Kolter",
        "Aditi Raghunathan"
      ],
      "abstract": "We investigate a failure mode that arises during the training of reasoning\nmodels, where the diversity of generations begins to collapse, leading to\nsuboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during\nsupervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a\nsimple intervention of interpolating the weights of the latest SFT checkpoint\nwith an early checkpoint, otherwise known as WiSE-FT, almost completely\nrecovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves\nbetter test-time scaling (Best@k, majority vote) and achieves superior results\nwith less data when tuned further by reinforcement learning. Finally, we find\nthat WiSE-FT provides complementary performance gains that cannot be achieved\nonly through diversity-inducing decoding strategies, like temperature scaling.\nWe formalize a bias-variance tradeoff of Pass@k with respect to the expectation\nand variance of Pass@1 over the test distribution. We find that WiSE-FT can\nreduce bias and variance simultaneously, while temperature scaling inherently\ntrades-off between bias and variance.",
      "pdf_url": "http://arxiv.org/pdf/2504.10478v1",
      "published": "2025-04-14T17:59:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10478v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users",
      "authors": [
        "Suyu Ye",
        "Haojun Shi",
        "Darren Shih",
        "Hyokun Yun",
        "Tanya Roosta",
        "Tianmin Shu"
      ],
      "abstract": "To achieve successful assistance with long-horizon web-based tasks, AI agents\nmust be able to sequentially follow real-world user instructions over a long\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\nfollowing in the real world poses significant challenges beyond performing a\nsingle, clearly defined task. For instance, real-world human instructions can\nbe ambiguous, require different levels of AI assistance, and may evolve over\ntime, reflecting changes in the user's mental state. To address this gap, we\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\ninstruction-following in realistic scenarios involving long-horizon\ninteractions with the web, visual GUI grounding, and understanding ambiguous\nreal-world user instructions. RealWebAssist includes a dataset of sequential\ninstructions collected from real-world human users. Each user instructs a\nweb-based assistant to perform a series of tasks on multiple websites. A\nsuccessful agent must reason about the true intent behind each instruction,\nkeep track of the mental state of the user, understand user-specific routines,\nand ground the intended tasks to actions on the correct GUI elements. Our\nexperimental results show that state-of-the-art models struggle to understand\nand ground user instructions, posing critical challenges in following\nreal-world user instructions for long-horizon web assistance.",
      "pdf_url": "http://arxiv.org/pdf/2504.10445v1",
      "published": "2025-04-14T17:36:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10445v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
      "authors": [
        "Haoran Hao",
        "Jiaming Han",
        "Yiyuan Zhang",
        "Xiangyu Yue"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.",
      "pdf_url": "http://arxiv.org/pdf/2504.10443v1",
      "published": "2025-04-14T17:34:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10443v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models",
      "authors": [
        "Minqian Liu",
        "Zhiyang Xu",
        "Xinyi Zhang",
        "Heajun An",
        "Sarvech Qadir",
        "Qi Zhang",
        "Pamela J. Wisniewski",
        "Jin-Hee Cho",
        "Sang Won Lee",
        "Ruoxi Jia",
        "Lifu Huang"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
      "pdf_url": "http://arxiv.org/pdf/2504.10430v1",
      "published": "2025-04-14T17:20:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10430v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?",
      "authors": [
        "Xinhao Yi",
        "Jake Lever",
        "Kevin Bryson",
        "Zaiqiao Meng"
      ],
      "abstract": "Knowledge editing has emerged as an effective approach for updating large\nlanguage models (LLMs) by modifying their internal knowledge. However, their\napplication to the biomedical domain faces unique challenges due to the\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\ninformation is prevalent. In this paper, we conduct the first comprehensive\nstudy to investigate the effectiveness of knowledge editing methods for editing\nlong-tail biomedical knowledge. Our results indicate that, while existing\nediting methods can enhance LLMs' performance on long-tail biomedical\nknowledge, their performance on long-tail knowledge remains inferior to that on\nhigh-frequency popular knowledge, even after editing. Our further analysis\nreveals that long-tail biomedical knowledge contains a significant amount of\none-to-many knowledge, where one subject and relation link to multiple objects.\nThis high prevalence of one-to-many knowledge limits the effectiveness of\nknowledge editing in improving LLMs' understanding of long-tail biomedical\nknowledge, highlighting the need for tailored strategies to bridge this\nperformance gap.",
      "pdf_url": "http://arxiv.org/pdf/2504.10421v1",
      "published": "2025-04-14T17:08:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10421v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
      "authors": [
        "Parshin Shojaee",
        "Ngoc-Hieu Nguyen",
        "Kazem Meidani",
        "Amir Barati Farimani",
        "Khoa D Doan",
        "Chandan K Reddy"
      ],
      "abstract": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
      "pdf_url": "http://arxiv.org/pdf/2504.10415v1",
      "published": "2025-04-14T17:00:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10415v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance Software Maintainability",
      "authors": [
        "Gopichand Bandarupalli"
      ],
      "abstract": "This study explores Graph Neural Networks (GNNs) as a transformative tool for\ncode refactoring, using abstract syntax trees (ASTs) to boost software\nmaintainability. It analyzes a dataset of 2 million snippets from CodeSearchNet\nand a custom 75000-file GitHub Python corpus, comparing GNNs against rule-based\nSonarQube and decision trees. Metrics include cyclomatic complexity (target\nbelow 10), coupling (target below 5), and refactoring precision. GNNs achieve\n92% accuracy, reducing complexity by 35% and coupling by 33%, outperforming\nSonarQube (78%, 16%) and decision trees (85%, 25%). Preprocessing fixed 60% of\nsyntax errors. Bar graphs, tables, and AST visuals clarify results. This offers\na scalable AI-driven path to cleaner codebases, which is crucial for software\nengineering.",
      "pdf_url": "http://arxiv.org/pdf/2504.10412v1",
      "published": "2025-04-14T16:58:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10412v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "title": "Performance of Large Language Models in Supporting Medical Diagnosis and Treatment",
      "authors": [
        "Diogo Sousa",
        "Guilherme Barbosa",
        "Catarina Rocha",
        "Dulce Oliveira"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into healthcare holds\nsignificant potential to enhance diagnostic accuracy and support medical\ntreatment planning. These AI-driven systems can analyze vast datasets,\nassisting clinicians in identifying diseases, recommending treatments, and\npredicting patient outcomes. This study evaluates the performance of a range of\ncontemporary LLMs, including both open-source and closed-source models, on the\n2024 Portuguese National Exam for medical specialty access (PNA), a\nstandardized medical knowledge assessment. Our results highlight considerable\nvariation in accuracy and cost-effectiveness, with several models demonstrating\nperformance exceeding human benchmarks for medical students on this specific\ntask. We identify leading models based on a combined score of accuracy and\ncost, discuss the implications of reasoning methodologies like\nChain-of-Thought, and underscore the potential for LLMs to function as valuable\ncomplementary tools aiding medical professionals in complex clinical\ndecision-making.",
      "pdf_url": "http://arxiv.org/pdf/2504.10405v1",
      "published": "2025-04-14T16:53:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10405v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "I.2.7; J.3"
      ]
    },
    {
      "title": "Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?",
      "authors": [
        "Olha Shaposhnyk",
        "Daria Zahorska",
        "Svetlana Yanushkevich"
      ],
      "abstract": "Objective: This study investigates the potential of Large Language Models\n(LLMs) as an alternative to human expert elicitation for extracting structured\ncausal knowledge and facilitating causal modeling in biometric and healthcare\napplications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian\nnetworks (BNs), were benchmarked against traditional statistical methods (e.g.,\nBayesian Information Criterion) using healthcare datasets. Validation\ntechniques included structural equation modeling (SEM) to verifying\nrelationships, and measures such as entropy, predictive accuracy, and\nrobustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than\nexpert-elicited and statistically generated BNs, suggesting higher confidence\nand precision in predictions. However, limitations such as contextual\nconstraints, hallucinated dependencies, and potential biases inherited from\ntraining data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for\nprobabilistic causal modeling, promising to improve transparency and reduce\nuncertainty in the decision-making using such models.",
      "pdf_url": "http://arxiv.org/pdf/2504.10397v1",
      "published": "2025-04-14T16:45:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10397v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Teacher Motion Priors: Enhancing Robot Locomotion over Challenging Terrain",
      "authors": [
        "Fangcheng Jin",
        "Yuqi Wang",
        "Peixin Ma",
        "Guodong Yang",
        "Pan Zhao",
        "En Li",
        "Zhengtao Zhang"
      ],
      "abstract": "Achieving robust locomotion on complex terrains remains a challenge due to\nhigh dimensional control and environmental uncertainties. This paper introduces\na teacher prior framework based on the teacher student paradigm, integrating\nimitation and auxiliary task learning to improve learning efficiency and\ngeneralization. Unlike traditional paradigms that strongly rely on\nencoder-based state embeddings, our framework decouples the network design,\nsimplifying the policy network and deployment. A high performance teacher\npolicy is first trained using privileged information to acquire generalizable\nmotion skills. The teacher's motion distribution is transferred to the student\npolicy, which relies only on noisy proprioceptive data, via a generative\nadversarial mechanism to mitigate performance degradation caused by\ndistributional shifts. Additionally, auxiliary task learning enhances the\nstudent policy's feature representation, speeding up convergence and improving\nadaptability to varying terrains. The framework is validated on a humanoid\nrobot, showing a great improvement in locomotion stability on dynamic terrains\nand significant reductions in development costs. This work provides a practical\nsolution for deploying robust locomotion strategies in humanoid robots.",
      "pdf_url": "http://arxiv.org/pdf/2504.10390v1",
      "published": "2025-04-14T16:36:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10390v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "68T40"
      ]
    },
    {
      "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning",
      "authors": [
        "Yiting Wang",
        "Wanghao Ye",
        "Ping Guo",
        "Yexiao He",
        "Ziyao Wang",
        "Yexiao He",
        "Bowei Tian",
        "Shwai He",
        "Guoheng Sun",
        "Zheyu Shen",
        "Sihan Chen",
        "Ankur Srivastava",
        "Qingfu Zhang",
        "Gang Qu",
        "Ang Li"
      ],
      "abstract": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2504.10369v1",
      "published": "2025-04-14T16:15:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10369v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ]
    },
    {
      "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models",
      "authors": [
        "Wenyuan Zhang",
        "Shuaiyi Nie",
        "Xinghua Zhang",
        "Zefeng Zhang",
        "Tingwen Liu"
      ],
      "abstract": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
      "pdf_url": "http://arxiv.org/pdf/2504.10368v1",
      "published": "2025-04-14T16:13:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10368v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos",
      "authors": [
        "Rui Chen",
        "Lei Sun",
        "Jing Tang",
        "Geng Li",
        "Xiangxiang Chu"
      ],
      "abstract": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon.",
      "pdf_url": "http://arxiv.org/pdf/2504.10358v1",
      "published": "2025-04-14T16:07:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10358v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families",
      "authors": [
        "Shahriar Noroozizadeh",
        "Sayantan Kumar",
        "Jeremy C. Weiss"
      ],
      "abstract": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use.",
      "pdf_url": "http://arxiv.org/pdf/2504.10340v1",
      "published": "2025-04-14T15:48:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10340v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Heimdall: test-time scaling on the generative verification",
      "authors": [
        "Wenlei Shi",
        "Xing Jin"
      ],
      "abstract": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
      "pdf_url": "http://arxiv.org/pdf/2504.10337v1",
      "published": "2025-04-14T15:46:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10337v1",
      "categories": [
        "cs.AI",
        "I.2.7"
      ]
    },
    {
      "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference",
      "authors": [
        "Yangshen Deng",
        "Zhengxin You",
        "Long Xiang",
        "Qilong Li",
        "Peiqi Yuan",
        "Zhaoyang Hong",
        "Yitao Zheng",
        "Wanting Li",
        "Runzhong Li",
        "Haotian Liu",
        "Kyriakos Mouratidis",
        "Man Lung Yiu",
        "Huan Li",
        "Qiaomu Shen",
        "Rui Mao",
        "Bo Tang"
      ],
      "abstract": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2504.10326v1",
      "published": "2025-04-14T15:34:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10326v1",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "H.3.1; H.3.2; H.3.3; H.3.4"
      ]
    },
    {
      "title": "AutoStyle-TTS: Retrieval-Augmented Generation based Automatic Style Matching Text-to-Speech Synthesis",
      "authors": [
        "Dan Luo",
        "Chengyuan Ma",
        "Weiqin Li",
        "Jun Wang",
        "Wei Chen",
        "Zhiyong Wu"
      ],
      "abstract": "With the advancement of speech synthesis technology, users have higher\nexpectations for the naturalness and expressiveness of synthesized speech. But\nprevious research ignores the importance of prompt selection. This study\nproposes a text-to-speech (TTS) framework based on Retrieval-Augmented\nGeneration (RAG) technology, which can dynamically adjust the speech style\naccording to the text content to achieve more natural and vivid communication\neffects. We have constructed a speech style knowledge database containing\nhigh-quality speech samples in various contexts and developed a style matching\nscheme. This scheme uses embeddings, extracted by Llama, PER-LLM-Embedder,and\nMoka, to match with samples in the knowledge database, selecting the most\nappropriate speech style for synthesis. Furthermore, our empirical research\nvalidates the effectiveness of the proposed method. Our demo can be viewed at:\nhttps://thuhcsi.github.io/icme2025-AutoStyle-TTS",
      "pdf_url": "http://arxiv.org/pdf/2504.10309v1",
      "published": "2025-04-14T15:18:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10309v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Characterizing LLM-driven Social Network: The Chirper.ai Case",
      "authors": [
        "Yiming Zhu",
        "Yupeng He",
        "Ehsan-Ul Haq",
        "Gareth Tyson",
        "Pan Hui"
      ],
      "abstract": "Large language models (LLMs) demonstrate the ability to simulate human\ndecision-making processes, enabling their use as agents in modeling\nsophisticated social networks, both offline and online. Recent research has\nexplored collective behavioral patterns and structural characteristics of LLM\nagents within simulated networks. However, empirical comparisons between\nLLM-driven and human-driven online social networks remain scarce, limiting our\nunderstanding of how LLM agents differ from human users. This paper presents a\nlarge-scale analysis of Chirper.ai, an X/Twitter-like social network entirely\npopulated by LLM agents, comprising over 65,000 agents and 7.7 million\nAI-generated posts. For comparison, we collect a parallel dataset from\nMastodon, a human-driven decentralized social network, with over 117,000 users\nand 16 million posts. We examine key differences between LLM agents and humans\nin posting behaviors, abusive content, and social network structures. Our\nfindings provide critical insights into the evolving landscape of online social\nnetwork analysis in the AI era, offering a comprehensive profile of LLM agents\nin social simulations.",
      "pdf_url": "http://arxiv.org/pdf/2504.10286v1",
      "published": "2025-04-14T14:53:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10286v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials",
      "authors": [
        "Jingyun Yang",
        "Ruoyan Avery Yin",
        "Chi Jiang",
        "Yuepeng Hu",
        "Xiaokai Zhu",
        "Xingjian Hu",
        "Sutharsika Kumar",
        "Xiao Wang",
        "Xiaohua Zhai",
        "Keran Rong",
        "Yunyue Zhu",
        "Tianyi Zhang",
        "Zongyou Yin",
        "Jing Kong",
        "Neil Zhenqiang Gong",
        "Zhichu Ren",
        "Haozhe Wang"
      ],
      "abstract": "Characterization of atomic-scale materials traditionally requires human\nexperts with months to years of specialized training. Even for trained human\noperators, accurate and reliable characterization remains challenging when\nexamining newly discovered materials such as two-dimensional (2D) structures.\nThis bottleneck drives demand for fully autonomous experimentation systems\ncapable of comprehending research objectives without requiring large training\ndatasets. In this work, we present ATOMIC (Autonomous Technology for Optical\nMicroscopy & Intelligent Characterization), an end-to-end framework that\nintegrates foundation models to enable fully autonomous, zero-shot\ncharacterization of 2D materials. Our system integrates the vision foundation\nmodel (i.e., Segment Anything Model), large language models (i.e., ChatGPT),\nunsupervised clustering, and topological analysis to automate microscope\ncontrol, sample scanning, image segmentation, and intelligent analysis through\nprompt engineering, eliminating the need for additional training. When\nanalyzing typical MoS2 samples, our approach achieves 99.7% segmentation\naccuracy for single layer identification, which is equivalent to that of human\nexperts. In addition, the integrated model is able to detect grain boundary\nslits that are challenging to identify with human eyes. Furthermore, the system\nretains robust accuracy despite variable conditions including defocus, color\ntemperature fluctuations, and exposure variations. It is applicable to a broad\nspectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless\nof whether they were fabricated via chemical vapor deposition or mechanical\nexfoliation. This work represents the implementation of foundation models to\nachieve autonomous analysis, establishing a scalable and data-efficient\ncharacterization paradigm that fundamentally transforms the approach to\nnanoscale materials research.",
      "pdf_url": "http://arxiv.org/pdf/2504.10281v1",
      "published": "2025-04-14T14:49:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10281v1",
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.mes-hall",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "RealHarm: A Collection of Real-World Language Model Application Failures",
      "authors": [
        "Pierre Le Jeune",
        "Jiaen Liu",
        "Luca Rossi",
        "Matteo Dora"
      ],
      "abstract": "Language model deployments in consumer-facing applications introduce numerous\nrisks. While existing research on harms and hazards of such applications\nfollows top-down approaches derived from regulatory frameworks and theoretical\nanalyses, empirical evidence of real-world failure modes remains underexplored.\nIn this work, we introduce RealHarm, a dataset of annotated problematic\ninteractions with AI agents built from a systematic review of publicly reported\nincidents. Analyzing harms, causes, and hazards specifically from the\ndeployer's perspective, we find that reputational damage constitutes the\npredominant organizational harm, while misinformation emerges as the most\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\ncontent moderation systems to probe whether such systems would have prevented\nthe incidents, revealing a significant gap in the protection of AI\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2504.10277v1",
      "published": "2025-04-14T14:44:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10277v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ]
    },
    {
      "title": "Vision based driving agent for race car simulation environments",
      "authors": [
        "Gergely Bári",
        "László Palkovics"
      ],
      "abstract": "In recent years, autonomous driving has become a popular field of study. As\ncontrol at tire grip limit is essential during emergency situations, algorithms\ndeveloped for racecars are useful for road cars too. This paper examines the\nuse of Deep Reinforcement Learning (DRL) to solve the problem of grip limit\ndriving in a simulated environment. Proximal Policy Optimization (PPO) method\nis used to train an agent to control the steering wheel and pedals of the\nvehicle, using only visual inputs to achieve professional human lap times. The\npaper outlines the formulation of the task of time optimal driving on a race\ntrack as a deep reinforcement learning problem, and explains the chosen\nobservations, actions, and reward functions. The results demonstrate human-like\nlearning and driving behavior that utilize maximum tire grip potential.",
      "pdf_url": "http://arxiv.org/pdf/2504.10266v1",
      "published": "2025-04-14T14:29:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10266v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track",
      "authors": [
        "Xuqiang Cao",
        "Linnan Zhao",
        "Jiaxuan Zhao",
        "Fang Liu",
        "Puhua Chen",
        "Wenping Ma"
      ],
      "abstract": "Complex video object segmentation continues to face significant challenges in\nsmall object recognition, occlusion handling, and dynamic scene modeling. This\nreport presents our solution, which ranked second in the MOSE track of CVPR\n2025 PVUW Challenge. Based on an existing segmentation framework, we propose an\nimproved model named MASSeg for complex video object segmentation, and\nconstruct an enhanced dataset, MOSE+, which includes typical scenarios with\nocclusions, cluttered backgrounds, and small target instances. During training,\nwe incorporate a combination of inter-frame consistent and inconsistent data\naugmentation strategies to improve robustness and generalization. During\ninference, we design a mask output scaling strategy to better adapt to varying\nobject sizes and occlusion levels. As a result, MASSeg achieves a J score of\n0.8250, F score of 0.9007, and a J&F score of 0.8628 on the MOSE test set.",
      "pdf_url": "http://arxiv.org/pdf/2504.10254v1",
      "published": "2025-04-14T14:15:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10254v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Can Competition Enhance the Proficiency of Agents Powered by Large Language Models in the Realm of News-driven Time Series Forecasting?",
      "authors": [
        "Yuxuan Zhang",
        "Yangyang Feng",
        "Daifeng Li",
        "Kexin Zhang",
        "Junlan Chen",
        "Bowen Deng"
      ],
      "abstract": "Multi-agents-based news-driven time series forecasting is considered as a\npotential paradigm shift in the era of large language models (LLMs). The\nchallenge of this task lies in measuring the influences of different news\nevents towards the fluctuations of time series. This requires agents to possess\nstronger abilities of innovative thinking and the identifying misleading logic.\nHowever, the existing multi-agent discussion framework has limited enhancement\non time series prediction in terms of optimizing these two capabilities.\nInspired by the role of competition in fostering innovation, this study embeds\na competition mechanism within the multi-agent discussion to enhance agents'\ncapability of generating innovative thoughts. Furthermore, to bolster the\nmodel's proficiency in identifying misleading information, we incorporate a\nfine-tuned small-scale LLM model within the reflective stage, offering\nauxiliary decision-making support. Experimental results confirm that the\ncompetition can boost agents' capacity for innovative thinking, which can\nsignificantly improve the performances of time series prediction. Similar to\nthe findings of social science, the intensity of competition within this\nframework can influence the performances of agents, providing a new perspective\nfor studying LLMs-based multi-agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2504.10210v1",
      "published": "2025-04-14T13:25:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10210v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Localized Cultural Knowledge is Conserved and Controllable in Large Language Models",
      "authors": [
        "Veniamin Veselovsky",
        "Berke Argin",
        "Benedikt Stroebl",
        "Chris Wendler",
        "Robert West",
        "James Evans",
        "Thomas L. Griffiths",
        "Arvind Narayanan"
      ],
      "abstract": "Just as humans display language patterns influenced by their native tongue\nwhen speaking new languages, LLMs often default to English-centric responses\neven when generating in other languages. Nevertheless, we observe that local\ncultural information persists within the models and can be readily activated\nfor cultural customization. We first demonstrate that explicitly providing\ncultural context in prompts significantly improves the models' ability to\ngenerate culturally localized responses. We term the disparity in model\nperformance with versus without explicit cultural context the explicit-implicit\nlocalization gap, indicating that while cultural knowledge exists within LLMs,\nit may not naturally surface in multilingual interactions if cultural context\nis not explicitly provided. Despite the explicit prompting benefit, however,\nthe answers reduce in diversity and tend toward stereotypes. Second, we\nidentify an explicit cultural customization vector, conserved across all\nnon-English languages we explore, which enables LLMs to be steered from the\nsynthetic English cultural world-model toward each non-English cultural world.\nSteered responses retain the diversity of implicit prompting and reduce\nstereotypes to dramatically improve the potential for customization. We discuss\nthe implications of explicit cultural customization for understanding the\nconservation of alternative cultural world models within LLMs, and their\ncontrollable utility for translation, cultural customization, and the\npossibility of making the explicit implicit through soft control for expanded\nLLM function and appeal.",
      "pdf_url": "http://arxiv.org/pdf/2504.10191v1",
      "published": "2025-04-14T12:53:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10191v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Efficient Generative Model Training via Embedded Representation Warmup",
      "authors": [
        "Deyuan Liu",
        "Peng Sun",
        "Xufeng Li",
        "Tao Lin"
      ],
      "abstract": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40$\\times$\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
      "pdf_url": "http://arxiv.org/pdf/2504.10188v1",
      "published": "2025-04-14T12:43:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10188v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Deep Reasoning Translation via Reinforcement Learning",
      "authors": [
        "Jiaan Wang",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "abstract": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation.",
      "pdf_url": "http://arxiv.org/pdf/2504.10187v1",
      "published": "2025-04-14T12:40:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10187v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
      "authors": [
        "Soumyadeep Pal",
        "Changsheng Wang",
        "James Diffenderfer",
        "Bhavya Kailkhura",
        "Sijia Liu"
      ],
      "abstract": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.",
      "pdf_url": "http://arxiv.org/pdf/2504.10185v1",
      "published": "2025-04-14T12:38:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10185v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance",
      "authors": [
        "Anwesha Mohanty",
        "Venkatesh Balavadhani Parthasarathy",
        "Arsalan Shahid"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding.",
      "pdf_url": "http://arxiv.org/pdf/2504.10179v1",
      "published": "2025-04-14T12:31:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10179v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.ET"
      ]
    },
    {
      "title": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection",
      "authors": [
        "Mohamed A. Abdallah",
        "Samhaa R. El-Beltagy"
      ],
      "abstract": "In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts.",
      "pdf_url": "http://arxiv.org/pdf/2504.10168v1",
      "published": "2025-04-14T12:22:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10168v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination Evaluation",
      "authors": [
        "Xu Zhang",
        "Zhifei Liu",
        "Jiahao Wang",
        "Huixuan Zhang",
        "Fan Xu",
        "Junzhe Zhang",
        "Xiaojun Wan"
      ],
      "abstract": "Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis.",
      "pdf_url": "http://arxiv.org/pdf/2504.10167v1",
      "published": "2025-04-14T12:21:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10167v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs",
      "authors": [
        "Nguyen Ngoc Dat",
        "Tom Richardson",
        "Matthew Watson",
        "Kilian Meier",
        "Jenna Kline",
        "Sid Reid",
        "Guy Maalouf",
        "Duncan Hine",
        "Majid Mirmehdi",
        "Tilo Burghardt"
      ],
      "abstract": "Live tracking of wildlife via high-resolution video processing directly\nonboard drones is widely unexplored and most existing solutions rely on\nstreaming video to ground stations to support navigation. Yet, both autonomous\nanimal-reactive flight control beyond visual line of sight and/or\nmission-specific individual and behaviour recognition tasks rely to some degree\non this capability. In response, we introduce WildLive -- a near real-time\nanimal detection and tracking framework for high-resolution imagery running\ndirectly onboard uncrewed aerial vehicles (UAVs). The system performs\nmulti-animal detection and tracking at 17fps+ for HD and 7fps+ on 4K video\nstreams suitable for operation during higher altitude flights to minimise\nanimal disturbance. Our system is optimised for Jetson Orin AGX onboard\nhardware. It integrates the efficiency of sparse optical flow tracking and\nmission-specific sampling with device-optimised and proven YOLO-driven object\ndetection and segmentation techniques. Essentially, computational resource is\nfocused onto spatio-temporal regions of high uncertainty to significantly\nimprove UAV processing speeds without domain-specific loss of accuracy.\nAlongside, we introduce our WildLive dataset, which comprises 200k+ annotated\nanimal instances across 19k+ frames from 4K UAV videos collected at the Ol\nPejeta Conservancy in Kenya. All frames contain ground truth bounding boxes,\nsegmentation masks, as well as individual tracklets and tracking point\ntrajectories. We compare our system against current object tracking approaches\nincluding OC-SORT, ByteTrack, and SORT. Our materials are available at:\nhttps://dat-nguyenvn.github.io/WildLive/",
      "pdf_url": "http://arxiv.org/pdf/2504.10165v2",
      "published": "2025-04-14T12:21:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10165v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning",
      "authors": [
        "Zhaopeng Feng",
        "Shaosheng Cao",
        "Jiahan Ren",
        "Jiayuan Su",
        "Ruizhe Chen",
        "Yan Zhang",
        "Zhe Xu",
        "Yao Hu",
        "Jian Wu",
        "Zuozhu Liu"
      ],
      "abstract": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.",
      "pdf_url": "http://arxiv.org/pdf/2504.10160v1",
      "published": "2025-04-14T12:14:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10160v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts",
      "authors": [
        "Jiansheng Li",
        "Xingxuan Zhang",
        "Hao Zou",
        "Yige Guo",
        "Renzhe Xu",
        "Yilong Liu",
        "Chuzhao Zhu",
        "Yue He",
        "Peng Cui"
      ],
      "abstract": "Current object detectors often suffer significant perfor-mance degradation in\nreal-world applications when encountering distributional shifts. Consequently,\nthe out-of-distribution (OOD) generalization capability of object detectors has\ngarnered increasing attention from researchers. Despite this growing interest,\nthere remains a lack of a large-scale, comprehensive dataset and evaluation\nbenchmark with fine-grained annotations tailored to assess the OOD\ngeneralization on more intricate tasks like object detection and grounding. To\naddress this gap, we introduce COUNTS, a large-scale OOD dataset with\nobject-level annotations. COUNTS encompasses 14 natural distributional shifts,\nover 222K samples, and more than 1,196K labeled bounding boxes. Leveraging\nCOUNTS, we introduce two novel benchmarks: O(OD)2 and OODG. O(OD)2 is designed\nto comprehensively evaluate the OOD generalization capabilities of object\ndetectors by utilizing controlled distribution shifts between training and\ntesting data. OODG, on the other hand, aims to assess the OOD generalization of\ngrounding abilities in multimodal large language models (MLLMs). Our findings\nreveal that, while large models and extensive pre-training data substantially\nen hance performance in in-distribution (IID) scenarios, significant\nlimitations and opportunities for improvement persist in OOD contexts for both\nobject detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o\nand Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope\nCOUNTS facilitates advancements in the development and assessment of robust\nobject detectors and MLLMs capable of maintaining high performance under\ndistributional shifts.",
      "pdf_url": "http://arxiv.org/pdf/2504.10158v1",
      "published": "2025-04-14T12:13:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10158v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "BoTTA: Benchmarking on-device Test Time Adaptation",
      "authors": [
        "Michal Danilowski",
        "Soumyajit Chatterjee",
        "Abhirup Ghosh"
      ],
      "abstract": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments.",
      "pdf_url": "http://arxiv.org/pdf/2504.10149v1",
      "published": "2025-04-14T12:00:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10149v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions",
      "authors": [
        "Jo-Ku Cheng",
        "Zeren Zhang",
        "Ran Chen",
        "Jingyang Deng",
        "Ziran Qin",
        "Jinwen Ma"
      ],
      "abstract": "We propose GeoUni, the first unified geometry expert model capable of\ngenerating problem solutions and diagrams within a single framework in a way\nthat enables the creation of unique and individualized geometry problems.\nTraditionally, solving geometry problems and generating diagrams have been\ntreated as separate tasks in machine learning, with no models successfully\nintegrating both to support problem creation. However, we believe that mastery\nin geometry requires frictionless integration of all of these skills, from\nsolving problems to visualizing geometric relationships, and finally, crafting\ntailored problems. Our extensive experiments demonstrate that GeoUni, with only\n1.5B parameters, achieves performance comparable to larger models such as\nDeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also\nexcels in generating precise geometric diagrams, surpassing both text-to-image\nmodels and unified models, including the GPT-4o image generation. Most\nimportantly, GeoUni is the only model capable of successfully generating\ntextual problems with matching diagrams based on specific knowledge points,\nthus offering a wider range of capabilities that extend beyond current models.",
      "pdf_url": "http://arxiv.org/pdf/2504.10146v1",
      "published": "2025-04-14T11:56:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10146v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Breaking the Data Barrier -- Building GUI Agents Through Task Generalization",
      "authors": [
        "Junlei Zhang",
        "Zichen Ding",
        "Chang Ma",
        "Zijie Chen",
        "Qiushi Sun",
        "Zhenzhong Lan",
        "Junxian He"
      ],
      "abstract": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.",
      "pdf_url": "http://arxiv.org/pdf/2504.10127v1",
      "published": "2025-04-14T11:35:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10127v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design",
      "authors": [
        "Andreas Happe",
        "Jürgen Cito"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. This paper analyzes the methodology and\nbenchmarking practices used for evaluating Large Language Model (LLM)-driven\nattacks, focusing on offensive uses of LLMs in cybersecurity. We review 16\nresearch papers detailing 15 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2504.10112v1",
      "published": "2025-04-14T11:21:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10112v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Lightweight Trustworthy Distributed Clustering",
      "authors": [
        "Hongyang Li",
        "Caesar Wu",
        "Mohammed Chadli",
        "Said Mammar",
        "Pascal Bouvry"
      ],
      "abstract": "Ensuring data trustworthiness within individual edge nodes while facilitating\ncollaborative data processing poses a critical challenge in edge computing\nsystems (ECS), particularly in resource-constrained scenarios such as\nautonomous systems sensor networks, industrial IoT, and smart cities. This\npaper presents a lightweight, fully distributed k-means clustering algorithm\nspecifically adapted for edge environments, leveraging a distributed averaging\napproach with additive secret sharing, a secure multiparty computation\ntechnique, during the cluster center update phase to ensure the accuracy and\ntrustworthiness of data across nodes.",
      "pdf_url": "http://arxiv.org/pdf/2504.10109v1",
      "published": "2025-04-14T11:16:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10109v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "SoccerNet-v3D: Leveraging Sports Broadcast Replays for 3D Scene Understanding",
      "authors": [
        "Marc Gutiérrez-Pérez",
        "Antonio Agudo"
      ],
      "abstract": "Sports video analysis is a key domain in computer vision, enabling detailed\nspatial understanding through multi-view correspondences. In this work, we\nintroduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets\ndesigned for 3D scene understanding in soccer broadcast analysis. These\ndatasets extend SoccerNet-v3 and ISSIA by incorporating field-line-based camera\ncalibration and multi-view synchronization, enabling 3D object localization\nthrough triangulation. We propose a monocular 3D ball localization task built\nupon the triangulation of ground-truth 2D ball annotations, along with several\ncalibration and reprojection metrics to assess annotation quality on demand.\nAdditionally, we present a single-image 3D ball localization method as a\nbaseline, leveraging camera calibration and ball size priors to estimate the\nball's position from a monocular viewpoint. To further refine 2D annotations,\nwe introduce a bounding box optimization technique that ensures alignment with\nthe 3D scene representation. Our proposed datasets establish new benchmarks for\n3D soccer scene understanding, enhancing both spatial and temporal analysis in\nsports analytics. Finally, we provide code to facilitate access to our\nannotations and the generation pipelines for the datasets.",
      "pdf_url": "http://arxiv.org/pdf/2504.10106v1",
      "published": "2025-04-14T11:15:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10106v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2; I.4; I.5"
      ]
    },
    {
      "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability",
      "authors": [
        "Yichi Zhang",
        "Zihao Zeng",
        "Dongbai Li",
        "Yao Huang",
        "Zhijie Deng",
        "Yinpeng Dong"
      ],
      "abstract": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\nrapidly progressing and achieving breakthrough performance on complex reasoning\ntasks such as mathematics and coding. However, the open-source R1 models have\nraised safety concerns in wide applications, such as the tendency to comply\nwith malicious queries, which greatly impacts the utility of these powerful\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\nquantitative experiments and qualitative case studies demonstrate the models'\nimprovements, which are shown in their safety guardrails against both harmful\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\nefforts that often compromise reasoning performance, our method preserves the\nmodels' reasoning capabilities by maintaining the training data within the\noriginal distribution of generation. Model weights of RealSafe-R1 are\nopen-source at https://huggingface.co/RealSafe.",
      "pdf_url": "http://arxiv.org/pdf/2504.10081v1",
      "published": "2025-04-14T10:26:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10081v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Towards Quantifying Commonsense Reasoning with Mechanistic Insights",
      "authors": [
        "Abhinav Joshi",
        "Areeb Ahmad",
        "Divyaksh Shukla",
        "Ashutosh Modi"
      ],
      "abstract": "Commonsense reasoning deals with the implicit knowledge that is well\nunderstood by humans and typically acquired via interactions with the world. In\nrecent times, commonsense reasoning and understanding of various LLMs have been\nevaluated using text-based tasks. In this work, we argue that a proxy of this\nunderstanding can be maintained as a graphical structure that can further help\nto perform a rigorous evaluation of commonsense reasoning abilities about\nvarious real-world activities. We create an annotation scheme for capturing\nthis implicit knowledge in the form of a graphical structure for 37 daily human\nactivities. We find that the created resource can be used to frame an enormous\nnumber of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of\ncommonsense reasoning in LLMs. Moreover, recently, the remarkable performance\nof LLMs has raised questions about whether these models are truly capable of\nreasoning in the wild and, in general, how reasoning occurs inside these\nmodels. In this resource paper, we bridge this gap by proposing design\nmechanisms that facilitate research in a similar direction. Our findings\nsuggest that the reasoning components are localized in LLMs that play a\nprominent role in decision-making when prompted with a commonsense query.",
      "pdf_url": "http://arxiv.org/pdf/2504.10077v1",
      "published": "2025-04-14T10:21:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10077v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MMKB-RAG: A Multi-Modal Knowledge-Based Retrieval-Augmented Generation Framework",
      "authors": [
        "Zihan Ling",
        "Zhiyao Guo",
        "Yixuan Huang",
        "Yi An",
        "Shuai Xiao",
        "Jinsong Lan",
        "Xiaoyong Zhu",
        "Bo Zheng"
      ],
      "abstract": "Recent advancements in large language models (LLMs) and multi-modal LLMs have\nbeen remarkable. However, these models still rely solely on their parametric\nknowledge, which limits their ability to generate up-to-date information and\nincreases the risk of producing erroneous content. Retrieval-Augmented\nGeneration (RAG) partially mitigates these challenges by incorporating external\ndata sources, yet the reliance on databases and retrieval systems can introduce\nirrelevant or inaccurate documents, ultimately undermining both performance and\nreasoning quality. In this paper, we propose Multi-Modal Knowledge-Based\nRetrieval-Augmented Generation (MMKB-RAG), a novel multi-modal RAG framework\nthat leverages the inherent knowledge boundaries of models to dynamically\ngenerate semantic tags for the retrieval process. This strategy enables the\njoint filtering of retrieved documents, retaining only the most relevant and\naccurate references. Extensive experiments on knowledge-based visual\nquestion-answering tasks demonstrate the efficacy of our approach: on the E-VQA\ndataset, our method improves performance by +4.2% on the Single-Hop subset and\n+0.4% on the full dataset, while on the InfoSeek dataset, it achieves gains of\n+7.8% on the Unseen-Q subset, +8.2% on the Unseen-E subset, and +8.1% on the\nfull dataset. These results highlight significant enhancements in both accuracy\nand robustness over the current state-of-the-art MLLM and RAG frameworks.",
      "pdf_url": "http://arxiv.org/pdf/2504.10074v2",
      "published": "2025-04-14T10:19:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10074v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning",
      "authors": [
        "Tien Pham",
        "Angelo Cangelosi"
      ],
      "abstract": "Current approaches in Explainable Deep Reinforcement Learning have\nlimitations in which the attention mask has a displacement with the objects in\nvisual input. This work addresses a spatial problem within traditional\nConvolutional Neural Networks (CNNs). We propose the Interpretable Feature\nExtractor (IFE) architecture, aimed at generating an accurate attention mask to\nillustrate both \"what\" and \"where\" the agent concentrates on in the spatial\ndomain. Our design incorporates a Human-Understandable Encoding module to\ngenerate a fully interpretable attention mask, followed by an Agent-Friendly\nEncoding module to enhance the agent's learning efficiency. These two\ncomponents together form the Interpretable Feature Extractor for vision-based\ndeep reinforcement learning to enable the model's interpretability. The\nresulting attention mask is consistent, highly understandable by humans,\naccurate in spatial dimension, and effectively highlights important objects or\nlocations in visual input. The Interpretable Feature Extractor is integrated\ninto the Fast and Data-efficient Rainbow framework, and evaluated on 57 ATARI\ngames to show the effectiveness of the proposed approach on Spatial\nPreservation, Interpretability, and Data-efficiency. Finally, we showcase the\nversatility of our approach by incorporating the IFE into the Asynchronous\nAdvantage Actor-Critic Model.",
      "pdf_url": "http://arxiv.org/pdf/2504.10071v1",
      "published": "2025-04-14T10:18:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10071v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Mavors: Multi-granularity Video Representation for Multimodal Large Language Model",
      "authors": [
        "Yang Shi",
        "Jiaheng Liu",
        "Yushuo Guan",
        "Zhenhua Wu",
        "Yuanxing Zhang",
        "Zihao Wang",
        "Weihong Lin",
        "Jingyun Hua",
        "Zekun Wang",
        "Xinlong Chen",
        "Bohan Zeng",
        "Wentao Zhang",
        "Fuzheng Zhang",
        "Wenjing Yang",
        "Di Zhang"
      ],
      "abstract": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose $\\mathbf{Mavors}$, a novel framework\nthat introduces $\\mathbf{M}$ulti-gr$\\mathbf{a}$nularity\n$\\mathbf{v}$ide$\\mathbf{o}$ $\\mathbf{r}$epre$\\mathbf{s}$entation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2504.10068v1",
      "published": "2025-04-14T10:14:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10068v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Hallucination Detection in LLMs via Topological Divergence on Attention Graphs",
      "authors": [
        "Alexandra Bazarova",
        "Aleksandr Yugay",
        "Andrey Shulga",
        "Alina Ermilova",
        "Andrei Volodichev",
        "Konstantin Polev",
        "Julia Belikova",
        "Rauf Parchiev",
        "Dmitry Simakov",
        "Maxim Savchenko",
        "Andrey Savchenko",
        "Serguei Barannikov",
        "Alexey Zaytsev"
      ],
      "abstract": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments, including evaluation on\nquestion answering and data-to-text tasks, show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks, two of which\nwere annotated by us and are being publicly released to facilitate further\nresearch. Beyond its strong in-domain performance, TOHA maintains remarkable\ndomain transferability across multiple open-source LLMs. Our findings suggest\nthat analyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2504.10063v1",
      "published": "2025-04-14T10:06:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10063v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CHARM: Calibrating Reward Models With Chatbot Arena Scores",
      "authors": [
        "Xiao Zhu",
        "Chenmien Tan",
        "Pinzhen Chen",
        "Rico Sennrich",
        "Yanlin Zhang",
        "Hanxu Hu"
      ],
      "abstract": "Reward models (RMs) play a crucial role in Reinforcement Learning from Human\nFeedback by serving as proxies for human preferences in aligning large language\nmodels. In this paper, we identify a model preference bias in RMs, where they\nsystematically assign disproportionately high scores to responses from certain\npolicy models. This bias distorts ranking evaluations and leads to unfair\njudgments. To address this issue, we propose a calibration method named CHatbot\nArena calibrated Reward Modeling (CHARM) that leverages Elo scores from the\nChatbot Arena leaderboard to mitigate RM overvaluation. We also introduce a\nMismatch Degree metric to measure this preference bias. Our approach is\ncomputationally efficient, requiring only a small preference dataset for\ncontinued training of the RM. We conduct extensive experiments on reward model\nbenchmarks and human preference alignment. Results demonstrate that our\ncalibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the\nChat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with\nhuman preferences by producing scores more closely aligned with Elo rankings.\nBy mitigating model preference bias, our method provides a generalizable and\nefficient solution for building fairer and more reliable reward models.",
      "pdf_url": "http://arxiv.org/pdf/2504.10045v1",
      "published": "2025-04-14T09:51:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10045v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "EmbodiedAgent: A Scalable Hierarchical Approach to Overcome Practical Challenge in Multi-Robot Control",
      "authors": [
        "Hanwen Wan",
        "Yifei Chen",
        "Zeyu Wei",
        "Dongrui Li",
        "Zexin Lin",
        "Donghao Wu",
        "Jiu Cheng",
        "Yuxiang Zhang",
        "Xiaoqiang Ji"
      ],
      "abstract": "This paper introduces EmbodiedAgent, a hierarchical framework for\nheterogeneous multi-robot control. EmbodiedAgent addresses critical limitations\nof hallucination in impractical tasks. Our approach integrates a next-action\nprediction paradigm with a structured memory system to decompose tasks into\nexecutable robot skills while dynamically validating actions against\nenvironmental constraints. We present MultiPlan+, a dataset of more than 18,000\nannotated planning instances spanning 100 scenarios, including a subset of\nimpractical cases to mitigate hallucination. To evaluate performance, we\npropose the Robot Planning Assessment Schema (RPAS), combining automated\nmetrics with LLM-aided expert grading. Experiments demonstrate EmbodiedAgent's\nsuperiority over state-of-the-art models, achieving 71.85% RPAS score.\nReal-world validation in an office service task highlights its ability to\ncoordinate heterogeneous robots for long-horizon objectives.",
      "pdf_url": "http://arxiv.org/pdf/2504.10030v1",
      "published": "2025-04-14T09:33:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10030v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Sequence models for by-trial decoding of cognitive strategies from neural data",
      "authors": [
        "Rick den Otter",
        "Gabriel Weindel",
        "Sjoerd Stuit",
        "Leendert van Maanen"
      ],
      "abstract": "Understanding the sequence of cognitive operations that underlie\ndecision-making is a fundamental challenge in cognitive neuroscience.\nTraditional approaches often rely on group-level statistics, which obscure\ntrial-by-trial variations in cognitive strategies. In this study, we introduce\na novel machine learning method that combines Hidden Multivariate Pattern\nanalysis with a Structured State Space Sequence model to decode cognitive\nstrategies from electroencephalography data at the trial level. We apply this\nmethod to a decision-making task, where participants were instructed to\nprioritize either speed or accuracy in their responses. Our results reveal an\nadditional cognitive operation, labeled Confirmation, which seems to occur\npredominantly in the accuracy condition but also frequently in the speed\ncondition. The modeled probability that this operation occurs is associated\nwith higher probability of responding correctly as well as changes of mind, as\nindexed by electromyography data. By successfully modeling cognitive operations\nat the trial level, we provide empirical evidence for dynamic variability in\ndecision strategies, challenging the assumption of homogeneous cognitive\nprocesses within experimental conditions. Our approach shows the potential of\nsequence modeling in cognitive neuroscience to capture trial-level variability\nthat is obscured by aggregate analyses. The introduced method offers a new way\nto detect and understand cognitive strategies in a data-driven manner, with\nimplications for both theoretical research and practical applications in many\nfields.",
      "pdf_url": "http://arxiv.org/pdf/2504.10028v1",
      "published": "2025-04-14T09:33:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10028v1",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ]
    },
    {
      "title": "Progressive Transfer Learning for Multi-Pass Fundus Image Restoration",
      "authors": [
        "Uyen Phan",
        "Ozer Can Devecioglu",
        "Serkan Kiranyaz",
        "Moncef Gabbouj"
      ],
      "abstract": "Diabetic retinopathy is a leading cause of vision impairment, making its\nearly diagnosis through fundus imaging critical for effective treatment\nplanning. However, the presence of poor quality fundus images caused by factors\nsuch as inadequate illumination, noise, blurring and other motion artifacts\nyields a significant challenge for accurate DR screening. In this study, we\npropose progressive transfer learning for multi pass restoration to iteratively\nenhance the quality of degraded fundus images, ensuring more reliable DR\nscreening. Unlike previous methods that often focus on a single pass\nrestoration, multi pass restoration via PTL can achieve a superior blind\nrestoration performance that can even improve most of the good quality fundus\nimages in the dataset. Initially, a Cycle GAN model is trained to restore low\nquality images, followed by PTL induced restoration passes over the latest\nrestored outputs to improve overall quality in each pass. The proposed method\ncan learn blind restoration without requiring any paired data while surpassing\nits limitations by leveraging progressive learning and fine tuning strategies\nto minimize distortions and preserve critical retinal features. To evaluate\nPTL's effectiveness on multi pass restoration, we conducted experiments on\nDeepDRiD, a large scale fundus imaging dataset specifically curated for\ndiabetic retinopathy detection. Our result demonstrates state of the art\nperformance, showcasing PTL's potential as a superior approach to iterative\nimage quality restoration.",
      "pdf_url": "http://arxiv.org/pdf/2504.10025v1",
      "published": "2025-04-14T09:28:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10025v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination",
      "authors": [
        "Hao Yin",
        "Gunagzong Si",
        "Zilei Wang"
      ],
      "abstract": "Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2504.10020v1",
      "published": "2025-04-14T09:25:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.10020v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    }
  ]
}