{
  "last_updated": "2025-11-08T00:48:20.063565",
  "papers": [
    {
      "title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations",
      "authors": [
        "Maximus A. Pace",
        "Prithwish Dan",
        "Chuanruo Ning",
        "Atiksh Bhardwaj",
        "Audrey Du",
        "Edward W. Duan",
        "Wei-Chiu Ma",
        "Kushal Kedia"
      ],
      "abstract": "Human videos can be recorded quickly and at scale, making them an appealing\nsource of training data for robot learning. However, humans and robots differ\nfundamentally in embodiment, resulting in mismatched action execution. Direct\nkinematic retargeting of human hand motion can therefore produce actions that\nare physically infeasible for robots. Despite these low-level differences,\nhuman demonstrations provide valuable motion cues about how to manipulate and\ninteract with objects. Our key idea is to exploit the forward diffusion\nprocess: as noise is added to actions, low-level execution differences fade\nwhile high-level task guidance is preserved. We present X-Diffusion, a\nprincipled framework for training diffusion policies that maximally leverages\nhuman data without learning dynamically infeasible motions. X-Diffusion first\ntrains a classifier to predict whether a noisy action is executed by a human or\nrobot. Then, a human action is incorporated into policy training only after\nadding sufficient noise such that the classifier cannot discern its embodiment.\nActions consistent with robot execution supervise fine-grained denoising at low\nnoise levels, while mismatched human actions provide only coarse guidance at\nhigher noise levels. Our experiments show that naive co-training under\nexecution mismatches degrades policy performance, while X-Diffusion\nconsistently improves it. Across five manipulation tasks, X-Diffusion achieves\na 16% higher average success rate than the best baseline. The project website\nis available at https://portal-cornell.github.io/X-Diffusion/.",
      "pdf_url": "http://arxiv.org/pdf/2511.04671v1",
      "published": "2025-11-06T18:56:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04671v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks",
      "authors": [
        "Yu Feng",
        "Nathaniel Weir",
        "Kaj Bostrom",
        "Sam Bayless",
        "Darion Cassel",
        "Sapana Chaudhary",
        "Benjamin Kiesl-Reiter",
        "Huzefa Rangwala"
      ],
      "abstract": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2511.04662v1",
      "published": "2025-11-06T18:50:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04662v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration",
      "authors": [
        "Narjes Nourzad",
        "Hanqing Yang",
        "Shiyu Chen",
        "Carlee Joe-Wong"
      ],
      "abstract": "Cooperative multi-agent planning requires agents to make joint decisions with\npartial information and limited communication. Coordination at the trajectory\nlevel often fails, as small deviations in timing or movement cascade into\nconflicts. Symbolic planning mitigates this challenge by raising the level of\nabstraction and providing a minimal vocabulary of actions that enable\nsynchronization and collective progress. We present DR. WELL, a decentralized\nneurosymbolic framework for cooperative multi-agent planning. Cooperation\nunfolds through a two-phase negotiation protocol: agents first propose\ncandidate roles with reasoning and then commit to a joint allocation under\nconsensus and environment constraints. After commitment, each agent\nindependently generates and executes a symbolic plan for its role without\nrevealing detailed trajectories. Plans are grounded in execution outcomes via a\nshared world model that encodes the current state and is updated as agents act.\nBy reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids\nbrittle step-level alignment and enables higher-level operations that are\nreusable, synchronizable, and interpretable. Experiments on cooperative\nblock-push tasks show that agents adapt across episodes, with the dynamic world\nmodel capturing reusable patterns and improving task completion rates and\nefficiency. Experiments on cooperative block-push tasks show that our dynamic\nworld model improves task completion and efficiency through negotiation and\nself-refinement, trading a time overhead for evolving, more efficient\ncollaboration strategies.",
      "pdf_url": "http://arxiv.org/pdf/2511.04646v1",
      "published": "2025-11-06T18:37:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04646v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Addressing divergent representations from causal interventions on neural networks",
      "authors": [
        "Satchel Grant",
        "Simon Jerome Han",
        "Alexa Tartaglini",
        "Christopher Potts"
      ],
      "abstract": "A common approach to mechanistic interpretability is to causally manipulate\nmodel representations via targeted interventions in order to understand what\nthose representations encode. Here we ask whether such interventions create\nout-of-distribution (divergent) representations, and whether this raises\nconcerns about how faithful their resulting explanations are to the target\nmodel in its natural state. First, we demonstrate empirically that common\ncausal intervention techniques often do shift internal representations away\nfrom the natural distribution of the target model. Then, we provide a\ntheoretical analysis of two classes of such divergences: `harmless' divergences\nthat occur in the null-space of the weights and from covariance within\nbehavioral decision boundaries, and `pernicious' divergences that activate\nhidden network pathways and cause dormant behavioral changes. Finally, in an\neffort to mitigate the pernicious cases, we modify the Counterfactual Latent\n(CL) loss from Grant (2025) that regularizes interventions to remain closer to\nthe natural distributions, reducing the likelihood of harmful divergences while\npreserving the interpretive power of interventions. Together, these results\nhighlight a path towards more reliable interpretability methods.",
      "pdf_url": "http://arxiv.org/pdf/2511.04638v1",
      "published": "2025-11-06T18:32:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04638v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Question the Questions: Auditing Representation in Online Deliberative Processes",
      "authors": [
        "Soham De",
        "Lodewijk Gelauff",
        "Ashish Goel",
        "Smitha Milli",
        "Ariel Procaccia",
        "Alice Siu"
      ],
      "abstract": "A central feature of many deliberative processes, such as citizens'\nassemblies and deliberative polls, is the opportunity for participants to\nengage directly with experts. While participants are typically invited to\npropose questions for expert panels, only a limited number can be selected due\nto time constraints. This raises the challenge of how to choose a small set of\nquestions that best represent the interests of all participants. We introduce\nan auditing framework for measuring the level of representation provided by a\nslate of questions, based on the social choice concept known as justified\nrepresentation (JR). We present the first algorithms for auditing JR in the\ngeneral utility setting, with our most efficient algorithm achieving a runtime\nof $O(mn\\log n)$, where $n$ is the number of participants and $m$ is the number\nof proposed questions. We apply our auditing methods to historical\ndeliberations, comparing the representativeness of (a) the actual questions\nposed to the expert panel (chosen by a moderator), (b) participants' questions\nchosen via integer linear programming, (c) summary questions generated by large\nlanguage models (LLMs). Our results highlight both the promise and current\nlimitations of LLMs in supporting deliberative processes. By integrating our\nmethods into an online deliberation platform that has been used for over\nhundreds of deliberations across more than 50 countries, we make it easy for\npractitioners to audit and improve representation in future deliberations.",
      "pdf_url": "http://arxiv.org/pdf/2511.04588v1",
      "published": "2025-11-06T17:45:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04588v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis",
      "authors": [
        "Daniel Gomm",
        "Cornelius Wolff",
        "Madelon Hulsebos"
      ],
      "abstract": "Natural language interfaces to tabular data must handle ambiguities inherent\nto queries. Instead of treating ambiguity as a deficiency, we reframe it as a\nfeature of cooperative interaction, where the responsibility of query\nspecification is shared among the user and the system. We develop a principled\nframework distinguishing cooperative queries, i.e., queries that yield a\nresolvable interpretation, from uncooperative queries that cannot be resolved.\nApplying the framework to evaluations for tabular question answering and\nanalysis, we analyze the queries in 15 popular datasets, and observe an\nuncontrolled mixing of query types neither adequate for evaluating a system's\nexecution accuracy nor for evaluating interpretation capabilities. Our\nframework and analysis of queries shifts the perspective from fixing ambiguity\nto embracing cooperation in resolving queries. This reflection enables more\ninformed design and evaluation for natural language interfaces for tabular\ndata, for which we outline implications and directions for future research.",
      "pdf_url": "http://arxiv.org/pdf/2511.04584v1",
      "published": "2025-11-06T17:39:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04584v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DB",
        "cs.HC"
      ]
    },
    {
      "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper",
      "authors": [
        "Atsuyuki Miyai",
        "Mashiro Toyooka",
        "Takashi Otonari",
        "Zaiying Zhao",
        "Kiyoharu Aizawa"
      ],
      "abstract": "Understanding the current capabilities and risks of AI Scientist systems is\nessential for ensuring trustworthy and sustainable AI-driven scientific\nprogress while preserving the integrity of the academic ecosystem. To this end,\nwe develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system\nthat mimics the core research workflow of a novice student researcher: Given\nthe baseline paper from the human mentor, it analyzes its limitations,\nformulates novel hypotheses for improvement, validates them through rigorous\nexperimentation, and writes a paper with the results. Unlike previous\napproaches that assume full automation or operate on small-scale code, Jr. AI\nScientist follows a well-defined research workflow and leverages modern coding\nagents to handle complex, multi-file implementations, leading to scientifically\nvaluable contributions. For evaluation, we conducted automated assessments\nusing AI Reviewers, author-led evaluations, and submissions to Agents4Science,\na venue dedicated to AI-driven scientific contributions. The findings\ndemonstrate that Jr. AI Scientist generates papers receiving higher review\nscores than existing fully automated systems. Nevertheless, we identify\nimportant limitations from both the author evaluation and the Agents4Science\nreviews, indicating the potential risks of directly applying current AI\nScientist systems and key challenges for future research. Finally, we\ncomprehensively report various risks identified during development. We hope\nthese insights will deepen understanding of current progress and risks in AI\nScientist development.",
      "pdf_url": "http://arxiv.org/pdf/2511.04583v1",
      "published": "2025-11-06T17:37:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04583v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning",
      "authors": [
        "Divyansha Lachi",
        "Mahmoud Mohammadi",
        "Joe Meyer",
        "Vinam Arora",
        "Tom Palczewski",
        "Eva L. Dyer"
      ],
      "abstract": "In domains such as healthcare, finance, and e-commerce, the temporal dynamics\nof relational data emerge from complex interactions-such as those between\npatients and providers, or users and products across diverse categories. To be\nbroadly useful, models operating on these data must integrate long-range\nspatial and temporal dependencies across diverse types of entities, while also\nsupporting multiple predictive tasks. However, existing graph models for\nrelational data primarily focus on spatial structure, treating temporal\ninformation merely as a filtering constraint to exclude future events rather\nthan a modeling signal, and are typically designed for single-task prediction.\nTo address these gaps, we introduce a temporal subgraph sampler that enhances\nglobal context by retrieving nodes beyond the immediate neighborhood to capture\ntemporally relevant relationships. In addition, we propose the Relational Graph\nPerceiver (RGP), a graph transformer architecture for relational deep learning\nthat leverages a cross-attention-based latent bottleneck to efficiently\nintegrate information from both structural and temporal contexts. This latent\nbottleneck integrates signals from different node and edge types into a common\nlatent space, enabling the model to build global context across the entire\nrelational system. RGP also incorporates a flexible cross-attention decoder\nthat supports joint learning across tasks with disjoint label spaces within a\nsingle model. Experiments on RelBench, SALT, and CTU show that RGP delivers\nstate-of-the-art performance, offering a general and scalable solution for\nrelational deep learning with support for diverse predictive tasks.",
      "pdf_url": "http://arxiv.org/pdf/2511.04557v1",
      "published": "2025-11-06T17:08:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04557v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach",
      "authors": [
        "Zihang Ding",
        "Kun Zhang"
      ],
      "abstract": "Urban surface water flooding, triggered by intense rainfall overwhelming\ndrainage systems, is increasingly frequent and widespread. While flood\nprediction and monitoring in high spatial-temporal resolution are desired,\npractical constraints in time, budget, and technology hinder its full\nimplementation. How to monitor urban drainage networks and predict flow\nconditions under constrained resource is a major challenge. This study presents\na data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to\noptimize sensor placement and reconstruct peak flowrates in a stormwater\nsystem, using the Woodland Avenue catchment in Duluth, Minnesota, as a case\nstudy. We utilized a SWMM model to generate a training dataset of peak flowrate\nprofiles across the stormwater network. Furthermore, we applied DSS -\nleveraging singular value decomposition for dimensionality reduction and QR\nfactorization for sensor allocation - to identify the optimal monitoring nodes\nbased on the simulated training dataset. We then validated the\nrepresentativeness of these identified monitoring nodes by comparing the\nDSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three\noptimally placed sensors among 77 nodes achieved satisfactory reconstruction\nperformance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to\n75th percentiles). In addition, the model showed good robustness to uncertainty\nin measurements. Its robustness to sensor failures is location-dependent and\nimproves with the number of sensors deployed. The framework balances\ncomputational efficiency and physical interpretability, enabling high-accuracy\nflow reconstruction with minimal sensors. This DSS framework can be further\nintegrated with predictive models to realize flood early warning and real-time\ncontrol under limited sensing and monitoring resource.",
      "pdf_url": "http://arxiv.org/pdf/2511.04556v1",
      "published": "2025-11-06T17:08:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04556v1",
      "categories": [
        "cs.AI",
        "cs.CE"
      ]
    },
    {
      "title": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems",
      "authors": [
        "Baptiste Bonin",
        "Maxime Heuillet",
        "Audrey Durand"
      ],
      "abstract": "Modeling user preferences across domains remains a key challenge in slate\nrecommendation (i.e. recommending an ordered sequence of items) research. We\ninvestigate how Large Language Models (LLM) can effectively act as world models\nof user preferences through pairwise reasoning over slates. We conduct an\nempirical study involving several LLMs on three tasks spanning different\ndatasets. Our results reveal relationships between task performance and\nproperties of the preference function captured by LLMs, hinting towards areas\nfor improvement and highlighting the potential of LLMs as world models in\nrecommender systems.",
      "pdf_url": "http://arxiv.org/pdf/2511.04541v1",
      "published": "2025-11-06T16:54:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04541v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics",
      "authors": [
        "Amir Zur",
        "Atticus Geiger",
        "Ekdeep Singh Lubana",
        "Eric Bigelow"
      ],
      "abstract": "When a language model generates text, the selection of individual tokens\nmight lead it down very different reasoning paths, making uncertainty difficult\nto quantify. In this work, we consider whether reasoning language models\nrepresent the alternate paths that they could take during generation. To test\nthis hypothesis, we use hidden activations to control and predict a language\nmodel's uncertainty during chain-of-thought reasoning. In our experiments, we\nfind a clear correlation between how uncertain a model is at different tokens,\nand how easily the model can be steered by controlling its activations. This\nsuggests that activation interventions are most effective when there are\nalternate paths available to the model -- in other words, when it has not yet\ncommitted to a particular final answer. We also find that hidden activations\ncan predict a model's future outcome distribution, demonstrating that models\nimplicitly represent the space of possible paths.",
      "pdf_url": "http://arxiv.org/pdf/2511.04527v1",
      "published": "2025-11-06T16:43:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04527v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Alternative Fairness and Accuracy Optimization in Criminal Justice",
      "authors": [
        "Shaolong Wu",
        "James Blume",
        "Geshi Yeung"
      ],
      "abstract": "Algorithmic fairness has grown rapidly as a research area, yet key concepts\nremain unsettled, especially in criminal justice. We review group, individual,\nand process fairness and map the conditions under which they conflict. We then\ndevelop a simple modification to standard group fairness. Rather than exact\nparity across protected groups, we minimize a weighted error loss while keeping\ndifferences in false negative rates within a small tolerance. This makes\nsolutions easier to find, can raise predictive accuracy, and surfaces the\nethical choice of error costs. We situate this proposal within three classes of\ncritique: biased and incomplete data, latent affirmative action, and the\nexplosion of subgroup constraints. Finally, we offer a practical framework for\ndeployment in public decision systems built on three pillars: need-based\ndecisions, Transparency and accountability, and narrowly tailored definitions\nand solutions. Together, these elements link technical design to legitimacy and\nprovide actionable guidance for agencies that use risk assessment and related\ntools.",
      "pdf_url": "http://arxiv.org/pdf/2511.04505v1",
      "published": "2025-11-06T16:24:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04505v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG",
      "authors": [
        "Joshua Gao",
        "Quoc Huy Pham",
        "Subin Varghese",
        "Silwal Saurav",
        "Vedhus Hoskere"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is a critical technique for grounding\nLarge Language Models (LLMs) in factual evidence, yet evaluating RAG systems in\nspecialized, safety-critical domains remains a significant challenge. Existing\nevaluation frameworks often rely on heuristic-based metrics that fail to\ncapture domain-specific nuances and other works utilize LLM-as-a-Judge\napproaches that lack validated alignment with human judgment. This paper\nintroduces RAGalyst, an automated, human-aligned agentic framework designed for\nthe rigorous evaluation of domain-specific RAG systems. RAGalyst features an\nagentic pipeline that generates high-quality, synthetic question-answering (QA)\ndatasets from source documents, incorporating an agentic filtering step to\nensure data fidelity. The framework refines two key LLM-as-a-Judge\nmetrics-Answer Correctness and Answerability-using prompt optimization to\nachieve a strong correlation with human annotations. Applying this framework to\nevaluate various RAG components across three distinct domains (military\noperations, cybersecurity, and bridge engineering), we find that performance is\nhighly context-dependent. No single embedding model, LLM, or hyperparameter\nconfiguration proves universally optimal. Additionally, we provide an analysis\non the most common low Answer Correctness reasons in RAG. These findings\nhighlight the necessity of a systematic evaluation framework like RAGalyst,\nwhich empowers practitioners to uncover domain-specific trade-offs and make\ninformed design choices for building reliable and effective RAG systems.\nRAGalyst is available on our Github.",
      "pdf_url": "http://arxiv.org/pdf/2511.04502v1",
      "published": "2025-11-06T16:22:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04502v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Large language models replicate and predict human cooperation across experiments in game theory",
      "authors": [
        "Andrea Cera Palatsi",
        "Samuel Martin-Gutierrez",
        "Ana S. Cardenal",
        "Max Pellert"
      ],
      "abstract": "Large language models (LLMs) are increasingly used both to make decisions in\ndomains such as health, education and law, and to simulate human behavior. Yet\nhow closely LLMs mirror actual human decision-making remains poorly understood.\nThis gap is critical: misalignment could produce harmful outcomes in practical\napplications, while failure to replicate human behavior renders LLMs\nineffective for social simulations. Here, we address this gap by developing a\ndigital twin of game-theoretic experiments and introducing a systematic\nprompting and probing framework for machine-behavioral evaluation. Testing\nthree open-source models (Llama, Mistral and Qwen), we find that Llama\nreproduces human cooperation patterns with high fidelity, capturing human\ndeviations from rational choice theory, while Qwen aligns closely with Nash\nequilibrium predictions. Notably, we achieved population-level behavioral\nreplication without persona-based prompting, simplifying the simulation\nprocess. Extending beyond the original human-tested games, we generate and\npreregister testable hypotheses for novel game configurations outside the\noriginal parameter grid. Our findings demonstrate that appropriately calibrated\nLLMs can replicate aggregate human behavioral patterns and enable systematic\nexploration of unexplored experimental spaces, offering a complementary\napproach to traditional research in the social and behavioral sciences that\ngenerates new empirical predictions about human social decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2511.04500v1",
      "published": "2025-11-06T16:21:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04500v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "cs.MA"
      ]
    },
    {
      "title": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering",
      "authors": [
        "Christos-Nikolaos Zacharopoulos",
        "Revekka Kyriakoglou"
      ],
      "abstract": "As Large Language Models (LLMs) become integral to human-centered\napplications, understanding their personality-like behaviors is increasingly\nimportant for responsible development and deployment. This paper systematically\nevaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to\nassess trait expressions under varying sampling temperatures. We find\nsignificant differences across four of the five personality dimensions, with\nNeuroticism and Extraversion susceptible to temperature adjustments. Further,\nhierarchical clustering reveals distinct model clusters, suggesting that\narchitectural features may predispose certain models toward stable trait\nprofiles. Taken together, these results offer new insights into the emergence\nof personality-like patterns in LLMs and provide a new perspective on model\ntuning, selection, and the ethical governance of AI systems. We share the data\nand code for this analysis here:\nhttps://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1",
      "pdf_url": "http://arxiv.org/pdf/2511.04499v1",
      "published": "2025-11-06T16:20:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04499v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation",
      "authors": [
        "Cuong Huynh",
        "Jie Cao"
      ],
      "abstract": "This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task\n(Alva-Manchego et al., 2025), designed for readability-controlled text\nsimplification using LLM-prompting-based generation. Based on the analysis of\nprompt-based text simplification methods, we discovered an interesting finding\nthat text simplification performance is highly related to the gap between the\nsource CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by\nthis finding, we propose two multi-round simplification methods and generate\nthem via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based\nLLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.\nLater improvements with MRS-Joint show that taking the LLM simplified\ncandidates as the starting point could further boost the multi-round\nsimplification performance.",
      "pdf_url": "http://arxiv.org/pdf/2511.04495v1",
      "published": "2025-11-06T16:16:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04495v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables",
      "authors": [
        "Nikhil Abhyankar",
        "Purvi Chaurasia",
        "Sanchit Kabra",
        "Ananya Srivastava",
        "Vivek Gupta",
        "Chandan K. Reddy"
      ],
      "abstract": "Existing tabular reasoning benchmarks mostly test models on small, uniform\ntables, underrepresenting the complexity of real-world data and giving an\nincomplete view of Large Language Models' (LLMs) reasoning abilities. Real\ntables are long, heterogeneous, and domain-specific, mixing structured fields\nwith free text and requiring multi-hop reasoning across thousands of tokens. To\naddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from\n2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)\nand ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates\nLLMs jointly across scale, heterogeneity, domain specificity, and reasoning\ncomplexity. Experiments with open-source and proprietary models show that LLMs\nstruggle with heterogeneous schemas and complex multi-hop inference, revealing\npersistent weaknesses in current architectures and prompting strategies.\nRUST-BENCH establishes a challenging new testbed for advancing tabular\nreasoning research.",
      "pdf_url": "http://arxiv.org/pdf/2511.04491v1",
      "published": "2025-11-06T16:10:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04491v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training",
      "authors": [
        "Ipsita Ghosh",
        "Ethan Nguyen",
        "Christian Kümmerle"
      ],
      "abstract": "Parameter-efficient training, based on low-rank optimization, has become a\nhighly successful tool for fine-tuning large deep-learning models. However,\nthese methods fail at low-rank pre-training tasks where maintaining the\nlow-rank structure and the objective remains a challenging task. We propose the\nQuadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel\nlow-rank inducing training strategy inspired by the iteratively reweighted\nleast squares (IRLS) framework. Q3R is based on a quadratic regularizer term\nwhich majorizes a smoothed log determinant serving as rank surrogate objective.\nUnlike other low-rank training techniques, Q3R is able to train weight matrices\nwith prescribed, low target ranks of models that achieve comparable predictive\nperformance as dense models, with small computational overhead, while remaining\nfully compatible with existing architectures. For example, we demonstrated one\nexperiment where we are able to truncate $60\\%$ and $80\\%$ of the parameters of\na ViT-Tiny model with $~1.3\\%$ and $~4\\%$ accuracy drop in CIFAR-10 performance\nrespectively. The efficacy of Q3R is confirmed on Transformers across both\nimage and language tasks, including for low-rank fine-tuning.",
      "pdf_url": "http://arxiv.org/pdf/2511.04485v1",
      "published": "2025-11-06T16:05:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04485v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ]
    },
    {
      "title": "Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis",
      "authors": [
        "Lars Krupp",
        "Daniel Geißler",
        "Vishal Banwari",
        "Paul Lukowicz",
        "Jakob Karolus"
      ],
      "abstract": "Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful\nagentic systems pushing the boundaries of Large Language Models (LLM). They can\nautonomously interact with the internet at the user's behest, such as\nnavigating websites, filling search masks, and comparing price lists. Though\nweb agent research is thriving, induced sustainability issues remain largely\nunexplored. To highlight the urgency of this issue, we provide an initial\nexploration of the energy and $CO_2$ cost associated with web agents from both\na theoretical -via estimation- and an empirical perspective -by benchmarking.\nOur results show how different philosophies in web agent creation can severely\nimpact the associated expended energy, and that more energy consumed does not\nnecessarily equate to better results. We highlight a lack of transparency\nregarding disclosing model parameters and processes used for some web agents as\na limiting factor when estimating energy consumption. Our work contributes\ntowards a change in thinking of how we evaluate web agents, advocating for\ndedicated metrics measuring energy consumption in benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2511.04481v1",
      "published": "2025-11-06T15:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04481v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop Refinement of LLM Judges",
      "authors": [
        "Hyo Jin Do",
        "Zahra Ashktorab",
        "Jasmina Gajcin",
        "Erik Miehling",
        "Martín Santillán Cooper",
        "Qian Pan",
        "Elizabeth M. Daly",
        "Werner Geyer"
      ],
      "abstract": "The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but\nits effectiveness is often limited by the scarcity of diverse, representative\ndata for refining criteria. We present a tool that integrates synthetic data\ngeneration into the LLM-as-a-judge workflow, empowering users to create\ntailored and challenging test cases with configurable domains, personas,\nlengths, and desired outcomes, including borderline cases. The tool also\nsupports AI-assisted inline editing of existing test cases. To enhance\ntransparency and interpretability, it reveals the prompts and explanations\nbehind each generation. In a user study (N=24), 83% of participants preferred\nthe tool over manually creating or selecting test cases, as it allowed them to\nrapidly generate diverse synthetic data without additional workload. The\ngenerated synthetic data proved as effective as hand-crafted data for both\nrefining evaluation criteria and aligning with human preferences. These\nfindings highlight synthetic data as a promising alternative, particularly in\ncontexts where efficiency and scalability are critical.",
      "pdf_url": "http://arxiv.org/pdf/2511.04478v1",
      "published": "2025-11-06T15:57:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04478v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs",
      "authors": [
        "Alberto Cattaneo",
        "Carlo Luschi",
        "Daniel Justus"
      ],
      "abstract": "Retrieval of information from graph-structured knowledge bases represents a\npromising direction for improving the factuality of LLMs. While various\nsolutions have been proposed, a comparison of methods is difficult due to the\nlack of challenging QA datasets with ground-truth targets for graph retrieval.\nWe present SynthKGQA, a framework for generating high-quality synthetic\nKnowledge Graph Question Answering datasets from any Knowledge Graph, providing\nthe full set of ground-truth facts in the KG to reason over each question. We\nshow how, in addition to enabling more informative benchmarking of KG\nretrievers, the data produced with SynthKGQA also allows us to train better\nmodels. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset\ndesigned to test zero-shot generalization abilities of KG retrievers with\nrespect to unseen graph structures and relation types, and benchmark popular\nsolutions for KG-augmented LLMs on it.",
      "pdf_url": "http://arxiv.org/pdf/2511.04473v1",
      "published": "2025-11-06T15:45:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04473v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ]
    },
    {
      "title": "Fraud-Proof Revenue Division on Subscription Platforms",
      "authors": [
        "Abheek Ghosh",
        "Tzeh Yuan Neoh",
        "Nicholas Teh",
        "Giannis Tyrovolas"
      ],
      "abstract": "We study a model of subscription-based platforms where users pay a fixed fee\nfor unlimited access to content, and creators receive a share of the revenue.\nExisting approaches to detecting fraud predominantly rely on machine learning\nmethods, engaging in an ongoing arms race with bad actors. We explore revenue\ndivision mechanisms that inherently disincentivize manipulation. We formalize\nthree types of manipulation-resistance axioms and examine which existing rules\nsatisfy these. We show that a mechanism widely used by streaming platforms, not\nonly fails to prevent fraud, but also makes detecting manipulation\ncomputationally intractable. We also introduce a novel rule, ScaledUserProp,\nthat satisfies all three manipulation-resistance axioms. Finally, experiments\nwith both real-world and synthetic streaming data support ScaledUserProp as a\nfairer alternative compared to existing rules.",
      "pdf_url": "http://arxiv.org/pdf/2511.04465v1",
      "published": "2025-11-06T15:39:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04465v1",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "econ.TH"
      ]
    },
    {
      "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
      "authors": [
        "Carnot Braun",
        "Rafael O. Jarczewski",
        "Gabriel U. Talasso",
        "Leandro A. Villas",
        "Allan M. de Souza"
      ],
      "abstract": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization.",
      "pdf_url": "http://arxiv.org/pdf/2511.04464v1",
      "published": "2025-11-06T15:37:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04464v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Deep Dictionary-Free Method for Identifying Linear Model of Nonlinear System with Input Delay",
      "authors": [
        "Patrik Valábek",
        "Marek Wadinger",
        "Michal Kvasnica",
        "Martin Klaučo"
      ],
      "abstract": "Nonlinear dynamical systems with input delays pose significant challenges for\nprediction, estimation, and control due to their inherent complexity and the\nimpact of delays on system behavior. Traditional linear control techniques\noften fail in these contexts, necessitating innovative approaches. This paper\nintroduces a novel approach to approximate the Koopman operator using an\nLSTM-enhanced Deep Koopman model, enabling linear representations of nonlinear\nsystems with time delays. By incorporating Long Short-Term Memory (LSTM)\nlayers, the proposed framework captures historical dependencies and efficiently\nencodes time-delayed system dynamics into a latent space. Unlike traditional\nextended Dynamic Mode Decomposition (eDMD) approaches that rely on predefined\ndictionaries, the LSTM-enhanced Deep Koopman model is dictionary-free, which\nmitigates the problems with the underlying dynamics being known and\nincorporated into the dictionary. Quantitative comparisons with extended eDMD\non a simulated system demonstrate highly significant performance gains in\nprediction accuracy in cases where the true nonlinear dynamics are unknown and\nachieve comparable results to eDMD with known dynamics of a system.",
      "pdf_url": "http://arxiv.org/pdf/2511.04451v1",
      "published": "2025-11-06T15:22:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04451v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ]
    },
    {
      "title": "The Peril of Preference: Why GRPO fails on Ordinal Rewards",
      "authors": [
        "Anisha Garg",
        "Ganesh Venkatesh"
      ],
      "abstract": "Group-relative Policy Optimization's (GRPO) simplicity makes it highly\ndesirable for adapting LLMs to become experts at specific tasks. But this\nsimplicity also makes it ill-specified as we seek to enhance RL training with\nricher, non-binary feedback. When using ordinal rewards to give partial credit,\nGRPO's simplicity starts to hurt, as its group-average baseline often assigns a\npositive advantage to failed trajectories and reinforces incorrect behavior.\n  We introduce Correctness Relative Policy Optimization (CoRPO), a new\nformulation that solves this flaw. CoRPO uses an adaptive baseline that\nenforces a minimum quality threshold, ensuring failed solutions are never\npositively reinforced. Once the policy consistently meets this threshold, the\nbaseline automatically transitions to a relative preference mode, pushing the\nmodel to find optimal solutions rather than just \"acceptable\" ones. We\nempirically validate CoRPO on a code verification task, where it demonstrates\nmore stable convergence and better out-of-domain generalization.\n  This work represents a critical step in our broader research program to\nenable LLMs to learn genuinely new capabilities through reinforcement learning.\nWe achieve this by enabling LLMs to learn from rich, multi-dimensional feedback\n- progressing from binary to ordinal rewards in this work, and onward to\ndenser, per-step supervision.",
      "pdf_url": "http://arxiv.org/pdf/2511.04439v1",
      "published": "2025-11-06T15:12:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04439v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Deep Koopman Economic Model Predictive Control of a Pasteurisation Unit",
      "authors": [
        "Patrik Valábek",
        "Michaela Horváthová",
        "Martin Klaučo"
      ],
      "abstract": "This paper presents a deep Koopman-based Economic Model Predictive Control\n(EMPC) for efficient operation of a laboratory-scale pasteurization unit (PU).\nThe method uses Koopman operator theory to transform the complex, nonlinear\nsystem dynamics into a linear representation, enabling the application of\nconvex optimization while representing the complex PU accurately. The deep\nKoopman model utilizes neural networks to learn the linear dynamics from\nexperimental data, achieving a 45% improvement in open-loop prediction accuracy\nover conventional N4SID subspace identification. Both analyzed models were\nemployed in the EMPC formulation that includes interpretable economic costs,\nsuch as energy consumption, material losses due to inadequate pasteurization,\nand actuator wear. The feasibility of EMPC is ensured using slack variables.\nThe deep Koopman EMPC and N4SID EMPC are numerically validated on a nonlinear\nmodel of multivariable PU under external disturbance. The disturbances include\nfeed pump fail-to-close scenario and the introduction of a cold batch to be\npastuerized. These results demonstrate that the deep Koopmand EMPC achieves a\n32% reduction in total economic cost compared to the N4SID baseline. This\nimprovement is mainly due to the reductions in material losses and energy\nconsumption. Furthermore, the steady-state operation via Koopman-based EMPC\nrequires 10.2% less electrical energy. The results highlight the practical\nadvantages of integrating deep Koopman representations with economic\noptimization to achieve resource-efficient control of thermal-intensive plants.",
      "pdf_url": "http://arxiv.org/pdf/2511.04437v1",
      "published": "2025-11-06T15:08:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04437v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ]
    },
    {
      "title": "Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development",
      "authors": [
        "Hao He",
        "Courtney Miller",
        "Shyam Agarwal",
        "Christian Kästner",
        "Bogdan Vasilescu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated the promise to revolutionize\nthe field of software engineering. Among other things, LLM agents are rapidly\ngaining momentum in their application to software development, with\npractitioners claiming a multifold productivity increase after adoption. Yet,\nempirical evidence is lacking around these claims. In this paper, we estimate\nthe causal effect of adopting a widely popular LLM agent assistant, namely\nCursor, on development velocity and software quality. The estimation is enabled\nby a state-of-the-art difference-in-differences design comparing\nCursor-adopting GitHub projects with a matched control group of similar GitHub\nprojects that do not use Cursor. We find that the adoption of Cursor leads to a\nsignificant, large, but transient increase in project-level development\nvelocity, along with a significant and persistent increase in static analysis\nwarnings and code complexity. Further panel generalized method of moments\nestimation reveals that the increase in static analysis warnings and code\ncomplexity acts as a major factor causing long-term velocity slowdown. Our\nstudy carries implications for software engineering practitioners, LLM agent\nassistant designers, and researchers.",
      "pdf_url": "http://arxiv.org/pdf/2511.04427v1",
      "published": "2025-11-06T15:00:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04427v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "On the Equivalence of Regression and Classification",
      "authors": [
        "Jayadeva",
        "Naman Dwivedi",
        "Hari Krishnan",
        "N. M. Anoop Krishnan"
      ],
      "abstract": "A formal link between regression and classification has been tenuous. Even\nthough the margin maximization term $\\|w\\|$ is used in support vector\nregression, it has at best been justified as a regularizer. We show that a\nregression problem with $M$ samples lying on a hyperplane has a one-to-one\nequivalence with a linearly separable classification task with $2M$ samples. We\nshow that margin maximization on the equivalent classification task leads to a\ndifferent regression formulation than traditionally used. Using the\nequivalence, we demonstrate a ``regressability'' measure, that can be used to\nestimate the difficulty of regressing a dataset, without needing to first learn\na model for it. We use the equivalence to train neural networks to learn a\nlinearizing map, that transforms input variables into a space where a linear\nregressor is adequate.",
      "pdf_url": "http://arxiv.org/pdf/2511.04422v1",
      "published": "2025-11-06T14:54:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04422v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "68T05, 68T10, 68Q32",
        "I.2.6; I.5.1; I.5.2"
      ]
    },
    {
      "title": "Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness",
      "authors": [
        "Subeen Park",
        "Joowang Kim",
        "Hakyung Lee",
        "Sunjae Yoo",
        "Kyungwoo Song"
      ],
      "abstract": "Deep learning models achieve strong performance across various domains but\noften rely on spurious correlations, making them vulnerable to distribution\nshifts. This issue is particularly severe in subpopulation shift scenarios,\nwhere models struggle in underrepresented groups. While existing methods have\nmade progress in mitigating this issue, their performance gains are still\nconstrained. They lack a rigorous theoretical framework connecting the\nembedding space representations with worst-group error. To address this\nlimitation, we propose Spurious Correlation-Aware Embedding Regularization for\nWorst-Group Robustness (SCER), a novel approach that directly regularizes\nfeature representations to suppress spurious cues. We show theoretically that\nworst-group error is influenced by how strongly the classifier relies on\nspurious versus core directions, identified from differences in group-wise mean\nembeddings across domains and classes. By imposing theoretical constraints at\nthe embedding level, SCER encourages models to focus on core features while\nreducing sensitivity to spurious patterns. Through systematic evaluation on\nmultiple vision and language, we show that SCER outperforms prior\nstate-of-the-art studies in worst-group accuracy. Our code is available at\n\\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.",
      "pdf_url": "http://arxiv.org/pdf/2511.04401v1",
      "published": "2025-11-06T14:28:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04401v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach",
      "authors": [
        "Chanwoo Park",
        "Ziyang Chen",
        "Asuman Ozdaglar",
        "Kaiqing Zhang"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as \"agents\" for\ndecision-making (DM) in interactive and dynamic environments. Yet, since they\nwere not originally designed for DM, recent studies show that LLMs can struggle\neven in basic online DM problems, failing to achieve low regret or an effective\nexploration-exploitation tradeoff. To address this, we introduce Iterative\nRegret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure\nthat repeatedly distills low-regret decision trajectories back into the base\nmodel. At each iteration, the model rolls out multiple decision trajectories,\nselects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior\nmethods that (a) distill action sequences from known DM algorithms or (b) rely\non manually crafted chain-of-thought templates, our approach leverages the\nregret metric to elicit the model's own DM ability and reasoning rationales.\nThis reliance on model-generated reasoning avoids rigid output engineering and\nprovides more flexible, natural-language training signals. Empirical results\nshow that Iterative RMFT improves LLMs' DM performance across diverse models -\nfrom Transformers with numerical input/output, to open-weight LLMs, and\nadvanced closed-weight models like GPT-4o mini. Its flexibility in output and\nreasoning formats enables generalization across tasks with varying horizons,\naction spaces, reward processes, and natural-language contexts. Finally, we\nprovide theoretical insight showing that a single-layer Transformer under this\nparadigm can act as a no-regret learner in a simplified setting. Overall,\nIterative RMFT offers a principled and general post-training framework for\nenhancing LLMs' decision-making capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2511.04393v1",
      "published": "2025-11-06T14:21:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04393v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers",
      "authors": [
        "Ali Boudaghi",
        "Hadi Zare"
      ],
      "abstract": "Music editing has emerged as an important and practical area of artificial\nintelligence, with applications ranging from video game and film music\nproduction to personalizing existing tracks according to user preferences.\nHowever, existing models face significant limitations, such as being restricted\nto editing synthesized music generated by their own models, requiring highly\nprecise prompts, or necessitating task-specific retraining, thus lacking true\nzero-shot capability. Leveraging recent advances in rectified flow and\ndiffusion transformers, we introduce MusRec, the first zero-shot text-to-music\nediting model capable of performing diverse editing tasks on real-world music\nefficiently and effectively. Experimental results demonstrate that our approach\noutperforms existing methods in preserving musical content, structural\nconsistency, and editing fidelity, establishing a strong foundation for\ncontrollable music editing in real-world scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2511.04376v1",
      "published": "2025-11-06T14:01:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04376v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ]
    },
    {
      "title": "Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning",
      "authors": [
        "Nick Oh",
        "Fernand Gobet"
      ],
      "abstract": "Test-time reasoning architectures such as those following the Generate-Verify\nparadigm -- where a model iteratively refines or verifies its own generated\noutputs -- prioritise generation and verification but exclude the monitoring\nprocesses that determine when and how reasoning should begin. This omission may\ncontribute to the prefix dominance trap, in which models commit early to\nsuboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy\nloss. We address this architectural gap by formalising Flavell's and Nelson and\nNarens' metacognitive theories into computational specifications, proposing the\nMonitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify\nparadigm by adding explicit monitoring that captures metacognitive experiences\n(from difficulty assessments to confidence judgements) before generation begins\nand refines future monitoring through verification feedback. Though we present\nno empirical validation, this work provides the first systematic computational\ntranslation of foundational metacognitive theories, offering a principled\nvocabulary for understanding reasoning system failures and suggesting specific\narchitectural interventions for future test-time reasoning designs.",
      "pdf_url": "http://arxiv.org/pdf/2511.04341v1",
      "published": "2025-11-06T13:22:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04341v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care",
      "authors": [
        "Federico Pirola",
        "Fabio Stella",
        "Marco Grzegorczyk"
      ],
      "abstract": "Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to\ntheir ability to model complex temporal relationships in patient data while\nmaintaining interpretability, an essential feature for clinical\ndecision-making. However, existing approaches to handling missing data in\nlongitudinal clinical datasets are largely derived from static Bayesian\nnetworks literature, failing to properly account for the temporal nature of the\ndata. This gap limits the ability to quantify uncertainty over time, which is\nparticularly critical in settings such as intensive care, where understanding\nthe temporal dynamics is fundamental for model trustworthiness and\napplicability across diverse patient groups. Despite the potential of DBNs, a\nfull Bayesian framework that integrates missing data handling remains\nunderdeveloped. In this work, we propose a novel Gibbs sampling-based method\nfor learning DBNs from incomplete data. Our method treats each missing value as\nan unknown parameter following a Gaussian distribution. At each iteration, the\nunobserved values are sampled from their full conditional distributions,\nallowing for principled imputation and uncertainty estimation. We evaluate our\nmethod on both simulated datasets and real-world intensive care data from\ncritically ill patients. Compared to standard model-agnostic techniques such as\nMICE, our Bayesian approach demonstrates superior reconstruction accuracy and\nconvergence properties. These results highlight the clinical relevance of\nincorporating full Bayesian inference in temporal models, providing more\nreliable imputations and offering deeper insight into model behavior. Our\napproach supports safer and more informed clinical decision-making,\nparticularly in settings where missing data are frequent and potentially\nimpactful.",
      "pdf_url": "http://arxiv.org/pdf/2511.04333v1",
      "published": "2025-11-06T13:13:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04333v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Differentially Private In-Context Learning with Nearest Neighbor Search",
      "authors": [
        "Antti Koskela",
        "Tejas Kulkarni",
        "Laith Zumot"
      ],
      "abstract": "Differentially private in-context learning (DP-ICL) has recently become an\nactive research topic due to the inherent privacy risks of in-context learning.\nHowever, existing approaches overlook a critical component of modern large\nlanguage model (LLM) pipelines: the similarity search used to retrieve relevant\ncontext data. In this work, we introduce a DP framework for in-context learning\nthat integrates nearest neighbor search of relevant examples in a privacy-aware\nmanner. Our method outperforms existing baselines by a substantial margin\nacross all evaluated benchmarks, achieving more favorable privacy-utility\ntrade-offs. To achieve this, we employ nearest neighbor retrieval from a\ndatabase of context data, combined with a privacy filter that tracks the\ncumulative privacy cost of selected samples to ensure adherence to a central\ndifferential privacy budget. Experimental results on text classification and\ndocument question answering show a clear advantage of the proposed method over\nexisting baselines.",
      "pdf_url": "http://arxiv.org/pdf/2511.04332v1",
      "published": "2025-11-06T13:06:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04332v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation",
      "authors": [
        "Jiahao Zhao",
        "Luxin Xu",
        "Minghuan Tan",
        "Lichao Zhang",
        "Ahmadreza Argha",
        "Hamid Alinejad-Rokny",
        "Min Yang"
      ],
      "abstract": "Numerous medical systems powered by Large Language Models (LLMs) have\nachieved remarkable progress in diverse healthcare tasks. However, research on\ntheir medication safety remains limited due to the lack of real world datasets,\nconstrained by privacy and accessibility issues. Moreover, evaluation of LLMs\nin realistic clinical consultation settings, particularly regarding medication\nsafety, is still underexplored. To address these gaps, we propose a framework\nthat simulates and evaluates clinical consultations to systematically assess\nthe medication safety capabilities of LLMs. Within this framework, we generate\ninquiry diagnosis dialogues with embedded medication risks and construct a\ndedicated medication safety database, RxRisk DB, containing 6,725\ncontraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.\nA two-stage filtering strategy ensures clinical realism and professional\nquality, resulting in the benchmark RxSafeBench with 2,443 high-quality\nconsultation scenarios. We evaluate leading open-source and proprietary LLMs\nusing structured multiple choice questions that test their ability to recommend\nsafe medications under simulated patient contexts. Results show that current\nLLMs struggle to integrate contraindication and interaction knowledge,\nespecially when risks are implied rather than explicit. Our findings highlight\nkey challenges in ensuring medication safety in LLM-based systems and provide\ninsights into improving reliability through better prompting and task-specific\ntuning. RxSafeBench offers the first comprehensive benchmark for evaluating\nmedication safety in LLMs, advancing safer and more trustworthy AI-driven\nclinical decision support.",
      "pdf_url": "http://arxiv.org/pdf/2511.04328v1",
      "published": "2025-11-06T12:56:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04328v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM",
      "authors": [
        "Yuanpeng Zhang",
        "Xing Hu",
        "Xi Chen",
        "Zhihang Yuan",
        "Cong Li",
        "Jingchen Zhu",
        "Zhao Wang",
        "Chenguang Zhang",
        "Xin Si",
        "Wei Gao",
        "Qiang Wu",
        "Runsheng Wang",
        "Guangyu Sun"
      ],
      "abstract": "SRAM Processing-in-Memory (PIM) has emerged as the most promising\nimplementation for high-performance PIM, delivering superior computing density,\nenergy efficiency, and computational precision. However, the pursuit of higher\nperformance necessitates more complex circuit designs and increased operating\nfrequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly\ndegrade chip performance and even threaten reliability. Conventional\ncircuit-level IR-drop mitigation methods, such as back-end optimizations, are\nresource-intensive and often compromise power, performance, and area (PPA). To\naddress these challenges, we propose AIM, comprehensive software and hardware\nco-design for architecture-level IR-drop mitigation in high-performance PIM.\nInitially, leveraging the bit-serial and in-situ dataflow processing properties\nof PIM, we introduce Rtog and HR, which establish a direct correlation between\nPIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,\nenabling extensive exploration of architecture-level IR-drop mitigation while\nmaintaining computational accuracy through software optimization. Subsequently,\nwe develop IR-Booster, a dynamic adjustment mechanism that integrates\nsoftware-level HR information with hardware-based IR-drop monitoring to adapt\nthe V-f pairs of the PIM macro, achieving enhanced energy efficiency and\nperformance. Finally, we propose the HR-aware task mapping method, bridging\nsoftware and hardware designs to achieve optimal improvement. Post-layout\nsimulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up\nto 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement\nand 1.152x speedup.",
      "pdf_url": "http://arxiv.org/pdf/2511.04321v1",
      "published": "2025-11-06T12:49:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04321v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research",
      "authors": [
        "Tim Beyer",
        "Jonas Dornbusch",
        "Jakob Steimle",
        "Moritz Ladenburger",
        "Leo Schwinn",
        "Stephan Günnemann"
      ],
      "abstract": "The rapid expansion of research on Large Language Model (LLM) safety and\nrobustness has produced a fragmented and oftentimes buggy ecosystem of\nimplementations, datasets, and evaluation methods. This fragmentation makes\nreproducibility and comparability across studies challenging, hindering\nmeaningful progress. To address these issues, we introduce AdversariaLLM, a\ntoolbox for conducting LLM jailbreak robustness research. Its design centers on\nreproducibility, correctness, and extensibility. The framework implements\ntwelve adversarial attack algorithms, integrates seven benchmark datasets\nspanning harmfulness, over-refusal, and utility evaluation, and provides access\nto a wide range of open-weight LLMs via Hugging Face. The implementation\nincludes advanced features for comparability and reproducibility such as\ncompute-resource tracking, deterministic results, and distributional evaluation\ntechniques. \\name also integrates judging through the companion package\nJudgeZoo, which can also be used independently. Together, these components aim\nto establish a robust foundation for transparent, comparable, and reproducible\nresearch in LLM safety.",
      "pdf_url": "http://arxiv.org/pdf/2511.04316v1",
      "published": "2025-11-06T12:38:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04316v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Probing the Probes: Methods and Metrics for Concept Alignment",
      "authors": [
        "Jacob Lysnæs-Larsen",
        "Marte Eggen",
        "Inga Strümke"
      ],
      "abstract": "In explainable AI, Concept Activation Vectors (CAVs) are typically obtained\nby training linear classifier probes to detect human-understandable concepts as\ndirections in the activation space of deep neural networks. It is widely\nassumed that a high probe accuracy indicates a CAV faithfully representing its\ntarget concept. However, we show that the probe's classification accuracy alone\nis an unreliable measure of concept alignment, i.e., the degree to which a CAV\ncaptures the intended concept. In fact, we argue that probes are more likely to\ncapture spurious correlations than they are to represent only the intended\nconcept. As part of our analysis, we demonstrate that deliberately misaligned\nprobes constructed to exploit spurious correlations, achieve an accuracy close\nto that of standard probes. To address this severe problem, we introduce a\nnovel concept localization method based on spatial linear attribution, and\nprovide a comprehensive comparison of it to existing feature visualization\ntechniques for detecting and mitigating concept misalignment. We further\npropose three classes of metrics for quantitatively assessing concept\nalignment: hard accuracy, segmentation scores, and augmentation robustness. Our\nanalysis shows that probes with translation invariance and spatial alignment\nconsistently increase concept alignment. These findings highlight the need for\nalignment-based evaluation metrics rather than probe accuracy, and the\nimportance of tailoring probes to both the model architecture and the nature of\nthe target concept.",
      "pdf_url": "http://arxiv.org/pdf/2511.04312v1",
      "published": "2025-11-06T12:34:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04312v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
      "authors": [
        "Jian Mu",
        "Chaoyun Zhang",
        "Chiming Ni",
        "Lu Wang",
        "Bo Qiao",
        "Kartik Mathur",
        "Qianhui Wu",
        "Yuhang Xie",
        "Xiaojun Ma",
        "Mengyu Zhou",
        "Si Qin",
        "Liqun Li",
        "Yu Kang",
        "Minghua Ma",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "abstract": "We introduce GUI-360$^\\circ$, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360$^\\circ$ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360$^\\circ$ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360$^\\circ$ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.",
      "pdf_url": "http://arxiv.org/pdf/2511.04307v1",
      "published": "2025-11-06T12:19:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04307v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data",
      "authors": [
        "Robin Spanier",
        "Thorsten Hoeser",
        "Claudia Kuenzer"
      ],
      "abstract": "The recent and ongoing expansion of marine infrastructure, including offshore\nwind farms, oil and gas platforms, artificial islands, and aquaculture\nfacilities, highlights the need for effective monitoring systems. The\ndevelopment of robust models for offshore infrastructure detection relies on\ncomprehensive, balanced datasets, but falls short when samples are scarce,\nparticularly for underrepresented object classes, shapes, and sizes. By\ntraining deep learning-based YOLOv10 object detection models with a combination\nof synthetic and real Sentinel-1 satellite imagery acquired in the fourth\nquarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of\nGuinea, and Coast of Brazil), this study investigates the use of synthetic\ntraining data to enhance model performance. We evaluated this approach by\napplying the model to detect offshore platforms in three unseen regions (Gulf\nof Mexico, North Sea, Persian Gulf) and thereby assess geographic\ntransferability. This region-holdout evaluation demonstrated that the model\ngeneralises beyond the training areas. In total, 3,529 offshore platforms were\ndetected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and\n1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which\nimproved to 0.90 upon incorporating synthetic data. We analysed how synthetic\ndata enhances the representation of unbalanced classes and overall model\nperformance, taking a first step toward globally transferable detection of\noffshore infrastructure. This study underscores the importance of balanced\ndatasets and highlights synthetic data generation as an effective strategy to\naddress common challenges in remote sensing, demonstrating the potential of\ndeep learning for scalable, global offshore infrastructure monitoring.",
      "pdf_url": "http://arxiv.org/pdf/2511.04304v1",
      "published": "2025-11-06T12:13:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04304v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference",
      "authors": [
        "Matteo Cercola",
        "Valeria Capretti",
        "Simone Formentin"
      ],
      "abstract": "Learning from human preferences is a cornerstone of aligning machine learning\nmodels with subjective human judgments. Yet, collecting such preference data is\noften costly and time-consuming, motivating the need for more efficient\nlearning paradigms. Two established approaches offer complementary advantages:\nRLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,\nwhile PBO achieves greater sample efficiency through active querying. We\npropose a hybrid framework that unifies RLHF's scalability with PBO's query\nefficiency by integrating an acquisition-driven module into the RLHF pipeline,\nthereby enabling active and sample-efficient preference gathering. We validate\nthe proposed approach on two representative domains: (i) high-dimensional\npreference optimization and (ii) LLM fine-tuning. Experimental results\ndemonstrate consistent improvements in both sample efficiency and overall\nperformance across these tasks.",
      "pdf_url": "http://arxiv.org/pdf/2511.04286v1",
      "published": "2025-11-06T11:27:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04286v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization",
      "authors": [
        "Zeng Zhiyuan",
        "Jiashuo Liu",
        "Zhangyue Yin",
        "Ge Zhang",
        "Wenhao Huang",
        "Xipeng Qiu"
      ],
      "abstract": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge: RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization and catastrophic forgetting of diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduce RLoop, a self-improving framework\nbuilt on iterative policy initialization. RLoop transforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop of exploration and exploitation via\niterative re-initialization effectively converts transient policy variations\ninto robust performance gains. Our experiments show RLoop mitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% and\npass@32 by over 15% compared to vanilla RL.",
      "pdf_url": "http://arxiv.org/pdf/2511.04285v1",
      "published": "2025-11-06T11:27:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04285v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery",
      "authors": [
        "Claudio Giusti",
        "Luca Guarnera",
        "Sebastiano Battiato"
      ],
      "abstract": "The growing sophistication of synthetic image and deepfake generation models\nhas turned source attribution and authenticity verification into a critical\nchallenge for modern computer vision systems. Recent studies suggest that\ndiffusion pipelines unintentionally imprint persistent statistical traces,\nknown as signal leaks, within their outputs, particularly in latent\nrepresentations. Building on this observation, we propose Proto-LeakNet, a\nsignal-leak-aware and interpretable attribution framework that integrates\nclosed-set classification with a density-based open-set evaluation on the\nlearned embeddings, enabling analysis of unseen generators without retraining.\nOperating in the latent domain of diffusion models, our method re-simulates\npartial forward diffusion to expose residual generator-specific cues. A\ntemporal attention encoder aggregates multi-step latent features, while a\nfeature-weighted prototype head structures the embedding space and enables\ntransparent attribution. Trained solely on closed data and achieving a Macro\nAUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under\npost-processing, surpassing state-of-the-art methods, and achieves strong\nseparability between known and unseen generators. These results demonstrate\nthat modeling signal-leak bias in latent space enables reliable and\ninterpretable AI-image and deepfake forensics. The code for the whole work will\nbe available upon submission.",
      "pdf_url": "http://arxiv.org/pdf/2511.04260v1",
      "published": "2025-11-06T10:51:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04260v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection",
      "authors": [
        "Marawan Elbatel",
        "Anbang Wang",
        "Keyuan Liu",
        "Kaouther Mouheb",
        "Enrique Almar-Munoz",
        "Lizhuo Lin",
        "Yanqi Yang",
        "Karim Lekadir",
        "Xiaomeng Li"
      ],
      "abstract": "This paper does not introduce a novel architecture; instead, it revisits a\nfundamental yet overlooked baseline: adapting human-centric foundation models\nfor anatomical landmark detection in medical imaging. While landmark detection\nhas traditionally relied on domain-specific models, the emergence of\nlarge-scale pre-trained vision models presents new opportunities. In this\nstudy, we investigate the adaptation of Sapiens, a human-centric foundation\nmodel designed for pose estimation, to medical imaging through multi-dataset\npretraining, establishing a new state of the art across multiple datasets. Our\nproposed model, MedSapiens, demonstrates that human-centric foundation models,\ninherently optimized for spatial pose localization, provide strong priors for\nanatomical landmark detection, yet this potential has remained largely\nuntapped. We benchmark MedSapiens against existing state-of-the-art models,\nachieving up to 5.26% improvement over generalist models and up to 21.81%\nimprovement over specialist models in the average success detection rate (SDR).\nTo further assess MedSapiens adaptability to novel downstream tasks with few\nannotations, we evaluate its performance in limited-data settings, achieving\n2.69% improvement over the few-shot state of the art in SDR. Code and model\nweights are available at https://github.com/xmed-lab/MedSapiens .",
      "pdf_url": "http://arxiv.org/pdf/2511.04255v1",
      "published": "2025-11-06T10:45:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04255v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "On the Brittleness of CLIP Text Encoders",
      "authors": [
        "Allie Tran",
        "Luca Rossetto"
      ],
      "abstract": "Multimodal co-embedding models, especially CLIP, have advanced the state of\nthe art in zero-shot classification and multimedia information retrieval in\nrecent years by aligning images and text in a shared representation space.\nHowever, such modals trained on a contrastive alignment can lack stability\ntowards small input perturbations. Especially when dealing with manually\nexpressed queries, minor variations in the query can cause large differences in\nthe ranking of the best-matching results. In this paper, we present a\nsystematic analysis of the effect of multiple classes of non-semantic query\nperturbations in an multimedia information retrieval scenario. We evaluate a\ndiverse set of lexical, syntactic, and semantic perturbations across multiple\nCLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video\ncollection. Across models, we find that syntactic and semantic perturbations\ndrive the largest instabilities, while brittleness is concentrated in trivial\nsurface edits such as punctuation and case. Our results highlight robustness as\na critical dimension for evaluating vision-language models beyond benchmark\naccuracy.",
      "pdf_url": "http://arxiv.org/pdf/2511.04247v1",
      "published": "2025-11-06T10:33:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04247v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "seqme: a Python library for evaluating biological sequence design",
      "authors": [
        "Rasmus Møller-Larsen",
        "Adam Izdebski",
        "Jan Olszewski",
        "Pankhil Gawade",
        "Michal Kmicikiewicz",
        "Wojciech Zarzecki",
        "Ewa Szczurek"
      ],
      "abstract": "Recent advances in computational methods for designing biological sequences\nhave sparked the development of metrics to evaluate these methods performance\nin terms of the fidelity of the designed sequences to a target distribution and\ntheir attainment of desired properties. However, a single software library\nimplementing these metrics was lacking. In this work we introduce seqme, a\nmodular and highly extendable open-source Python library, containing\nmodel-agnostic metrics for evaluating computational methods for biological\nsequence design. seqme considers three groups of metrics: sequence-based,\nembedding-based, and property-based, and is applicable to a wide range of\nbiological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins.\nThe library offers a number of embedding and property models for biological\nsequences, as well as diagnostics and visualization functions to inspect the\nresults. seqme can be used to evaluate both one-shot and iterative\ncomputational design methods.",
      "pdf_url": "http://arxiv.org/pdf/2511.04239v1",
      "published": "2025-11-06T10:24:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04239v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T01"
      ]
    },
    {
      "title": "Denoised Recommendation Model with Collaborative Signal Decoupling",
      "authors": [
        "Zefeng Li",
        "Ning Yang"
      ],
      "abstract": "Although the collaborative filtering (CF) algorithm has achieved remarkable\nperformance in recommendation systems, it suffers from suboptimal\nrecommendation performance due to noise in the user-item interaction matrix.\nNumerous noise-removal studies have improved recommendation models, but most\nexisting approaches conduct denoising on a single graph. This may cause\nattenuation of collaborative signals: removing edges between two nodes can\ninterrupt paths between other nodes, weakening path-dependent collaborative\ninformation. To address these limitations, this study proposes a novel\nGNN-based CF model called DRCSD for denoising unstable interactions. DRCSD\nincludes two core modules: a collaborative signal decoupling module (decomposes\nsignals into distinct orders by structural characteristics) and an order-wise\ndenoising module (performs targeted denoising on each order). Additionally, the\ninformation aggregation mechanism of traditional GNN-based CF models is\nmodified to avoid cross-order signal interference until the final pooling\noperation. Extensive experiments on three public real-world datasets show that\nDRCSD has superior robustness against unstable interactions and achieves\nstatistically significant performance improvements in recommendation accuracy\nmetrics compared to state-of-the-art baseline models.",
      "pdf_url": "http://arxiv.org/pdf/2511.04237v1",
      "published": "2025-11-06T10:18:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04237v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Shared Spatial Memory Through Predictive Coding",
      "authors": [
        "Zhengru Fang",
        "Yu Guo",
        "Jingjing Wang",
        "Yuang Zhang",
        "Haonan An",
        "Yinhai Wang",
        "Yuguang Fang"
      ],
      "abstract": "Sharing and reconstructing a consistent spatial memory is a critical\nchallenge in multi-agent systems, where partial observability and limited\nbandwidth often lead to catastrophic failures in coordination. We introduce a\nmulti-agent predictive coding framework that formulate coordination as the\nminimization of mutual uncertainty among agents. Instantiated as an information\nbottleneck objective, it prompts agents to learn not only who and what to\ncommunicate but also when. At the foundation of this framework lies a\ngrid-cell-like metric as internal spatial coding for self-localization,\nemerging spontaneously from self-supervised motion prediction. Building upon\nthis internal spatial code, agents gradually develop a bandwidth-efficient\ncommunication mechanism and specialized neural populations that encode\npartners' locations: an artificial analogue of hippocampal social place cells\n(SPCs). These social representations are further enacted by a hierarchical\nreinforcement learning policy that actively explores to reduce joint\nuncertainty. On the Memory-Maze benchmark, our approach shows exceptional\nresilience to bandwidth constraints: success degrades gracefully from 73.5% to\n64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast\nbaseline collapses from 67.6% to 28.6%. Our findings establish a theoretically\nprincipled and biologically plausible basis for how complex social\nrepresentations emerge from a unified predictive drive, leading to social\ncollective intelligence.",
      "pdf_url": "http://arxiv.org/pdf/2511.04235v1",
      "published": "2025-11-06T10:12:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04235v1",
      "categories": [
        "cs.AI",
        "cs.CE"
      ]
    },
    {
      "title": "Opus: A Quantitative Framework for Workflow Evaluation",
      "authors": [
        "Alan Seroul",
        "Théo Fagnoni",
        "Inès Adnani",
        "Dana O. Mohamed",
        "Phillip Kingston"
      ],
      "abstract": "This paper introduces the Opus Workflow Evaluation Framework, a\nprobabilistic-normative formulation for quantifying Workflow quality and\nefficiency. It integrates notions of correctness, reliability, and cost into a\ncoherent mathematical model that enables direct comparison, scoring, and\noptimization of Workflows. The framework combines the Opus Workflow Reward, a\nprobabilistic function estimating expected performance through success\nlikelihood, resource usage, and output gain, with the Opus Workflow Normative\nPenalties, a set of measurable functions capturing structural and informational\nquality across Cohesion, Coupling, Observability, and Information Hygiene. It\nsupports automated Workflow assessment, ranking, and optimization within modern\nautomation systems such as Opus and can be integrated into Reinforcement\nLearning loops to guide Workflow discovery and refinement. In this paper, we\nintroduce the Opus Workflow Reward model that formalizes Workflow success as a\nprobabilistic expectation over costs and outcomes. We define measurable Opus\nWorkflow Normative Penalties capturing structural, semantic, and signal-related\nproperties of Workflows. Finally, we propose a unified optimization formulation\nfor identifying and ranking optimal Workflows under joint Reward-Penalty\ntrade-offs.",
      "pdf_url": "http://arxiv.org/pdf/2511.04220v1",
      "published": "2025-11-06T09:35:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04220v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms",
      "authors": [
        "Hikari Otsuka",
        "Daiki Chijiwa",
        "Yasuyuki Okoshi",
        "Daichi Fujiki",
        "Susumu Takeuchi",
        "Masato Motomura"
      ],
      "abstract": "The strong lottery ticket hypothesis (SLTH) conjectures that high-performing\nsubnetworks, called strong lottery tickets (SLTs), are hidden in randomly\ninitialized neural networks. Although recent theoretical studies have\nestablished the SLTH across various neural architectures, the SLTH for\ntransformer architectures still lacks theoretical understanding. In particular,\nthe current theory of the SLTH does not yet account for the multi-head\nattention (MHA) mechanism, a core component of transformers. To address this\ngap, we introduce a theoretical analysis of the existence of SLTs within MHAs.\nWe prove that, if a randomly initialized MHA of $H$ heads and input dimension\n$d$ has the hidden dimension $O(d\\log(Hd^{3/2}))$ for the key and value, it\ncontains an SLT that approximates an arbitrary MHA with the same input\ndimension with high probability. Furthermore, by leveraging this theory for\nMHAs, we extend the SLTH to transformers without normalization layers. We\nempirically validate our theoretical findings, demonstrating that the\napproximation error between the SLT within a source model (MHA and transformer)\nand an approximate target counterpart decreases exponentially by increasing the\nhidden dimension of the source model.",
      "pdf_url": "http://arxiv.org/pdf/2511.04217v1",
      "published": "2025-11-06T09:29:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.04217v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}