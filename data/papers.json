{
  "last_updated": "2025-05-07T00:51:31.906247",
  "papers": [
    {
      "title": "LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery",
      "authors": [
        "Jerome Quenum",
        "Wen-Han Hsieh",
        "Tsung-Han Wu",
        "Ritwik Gupta",
        "Trevor Darrell",
        "David M. Chan"
      ],
      "abstract": "Segmentation models can recognize a pre-defined set of objects in images.\nHowever, models that can reason over complex user queries that implicitly refer\nto multiple objects of interest are still in their infancy. Recent advances in\nreasoning segmentation--generating segmentation masks from complex, implicit\nquery text--demonstrate that vision-language models can operate across an open\ndomain and produce reasonable outputs. However, our experiments show that such\nmodels struggle with complex remote-sensing imagery. In this work, we introduce\nLISAt, a vision-language model designed to describe complex remote-sensing\nscenes, answer questions about them, and segment objects of interest. We\ntrained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,\nwith 27,615 annotations over 9,205 images, and a multimodal pretraining\ndataset, PreGRES, containing over 1 million question-answer pairs. LISAt\noutperforms existing geospatial foundation models such as RS-GPT4V by over\n10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses\nstate-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %\n(gIoU). Our model, datasets, and code are available at\nhttps://lisat-bair.github.io/LISAt/",
      "pdf_url": "http://arxiv.org/pdf/2505.02829v1",
      "published": "2025-05-05T17:56:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02829v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review",
      "authors": [
        "Sonal Allana",
        "Mohan Kankanhalli",
        "Rozita Dara"
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a pillar of\nTrustworthy AI and aims to bring transparency in complex models that are opaque\nby nature. Despite the benefits of incorporating explanations in models, an\nurgent need is found in addressing the privacy concerns of providing this\nadditional information to end users. In this article, we conduct a scoping\nreview of existing literature to elicit details on the conflict between privacy\nand explainability. Using the standard methodology for scoping review, we\nextracted 57 articles from 1,943 studies published from January 2019 to\nDecember 2024. The review addresses 3 research questions to present readers\nwith more understanding of the topic: (1) what are the privacy risks of\nreleasing explanations in AI systems? (2) what current methods have researchers\nemployed to achieve privacy preservation in XAI systems? (3) what constitutes a\nprivacy preserving explanation? Based on the knowledge synthesized from the\nselected studies, we categorize the privacy risks and preservation methods in\nXAI and propose the characteristics of privacy preserving explanations to aid\nresearchers and practitioners in understanding the requirements of XAI that is\nprivacy compliant. Lastly, we identify the challenges in balancing privacy with\nother system desiderata and provide recommendations for achieving privacy\npreserving XAI. We expect that this review will shed light on the complex\nrelationship of privacy and explainability, both being the fundamental\nprinciples of Trustworthy AI.",
      "pdf_url": "http://arxiv.org/pdf/2505.02828v1",
      "published": "2025-05-05T17:53:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02828v1",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.ET"
      ]
    },
    {
      "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
      "authors": [
        "Kuofeng Gao",
        "Yufei Zhu",
        "Yiming Li",
        "Jiawang Bai",
        "Yong Yang",
        "Zhifeng Li",
        "Shu-Tao Xia"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance.",
      "pdf_url": "http://arxiv.org/pdf/2505.02824v1",
      "published": "2025-05-05T17:51:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02824v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback",
      "authors": [
        "Hao Zhu",
        "Phil Cuvin",
        "Xinkai Yu",
        "Charlotte Ka Yee Yan",
        "Jason Zhang",
        "Diyi Yang"
      ],
      "abstract": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents.",
      "pdf_url": "http://arxiv.org/pdf/2505.02820v1",
      "published": "2025-05-05T17:47:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02820v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing",
      "authors": [
        "Diji Yang",
        "Linda Zeng",
        "Jinmeng Rao",
        "Yi Zhang"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data.",
      "pdf_url": "http://arxiv.org/pdf/2505.02811v1",
      "published": "2025-05-05T17:39:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02811v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ]
    },
    {
      "title": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models",
      "authors": [
        "Zheng Lin",
        "Yuxin Zhang",
        "Zhe Chen",
        "Zihan Fang",
        "Xianhao Chen",
        "Praneeth Vepakomma",
        "Wei Ni",
        "Jun Luo",
        "Yue Gao"
      ],
      "abstract": "Recently, large language models (LLMs) have achieved remarkable\nbreakthroughs, revolutionizing the natural language processing domain and\nbeyond. Due to immense parameter sizes, fine-tuning these models with private\ndata for diverse downstream tasks has become mainstream. Though federated\nlearning (FL) offers a promising solution for fine-tuning LLMs without sharing\nraw data, substantial computing costs hinder its democratization. Moreover, in\nreal-world scenarios, private client devices often possess heterogeneous\ncomputing resources, further complicating LLM fine-tuning. To combat these\nchallenges, we propose HSplitLoRA, a heterogeneous parameter-efficient\nfine-tuning (PEFT) framework built on split learning (SL) and low-rank\nadaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on\nheterogeneous client devices. HSplitLoRA first identifies important weights\nbased on their contributions to LLM training. It then dynamically configures\nthe decomposition ranks of LoRA adapters for selected weights and determines\nthe model split point according to varying computing budgets of client devices.\nFinally, a noise-free adapter aggregation mechanism is devised to support\nheterogeneous adapter aggregation without introducing noise. Extensive\nexperiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks\nin training accuracy and convergence speed.",
      "pdf_url": "http://arxiv.org/pdf/2505.02795v1",
      "published": "2025-05-05T17:09:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02795v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "Local Markov Equivalence and Local Causal Discovery for Identifying Controlled Direct Effects",
      "authors": [
        "Timothée Loranchet",
        "Charles K. Assaad"
      ],
      "abstract": "Understanding and identifying controlled direct effects (CDEs) is crucial\nacross numerous scientific domains, including public health. While existing\nmethods can identify these effects from causal directed acyclic graphs (DAGs),\nthe true underlying structure is often unknown in practice. Essential graphs,\nwhich represent a Markov equivalence class of DAGs characterized by the same\nset of d-separations, provide a more practical and realistic alternative.\nHowever, learning the full essential graph is computationally intensive and\ntypically depends on strong, untestable assumptions. In this work, we\ncharacterize a local class of graphs, defined relative to a target variable,\nthat share a specific subset of d-separations, and introduce a graphical\nrepresentation of this class, called the local essential graph (LEG). We then\npresent LocPC, a novel algorithm designed to recover the LEG from an observed\ndistribution using only local conditional independence tests. Building on\nLocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG\nthat is sufficient to identify a CDE, bypassing the need of retrieving the full\nessential graph. Compared to global methods, our algorithms require less\nconditional independence tests and operate under weaker assumptions while\nmaintaining theoretical guarantees.",
      "pdf_url": "http://arxiv.org/pdf/2505.02781v1",
      "published": "2025-05-05T16:47:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02781v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced Digital Pathology Workflow",
      "authors": [
        "Jai Prakash Veerla",
        "Partha Sai Guttikonda",
        "Helen H. Shang",
        "Mohammad Sadegh Nasr",
        "Cesar Torres",
        "Jacob M. Luber"
      ],
      "abstract": "Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases\nlike cancer, yet current digital pathology tools hinder diagnosis. The immense\nscale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the\nlimited views traditional monitors offer. This mismatch forces constant panning\nand zooming, increasing pathologist cognitive load, causing diagnostic fatigue,\nand slowing pathologists' adoption of digital methods. PathVis, our\nmixed-reality visualization platform for Apple Vision Pro, addresses these\nchallenges. It transforms the pathologist's interaction with data, replacing\ncumbersome mouse-and-monitor navigation with intuitive exploration using\nnatural hand gestures, eye gaze, and voice commands in an immersive workspace.\nPathVis integrates AI to enhance diagnosis. An AI-driven search function\ninstantly retrieves and displays the top five similar patient cases\nside-by-side, improving diagnostic precision and efficiency through rapid\ncomparison. Additionally, a multimodal conversational AI assistant offers\nreal-time image interpretation support and aids collaboration among\npathologists across multiple Apple devices. By merging the directness of\ntraditional pathology with advanced mixed-reality visualization and AI, PathVis\nimproves diagnostic workflows, reduces cognitive strain, and makes pathology\npractice more effective and engaging. The PathVis source code and a demo video\nare publicly available at: https://github.com/jaiprakash1824/Path_Vis",
      "pdf_url": "http://arxiv.org/pdf/2505.02780v1",
      "published": "2025-05-05T16:46:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02780v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET",
        "q-bio.TO"
      ]
    },
    {
      "title": "Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control",
      "authors": [
        "Nam H. Le",
        "Patrick Erikson",
        "Yanbo Zhang",
        "Michael Levin",
        "Josh Bongard"
      ],
      "abstract": "Guiding biological systems toward desired states, such as morphogenetic\noutcomes, remains a fundamental challenge with far-reaching implications for\nmedicine and synthetic biology. While large language models (LLMs) have enabled\nnatural language as an interface for interpretable control in AI systems, their\nuse as mediators for steering biological or cellular dynamics remains largely\nunexplored.\n  In this work, we present a functional pipeline that translates natural\nlanguage prompts into spatial vector fields capable of directing simulated\ncellular collectives. Our approach combines a large language model with an\nevolvable neural controller (Prompt-to-Intervention, or P2I), optimized via\nevolutionary strategies to generate behaviors such as clustering or scattering\nin a simulated 2D environment.\n  We demonstrate that even with constrained vocabulary and simplified cell\nmodels, evolved P2I networks can successfully align cellular dynamics with\nuser-defined goals expressed in plain language. This work offers a complete\nloop from language input to simulated bioelectric-like intervention to\nbehavioral output, providing a foundation for future systems capable of natural\nlanguage-driven cellular control.",
      "pdf_url": "http://arxiv.org/pdf/2505.02766v1",
      "published": "2025-05-05T16:21:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02766v1",
      "categories": [
        "cs.AI",
        "cs.NE",
        "cs.RO",
        "q-bio.TO"
      ]
    },
    {
      "title": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models",
      "authors": [
        "Matthew Dahl"
      ],
      "abstract": "Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount.",
      "pdf_url": "http://arxiv.org/pdf/2505.02763v1",
      "published": "2025-05-05T16:18:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02763v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "The use of Artificial Intelligence for Intervention and Assessment in Individuals with ASD",
      "authors": [
        "Aggeliki Sideraki",
        "Christos-Nikolaos Anagnostopoulos"
      ],
      "abstract": "This paper explores the use of Artificial Intelligence (AI) as a tool for\ndiagnosis, assessment, and intervention for individuals with Autism Spectrum\nDisorder (ASD). It focuses particularly on AI's role in early diagnosis,\nutilizing advanced machine learning techniques and data analysis. Recent\nstudies demonstrate that deep learning algorithms can identify behavioral\npatterns through biometric data analysis, video-based interaction assessments,\nand linguistic feature extraction, providing a more accurate and timely\ndiagnosis compared to traditional methods. Additionally, AI automates\ndiagnostic tools, reducing subjective biases and enabling the development of\npersonalized assessment protocols for ASD monitoring. At the same time, the\npaper examines AI-powered intervention technologies, emphasizing educational\nrobots and adaptive communication tools. Social robotic assistants, such as NAO\nand Kaspar, have been shown to enhance social skills in children by offering\nstructured, repetitive interactions that reinforce learning. Furthermore,\nAI-driven Augmentative and Alternative Communication (AAC) systems allow\nchildren with ASD to express themselves more effectively, while\nmachine-learning chatbots provide language development support through\npersonalized responses. The study presents research findings supporting the\neffectiveness of these AI applications while addressing challenges such as\nlong-term evaluation and customization to individual needs. In conclusion, the\npaper highlights the significance of AI as an innovative tool in ASD diagnosis\nand intervention, advocating for further research to assess its long-term\nimpact.",
      "pdf_url": "http://arxiv.org/pdf/2505.02747v1",
      "published": "2025-05-05T15:58:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02747v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation",
      "authors": [
        "Gerard Pons",
        "Besim Bilalli",
        "Anna Queralt"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance.",
      "pdf_url": "http://arxiv.org/pdf/2505.02737v2",
      "published": "2025-05-05T15:40:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02737v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ]
    },
    {
      "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models",
      "authors": [
        "Zhouliang Yu",
        "Ruotian Peng",
        "Keyi Ding",
        "Yizhe Li",
        "Zhongyuan Peng",
        "Minghao Liu",
        "Yifan Zhang",
        "Zheng Yuan",
        "Huajian Xin",
        "Wenhao Huang",
        "Yandong Wen",
        "Ge Zhang",
        "Weiyang Liu"
      ],
      "abstract": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2505.02735v1",
      "published": "2025-05-05T15:37:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02735v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry",
      "authors": [
        "Junu Kim",
        "Chaeeun Shim",
        "Sungjin Park",
        "Su Yeon Lee",
        "Gee Young Suh",
        "Chae-Man Lim",
        "Seong Jin Choi",
        "Song Mi Moon",
        "Kyoung-Ho Song",
        "Eu Suk Kim",
        "Hong Bin Kim",
        "Sejoong Kim",
        "Chami Im",
        "Dong-Wan Kang",
        "Yong Soo Kim",
        "Hee-Joon Bae",
        "Sung Yoon Lim",
        "Han-Gil Jeong",
        "Edward Choi"
      ],
      "abstract": "Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models.",
      "pdf_url": "http://arxiv.org/pdf/2505.02722v1",
      "published": "2025-05-05T15:23:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02722v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework",
      "authors": [
        "Andrzej Mizera",
        "Jakub Zarzycki"
      ],
      "abstract": "Cellular reprogramming, the artificial transformation of one cell type into\nanother, has been attracting increasing research attention due to its\ntherapeutic potential for complex diseases. However, discovering reprogramming\nstrategies through classical wet-lab experiments is hindered by lengthy time\ncommitments and high costs. In this study, we explore the use of deep\nreinforcement learning (DRL) to control Boolean network models of complex\nbiological systems, such as gene regulatory networks and signalling pathway\nnetworks. We formulate a novel control problem for Boolean network models under\nthe asynchronous update mode in the context of cellular reprogramming. To\nfacilitate scalability, we consider our previously introduced concept of a\npseudo-attractor and we improve our procedure for effective identification of\npseudo-attractor states. Finally, we devise a computational framework to solve\nthe control problem. To leverage the structure of biological systems, we\nincorporate graph neural networks with graph convolutions into the artificial\nneural network approximator for the action-value function learned by the DRL\nagent. Experiments on a number of large real-world biological networks from\nliterature demonstrate the scalability and effectiveness of our approach.",
      "pdf_url": "http://arxiv.org/pdf/2505.02712v1",
      "published": "2025-05-05T15:07:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02712v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.MN"
      ]
    },
    {
      "title": "Technical Report: Evaluating Goal Drift in Language Model Agents",
      "authors": [
        "Rauno Arike",
        "Elizabeth Donoway",
        "Henning Bartsch",
        "Marius Hobbhahn"
      ],
      "abstract": "As language models (LMs) are increasingly deployed as autonomous agents,\ntheir robust adherence to human-assigned objectives becomes crucial for safe\noperation. When these agents operate independently for extended periods without\nhuman oversight, even initially well-specified goals may gradually shift.\nDetecting and measuring goal drift - an agent's tendency to deviate from its\noriginal objective over time - presents significant challenges, as goals can\nshift gradually, causing only subtle behavioral changes. This paper proposes a\nnovel approach to analyzing goal drift in LM agents. In our experiments, agents\nare first explicitly given a goal through their system prompt, then exposed to\ncompeting objectives through environmental pressures. We demonstrate that while\nthe best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains\nnearly perfect goal adherence for more than 100,000 tokens in our most\ndifficult evaluation setting, all evaluated models exhibit some degree of goal\ndrift. We also find that goal drift correlates with models' increasing\nsusceptibility to pattern-matching behaviors as the context length grows.",
      "pdf_url": "http://arxiv.org/pdf/2505.02709v1",
      "published": "2025-05-05T15:06:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02709v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play",
      "authors": [
        "Yemin Shi",
        "Yu Shu",
        "Siwei Dong",
        "Guangyi Liu",
        "Jaward Sesay",
        "Jingwen Li",
        "Zhiting Hu"
      ],
      "abstract": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
      "pdf_url": "http://arxiv.org/pdf/2505.02707v1",
      "published": "2025-05-05T15:05:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02707v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ]
    },
    {
      "title": "AI Standardized Patient Improves Human Conversations in Advanced Cancer Care",
      "authors": [
        "Kurtis Haut",
        "Masum Hasan",
        "Thomas Carroll",
        "Ronald Epstein",
        "Taylan Sen",
        "Ehsan Hoque"
      ],
      "abstract": "Serious illness communication (SIC) in end-of-life care faces challenges such\nas emotional stress, cultural barriers, and balancing hope with honesty.\nDespite its importance, one of the few available ways for clinicians to\npractice SIC is with standardized patients, which is expensive, time-consuming,\nand inflexible. In this paper, we present SOPHIE, an AI-powered standardized\npatient simulation and automated feedback system. SOPHIE combines large\nlanguage models (LLMs), a lifelike virtual avatar, and automated, personalized\nfeedback based on clinical literature to provide remote, on-demand SIC\ntraining. In a randomized control study with healthcare students and\nprofessionals, SOPHIE users demonstrated significant improvement across three\ncritical SIC domains: Empathize, Be Explicit, and Empower. These results\nsuggest that AI-driven tools can enhance complex interpersonal communication\nskills, offering scalable, accessible solutions to address a critical gap in\nclinician education.",
      "pdf_url": "http://arxiv.org/pdf/2505.02694v1",
      "published": "2025-05-05T14:44:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02694v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
      "authors": [
        "Qianjun Pan",
        "Wenkai Ji",
        "Yuyang Ding",
        "Junsong Li",
        "Shilian Chen",
        "Junyi Wang",
        "Jie Zhou",
        "Qin Chen",
        "Min Zhang",
        "Yulan Wu",
        "Liang He"
      ],
      "abstract": "This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.02665v1",
      "published": "2025-05-05T14:14:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02665v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "A Note on Statistically Accurate Tabular Data Generation Using Large Language Models",
      "authors": [
        "Andrey Sidorenko"
      ],
      "abstract": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data.",
      "pdf_url": "http://arxiv.org/pdf/2505.02659v2",
      "published": "2025-05-05T14:05:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02659v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting",
      "authors": [
        "Shiwei Guo",
        "Ziang Chen",
        "Yupeng Ma",
        "Yunfei Han",
        "Yi Wang"
      ],
      "abstract": "The Transformer model has shown strong performance in multivariate time\nseries forecasting by leveraging channel-wise self-attention. However, this\napproach lacks temporal constraints when computing temporal features and does\nnot utilize cumulative historical series effectively.To address these\nlimitations, we propose the Structured Channel-wise Transformer with Cumulative\nHistorical state (SCFormer). SCFormer introduces temporal constraints to all\nlinear transformations, including the query, key, and value matrices, as well\nas the fully connected layers within the Transformer. Additionally, SCFormer\nemploys High-order Polynomial Projection Operators (HiPPO) to deal with\ncumulative historical time series, allowing the model to incorporate\ninformation beyond the look-back window during prediction. Extensive\nexperiments on multiple real-world datasets demonstrate that SCFormer\nsignificantly outperforms mainstream baselines, highlighting its effectiveness\nin enhancing time series forecasting. The code is publicly available at\nhttps://github.com/ShiweiGuo1995/SCFormer",
      "pdf_url": "http://arxiv.org/pdf/2505.02655v1",
      "published": "2025-05-05T13:59:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02655v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Eye Movements as Indicators of Deception: A Machine Learning Approach",
      "authors": [
        "Valentin Foucher",
        "Santiago de Leon-Martinez",
        "Robert Moro"
      ],
      "abstract": "Gaze may enhance the robustness of lie detectors but remains under-studied.\nThis study evaluated the efficacy of AI models (using fixations, saccades,\nblinks, and pupil size) for detecting deception in Concealed Information Tests\nacross two datasets. The first, collected with Eyelink 1000, contains gaze data\nfrom a computerized experiment where 87 participants revealed, concealed, or\nfaked the value of a previously selected card. The second, collected with Pupil\nNeon, involved 36 participants performing a similar task but facing an\nexperimenter. XGBoost achieved accuracies up to 74% in a binary classification\ntask (Revealing vs. Concealing) and 49% in a more challenging\nthree-classification task (Revealing vs. Concealing vs. Faking). Feature\nanalysis identified saccade number, duration, amplitude, and maximum pupil size\nas the most important for deception prediction. These results demonstrate the\nfeasibility of using gaze and AI to enhance lie detectors and encourage future\nresearch that may improve on this.",
      "pdf_url": "http://arxiv.org/pdf/2505.02649v1",
      "published": "2025-05-05T13:50:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02649v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints",
      "authors": [
        "Shubham Vaishnav",
        "Praveen Kumar Donta",
        "Sindri Magnússon"
      ],
      "abstract": "Internet of Things (IoT) systems increasingly operate in environments where\ndevices must respond in real time while managing fluctuating resource\nconstraints, including energy and bandwidth. Yet, current approaches often fall\nshort in addressing scenarios where operational constraints evolve over time.\nTo address these limitations, we propose a novel Budgeted Multi-Armed Bandit\nframework tailored for IoT applications with dynamic operational limits. Our\nmodel introduces a decaying violation budget, which permits limited constraint\nviolations early in the learning process and gradually enforces stricter\ncompliance over time. We present the Budgeted Upper Confidence Bound (UCB)\nalgorithm, which adaptively balances performance optimization and compliance\nwith time-varying constraints. We provide theoretical guarantees showing that\nBudgeted UCB achieves sublinear regret and logarithmic constraint violations\nover the learning horizon. Extensive simulations in a wireless communication\nsetting show that our approach achieves faster adaptation and better constraint\nsatisfaction than standard online learning methods. These results highlight the\nframework's potential for building adaptive, resource-aware IoT systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.02640v1",
      "published": "2025-05-05T13:33:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02640v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning",
      "authors": [
        "Xuan Lin",
        "Qingrui Liu",
        "Hongxin Xiang",
        "Daojian Zeng",
        "Xiangxiang Zeng"
      ],
      "abstract": "Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design.",
      "pdf_url": "http://arxiv.org/pdf/2505.02639v1",
      "published": "2025-05-05T13:31:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02639v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition",
      "authors": [
        "Yuanpeng Li"
      ],
      "abstract": "Compositional generalization is a crucial property in artificial\nintelligence, enabling models to handle novel combinations of known components.\nWhile most deep learning models lack this capability, certain models succeed in\nspecific tasks, suggesting the existence of governing conditions. This paper\nderives a necessary and sufficient condition for compositional generalization\nin neural networks. Conceptually, it requires that (i) the computational graph\nmatches the true compositional structure, and (ii) components encode just\nenough information in training. The condition is supported by mathematical\nproofs. This criterion combines aspects of architecture design, regularization,\nand training data properties. A carefully designed minimal example illustrates\nan intuitive understanding of the condition. We also discuss the potential of\nthe condition for assessing compositional generalization before training. This\nwork is a fundamental theoretical study of compositional generalization in\nneural networks.",
      "pdf_url": "http://arxiv.org/pdf/2505.02627v1",
      "published": "2025-05-05T13:13:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02627v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis",
      "authors": [
        "Qingkai Fang",
        "Yan Zhou",
        "Shoutao Guo",
        "Shaolei Zhang",
        "Yang Feng"
      ],
      "abstract": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
      "pdf_url": "http://arxiv.org/pdf/2505.02625v1",
      "published": "2025-05-05T12:53:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02625v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview",
      "authors": [
        "Shuyu Wang",
        "Angélique Saillet",
        "Philomène Le Gall",
        "Alain Lacroux",
        "Christelle Martin-Lacroux",
        "Vincent Brault"
      ],
      "abstract": "Artificial intelligence is used at various stages of the recruitment process\nto automatically select the best candidate for a position, with companies\nguaranteeing unbiased recruitment. However, the algorithms used are either\ntrained by humans or are based on learning from past experiences that were\nbiased. In this article, we propose to generate data mimicking external\n(discrimination) and internal biases (self-censorship) in order to train five\nclassic algorithms and to study the extent to which they do or do not find the\nbest candidates according to objective criteria. In addition, we study the\ninfluence of the anonymisation of files on the quality of predictions.",
      "pdf_url": "http://arxiv.org/pdf/2505.02609v1",
      "published": "2025-05-05T12:24:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02609v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "stat.AP",
        "stat.ME"
      ]
    },
    {
      "title": "Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem",
      "authors": [
        "Alberto Hernández-Espinosa",
        "Felipe S. Abrahão",
        "Olaf Witkowski",
        "Hector Zenil"
      ],
      "abstract": "The AI alignment problem, which focusses on ensuring that artificial\nintelligence (AI), including AGI and ASI, systems act according to human\nvalues, presents profound challenges. With the progression from narrow AI to\nArtificial General Intelligence (AGI) and Superintelligence, fears about\ncontrol and existential risk have escalated. This paper demonstrates that\nachieving complete alignment is inherently unattainable due to mathematical\nprinciples rooted in the foundations of predicate logic and computability, in\nparticular Turing's computational universality, G\\\"odel's incompleteness and\nChaitin's randomness. Instead, we argue that embracing AI misalignment or\nagent's `neurodivergence' as a contingent strategy, defined as fostering a\ndynamic ecosystem of competing, partially aligned agents, is a possible only\nviable path to mitigate risks. Through mathematical proofs and an experimental\ndesign, we explore how misalignment may serve and should be promoted as a\ncounterbalancing mechanism to team up with whichever agents are most aligned AI\nto human values, ensuring that no single system dominates destructively. The\nmain premise of our contribution is that misalignment is inevitable because\nfull AI-human alignment is a mathematical impossibility from Turing-complete\nsystems which we also prove in this paper, a feature then inherited to AGI and\nASI systems. We introduce and test `change-of-opinion' attacks based on this\nkind of perturbation and intervention analysis to study how agents may\nneutralise friendly or unfriendly AIs through cooperation, competition or\nmalice.",
      "pdf_url": "http://arxiv.org/pdf/2505.02581v1",
      "published": "2025-05-05T11:33:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02581v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning",
      "authors": [
        "Lingxiao Kong",
        "Cong Yang",
        "Susanne Neufang",
        "Oya Deniz Beyan",
        "Zeyd Boukhers"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.",
      "pdf_url": "http://arxiv.org/pdf/2505.02579v2",
      "published": "2025-05-05T11:30:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02579v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning",
      "authors": [
        "Sergio Hernández-Gutiérrez",
        "Minttu Alakuijala",
        "Alexander V. Nikitin",
        "Pekka Marttinen"
      ],
      "abstract": "Reasoning tasks are crucial in many domains, especially in science and\nengineering. Although large language models (LLMs) have made progress in\nreasoning tasks using techniques such as chain-of-thought and least-to-most\nprompting, these approaches still do not effectively scale to complex problems\nin either their performance or execution time. Moreover, they often require\nadditional supervision for each new task, such as in-context examples. In this\nwork, we introduce Recursive Decomposition with Dependencies (RDD), a scalable\ndivide-and-conquer method for solving reasoning problems that requires less\nsupervision than prior approaches. Our method can be directly applied to a new\nproblem class even in the absence of any task-specific guidance. Furthermore,\nRDD supports sub-task dependencies, allowing for ordered execution of\nsub-tasks, as well as an error recovery mechanism that can correct mistakes\nmade in previous steps. We evaluate our approach on two benchmarks with six\ndifficulty levels each and in two in-context settings: one with task-specific\nexamples and one without. Our results demonstrate that RDD outperforms other\nmethods in a compute-matched setting as task complexity increases, while also\nbeing more computationally efficient.",
      "pdf_url": "http://arxiv.org/pdf/2505.02576v1",
      "published": "2025-05-05T11:24:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02576v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Rethinking Federated Graph Learning: A Data Condensation Perspective",
      "authors": [
        "Hao Zhang",
        "Xunkai Li",
        "Yinlin Zhu",
        "Lianglin Hu"
      ],
      "abstract": "Federated graph learning is a widely recognized technique that promotes\ncollaborative training of graph neural networks (GNNs) by multi-client\ngraphs.However, existing approaches heavily rely on the communication of model\nparameters or gradients for federated optimization and fail to adequately\naddress the data heterogeneity introduced by intricate and diverse graph\ndistributions. Although some methods attempt to share additional messages among\nthe server and clients to improve federated convergence during communication,\nthey introduce significant privacy risks and increase communication overhead.\nTo address these issues, we introduce the concept of a condensed graph as a\nnovel optimization carrier to address FGL data heterogeneity and propose a new\nFGL paradigm called FedGM. Specifically, we utilize a generalized condensation\ngraph consensus to aggregate comprehensive knowledge from distributed graphs,\nwhile minimizing communication costs and privacy risks through a single\ntransmission of the condensed data. Extensive experiments on six public\ndatasets consistently demonstrate the superiority of FedGM over\nstate-of-the-art baselines, highlighting its potential for a novel FGL\nparadigm.",
      "pdf_url": "http://arxiv.org/pdf/2505.02573v1",
      "published": "2025-05-05T11:23:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02573v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.SI"
      ]
    },
    {
      "title": "Robustness questions the interpretability of graph neural networks: what to do?",
      "authors": [
        "Kirill Lukyanov",
        "Georgii Sazonov",
        "Serafim Boyarsky",
        "Ilya Makarov"
      ],
      "abstract": "Graph Neural Networks (GNNs) have become a cornerstone in graph-based data\nanalysis, with applications in diverse domains such as bioinformatics, social\nnetworks, and recommendation systems. However, the interplay between model\ninterpretability and robustness remains poorly understood, especially under\nadversarial scenarios like poisoning and evasion attacks. This paper presents a\ncomprehensive benchmark to systematically analyze the impact of various factors\non the interpretability of GNNs, including the influence of\nrobustness-enhancing defense mechanisms.\n  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across\nfive datasets from two distinct domains, employing four interpretability\nmetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how\ndefenses against poisoning and evasion attacks, applied before and during model\ntraining, affect interpretability and highlights critical trade-offs between\nrobustness and interpretability. The framework will be published as open\nsource.\n  The results reveal significant variations in interpretability depending on\nthe chosen defense methods and model architecture characteristics. By\nestablishing a standardized benchmark, this work provides a foundation for\ndeveloping GNNs that are both robust to adversarial threats and interpretable,\nfacilitating trust in their deployment in sensitive applications.",
      "pdf_url": "http://arxiv.org/pdf/2505.02566v1",
      "published": "2025-05-05T11:14:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02566v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Bielik v3 Small: Technical Report",
      "authors": [
        "Krzysztof Ociepa",
        "Łukasz Flis",
        "Remigiusz Kinas",
        "Krzysztof Wróbel",
        "Adrian Gwoździej"
      ],
      "abstract": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.",
      "pdf_url": "http://arxiv.org/pdf/2505.02550v1",
      "published": "2025-05-05T10:39:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02550v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T50",
        "I.2.7"
      ]
    },
    {
      "title": "Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data",
      "authors": [
        "Ljubomir Rokvic",
        "Panayiotis Danassis",
        "Boi Faltings"
      ],
      "abstract": "In Federated Learning, heterogeneity in client data distributions often means\nthat a single global model does not have the best performance for individual\nclients. Consider for example training a next-word prediction model for\nkeyboards: user-specific language patterns due to demographics (dialect, age,\netc.), language proficiency, and writing style result in a highly non-IID\ndataset across clients. Other examples are medical images taken with different\nmachines, or driving data from different vehicle types. To address this, we\npropose a simple yet effective personalized federated learning framework\n(pFedLIA) that utilizes a computationally efficient influence approximation,\ncalled `Lazy Influence', to cluster clients in a distributed manner before\nmodel aggregation. Within each cluster, data owners collaborate to jointly\ntrain a model that captures the specific data patterns of the clients. Our\nmethod has been shown to successfully recover the global model's performance\ndrop due to the non-IID-ness in various synthetic and real-world settings,\nspecifically a next-word prediction task on the Nordic languages as well as\nseveral benchmark tasks. It matches the performance of a hypothetical Oracle\nclustering, and significantly improves on existing baselines, e.g., an\nimprovement of 17% on CIFAR100.",
      "pdf_url": "http://arxiv.org/pdf/2505.02540v1",
      "published": "2025-05-05T10:26:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02540v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations",
      "authors": [
        "Davide Sartor",
        "Alberto Sinigaglia",
        "Gian Antonio Susto"
      ],
      "abstract": "Conventional techniques for imposing monotonicity in MLPs by construction\ninvolve the use of non-negative weight constraints and bounded activation\nfunctions, which pose well-known optimization challenges. In this work, we\ngeneralize previous theoretical results, showing that MLPs with non-negative\nweight constraint and activations that saturate on alternating sides are\nuniversal approximators for monotonic functions. Additionally, we show an\nequivalence between the saturation side in the activations and the sign of the\nweight constraint. This connection allows us to prove that MLPs with convex\nmonotone activations and non-positive constrained weights also qualify as\nuniversal approximators, in contrast to their non-negative constrained\ncounterparts. Our results provide theoretical grounding to the empirical\neffectiveness observed in previous works while leading to possible\narchitectural simplification. Moreover, to further alleviate the optimization\ndifficulties, we propose an alternative formulation that allows the network to\nadjust its activations according to the sign of the weights. This eliminates\nthe requirement for weight reparameterization, easing initialization and\nimproving training stability. Experimental evaluation reinforces the validity\nof the theoretical results, showing that our novel approach compares favourably\nto traditional monotonic architectures.",
      "pdf_url": "http://arxiv.org/pdf/2505.02537v2",
      "published": "2025-05-05T10:18:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02537v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
      "authors": [
        "Dimitrios Kafetzis",
        "Ramin Khalili",
        "Iordanis Koutsopoulos"
      ],
      "abstract": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
      "pdf_url": "http://arxiv.org/pdf/2505.02533v1",
      "published": "2025-05-05T10:16:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02533v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ]
    },
    {
      "title": "Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics",
      "authors": [
        "MohammadAli Shaeri",
        "Jinhan Liu",
        "Mahsa Shoaran"
      ],
      "abstract": "Advanced neural interfaces are transforming applications ranging from\nneuroscience research to diagnostic tools (for mental state recognition, tremor\nand seizure detection) as well as prosthetic devices (for motor and\ncommunication recovery). By integrating complex functions into miniaturized\nneural devices, these systems unlock significant opportunities for personalized\nassistive technologies and adaptive therapeutic interventions. Leveraging\nhigh-density neural recordings, on-site signal processing, and machine learning\n(ML), these interfaces extract critical features, identify disease\nneuro-markers, and enable accurate, low-latency neural decoding. This\nintegration facilitates real-time interpretation of neural signals, adaptive\nmodulation of brain activity, and efficient control of assistive devices.\nMoreover, the synergy between neural interfaces and ML has paved the way for\nself-sufficient, ubiquitous platforms capable of operating in diverse\nenvironments with minimal hardware costs and external dependencies. In this\nwork, we review recent advancements in AI-driven decoding algorithms and\nenergy-efficient System-on-Chip (SoC) platforms for next-generation\nminiaturized neural devices. These innovations highlight the potential for\ndeveloping intelligent neural interfaces, addressing critical challenges in\nscalability, reliability, interpretability, and user adaptability.",
      "pdf_url": "http://arxiv.org/pdf/2505.02516v1",
      "published": "2025-05-05T09:49:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02516v1",
      "categories": [
        "cs.AI",
        "cs.AR",
        "cs.LG",
        "eess.SP",
        "q-bio.NC",
        "I.2.0; B.7.0; I.5.1; C.3"
      ]
    },
    {
      "title": "Unveiling the Landscape of LLM Deployment in the Wild: An Empirical Study",
      "authors": [
        "Xinyi Hou",
        "Jiahao Han",
        "Yanjie Zhao",
        "Haoyu Wang"
      ],
      "abstract": "Background: Large language models (LLMs) are increasingly deployed via\nopen-source and commercial frameworks, enabling individuals and organizations\nto self-host advanced AI capabilities. However, insecure defaults and\nmisconfigurations often expose LLM services to the public Internet, posing\nsignificant security and system engineering risks. Aims: This study aims to\nunveil the current landscape of public-facing LLM deployments in the wild\nthrough a large-scale empirical study, focusing on service prevalence, exposure\ncharacteristics, systemic vulnerabilities, and associated risks. Method: We\nconducted an Internet-wide measurement to identify public-facing LLM\ndeployments across 15 frameworks, discovering 320,102 services. We extracted\n158 unique API endpoints, grouped into 12 functional categories based on\ncapabilities and security risks. We further analyzed configurations,\nauthentication practices, and geographic distributions, revealing deployment\ntrends and systemic issues in real-world LLM system engineering. Results: Our\nstudy shows that public LLM deployments are rapidly growing but often insecure.\nAmong all endpoints, we observe widespread use of insecure protocols, poor TLS\nconfigurations, and unauthenticated access to critical operations. Security\nrisks, including model disclosure, system leakage, and unauthorized access, are\npervasive, highlighting the need for secure-by-default frameworks and stronger\ndeployment practices. Conclusions: Public-facing LLM deployments suffer from\nwidespread security and configuration flaws, exposing services to misuse, model\ntheft, resource hijacking, and remote exploitation. Strengthening default\nsecurity, deployment practices, and operational standards is critical for the\ngrowing self-hosted LLM ecosystem.",
      "pdf_url": "http://arxiv.org/pdf/2505.02502v1",
      "published": "2025-05-05T09:30:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02502v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions",
      "authors": [
        "Asma Brazi",
        "Boris Meden",
        "Fabrice Mayran de Chamisso",
        "Steve Bourgeois",
        "Vincent Lepetit"
      ],
      "abstract": "We introduce Corr2Distrib, the first correspondence-based method which\nestimates a 6D camera pose distribution from an RGB image, explaining the\nobservations. Indeed, symmetries and occlusions introduce visual ambiguities,\nleading to multiple valid poses. While a few recent methods tackle this\nproblem, they do not rely on local correspondences which, according to the BOP\nChallenge, are currently the most effective way to estimate a single 6DoF pose\nsolution. Using correspondences to estimate a pose distribution is not\nstraightforward, since ambiguous correspondences induced by visual ambiguities\ndrastically decrease the performance of PnP. With Corr2Distrib, we turn these\nambiguities into an advantage to recover all valid poses. Corr2Distrib first\nlearns a symmetry-aware representation for each 3D point on the object's\nsurface, characterized by a descriptor and a local frame. This representation\nenables the generation of 3DoF rotation hypotheses from single 2D-3D\ncorrespondences. Next, we refine these hypotheses into a 6DoF pose distribution\nusing PnP and pose scoring. Our experimental evaluations on complex\nnon-synthetic scenes show that Corr2Distrib outperforms state-of-the-art\nsolutions for both pose distribution estimation and single pose estimation from\nan RGB image, demonstrating the potential of correspondences-based approaches.",
      "pdf_url": "http://arxiv.org/pdf/2505.02501v1",
      "published": "2025-05-05T09:29:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02501v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Beyond the model: Key differentiators in large language models and multi-agent services",
      "authors": [
        "Muskaan Goyal",
        "Pranav Bhasin"
      ],
      "abstract": "With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it\nhas become evident that large language models (LLMs) are no longer the sole\ndefining factor in generative AI. As many now operate at comparable levels of\ncapability, the real race is not about having the biggest model but optimizing\nthe surrounding ecosystem, including data quality and management, computational\nefficiency, latency, and evaluation frameworks. This review article delves into\nthese critical differentiators that ensure modern AI services are efficient and\nprofitable.",
      "pdf_url": "http://arxiv.org/pdf/2505.02489v1",
      "published": "2025-05-05T09:15:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02489v1",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.MA",
        "cs.SE"
      ]
    },
    {
      "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
      "authors": [
        "Jinpeng Chen",
        "Runmin Cong",
        "Yuzhi Zhao",
        "Hongzheng Yang",
        "Guangneng Hu",
        "Horace Ho Shing Ip",
        "Sam Kwong"
      ],
      "abstract": "Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal\nLarge Language Models (MLLMs) to incrementally learn new tasks without\ncatastrophic forgetting. In this paper, we explore forgetting in this context,\ncategorizing it into superficial forgetting and essential forgetting.\nSuperficial forgetting refers to cases where the model's knowledge may not be\ngenuinely lost, but its responses to previous tasks deviate from expected\nformats due to the influence of subsequent tasks' answer styles, making the\nresults unusable. By contrast, essential forgetting refers to situations where\nthe model provides correctly formatted but factually inaccurate answers,\nindicating a true loss of knowledge. Assessing essential forgetting\nnecessitates addressing superficial forgetting first, as severe superficial\nforgetting can obscure the model's knowledge state. Hence, we first introduce\nthe Answer Style Diversification (ASD) paradigm, which defines a standardized\nprocess for transforming data styles across different tasks, unifying their\ntraining sets into similarly diversified styles to prevent superficial\nforgetting caused by style shifts. Building on this, we propose RegLoRA to\nmitigate essential forgetting. RegLoRA stabilizes key parameters where prior\nknowledge is primarily stored by applying regularization, enabling the model to\nretain existing competencies. Experimental results demonstrate that our overall\nmethod, SEFE, achieves state-of-the-art performance.",
      "pdf_url": "http://arxiv.org/pdf/2505.02486v1",
      "published": "2025-05-05T09:09:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02486v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Integrating Column Generation and Large Neighborhood Search for Bus Driver Scheduling with Complex Break Constraints",
      "authors": [
        "Lucas Kletzander",
        "Tommaso Mannelli Mazzoli",
        "Nysret Musliu",
        "Pascal Van Hentenryck"
      ],
      "abstract": "The Bus Driver Scheduling Problem (BDSP) is a combinatorial optimization\nproblem with the goal to design shifts to cover prearranged bus tours. The\nobjective takes into account the operational cost as well as the satisfaction\nof drivers. This problem is heavily constrained due to strict legal rules and\ncollective agreements. The objective of this article is to provide\nstate-of-the-art exact and hybrid solution methods that can provide\nhigh-quality solutions for instances of different sizes. This work presents a\ncomprehensive study of both an exact method, Branch and Price (B&P), as well as\na Large Neighborhood Search (LNS) framework which uses B&P or Column Generation\n(CG) for the repair phase to solve the BDSP. It further proposes and evaluates\na novel deeper integration of B&P and LNS, storing the generated columns from\nthe LNS subproblems and reusing them for other subproblems, or to find better\nglobal solutions. The article presents a detailed analysis of several\ncomponents of the solution methods and their impact, including general\nimprovements for the B&P subproblem, which is a high-dimensional Resource\nConstrained Shortest Path Problem (RCSPP), and the components of the LNS. The\nevaluation shows that our approach provides new state-of-the-art results for\ninstances of all sizes, including exact solutions for small instances, and low\ngaps to a known lower bound for mid-sized instances. Conclusions: We observe\nthat B&P provides the best results for small instances, while the tight\nintegration of LNS and CG can provide high-quality solutions for larger\ninstances, further improving over LNS which just uses CG as a black box. The\nproposed methods are general and can also be applied to other rule sets and\nrelated optimization problems",
      "pdf_url": "http://arxiv.org/pdf/2505.02485v1",
      "published": "2025-05-05T09:08:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02485v1",
      "categories": [
        "math.OC",
        "cs.AI"
      ]
    },
    {
      "title": "El Agente: An Autonomous Agent for Quantum Chemistry",
      "authors": [
        "Yunheng Zou",
        "Austin H. Cheng",
        "Abdulrahman Aldossary",
        "Jiaru Bai",
        "Shi Xuan Leong",
        "Jorge Arturo Campos-Gonzalez-Angulo",
        "Changhyeok Choi",
        "Cher Tian Ser",
        "Gary Tom",
        "Andrew Wang",
        "Zijian Zhang",
        "Ilya Yakavets",
        "Han Hao",
        "Chris Crebolder",
        "Varinia Bernales",
        "Alán Aspuru-Guzik"
      ],
      "abstract": "Computational chemistry tools are widely used to study the behaviour of\nchemical phenomena. Yet, the complexity of these tools can make them\ninaccessible to non-specialists and challenging even for experts. In this work,\nwe introduce El Agente Q, an LLM-based multi-agent system that dynamically\ngenerates and executes quantum chemistry workflows from natural language user\nprompts. The system is built on a novel cognitive architecture featuring a\nhierarchical memory framework that enables flexible task decomposition,\nadaptive tool selection, post-analysis, and autonomous file handling and\nsubmission. El Agente Q is benchmarked on six university-level course exercises\nand two case studies, demonstrating robust problem-solving performance\n(averaging >87% task success) and adaptive error handling through in situ\ndebugging. It also supports longer-term, multi-step task execution for more\ncomplex workflows, while maintaining transparency through detailed action trace\nlogs. Together, these capabilities lay the foundation for increasingly\nautonomous and accessible quantum chemistry.",
      "pdf_url": "http://arxiv.org/pdf/2505.02484v1",
      "published": "2025-05-05T09:07:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02484v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "physics.chem-ph"
      ]
    },
    {
      "title": "Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning",
      "authors": [
        "Changxin Huang",
        "Junyang Liang",
        "Yanbin Chang",
        "Jingzhao Xu",
        "Jianqiang Li"
      ],
      "abstract": "Enabling a high-degree-of-freedom robot to learn specific skills is a\nchallenging task due to the complexity of robotic dynamics. Reinforcement\nlearning (RL) has emerged as a promising solution; however, addressing such\nproblems requires the design of multiple reward functions to account for\nvarious constraints in robotic motion. Existing approaches typically sum all\nreward components indiscriminately to optimize the RL value function and\npolicy. We argue that this uniform inclusion of all reward components in policy\noptimization is inefficient and limits the robot's learning performance. To\naddress this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework\nbased on Large Language Models (LLMs). This paradigm dynamically adjusts the\nlearning intensity of each reward component throughout the policy optimization\nprocess, enabling robots to acquire skills in a gradual and structured manner.\nSpecifically, we design a multi-branch value network, where each branch\ncorresponds to a distinct reward component. During policy optimization, each\nbranch is assigned a weight that reflects its importance, and these weights are\nautomatically computed based on rules designed by LLMs. The LLM generates a\nrule set in advance, derived from the task description, and during training, it\nselects a weight calculation rule from the library based on language prompts\nthat evaluate the performance of each branch. Experimental results demonstrate\nthat the AHRS method achieves an average 6.48% performance improvement across\nmultiple high-degree-of-freedom robotic tasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.02483v1",
      "published": "2025-05-05T09:06:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02483v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging",
      "authors": [
        "Valerio Guarrasi",
        "Klara Mogensen",
        "Sara Tassinari",
        "Sara Qvarlander",
        "Paolo Soda"
      ],
      "abstract": "Multimodal deep learning harnesses diverse imaging modalities, such as MRI\nsequences, to enhance diagnostic accuracy in medical imaging. A key challenge\nis determining the optimal timing for integrating these\nmodalities-specifically, identifying the network layers where fusion modules\nshould be inserted. Current approaches often rely on manual tuning or\nexhaustive search, which are computationally expensive without any guarantee of\nconverging to optimal results. We propose a sequential forward search algorithm\nthat incrementally activates and evaluates candidate fusion modules at\ndifferent layers of a multimodal network. At each step, the algorithm retrains\nfrom previously learned weights and compares validation loss to identify the\nbest-performing configuration. This process systematically reduces the search\nspace, enabling efficient identification of the optimal fusion timing without\nexhaustively testing all possible module placements. The approach is validated\non two multimodal MRI datasets, each addressing different classification tasks.\nOur algorithm consistently identified configurations that outperformed unimodal\nbaselines, late fusion, and a brute-force ensemble of all potential fusion\nplacements. These architectures demonstrated superior accuracy, F-score, and\nspecificity while maintaining competitive or improved AUC values. Furthermore,\nthe sequential nature of the search significantly reduced computational\noverhead, making the optimization process more practical. By systematically\ndetermining the optimal timing to fuse imaging modalities, our method advances\nmultimodal deep learning for medical imaging. It provides an efficient and\nrobust framework for fusion optimization, paving the way for improved clinical\ndecision-making and more adaptable, scalable architectures in medical AI\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2505.02467v1",
      "published": "2025-05-05T08:53:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02467v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Incentivizing Inclusive Contributions in Model Sharing Markets",
      "authors": [
        "Enpei Zhang",
        "Jingyi Chai",
        "Rui Ye",
        "Yanfeng Wang",
        "Siheng Chen"
      ],
      "abstract": "While data plays a crucial role in training contemporary AI models, it is\nacknowledged that valuable public data will be exhausted in a few years,\ndirecting the world's attention towards the massive decentralized private data.\nHowever, the privacy-sensitive nature of raw data and lack of incentive\nmechanism prevent these valuable data from being fully exploited. Addressing\nthese challenges, this paper proposes inclusive and incentivized personalized\nfederated learning (iPFL), which incentivizes data holders with diverse\npurposes to collaboratively train personalized models without revealing raw\ndata. iPFL constructs a model-sharing market by solving a graph-based training\noptimization and incorporates an incentive mechanism based on game theory\nprinciples. Theoretical analysis shows that iPFL adheres to two key incentive\nproperties: individual rationality and truthfulness. Empirical studies on\neleven AI tasks (e.g., large language models' instruction-following tasks)\ndemonstrate that iPFL consistently achieves the highest economic utility, and\nbetter or comparable model performance compared to baseline methods. We\nanticipate that our iPFL can serve as a valuable technique for boosting future\nAI models on decentralized private data while making everyone satisfied.",
      "pdf_url": "http://arxiv.org/pdf/2505.02462v1",
      "published": "2025-05-05T08:45:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02462v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT"
      ]
    },
    {
      "title": "Investigating the Impact of Personalized AI Tutors on Language Learning Performance",
      "authors": [
        "Simon Suh"
      ],
      "abstract": "Driven by the global shift towards online learning prompted by the COVID 19\npandemic, Artificial Intelligence has emerged as a pivotal player in the field\nof education. Intelligent Tutoring Systems offer a new method of personalized\nteaching, replacing the limitations of traditional teaching methods. However,\nconcerns arise about the ability of AI tutors to address skill development and\nengagement during the learning process. In this paper, I will conduct a quasi\nexperiment with paired sample t test on 34 students pre and post use of AI\ntutors in language learning platforms like Santa and Duolingo to examine the\nrelationship between students engagement, academic performance, and students\nsatisfaction during a personalized language learning experience.",
      "pdf_url": "http://arxiv.org/pdf/2505.02443v1",
      "published": "2025-05-05T08:11:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02443v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "I.2.6; K.3.1"
      ]
    },
    {
      "title": "MSFNet-CPD: Multi-Scale Cross-Modal Fusion Network for Crop Pest Detection",
      "authors": [
        "Jiaqi Zhang",
        "Zhuodong Liu",
        "Kejian Yu"
      ],
      "abstract": "Accurate identification of agricultural pests is essential for crop\nprotection but remains challenging due to the large intra-class variance and\nfine-grained differences among pest species. While deep learning has advanced\npest detection, most existing approaches rely solely on low-level visual\nfeatures and lack effective multi-modal integration, leading to limited\naccuracy and poor interpretability. Moreover, the scarcity of high-quality\nmulti-modal agricultural datasets further restricts progress in this field. To\naddress these issues, we construct two novel multi-modal benchmarks-CTIP102 and\nSTIP102-based on the widely-used IP102 dataset, and introduce a Multi-scale\nCross-Modal Fusion Network (MSFNet-CPD) for robust pest detection. Our approach\nenhances visual quality via a super-resolution reconstruction module, and feeds\nboth the original and reconstructed images into the network to improve clarity\nand detection performance. To better exploit semantic cues, we propose an\nImage-Text Fusion (ITF) module for joint modeling of visual and textual\nfeatures, and an Image-Text Converter (ITC) that reconstructs fine-grained\ndetails across multiple scales to handle challenging backgrounds. Furthermore,\nwe introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to\ngenerate a more complex and diverse pest detection dataset, MTIP102, improving\nthe model's generalization to real-world scenarios. Extensive experiments\ndemonstrate that MSFNet-CPD consistently outperforms state-of-the-art methods\non multiple pest detection benchmarks. All code and datasets will be made\npublicly available at: https://github.com/Healer-ML/MSFNet-CPD.",
      "pdf_url": "http://arxiv.org/pdf/2505.02441v1",
      "published": "2025-05-05T08:10:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02441v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning",
      "authors": [
        "Yang Deng",
        "Yaohui Liu",
        "Rui Liang",
        "Dafang Zhao",
        "Donghua Xie",
        "Ittetsu Taniguchi",
        "Dan Wang"
      ],
      "abstract": "The building thermodynamics model, which predicts real-time indoor\ntemperature changes under potential HVAC (Heating, Ventilation, and Air\nConditioning) control operations, is crucial for optimizing HVAC control in\nbuildings. While pioneering studies have attempted to develop such models for\nvarious building environments, these models often require extensive data\ncollection periods and rely heavily on expert knowledge, making the modeling\nprocess inefficient and limiting the reusability of the models. This paper\nexplores a model ensemble perspective that utilizes existing developed models\nas base models to serve a target building environment, thereby providing\naccurate predictions while reducing the associated efforts. Given that building\ndata streams are non-stationary and the number of base models may increase, we\npropose a Hierarchical Reinforcement Learning (HRL) approach to dynamically\nselect and weight the base models. Our approach employs a two-tiered\ndecision-making process: the high-level focuses on model selection, while the\nlow-level determines the weights of the selected models. We thoroughly evaluate\nthe proposed approach through offline experiments and an on-site case study,\nand the experimental results demonstrate the effectiveness of our method.",
      "pdf_url": "http://arxiv.org/pdf/2505.02439v1",
      "published": "2025-05-05T08:09:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02439v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability",
      "authors": [
        "Pouria Fatemi",
        "Ehsan Sharifian",
        "Mohammad Hossein Yassaee"
      ],
      "abstract": "Counterfactual explanations enhance interpretability by identifying\nalternative inputs that produce different outputs, offering localized insights\ninto model decisions. However, traditional methods often neglect causal\nrelationships, leading to unrealistic examples. While newer approaches\nintegrate causality, they are computationally expensive. To address these\nchallenges, we propose an efficient method based on backtracking\ncounterfactuals that incorporates causal reasoning to generate actionable\nexplanations. We first examine the limitations of existing methods and then\nintroduce our novel approach and its features. We also explore the relationship\nbetween our method and previous techniques, demonstrating that it generalizes\nthem in specific scenarios. Finally, experiments show that our method provides\ndeeper insights into model outputs.",
      "pdf_url": "http://arxiv.org/pdf/2505.02435v1",
      "published": "2025-05-05T08:01:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.02435v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    }
  ]
}