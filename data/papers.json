{
  "last_updated": "2025-07-18T00:56:01.553755",
  "papers": [
    {
      "title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis",
      "authors": [
        "Trong-Thang Pham",
        "Anh Nguyen",
        "Zhigang Deng",
        "Carol C. Wu",
        "Hien Van Nguyen",
        "Ngan Le"
      ],
      "abstract": "Radiologists rely on eye movements to navigate and interpret medical images.\nA trained radiologist possesses knowledge about the potential diseases that may\nbe present in the images and, when searching, follows a mental checklist to\nlocate them using their gaze. This is a key observation, yet existing models\nfail to capture the underlying intent behind each fixation. In this paper, we\nintroduce a deep learning-based approach, RadGazeIntent, designed to model this\nbehavior: having an intention to find something and actively searching for it.\nOur transformer-based architecture processes both the temporal and spatial\ndimensions of gaze data, transforming fine-grained fixation features into\ncoarse, meaningful representations of diagnostic intent to interpret\nradiologists' goals. To capture the nuances of radiologists' varied\nintention-driven behaviors, we process existing medical eye-tracking datasets\nto create three intention-labeled subsets: RadSeq (Systematic Sequential\nSearch), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid\nPattern). Experimental results demonstrate RadGazeIntent's ability to predict\nwhich findings radiologists are examining at specific moments, outperforming\nbaseline methods across all intention-labeled datasets.",
      "pdf_url": "http://arxiv.org/pdf/2507.12461v1",
      "published": "2025-07-16T17:58:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12461v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling",
      "authors": [
        "Suman Adhya",
        "Debarshi Kumar Sanyal"
      ],
      "abstract": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.",
      "pdf_url": "http://arxiv.org/pdf/2507.12451v1",
      "published": "2025-07-16T17:47:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12451v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LLM-Based Config Synthesis requires Disambiguation",
      "authors": [
        "Rajdeep Mondal",
        "Nikolaj Bjorner",
        "Todd Millstein",
        "Alan Tang",
        "George Varghese"
      ],
      "abstract": "Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors.",
      "pdf_url": "http://arxiv.org/pdf/2507.12443v1",
      "published": "2025-07-16T17:29:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12443v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.HC",
        "cs.PL"
      ]
    },
    {
      "title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length",
      "authors": [
        "Saptarshi Mitra",
        "Rachid Karami",
        "Haocheng Xu",
        "Sitao Huang",
        "Hyoukjun Kwon"
      ],
      "abstract": "The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework.",
      "pdf_url": "http://arxiv.org/pdf/2507.12442v1",
      "published": "2025-07-16T17:28:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12442v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos",
      "authors": [
        "Ruihan Yang",
        "Qinxi Yu",
        "Yecheng Wu",
        "Rui Yan",
        "Borui Li",
        "An-Chieh Cheng",
        "Xueyan Zou",
        "Yunhao Fang",
        "Hongxu Yin",
        "Sifei Liu",
        "Song Han",
        "Yao Lu",
        "Xiaolong Wang"
      ],
      "abstract": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Ego\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA",
      "pdf_url": "http://arxiv.org/pdf/2507.12440v2",
      "published": "2025-07-16T17:27:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12440v2",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models",
      "authors": [
        "Yik Siu Chan",
        "Zheng-Xin Yong",
        "Stephen H. Bach"
      ],
      "abstract": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.",
      "pdf_url": "http://arxiv.org/pdf/2507.12428v1",
      "published": "2025-07-16T17:16:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12428v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation",
      "authors": [
        "Ashkan Shakarami",
        "Azade Farshad",
        "Yousef Yeganeh",
        "Lorenzo Nicole",
        "Peter Schuffler",
        "Stefano Ghidoni",
        "Nassir Navab"
      ],
      "abstract": "We propose UTS, a unit-based tissue segmentation framework for histopathology\nthat classifies each fixed-size 32 * 32 tile, rather than each pixel, as the\nsegmentation unit. This approach reduces annotation effort and improves\ncomputational efficiency without compromising accuracy. To implement this\napproach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits\nthe multi-level feature representation to capture both fine-grained morphology\nand global tissue context. Trained to segment breast tissue into three\ncategories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports\nclinically relevant tasks such as tumor-stroma quantification and surgical\nmargin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it\noutperforms U-Net variants and transformer-based baselines. Code and Dataset\nwill be available at GitHub.",
      "pdf_url": "http://arxiv.org/pdf/2507.12427v1",
      "published": "2025-07-16T17:15:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12427v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data",
      "authors": [
        "Chandana Cheerla"
      ],
      "abstract": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot",
      "pdf_url": "http://arxiv.org/pdf/2507.12425v1",
      "published": "2025-07-16T17:13:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12425v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.IR"
      ]
    },
    {
      "title": "Mixture of Raytraced Experts",
      "authors": [
        "Andrea Perin",
        "Giacomo Lagomarsini",
        "Claudio Gallicchio",
        "Giuseppe Nuti"
      ],
      "abstract": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing",
      "pdf_url": "http://arxiv.org/pdf/2507.12419v1",
      "published": "2025-07-16T17:08:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12419v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval",
      "authors": [
        "Jaehyun Kwak",
        "Ramahdani Muhammad Izaaz Inhar",
        "Se-Young Yun",
        "Sung-Ju Lee"
      ],
      "abstract": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.",
      "pdf_url": "http://arxiv.org/pdf/2507.12416v1",
      "published": "2025-07-16T17:06:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12416v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models",
      "authors": [
        "Santosh Vasa",
        "Aditi Ramadwar",
        "Jnana Rama Krishna Darabattula",
        "Md Zafar Anwar",
        "Stanislaw Antol",
        "Andrei Vatavu",
        "Thomas Monninger",
        "Sihao Ding"
      ],
      "abstract": "Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.",
      "pdf_url": "http://arxiv.org/pdf/2507.12414v1",
      "published": "2025-07-16T17:04:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12414v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data",
      "authors": [
        "Dzung Dinh",
        "Boqi Chen",
        "Marc Niethammer",
        "Junier Oliva"
      ],
      "abstract": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.",
      "pdf_url": "http://arxiv.org/pdf/2507.12412v1",
      "published": "2025-07-16T17:00:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12412v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Probing for Arithmetic Errors in Language Models",
      "authors": [
        "Yucheng Sun",
        "Alessandro Stolfo",
        "Mrinmaya Sachan"
      ],
      "abstract": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.",
      "pdf_url": "http://arxiv.org/pdf/2507.12379v1",
      "published": "2025-07-16T16:27:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12379v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities",
      "authors": [
        "Diganta Misra",
        "Nizar Islah",
        "Victor May",
        "Brice Rauby",
        "Zihan Wang",
        "Justine Gehring",
        "Antonio Orvieto",
        "Muawiz Chaudhary",
        "Eilif B. Muller",
        "Irina Rish",
        "Samira Ebrahimi Kahou",
        "Massimo Caccia"
      ],
      "abstract": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.",
      "pdf_url": "http://arxiv.org/pdf/2507.12367v1",
      "published": "2025-07-16T16:10:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12367v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ]
    },
    {
      "title": "FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization",
      "authors": [
        "Yifei Zhou",
        "Xuchu Huang",
        "Chenyu Ni",
        "Min Zhou",
        "Zheyu Yan",
        "Xunzhao Yin",
        "Cheng Zhuo"
      ],
      "abstract": "Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical\nanalysis and reasoning. Hyperdimensional Computing (HDC), a promising\nbrain-inspired computational model, is integral to neuro-symbolic AI. Various\nHDC models have been proposed to represent class-instance and class-class\nrelations, but when representing the more complex class-subclass relation,\nwhere multiple objects associate different levels of classes and subclasses,\nthey face challenges for factorization, a crucial task for neuro-symbolic AI\nsystems. In this article, we propose FactorHD, a novel HDC model capable of\nrepresenting and factorizing the complex class-subclass relation efficiently.\nFactorHD features a symbolic encoding method that embeds an extra memorization\nclause, preserving more information for multiple objects. In addition, it\nemploys an efficient factorization algorithm that selectively eliminates\nredundant classes by identifying the memorization clause of the target class.\nSuch model significantly enhances computing efficiency and accuracy in\nrepresenting and factorizing multiple objects with class-subclass relation,\novercoming limitations of existing HDC models such as \"superposition\ncatastrophe\" and \"the problem of 2\". Evaluations show that FactorHD achieves\napproximately 5667x speedup at a representation size of 10^9 compared to\nexisting HDC models. When integrated with the ResNet-18 neural network,\nFactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.",
      "pdf_url": "http://arxiv.org/pdf/2507.12366v1",
      "published": "2025-07-16T16:09:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12366v1",
      "categories": [
        "cs.SC",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Cluster Contrast for Unsupervised Visual Representation Learning",
      "authors": [
        "Nikolaos Giakoumoglou",
        "Tania Stathaki"
      ],
      "abstract": "We introduce Cluster Contrast (CueCo), a novel approach to unsupervised\nvisual representation learning that effectively combines the strengths of\ncontrastive learning and clustering methods. Inspired by recent advancements,\nCueCo is designed to simultaneously scatter and align feature representations\nwithin the feature space. This method utilizes two neural networks, a query and\na key, where the key network is updated through a slow-moving average of the\nquery outputs. CueCo employs a contrastive loss to push dissimilar features\napart, enhancing inter-class separation, and a clustering objective to pull\ntogether features of the same cluster, promoting intra-class compactness. Our\nmethod achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on\nCIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18\nbackbone. By integrating contrastive learning with clustering, CueCo sets a new\ndirection for advancing unsupervised visual representation learning.",
      "pdf_url": "http://arxiv.org/pdf/2507.12359v1",
      "published": "2025-07-16T15:59:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12359v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Neural Polar Decoders for Deletion Channels",
      "authors": [
        "Ziv Aharoni",
        "Henry D. Pfister"
      ],
      "abstract": "This paper introduces a neural polar decoder (NPD) for deletion channels with\na constant deletion rate. Existing polar decoders for deletion channels exhibit\nhigh computational complexity of $O(N^4)$, where $N$ is the block length. This\nlimits the application of polar codes for deletion channels to\nshort-to-moderate block lengths. In this work, we demonstrate that employing\nNPDs for deletion channels can reduce the computational complexity. First, we\nextend the architecture of the NPD to support deletion channels. Specifically,\nthe NPD architecture consists of four neural networks (NNs), each replicating\nfundamental successive cancellation (SC) decoder operations. To support\ndeletion channels, we change the architecture of only one. The computational\ncomplexity of the NPD is $O(AN\\log N)$, where the parameter $A$ represents a\ncomputational budget determined by the user and is independent of the channel.\nWe evaluate the new extended NPD for deletion channels with deletion rates\n$\\delta\\in\\{0.01, 0.1\\}$ and we verify the NPD with the ground truth given by\nthe trellis decoder by Tal et al. We further show that due to the reduced\ncomplexity of the NPD, we are able to incorporate list decoding and further\nimprove performance. We believe that the extended NPD presented here could have\napplications in future technologies like DNA storage.",
      "pdf_url": "http://arxiv.org/pdf/2507.12329v1",
      "published": "2025-07-16T15:22:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12329v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math.IT"
      ]
    },
    {
      "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models",
      "authors": [
        "Samuel Lavoie",
        "Michael Noukhovitch",
        "Aaron Courville"
      ],
      "abstract": "We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.",
      "pdf_url": "http://arxiv.org/pdf/2507.12318v2",
      "published": "2025-07-16T15:12:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12318v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack",
      "authors": [
        "Zihao Xue",
        "Zhen Bi",
        "Long Ma",
        "Zhenlin Hu",
        "Yan Wang",
        "Zhenfang Liu",
        "Qing Sheng",
        "Jie Xiao",
        "Jungang Lou"
      ],
      "abstract": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.",
      "pdf_url": "http://arxiv.org/pdf/2507.12314v1",
      "published": "2025-07-16T15:09:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12314v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CR"
      ]
    },
    {
      "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization",
      "authors": [
        "Prashanth Vijayaraghavan",
        "Apoorva Nitsure",
        "Charles Mackin",
        "Luyao Shi",
        "Stefano Ambrogio",
        "Arvind Haran",
        "Viresh Paruthi",
        "Ali Elzein",
        "Dan Coops",
        "David Beymer",
        "Tyler Baldwin",
        "Ehsan Degan"
      ],
      "abstract": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.",
      "pdf_url": "http://arxiv.org/pdf/2507.12308v1",
      "published": "2025-07-16T15:05:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12308v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.AR"
      ]
    },
    {
      "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning",
      "authors": [
        "M. Anwar Ma'sum",
        "Mahardhika Pratama",
        "Savitha Ramasamy",
        "Lin Liu",
        "Habibullah Habibullah",
        "Ryszard Kowalczyk"
      ],
      "abstract": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.",
      "pdf_url": "http://arxiv.org/pdf/2507.12305v1",
      "published": "2025-07-16T15:04:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12305v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding",
      "authors": [
        "Feng Xiao",
        "Jicong Fan"
      ],
      "abstract": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.12295v1",
      "published": "2025-07-16T14:47:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12295v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks",
      "authors": [
        "Artem Chervyakov",
        "Alexander Kharitonov",
        "Pavel Zadorozhny",
        "Adamenko Pavel",
        "Rodion Levichev",
        "Dmitrii Vorobev",
        "Dmitrii Salikhov",
        "Aidar Valeev",
        "Alena Pestova",
        "Maria Dziuba",
        "Ilseyar Alimova",
        "Artem Zavgorodnev",
        "Aleksandr Medvedev",
        "Stanislav Moiseev",
        "Elena Bruches",
        "Daniil Grebenkin",
        "Roman Derunets",
        "Vikulov Vladimir",
        "Anton Emelyanov",
        "Dmitrii Babaev",
        "Vladimir V. Ivanov",
        "Valentin Malykh",
        "Alena Fenogenova"
      ],
      "abstract": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.",
      "pdf_url": "http://arxiv.org/pdf/2507.12284v2",
      "published": "2025-07-16T14:31:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12284v2",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants",
      "authors": [
        "Sybelle Goedicke-Fritz",
        "Michelle Bous",
        "Annika Engel",
        "Matthias Flotho",
        "Pascal Hirsch",
        "Hannah Wittig",
        "Dino Milanovic",
        "Dominik Mohr",
        "Mathias Kaspar",
        "Sogand Nemat",
        "Dorothea Kerner",
        "Arno Bücker",
        "Andreas Keller",
        "Sascha Meyer",
        "Michael Zemlin",
        "Philipp Flotho"
      ],
      "abstract": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.",
      "pdf_url": "http://arxiv.org/pdf/2507.12269v2",
      "published": "2025-07-16T14:19:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12269v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters",
      "authors": [
        "Zachary James",
        "Joseph Guinness"
      ],
      "abstract": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.",
      "pdf_url": "http://arxiv.org/pdf/2507.12262v1",
      "published": "2025-07-16T14:09:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12262v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes",
      "authors": [
        "Johann Frei",
        "Nils Feldhus",
        "Lisa Raithel",
        "Roland Roller",
        "Alexander Meyer",
        "Frank Kramer"
      ],
      "abstract": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.",
      "pdf_url": "http://arxiv.org/pdf/2507.12261v1",
      "published": "2025-07-16T14:06:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12261v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models",
      "authors": [
        "Shilin Zhou",
        "Zhenghua Li"
      ],
      "abstract": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.",
      "pdf_url": "http://arxiv.org/pdf/2507.12252v1",
      "published": "2025-07-16T13:59:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12252v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Looking for Fairness in Recommender Systems",
      "authors": [
        "Cécile Logé"
      ],
      "abstract": "Recommender systems can be found everywhere today, shaping our everyday\nexperience whenever we're consuming content, ordering food, buying groceries\nonline, or even just reading the news. Let's imagine we're in the process of\nbuilding a recommender system to make content suggestions to users on social\nmedia. When thinking about fairness, it becomes clear there are several\nperspectives to consider: the users asking for tailored suggestions, the\ncontent creators hoping for some limelight, and society at large, navigating\nthe repercussions of algorithmic recommendations. A shared fairness concern\nacross all three is the emergence of filter bubbles, a side-effect that takes\nplace when recommender systems are almost \"too good\", making recommendations so\ntailored that users become inadvertently confined to a narrow set of\nopinions/themes and isolated from alternative ideas. From the user's\nperspective, this is akin to manipulation. From the small content creator's\nperspective, this is an obstacle preventing them access to a whole range of\npotential fans. From society's perspective, the potential consequences are\nfar-reaching, influencing collective opinions, social behavior and political\ndecisions. How can our recommender system be fine-tuned to avoid the creation\nof filter bubbles, and ensure a more inclusive and diverse content landscape?\nApproaching this problem involves defining one (or more) performance metric to\nrepresent diversity, and tweaking our recommender system's performance through\nthe lens of fairness. By incorporating this metric into our evaluation\nframework, we aim to strike a balance between personalized recommendations and\nthe broader societal goal of fostering rich and varied cultures and points of\nview.",
      "pdf_url": "http://arxiv.org/pdf/2507.12242v1",
      "published": "2025-07-16T13:53:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12242v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning",
      "authors": [
        "Yuhao Chen",
        "Shuochen Liu",
        "Yuanjie Lyu",
        "Chao Zhang",
        "Jiayao Shi",
        "Tong Xu"
      ],
      "abstract": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.",
      "pdf_url": "http://arxiv.org/pdf/2507.12215v1",
      "published": "2025-07-16T13:19:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12215v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness",
      "authors": [
        "Garyoung Kim",
        "Huisung Kwon",
        "Seoju Yun",
        "Yu-Won Youn"
      ],
      "abstract": "Generative AI does not only replicate human creativity but also reproduces\ndeep-seated cultural biases, making it crucial to critically examine how\nconcepts like ugliness are understood and expressed by these tools. This study\ninvestigates how four different generative AI models understand and express\nugliness through text and image and explores the biases embedded within these\nrepresentations. We extracted 13 adjectives associated with ugliness through\niterative prompting of a large language model and generated 624 images across\nfour AI models and three prompts. Demographic and socioeconomic attributes\nwithin the images were independently coded and thematically analyzed. Our\nfindings show that AI models disproportionately associate ugliness with old\nwhite male figures, reflecting entrenched social biases as well as paradoxical\nbiases, where efforts to avoid stereotypical depictions of marginalized groups\ninadvertently result in the disproportionate projection of negative attributes\nonto majority groups. Qualitative analysis further reveals that, despite\nsupposed attempts to frame ugliness within social contexts, conventional\nphysical markers such as asymmetry and aging persist as central visual motifs.\nThese findings demonstrate that despite attempts to create more equal\nrepresentations, generative AI continues to perpetuate inherited and\nparadoxical biases, underscoring the critical work being done to create ethical\nAI training paradigms and advance methodologies for more inclusive AI\ndevelopment.",
      "pdf_url": "http://arxiv.org/pdf/2507.12212v1",
      "published": "2025-07-16T13:16:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12212v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution",
      "authors": [
        "Subin Lin",
        "Chuanbo Hua"
      ],
      "abstract": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.12207v1",
      "published": "2025-07-16T13:07:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12207v1",
      "categories": [
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control",
      "authors": [
        "Anton Klenitskiy",
        "Konstantin Polev",
        "Daria Denisova",
        "Alexey Vasilev",
        "Dmitry Simakov",
        "Gleb Gusev"
      ],
      "abstract": "Many current state-of-the-art models for sequential recommendations are based\non transformer architectures. Interpretation and explanation of such black box\nmodels is an important research question, as a better understanding of their\ninternals can help understand, influence, and control their behavior, which is\nvery important in a variety of real-world applications. Recently sparse\nautoencoders (SAE) have been shown to be a promising unsupervised approach for\nextracting interpretable features from language models. These autoencoders\nlearn to reconstruct hidden states of the transformer's internal layers from\nsparse linear combinations of directions in their activation space.\n  This paper is focused on the application of SAE to the sequential\nrecommendation domain. We show that this approach can be successfully applied\nto the transformer trained on a sequential recommendation task: learned\ndirections turn out to be more interpretable and monosemantic than the original\nhidden state dimensions. Moreover, we demonstrate that the features learned by\nSAE can be used to effectively and flexibly control the model's behavior,\nproviding end-users with a straightforward method to adjust their\nrecommendations to different custom scenarios and contexts.",
      "pdf_url": "http://arxiv.org/pdf/2507.12202v1",
      "published": "2025-07-16T12:57:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12202v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Quantize More, Lose Less: Autoregressive Generation from Residually Quantized Speech Representations",
      "authors": [
        "Yichen Han",
        "Xiaoyang Hao",
        "Keming Chen",
        "Weibo Xiong",
        "Jun He",
        "Ruonan Zhang",
        "Junjie Cao",
        "Yue Liu",
        "Bowen Li",
        "Dongrui Zhang",
        "Hui Xia",
        "Huilei Fu",
        "Kai Jia",
        "Kaixuan Guo",
        "Mingli Jin",
        "Qingyun Meng",
        "Ruidong Ma",
        "Ruiqian Fang",
        "Shaotong Guo",
        "Xuhui Li",
        "Yang Xiang",
        "Ying Zhang",
        "Yulong Liu",
        "Yunfeng Li",
        "Yuyi Zhang",
        "Yuze Zhou",
        "Zhen Wang",
        "Zhaowen Chen"
      ],
      "abstract": "Text-to-speech (TTS) synthesis has seen renewed progress under the discrete\nmodeling paradigm. Existing autoregressive approaches often rely on\nsingle-codebook representations, which suffer from significant information\nloss. Even with post-hoc refinement techniques such as flow matching, these\nmethods fail to recover fine-grained details (e.g., prosodic nuances,\nspeaker-specific timbres), especially in challenging scenarios like singing\nvoice or music synthesis. We propose QTTS, a novel TTS framework built upon our\nnew audio codec, QDAC. The core innovation of QDAC lies in its end-to-end\ntraining of an ASR-based auto-regressive network with a GAN, which achieves\nsuperior semantic feature disentanglement for scalable, near-lossless\ncompression. QTTS models these discrete codes using two innovative strategies:\nthe Hierarchical Parallel architecture, which uses a dual-AR structure to model\ninter-codebook dependencies for higher-quality synthesis, and the Delay\nMultihead approach, which employs parallelized prediction with a fixed delay to\naccelerate inference speed. Our experiments demonstrate that the proposed\nframework achieves higher synthesis quality and better preserves expressive\ncontent compared to baseline. This suggests that scaling up compression via\nmulti-codebook modeling is a promising direction for high-fidelity,\ngeneral-purpose speech and audio generation.",
      "pdf_url": "http://arxiv.org/pdf/2507.12197v1",
      "published": "2025-07-16T12:47:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12197v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision",
      "authors": [
        "Arkaprabha Basu"
      ],
      "abstract": "Modern digitised approaches have dramatically changed the preservation and\nrestoration of cultural treasures, integrating computer scientists into\nmultidisciplinary projects with ease. Machine learning, deep learning, and\ncomputer vision techniques have revolutionised developing sectors like 3D\nreconstruction, picture inpainting,IoT-based methods, genetic algorithms, and\nimage processing with the integration of computer scientists into\nmultidisciplinary initiatives. We suggest three cutting-edge techniques in\nrecognition of the special qualities of Indian monuments, which are famous for\ntheir architectural skill and aesthetic appeal. First is the Fractal\nConvolution methodology, a segmentation method based on image processing that\nsuccessfully reveals subtle architectural patterns within these irreplaceable\ncultural buildings. The second is a revolutionary Self-Sensitive Tile Filling\n(SSTF) method created especially for West Bengal's mesmerising Bankura\nTerracotta Temples with a brand-new data augmentation method called MosaicSlice\non the third. Furthermore, we delve deeper into the Super Resolution strategy\nto upscale the images without losing significant amount of quality. Our methods\nallow for the development of seamless region-filling and highly detailed tiles\nwhile maintaining authenticity using a novel data augmentation strategy within\naffordable costs introducing automation. By providing effective solutions that\npreserve the delicate balance between tradition and innovation, this study\nimproves the subject and eventually ensures unrivalled efficiency and aesthetic\nexcellence in cultural heritage protection. The suggested approaches advance\nthe field into an era of unmatched efficiency and aesthetic quality while\ncarefully upholding the delicate equilibrium between tradition and innovation.",
      "pdf_url": "http://arxiv.org/pdf/2507.12195v1",
      "published": "2025-07-16T12:46:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12195v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Selective Quantization Tuning for ONNX Models",
      "authors": [
        "Nikolaos Louloudakis",
        "Ajitha Rajan"
      ],
      "abstract": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.",
      "pdf_url": "http://arxiv.org/pdf/2507.12196v1",
      "published": "2025-07-16T12:46:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12196v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search",
      "authors": [
        "Azhar Ikhtiarudin",
        "Aditi Das",
        "Param Thakkar",
        "Akash Kundu"
      ],
      "abstract": "We introduce BenchRL-QAS, a unified benchmarking framework for systematically\nevaluating reinforcement learning (RL) algorithms in quantum architecture\nsearch (QAS) across diverse variational quantum algorithm tasks and system\nsizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including\nboth value-based and policy-gradient methods on representative quantum problems\nsuch as variational quantum eigensolver, variational quantum state\ndiagonalization, quantum classification, and state preparation, spanning both\nnoiseless and realistic noisy regimes. We propose a weighted ranking metric\nthat balances accuracy, circuit depth, gate count, and computational\nefficiency, enabling fair and comprehensive comparison. Our results first\nreveal that RL-based quantum classifier outperforms baseline variational\nclassifiers. Then we conclude that no single RL algorithm is universally\noptimal when considering a set of QAS tasks; algorithmic performance is highly\ncontext-dependent, varying with task structure, qubit count, and noise. This\nempirical finding provides strong evidence for the \"no free lunch\" principle in\nRL-based quantum circuit design and highlights the necessity of tailored\nalgorithm selection and systematic benchmarking for advancing quantum circuit\nsynthesis. This work represents the most comprehensive RL-QAS benchmarking\neffort to date, and BenchRL-QAS along with all experimental data are made\npublicly available to support reproducibility and future research\nhttps://github.com/azhar-ikhtiarudin/bench-rlqas.",
      "pdf_url": "http://arxiv.org/pdf/2507.12189v1",
      "published": "2025-07-16T12:43:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12189v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ]
    },
    {
      "title": "Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement",
      "authors": [
        "Shuangli Du",
        "Siming Yan",
        "Zhenghao Shi",
        "Zhenzhen You",
        "Lu Sun"
      ],
      "abstract": "Low-light images suffer from complex degradation, and existing enhancement\nmethods often encode all degradation factors within a single latent space. This\nleads to highly entangled features and strong black-box characteristics, making\nthe model prone to shortcut learning. To mitigate the above issues, this paper\nproposes a wavelet-based low-light stereo image enhancement method with feature\nspace decoupling. Our insight comes from the following findings: (1) Wavelet\ntransform enables the independent processing of low-frequency and\nhigh-frequency information. (2) Illumination adjustment can be achieved by\nadjusting the low-frequency component of a low-light image, extracted through\nmulti-level wavelet decomposition. Thus, by using wavelet transform the feature\nspace is decomposed into a low-frequency branch for illumination adjustment and\nmultiple high-frequency branches for texture enhancement. Additionally, stereo\nlow-light image enhancement can extract useful cues from another view to\nimprove enhancement. To this end, we propose a novel high-frequency guided\ncross-view interaction module (HF-CIM) that operates within high-frequency\nbranches rather than across the entire feature space, effectively extracting\nvaluable image details from the other view. Furthermore, to enhance the\nhigh-frequency information, a detail and texture enhancement module (DTEM) is\nproposed based on cross-attention mechanism. The model is trained on a dataset\nconsisting of images with uniform illumination and images with non-uniform\nillumination. Experimental results on both real and synthetic images indicate\nthat our algorithm offers significant advantages in light adjustment while\neffectively recovering high-frequency information. The code and dataset are\npublicly available at: https://github.com/Cherisherr/WDCI-Net.git.",
      "pdf_url": "http://arxiv.org/pdf/2507.12188v1",
      "published": "2025-07-16T12:42:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12188v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation",
      "authors": [
        "Edward Kim",
        "Hanna Kurniawati"
      ],
      "abstract": "This paper proposes Partially Observable Reference Policy Programming, a\nnovel anytime online approximate POMDP solver which samples meaningful future\nhistories very deeply while simultaneously forcing a gradual policy update. We\nprovide theoretical guarantees for the algorithm's underlying scheme which say\nthat the performance loss is bounded by the average of the sampling\napproximation errors rather than the usual maximum, a crucial requirement given\nthe sampling sparsity of online planning. Empirical evaluations on two\nlarge-scale problems with dynamically evolving environments -- including a\nhelicopter emergency scenario in the Corsica region requiring approximately 150\nplanning steps -- corroborate the theoretical results and indicate that our\nsolver considerably outperforms current online benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2507.12186v1",
      "published": "2025-07-16T12:33:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12186v1",
      "categories": [
        "cs.AI",
        "I.2.8; I.2.9"
      ]
    },
    {
      "title": "PRISM: Distributed Inference for Foundation Models at Edge",
      "authors": [
        "Muhammad Azlan Qazi",
        "Alexandros Iosifidis",
        "Qi Zhang"
      ],
      "abstract": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.",
      "pdf_url": "http://arxiv.org/pdf/2507.12145v1",
      "published": "2025-07-16T11:25:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12145v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations",
      "authors": [
        "Timothy Heightman",
        "Edward Jiang",
        "Ruth Mora-Soto",
        "Maciej Lewenstein",
        "Marcin Płodzień"
      ],
      "abstract": "Quantum machine learning (QML) seeks to exploit the intrinsic properties of\nquantum mechanical systems, including superposition, coherence, and quantum\nentanglement for classical data processing. However, due to the exponential\ngrowth of the Hilbert space, QML faces practical limits in classical\nsimulations with the state-vector representation of quantum system. On the\nother hand, phase-space methods offer an alternative by encoding quantum states\nas quasi-probability functions. Building on prior work in qubit phase-space and\nthe Stratonovich-Weyl (SW) correspondence, we construct a closed, composable\ndynamical formalism for one- and many-qubit systems in phase-space. This\nformalism replaces the operator algebra of the Pauli group with function\ndynamics on symplectic manifolds, and recasts the curse of dimensionality in\nterms of harmonic support on a domain that scales linearly with the number of\nqubits. It opens a new route for QML based on variational modelling over\nphase-space.",
      "pdf_url": "http://arxiv.org/pdf/2507.12117v1",
      "published": "2025-07-16T10:37:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12117v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "math-ph",
        "math.MP"
      ]
    },
    {
      "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs",
      "authors": [
        "Ye Han",
        "Lijun Zhang",
        "Dejian Meng",
        "Zhuang Zhang"
      ],
      "abstract": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.",
      "pdf_url": "http://arxiv.org/pdf/2507.12110v1",
      "published": "2025-07-16T10:27:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12110v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies",
      "authors": [
        "Lorenzo Mannocci",
        "Stefano Cresci",
        "Matteo Magnani",
        "Anna Monreale",
        "Maurizio Tesconi"
      ],
      "abstract": "Coordinated online behavior, which spans from beneficial collective actions\nto harmful manipulation such as disinformation campaigns, has become a key\nfocus in digital ecosystem analysis. Traditional methods often rely on\nmonomodal approaches, focusing on single types of interactions like co-retweets\nor co-hashtags, or consider multiple modalities independently of each other.\nHowever, these approaches may overlook the complex dynamics inherent in\nmultimodal coordination. This study compares different ways of operationalizing\nthe detection of multimodal coordinated behavior. It examines the trade-off\nbetween weakly and strongly integrated multimodal models, highlighting the\nbalance between capturing broader coordination patterns and identifying tightly\ncoordinated behavior. By comparing monomodal and multimodal approaches, we\nassess the unique contributions of different data modalities and explore how\nvarying implementations of multimodality impact detection outcomes. Our\nfindings reveal that not all the modalities provide distinct insights, but that\nwith a multimodal approach we can get a more comprehensive understanding of\ncoordination dynamics. This work enhances the ability to detect and analyze\ncoordinated online behavior, offering new perspectives for safeguarding the\nintegrity of digital platforms.",
      "pdf_url": "http://arxiv.org/pdf/2507.12108v1",
      "published": "2025-07-16T10:25:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12108v1",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Non-Adaptive Adversarial Face Generation",
      "authors": [
        "Sunpill Kim",
        "Seunghun Paik",
        "Chanwoo Hwang",
        "Minsu Kim",
        "Jae Hong Seo"
      ],
      "abstract": "Adversarial attacks on face recognition systems (FRSs) pose serious security\nand privacy threats, especially when these systems are used for identity\nverification. In this paper, we propose a novel method for generating\nadversarial faces-synthetic facial images that are visually distinct yet\nrecognized as a target identity by the FRS. Unlike iterative optimization-based\napproaches (e.g., gradient descent or other iterative solvers), our method\nleverages the structural characteristics of the FRS feature space. We figure\nout that individuals sharing the same attribute (e.g., gender or race) form an\nattributed subsphere. By utilizing such subspheres, our method achieves both\nnon-adaptiveness and a remarkably small number of queries. This eliminates the\nneed for relying on transferability and open-source surrogate models, which\nhave been a typical strategy when repeated adaptive queries to commercial FRSs\nare impossible. Despite requiring only a single non-adaptive query consisting\nof 100 face images, our method achieves a high success rate of over 93% against\nAWS's CompareFaces API at its default threshold. Furthermore, unlike many\nexisting attacks that perturb a given image, our method can deliberately\nproduce adversarial faces that impersonate the target identity while exhibiting\nhigh-level attributes chosen by the adversary.",
      "pdf_url": "http://arxiv.org/pdf/2507.12107v1",
      "published": "2025-07-16T10:24:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12107v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "I.2.6; I.5.4; D.4.6; K.6.5; I.4.8"
      ]
    },
    {
      "title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs",
      "authors": [
        "Francisco Javier Cavero",
        "Juan C. Alonso",
        "Antonio Ruiz-Cortés"
      ],
      "abstract": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites.",
      "pdf_url": "http://arxiv.org/pdf/2507.12104v1",
      "published": "2025-07-16T10:20:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12104v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "BOOKCOREF: Coreference Resolution at Book Scale",
      "authors": [
        "Giuliano Martinelli",
        "Tommaso Bonomo",
        "Pere-Lluís Huguet Cabot",
        "Roberto Navigli"
      ],
      "abstract": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.",
      "pdf_url": "http://arxiv.org/pdf/2507.12075v1",
      "published": "2025-07-16T09:35:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12075v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features",
      "authors": [
        "Jeremi K. Ochab",
        "Mateusz Matias",
        "Tymoteusz Boba",
        "Tomasz Walkowiak"
      ],
      "abstract": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.",
      "pdf_url": "http://arxiv.org/pdf/2507.12064v1",
      "published": "2025-07-16T09:21:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12064v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing",
      "authors": [
        "Kun-Hsiang Lin",
        "Yu-Wen Tseng",
        "Kang-Yang Huang",
        "Jhih-Ciang Wu",
        "Wen-Huang Cheng"
      ],
      "abstract": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.",
      "pdf_url": "http://arxiv.org/pdf/2507.12060v1",
      "published": "2025-07-16T09:16:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12060v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery",
      "authors": [
        "Xinhang Wan",
        "Jiyuan Liu",
        "Qian Qu",
        "Suyuan Liu",
        "Chuyu Zhang",
        "Fangdi Wang",
        "Xinwang Liu",
        "En Zhu",
        "Kunlun He"
      ],
      "abstract": "In this paper, we address the problem of novel class discovery (NCD), which\naims to cluster novel classes by leveraging knowledge from disjoint known\nclasses. While recent advances have made significant progress in this area,\nexisting NCD methods face two major limitations. First, they primarily focus on\nsingle-view data (e.g., images), overlooking the increasingly common multi-view\ndata, such as multi-omics datasets used in disease diagnosis. Second, their\nreliance on pseudo-labels to supervise novel class clustering often results in\nunstable performance, as pseudo-label quality is highly sensitive to factors\nsuch as data noise and feature dimensionality. To address these challenges, we\npropose a novel framework named Intra-view and Inter-view Correlation Guided\nMulti-view Novel Class Discovery (IICMVNCD), which is the first attempt to\nexplore NCD in multi-view setting so far. Specifically, at the intra-view\nlevel, leveraging the distributional similarity between known and novel\nclasses, we employ matrix factorization to decompose features into\nview-specific shared base matrices and factor matrices. The base matrices\ncapture distributional consistency among the two datasets, while the factor\nmatrices model pairwise relationships between samples. At the inter-view level,\nwe utilize view relationships among known classes to guide the clustering of\nnovel classes. This includes generating predicted labels through the weighted\nfusion of factor matrices and dynamically adjusting view weights of known\nclasses based on the supervision loss, which are then transferred to novel\nclass learning. Experimental results validate the effectiveness of our proposed\napproach.",
      "pdf_url": "http://arxiv.org/pdf/2507.12029v1",
      "published": "2025-07-16T08:42:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12029v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection",
      "authors": [
        "Xiwei Zhang",
        "Chunjin Yang",
        "Yiming Xiao",
        "Runtong Zhang",
        "Fanman Meng"
      ],
      "abstract": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain\nto the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB\ndomain as a unified domain and neglect the multiple subdomains within it, such\nas daytime, nighttime, and foggy scenes. We argue that decoupling the\ndomain-invariant (DI) and domain-specific (DS) features across these multiple\nsubdomains is beneficial for RGB-IR domain adaptation. To this end, this paper\nproposes a new SS-DC framework based on a decoupling-coupling strategy. In\nterms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)\nmodule in the aspect of spectral decomposition. Due to the style and content\ninformation being highly embedded in different frequency bands, this module can\ndecouple DI and DS components more accurately and interpretably. A novel filter\nbank-based spectral processing paradigm and a self-distillation-driven\ndecoupling loss are proposed to improve the spectral domain decoupling. In\nterms of coupling, a new spatial-spectral coupling method is proposed, which\nrealizes joint coupling through spatial and spectral DI feature pyramids.\nMeanwhile, this paper introduces DS from decoupling to reduce the domain bias.\nExtensive experiments demonstrate that our method can significantly improve the\nbaseline performance and outperform existing UDAOD methods on multiple RGB-IR\ndatasets, including a new experimental protocol proposed in this paper based on\nthe FLIR-ADAS dataset.",
      "pdf_url": "http://arxiv.org/pdf/2507.12017v1",
      "published": "2025-07-16T08:21:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12017v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease",
      "authors": [
        "Matthias Perkonigg",
        "Nina Bastati",
        "Ahmed Ba-Ssalamah",
        "Peter Mesenbrink",
        "Alexander Goehler",
        "Miljen Martic",
        "Xiaofei Zhou",
        "Michael Trauner",
        "Georg Langs"
      ],
      "abstract": "Quantifiable image patterns associated with disease progression and treatment\nresponse are critical tools for guiding individual treatment, and for\ndeveloping novel therapies. Here, we show that unsupervised machine learning\ncan identify a pattern vocabulary of liver tissue in magnetic resonance images\nthat quantifies treatment response in diffuse liver disease. Deep clustering\nnetworks simultaneously encode and cluster patches of medical images into a\nlow-dimensional latent space to establish a tissue vocabulary. The resulting\ntissue types capture differential tissue change and its location in the liver\nassociated with treatment response. We demonstrate the utility of the\nvocabulary on a randomized controlled trial cohort of non-alcoholic\nsteatohepatitis patients. First, we use the vocabulary to compare longitudinal\nliver change in a placebo and a treatment cohort. Results show that the method\nidentifies specific liver tissue change pathways associated with treatment, and\nenables a better separation between treatment groups than established\nnon-imaging measures. Moreover, we show that the vocabulary can predict biopsy\nderived features from non-invasive imaging data. We validate the method on a\nseparate replication cohort to demonstrate the applicability of the proposed\nmethod.",
      "pdf_url": "http://arxiv.org/pdf/2507.12012v1",
      "published": "2025-07-16T08:11:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.12012v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    }
  ]
}