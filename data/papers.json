{
  "last_updated": "2025-06-13T00:53:21.872096",
  "papers": [
    {
      "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos",
      "authors": [
        "Chieh Hubert Lin",
        "Zhaoyang Lv",
        "Songyin Wu",
        "Zhen Xu",
        "Thu Nguyen-Phuoc",
        "Hung-Yu Tseng",
        "Julian Straub",
        "Numair Khan",
        "Lei Xiao",
        "Ming-Hsuan Yang",
        "Yuheng Ren",
        "Richard Newcombe",
        "Zhao Dong",
        "Zhengqin Li"
      ],
      "abstract": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.",
      "pdf_url": "http://arxiv.org/pdf/2506.09997v1",
      "published": "2025-06-11T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09997v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Text-Aware Image Restoration with Diffusion Models",
      "authors": [
        "Jaewon Min",
        "Jin Hyeon Kim",
        "Paul Hyunbin Cho",
        "Jaeeun Lee",
        "Jihye Park",
        "Minkyu Park",
        "Sangpil Kim",
        "Hyunhee Park",
        "Seungryong Kim"
      ],
      "abstract": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
      "pdf_url": "http://arxiv.org/pdf/2506.09993v1",
      "published": "2025-06-11T17:59:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09993v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures",
      "authors": [
        "Venkatesh Pattabiraman",
        "Zizhou Huang",
        "Daniele Panozzo",
        "Denis Zorin",
        "Lerrel Pinto",
        "Raunaq Bhirangi"
      ],
      "abstract": "If human experience is any guide, operating effectively in unstructured\nenvironments -- like homes and offices -- requires robots to sense the forces\nduring physical interaction. Yet, the lack of a versatile, accessible, and\neasily customizable tactile sensor has led to fragmented, sensor-specific\nsolutions in robotic manipulation -- and in many cases, to force-unaware,\nsensorless approaches. With eFlesh, we bridge this gap by introducing a\nmagnetic tactile sensor that is low-cost, easy to fabricate, and highly\ncustomizable. Building an eFlesh sensor requires only four components: a\nhobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired\nshape, and a magnetometer circuit board. The sensor is constructed from tiled,\nparameterized microstructures, which allow for tuning the sensor's geometry and\nits mechanical response. We provide an open-source design tool that converts\nconvex OBJ/STL files into 3D-printable STLs for fabrication. This modular\ndesign framework enables users to create application-specific sensors, and to\nadjust sensitivity depending on the task. Our sensor characterization\nexperiments demonstrate the capabilities of eFlesh: contact localization RMSE\nof 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for\nshear force. We also present a learned slip detection model that generalizes to\nunseen objects with 95% accuracy, and visuotactile control policies that\nimprove manipulation performance by 40% over vision-only baselines -- achieving\n91% average success rate for four precise tasks that require sub-mm accuracy\nfor successful completion. All design files, code and the CAD-to-eFlesh STL\nconversion tool are open-sourced and available on https://e-flesh.com.",
      "pdf_url": "http://arxiv.org/pdf/2506.09994v1",
      "published": "2025-06-11T17:59:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09994v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits",
      "authors": [
        "Ron Yosef",
        "Moran Yanuka",
        "Yonatan Bitton",
        "Dani Lischinski"
      ],
      "abstract": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.",
      "pdf_url": "http://arxiv.org/pdf/2506.09988v1",
      "published": "2025-06-11T17:58:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09988v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions",
      "authors": [
        "Zhenzhi Wang",
        "Jiaqi Yang",
        "Jianwen Jiang",
        "Chao Liang",
        "Gaojie Lin",
        "Zerong Zheng",
        "Ceyuan Yang",
        "Dahua Lin"
      ],
      "abstract": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
      "pdf_url": "http://arxiv.org/pdf/2506.09984v1",
      "published": "2025-06-11T17:57:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09984v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",
      "authors": [
        "Mido Assran",
        "Adrien Bardes",
        "David Fan",
        "Quentin Garrido",
        "Russell Howes",
        "Mojtaba",
        "Komeili",
        "Matthew Muckley",
        "Ammar Rizvi",
        "Claire Roberts",
        "Koustuv Sinha",
        "Artem Zholus",
        "Sergio Arnaud",
        "Abha Gejji",
        "Ada Martin",
        "Francois Robert Hogan",
        "Daniel Dugas",
        "Piotr Bojanowski",
        "Vasil Khalidov",
        "Patrick Labatut",
        "Francisco Massa",
        "Marc Szafraniec",
        "Kapil Krishnakumar",
        "Yong Li",
        "Xiaodong Ma",
        "Sarath Chandar",
        "Franziska Meier",
        "Yann LeCun",
        "Michael Rabbat",
        "Nicolas Ballas"
      ],
      "abstract": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.",
      "pdf_url": "http://arxiv.org/pdf/2506.09985v1",
      "published": "2025-06-11T17:57:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09985v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies",
      "authors": [
        "Stylianos Loukas Vasileiou",
        "Antonio Rago",
        "Maria Vanina Martinez",
        "William Yeoh"
      ],
      "abstract": "Understanding how humans revise their beliefs in light of new information is\ncrucial for developing AI systems which can effectively model, and thus align\nwith, human reasoning. While theoretical belief revision frameworks rely on a\nset of principles that establish how these operations are performed, empirical\nevidence from cognitive psychology suggests that people may follow different\npatterns when presented with conflicting information. In this paper, we present\nthree comprehensive user studies showing that people consistently prefer\nexplanation-based revisions, i.e., those which are guided by explanations, that\nresult in changes to their belief systems that are not necessarily captured by\nclassical belief change theory. Our experiments systematically investigate how\npeople revise their beliefs with explanations for inconsistencies, whether they\nare provided with them or left to formulate them themselves, demonstrating a\nrobust preference for what may seem non-minimal revisions across different\ntypes of scenarios. These findings have implications for AI systems designed to\nmodel human reasoning or interact with humans, suggesting that such systems\nshould accommodate explanation-based, potentially non-minimal belief revision\noperators to better align with human cognitive processes.",
      "pdf_url": "http://arxiv.org/pdf/2506.09977v1",
      "published": "2025-06-11T17:52:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09977v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing",
      "authors": [
        "Junfei Wu",
        "Jian Guan",
        "Kaituo Feng",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang",
        "Wei Wu",
        "Tieniu Tan"
      ],
      "abstract": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.",
      "pdf_url": "http://arxiv.org/pdf/2506.09965v1",
      "published": "2025-06-11T17:41:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09965v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2"
      ]
    },
    {
      "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge",
      "authors": [
        "Sahar Abdelnabi",
        "Aideen Fay",
        "Ahmed Salem",
        "Egor Zverev",
        "Kai-Chieh Liao",
        "Chi-Huang Liu",
        "Chun-Chih Kuo",
        "Jannis Weigend",
        "Danyael Manlangit",
        "Alex Apostolov",
        "Haris Umair",
        "João Donato",
        "Masayuki Kawakita",
        "Athar Mahboob",
        "Tran Huu Bach",
        "Tsun-Han Chiang",
        "Myeongjin Cho",
        "Hajin Choi",
        "Byeonghyeon Kim",
        "Hyeonjin Lee",
        "Benjamin Pannell",
        "Conor McCauley",
        "Mark Russinovich",
        "Andrew Paverd",
        "Giovanni Cherubin"
      ],
      "abstract": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection.",
      "pdf_url": "http://arxiv.org/pdf/2506.09956v1",
      "published": "2025-06-11T17:30:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09956v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Vision Generalist Model: A Survey",
      "authors": [
        "Ziyi Wang",
        "Yongming Rao",
        "Shuofeng Sun",
        "Xinrun Liu",
        "Yi Wei",
        "Xumin Yu",
        "Zuyan Liu",
        "Yanbo Wang",
        "Hongmin Liu",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Recently, we have witnessed the great success of the generalist model in\nnatural language processing. The generalist model is a general framework\ntrained with massive data and is able to process various downstream tasks\nsimultaneously. Encouraged by their impressive performance, an increasing\nnumber of researchers are venturing into the realm of applying these models to\ncomputer vision tasks. However, the inputs and outputs of vision tasks are more\ndiverse, and it is difficult to summarize them as a unified representation. In\nthis paper, we provide a comprehensive overview of the vision generalist\nmodels, delving into their characteristics and capabilities within the field.\nFirst, we review the background, including the datasets, tasks, and benchmarks.\nThen, we dig into the design of frameworks that have been proposed in existing\nresearch, while also introducing the techniques employed to enhance their\nperformance. To better help the researchers comprehend the area, we take a\nbrief excursion into related domains, shedding light on their interconnections\nand potential synergies. To conclude, we provide some real-world application\nscenarios, undertake a thorough examination of the persistent challenges, and\noffer insights into possible directions for future research endeavors.",
      "pdf_url": "http://arxiv.org/pdf/2506.09954v1",
      "published": "2025-06-11T17:23:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09954v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos",
      "authors": [
        "Benjamin Reichman",
        "Constantin Patsch",
        "Jack Truxal",
        "Atishay Jain",
        "Larry Heck"
      ],
      "abstract": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.",
      "pdf_url": "http://arxiv.org/pdf/2506.09953v1",
      "published": "2025-06-11T17:23:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09953v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting",
      "authors": [
        "Ziyi Wang",
        "Yanran Zhang",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
      "pdf_url": "http://arxiv.org/pdf/2506.09952v1",
      "published": "2025-06-11T17:23:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09952v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following",
      "authors": [
        "Hao Peng",
        "Yunjia Qi",
        "Xiaozhi Wang",
        "Bin Xu",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
      "pdf_url": "http://arxiv.org/pdf/2506.09942v1",
      "published": "2025-06-11T17:10:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09942v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models",
      "authors": [
        "Aaron Foss",
        "Chloe Evans",
        "Sasha Mitts",
        "Koustuv Sinha",
        "Ammar Rizvi",
        "Justine T. Kao"
      ],
      "abstract": "We introduce CausalVQA, a benchmark dataset for video question answering\n(VQA) composed of question-answer pairs that probe models' understanding of\ncausality in the physical world. Existing VQA benchmarks either tend to focus\non surface perceptual understanding of real-world videos, or on narrow physical\nreasoning questions created using simulation environments. CausalVQA fills an\nimportant gap by presenting challenging questions that are grounded in\nreal-world scenarios, while focusing on models' ability to predict the likely\noutcomes of different actions and events through five question types:\ncounterfactual, hypothetical, anticipation, planning and descriptive. We\ndesigned quality control mechanisms that prevent models from exploiting trivial\nshortcuts, requiring models to base their answers on deep visual understanding\ninstead of linguistic cues. We find that current frontier multimodal models\nfall substantially below human performance on the benchmark, especially on\nanticipation and hypothetical questions. This highlights a challenge for\ncurrent systems to leverage spatial-temporal reasoning, understanding of\nphysical principles, and comprehension of possible alternatives to make\naccurate predictions in real-world settings.",
      "pdf_url": "http://arxiv.org/pdf/2506.09943v1",
      "published": "2025-06-11T17:10:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09943v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.10; I.4.8"
      ]
    },
    {
      "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability",
      "authors": [
        "Jiachen Hu",
        "Rui Ai",
        "Han Zhong",
        "Xiaoyu Chen",
        "Liwei Wang",
        "Zhaoran Wang",
        "Zhuoran Yang"
      ],
      "abstract": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.",
      "pdf_url": "http://arxiv.org/pdf/2506.09940v1",
      "published": "2025-06-11T17:06:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09940v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
      "authors": [
        "Qiao Gu",
        "Yuanliang Ju",
        "Shengxiang Sun",
        "Igor Gilitschenski",
        "Haruki Nishimura",
        "Masha Itkina",
        "Florian Shkurti"
      ],
      "abstract": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2506.09937v1",
      "published": "2025-06-11T16:59:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09937v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations",
      "authors": [
        "Marco Federici",
        "Riccardo Del Chiaro",
        "Boris van Breugel",
        "Paul Whatmough",
        "Markus Nagel"
      ],
      "abstract": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2506.09932v1",
      "published": "2025-06-11T16:54:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09932v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants",
      "authors": [
        "Zheng Zhao",
        "Clara Vania",
        "Subhradeep Kayal",
        "Naila Khan",
        "Shay B. Cohen",
        "Emine Yilmaz"
      ],
      "abstract": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.",
      "pdf_url": "http://arxiv.org/pdf/2506.09902v1",
      "published": "2025-06-11T16:16:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09902v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Causal Climate Emulation with Bayesian Filtering",
      "authors": [
        "Sebastian Hickman",
        "Ilija Trajkovic",
        "Julia Kaltenborn",
        "Francis Pelletier",
        "Alex Archibald",
        "Yaniv Gurwicz",
        "Peer Nowack",
        "David Rolnick",
        "Julien Boussard"
      ],
      "abstract": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.",
      "pdf_url": "http://arxiv.org/pdf/2506.09891v1",
      "published": "2025-06-11T16:00:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09891v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "physics.ao-ph"
      ]
    },
    {
      "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language",
      "authors": [
        "Yuxin Chen",
        "Yiran Zhao",
        "Yang Zhang",
        "An Zhang",
        "Kenji Kawaguchi",
        "Shafiq Joty",
        "Junnan Li",
        "Tat-Seng Chua",
        "Michael Qizhe Shieh",
        "Wenxuan Zhang"
      ],
      "abstract": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.",
      "pdf_url": "http://arxiv.org/pdf/2506.09890v1",
      "published": "2025-06-11T16:00:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09890v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs",
      "authors": [
        "Rodion Oblovatny",
        "Alexandra Bazarova",
        "Alexey Zaytsev"
      ],
      "abstract": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.",
      "pdf_url": "http://arxiv.org/pdf/2506.09886v1",
      "published": "2025-06-11T15:59:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09886v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation",
      "authors": [
        "Seonho Lee",
        "Jiho Choi",
        "Inha Kang",
        "Jiwook Kim",
        "Junsung Park",
        "Hyunjung Shim"
      ],
      "abstract": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.09883v1",
      "published": "2025-06-11T15:56:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09883v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice",
      "authors": [
        "Emma Kallina",
        "Thomas Bohné",
        "Jat Singh"
      ],
      "abstract": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement\n(SHI) during AI development. At the same time, SHI is already common in\ncommercial software development, but with potentially different foci. This\nstudy clarifies the extent to which established SHI practices are able to\ncontribute to rAI efforts as well as potential disconnects -- essential\ninsights to inform and tailor future interventions that further shift industry\npractice towards rAI efforts. First, we analysed 56 rAI guidance documents to\nidentify why SHI is recommended (i.e. its expected benefits for rAI) and\nuncovered goals such as redistributing power, improving socio-technical\nunderstandings, anticipating risks, and enhancing public oversight. To\nunderstand why and how SHI is currently practised in commercial settings, we\nthen conducted an online survey (n=130) and semi-structured interviews (n=10)\nwith AI practitioners. Our findings reveal that SHI in practice is primarily\ndriven by commercial priorities (e.g. customer value, compliance) and several\nfactors currently discourage more rAI-aligned SHI practices. This suggests that\nestablished SHI practices are largely not contributing to rAI efforts. To\naddress this disconnect, we propose interventions and research opportunities to\nadvance rAI development in practice.",
      "pdf_url": "http://arxiv.org/pdf/2506.09873v1",
      "published": "2025-06-11T15:43:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09873v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Guided Graph Compression for Quantum Graph Neural Networks",
      "authors": [
        "Mikel Casals",
        "Vasilis Belis",
        "Elias F. Combarro",
        "Eduard Alarcón",
        "Sofia Vallecorsa",
        "Michele Grossi"
      ],
      "abstract": "Graph Neural Networks (GNNs) are effective for processing graph-structured\ndata but face challenges with large graphs due to high memory requirements and\ninefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a\npromising avenue to address these issues and inspires new algorithmic\napproaches. In particular, Quantum Graph Neural Networks (QGNNs) have been\nexplored in recent literature. However, current quantum hardware limits the\ndimension of the data that can be effectively encoded. Existing approaches\neither simplify datasets manually or use artificial graph datasets. This work\nintroduces the Guided Graph Compression (GGC) framework, which uses a graph\nautoencoder to reduce both the number of nodes and the dimensionality of node\nfeatures. The compression is guided to enhance the performance of a downstream\nclassification task, which can be applied either with a quantum or a classical\nclassifier. The framework is evaluated on the Jet Tagging task, a\nclassification problem of fundamental importance in high energy physics that\ninvolves distinguishing particle jets initiated by quarks from those by gluons.\nThe GGC is compared against using the autoencoder as a standalone preprocessing\nstep and against a baseline classical GNN classifier. Our numerical results\ndemonstrate that GGC outperforms both alternatives, while also facilitating the\ntesting of novel QGNN ansatzes on realistic datasets.",
      "pdf_url": "http://arxiv.org/pdf/2506.09862v1",
      "published": "2025-06-11T15:36:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09862v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "hep-ex",
        "quant-ph"
      ]
    },
    {
      "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning",
      "authors": [
        "Xiangning Yu",
        "Zhuohan Wang",
        "Linyi Yang",
        "Haoxuan Li",
        "Anjie Liu",
        "Xiao Xue",
        "Jun Wang",
        "Mengyue Yang"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.",
      "pdf_url": "http://arxiv.org/pdf/2506.09853v1",
      "published": "2025-06-11T15:22:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09853v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ]
    },
    {
      "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment",
      "authors": [
        "Tomas Peterka",
        "Matyas Bohacek"
      ],
      "abstract": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.",
      "pdf_url": "http://arxiv.org/pdf/2506.09847v1",
      "published": "2025-06-11T15:21:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09847v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ]
    },
    {
      "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition",
      "authors": [
        "Panagiotis Kaliosis",
        "John Pavlopoulos"
      ],
      "abstract": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.",
      "pdf_url": "http://arxiv.org/pdf/2506.09846v1",
      "published": "2025-06-11T15:20:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09846v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "OctoNav: Towards Generalist Embodied Navigation",
      "authors": [
        "Chen Gao",
        "Liankai Jin",
        "Xingyu Peng",
        "Jiazhao Zhang",
        "Yue Deng",
        "Annan Li",
        "He Wang",
        "Si Liu"
      ],
      "abstract": "Embodied navigation stands as a foundation pillar within the broader pursuit\nof embodied AI. However, previous navigation research is divided into different\ntasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task\nobjectives and modalities, making datasets and methods are designed\nindividually. In this work, we take steps toward generalist navigation agents,\nwhich can follow free-form instructions that include arbitrary compounds of\nmulti-modal and multi-capability. To achieve this, we propose a large-scale\nbenchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.\nSpecifically, OctoNav-Bench features continuous environments and is constructed\nvia a designed annotation pipeline. We thoroughly craft instruction-trajectory\npairs, where instructions are diverse in free-form with arbitrary modality and\ncapability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within\nOctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,\nwe build it upon MLLMs and adapt it to a VLA-type model, which can produce\nlow-level actions solely based on 2D visual observations. Moreover, we design a\nHybrid Training Paradigm (HTP) that consists of three stages, i.e.,\nAction-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains\nspecifically designed learning policies and rewards. Importantly, for TBA-SFT\nand Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which\nshow impressive reasoning ability via thinking-before-answer. Thus, we aim to\ninvestigate how to achieve thinking-before-action in the embodied navigation\nfield, to improve model's reasoning ability toward generalists. Specifically,\nwe propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a\ncold-start phrase and then leverage Nav-GPRO to improve its thinking ability.\nFinally, OctoNav-R1 shows superior performance compared with previous methods.",
      "pdf_url": "http://arxiv.org/pdf/2506.09839v1",
      "published": "2025-06-11T15:15:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09839v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction",
      "authors": [
        "Junli Deng",
        "Ping Shi",
        "Qipei Li",
        "Jinyang Guo"
      ],
      "abstract": "Reconstructing intricate, ever-changing environments remains a central\nambition in computer vision, yet existing solutions often crumble before the\ncomplexity of real-world dynamics. We present DynaSplat, an approach that\nextends Gaussian Splatting to dynamic scenes by integrating dynamic-static\nseparation and hierarchical motion modeling. First, we classify scene elements\nas static or dynamic through a novel fusion of deformation offset statistics\nand 2D motion flow consistency, refining our spatial representation to focus\nprecisely where motion matters. We then introduce a hierarchical motion\nmodeling strategy that captures both coarse global transformations and\nfine-grained local movements, enabling accurate handling of intricate,\nnon-rigid motions. Finally, we integrate physically-based opacity estimation to\nensure visually coherent reconstructions, even under challenging occlusions and\nperspective shifts. Extensive experiments on challenging datasets reveal that\nDynaSplat not only surpasses state-of-the-art alternatives in accuracy and\nrealism but also provides a more intuitive, compact, and efficient route to\ndynamic scene reconstruction.",
      "pdf_url": "http://arxiv.org/pdf/2506.09836v1",
      "published": "2025-06-11T15:13:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09836v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection",
      "authors": [
        "Christoph Schuhmann",
        "Robert Kaczmarczyk",
        "Gollam Rabby",
        "Felix Friedrich",
        "Maurice Kraus",
        "Kourosh Nadi",
        "Huu Nguyen",
        "Kristian Kersting",
        "Sören Auer"
      ],
      "abstract": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.",
      "pdf_url": "http://arxiv.org/pdf/2506.09827v1",
      "published": "2025-06-11T15:06:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09827v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Superstudent intelligence in thermodynamics",
      "authors": [
        "Rebecca Loubet",
        "Pascal Zittlau",
        "Marco Hoffmann",
        "Luisa Vollmer",
        "Sophie Fellenz",
        "Heike Leitte",
        "Fabian Jirasek",
        "Johannes Lenhard",
        "Hans Hasse"
      ],
      "abstract": "In this short note, we report and analyze a striking event: OpenAI's large\nlanguage model o3 has outwitted all students in a university exam on\nthermodynamics. The thermodynamics exam is a difficult hurdle for most\nstudents, where they must show that they have mastered the fundamentals of this\nimportant topic. Consequently, the failure rates are very high, A-grades are\nrare - and they are considered proof of the students' exceptional intellectual\nabilities. This is because pattern learning does not help in the exam. The\nproblems can only be solved by knowledgeably and creatively combining\nprinciples of thermodynamics. We have given our latest thermodynamics exam not\nonly to the students but also to OpenAI's most powerful reasoning model, o3,\nand have assessed the answers of o3 exactly the same way as those of the\nstudents. In zero-shot mode, the model o3 solved all problems correctly, better\nthan all students who took the exam; its overall score was in the range of the\nbest scores we have seen in more than 10,000 similar exams since 1985. This is\na turning point: machines now excel in complex tasks, usually taken as proof of\nhuman intellectual capabilities. We discuss the consequences this has for the\nwork of engineers and the education of future engineers.",
      "pdf_url": "http://arxiv.org/pdf/2506.09822v1",
      "published": "2025-06-11T15:01:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09822v1",
      "categories": [
        "cs.CE",
        "cs.AI"
      ]
    },
    {
      "title": "CoRT: Code-integrated Reasoning within Thinking",
      "authors": [
        "Chengpeng Li",
        "Zhengyang Tang",
        "Ziniu Li",
        "Mingfeng Xue",
        "Keqin Bao",
        "Tian Ding",
        "Ruoyu Sun",
        "Benyou Wang",
        "Xiang Wang",
        "Junyang Lin",
        "Dayiheng Liu"
      ],
      "abstract": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.",
      "pdf_url": "http://arxiv.org/pdf/2506.09820v2",
      "published": "2025-06-11T14:59:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09820v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A theoretical framework for self-supervised contrastive learning for continuous dependent data",
      "authors": [
        "Alexander Marusov",
        "Alexander Yuhay",
        "Alexey Zaytsev"
      ],
      "abstract": "Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.",
      "pdf_url": "http://arxiv.org/pdf/2506.09785v1",
      "published": "2025-06-11T14:23:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09785v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2",
      "authors": [
        "Nicola Farronato",
        "Florian Scheidegger",
        "Mattia Rigotti",
        "Cristiano Malossi",
        "Michele Magno",
        "Haotong Qin"
      ],
      "abstract": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.",
      "pdf_url": "http://arxiv.org/pdf/2506.09782v1",
      "published": "2025-06-11T14:21:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09782v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space",
      "authors": [
        "Anton Razzhigaev",
        "Matvey Mikhalchuk",
        "Klim Kireev",
        "Igor Udovichenko",
        "Andrey Kuznetsov",
        "Aleksandr Petiushko"
      ],
      "abstract": "Reconstructing facial images from black-box recognition models poses a\nsignificant privacy threat. While many methods require access to embeddings, we\naddress the more challenging scenario of model inversion using only similarity\nscores. This paper introduces DarkerBB, a novel approach that reconstructs\ncolor faces by performing zero-order optimization within a PCA-derived\neigenface space. Despite this highly limited information, experiments on LFW,\nAgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves\nstate-of-the-art verification accuracies in the similarity-only setting, with\ncompetitive query efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2506.09777v1",
      "published": "2025-06-11T14:15:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09777v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning",
      "authors": [
        "Haruki Kainuma",
        "Takayuki Nishio"
      ],
      "abstract": "This paper proposes Load-aware Tram-FL, an extension of Tram-FL that\nintroduces a training scheduling mechanism to minimize total training time in\ndecentralized federated learning by accounting for both computational and\ncommunication loads. The scheduling problem is formulated as a global\noptimization task, which-though intractable in its original form-is made\nsolvable by decomposing it into node-wise subproblems. To promote balanced data\nutilization under non-IID distributions, a variance constraint is introduced,\nwhile the overall training latency, including both computation and\ncommunication costs, is minimized through the objective function. Simulation\nresults on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly\nreduces training time and accelerates convergence compared to baseline methods.",
      "pdf_url": "http://arxiv.org/pdf/2506.09769v1",
      "published": "2025-06-11T14:09:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09769v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era",
      "authors": [
        "Shuo Jiang",
        "Min Xie",
        "Frank Youhua Chen",
        "Jian Ma",
        "Jianxi Luo"
      ],
      "abstract": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.",
      "pdf_url": "http://arxiv.org/pdf/2506.09755v1",
      "published": "2025-06-11T13:57:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09755v1",
      "categories": [
        "cs.CE",
        "cs.AI",
        "I.2.7; I.2.1"
      ]
    },
    {
      "title": "Large Language Models for Design Structure Matrix Optimization",
      "authors": [
        "Shuo Jiang",
        "Min Xie",
        "Jianxi Luo"
      ],
      "abstract": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization.",
      "pdf_url": "http://arxiv.org/pdf/2506.09749v1",
      "published": "2025-06-11T13:53:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09749v1",
      "categories": [
        "cs.CE",
        "cs.AI",
        "I.2.7; I.2.1"
      ]
    },
    {
      "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring",
      "authors": [
        "Gusseppe Bravo-Rocca",
        "Peini Liu",
        "Jordi Guitart",
        "Rodrigo M Carrillo-Larco",
        "Ajay Dholakia",
        "David Ellison"
      ],
      "abstract": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.",
      "pdf_url": "http://arxiv.org/pdf/2506.09742v1",
      "published": "2025-06-11T13:48:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09742v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models",
      "authors": [
        "Qin Zhou",
        "Zhiyang Zhang",
        "Jinglong Wang",
        "Xiaobin Li",
        "Jing Zhang",
        "Qian Yu",
        "Lu Sheng",
        "Dong Xu"
      ],
      "abstract": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.",
      "pdf_url": "http://arxiv.org/pdf/2506.09740v1",
      "published": "2025-06-11T13:47:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09740v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning",
      "authors": [
        "Yuting Li",
        "Lai Wei",
        "Kaipeng Zheng",
        "Jingyuan Huang",
        "Linghe Kong",
        "Lichao Sun",
        "Weiran Huang"
      ],
      "abstract": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.",
      "pdf_url": "http://arxiv.org/pdf/2506.09736v1",
      "published": "2025-06-11T13:39:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09736v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale",
      "authors": [
        "Minjong Cheon"
      ],
      "abstract": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.",
      "pdf_url": "http://arxiv.org/pdf/2506.09733v1",
      "published": "2025-06-11T13:38:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09733v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "physics.ao-ph"
      ]
    },
    {
      "title": "Non-Contact Health Monitoring During Daily Personal Care Routines",
      "authors": [
        "Xulin Ma",
        "Jiankai Tang",
        "Zhang Jiang",
        "Songqin Cheng",
        "Yuanchun Shi",
        "Dong LI",
        "Xin Liu",
        "Daniel McDuff",
        "Xiaojing Liu",
        "Yuntao Wang"
      ],
      "abstract": "Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring\nof physiological signals and offers a practical alternative to traditional\nhealth sensing methods. Although rPPG is promising for daily health monitoring,\nits application in long-term personal care scenarios, such as mirror-facing\nroutines in high-altitude environments, remains challenging due to ambient\nlighting variations, frequent occlusions from hand movements, and dynamic\nfacial postures. To address these challenges, we present LADH (Long-term\nAltitude Daily Health), the first long-term rPPG dataset containing 240\nsynchronized RGB and infrared (IR) facial videos from 21 participants across\nfive common personal care scenarios, along with ground-truth PPG, respiration,\nand blood oxygen signals. Our experiments demonstrate that combining RGB and IR\nvideo inputs improves the accuracy and robustness of non-contact physiological\nmonitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate\nestimation. Furthermore, we find that multi-task learning enhances performance\nacross multiple physiological indicators simultaneously. Dataset and code are\nopen at https://github.com/McJackTang/FusionVitals.",
      "pdf_url": "http://arxiv.org/pdf/2506.09718v1",
      "published": "2025-06-11T13:29:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09718v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal",
      "authors": [
        "Vincenzo Collura",
        "Karim Tit",
        "Laura Bussi",
        "Eleonora Giunchiglia",
        "Maxime Cordy"
      ],
      "abstract": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.",
      "pdf_url": "http://arxiv.org/pdf/2506.09701v1",
      "published": "2025-06-11T13:14:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09701v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model",
      "authors": [
        "Changwei Wu",
        "Yifei Chen",
        "Yuxin Du",
        "Jinying Zong",
        "Jie Dong",
        "Mingxuan Liu",
        "Yong Peng",
        "Jin Fan",
        "Feiwei Qin",
        "Changmiao Wang"
      ],
      "abstract": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.",
      "pdf_url": "http://arxiv.org/pdf/2506.09695v1",
      "published": "2025-06-11T13:10:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09695v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Reasoning Models Are More Easily Gaslighted Than You Think",
      "authors": [
        "Bin Zhu",
        "Hailong Yin",
        "Jingjing Chen",
        "Yu-Gang Jiang"
      ],
      "abstract": "Recent advances in reasoning-centric models promise improved robustness\nthrough mechanisms such as chain-of-thought prompting and test-time scaling.\nHowever, their ability to withstand misleading user input remains\nunderexplored. In this paper, we conduct a systematic evaluation of three\nstate-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet\nand Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and\nCharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)\nfollowing gaslighting negation prompts, indicating that even top-tier reasoning\nmodels struggle to preserve correct answers under manipulative user feedback.\nBuilt upon the insights of the evaluation and to further probe this\nvulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark\nspecifically designed to evaluate reasoning models' susceptibility to defend\ntheir belief under gaslighting negation prompt. Constructed by filtering and\ncurating 1,025 challenging samples from the existing benchmarks,\nGaslightingBench-R induces even more dramatic failures, with accuracy drops\nexceeding 53% on average. Our findings reveal fundamental limitations in the\nrobustness of reasoning models, highlighting the gap between step-by-step\nreasoning and belief persistence.",
      "pdf_url": "http://arxiv.org/pdf/2506.09677v1",
      "published": "2025-06-11T12:52:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09677v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data",
      "authors": [
        "Hao Xiong",
        "Chuanyuan Tan",
        "Wenliang Chen"
      ],
      "abstract": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%",
      "pdf_url": "http://arxiv.org/pdf/2506.09672v1",
      "published": "2025-06-11T12:43:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09672v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Empirical Quantification of Spurious Correlations in Malware Detection",
      "authors": [
        "Bianca Perasso",
        "Ludovico Lozza",
        "Andrea Ponte",
        "Luca Demetrio",
        "Luca Oneto",
        "Fabio Roli"
      ],
      "abstract": "End-to-end deep learning exhibits unmatched performance for detecting\nmalware, but such an achievement is reached by exploiting spurious correlations\n-- features with high relevance at inference time, but known to be useless\nthrough domain knowledge. While previous work highlighted that deep networks\nmainly focus on metadata, none investigated the phenomenon further, without\nquantifying their impact on the decision. In this work, we deepen our\nunderstanding of how spurious correlation affects deep learning for malware\ndetection by highlighting how much models rely on empty spaces left by the\ncompiler, which diminishes the relevance of the compiled code. Through our\nseminal analysis on a small-scale balanced dataset, we introduce a ranking of\ntwo end-to-end models to better understand which is more suitable to be put in\nproduction.",
      "pdf_url": "http://arxiv.org/pdf/2506.09662v1",
      "published": "2025-06-11T12:32:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09662v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model",
      "authors": [
        "Eltayeb Ahmed",
        "Uljad Berdica",
        "Martha Elliott",
        "Danijela Horak",
        "Jakob N. Foerster"
      ],
      "abstract": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.09659v1",
      "published": "2025-06-11T12:26:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09659v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives",
      "authors": [
        "Wei Zeng",
        "Hengshu Zhu",
        "Chuan Qin",
        "Han Wu",
        "Yihang Cheng",
        "Sirui Zhang",
        "Xiaowei Jin",
        "Yinuo Shen",
        "Zhenxing Wang",
        "Feimin Zhong",
        "Hui Xiong"
      ],
      "abstract": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field.",
      "pdf_url": "http://arxiv.org/pdf/2506.09656v1",
      "published": "2025-06-11T12:25:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.09656v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}