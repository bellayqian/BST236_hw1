{
  "last_updated": "2025-03-06T00:45:47.402607",
  "papers": [
    {
      "title": "Bringing Comparative Cognition To Computers",
      "authors": [
        "Konstantinos Voudouris",
        "Lucy G. Cheke",
        "Eric Schulz"
      ],
      "abstract": "Researchers are increasingly subjecting artificial intelligence systems to\npsychological testing. But to rigorously compare their cognitive capacities\nwith humans and other animals, we must avoid both over- and under-stating our\nsimilarities and differences. By embracing a comparative approach, we can\nintegrate AI cognition research into the broader cognitive sciences.",
      "pdf_url": "http://arxiv.org/pdf/2503.02882v1",
      "published": "2025-03-04T18:58:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02882v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation",
      "authors": [
        "Han Xue",
        "Jieji Ren",
        "Wendi Chen",
        "Gu Zhang",
        "Yuan Fang",
        "Guoying Gu",
        "Huazhe Xu",
        "Cewu Lu"
      ],
      "abstract": "Humans can accomplish complex contact-rich tasks using vision and touch, with\nhighly reactive capabilities such as quick adjustments to environmental changes\nand adaptive control of contact forces; however, this remains challenging for\nrobots. Existing visual imitation learning (IL) approaches rely on action\nchunking to model complex behaviors, which lacks the ability to respond\ninstantly to real-time tactile feedback during the chunk execution.\nFurthermore, most teleoperation systems struggle to provide fine-grained\ntactile / force feedback, which limits the range of tasks that can be\nperformed. To address these challenges, we introduce TactAR, a low-cost\nteleoperation system that provides real-time tactile feedback through Augmented\nReality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast\nvisual-tactile imitation learning algorithm for learning contact-rich\nmanipulation skills. RDP employs a two-level hierarchy: (1) a slow latent\ndiffusion policy for predicting high-level action chunks in latent space at low\nfrequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback\ncontrol at high frequency. This design enables both complex trajectory modeling\nand quick reactive behavior within a unified framework. Through extensive\nevaluation across three challenging contact-rich tasks, RDP significantly\nimproves performance compared to state-of-the-art visual IL baselines through\nrapid response to tactile / force feedback. Furthermore, experiments show that\nRDP is applicable across different tactile / force sensors. Code and videos are\navailable on https://reactive-diffusion-policy.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2503.02881v1",
      "published": "2025-03-04T18:58:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02881v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
      "authors": [
        "Siming Huang",
        "Yuliang Xu",
        "Mingmeng Geng",
        "Yao Wan",
        "Dongping Chen"
      ],
      "abstract": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks.",
      "pdf_url": "http://arxiv.org/pdf/2503.02879v1",
      "published": "2025-03-04T18:58:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02879v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
      "authors": [
        "Ethan Mendes",
        "Alan Ritter"
      ],
      "abstract": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards.",
      "pdf_url": "http://arxiv.org/pdf/2503.02878v1",
      "published": "2025-03-04T18:58:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02878v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Evaluation of Architectural Synthesis Using Generative AI",
      "authors": [
        "Jingfei Huang",
        "Alexandros Haridis"
      ],
      "abstract": "Recent advancements in multimodal Generative AI have the potential to\ndemocratize specialized architectural tasks, such as interpreting technical\ndrawings and creating 3D CAD models, which traditionally require expert\nknowledge. This paper presents a comparative evaluation of two systems: GPT-4o\nand Claude 3.5, in the task of architectural 3D synthesis. We conduct a case\nstudy on two buildings from Palladio's Four Books of Architecture (1965): Villa\nRotonda and Palazzo Porto. High-level architectural models and drawings of\nthese buildings were prepared, inspired by Palladio's original texts and\ndrawings. Through sequential text and image prompting, we assess the systems'\nabilities in (1) interpreting 2D and 3D representations of buildings from\ndrawings, (2) encoding the buildings into a CAD software script, and (3)\nself-improving based on outputs. While both systems successfully generate\nindividual parts, they struggle to accurately assemble these parts into the\ndesired spatial relationships, with Claude 3.5 demonstrating better\nperformance, particularly in self-correcting its output. This study contributes\nto ongoing research on benchmarking the strengths and weaknesses of\noff-the-shelf AI systems in performing intelligent human tasks that require\ndiscipline-specific knowledge. The findings highlight the potential of\nlanguage-enabled AI systems to act as collaborative technical assistants in the\narchitectural design process.",
      "pdf_url": "http://arxiv.org/pdf/2503.02861v1",
      "published": "2025-03-04T18:39:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02861v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024",
      "authors": [
        "Nuria Alina Chandra",
        "Ryan Murtfeldt",
        "Lin Qiu",
        "Arnab Karmakar",
        "Hannah Lee",
        "Emmanuel Tanumihardja",
        "Kevin Farhat",
        "Ben Caffee",
        "Sejin Paik",
        "Changyeon Lee",
        "Jongwook Choi",
        "Aerin Kim",
        "Oren Etzioni"
      ],
      "abstract": "In the age of increasingly realistic generative AI, robust deepfake detection\nis essential for mitigating fraud and disinformation. While many deepfake\ndetectors report high accuracy on academic datasets, we show that these\nacademic benchmarks are out of date and not representative of recent deepfakes.\nWe introduce Deepfake-Eval-2024, a new deepfake detection benchmark consisting\nof in-the-wild deepfakes collected from social media and deepfake detection\nplatform users in 2024. Deepfake-Eval-2024 consists of 44 hours of videos, 56.5\nhours of audio, and 1,975 images, encompassing the latest manipulation\ntechnologies. The benchmark contains diverse media content from 88 different\nwebsites in 52 different languages. We find that the performance of open-source\nstate-of-the-art deepfake detection models drops precipitously when evaluated\non Deepfake-Eval-2024, with AUC decreasing by 50% for video, 48% for audio, and\n45% for image models compared to previous benchmarks. We also evaluate\ncommercial deepfake detection models and models finetuned on\nDeepfake-Eval-2024, and find that they have superior performance to\noff-the-shelf open-source models, but they do not yet reach the accuracy of\nhuman deepfake forensic analysts. The dataset is available at\nhttps://github.com/nuriachandra/Deepfake-Eval-2024.",
      "pdf_url": "http://arxiv.org/pdf/2503.02857v1",
      "published": "2025-03-04T18:33:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02857v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "(How) Do Language Models Track State?",
      "authors": [
        "Belinda Z. Li",
        "Zifan Carl Guo",
        "Jacob Andreas"
      ],
      "abstract": "Transformer language models (LMs) exhibit behaviors -- from storytelling to\ncode generation -- that appear to require tracking the unobserved state of an\nevolving world. How do they do so? We study state tracking in LMs trained or\nfine-tuned to compose permutations (i.e., to compute the order of a set of\nobjects after a sequence of swaps). Despite the simple algebraic structure of\nthis problem, many other tasks (e.g., simulation of finite automata and\nevaluation of boolean expressions) can be reduced to permutation composition,\nmaking it a natural model for state tracking in general. We show that LMs\nconsistently learn one of two state tracking mechanisms for this task. The\nfirst closely resembles the \"associative scan\" construction used in recent\ntheoretical work by Liu et al. (2023) and Merrill et al. (2024). The second\nuses an easy-to-compute feature (permutation parity) to partially prune the\nspace of outputs, then refines this with an associative scan. The two\nmechanisms exhibit markedly different robustness properties, and we show how to\nsteer LMs toward one or the other with intermediate training tasks that\nencourage or suppress the heuristics. Our results demonstrate that transformer\nLMs, whether pretrained or fine-tuned, can learn to implement efficient and\ninterpretable state tracking mechanisms, and the emergence of these mechanisms\ncan be predicted and controlled.",
      "pdf_url": "http://arxiv.org/pdf/2503.02854v1",
      "published": "2025-03-04T18:31:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02854v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multimodal Deep Learning for Subtype Classification in Breast Cancer Using Histopathological Images and Gene Expression Data",
      "authors": [
        "Amin Honarmandi Shandiz"
      ],
      "abstract": "Molecular subtyping of breast cancer is crucial for personalized treatment\nand prognosis. Traditional classification approaches rely on either\nhistopathological images or gene expression profiling, limiting their\npredictive power. In this study, we propose a deep multimodal learning\nframework that integrates histopathological images and gene expression data to\nclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Our\napproach employs a ResNet-50 model for image feature extraction and fully\nconnected layers for gene expression processing, with a cross-attention fusion\nmechanism to enhance modality interaction. We conduct extensive experiments\nusing five-fold cross-validation, demonstrating that our multimodal integration\noutperforms unimodal approaches in terms of classification accuracy,\nprecision-recall AUC, and F1-score. Our findings highlight the potential of\ndeep learning for robust and interpretable breast cancer subtype\nclassification, paving the way for improved clinical decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2503.02849v1",
      "published": "2025-03-04T18:24:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02849v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SeqFusion: Sequential Fusion of Pre-Trained Models for Zero-Shot Time-Series Forecasting",
      "authors": [
        "Ting-Ji Huang",
        "Xu-Yang Chen",
        "Han-Jia Ye"
      ],
      "abstract": "Unlike traditional time-series forecasting methods that require extensive\nin-task data for training, zero-shot forecasting can directly predict future\nvalues given a target time series without additional training data. Current\nzero-shot approaches primarily rely on pre-trained generalized models, with\ntheir performance often depending on the variety and relevance of the\npre-training data, which can raise privacy concerns. Instead of collecting\ndiverse pre-training data, we introduce SeqFusion in this work, a novel\nframework that collects and fuses diverse pre-trained models (PTMs)\nsequentially for zero-shot forecasting. Based on the specific temporal\ncharacteristics of the target time series, SeqFusion selects the most suitable\nPTMs from a batch of pre-collected PTMs, performs sequential predictions, and\nfuses all the predictions while using minimal data to protect privacy. Each of\nthese PTMs specializes in different temporal patterns and forecasting tasks,\nallowing SeqFusion to select by measuring distances in a shared representation\nspace of the target time series with each PTM. Experiments demonstrate that\nSeqFusion achieves competitive accuracy in zero-shot forecasting compared to\nstate-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2503.02836v1",
      "published": "2025-03-04T17:59:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02836v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation",
      "authors": [
        "Songming Zhang",
        "Xue Zhang",
        "Tong Zhang",
        "Bojie Hu",
        "Yufeng Chen",
        "Jinan Xu"
      ],
      "abstract": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.",
      "pdf_url": "http://arxiv.org/pdf/2503.02832v1",
      "published": "2025-03-04T17:57:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02832v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Developing a PET/CT Foundation Model for Cross-Modal Anatomical and Functional Imaging",
      "authors": [
        "Yujin Oh",
        "Robert Seifert",
        "Yihan Cao",
        "Christoph Clement",
        "Justin Ferdinandus",
        "Constantin Lapa",
        "Alessandro Liebich",
        "Michelle Amon",
        "Johanna Enke",
        "Sifan Song",
        "Runqi Meng",
        "Fang Zeng",
        "Ning Guo",
        "Xiang Li",
        "Pedram Heidari",
        "Axel Rominger",
        "Kuangyu Shi",
        "Quanzheng Li"
      ],
      "abstract": "In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) is\nwidely used in cancer diagnosis, staging, and treatment monitoring, as it\ncombines anatomical details from CT with functional metabolic activity and\nmolecular marker expression information from PET. However, existing artificial\nintelligence-driven PET/CT analyses rely predominantly on task-specific models\ntrained from scratch or on limited datasets, limiting their generalizability\nand robustness. To address this, we propose a foundation model approach\nspecifically designed for multimodal PET/CT imaging. We introduce the\nCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework that\neffectively integrates whole-body anatomical and functional or molecular\ninformation. FratMAE employs separate Vision Transformer (ViT) encoders for PET\nand CT scans, along with cross-attention decoders that enable synergistic\ninteractions between modalities during masked autoencoder training.\nAdditionally, it incorporates textual metadata to enhance PET representation\nlearning. By pre-training on PET/CT datasets, FratMAE captures intricate\ncross-modal relationships and global uptake patterns, achieving superior\nperformance on downstream tasks and demonstrating its potential as a\ngeneralizable foundation model.",
      "pdf_url": "http://arxiv.org/pdf/2503.02824v1",
      "published": "2025-03-04T17:49:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02824v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
      "authors": [
        "Matteo Spanio",
        "Massimiliano Zampini",
        "Antonio Rodà",
        "Franco Pierucci"
      ],
      "abstract": "In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.",
      "pdf_url": "http://arxiv.org/pdf/2503.02823v1",
      "published": "2025-03-04T17:48:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02823v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "I.2.6; J.5"
      ]
    },
    {
      "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
      "authors": [
        "Nathan Godey",
        "Alessio Devoto",
        "Yu Zhao",
        "Simone Scardapane",
        "Pasquale Minervini",
        "Éric de la Clergerie",
        "Benoît Sagot"
      ],
      "abstract": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
      "pdf_url": "http://arxiv.org/pdf/2503.02812v1",
      "published": "2025-03-04T17:37:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02812v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Causal Framework for Aligning Image Quality Metrics and Deep Neural Network Robustness",
      "authors": [
        "Nathan Drenkow",
        "Mathias Unberath"
      ],
      "abstract": "Image quality plays an important role in the performance of deep neural\nnetworks (DNNs) and DNNs have been widely shown to exhibit sensitivity to\nchanges in imaging conditions. Large-scale datasets often contain images under\na wide range of conditions prompting a need to quantify and understand their\nunderlying quality distribution in order to better characterize DNN performance\nand robustness. Aligning the sensitivities of image quality metrics and DNNs\nensures that estimates of quality can act as proxies for image/dataset\ndifficulty independent of the task models trained/evaluated on the data.\nConventional image quality assessment (IQA) seeks to measure and align quality\nrelative to human perceptual judgments, but here we seek a quality measure that\nis not only sensitive to imaging conditions but also well-aligned with DNN\nsensitivities. We first ask whether conventional IQA metrics are also\ninformative of DNN performance. In order to answer this question, we reframe\nIQA from a causal perspective and examine conditions under which quality\nmetrics are predictive of DNN performance. We show theoretically and\nempirically that current IQA metrics are weak predictors of DNN performance in\nthe context of classification. We then use our causal framework to provide an\nalternative formulation and a new image quality metric that is more strongly\ncorrelated with DNN performance and can act as a prior on performance without\ntraining new task models. Our approach provides a means to directly estimate\nthe quality distribution of large-scale image datasets towards characterizing\nthe relationship between dataset composition and DNN performance.",
      "pdf_url": "http://arxiv.org/pdf/2503.02797v1",
      "published": "2025-03-04T17:15:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02797v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Do Not Trust Licenses You See -- Dataset Compliance Requires Massive-Scale AI-Powered Lifecycle Tracing",
      "authors": [
        "Jaekyeom Kim",
        "Sungryull Sohn",
        "Gerrard Jeongwon Jo",
        "Jihoon Choi",
        "Kyunghoon Bae",
        "Hwayoung Lee",
        "Yongmin Park",
        "Honglak Lee"
      ],
      "abstract": "This paper argues that a dataset's legal risk cannot be accurately assessed\nby its license terms alone; instead, tracking dataset redistribution and its\nfull lifecycle is essential. However, this process is too complex for legal\nexperts to handle manually at scale. Tracking dataset provenance, verifying\nredistribution rights, and assessing evolving legal risks across multiple\nstages require a level of precision and efficiency that exceeds human\ncapabilities. Addressing this challenge effectively demands AI agents that can\nsystematically trace dataset redistribution, analyze compliance, and identify\nlegal risks. We develop an automated data compliance system called NEXUS and\nshow that AI can perform these tasks with higher accuracy, efficiency, and\ncost-effectiveness than human experts. Our massive legal analysis of 17,429\nunique entities and 8,072 license terms using this approach reveals the\ndiscrepancies in legal rights between the original datasets before\nredistribution and their redistributed subsets, underscoring the necessity of\nthe data lifecycle-aware compliance. For instance, we find that out of 2,852\ndatasets with commercially viable individual license terms, only 605 (21%) are\nlegally permissible for commercialization. This work sets a new standard for AI\ndata governance, advocating for a framework that systematically examines the\nentire lifecycle of dataset redistribution to ensure transparent, legal, and\nresponsible dataset management.",
      "pdf_url": "http://arxiv.org/pdf/2503.02784v1",
      "published": "2025-03-04T16:57:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02784v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging",
      "authors": [
        "Jie Wu",
        "Haoling Li",
        "Xin Zhang",
        "Jianwen Luo",
        "Yangyu Huang",
        "Ruihang Chu",
        "Yujiu Yang",
        "Scarlett Li"
      ],
      "abstract": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.",
      "pdf_url": "http://arxiv.org/pdf/2503.02783v1",
      "published": "2025-03-04T16:56:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02783v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Implicit Bias in LLMs: A Survey",
      "authors": [
        "Xinru Lin",
        "Luyang Li"
      ],
      "abstract": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task.",
      "pdf_url": "http://arxiv.org/pdf/2503.02776v1",
      "published": "2025-03-04T16:49:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02776v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Prime Convolutional Model: Breaking the Ground for Theoretical Explainability",
      "authors": [
        "Francesco Panelli",
        "Doaa Almhaithawi",
        "Tania Cerquitelli",
        "Alessandro Bellini"
      ],
      "abstract": "In this paper, we propose a new theoretical approach to Explainable AI.\nFollowing the Scientific Method, this approach consists in formulating on the\nbasis of empirical evidence, a mathematical model to explain and predict the\nbehaviors of Neural Networks. We apply the method to a case study created in a\ncontrolled environment, which we call Prime Convolutional Model (p-Conv for\nshort). p-Conv operates on a dataset consisting of the first one million\nnatural numbers and is trained to identify the congruence classes modulo a\ngiven integer $m$. Its architecture uses a convolutional-type neural network\nthat contextually processes a sequence of $B$ consecutive numbers to each\ninput. We take an empirical approach and exploit p-Conv to identify the\ncongruence classes of numbers in a validation set using different values for\n$m$ and $B$. The results show that the different behaviors of p-Conv (i.e.,\nwhether it can perform the task or not) can be modeled mathematically in terms\nof $m$ and $B$. The inferred mathematical model reveals interesting patterns\nable to explain when and why p-Conv succeeds in performing task and, if not,\nwhich error pattern it follows.",
      "pdf_url": "http://arxiv.org/pdf/2503.02773v1",
      "published": "2025-03-04T16:42:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02773v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Improving Oil Slick Trajectory Simulations with Bayesian Optimization",
      "authors": [
        "Gabriele Accarino",
        "Marco M. De Carlo",
        "Igor Atake",
        "Donatello Elia",
        "Anusha L. Dissanayake",
        "Antonio Augusto Sepp Neves",
        "Juan Peña Ibañez",
        "Italo Epicoco",
        "Paola Nassisi",
        "Sandro Fiore",
        "Giovanni Coppini"
      ],
      "abstract": "Accurate simulations of oil spill trajectories are essential for supporting\npractitioners' response and mitigating environmental and socioeconomic impacts.\nNumerical models, such as MEDSLIK-II, simulate advection, dispersion, and\ntransformation processes of oil particles. However, simulations heavily rely on\naccurate parameter tuning, still based on expert knowledge and manual\ncalibration. To overcome these limitations, we integrate the MEDSLIK-II\nnumerical oil spill model with a Bayesian optimization framework to iteratively\nestimate the best physical parameter configuration that yields simulation\ncloser to satellite observations of the slick. We focus on key parameters, such\nas horizontal diffusivity and drift factor, maximizing the Fraction Skill Score\n(FSS) as a measure of spatio-temporal overlap between simulated and observed\noil distributions. We validate the framework for the Baniyas oil incident that\noccurred in Syria between August 23 and September 4, 2021, which released over\n12,000 $m^3$ of oil. We show that, on average, the proposed approach\nsystematically improves the FSS from 5.82% to 11.07% compared to control\nsimulations initialized with default parameters. The optimization results in\nconsistent improvement across multiple time steps, particularly during periods\nof increased drift variability, demonstrating the robustness of our method in\ndynamic environmental conditions.",
      "pdf_url": "http://arxiv.org/pdf/2503.02749v1",
      "published": "2025-03-04T16:14:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02749v1",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "I.2; I.6; J.2; G.3"
      ]
    },
    {
      "title": "UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression",
      "authors": [
        "Jia Wang",
        "Xinfeng Zhang",
        "Gai Zhang",
        "Jun Zhu",
        "Lv Tang",
        "Li Zhang"
      ],
      "abstract": "Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models.",
      "pdf_url": "http://arxiv.org/pdf/2503.02733v1",
      "published": "2025-03-04T15:54:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02733v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Vibration-Assisted Hysteresis Mitigation for Achieving High Compensation Efficiency",
      "authors": [
        "Myeongbo Park",
        "Chunggil An",
        "Junhyun Park",
        "Jonghyun Kang",
        "Minho Hwang"
      ],
      "abstract": "Tendon-sheath mechanisms (TSMs) are widely used in minimally invasive\nsurgical (MIS) applications, but their inherent hysteresis-caused by friction,\nbacklash, and tendon elongation-leads to significant tracking errors.\nConventional modeling and compensation methods struggle with these\nnonlinearities and require extensive parameter tuning. To address this, we\npropose a vibration-assisted hysteresis compensation approach, where controlled\nvibrational motion is applied along the tendon's movement direction to mitigate\nfriction and reduce dead zones. Experimental results demonstrate that the\nexerted vibration consistently reduces hysteresis across all tested\nfrequencies, decreasing RMSE by up to 23.41% (from 2.2345 mm to 1.7113 mm) and\nimproving correlation, leading to more accurate trajectory tracking. When\ncombined with a Temporal Convolutional Network (TCN)-based compensation model,\nvibration further enhances performance, achieving an 85.2% reduction in MAE\n(from 1.334 mm to 0.1969 mm). Without vibration, the TCN-based approach still\nreduces MAE by 72.3% (from 1.334 mm to 0.370 mm) under the same parameter\nsettings. These findings confirm that vibration effectively mitigates\nhysteresis, improving trajectory accuracy and enabling more efficient\ncompensation models with fewer trainable parameters. This approach provides a\nscalable and practical solution for TSM-based robotic applications,\nparticularly in MIS.",
      "pdf_url": "http://arxiv.org/pdf/2503.02720v1",
      "published": "2025-03-04T15:36:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02720v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Generative Tools for Graphical Assets: Empirical Guidelines based on Game Designers' and Developers' Preferences",
      "authors": [
        "Kaisei Fukaya",
        "Damon Daylamani-Zad",
        "Harry Agius"
      ],
      "abstract": "Graphical assets play an important role in the design and development of\ngames. There is potential in the use of generative tools, to aid in creating\ngraphical assets, thus improving game design and development pipelines.\nHowever, there is little research to address how the generative methods can fit\ninto the wider pipeline. We conducted a user study with 16 game designers and\ndevelopers to examine their preferences regarding generative tools for\ngraphical assets. The findings highlight that early design stage is preferred\nby all participants (mean values above 0.67 and p < .001 for early stages).\nDesigners and developers prefer to use such tools for creating large amounts of\nvariations at the cost of quality as they can improve the quality of the\nartefacts once they generate a suitable asset (mean value 0.17 where 1 is high\nquality, p < .001). They also strongly (mean value .78, p < .001) raised the\nneed for better integration of such tools in existing design and development\nenvironments and the need for the outputs to be in common data formats, to be\nmanipulatable and integrate smoothly into existing environments (mean 3.5 out\nof 5, p = .004). The study also highlights the requirement for further emphasis\non the needs of the users to incorporate these tools effectively in existing\npipelines. Informed by these results, we provide a set of guidelines for\ncreating tools that meet the expectations and needs of game designers and\ndevelopers.",
      "pdf_url": "http://arxiv.org/pdf/2503.02703v1",
      "published": "2025-03-04T15:18:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02703v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "MindBridge: Scalable and Cross-Model Knowledge Editing via Memory-Augmented Modality",
      "authors": [
        "Shuaike Li",
        "Kai Zhang",
        "Qi Liu",
        "Enhong Chen"
      ],
      "abstract": "Knowledge editing is a technique for efficiently and accurately updating the\nknowledge of large language models (LLMs) to alleviate obsolescence and correct\nerrors. However, most existing methods overfit to specific models, causing\nedited knowledge to be discarded during each LLM update and requiring frequent\nre-editing, which is particularly burdensome in today's rapidly evolving\nopen-source community. To address this issue, we propose the problem of\ncross-model knowledge editing and introduce MindBridge, a scalable solution\ninspired by the low coupling between modality processing and LLMs in\nmulti-modal models. MindBridge introduces the novel concept of memory modality,\nwhich encodes edited knowledge as an independent modality. It first performs\nLLM-agnostic pre-training of the memory modality and then integrates it with\nvarious LLMs. Extensive experiments on multiple LLMs and popular knowledge\nediting datasets demonstrate that MindBridge achieves superior performance even\nin editing tens of thousands of knowledge entries and can flexibly adapt to\ndifferent LLMs. Our code is available at\nhttps://github.com/CrashBugger/MindBridge.",
      "pdf_url": "http://arxiv.org/pdf/2503.02701v1",
      "published": "2025-03-04T15:17:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02701v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Memory Efficient Continual Learning for Edge-Based Visual Anomaly Detection",
      "authors": [
        "Manuel Barusco",
        "Lorenzo D'Antoni",
        "Davide Dalle Pezze",
        "Francesco Borsatti",
        "Gian Antonio Susto"
      ],
      "abstract": "Visual Anomaly Detection (VAD) is a critical task in computer vision with\nnumerous real-world applications. However, deploying these models on edge\ndevices presents significant challenges, such as constrained computational and\nmemory resources. Additionally, dynamic data distributions in real-world\nsettings necessitate continuous model adaptation, further complicating\ndeployment under limited resources. To address these challenges, we present a\nnovel investigation into the problem of Continual Learning for Visual Anomaly\nDetection (CLAD) on edge devices. We evaluate the STFPM approach, given its low\nmemory footprint on edge devices, which demonstrates good performance when\ncombined with the Replay approach. Furthermore, we propose to study the\nbehavior of a recently proposed approach, PaSTe, specifically designed for the\nedge but not yet explored in the Continual Learning context. Our results show\nthat PaSTe is not only a lighter version of STPFM, but it also achieves\nsuperior anomaly detection performance, improving the f1 pixel performance by\n10% with the Replay technique. In particular, the structure of PaSTe allows us\nto test it using a series of Compressed Replay techniques, reducing memory\noverhead by a maximum of 91.5% compared to the traditional Replay for STFPM.\nOur study proves the feasibility of deploying VAD models that adapt and learn\nincrementally on CLAD scenarios on resource-constrained edge devices.",
      "pdf_url": "http://arxiv.org/pdf/2503.02691v1",
      "published": "2025-03-04T15:03:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02691v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?",
      "authors": [
        "Miao Zhang",
        "Sherif Abdulatif",
        "Benedikt Loesch",
        "Marco Altmann",
        "Bin Yang"
      ],
      "abstract": "Due to the significant effort required for data collection and annotation in\n3D perception tasks, mixed sample data augmentation (MSDA) has been widely\nstudied to generate diverse training samples by mixing existing data. Recently,\nmany MSDA techniques have been developed for point clouds, but they mainly\ntarget LiDAR data, leaving their application to radar point clouds largely\nunexplored. In this paper, we examine the feasibility of applying existing MSDA\nmethods to radar point clouds and identify several challenges in adapting these\ntechniques. These obstacles stem from the radar's irregular angular\ndistribution, deviations from a single-sensor polar layout in multi-radar\nsetups, and point sparsity. To address these issues, we propose Class-Aware\nPillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar\nlevel in 3D point clouds, guided by class labels. Unlike methods that rely a\nsingle mix ratio to the entire sample, CAPMix assigns an independent ratio to\neach pillar, boosting sample diversity. To account for the density of different\nclasses, we use class-specific distributions: for dense objects (e.g., large\nvehicles), we skew ratios to favor points from another sample, while for sparse\nobjects (e.g., pedestrians), we sample more points from the original. This\nclass-aware mixing retains critical details and enriches each sample with new\ninformation, ultimately generating more diverse training data. Experimental\nresults demonstrate that our method not only significantly boosts performance\nbut also outperforms existing MSDA approaches across two datasets (Bosch Street\nand K-Radar). We believe that this straightforward yet effective approach will\nspark further investigation into MSDA techniques for radar data.",
      "pdf_url": "http://arxiv.org/pdf/2503.02687v1",
      "published": "2025-03-04T15:02:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02687v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Seeding for Success: Skill and Stochasticity in Tabletop Games",
      "authors": [
        "James Goodman",
        "Diego Perez-Liebana",
        "Simon Lucas"
      ],
      "abstract": "Games often incorporate random elements in the form of dice or shuffled card\ndecks. This randomness is a key contributor to the player experience and the\nvariety of game situations encountered. There is a tension between a level of\nrandomness that makes the game interesting and contributes to the player\nenjoyment of a game, and a level at which the outcome itself is effectively\nrandom and the game becomes dull. The optimal level for a game will depend on\nthe design goals and target audience. We introduce a new technique to quantify\nthe level of randomness in game outcome and use it to compare 15 tabletop games\nand disentangle the different contributions to the overall randomness from\nspecific parts of some games. We further explore the interaction between game\nrandomness and player skill, and how this innate randomness can affect error\nanalysis in common game experiments.",
      "pdf_url": "http://arxiv.org/pdf/2503.02686v1",
      "published": "2025-03-04T14:58:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02686v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
      "authors": [
        "Weimin Xiong",
        "Yifan Song",
        "Qingxiu Dong",
        "Bingchan Zhao",
        "Feifan Song",
        "Xun Wang",
        "Sujian Li"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2503.02682v1",
      "published": "2025-03-04T14:54:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02682v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "State of play and future directions in industrial computer vision AI standards",
      "authors": [
        "Artemis Stefanidou",
        "Panagiotis Radoglou-Grammatikis",
        "Vasileios Argyriou",
        "Panagiotis Sarigiannidis",
        "Iraklis Varlamis",
        "Georgios Th. Papadopoulos"
      ],
      "abstract": "The recent tremendous advancements in the areas of Artificial Intelligence\n(AI) and Deep Learning (DL) have also resulted into corresponding remarkable\nprogress in the field of Computer Vision (CV), showcasing robust technological\nsolutions in a wide range of application sectors of high industrial interest\n(e.g., healthcare, autonomous driving, automation, etc.). Despite the\noutstanding performance of CV systems in specific domains, their development\nand exploitation at industrial-scale necessitates, among other, the addressing\nof requirements related to the reliability, transparency, trustworthiness,\nsecurity, safety, and robustness of the developed AI models. The latter raises\nthe imperative need for the development of efficient, comprehensive and\nwidely-adopted industrial standards. In this context, this study investigates\nthe current state of play regarding the development of industrial computer\nvision AI standards, emphasizing on critical aspects, like model\ninterpretability, data quality, and regulatory compliance. In particular, a\nsystematic analysis of launched and currently developing CV standards, proposed\nby the main international standardization bodies (e.g. ISO/IEC, IEEE, DIN,\netc.) is performed. The latter is complemented by a comprehensive discussion on\nthe current challenges and future directions observed in this regularization\nendeavor.",
      "pdf_url": "http://arxiv.org/pdf/2503.02675v1",
      "published": "2025-03-04T14:46:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02675v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A dataset-free approach for self-supervised learning of 3D reflectional symmetries",
      "authors": [
        "Issac Aguirre",
        "Ivan Sipiran",
        "Gabriel Montañana"
      ],
      "abstract": "In this paper, we explore a self-supervised model that learns to detect the\nsymmetry of a single object without requiring a dataset-relying solely on the\ninput object itself. We hypothesize that the symmetry of an object can be\ndetermined by its intrinsic features, eliminating the need for large datasets\nduring training. Additionally, we design a self-supervised learning strategy\nthat removes the necessity of ground truth labels. These two key elements make\nour approach both effective and efficient, addressing the prohibitive costs\nassociated with constructing large, labeled datasets for this task. The novelty\nof our method lies in computing features for each point on the object based on\nthe idea that symmetric points should exhibit similar visual appearances. To\nachieve this, we leverage features extracted from a foundational image model to\ncompute a visual descriptor for the points. This approach equips the point\ncloud with visual features that facilitate the optimization of our\nself-supervised model. Experimental results demonstrate that our method\nsurpasses the state-of-the-art models trained on large datasets. Furthermore,\nour model is more efficient, effective, and operates with minimal computational\nand data resources.",
      "pdf_url": "http://arxiv.org/pdf/2503.02660v1",
      "published": "2025-03-04T14:22:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02660v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats",
      "authors": [
        "William Brach",
        "Kristián Košťál",
        "Michal Ries"
      ],
      "abstract": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.",
      "pdf_url": "http://arxiv.org/pdf/2503.02650v1",
      "published": "2025-03-04T14:14:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02650v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "YARE-GAN: Yet Another Resting State EEG-GAN",
      "authors": [
        "Yeganeh Farahzadi",
        "Morteza Ansarinia",
        "Zoltan Kekecs"
      ],
      "abstract": "Generative Adversarial Networks (GANs) have shown promise in synthesising\nrealistic neural data, yet their potential for unsupervised representation\nlearning in resting-state EEG remains under explored. In this study, we\nimplement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generate\nmulti-channel resting-state EEG data and assess the quality of the synthesised\nsignals through both visual and feature-based evaluations. Our results indicate\nthat the model effectively captures the statistical and spectral\ncharacteristics of real EEG data, although challenges remain in replicating\nhigh-frequency oscillations in the frontal region. Additionally, we demonstrate\nthat the Critic's learned representations can be fine-tuned for age group\nclassification, achieving an out-of-sample accuracy, significantly better than\na shuffled-label baseline. These findings suggest that generative models can\nserve not only as EEG data generators but also as unsupervised feature\nextractors, reducing the need for manual feature engineering. This study\nhighlights the potential of GAN-based unsupervised learning for EEG analysis,\nsuggesting avenues for more data-efficient deep learning applications in\nneuroscience.",
      "pdf_url": "http://arxiv.org/pdf/2503.02636v1",
      "published": "2025-03-04T14:01:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02636v1",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ]
    },
    {
      "title": "Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective",
      "authors": [
        "Haotian Li",
        "Yun Wang",
        "Huamin Qu"
      ],
      "abstract": "Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the barrier of expertise and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify persistent collaboration patterns, e.g., human-creator +\nAI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The\nbenefits of these AI techniques and other implications to human-AI\ncollaboration are also revealed. We further propose future directions to\nhopefully ignite innovations.",
      "pdf_url": "http://arxiv.org/pdf/2503.02631v1",
      "published": "2025-03-04T13:56:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02631v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction",
      "authors": [
        "Wenxuan Liu",
        "Zixuan Li",
        "Long Bai",
        "Yuxin Zuo",
        "Daozhu Xu",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2503.02628v1",
      "published": "2025-03-04T13:53:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02628v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence Calibration of Large Language Models",
      "authors": [
        "Paul Stangel",
        "David Bani-Harouni",
        "Chantal Pellegrini",
        "Ege Özsoy",
        "Kamilia Zaripova",
        "Matthias Keicher",
        "Nassir Navab"
      ],
      "abstract": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2503.02623v1",
      "published": "2025-03-04T13:48:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02623v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Reinforcement Learning-based Threat Assessment",
      "authors": [
        "Wuzhou Sun",
        "Siyi Li",
        "Qingxiang Zou",
        "Zixing Liao"
      ],
      "abstract": "In some game scenarios, due to the uncertainty of the number of enemy units\nand the priority of various attributes, the evaluation of the threat level of\nenemy units as well as the screening has been a challenging research topic, and\nthe core difficulty lies in how to reasonably set the priority of different\nattributes in order to achieve quantitative evaluation of the threat. In this\npaper, we innovatively transform the problem of threat assessment into a\nreinforcement learning problem, and through systematic reinforcement learning\ntraining, we successfully construct an efficient neural network evaluator. The\nevaluator can not only comprehensively integrate the multidimensional attribute\nfeatures of the enemy, but also effectively combine our state information, thus\nrealizing a more accurate and scientific threat assessment.",
      "pdf_url": "http://arxiv.org/pdf/2503.02612v1",
      "published": "2025-03-04T13:32:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02612v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs",
      "authors": [
        "Wei-Yao Wang",
        "Zhao Wang",
        "Helen Suzuki",
        "Yoshiyuki Kobayashi"
      ],
      "abstract": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of earlier\nmodalities (e.g., images) to incorporate information from later modalities\n(e.g., text). To address this problem, we propose AKI, a novel MLLM that\nunlocks causal attention into modality-mutual attention (MMA) to enable image\ntokens to attend to text tokens. This simple yet effective design allows AKI to\nachieve superior performance in 12 multimodal understanding benchmarks (+7.2%\non average) without introducing additional parameters and increasing training\ntime. Our MMA design is intended to be generic, allowing for application across\nvarious modalities, and scalable to accommodate diverse multimodal scenarios.\nThe code is publicly available at https://github.com/sony/aki, and we will\nrelease our AKI-4B model to encourage further advancements in MLLMs across\nvarious directions.",
      "pdf_url": "http://arxiv.org/pdf/2503.02597v1",
      "published": "2025-03-04T13:18:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02597v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts",
      "authors": [
        "Zhaoxing Gan",
        "Mengtian Li",
        "Ruhua Chen",
        "Zhongxia Ji",
        "Sichen Guo",
        "Huanling Hu",
        "Guangnan Ye",
        "Zuo Hu"
      ],
      "abstract": "In this work, we introduce StageDesigner, the first comprehensive framework\nfor artistic stage generation using large language models combined with\nlayout-controlled diffusion models. Given the professional requirements of\nstage scenography, StageDesigner simulates the workflows of seasoned artists to\ngenerate immersive 3D stage scenes. Specifically, our approach is divided into\nthree primary modules: Script Analysis, which extracts thematic and spatial\ncues from input scripts; Foreground Generation, which constructs and arranges\nessential 3D objects; and Background Generation, which produces a harmonious\nbackground aligned with the narrative atmosphere and maintains spatial\ncoherence by managing occlusions between foreground and background elements.\nFurthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276\nunique stage scenes spanning different historical styles and annotated with\nscripts, images, and detailed 3D layouts, specifically tailored for this task.\nFinally, evaluations using both standard and newly proposed metrics, along with\nextensive user studies, demonstrate the effectiveness of StageDesigner. Project\ncan be found at: https://deadsmither5.github.io/2025/01/03/StageDesigner/",
      "pdf_url": "http://arxiv.org/pdf/2503.02595v1",
      "published": "2025-03-04T13:17:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02595v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Playing games with Large language models: Randomness and strategy",
      "authors": [
        "Alicia Vidler",
        "Toby Walsh"
      ],
      "abstract": "Playing games has a long history of describing intricate interactions in\nsimplified forms. In this paper we explore if large language models (LLMs) can\nplay games, investigating their capabilities for randomisation and strategic\nadaptation through both simultaneous and sequential game interactions. We focus\non GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors\n(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as\nstochastic parrots, and while they may indeed be parrots, our results suggest\nthat they are not very stochastic in the sense that their outputs - when\nprompted to be random - are often very biased. Our research reveals that LLMs\nappear to develop loss aversion strategies in repeated games, with RPS\nconverging to stalemate conditions while PD shows systematic shifts between\ncooperative and competitive outcomes based on prompt design. We detail\nprogrammatic tools for independent agent interactions and the Agentic AI\nchallenges faced in implementation. We show that LLMs can indeed play games,\njust not very well. These results have implications for the use of LLMs in\nmulti-agent LLM systems and showcase limitations in current approaches to model\noutput for strategic decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2503.02582v1",
      "published": "2025-03-04T13:04:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02582v1",
      "categories": [
        "cs.AI",
        "cs.GT"
      ]
    },
    {
      "title": "LLM-Safety Evaluations Lack Robustness",
      "authors": [
        "Tim Beyer",
        "Sophie Xhonneux",
        "Simon Geisler",
        "Gauthier Gidel",
        "Leo Schwinn",
        "Stephan Günnemann"
      ],
      "abstract": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress.",
      "pdf_url": "http://arxiv.org/pdf/2503.02574v1",
      "published": "2025-03-04T12:55:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02574v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour",
      "authors": [
        "Valerii Serpiva",
        "Artem Lykov",
        "Artyom Myshlyaev",
        "Muhammad Haris Khan",
        "Ali Alridha Abdulkarim",
        "Oleg Sautenkov",
        "Dzmitry Tsetserukou"
      ],
      "abstract": "RaceVLA presents an innovative approach for autonomous racing drone\nnavigation by leveraging Visual-Language-Action (VLA) to emulate human-like\nbehavior. This research explores the integration of advanced algorithms that\nenable drones to adapt their navigation strategies based on real-time\nenvironmental feedback, mimicking the decision-making processes of human\npilots. The model, fine-tuned on a collected racing drone dataset, demonstrates\nstrong generalization despite the complexity of drone racing environments.\nRaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic\ngeneralization (45.5 vs 36.3), benefiting from the dynamic camera and\nsimplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs\n76.7) generalization were slightly reduced due to the challenges of maneuvering\nin dynamic environments with varying object sizes. RaceVLA also outperforms\nRT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical\n(50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for\nreal-time adjustments in complex environments. Experiments revealed an average\nvelocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent\nmaneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios\neffectively. These findings highlight the potential of RaceVLA for\nhigh-performance navigation in competitive racing contexts. The RaceVLA\ncodebase, pretrained weights, and dataset are available at this http URL:\nhttps://racevla.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.02572v1",
      "published": "2025-03-04T12:54:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02572v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "World Models for Anomaly Detection during Model-Based Reinforcement Learning Inference",
      "authors": [
        "Fabian Domberg",
        "Georg Schildbach"
      ],
      "abstract": "Learning-based controllers are often purposefully kept out of real-world\napplications due to concerns about their safety and reliability. We explore how\nstate-of-the-art world models in Model-Based Reinforcement Learning can be\nutilized beyond the training phase to ensure a deployed policy only operates\nwithin regions of the state-space it is sufficiently familiar with. This is\nachieved by continuously monitoring discrepancies between a world model's\npredictions and observed system behavior during inference. It allows for\ntriggering appropriate measures, such as an emergency stop, once an error\nthreshold is surpassed. This does not require any task-specific knowledge and\nis thus universally applicable. Simulated experiments on established robot\ncontrol tasks show the effectiveness of this method, recognizing changes in\nlocal robot geometry and global gravitational magnitude. Real-world experiments\nusing an agile quadcopter further demonstrate the benefits of this approach by\ndetecting unexpected forces acting on the vehicle. These results indicate how\neven in new and adverse conditions, safe and reliable operation of otherwise\nunpredictable learning-based controllers can be achieved.",
      "pdf_url": "http://arxiv.org/pdf/2503.02552v1",
      "published": "2025-03-04T12:25:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02552v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
      "authors": [
        "Grzegorz Skorupko",
        "Fotios Avgoustidis",
        "Carlos Martín-Isla",
        "Lidia Garrucho",
        "Dimitri A. Kessler",
        "Esmeralda Ruiz Pujadas",
        "Oliver Díaz",
        "Maciej Bobowicz",
        "Katarzyna Gwoździewicz",
        "Xavier Bargalló",
        "Paulius Jaruševičius",
        "Kaisar Kushibar",
        "Karim Lekadir"
      ],
      "abstract": "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet .",
      "pdf_url": "http://arxiv.org/pdf/2503.02549v1",
      "published": "2025-03-04T12:20:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02549v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification",
      "authors": [
        "Zhen Yang",
        "Guibao Shen",
        "Liang Hou",
        "Mushui Liu",
        "Luozhou Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Di Zhang",
        "Ying-Cong Chen"
      ],
      "abstract": "Diffusion models have achieved remarkable advances in various image\ngeneration tasks. However, their performance notably declines when generating\nimages at resolutions higher than those used during the training period.\nDespite the existence of numerous methods for producing high-resolution images,\nthey either suffer from inefficiency or are hindered by complex operations. In\nthis paper, we propose RectifiedHR, an efficient and straightforward solution\nfor training-free high-resolution image generation. Specifically, we introduce\nthe noise refresh strategy, which theoretically only requires a few lines of\ncode to unlock the model's high-resolution generation ability and improve\nefficiency. Additionally, we first observe the phenomenon of energy decay that\nmay cause image blurriness during the high-resolution image generation process.\nTo address this issue, we propose an Energy Rectification strategy, where\nmodifying the hyperparameters of the classifier-free guidance effectively\nimproves the generation performance. Our method is entirely training-free and\nboasts a simple implementation logic. Through extensive comparisons with\nnumerous baseline methods, our RectifiedHR demonstrates superior effectiveness\nand efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2503.02537v1",
      "published": "2025-03-04T12:03:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02537v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "LTL Verification of Memoryful Neural Agents",
      "authors": [
        "Mehran Hosseini",
        "Alessio Lomuscio",
        "Nicola Paoletti"
      ],
      "abstract": "We present a framework for verifying Memoryful Neural Multi-Agent Systems\n(MN-MAS) against full Linear Temporal Logic (LTL) specifications. In MN-MAS,\nagents interact with a non-deterministic, partially observable environment.\nExamples of MN-MAS include multi-agent systems based on feed-forward and\nrecurrent neural networks or state-space models. Different from previous\napproaches, we support the verification of both bounded and unbounded LTL\nspecifications. We leverage well-established bounded model checking techniques,\nincluding lasso search and invariant synthesis, to reduce the verification\nproblem to that of constraint solving. To solve these constraints, we develop\nefficient methods based on bound propagation, mixed-integer linear programming,\nand adaptive splitting. We evaluate the effectiveness of our algorithms in\nsingle and multi-agent environments from the Gymnasium and PettingZoo\nlibraries, verifying unbounded specifications for the first time and improving\nthe verification time for bounded specifications by an order of magnitude\ncompared to the SoA.",
      "pdf_url": "http://arxiv.org/pdf/2503.02512v1",
      "published": "2025-03-04T11:20:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02512v1",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SC",
        "68Q60 (Primary) 68T27, 68T07, 68T37, 68T40, 68T42 (Secondary)",
        "D.2.4; F.3.1; I.2.4; I.2.11; I.2.8; F.4.1; I.2.2; I.2.3"
      ]
    },
    {
      "title": "ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment",
      "authors": [
        "Shaofei Cai",
        "Zhancun Mu",
        "Anji Liu",
        "Yitao Liang"
      ],
      "abstract": "We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.",
      "pdf_url": "http://arxiv.org/pdf/2503.02505v1",
      "published": "2025-03-04T11:16:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02505v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel PennyLane-Centric Dataset",
      "authors": [
        "Haider Asif",
        "Abdul Basit",
        "Nouhaila Innan",
        "Muhammad Kashif",
        "Alberto Marchisio",
        "Muhammad Shafique"
      ],
      "abstract": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning. Their\npotential in aiding quantum software development remains underexplored,\nparticularly for the PennyLane framework-a leading platform for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific code samples of\nquantum circuits and their contextual descriptions, specifically curated to\ntrain/fine-tune LLM-based quantum code assistance. Our key contributions are\nthreefold: (1) the automatic creation and open-source release of a\ncomprehensive PennyLane dataset leveraging quantum computing textbooks,\nofficial documentation, and open-source repositories; (2) the development of a\nsystematic methodology for data refinement, annotation, and formatting to\noptimize LLM training efficiency; and (3) a thorough evaluation, based on a\nRetrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness\nof our dataset in streamlining PennyLane code generation and improving quantum\ndevelopment workflows. Compared to existing efforts that predominantly focus on\nQiskit, our dataset significantly broadens the spectrum of quantum frameworks\ncovered in AI-driven code assistance. By bridging this gap and providing\nreproducible dataset-creation methodologies, we aim to advance the field of\nAI-assisted quantum programming, making quantum computing more accessible to\nboth newcomers and experienced developers.",
      "pdf_url": "http://arxiv.org/pdf/2503.02497v1",
      "published": "2025-03-04T11:04:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02497v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "quant-ph",
        "68T50 (Primary)",
        "I.2.7"
      ]
    },
    {
      "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
      "authors": [
        "Yujiao Yang",
        "Jing Lian",
        "Linhui Li"
      ],
      "abstract": "Mixture-of-Experts (MoE) enhances model performance while maintaining\ncomputational efficiency, making it well-suited for large-scale applications.\nHowever, expert in exist MoE paradigm works as an individual, thereby lacking\nhigh-quality expert interactions. Moreover, they have not been effectively\nextended to attention block, which constrains further efficiency improvements.\nTo tackle these issues, we propose Union-of-Experts (UoE), which decomposes\ntransformer into an equitant group of experts, and then implement dynamic\nrouting on input data and experts. Our approach advances MoE design with three\nkey innovations: (1) We conducted equitant expert decomposition on both MLP\nblocks and attention blocks based on matrix partition in tensor parallelism.\n(2) We developed two routing paradigms: patch wise data selection and expert\nselection, to apply routing across different levels. (3) We design the\narchitecture of UoE model, including Selective Multi-Head Attention (SMHA) and\nUnion-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's\nrouting and computation operation, and optimize efficiency based on the\nhardware processing analysis. The experiments demonstrate that the model\nemployed with UoE surpass Full Attention, state-of-art MoEs and efficient\ntransformers in several tasks across image and natural language domains. The\nsource codes are available at https://github.com/YujiaoYang-work/UoE.",
      "pdf_url": "http://arxiv.org/pdf/2503.02495v1",
      "published": "2025-03-04T11:01:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02495v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T07",
        "I.5.1; I.2.0"
      ]
    },
    {
      "title": "ERetinex: Event Camera Meets Retinex Theory for Low-Light Image Enhancement",
      "authors": [
        "Xuejian Guo",
        "Zhiqiang Tian",
        "Yuehang Wang",
        "Siqi Li",
        "Yu Jiang",
        "Shaoyi Du",
        "Yue Gao"
      ],
      "abstract": "Low-light image enhancement aims to restore the under-exposure image captured\nin dark scenarios. Under such scenarios, traditional frame-based cameras may\nfail to capture the structure and color information due to the exposure time\nlimitation. Event cameras are bio-inspired vision sensors that respond to\npixel-wise brightness changes asynchronously. Event cameras' high dynamic range\nis pivotal for visual perception in extreme low-light scenarios, surpassing\ntraditional cameras and enabling applications in challenging dark environments.\nIn this paper, inspired by the success of the retinex theory for traditional\nframe-based low-light image restoration, we introduce the first methods that\ncombine the retinex theory with event cameras and propose a novel retinex-based\nlow-light image restoration framework named ERetinex. Among our contributions,\nthe first is developing a new approach that leverages the high temporal\nresolution data from event cameras with traditional image information to\nestimate scene illumination accurately. This method outperforms traditional\nimage-only techniques, especially in low-light environments, by providing more\nprecise lighting information. Additionally, we propose an effective fusion\nstrategy that combines the high dynamic range data from event cameras with the\ncolor information of traditional images to enhance image quality. Through this\nfusion, we can generate clearer and more detail-rich images, maintaining the\nintegrity of visual information even under extreme lighting conditions. The\nexperimental results indicate that our proposed method outperforms\nstate-of-the-art (SOTA) methods, achieving a gain of 1.0613 dB in PSNR while\nreducing FLOPS by \\textbf{84.28}\\%.",
      "pdf_url": "http://arxiv.org/pdf/2503.02484v1",
      "published": "2025-03-04T10:48:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02484v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for Biomedical VQA",
      "authors": [
        "Zhengyang Ji",
        "Shang Gao",
        "Li Liu",
        "Yifan Jia",
        "Yutao Yue"
      ],
      "abstract": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.",
      "pdf_url": "http://arxiv.org/pdf/2503.02476v1",
      "published": "2025-03-04T10:39:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02476v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Don't Get Too Excited -- Eliciting Emotions in LLMs",
      "authors": [
        "Gino Franco Fazzi",
        "Julie Skoven Hinge",
        "Stefan Heinrich",
        "Paolo Burelli"
      ],
      "abstract": "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2503.02457v1",
      "published": "2025-03-04T10:06:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.02457v1",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}