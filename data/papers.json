{
  "last_updated": "2025-02-21T00:44:32.899047",
  "papers": [
    {
      "title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
      "authors": [
        "Michael Luo",
        "Xiaoxiang Shi",
        "Colin Cai",
        "Tianjun Zhang",
        "Justin Wong",
        "Yichuan Wang",
        "Chi Wang",
        "Yanping Huang",
        "Zhifeng Chen",
        "Joseph E. Gonzalez",
        "Ion Stoica"
      ],
      "abstract": "Large language model (LLM) applications are evolving beyond simple chatbots\ninto dynamic, general-purpose agentic programs, which scale LLM calls and\noutput tokens to help AI agents reason, explore, and solve complex tasks.\nHowever, existing LLM serving systems ignore dependencies between programs and\ncalls, missing significant opportunities for optimization. Our analysis reveals\nthat programs submitted to LLM serving engines experience long cumulative wait\ntimes, primarily due to head-of-line blocking at both the individual LLM\nrequest and the program. To address this, we introduce Autellix, an LLM serving\nsystem that treats programs as first-class citizens to minimize their\nend-to-end latencies. Autellix intercepts LLM calls submitted by programs,\nenriching schedulers with program-level context. We propose two scheduling\nalgorithms-for single-threaded and distributed programs-that preempt and\nprioritize LLM calls based on their programs' previously completed calls. Our\nevaluation demonstrates that across diverse LLMs and agentic workloads,\nAutellix improves throughput of programs by 4-15x at the same latency compared\nto state-of-the-art systems, such as vLLM.",
      "pdf_url": "http://arxiv.org/pdf/2502.13965v1",
      "published": "2025-02-19T18:59:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13965v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "A Training-Free Framework for Precise Mobile Manipulation of Small Everyday Objects",
      "authors": [
        "Arjun Gupta",
        "Rishik Sathua",
        "Saurabh Gupta"
      ],
      "abstract": "Many everyday mobile manipulation tasks require precise interaction with\nsmall objects, such as grasping a knob to open a cabinet or pressing a light\nswitch. In this paper, we develop Servoing with Vision Models (SVM), a\nclosed-loop training-free framework that enables a mobile manipulator to tackle\nsuch precise tasks involving the manipulation of small objects. SVM employs an\nRGB-D wrist camera and uses visual servoing for control. Our novelty lies in\nthe use of state-of-the-art vision models to reliably compute 3D targets from\nthe wrist image for diverse tasks and under occlusion due to the end-effector.\nTo mitigate occlusion artifacts, we employ vision models to out-paint the\nend-effector thereby significantly enhancing target localization. We\ndemonstrate that aided by out-painting methods, open-vocabulary object\ndetectors can serve as a drop-in module to identify semantic targets (e.g.\nknobs) and point tracking methods can reliably track interaction sites\nindicated by user clicks. This training-free method obtains an 85% zero-shot\nsuccess rate on manipulating unseen objects in novel environments in the real\nworld, outperforming an open-loop control method and an imitation learning\nbaseline trained on 1000+ demonstrations by an absolute success rate of 50%.",
      "pdf_url": "http://arxiv.org/pdf/2502.13964v1",
      "published": "2025-02-19T18:59:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13964v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision",
      "authors": [
        "Guangzhi Xiong",
        "Qiao Jin",
        "Xiao Wang",
        "Yin Fang",
        "Haolin Liu",
        "Yifan Yang",
        "Fangyuan Chen",
        "Zhixing Song",
        "Dengyu Wang",
        "Minjia Zhang",
        "Zhiyong Lu",
        "Aidong Zhang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has shown great potential for\nknowledge-intensive tasks, but its traditional architectures rely on static\nretrieval, limiting their effectiveness for complex questions that require\nsequential information-seeking. While agentic reasoning and search offer a more\nadaptive approach, most existing methods depend heavily on prompt engineering.\nIn this work, we introduce RAG-Gym, a unified optimization framework that\nenhances information-seeking agents through fine-grained process supervision at\neach search step. We also propose ReSearch, a novel agent architecture that\nsynergizes answer reasoning and search query generation within the RAG-Gym\nframework. Experiments on four challenging datasets show that RAG-Gym improves\nperformance by up to 25.6\\% across various agent architectures, with ReSearch\nconsistently outperforming existing baselines. Further analysis highlights the\neffectiveness of advanced LLMs as process reward judges and the transferability\nof trained reward models as verifiers for different LLMs. Additionally, we\nexamine the scaling properties of training and inference in agentic RAG. The\nproject homepage is available at https://rag-gym.github.io/.",
      "pdf_url": "http://arxiv.org/pdf/2502.13957v1",
      "published": "2025-02-19T18:56:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13957v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Neurosymbolic artificial intelligence via large language models and coherence-driven inference",
      "authors": [
        "Steve Huntsman",
        "Jewell Thomas"
      ],
      "abstract": "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
      "pdf_url": "http://arxiv.org/pdf/2502.13953v1",
      "published": "2025-02-19T18:53:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13953v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region",
      "authors": [
        "Chak Tou Leong",
        "Qingyu Yin",
        "Jian Wang",
        "Wenjie Li"
      ],
      "abstract": "The safety alignment of large language models (LLMs) remains vulnerable, as\ntheir initial behavior can be easily jailbroken by even relatively simple\nattacks. Since infilling a fixed template between the input instruction and\ninitial model output is a common practice for existing LLMs, we hypothesize\nthat this template is a key factor behind their vulnerabilities: LLMs'\nsafety-related decision-making overly relies on the aggregated information from\nthe template region, which largely influences these models' safety behavior. We\nrefer to this issue as template-anchored safety alignment. In this paper, we\nconduct extensive experiments and verify that template-anchored safety\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\ndemonstrate how it leads to models' susceptibility when encountering\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\nmechanisms from the template region is promising in mitigating vulnerabilities\nto jailbreak attacks. We encourage future research to develop more robust\nsafety alignment techniques that reduce reliance on the template region.",
      "pdf_url": "http://arxiv.org/pdf/2502.13946v1",
      "published": "2025-02-19T18:42:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13946v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence",
      "authors": [
        "Yuliang Liu",
        "Junjie Lu",
        "Zhaoling Chen",
        "Chaofeng Qu",
        "Jason Klein Liu",
        "Chonghan Liu",
        "Zefan Cai",
        "Yunhui Xia",
        "Li Zhao",
        "Jiang Bian",
        "Chuheng Zhang",
        "Wei Shen",
        "Zhouhan Lin"
      ],
      "abstract": "Current approaches for training Process Reward Models (PRMs) often involve\nbreaking down responses into multiple reasoning steps using rule-based\ntechniques, such as using predefined placeholder tokens or setting the\nreasoning step's length into a fixed size. These approaches overlook the fact\nthat specific words do not typically mark true decision points in a text. To\naddress this, we propose AdaptiveStep, a method that divides reasoning steps\nbased on the model's confidence in predicting the next word. This division\nmethod provides more decision-making information at each step, enhancing\ndownstream tasks, such as reward model learning. Moreover, our method does not\nrequire manual annotation. We demonstrate its effectiveness through experiments\nwith AdaptiveStep-trained PRMs in mathematical reasoning and code generation\ntasks. Experimental results indicate that the outcome PRM achieves\nstate-of-the-art Best-of-N performance, surpassing greedy search strategy with\ntoken-level value-guided decoding, while also reducing construction costs by\nover 30% compared to existing open-source PRMs. In addition, we provide a\nthorough analysis and case study on the PRM's performance, transferability, and\ngeneralization capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2502.13943v1",
      "published": "2025-02-19T18:35:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13943v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Continually Learning Structured Visual Representations via Network Refinement with Rerelation",
      "authors": [
        "Zeki Doruk Erden",
        "Boi Faltings"
      ],
      "abstract": "Current machine learning paradigm relies on continuous representations like\nneural networks, which iteratively adjust parameters to approximate outcomes\nrather than directly learning the structure of problem. This spreads\ninformation across the network, causing issues like information loss and\nincomprehensibility Building on prior work in environment dynamics modeling, we\npropose a method that learns visual space in a structured, continual manner.\nOur approach refines networks to capture the core structure of objects while\nrepresenting significant subvariants in structure efficiently. We demonstrate\nthis with 2D shape detection, showing incremental learning on MNIST without\noverwriting knowledge and creating compact, comprehensible representations.\nThese results offer a promising step toward a transparent, continually learning\nalternative to traditional neural networks for visual processing.",
      "pdf_url": "http://arxiv.org/pdf/2502.13935v1",
      "published": "2025-02-19T18:18:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13935v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images",
      "authors": [
        "Shengguang Wu",
        "Fan-Yun Sun",
        "Kaiyue Wen",
        "Nick Haber"
      ],
      "abstract": "Recent studies have shown that Large Vision-Language Models (VLMs) tend to\nneglect image content and over-rely on language-model priors, resulting in\nerrors in visually grounded tasks and hallucinations. We hypothesize that this\nissue arises because existing VLMs are not explicitly trained to generate texts\nthat are accurately grounded in fine-grained image details. To enhance visual\nfeedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive\nOptimization), a novel finetuning objective that steers the model toward\ncapturing important visual details and aligning them with corresponding text\ntokens. To further facilitate this detailed alignment, we introduce MVC, a\npaired image-text dataset built by automatically filtering and augmenting\nvisual counterfactual data to challenge the model with hard contrastive cases\ninvolving Minimal Visual Contrasts. Experiments show that our method\nconsistently improves VLM performance across diverse benchmarks covering\nvarious abilities and domains, achieving up to a 22% reduction in\nhallucinations, and significant gains in vision-centric and general tasks.\nNotably, these improvements become increasingly pronounced in benchmarks with\nhigher visual dependency. In short, S-VCO offers a significant enhancement of\nVLM's visually-dependent task performance while retaining or even improving the\nmodel's general abilities. We opensource our code at https://s-vco.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2502.13928v1",
      "published": "2025-02-19T18:05:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13928v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "How Do LLMs Perform Two-Hop Reasoning in Context?",
      "authors": [
        "Tianyu Guo",
        "Hanlin Zhu",
        "Ruiqi Zhang",
        "Jiantao Jiao",
        "Song Mei",
        "Michael I. Jordan",
        "Stuart Russell"
      ],
      "abstract": "\"Socrates is human. All humans are mortal. Therefore, Socrates is mortal.\"\nThis classical example demonstrates two-hop reasoning, where a conclusion\nlogically follows from two connected premises. While transformer-based Large\nLanguage Models (LLMs) can make two-hop reasoning, they tend to collapse to\nrandom guessing when faced with distracting premises. To understand the\nunderlying mechanism, we train a three-layer transformer on synthetic two-hop\nreasoning tasks. The training dynamics show two stages: a slow learning phase,\nwhere the 3-layer transformer performs random guessing like LLMs, followed by\nan abrupt phase transitions, where the 3-layer transformer suddenly reaches\n$100%$ accuracy. Through reverse engineering, we explain the inner mechanisms\nfor how models learn to randomly guess between distractions initially, and how\nthey learn to ignore distractions eventually. We further propose a\nthree-parameter model that supports the causal claims for the mechanisms to the\ntraining dynamics of the transformer. Finally, experiments on LLMs suggest that\nthe discovered mechanisms generalize across scales. Our methodologies provide\nnew perspectives for scientific understandings of LLMs and our findings provide\nnew insights into how reasoning emerges during training.",
      "pdf_url": "http://arxiv.org/pdf/2502.13913v1",
      "published": "2025-02-19T17:46:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13913v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Lost in Sequence: Do Large Language Models Understand Sequential Recommendation?",
      "authors": [
        "Sein Kim",
        "Hongseok Kang",
        "Kibum Kim",
        "Jiwan Kim",
        "Donghyun Kim",
        "Minchul Yang",
        "Kwangjin Oh",
        "Julian McAuley",
        "Chanyoung Park"
      ],
      "abstract": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.",
      "pdf_url": "http://arxiv.org/pdf/2502.13909v1",
      "published": "2025-02-19T17:41:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13909v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Partially Observable Gaussian Process Network and Doubly Stochastic Variational Inference",
      "authors": [
        "Saksham Kiroriwal",
        "Julius Pfrommer",
        "Jürgen Beyerer"
      ],
      "abstract": "To reduce the curse of dimensionality for Gaussian processes (GP), they can\nbe decomposed into a Gaussian Process Network (GPN) of coupled subprocesses\nwith lower dimensionality. In some cases, intermediate observations are\navailable within the GPN. However, intermediate observations are often\nindirect, noisy, and incomplete in most real-world systems. This work\nintroduces the Partially Observable Gaussian Process Network (POGPN) to model\nreal-world process networks. We model a joint distribution of latent functions\nof subprocesses and make inferences using observations from all subprocesses.\nPOGPN incorporates observation lenses (observation likelihoods) into the\nwell-established inference method of deep Gaussian processes. We also introduce\ntwo training methods for POPGN to make inferences on the whole network using\nnode observations. The application to benchmark problems demonstrates how\nincorporating partial observations during training and inference can improve\nthe predictive performance of the overall network, offering a promising outlook\nfor its practical application.",
      "pdf_url": "http://arxiv.org/pdf/2502.13905v1",
      "published": "2025-02-19T17:39:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13905v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "DataSciBench: An LLM Agent Benchmark for Data Science",
      "authors": [
        "Dan Zhang",
        "Sining Zhoubian",
        "Min Cai",
        "Fengzu Li",
        "Lekang Yang",
        "Wei Wang",
        "Tianjiao Dong",
        "Ziniu Hu",
        "Jie Tang",
        "Yisong Yue"
      ],
      "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating\nLarge Language Model (LLM) capabilities in data science. Recent related\nbenchmarks have primarily focused on single tasks, easily obtainable ground\ntruth, and straightforward evaluation metrics, which limits the scope of tasks\nthat can be evaluated. In contrast, DataSciBench is constructed based on a more\ncomprehensive and curated collection of natural and challenging prompts for\nuncertain ground truth and evaluation metrics. We develop a semi-automated\npipeline for generating ground truth (GT) and validating evaluation metrics.\nThis pipeline utilizes and implements an LLM-based self-consistency and human\nverification strategy to produce accurate GT by leveraging collected prompts,\npredefined task types, and aggregate functions (metrics). Furthermore, we\npropose an innovative Task - Function - Code (TFC) framework to assess each\ncode execution outcome based on precisely defined metrics and programmatic\nrules. Our experimental framework involves testing 6 API-based models, 8\nopen-source general models, and 9 open-source code generation models using the\ndiverse set of prompts we have gathered. This approach aims to provide a more\ncomprehensive and rigorous evaluation of LLMs in data science, revealing their\nstrengths and weaknesses. Experimental results demonstrate that API-based\nmodels outperform open-sourced models on all metrics and\nDeepseek-Coder-33B-Instruct achieves the highest score among open-sourced\nmodels. We release all code and data at https://github.com/THUDM/DataSciBench.",
      "pdf_url": "http://arxiv.org/pdf/2502.13897v1",
      "published": "2025-02-19T17:31:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13897v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "PSCon: Toward Conversational Product Search",
      "authors": [
        "Jie Zou",
        "Mohammad Aliannejadi",
        "Evangelos Kanoulas",
        "Shuxi Han",
        "Heli Ma",
        "Zheng Wang",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "abstract": "Conversational Product Search (CPS) is confined to simulated conversations\ndue to the lack of real-world CPS datasets that reflect human-like language.\nAdditionally, current conversational datasets are limited to support\ncross-market and multi-lingual usage. In this paper, we introduce a new CPS\ndata collection protocol and present PSCon, a novel CPS dataset designed to\nassist product search via human-like conversations. The dataset is constructed\nusing a coached human-to-human data collection protocol and supports two\nlanguages and dual markets. Also, the dataset enables thorough exploration of\nsix subtasks of CPS: user intent detection, keyword extraction, system action\nprediction, question selection, item ranking, and response generation.\nFurthermore, we also offer an analysis of the dataset and propose a benchmark\nmodel on the proposed CPS dataset.",
      "pdf_url": "http://arxiv.org/pdf/2502.13881v1",
      "published": "2025-02-19T17:05:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13881v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "MEX: Memory-efficient Approach to Referring Multi-Object Tracking",
      "authors": [
        "Huu-Thien Tran",
        "Phuoc-Sang Pham",
        "Thai-Son Tran",
        "Khoa Luu"
      ],
      "abstract": "Referring Multi-Object Tracking (RMOT) is a relatively new concept that has\nrapidly gained traction as a promising research direction at the intersection\nof computer vision and natural language processing. Unlike traditional\nmulti-object tracking, RMOT identifies and tracks objects and incorporates\ntextual descriptions for object class names, making the approach more\nintuitive. Various techniques have been proposed to address this challenging\nproblem; however, most require the training of the entire network due to their\nend-to-end nature. Among these methods, iKUN has emerged as a particularly\npromising solution. Therefore, we further explore its pipeline and enhance its\nperformance. In this paper, we introduce a practical module dubbed\nMemory-Efficient Cross-modality -- MEX. This memory-efficient technique can be\ndirectly applied to off-the-shelf trackers like iKUN, resulting in significant\narchitectural improvements. Our method proves effective during inference on a\nsingle GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI\ndataset, which offers diverse autonomous driving scenes with relevant language\nexpressions, is particularly useful for studying this problem. Empirically, our\nmethod demonstrates effectiveness and efficiency regarding HOTA tracking\nscores, substantially improving memory allocation and processing speed.",
      "pdf_url": "http://arxiv.org/pdf/2502.13875v1",
      "published": "2025-02-19T16:58:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13875v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
      "authors": [
        "Hui Wang",
        "Zhengpeng Zhao",
        "Jing Wang",
        "Yushu Du",
        "Yuan Cheng",
        "Bing Guo",
        "He Xiao",
        "Chenhao Ma",
        "Xiaomeng Han",
        "Dean You",
        "Jiapeng Guan",
        "Ran Wei",
        "Dawei Yang",
        "Zhe Jiang"
      ],
      "abstract": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
      "pdf_url": "http://arxiv.org/pdf/2502.13873v1",
      "published": "2025-02-19T16:54:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13873v1",
      "categories": [
        "cs.AR",
        "cs.AI"
      ]
    },
    {
      "title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
      "authors": [
        "Justin Singh Kang",
        "Landon Butler",
        "Abhineet Agarwal",
        "Yigit Efe Erginbas",
        "Ramtin Pedarsani",
        "Kannan Ramchandran",
        "Bin Yu"
      ],
      "abstract": "Large language models (LLMs) have revolutionized machine learning due to\ntheir ability to capture complex interactions between input features. Popular\npost-hoc explanation methods like SHAP provide marginal feature attributions,\nwhile their extensions to interaction importances only scale to small input\nlengths ($\\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic\ninteraction attribution algorithm that efficiently scales to large input\nlengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among\ninteractions -- common in real-world data -- and applies a sparse Fourier\ntransform using a channel decoding algorithm to efficiently identify important\ninteractions. We perform experiments across three difficult long-context\ndatasets that require LLMs to utilize interactions between inputs to complete\nthe task. For large inputs, SPEX outperforms marginal attribution methods by up\nto 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX\nsuccessfully identifies key features and interactions that strongly influence\nmodel output. For one of our datasets, HotpotQA, SPEX provides interactions\nthat align with human annotations. Finally, we use our model-agnostic approach\nto generate explanations to demonstrate abstract reasoning in closed-source\nLLMs (GPT-4o mini) and compositional reasoning in vision-language models.",
      "pdf_url": "http://arxiv.org/pdf/2502.13870v1",
      "published": "2025-02-19T16:49:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13870v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue",
      "authors": [
        "Feiyuan Zhang",
        "Dezhi Zhu",
        "James Ming",
        "Yilun Jin",
        "Di Chai",
        "Liu Yang",
        "Han Tian",
        "Zhaoxin Fan",
        "Kai Chen"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems have shown substantial benefits\nin applications such as question answering and multi-turn dialogue\n\\citep{lewis2020retrieval}. However, traditional RAG methods, while leveraging\nstatic knowledge bases, often overlook the potential of dynamic historical\ninformation in ongoing conversations. To bridge this gap, we introduce DH-RAG,\na Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for\nMulti-Turn Dialogue. DH-RAG is inspired by human cognitive processes that\nutilize both long-term memory and immediate historical context in\nconversational responses \\citep{stafford1987conversational}. DH-RAG is\nstructured around two principal components: a History-Learning based Query\nReconstruction Module, designed to generate effective queries by synthesizing\ncurrent and prior interactions, and a Dynamic History Information Updating\nModule, which continually refreshes historical context throughout the dialogue.\nThe center of DH-RAG is a Dynamic Historical Information database, which is\nfurther refined by three strategies within the Query Reconstruction Module:\nHistorical Query Clustering, Hierarchical Matching, and Chain of Thought\nTracking. Experimental evaluations show that DH-RAG significantly surpasses\nconventional models on several benchmarks, enhancing response relevance,\ncoherence, and dialogue quality.",
      "pdf_url": "http://arxiv.org/pdf/2502.13847v1",
      "published": "2025-02-19T16:10:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13847v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Enhancing LLM-Based Recommendations Through Personalized Reasoning",
      "authors": [
        "Jiahao Liu",
        "Xueshuo Yan",
        "Dongsheng Li",
        "Guangping Zhang",
        "Hansu Gu",
        "Peng Zhang",
        "Tun Lu",
        "Li Shang",
        "Ning Gu"
      ],
      "abstract": "Current recommendation systems powered by large language models (LLMs) often\nunderutilize their reasoning capabilities due to a lack of explicit logical\nstructuring. To address this limitation, we introduce CoT-Rec, a framework that\nintegrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by\nincorporating two crucial processes: user preference analysis and item\nperception evaluation. CoT-Rec operates in two key phases: (1) personalized\ndata extraction, where user preferences and item perceptions are identified,\nand (2) personalized data application, where this information is leveraged to\nrefine recommendations. Our experimental analysis demonstrates that CoT-Rec\nimproves recommendation accuracy by making better use of LLMs' reasoning\npotential. The implementation is publicly available at\nhttps://anonymous.4open.science/r/CoT-Rec.",
      "pdf_url": "http://arxiv.org/pdf/2502.13845v1",
      "published": "2025-02-19T16:08:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13845v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based User Agents",
      "authors": [
        "Jiahao Liu",
        "Shengkang Gu",
        "Dongsheng Li",
        "Guangping Zhang",
        "Mingzhe Han",
        "Hansu Gu",
        "Peng Zhang",
        "Tun Lu",
        "Li Shang",
        "Ning Gu"
      ],
      "abstract": "Large Language Model (LLM)-based user agents have emerged as a powerful tool\nfor improving recommender systems by simulating user interactions. However,\nexisting methods struggle with cross-domain scenarios due to inefficient memory\nstructures, leading to irrelevant information retention and failure to account\nfor social influence factors such as popularity. To address these limitations,\nwe introduce AgentCF++, a novel framework featuring a dual-layer memory\narchitecture and a two-step fusion mechanism to filter domain-specific\npreferences effectively. Additionally, we propose interest groups with shared\nmemory, allowing the model to capture the impact of popularity trends on users\nwith similar interests. Through extensive experiments on multiple cross-domain\ndatasets, AgentCF++ demonstrates superior performance over baseline models,\nhighlighting its effectiveness in refining user behavior simulation for\nrecommender systems. Our code is available at\nhttps://anonymous.4open.science/r/AgentCF-plus.",
      "pdf_url": "http://arxiv.org/pdf/2502.13843v1",
      "published": "2025-02-19T16:02:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13843v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Mitigating Popularity Bias in Collaborative Filtering through Fair Sampling",
      "authors": [
        "Jiahao Liu",
        "Dongsheng Li",
        "Hansu Gu",
        "Peng Zhang",
        "Tun Lu",
        "Li Shang",
        "Ning Gu"
      ],
      "abstract": "Recommender systems often suffer from popularity bias, where frequently\ninteracted items are overrepresented in recommendations. This bias stems from\npropensity factors influencing training data, leading to imbalanced exposure.\nIn this paper, we introduce a Fair Sampling (FS) approach to address this issue\nby ensuring that both users and items are selected with equal probability as\npositive and negative instances. Unlike traditional inverse propensity score\n(IPS) methods, FS does not require propensity estimation, eliminating errors\nassociated with inaccurate calculations. Our theoretical analysis demonstrates\nthat FS effectively neutralizes the influence of propensity factors, achieving\nunbiased learning. Experimental results validate that FS outperforms\nstate-of-the-art methods in both point-wise and pair-wise recommendation tasks,\nenhancing recommendation fairness without sacrificing accuracy. The\nimplementation is available at https://anonymous.4open.science/r/Fair-Sampling.",
      "pdf_url": "http://arxiv.org/pdf/2502.13840v1",
      "published": "2025-02-19T15:59:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13840v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models",
      "authors": [
        "Peter Carragher",
        "Abhinand Jha",
        "R Raghav",
        "Kathleen M. Carley"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in question\nanswering (QA), but metrics for assessing their reliance on memorization versus\nretrieval remain underdeveloped. Moreover, while finetuned models are\nstate-of-the-art on closed-domain tasks, general-purpose models like GPT-4o\nexhibit strong zero-shot performance. This raises questions about the\ntrade-offs between memorization, generalization, and retrieval. In this work,\nwe analyze the extent to which multimodal retrieval-augmented VLMs memorize\ntraining data compared to baseline VLMs. Using the WebQA benchmark, we contrast\nfinetuned models with baseline VLMs on multihop retrieval and question\nanswering, examining the impact of finetuning on data memorization. To quantify\nmemorization in end-to-end retrieval and QA systems, we propose several proxy\nmetrics by investigating instances where QA succeeds despite retrieval failing.\nOur results reveal the extent to which finetuned models rely on memorization.\nIn contrast, retrieval-augmented VLMs have lower memorization scores, at the\ncost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a\nchallenge for future work to reconcile memorization and generalization in both\nOpen-Domain QA and joint Retrieval-QA tasks.",
      "pdf_url": "http://arxiv.org/pdf/2502.13836v1",
      "published": "2025-02-19T15:58:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13836v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
      "authors": [
        "Zenan Li",
        "Zhaoyu Li",
        "Wen Tang",
        "Xian Zhang",
        "Yuan Yao",
        "Xujie Si",
        "Fan Yang",
        "Kaiyu Yang",
        "Xiaoxing Ma"
      ],
      "abstract": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.",
      "pdf_url": "http://arxiv.org/pdf/2502.13834v1",
      "published": "2025-02-19T15:54:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13834v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Scoring Verifiers: Evaluating Synthetic Verification in Code and Reasoning",
      "authors": [
        "Aleksander Ficek",
        "Somshubra Majumdar",
        "Vahid Noroozi",
        "Boris Ginsburg"
      ],
      "abstract": "Code verification has recently found great success as a critical component in\ntraining large scale reasoning models for coding. Synthetic techniques such as\nself-generated test cases and reward models provide a way to enhance code\ncapabilities beyond predefined tests. Building on these advancements, we\npropose new benchmarks designed to systematically evaluate the impact of\nsynthetic verification methods on assessing solution correctness. We introduce\nHE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks\ninto scoring and ranking datasets to evaluate the effectiveness of synthetic\nverifiers. Using these benchmarks, we analyze synthetic verification methods in\nstandard, reasoning-based, and reward-based LLMs. Our results show that recent\nreasoning models significantly improve test case generation and that scaling\ntest cases enhances verification accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2502.13820v1",
      "published": "2025-02-19T15:32:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13820v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "title": "AnDB: Breaking Boundaries with an AI-Native Database for Universal Semantic Analysis",
      "authors": [
        "Tianqing Wang",
        "Xun Xue",
        "Guoliang Li",
        "Yong Wang"
      ],
      "abstract": "In this demonstration, we present AnDB, an AI-native database that supports\ntraditional OLTP workloads and innovative AI-driven tasks, enabling unified\nsemantic analysis across structured and unstructured data. While structured\ndata analytics is mature, challenges remain in bridging the semantic gap\nbetween user queries and unstructured data. AnDB addresses these issues by\nleveraging cutting-edge AI-native technologies, allowing users to perform\nsemantic queries using intuitive SQL-like statements without requiring AI\nexpertise. This approach eliminates the ambiguity of traditional text-to-SQL\nsystems and provides a seamless end-to-end optimization for analyzing all data\ntypes. AnDB automates query processing by generating multiple execution plans\nand selecting the optimal one through its optimizer, which balances accuracy,\nexecution time, and financial cost based on user policies and internal\noptimizing mechanisms. AnDB future-proofs data management infrastructure,\nempowering users to effectively and efficiently harness the full potential of\nall kinds of data without starting from scratch.",
      "pdf_url": "http://arxiv.org/pdf/2502.13805v1",
      "published": "2025-02-19T15:15:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13805v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "LESA: Learnable LLM Layer Scaling-Up",
      "authors": [
        "Yifei Yang",
        "Zouying Cao",
        "Xinbei Ma",
        "Yao Yao",
        "Libo Qin",
        "Zhi Chen",
        "Hai Zhao"
      ],
      "abstract": "Training Large Language Models (LLMs) from scratch requires immense\ncomputational resources, making it prohibitively expensive. Model scaling-up\noffers a promising solution by leveraging the parameters of smaller models to\ncreate larger ones. However, existing depth scaling-up methods rely on\nempirical heuristic rules for layer duplication, which result in poorer\ninitialization and slower convergence during continual pre-training. We propose\n\\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating\nparameters from each layer and applying Singular Value Decomposition, we\nuncover latent patterns between layers, suggesting that inter-layer parameters\ncan be learned. LESA uses a neural network to predict the parameters inserted\nbetween adjacent layers, enabling better initialization and faster training.\nExperiments show that LESA outperforms existing baselines, achieving superior\nperformance with less than half the computational cost during continual\npre-training. Extensive analyses demonstrate its effectiveness across different\nmodel sizes and tasks.",
      "pdf_url": "http://arxiv.org/pdf/2502.13794v1",
      "published": "2025-02-19T14:58:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13794v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA Therapeutics",
      "authors": [
        "Matthew Wood",
        "Mathieu Klop",
        "Maxime Allard"
      ],
      "abstract": "mRNA-based vaccines have become a major focus in the pharmaceutical industry.\nThe coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can\nstrongly influence translation efficiency, stability, degradation, and other\nfactors that collectively determine a vaccine's effectiveness. However,\noptimizing mRNA sequences for those properties remains a complex challenge.\nExisting deep learning models often focus solely on coding region optimization,\noverlooking the UTRs. We present Helix-mRNA, a structured state-space-based and\nattention hybrid model to address these challenges. In addition to a first\npre-training, a second pre-training stage allows us to specialise the model\nwith high-quality data. We employ single nucleotide tokenization of mRNA\nsequences with codon separation, ensuring prior biological and structural\ninformation from the original mRNA sequence is not lost. Our model, Helix-mRNA,\noutperforms existing methods in analysing both UTRs and coding region\nproperties. It can process sequences 6x longer than current approaches while\nusing only 10% of the parameters of existing foundation models. Its predictive\ncapabilities extend to all mRNA regions. We open-source the model\n(https://github.com/helicalAI/helical) and model weights\n(https://huggingface.co/helical-ai/helix-mRNA).",
      "pdf_url": "http://arxiv.org/pdf/2502.13785v1",
      "published": "2025-02-19T14:51:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13785v1",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ]
    },
    {
      "title": "Poster: SpiderSim: Multi-Agent Driven Theoretical Cybersecurity Simulation for Industrial Digitalization",
      "authors": [
        "Jiaqi Li",
        "Xizhong Guo",
        "Yang Zhao",
        "Lvyang Zhang",
        "Lidong Zhai"
      ],
      "abstract": "Rapid industrial digitalization has created intricate cybersecurity demands\nthat necessitate effective validation methods. While cyber ranges and\nsimulation platforms are widely deployed, they frequently face limitations in\nscenario diversity and creation efficiency. In this paper, we present\nSpiderSim, a theoretical cybersecurity simulation platform enabling rapid and\nlightweight scenario generation for industrial digitalization security\nresearch. At its core, our platform introduces three key innovations: a\nstructured framework for unified scenario modeling, a multi-agent collaboration\nmechanism for automated generation, and modular atomic security capabilities\nfor flexible scenario composition. Extensive implementation trials across\nmultiple industrial digitalization contexts, including marine ranch monitoring\nsystems, validate our platform's capacity for broad scenario coverage with\nefficient generation processes. Built on solid theoretical foundations and\nreleased as open-source software, SpiderSim facilitates broader research and\ndevelopment in automated security testing for industrial digitalization.",
      "pdf_url": "http://arxiv.org/pdf/2502.13778v1",
      "published": "2025-02-19T14:42:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13778v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare",
      "authors": [
        "Anudeex Shetty",
        "Amin Beheshti",
        "Mark Dras",
        "Usman Naseem"
      ],
      "abstract": "Alignment techniques have become central to ensuring that Large Language\nModels (LLMs) generate outputs consistent with human values. However, existing\nalignment paradigms often model an averaged or monolithic preference, failing\nto account for the diversity of perspectives across cultures, demographics, and\ncommunities. This limitation is particularly critical in health-related\nscenarios, where plurality is essential due to the influence of culture,\nreligion, personal values, and conflicting opinions. Despite progress in\npluralistic alignment, no prior work has focused on health, likely due to the\nunavailability of publicly available datasets. To address this gap, we\nintroduce VITAL, a new benchmark dataset comprising 13.1K value-laden\nsituations and 5.4K multiple-choice questions focused on health, designed to\nassess and benchmark pluralistic alignment methodologies. Through extensive\nevaluation of eight LLMs of varying sizes, we demonstrate that existing\npluralistic alignment techniques fall short in effectively accommodating\ndiverse healthcare beliefs, underscoring the need for tailored AI alignment in\nspecific domains. This work highlights the limitations of current approaches\nand lays the groundwork for developing health-specific alignment solutions.",
      "pdf_url": "http://arxiv.org/pdf/2502.13775v1",
      "published": "2025-02-19T14:38:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13775v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A consensus set for the aggregation of partial rankings: the case of the Optimal Set of Bucket Orders Problem",
      "authors": [
        "Juan A. Aledo",
        "José A. Gámez",
        "Alejandro Rosete"
      ],
      "abstract": "In rank aggregation problems (RAP), the solution is usually a consensus\nranking that generalizes a set of input orderings. There are different variants\nthat differ not only in terms of the type of rankings that are used as input\nand output, but also in terms of the objective function employed to evaluate\nthe quality of the desired output ranking. In contrast, in some machine\nlearning tasks (e.g. subgroup discovery) or multimodal optimization tasks,\nattention is devoted to obtaining several models/results to account for the\ndiversity in the input data or across the search landscape. Thus, in this paper\nwe propose to provide, as the solution to an RAP, a set of rankings to better\nexplain the preferences expressed in the input orderings. We exemplify our\nproposal through the Optimal Bucket Order Problem (OBOP), an RAP which consists\nin finding a single consensus ranking (with ties) that generalizes a set of\ninput rankings codified as a precedence matrix. To address this, we introduce\nthe Optimal Set of Bucket Orders Problem (OSBOP), a generalization of the OBOP\nthat aims to produce not a single ranking as output but a set of consensus\nrankings. Experimental results are presented to illustrate this proposal,\nshowing how, by providing a set of consensus rankings, the fitness of the\nsolution significantly improves with respect to the one of the original OBOP,\nwithout losing comprehensibility.",
      "pdf_url": "http://arxiv.org/pdf/2502.13769v1",
      "published": "2025-02-19T14:32:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13769v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AI Software Engineer: Programming with Trust",
      "authors": [
        "Abhik Roychoudhury",
        "Corina Pasareanu",
        "Michael Pradel",
        "Baishakhi Ray"
      ],
      "abstract": "Large Language Models (LLMs) have shown surprising proficiency in generating\ncode snippets, promising to automate large parts of software engineering via\nartificial intelligence (AI). We argue that successfully deploying AI software\nengineers requires a level of trust equal to or even greater than the trust\nestablished by human-driven software engineering practices. The recent trend\ntoward LLM agents offers a path toward integrating the power of LLMs to create\nnew code with the power of analysis tools to increase trust in the code. This\nopinion piece comments on whether LLM agents could dominate software\nengineering workflows in the future and whether the focus of programming will\nshift from programming at scale to programming with trust.",
      "pdf_url": "http://arxiv.org/pdf/2502.13767v1",
      "published": "2025-02-19T14:28:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13767v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice",
      "authors": [
        "Wanke Xia",
        "Ruxin Peng",
        "Haoqi Chu",
        "Xinlei Zhu",
        "Zhiyu Yang",
        "Yaojun Wang"
      ],
      "abstract": "Rice is one of the most widely cultivated crops globally and has been\ndeveloped into numerous varieties. The quality of rice during cultivation is\nprimarily determined by its cultivar and characteristics. Traditionally, rice\nclassification and quality assessment rely on manual visual inspection, a\nprocess that is both time-consuming and prone to errors. However, with\nadvancements in machine vision technology, automating rice classification and\nquality evaluation based on its cultivar and characteristics has become\nincreasingly feasible, enhancing both accuracy and efficiency. This study\nproposes a real-time evaluation mechanism for comprehensive rice grain\nassessment, integrating a one-stage object detection approach, a deep\nconvolutional neural network, and traditional machine learning techniques. The\nproposed framework enables rice variety identification, grain completeness\ngrading, and grain chalkiness evaluation. The rice grain dataset used in this\nstudy comprises approximately 20,000 images from six widely cultivated rice\nvarieties in China. Experimental results demonstrate that the proposed\nmechanism achieves a mean average precision (mAP) of 99.14% in the object\ndetection task and an accuracy of 97.89% in the classification task.\nFurthermore, the framework attains an average accuracy of 97.56% in grain\ncompleteness grading within the same rice variety, contributing to an effective\nquality evaluation system.",
      "pdf_url": "http://arxiv.org/pdf/2502.13764v1",
      "published": "2025-02-19T14:24:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13764v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "GPA: Grover Policy Agent for Generating Optimal Quantum Sensor Circuits",
      "authors": [
        "Ahmad Alomari",
        "Sathish A. P. Kumar"
      ],
      "abstract": "This study proposes a GPA for designing optimal Quantum Sensor Circuits\n(QSCs) to address complex quantum physics problems. The GPA consists of two\nparts: the Quantum Policy Evaluation (QPE) and the Quantum Policy Improvement\n(QPI). The QPE performs phase estimation to generate the search space, while\nthe QPI utilizes Grover search and amplitude amplification techniques to\nefficiently identify an optimal policy that generates optimal QSCs. The GPA\ngenerates QSCs by selecting sequences of gates that maximize the Quantum Fisher\nInformation (QFI) while minimizing the number of gates. The QSCs generated by\nthe GPA are capable of producing entangled quantum states, specifically the\nsqueezed states. High QFI indicates increased sensitivity to parameter changes,\nmaking the circuit useful for quantum state estimation and control tasks.\nEvaluation of the GPA on a QSC that consists of two qubits and a sequence of\nR_x, R_y, and S gates demonstrates its efficiency in generating optimal QSCs\nwith a QFI of 1. Compared to existing quantum agents, the GPA achieves higher\nQFI with fewer gates, demonstrating a more efficient and scalable approach to\nthe design of QSCs. This work illustrates the potential computational power of\nquantum agents for solving quantum physics problems",
      "pdf_url": "http://arxiv.org/pdf/2502.13755v1",
      "published": "2025-02-19T14:20:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13755v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "RobustX: Robust Counterfactual Explanations Made Easy",
      "authors": [
        "Junqi Jiang",
        "Luca Marzari",
        "Aaryan Purohit",
        "Francesco Leofante"
      ],
      "abstract": "The increasing use of Machine Learning (ML) models to aid decision-making in\nhigh-stakes industries demands explainability to facilitate trust.\nCounterfactual Explanations (CEs) are ideally suited for this, as they can\noffer insights into the predictions of an ML model by illustrating how changes\nin its input data may lead to different outcomes. However, for CEs to realise\ntheir explanatory potential, significant challenges remain in ensuring their\nrobustness under slight changes in the scenario being explained. Despite the\nwidespread recognition of CEs' robustness as a fundamental requirement, a lack\nof standardised tools and benchmarks hinders a comprehensive and effective\ncomparison of robust CE generation methods. In this paper, we introduce\nRobustX, an open-source Python library implementing a collection of CE\ngeneration and evaluation methods, with a focus on the robustness property.\nRobustX provides interfaces to several existing methods from the literature,\nenabling streamlined access to state-of-the-art techniques. The library is also\neasily extensible, allowing fast prototyping of novel robust CE generation and\nevaluation methods.",
      "pdf_url": "http://arxiv.org/pdf/2502.13751v1",
      "published": "2025-02-19T14:12:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13751v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Inference of Abstraction for Grounded Predicate Logic",
      "authors": [
        "Hiroyuki Kido"
      ],
      "abstract": "An important open question in AI is what simple and natural principle enables\na machine to reason logically for meaningful abstraction with grounded symbols.\nThis paper explores a conceptually new approach to combining probabilistic\nreasoning and predicative symbolic reasoning over data. We return to the era of\nreasoning with a full joint distribution before the advent of Bayesian\nnetworks. We then discuss that a full joint distribution over models of\nexponential size in propositional logic and of infinite size in predicate logic\nshould be simply derived from a full joint distribution over data of linear\nsize. We show that the same process is not only enough to generalise the\nlogical consequence relation of predicate logic but also to provide a new\nperspective to rethink well-known limitations such as the undecidability of\npredicate logic, the symbol grounding problem and the principle of explosion.\nThe reproducibility of this theoretical work is fully demonstrated by the\nincluded proofs.",
      "pdf_url": "http://arxiv.org/pdf/2502.13743v1",
      "published": "2025-02-19T14:07:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13743v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Robust Counterfactual Inference in Markov Decision Processes",
      "authors": [
        "Jessica Lally",
        "Milad Kazemi",
        "Nicola Paoletti"
      ],
      "abstract": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods.",
      "pdf_url": "http://arxiv.org/pdf/2502.13731v1",
      "published": "2025-02-19T13:56:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13731v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Secure Federated Data Distillation",
      "authors": [
        "Marco Arazzi",
        "Mert Cihangiroglu",
        "Serena Nicolazzo",
        "Antonino Nocera"
      ],
      "abstract": "Dataset Distillation (DD) is a powerful technique for reducing large datasets\ninto compact, representative synthetic datasets, accelerating Machine Learning\ntraining. However, traditional DD methods operate in a centralized manner,\nwhich poses significant privacy threats and reduces its applicability. To\nmitigate these risks, we propose a Secure Federated Data Distillation framework\n(SFDD) to decentralize the distillation process while preserving privacy.Unlike\nexisting Federated Distillation techniques that focus on training global models\nwith distilled knowledge, our approach aims to produce a distilled dataset\nwithout exposing local contributions. We leverage the gradient-matching-based\ndistillation method, adapting it for a distributed setting where clients\ncontribute to the distillation process without sharing raw data. The central\naggregator iteratively refines a synthetic dataset by integrating client-side\nupdates while ensuring data confidentiality. To make our approach resilient to\ninference attacks perpetrated by the server that could exploit gradient updates\nto reconstruct private data, we create an optimized Local Differential Privacy\napproach, called LDPO-RLD (Label Differential Privacy Obfuscation via\nRandomized Linear Dispersion). Furthermore, we assess the framework's\nresilience against malicious clients executing backdoor attacks and demonstrate\nrobustness under the assumption of a sufficient number of participating\nclients. Our experimental results demonstrate the effectiveness of SFDD and\nthat the proposed defense concretely mitigates the identified vulnerabilities,\nwith minimal impact on the performance of the distilled dataset. By addressing\nthe interplay between privacy and federation in dataset distillation, this work\nadvances the field of privacy-preserving Machine Learning making our SFDD\nframework a viable solution for sensitive data-sharing applications.",
      "pdf_url": "http://arxiv.org/pdf/2502.13728v1",
      "published": "2025-02-19T13:54:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13728v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values",
      "authors": [
        "Hongbo Zhang",
        "Han Cui",
        "Guangsheng Bao",
        "Linyi Yang",
        "Jun Wang",
        "Yue Zhang"
      ],
      "abstract": "We introduce Direct Value Optimization (DVO), an innovative reinforcement\nlearning framework for enhancing large language models in complex reasoning\ntasks. Unlike traditional methods relying on preference labels, DVO utilizes\nvalue signals at individual reasoning steps, optimizing models via a mean\nsquared error loss. The key benefit of DVO lies in its fine-grained\nsupervision, circumventing the need for labor-intensive human annotations.\nTarget values within the DVO are estimated using either Monte Carlo Tree Search\nor an outcome value model. Our empirical analysis on both mathematical and\ncommonsense reasoning tasks shows that DVO consistently outperforms existing\noffline preference optimization techniques, even with fewer training steps.\nThese findings underscore the importance of value signals in advancing\nreasoning capabilities and highlight DVO as a superior methodology under\nscenarios lacking explicit human preference information.",
      "pdf_url": "http://arxiv.org/pdf/2502.13723v1",
      "published": "2025-02-19T13:51:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13723v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "TrustRAG: An Information Assistant with Retrieval Augmented Generation",
      "authors": [
        "Yixing Fan",
        "Qiang Yan",
        "Wenshan Wang",
        "Jiafeng Guo",
        "Ruqing Zhang",
        "Xueqi Cheng"
      ],
      "abstract": "\\Ac{RAG} has emerged as a crucial technique for enhancing large models with\nreal-time and domain-specific knowledge. While numerous improvements and\nopen-source tools have been proposed to refine the \\ac{RAG} framework for\naccuracy, relatively little attention has been given to improving the\ntrustworthiness of generated results. To address this gap, we introduce\nTrustRAG, a novel framework that enhances \\ac{RAG} from three perspectives:\nindexing, retrieval, and generation. Specifically, in the indexing stage, we\npropose a semantic-enhanced chunking strategy that incorporates hierarchical\nindexing to supplement each chunk with contextual information, ensuring\nsemantic completeness. In the retrieval stage, we introduce a utility-based\nfiltering mechanism to identify high-quality information, supporting answer\ngeneration while reducing input length. In the generation stage, we propose\nfine-grained citation enhancement, which detects opinion-bearing sentences in\nresponses and infers citation relationships at the sentence-level, thereby\nimproving citation accuracy. We open-source the TrustRAG framework and provide\na demonstration studio designed for excerpt-based question answering tasks\n\\footnote{https://huggingface.co/spaces/golaxy/TrustRAG}. Based on these, we\naim to help researchers: 1) systematically enhancing the trustworthiness of\n\\ac{RAG} systems and (2) developing their own \\ac{RAG} systems with more\nreliable outputs.",
      "pdf_url": "http://arxiv.org/pdf/2502.13719v1",
      "published": "2025-02-19T13:45:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13719v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Causes and Strategies in Multiagent Systems",
      "authors": [
        "Sylvia S. Kerkhove",
        "Natasha Alechina",
        "Mehdi Dastani"
      ],
      "abstract": "Causality plays an important role in daily processes, human reasoning, and\nartificial intelligence. There has however not been much research on causality\nin multi-agent strategic settings. In this work, we introduce a systematic way\nto build a multi-agent system model, represented as a concurrent game\nstructure, for a given structural causal model. In the obtained so-called\ncausal concurrent game structure, transitions correspond to interventions on\nagent variables of the given causal model. The Halpern and Pearl framework of\ncausality is used to determine the effects of a certain value for an agent\nvariable on other variables. The causal concurrent game structure allows us to\nanalyse and reason about causal effects of agents' strategic decisions. We\nformally investigate the relation between causal concurrent game structures and\nthe original structural causal models.",
      "pdf_url": "http://arxiv.org/pdf/2502.13701v1",
      "published": "2025-02-19T13:18:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13701v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories",
      "authors": [
        "Jusen Du",
        "Weigao Sun",
        "Disen Lan",
        "Jiaxi Hu",
        "Yu Cheng"
      ],
      "abstract": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
      "pdf_url": "http://arxiv.org/pdf/2502.13685v1",
      "published": "2025-02-19T12:53:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13685v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "An LLM-based Agent for Reliable Docker Environment Configuration",
      "authors": [
        "Ruida Hu",
        "Chao Peng",
        "Xinchen Wang",
        "Cuiyun Gao"
      ],
      "abstract": "Environment configuration is a critical yet time-consuming step in software\ndevelopment, especially when dealing with unfamiliar code repositories. While\nLarge Language Models (LLMs) demonstrate the potential to accomplish software\nengineering tasks, existing methods for environment configuration often rely on\nmanual efforts or fragile scripts, leading to inefficiencies and unreliable\noutcomes. We introduce Repo2Run, the first LLM-based agent designed to fully\nautomate environment configuration and generate executable Dockerfiles for\narbitrary Python repositories. We address two major challenges: (1) enabling\nthe LLM agent to configure environments within isolated Docker containers, and\n(2) ensuring the successful configuration process is recorded and accurately\ntransferred to a Dockerfile without error. To achieve this, we propose atomic\nconfiguration synthesis, featuring a dual-environment architecture (internal\nand external environment) with a rollback mechanism to prevent environment\n\"pollution\" from failed commands, guaranteeing atomic execution (execute fully\nor not at all) and a Dockerfile generator to transfer successful configuration\nsteps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark\nof 420 recent Python repositories with unit tests, where it achieves an 86.0%\nsuccess rate, outperforming the best baseline by 63.9%.",
      "pdf_url": "http://arxiv.org/pdf/2502.13681v1",
      "published": "2025-02-19T12:51:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13681v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "PeerQA: A Scientific Question Answering Dataset from Peer Reviews",
      "authors": [
        "Tim Baumgärtner",
        "Ted Briscoe",
        "Iryna Gurevych"
      ],
      "abstract": "We present PeerQA, a real-world, scientific, document-level Question\nAnswering (QA) dataset. PeerQA questions have been sourced from peer reviews,\nwhich contain questions that reviewers raised while thoroughly examining the\nscientific article. Answers have been annotated by the original authors of each\npaper. The dataset contains 579 QA pairs from 208 academic articles, with a\nmajority from ML and NLP, as well as a subset of other scientific communities\nlike Geoscience and Public Health. PeerQA supports three critical tasks for\ndeveloping practical QA systems: Evidence retrieval, unanswerable question\nclassification, and answer generation. We provide a detailed analysis of the\ncollected dataset and conduct experiments establishing baseline systems for all\nthree tasks. Our experiments and analyses reveal the need for\ndecontextualization in document-level retrieval, where we find that even simple\ndecontextualization approaches consistently improve retrieval performance\nacross architectures. On answer generation, PeerQA serves as a challenging\nbenchmark for long-context modeling, as the papers have an average size of 12k\ntokens. Our code and data is available at https://github.com/UKPLab/peerqa.",
      "pdf_url": "http://arxiv.org/pdf/2502.13668v1",
      "published": "2025-02-19T12:24:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13668v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "C2T: A Classifier-Based Tree Construction Method in Speculative Decoding",
      "authors": [
        "Feiye Huo",
        "Jianchao Tan",
        "Kefeng Zhang",
        "Xunliang Cai",
        "Shengli Sun"
      ],
      "abstract": "The growing scale of Large Language Models (LLMs) has exacerbated inference\nlatency and computational costs. Speculative decoding methods, which aim to\nmitigate these issues, often face inefficiencies in the construction of token\ntrees and the verification of candidate tokens. Existing strategies, including\nchain mode, static tree, and dynamic tree approaches, have limitations in\naccurately preparing candidate token trees for verification. We propose a novel\nmethod named C2T that adopts a lightweight classifier to generate and prune\ntoken trees dynamically. Our classifier considers additional feature variables\nbeyond the commonly used joint probability to predict the confidence score for\neach draft token to determine whether it is the candidate token for\nverification. This method outperforms state-of-the-art (SOTA) methods such as\nEAGLE-2 on multiple benchmarks, by reducing the total number of candidate\ntokens by 25% while maintaining or even improving the acceptance length.",
      "pdf_url": "http://arxiv.org/pdf/2502.13652v1",
      "published": "2025-02-19T11:57:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13652v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Integrating Inverse and Forward Modeling for Sparse Temporal Data from Sensor Networks",
      "authors": [
        "Julian Vexler",
        "Björn Vieten",
        "Martin Nelke",
        "Stefan Kramer"
      ],
      "abstract": "We present CavePerception, a framework for the analysis of sparse data from\nsensor networks that incorporates elements of inverse modeling and forward\nmodeling. By integrating machine learning with physical modeling in a\nhypotheses space, we aim to improve the interpretability of sparse, noisy, and\npotentially incomplete sensor data. The framework assumes data from a\ntwo-dimensional sensor network laid out in a graph structure that detects\ncertain objects, with certain motion patterns. Examples of such sensors are\nmagnetometers. Given knowledge about the objects and the way they act on the\nsensors, one can develop a data generator that produces data from simulated\nmotions of the objects across the sensor field. The framework uses the\nsimulated data to infer object behaviors across the sensor network. The\napproach is experimentally tested on real-world data, where magnetometers are\nused on an airport to detect and identify aircraft motions. Experiments\ndemonstrate the value of integrating inverse and forward modeling, enabling\nintelligent systems to better understand and predict complex, sensor-driven\nevents.",
      "pdf_url": "http://arxiv.org/pdf/2502.13638v1",
      "published": "2025-02-19T11:24:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13638v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization",
      "authors": [
        "Or Raphael Bidusa",
        "Shaul Markovitch"
      ],
      "abstract": "The opaque nature of Large Language Models (LLMs) has led to significant\nresearch efforts aimed at enhancing their interpretability, primarily through\npost-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck\nModels (CBMs), offer both interpretability and intervenability by incorporating\nexplicit concept representations. However, these methods suffer from key\nlimitations, including reliance on labeled concept datasets and significant\narchitectural modifications that challenges re-integration into existing system\npipelines. In this work, we introduce a new methodology for incorporating\ninterpretability and intervenability into an existing model by integrating\nConcept Layers (CLs) into its architecture. Our approach projects the model's\ninternal vector representations into a conceptual, explainable vector space\nbefore reconstructing and feeding them back into the model. Furthermore, we\neliminate the need for a human-selected concept set by algorithmically\nsearching an ontology for a set of concepts that can be either task-specific or\ntask-agnostic. We evaluate CLs across multiple tasks, demonstrating that they\nmaintain the original model's performance and agreement while enabling\nmeaningful interventions. Additionally, we present a proof of concept\nshowcasing an intervenability interface, allowing users to adjust model\nbehavior dynamically, such as mitigating biases during inference.",
      "pdf_url": "http://arxiv.org/pdf/2502.13632v1",
      "published": "2025-02-19T11:10:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13632v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models",
      "authors": [
        "DongGeon Lee",
        "Hwanjo Yu"
      ],
      "abstract": "Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages.",
      "pdf_url": "http://arxiv.org/pdf/2502.13622v1",
      "published": "2025-02-19T10:59:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13622v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Decentralized Planning Using Probabilistic Hyperproperties",
      "authors": [
        "Francesco Pontiggia",
        "Filip Macák",
        "Roman Andriushchenko",
        "Michele Chiari",
        "Milan Češka"
      ],
      "abstract": "Multi-agent planning under stochastic dynamics is usually formalised using\ndecentralized (partially observable) Markov decision processes ( MDPs) and\nreachability or expected reward specifications. In this paper, we propose a\ndifferent approach: we use an MDP describing how a single agent operates in an\nenvironment and probabilistic hyperproperties to capture desired temporal\nobjectives for a set of decentralized agents operating in the environment. We\nextend existing approaches for model checking probabilistic hyperproperties to\nhandle temporal formulae relating paths of different agents, thus requiring the\nself-composition between multiple MDPs. Using several case studies, we\ndemonstrate that our approach provides a flexible and expressive framework to\nbroaden the specification capabilities with respect to existing planning\ntechniques. Additionally, we establish a close connection between a subclass of\nprobabilistic hyperproperties and planning for a particular type of Dec-MDPs,\nfor both of which we show undecidability. This lays the ground for the use of\nexisting decentralized planning tools in the field of probabilistic\nhyperproperty verification.",
      "pdf_url": "http://arxiv.org/pdf/2502.13621v1",
      "published": "2025-02-19T10:59:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13621v1",
      "categories": [
        "cs.LO",
        "cs.AI"
      ]
    },
    {
      "title": "Complex Ontology Matching with Large Language Model Embeddings",
      "authors": [
        "Guilherme Sousa",
        "Rinaldo Lima",
        "Cassia Trojahn"
      ],
      "abstract": "Ontology, and more broadly, Knowledge Graph Matching is a challenging task in\nwhich expressiveness has not been fully addressed. Despite the increasing use\nof embeddings and language models for this task, approaches for generating\nexpressive correspondences still do not take full advantage of these models, in\nparticular, large language models (LLMs). This paper proposes to integrate LLMs\ninto an approach for generating expressive correspondences based on alignment\nneed and ABox-based relation discovery. The generation of correspondences is\nperformed by matching similar surroundings of instance sub-graphs. The\nintegration of LLMs results in different architectural modifications, including\nlabel similarity, sub-graph matching, and entity matching. The performance word\nembeddings, sentence embeddings, and LLM-based embeddings, was compared. The\nresults demonstrate that integrating LLMs surpasses all other models, enhancing\nthe baseline version of the approach with a 45\\% increase in F-measure.",
      "pdf_url": "http://arxiv.org/pdf/2502.13619v1",
      "published": "2025-02-19T10:56:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13619v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LaVCa: LLM-assisted Visual Cortex Captioning",
      "authors": [
        "Takuya Matsuyama",
        "Shinji Nishimoto",
        "Yu Takagi"
      ],
      "abstract": "Understanding the property of neural populations (or voxels) in the human\nbrain can advance our comprehension of human perceptual and cognitive\nprocessing capabilities and contribute to developing brain-inspired computer\nmodels. Recent encoding models using deep neural networks (DNNs) have\nsuccessfully predicted voxel-wise activity. However, interpreting the\nproperties that explain voxel responses remains challenging because of the\nblack-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex\nCaptioning (LaVCa), a data-driven approach that uses large language models\n(LLMs) to generate natural-language captions for images to which voxels are\nselective. By applying LaVCa for image-evoked brain activity, we demonstrate\nthat LaVCa generates captions that describe voxel selectivity more accurately\nthan the previously proposed method. Furthermore, the captions generated by\nLaVCa quantitatively capture more detailed properties than the existing method\nat both the inter-voxel and intra-voxel levels. Furthermore, a more detailed\nanalysis of the voxel-specific properties generated by LaVCa reveals\nfine-grained functional differentiation within regions of interest (ROIs) in\nthe visual cortex and voxels that simultaneously represent multiple distinct\nconcepts. These findings offer profound insights into human visual\nrepresentations by assigning detailed captions throughout the visual cortex\nwhile highlighting the potential of LLM-based methods in understanding brain\nrepresentations. Please check out our webpage at\nhttps://sites.google.com/view/lavca-llm/",
      "pdf_url": "http://arxiv.org/pdf/2502.13606v1",
      "published": "2025-02-19T10:37:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13606v1",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs",
      "authors": [
        "Dario Garcia-Gasulla",
        "Anna Arias-Duart",
        "Adrian Tormos",
        "Daniel Hinjos",
        "Oscar Molina-Sedano",
        "Ashwin Kumar Gururajan",
        "Maria Eugenia Cardello"
      ],
      "abstract": "Direct Preference Optimization (DPO) is an efficient alignment technique that\nsteers LLMs towards preferable outputs by training on preference data,\nbypassing the need for explicit reward models. Its simplicity enables easy\nadaptation to various domains and safety requirements. This paper examines\nDPO's effectiveness in model safety against jailbreaking attacks while\nminimizing data requirements and training costs. We introduce Egida, a dataset\nexpanded from multiple sources, which includes 27 different safety topics and\n18 different attack styles, complemented with synthetic and human labels. This\ndata is used to boost the safety of state-of-the-art LLMs\n(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack\nstyles. In addition to safety evaluations, we assess their post-alignment\nperformance degradation in general purpose tasks, and their tendency to over\nrefusal. Following the proposed methodology, trained models reduce their Attack\nSuccess Rate by 10%-30%, using small training efforts (2,000 samples) with low\ncomputational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned\nmodels generalize to unseen topics and attack styles, with the most successful\nattack style reaching a success rate around 5%. Size and family are found to\nstrongly influence model malleability towards safety, pointing at the\nimportance of pre-training choices. To validate our findings, a large\nindependent assessment of human preference agreement with Llama-Guard-3-8B is\nconducted by the authors and the associated dataset Egida-HSafe is released.\nOverall, this study illustrates how affordable and accessible it is to enhance\nLLM safety using DPO while outlining its current limitations. All datasets and\nmodels are released to enable reproducibility and further research.",
      "pdf_url": "http://arxiv.org/pdf/2502.13603v1",
      "published": "2025-02-19T10:33:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.13603v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    }
  ]
}