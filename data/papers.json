{
  "last_updated": "2025-03-02T00:49:13.216212",
  "papers": [
    {
      "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
      "authors": [
        "Toru Lin",
        "Kartik Sachdev",
        "Linxi Fan",
        "Jitendra Malik",
        "Yuke Zhu"
      ],
      "abstract": "Reinforcement learning has delivered promising results in achieving human- or\neven superhuman-level capabilities across diverse problem domains, but success\nin dexterous robot manipulation remains limited. This work investigates the key\nchallenges in applying reinforcement learning to solve a collection of\ncontact-rich manipulation tasks on a humanoid embodiment. We introduce novel\ntechniques to overcome the identified challenges with empirical validation. Our\nmain contributions include an automated real-to-sim tuning module that brings\nthe simulated environment closer to the real world, a generalized reward design\nscheme that simplifies reward engineering for long-horizon contact-rich\nmanipulation tasks, a divide-and-conquer distillation process that improves the\nsample efficiency of hard-exploration problems while maintaining sim-to-real\nperformance, and a mixture of sparse and dense object representations to bridge\nthe sim-to-real perception gap. We show promising results on three humanoid\ndexterous manipulation tasks, with ablation studies on each technique. Our work\npresents a successful approach to learning humanoid dexterous manipulation\nusing sim-to-real reinforcement learning, achieving robust generalization and\nhigh performance without the need for human demonstration.",
      "pdf_url": "http://arxiv.org/pdf/2502.20396v1",
      "published": "2025-02-27T18:59:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20396v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Walking the Web of Concept-Class Relationships in Incrementally Trained Interpretable Models",
      "authors": [
        "Susmit Agrawal",
        "Deepika Vemuri",
        "Sri Siddarth Chakaravarthy P",
        "Vineeth N. Balasubramanian"
      ],
      "abstract": "Concept-based methods have emerged as a promising direction to develop\ninterpretable neural networks in standard supervised settings. However, most\nworks that study them in incremental settings assume either a static concept\nset across all experiences or assume that each experience relies on a distinct\nset of concepts. In this work, we study concept-based models in a more\nrealistic, dynamic setting where new classes may rely on older concepts in\naddition to introducing new concepts themselves. We show that concepts and\nclasses form a complex web of relationships, which is susceptible to\ndegradation and needs to be preserved and augmented across experiences. We\nintroduce new metrics to show that existing concept-based models cannot\npreserve these relationships even when trained using methods to prevent\ncatastrophic forgetting, since they cannot handle forgetting at concept, class,\nand concept-class relationship levels simultaneously. To address these issues,\nwe propose a novel method - MuCIL - that uses multimodal concepts to perform\nclassification without increasing the number of trainable parameters across\nexperiences. The multimodal concepts are aligned to concepts provided in\nnatural language, making them interpretable by design. Through extensive\nexperimentation, we show that our approach obtains state-of-the-art\nclassification performance compared to other concept-based models, achieving\nover 2$\\times$ the classification performance in some cases. We also study the\nability of our model to perform interventions on concepts, and show that it can\nlocalize visual concepts in input images, providing post-hoc interpretations.",
      "pdf_url": "http://arxiv.org/pdf/2502.20393v1",
      "published": "2025-02-27T18:59:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20393v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization",
      "authors": [
        "Lujie Yang",
        "H. J. Terry Suh",
        "Tong Zhao",
        "Bernhard Paus Graesdal",
        "Tarik Kelestemur",
        "Jiuguang Wang",
        "Tao Pang",
        "Russ Tedrake"
      ],
      "abstract": "We present a low-cost data generation pipeline that integrates physics-based\nsimulation, human demonstrations, and model-based planning to efficiently\ngenerate large-scale, high-quality datasets for contact-rich robotic\nmanipulation tasks. Starting with a small number of embodiment-flexible human\ndemonstrations collected in a virtual reality simulation environment, the\npipeline refines these demonstrations using optimization-based kinematic\nretargeting and trajectory optimization to adapt them across various robot\nembodiments and physical parameters. This process yields a diverse, physically\nconsistent dataset that enables cross-embodiment data transfer, and offers the\npotential to reuse legacy datasets collected under different hardware\nconfigurations or physical parameters. We validate the pipeline's effectiveness\nby training diffusion policies from the generated datasets for challenging\ncontact-rich manipulation tasks across multiple robot embodiments, including a\nfloating Allegro hand and bimanual robot arms. The trained policies are\ndeployed zero-shot on hardware for bimanual iiwa arms, achieving high success\nrates with minimal human input. Project website:\nhttps://lujieyang.github.io/physicsgen/.",
      "pdf_url": "http://arxiv.org/pdf/2502.20382v1",
      "published": "2025-02-27T18:56:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20382v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Multi-Turn Code Generation Through Single-Step Rewards",
      "authors": [
        "Arnav Kumar Jain",
        "Gonzalo Gonzalez-Pumariega",
        "Wayne Chen",
        "Alexander M Rush",
        "Wenting Zhao",
        "Sanjiban Choudhury"
      ],
      "abstract": "We address the problem of code generation from multi-turn execution feedback.\nExisting methods either generate code without feedback or use complex,\nhierarchical reinforcement learning to optimize multi-turn rewards. We propose\na simple yet scalable approach, $\\mu$Code, that solves multi-turn code\ngeneration using only single-step rewards. Our key insight is that code\ngeneration is a one-step recoverable MDP, where the correct code can be\nrecovered from any intermediate code state in a single turn. $\\mu$Code\niteratively trains both a generator to provide code solutions conditioned on\nmulti-turn execution feedback and a verifier to score the newly generated code.\nExperimental evaluations show that our approach achieves significant\nimprovements over the state-of-the-art baselines. We provide analysis of the\ndesign choices of the reward models and policy, and show the efficacy of\n$\\mu$Code at utilizing the execution feedback. Our code is available at\nhttps://github.com/portal-cornell/muCode.",
      "pdf_url": "http://arxiv.org/pdf/2502.20380v1",
      "published": "2025-02-27T18:55:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20380v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers",
      "authors": [
        "Shalev Lifshitz",
        "Sheila A. McIlraith",
        "Yilun Du"
      ],
      "abstract": "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
      "pdf_url": "http://arxiv.org/pdf/2502.20379v1",
      "published": "2025-02-27T18:53:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20379v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation",
      "authors": [
        "Albert Gong",
        "Kamilė Stankevičiūtė",
        "Chao Wan",
        "Anmol Kabra",
        "Raphael Thesmar",
        "Johann Lee",
        "Julius Klenke",
        "Carla P. Gomes",
        "Kilian Q. Weinberger"
      ],
      "abstract": "High-quality benchmarks are essential for evaluating reasoning and retrieval\ncapabilities of large language models (LLMs). However, curating datasets for\nthis purpose is not a permanent solution as they are prone to data leakage and\ninflated performance results. To address these challenges, we propose\nPhantomWiki: a pipeline to generate unique, factually consistent document\ncorpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is\nneither a fixed dataset, nor is it based on any existing data. Instead, a new\nPhantomWiki instance is generated on demand for each evaluation. We vary the\nquestion difficulty and corpus size to disentangle reasoning and retrieval\ncapabilities respectively, and find that PhantomWiki datasets are surprisingly\nchallenging for frontier LLMs. Thus, we contribute a scalable and data\nleakage-resistant framework for disentangled evaluation of reasoning,\nretrieval, and tool-use abilities. Our code is available at\nhttps://github.com/kilian-group/phantom-wiki.",
      "pdf_url": "http://arxiv.org/pdf/2502.20377v1",
      "published": "2025-02-27T18:51:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20377v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization",
      "authors": [
        "Ryan C. Barron",
        "Maksim E. Eren",
        "Olga M. Serafimova",
        "Cynthia Matuszek",
        "Boian S. Alexandrov"
      ],
      "abstract": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI.",
      "pdf_url": "http://arxiv.org/pdf/2502.20364v1",
      "published": "2025-02-27T18:35:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20364v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs",
      "authors": [
        "Kuan Lok Zhou",
        "Jiayi Chen",
        "Siddharth Suresh",
        "Reuben Narad",
        "Timothy T. Rogers",
        "Lalit K Jain",
        "Robert D Nowak",
        "Bob Mankoff",
        "Jifan Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have shown significant limitations in\nunderstanding creative content, as demonstrated by Hessel et al. (2023)'s\ninfluential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study\nexposed a substantial gap between LLMs and humans in humor comprehension,\nestablishing that understanding and evaluating creative content is key\nchallenge in AI development. We revisit this challenge by decomposing humor\nunderstanding into three components and systematically improve each: enhancing\nvisual understanding through improved annotation, utilizing LLM-generated humor\nreasoning and explanations, and implementing targeted alignment with human\npreference data. Our refined approach achieves 82.4% accuracy in caption\nranking, singificantly improving upon the previous 67% benchmark and matching\nthe performance of world-renowned human experts in this domain. Notably, while\nattempts to mimic subgroup preferences through various persona prompts showed\nminimal impact, model finetuning with crowd preferences proved remarkably\neffective. These findings reveal that LLM limitations in creative judgment can\nbe effectively addressed through focused alignment to specific subgroups and\nindividuals. Lastly, we propose the position that achieving artificial general\nintelligence necessitates systematic collection of human preference data across\ncreative domains. We advocate that just as human creativity is deeply\ninfluenced by individual and cultural preferences, training LLMs with diverse\nhuman preference data may be essential for developing true creative\nunderstanding.",
      "pdf_url": "http://arxiv.org/pdf/2502.20356v1",
      "published": "2025-02-27T18:29:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20356v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Towards Responsible AI in Education: Hybrid Recommendation System for K-12 Students Case Study",
      "authors": [
        "Nazarii Drushchak",
        "Vladyslava Tyshchenko",
        "Nataliya Polyakovska"
      ],
      "abstract": "The growth of Educational Technology (EdTech) has enabled highly personalized\nlearning experiences through Artificial Intelligence (AI)-based recommendation\nsystems tailored to each student needs. However, these systems can\nunintentionally introduce biases, potentially limiting fair access to learning\nresources. This study presents a recommendation system for K-12 students,\ncombining graph-based modeling and matrix factorization to provide personalized\nsuggestions for extracurricular activities, learning resources, and\nvolunteering opportunities. To address fairness concerns, the system includes a\nframework to detect and reduce biases by analyzing feedback across protected\nstudent groups. This work highlights the need for continuous monitoring in\neducational recommendation systems to support equitable, transparent, and\neffective learning opportunities for all students.",
      "pdf_url": "http://arxiv.org/pdf/2502.20354v1",
      "published": "2025-02-27T18:27:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20354v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Naturalistic Computational Cognitive Science: Towards generalizable models and theories that capture the full range of natural behavior",
      "authors": [
        "Wilka Carvalho",
        "Andrew Lampinen"
      ],
      "abstract": "Artificial Intelligence increasingly pursues large, complex models that\nperform many tasks within increasingly realistic domains. How, if at all,\nshould these developments in AI influence cognitive science?\n  We argue that progress in AI offers timely opportunities for cognitive\nscience to embrace experiments with increasingly naturalistic stimuli, tasks,\nand behaviors; and computational models that can accommodate these changes. We\nfirst review a growing body of research spanning neuroscience, cognitive\nscience, and AI that suggests that incorporating a broader range of\nnaturalistic experimental paradigms (and models that accommodate them) may be\nnecessary to resolve some aspects of natural intelligence and ensure that our\ntheories generalize. We then suggest that integrating recent progress in AI and\ncognitive science will enable us to engage with more naturalistic phenomena\nwithout giving up experimental control or the pursuit of theoretically grounded\nunderstanding. We offer practical guidance on how methodological practices can\ncontribute to cumulative progress in naturalistic computational cognitive\nscience, and illustrate a path towards building computational models that solve\nthe real problems of natural cognition - together with a reductive\nunderstanding of the processes and principles by which they do so.",
      "pdf_url": "http://arxiv.org/pdf/2502.20349v1",
      "published": "2025-02-27T18:20:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20349v1",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ]
    },
    {
      "title": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners",
      "authors": [
        "Daniele Paliotta",
        "Junxiong Wang",
        "Matteo Pagliardini",
        "Kevin Y. Li",
        "Aviv Bick",
        "J. Zico Kolter",
        "Albert Gu",
        "François Fleuret",
        "Tri Dao"
      ],
      "abstract": "Recent advancements have demonstrated that the performance of large language\nmodels (LLMs) can be significantly enhanced by scaling computational resources\nat test time. A common strategy involves generating multiple Chain-of-Thought\n(CoT) trajectories and aggregating their outputs through various selection\nmechanisms. This raises a fundamental question: can models with lower\ncomplexity leverage their superior generation throughput to outperform\nsimilarly sized Transformers for a fixed computational budget? To address this\nquestion and overcome the lack of strong subquadratic reasoners, we distill\npure and hybrid Mamba models from pretrained Transformers. Trained on only 8\nbillion tokens, our distilled models show strong performance and scaling on\nmathematical reasoning datasets while being much faster at inference for large\nbatches and long sequences. Despite the zero-shot performance hit due to\ndistillation, both pure and hybrid Mamba models can scale their coverage and\naccuracy performance past their Transformer teacher models under fixed time\nbudgets, opening a new direction for scaling inference compute.",
      "pdf_url": "http://arxiv.org/pdf/2502.20339v1",
      "published": "2025-02-27T18:08:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20339v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Expertise Is What We Want",
      "authors": [
        "Alan Ashworth",
        "Munir Al-Dajani",
        "Keegan Duchicela",
        "Kiril Kafadarov",
        "Allison Kurian",
        "Othman Laraki",
        "Amina Lazrak",
        "Divneet Mandair",
        "Wendy McKennon",
        "Rebecca Miksad",
        "Jayodita Sanghvi",
        "Travis Zack"
      ],
      "abstract": "Clinical decision-making depends on expert reasoning, which is guided by\nstandardized, evidence-based guidelines. However, translating these guidelines\ninto automated clinical decision support systems risks inaccuracy and\nimportantly, loss of nuance. We share an application architecture, the Large\nLanguage Expert (LLE), that combines the flexibility and power of Large\nLanguage Models (LLMs) with the interpretability, explainability, and\nreliability of Expert Systems. LLMs help address key challenges of Expert\nSystems, such as integrating and codifying knowledge, and data normalization.\nConversely, an Expert System-like approach helps overcome challenges with LLMs,\nincluding hallucinations, atomic and inexpensive updates, and testability.\n  To highlight the power of the Large Language Expert (LLE) system, we built an\nLLE to assist with the workup of patients newly diagnosed with cancer. Timely\ninitiation of cancer treatment is critical for optimal patient outcomes.\nHowever, increasing complexity in diagnostic recommendations has made it\ndifficult for primary care physicians to ensure their patients have completed\nthe necessary workup before their first visit with an oncologist. As with many\nreal-world clinical tasks, these workups require the analysis of unstructured\nhealth records and the application of nuanced clinical decision logic. In this\nstudy, we describe the design & evaluation of an LLE system built to rapidly\nidentify and suggest the correct diagnostic workup. The system demonstrated a\nhigh degree of clinical-level accuracy (>95%) and effectively addressed gaps\nidentified in real-world data from breast and colon cancer patients at a large\nacademic center.",
      "pdf_url": "http://arxiv.org/pdf/2502.20335v1",
      "published": "2025-02-27T18:05:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20335v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; J.3"
      ]
    },
    {
      "title": "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models",
      "authors": [
        "Yukang Yang",
        "Declan Campbell",
        "Kaixuan Huang",
        "Mengdi Wang",
        "Jonathan Cohen",
        "Taylor Webb"
      ],
      "abstract": "Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms.",
      "pdf_url": "http://arxiv.org/pdf/2502.20332v1",
      "published": "2025-02-27T18:02:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20332v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application",
      "authors": [
        "Thomas Hickling",
        "Maxwell Hogan",
        "Abdulla Tammam",
        "Nabil Aouf"
      ],
      "abstract": "This paper proposes a holistic framework for autonomous guidance, navigation,\nand task distribution among multi-drone systems operating in Global Navigation\nSatellite System (GNSS)-denied indoor settings. We advocate for a Deep\nReinforcement Learning (DRL)-based guidance mechanism, utilising the Twin\nDelayed Deep Deterministic Policy Gradient algorithm. To improve the efficiency\nof the training process, we incorporate an Artificial Potential Field\n(APF)-based reward structure, enabling the agent to refine its movements,\nthereby promoting smoother paths and enhanced obstacle avoidance in indoor\ncontexts. Furthermore, we tackle the issue of task distribution among\ncooperative UAVs through a DRL-trained Graph Convolutional Network (GCN). This\nGCN represents the interactions between drones and tasks, facilitating dynamic\nand real-time task allocation that reflects the current environmental\nconditions and the capabilities of the drones. Such an approach fosters\neffective coordination and collaboration among multiple drones during search\nand rescue operations or other exploratory endeavours. Lastly, to ensure\nprecise odometry in environments lacking GNSS, we employ Light Detection And\nRanging Simultaneous Localisation and Mapping complemented by a depth camera to\nmitigate the hallway problem. This integration offers robust localisation and\nmapping functionalities, thereby enhancing the systems dependability in indoor\nnavigation. The proposed multi-drone framework not only elevates individual\nnavigation capabilities but also optimises coordinated task allocation in\ncomplex, obstacle-laden environments. Experimental evaluations conducted in a\nsetup tailored to meet the requirements of the NATO Sapience Autonomous\nCooperative Drone Competition demonstrate the efficacy of the proposed system,\nyielding outstanding results and culminating in a first-place finish in the\n2024 Sapience competition.",
      "pdf_url": "http://arxiv.org/pdf/2502.20326v1",
      "published": "2025-02-27T17:53:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20326v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
      "authors": [
        "Chuofan Ma",
        "Yi Jiang",
        "Junfeng Wu",
        "Jihan Yang",
        "Xin Yu",
        "Zehuan Yuan",
        "Bingyue Peng",
        "Xiaojuan Qi"
      ],
      "abstract": "The representation disparity between visual generation and understanding\nimposes a critical gap in integrating these capabilities into a single\nframework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer\nthat encodes fine-grained details for generation while also capturing\nhigh-level semantics for understanding. Despite recent studies have shown that\nthese objectives could induce loss conflicts in training, we reveal that the\nunderlying bottleneck stems from limited representational capacity of discrete\ntokens. We address this by introducing multi-codebook quantization, which\ndivides vector quantization with several independent sub-codebooks to expand\nthe latent feature space, while avoiding training instability caused by\noverlarge codebooks. Our method significantly raises the upper limit of unified\ndiscrete tokenizers to match or even surpass domain-specific continuous\ntokenizers. For instance, UniTok achieves a remarkable rFID of 0.38 (versus\n0.87 for SD-VAE) and a zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on\nImageNet. Our code is available at https://github.com/FoundationVision/UniTok.",
      "pdf_url": "http://arxiv.org/pdf/2502.20321v1",
      "published": "2025-02-27T17:47:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20321v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
      "authors": [
        "Yongjia Lei",
        "Haoyu Han",
        "Ryan A. Rossi",
        "Franck Dernoncourt",
        "Nedim Lipka",
        "Mahantesh M Halappanavar",
        "Jiliang Tang",
        "Yu Wang"
      ],
      "abstract": "Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for\nanswering queries by providing textual and structural knowledge. However,\ncurrent retrieval methods often retrieve these two types of knowledge in\nisolation without considering their mutual reinforcement and some hybrid\nmethods even bypass structural retrieval entirely after neighboring\naggregation. To fill in this gap, we propose a Mixture of\nStructural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge\nvia a Planning-Reasoning-Organizing framework. In the Planning stage, MoR\ngenerates textual planning graphs delineating the logic for answering queries.\nFollowing planning graphs, in the Reasoning stage, MoR interweaves structural\ntraversal and textual matching to obtain candidates from TG-KBs. In the\nOrganizing stage, MoR further reranks fetched candidates based on their\nstructural trajectory. Extensive experiments demonstrate the superiority of MoR\nin harmonizing structural and textual retrieval with insights, including uneven\nretrieving performance across different query logics and the benefits of\nintegrating structural trajectories for candidate reranking. Our code is\navailable at https://github.com/Yoega/MoR.",
      "pdf_url": "http://arxiv.org/pdf/2502.20317v1",
      "published": "2025-02-27T17:42:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20317v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds",
      "authors": [
        "Mohamed Abdelsamad",
        "Michael Ulrich",
        "Claudius Gläser",
        "Abhinav Valada"
      ],
      "abstract": "Masked autoencoders (MAE) have shown tremendous potential for self-supervised\nlearning (SSL) in vision and beyond. However, point clouds from LiDARs used in\nautomated driving are particularly challenging for MAEs since large areas of\nthe 3D volume are empty. Consequently, existing work suffers from leaking\noccupancy information into the decoder and has significant computational\ncomplexity, thereby limiting the SSL pre-training to only 2D bird's eye view\nencoders in practice. In this work, we propose the novel neighborhood occupancy\nMAE (NOMAE) that overcomes the aforementioned challenges by employing masked\noccupancy reconstruction only in the neighborhood of non-masked voxels. We\nincorporate voxel masking and occupancy reconstruction at multiple scales with\nour proposed hierarchical mask generation technique to capture features of\nobjects of different sizes in the point cloud. NOMAEs are extremely flexible\nand can be directly employed for SSL in existing 3D architectures. We perform\nextensive evaluations on the nuScenes and Waymo Open datasets for the\ndownstream perception tasks of semantic segmentation and 3D object detection,\ncomparing with both discriminative and generative SSL methods. The results\ndemonstrate that NOMAE sets the new state-of-the-art on multiple benchmarks for\nmultiple point cloud perception tasks.",
      "pdf_url": "http://arxiv.org/pdf/2502.20316v1",
      "published": "2025-02-27T17:42:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20316v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "LangProBe: a Language Programs Benchmark",
      "authors": [
        "Shangyin Tan",
        "Lakshya A Agrawal",
        "Arnav Singhvi",
        "Liheng Lai",
        "Michael J Ryan",
        "Dan Klein",
        "Omar Khattab",
        "Koushik Sen",
        "Matei Zaharia"
      ],
      "abstract": "Composing language models (LMs) into multi-step language programs and\nautomatically optimizing their modular prompts is now a mainstream paradigm for\nbuilding AI systems, but the tradeoffs in this space have only scarcely been\nstudied before. We introduce LangProBe, the first large-scale benchmark for\nevaluating the architectures and optimization strategies for language programs,\nwith over 2000 combinations of tasks, architectures, optimizers, and choices of\nLMs. Using LangProBe, we are the first to study the impact of program\narchitectures and optimizers (and their compositions together and with\ndifferent models) on tradeoffs of quality and cost. We find that optimized\nlanguage programs offer strong cost--quality Pareto improvement over raw calls\nto models, but simultaneously demonstrate that human judgment (or empirical\ndecisions) about which compositions to pursue is still necessary for best\nperformance. We will open source the code and evaluation data for LangProBe.",
      "pdf_url": "http://arxiv.org/pdf/2502.20315v1",
      "published": "2025-02-27T17:41:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20315v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
      "authors": [
        "Franck Cappello",
        "Sandeep Madireddy",
        "Robert Underwood",
        "Neil Getty",
        "Nicholas Lee-Ping Chia",
        "Nesar Ramachandra",
        "Josh Nguyen",
        "Murat Keceli",
        "Tanwi Mallick",
        "Zilinghan Li",
        "Marieme Ngom",
        "Chenhui Zhang",
        "Angel Yanguas-Gil",
        "Evan Antoniuk",
        "Bhavya Kailkhura",
        "Minyang Tian",
        "Yufeng Du",
        "Yuan-Sen Ting",
        "Azton Wells",
        "Bogdan Nicolae",
        "Avinash Maurya",
        "M. Mustafa Rafique",
        "Eliu Huerta",
        "Bo Li",
        "Ian Foster",
        "Rick Stevens"
      ],
      "abstract": "Recent advancements have positioned AI, and particularly Large Language\nModels (LLMs), as transformative tools for scientific research, capable of\naddressing complex tasks that require reasoning, problem-solving, and\ndecision-making. Their exceptional capabilities suggest their potential as\nscientific research assistants but also highlight the need for holistic,\nrigorous, and domain-specific evaluation to assess effectiveness in real-world\nscientific applications. This paper describes a multifaceted methodology for\nEvaluating AI models as scientific Research Assistants (EAIRA) developed at\nArgonne National Laboratory. This methodology incorporates four primary classes\nof evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open\nResponse to evaluate advanced reasoning and problem-solving skills; 3)\nLab-Style Experiments involving detailed analysis of capabilities as research\nassistants in controlled environments; and 4) Field-Style Experiments to\ncapture researcher-LLM interactions at scale in a wide range of scientific\ndomains and applications. These complementary methods enable a comprehensive\nanalysis of LLM strengths and weaknesses with respect to their scientific\nknowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of\nLLM advancements, we designed the methodology to evolve and adapt so as to\nensure its continued relevance and applicability. This paper describes the\nmethodology state at the end of February 2025. Although developed within a\nsubset of scientific domains, the methodology is designed to be generalizable\nto a wide range of scientific domains.",
      "pdf_url": "http://arxiv.org/pdf/2502.20309v1",
      "published": "2025-02-27T17:35:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20309v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging",
      "authors": [
        "Jinghao Feng",
        "Qiaoyu Zheng",
        "Chaoyi Wu",
        "Ziheng Zhao",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
      ],
      "abstract": "Agentic AI systems have gained significant attention for their ability to\nautonomously perform complex tasks. However, their reliance on well-prepared\ntools limits their applicability in the medical domain, which requires to train\nspecialized models. In this paper, we make three contributions: (i) We present\nM3Builder, a novel multi-agent system designed to automate machine learning\n(ML) in medical imaging. At its core, M3Builder employs four specialized agents\nthat collaborate to tackle complex, multi-step medical ML workflows, from\nautomated data processing and environment configuration to self-contained auto\ndebugging and model training. These agents operate within a medical imaging ML\nworkspace, a structured environment designed to provide agents with free-text\ndescriptions of datasets, training codes, and interaction tools, enabling\nseamless communication and task execution. (ii) To evaluate progress in\nautomated medical imaging ML, we propose M3Bench, a benchmark comprising four\ngeneral tasks on 14 training datasets, across five anatomies and three imaging\nmodalities, covering both 2D and 3D data. (iii) We experiment with seven\nstate-of-the-art large language models serving as agent cores for our system,\nsuch as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic\ndesigns, M3Builder shows superior performance on completing ML tasks in medical\nimaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent\ncore, showing huge potential towards fully automated machine learning in\nmedical imaging.",
      "pdf_url": "http://arxiv.org/pdf/2502.20301v1",
      "published": "2025-02-27T17:29:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20301v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "An exploration of features to improve the generalisability of fake news detection models",
      "authors": [
        "Nathaniel Hoy",
        "Theodora Koulouri"
      ],
      "abstract": "Fake news poses global risks by influencing elections and spreading\nmisinformation, making detection critical. Existing NLP and supervised Machine\nLearning methods perform well under cross-validation but struggle to generalise\nacross datasets, even within the same domain. This issue stems from coarsely\nlabelled training data, where articles are labelled based on their publisher,\nintroducing biases that token-based models like TF-IDF and BERT are sensitive\nto. While Large Language Models (LLMs) offer promise, their application in fake\nnews detection remains limited. This study demonstrates that meaningful\nfeatures can still be extracted from coarsely labelled data to improve\nreal-world robustness. Stylistic features-lexical, syntactic, and semantic-are\nexplored due to their reduced sensitivity to dataset biases. Additionally,\nnovel social-monetisation features are introduced, capturing economic\nincentives behind fake news, such as advertisements, external links, and social\nmedia elements. The study trains on the coarsely labelled NELA 2020-21 dataset\nand evaluates using the manually labelled Facebook URLs dataset, a gold\nstandard for generalisability. Results highlight the limitations of token-based\nmodels trained on biased data and contribute to the scarce evidence on LLMs\nlike LLaMa in this field. Findings indicate that stylistic and\nsocial-monetisation features offer more generalisable predictions than\ntoken-based methods and LLMs. Statistical and permutation feature importance\nanalyses further reveal their potential to enhance performance and mitigate\ndataset biases, providing a path forward for improving fake news detection.",
      "pdf_url": "http://arxiv.org/pdf/2502.20299v1",
      "published": "2025-02-27T17:26:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20299v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page Handwritten Document Transcription",
      "authors": [
        "Benjamin Gutteridge",
        "Matthew Thomas Jackson",
        "Toni Kukurin",
        "Xiaowen Dong"
      ],
      "abstract": "Handwritten text recognition (HTR) remains a challenging task, particularly\nfor multi-page documents where pages share common formatting and contextual\nfeatures. While modern optical character recognition (OCR) engines are\nproficient with printed text, their performance on handwriting is limited,\noften requiring costly labeled data for fine-tuning. In this paper, we explore\nthe use of multi-modal large language models (MLLMs) for transcribing\nmulti-page handwritten documents in a zero-shot setting. We investigate various\nconfigurations of commercial OCR engines and MLLMs, utilizing the latter both\nas end-to-end transcribers and as post-processors, with and without image\ncomponents. We propose a novel method, '+first page', which enhances MLLM\ntranscription by providing the OCR output of the entire document along with\njust the first page image. This approach leverages shared document features\nwithout incurring the high cost of processing all images. Experiments on a\nmulti-page version of the IAM Handwriting Database demonstrate that '+first\npage' improves transcription accuracy, balances cost with performance, and even\nenhances results on out-of-sample text by extrapolating formatting and OCR\nerror patterns from a single page.",
      "pdf_url": "http://arxiv.org/pdf/2502.20295v1",
      "published": "2025-02-27T17:21:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20295v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Evaluating Human Trust in LLM-Based Planners: A Preliminary Study",
      "authors": [
        "Shenghui Chen",
        "Yunhao Yang",
        "Kayla Boggess",
        "Seongkook Heo",
        "Lu Feng",
        "Ufuk Topcu"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used for planning tasks,\noffering unique capabilities not found in classical planners such as generating\nexplanations and iterative refinement. However, trust--a critical factor in the\nadoption of planning systems--remains underexplored in the context of LLM-based\nplanning tasks. This study bridges this gap by comparing human trust in\nLLM-based planners with classical planners through a user study in a Planning\nDomain Definition Language (PDDL) domain. Combining subjective measures, such\nas trust questionnaires, with objective metrics like evaluation accuracy, our\nfindings reveal that correctness is the primary driver of trust and\nperformance. Explanations provided by the LLM improved evaluation accuracy but\nhad limited impact on trust, while plan refinement showed potential for\nincreasing trust without significantly enhancing evaluation accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2502.20284v1",
      "published": "2025-02-27T17:10:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20284v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Explainable, Multi-modal Wound Infection Classification from Images Augmented with Generated Captions",
      "authors": [
        "Palawat Busaranuvong",
        "Emmanuel Agu",
        "Reza Saadati Fard",
        "Deepak Kumar",
        "Shefalika Gautam",
        "Bengisu Tulu",
        "Diane Strong"
      ],
      "abstract": "Infections in Diabetic Foot Ulcers (DFUs) can cause severe complications,\nincluding tissue death and limb amputation, highlighting the need for accurate,\ntimely diagnosis. Previous machine learning methods have focused on identifying\ninfections by analyzing wound images alone, without utilizing additional\nmetadata such as medical notes. In this study, we aim to improve infection\ndetection by introducing Synthetic Caption Augmented Retrieval for Wound\nInfection Detection (SCARWID), a novel deep learning framework that leverages\nsynthetic textual descriptions to augment DFU images. SCARWID consists of two\ncomponents: (1) Wound-BLIP, a Vision-Language Model (VLM) fine-tuned on\nGPT-4o-generated descriptions to synthesize consistent captions from images;\nand (2) an Image-Text Fusion module that uses cross-attention to extract\ncross-modal embeddings from an image and its corresponding Wound-BLIP caption.\nInfection status is determined by retrieving the top-k similar items from a\nlabeled support set. To enhance the diversity of training data, we utilized a\nlatent diffusion model to generate additional wound images. As a result,\nSCARWID outperformed state-of-the-art models, achieving average sensitivity,\nspecificity, and accuracy of 0.85, 0.78, and 0.81, respectively, for wound\ninfection classification. Displaying the generated captions alongside the wound\nimages and infection detection results enhances interpretability and trust,\nenabling nurses to align SCARWID outputs with their medical knowledge. This is\nparticularly valuable when wound notes are unavailable or when assisting novice\nnurses who may find it difficult to identify visual attributes of wound\ninfection.",
      "pdf_url": "http://arxiv.org/pdf/2502.20277v1",
      "published": "2025-02-27T17:04:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20277v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "HVI: A New color space for Low-light Image Enhancement",
      "authors": [
        "Qingsen Yan",
        "Yixu Feng",
        "Cheng Zhang",
        "Guansong Pang",
        "Kangbiao Shi",
        "Peng Wu",
        "Wei Dong",
        "Jinqiu Sun",
        "Yanning Zhang"
      ],
      "abstract": "Low-Light Image Enhancement (LLIE) is a crucial computer vision task that\naims to restore detailed visual information from corrupted low-light images.\nMany existing LLIE methods are based on standard RGB (sRGB) space, which often\nproduce color bias and brightness artifacts due to inherent high color\nsensitivity in sRGB. While converting the images using Hue, Saturation and\nValue (HSV) color space helps resolve the brightness issue, it introduces\nsignificant red and black noise artifacts. To address this issue, we propose a\nnew color space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined\nby polarized HS maps and learnable intensity. The former enforces small\ndistances for red coordinates to remove the red artifacts, while the latter\ncompresses the low-light regions to remove the black artifacts. To fully\nleverage the chromatic and intensity information, a novel Color and Intensity\nDecoupling Network (CIDNet) is further introduced to learn accurate photometric\nmapping function under different lighting conditions in the HVI space.\nComprehensive results from benchmark and ablation experiments show that the\nproposed HVI color space with CIDNet outperforms the state-of-the-art methods\non 10 datasets. The code is available at https://github.com/Fediory/HVI-CIDNet.",
      "pdf_url": "http://arxiv.org/pdf/2502.20272v1",
      "published": "2025-02-27T16:59:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20272v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Large Language Models as Attribution Regularizers for Efficient Model Training",
      "authors": [
        "Davor Vukadin",
        "Marin Šilić",
        "Goran Delač"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.",
      "pdf_url": "http://arxiv.org/pdf/2502.20268v1",
      "published": "2025-02-27T16:55:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20268v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6"
      ]
    },
    {
      "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
      "authors": [
        "Amr Mohamed",
        "Mingmeng Geng",
        "Michalis Vazirgiannis",
        "Guokan Shang"
      ],
      "abstract": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows.",
      "pdf_url": "http://arxiv.org/pdf/2502.20258v1",
      "published": "2025-02-27T16:46:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20258v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks",
      "authors": [
        "Gianluca Bencomo",
        "Max Gupta",
        "Ioana Marinescu",
        "R. Thomas McCoy",
        "Thomas L. Griffiths"
      ],
      "abstract": "Artificial neural networks can acquire many aspects of human knowledge from\ndata, making them promising as models of human learning. But what those\nnetworks can learn depends upon their inductive biases -- the factors other\nthan the data that influence the solutions they discover -- and the inductive\nbiases of neural networks remain poorly understood, limiting our ability to\ndraw conclusions about human learning from the performance of these systems.\nCognitive scientists and machine learning researchers often focus on the\narchitecture of a neural network as a source of inductive bias. In this paper\nwe explore the impact of another source of inductive bias -- the initial\nweights of the network -- using meta-learning as a tool for finding initial\nweights that are adapted for specific problems. We evaluate four widely-used\narchitectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430\ndifferent models across three tasks requiring different biases and forms of\ngeneralization. We find that meta-learning can substantially reduce or entirely\neliminate performance differences across architectures and data\nrepresentations, suggesting that these factors may be less important as sources\nof inductive bias than is typically assumed. When differences are present,\narchitectures and data representations that perform well without meta-learning\ntend to meta-train more effectively. Moreover, all architectures generalize\npoorly on problems that are far from their meta-training experience,\nunderscoring the need for stronger inductive biases for robust generalization.",
      "pdf_url": "http://arxiv.org/pdf/2502.20237v1",
      "published": "2025-02-27T16:22:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20237v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Selective Use of Yannakakis' Algorithm to Improve Query Performance: Machine Learning to the Rescue",
      "authors": [
        "Daniela Böhm",
        "Georg Gottlob",
        "Matthias Lanzinger",
        "Davide Longo",
        "Cem Okulmus",
        "Reinhard Pichler",
        "Alexander Selzer"
      ],
      "abstract": "Query optimization has played a central role in database research for\ndecades. However, more often than not, the proposed optimization techniques\nlead to a performance improvement in some, but not in all, situations.\nTherefore, we urgently need a methodology for designing a decision procedure\nthat decides for a given query whether the optimization technique should be\napplied or not.\n  In this work, we propose such a methodology with a focus on Yannakakis-style\nquery evaluation as our optimization technique of interest. More specifically,\nwe formulate this decision problem as an algorithm selection problem and we\npresent a Machine Learning based approach for its solution. Empirical results\nwith several benchmarks on a variety of database systems show that our approach\nindeed leads to a statistically significant performance improvement.",
      "pdf_url": "http://arxiv.org/pdf/2502.20233v1",
      "published": "2025-02-27T16:19:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20233v1",
      "categories": [
        "cs.DB",
        "cs.AI"
      ]
    },
    {
      "title": "AI Will Always Love You: Studying Implicit Biases in Romantic AI Companions",
      "authors": [
        "Clare Grogan",
        "Jackie Kay",
        "María Pérez-Ortiz"
      ],
      "abstract": "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway.",
      "pdf_url": "http://arxiv.org/pdf/2502.20231v1",
      "published": "2025-02-27T16:16:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20231v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "RURANET++: An Unsupervised Learning Method for Diabetic Macular Edema Based on SCSE Attention Mechanisms and Dynamic Multi-Projection Head Clustering",
      "authors": [
        "Wei Yang",
        "Yiran Zhu",
        "Jiayu Shen",
        "Yuhan Tang",
        "Chengchang Pan",
        "Hui He",
        "Yan Su",
        "Honggang Qi"
      ],
      "abstract": "Diabetic Macular Edema (DME), a prevalent complication among diabetic\npatients, constitutes a major cause of visual impairment and blindness.\nAlthough deep learning has achieved remarkable progress in medical image\nanalysis, traditional DME diagnosis still relies on extensive annotated data\nand subjective ophthalmologist assessments, limiting practical applications. To\naddress this, we present RURANET++, an unsupervised learning-based automated\nDME diagnostic system. This framework incorporates an optimized U-Net\narchitecture with embedded Spatial and Channel Squeeze & Excitation (SCSE)\nattention mechanisms to enhance lesion feature extraction. During feature\nprocessing, a pre-trained GoogLeNet model extracts deep features from retinal\nimages, followed by PCA-based dimensionality reduction to 50 dimensions for\ncomputational efficiency. Notably, we introduce a novel clustering algorithm\nemploying multi-projection heads to explicitly control cluster diversity while\ndynamically adjusting similarity thresholds, thereby optimizing intra-class\nconsistency and inter-class discrimination. Experimental results demonstrate\nsuperior performance across multiple metrics, achieving maximum accuracy\n(0.8411), precision (0.8593), recall (0.8411), and F1-score (0.8390), with\nexceptional clustering quality. This work provides an efficient unsupervised\nsolution for DME diagnosis with significant clinical implications.",
      "pdf_url": "http://arxiv.org/pdf/2502.20224v1",
      "published": "2025-02-27T16:06:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20224v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Deep Convolutional Neural Networks for Palm Fruit Maturity Classification",
      "authors": [
        "Mingqiang Han",
        "Chunlin Yi"
      ],
      "abstract": "To maximize palm oil yield and quality, it is essential to harvest palm fruit\nat the optimal maturity stage. This project aims to develop an automated\ncomputer vision system capable of accurately classifying palm fruit images into\nfive ripeness levels. We employ deep Convolutional Neural Networks (CNNs) to\nclassify palm fruit images based on their maturity stage. A shallow CNN serves\nas the baseline model, while transfer learning and fine-tuning are applied to\npre-trained ResNet50 and InceptionV3 architectures. The study utilizes a\npublicly available dataset of over 8,000 images with significant variations,\nwhich is split into 80\\% for training and 20\\% for testing. The proposed deep\nCNN models achieve test accuracies exceeding 85\\% in classifying palm fruit\nmaturity stages. This research highlights the potential of deep learning for\nautomating palm fruit ripeness assessment, which can contribute to optimizing\nharvesting decisions and improving palm oil production efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2502.20223v1",
      "published": "2025-02-27T16:06:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20223v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "DIPSER: A Dataset for In-Person Student1 Engagement Recognition in the Wild",
      "authors": [
        "Luis Marquez-Carpintero",
        "Sergio Suescun-Ferrandiz",
        "Carolina Lorenzo Álvarez",
        "Jorge Fernandez-Herrero",
        "Diego Viejo",
        "Rosabel Roig-Vila",
        "Miguel Cazorla"
      ],
      "abstract": "In this paper, a novel dataset is introduced, designed to assess student\nattention within in-person classroom settings. This dataset encompasses RGB\ncamera data, featuring multiple cameras per student to capture both posture and\nfacial expressions, in addition to smartwatch sensor data for each individual.\nThis dataset allows machine learning algorithms to be trained to predict\nattention and correlate it with emotion. A comprehensive suite of attention and\nemotion labels for each student is provided, generated through self-reporting\nas well as evaluations by four different experts. Our dataset uniquely combines\nfacial and environmental camera data, smartwatch metrics, and includes\nunderrepresented ethnicities in similar datasets, all within in-the-wild,\nin-person settings, making it the most comprehensive dataset of its kind\ncurrently available.\n  The dataset presented offers an extensive and diverse collection of data\npertaining to student interactions across different educational contexts,\naugmented with additional metadata from other tools. This initiative addresses\nexisting deficiencies by offering a valuable resource for the analysis of\nstudent attention and emotion in face-to-face lessons.",
      "pdf_url": "http://arxiv.org/pdf/2502.20209v1",
      "published": "2025-02-27T15:50:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20209v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Highly Parallelized Reinforcement Learning Training with Relaxed Assignment Dependencies",
      "authors": [
        "Zhouyu He",
        "Peng Qiao",
        "Rongchun Li",
        "Yong Dou",
        "Yusong Tan"
      ],
      "abstract": "As the demands for superior agents grow, the training complexity of Deep\nReinforcement Learning (DRL) becomes higher. Thus, accelerating training of DRL\nhas become a major research focus. Dividing the DRL training process into\nsubtasks and using parallel computation can effectively reduce training costs.\nHowever, current DRL training systems lack sufficient parallelization due to\ndata assignment between subtask components. This assignment issue has been\nignored, but addressing it can further boost training efficiency. Therefore, we\npropose a high-throughput distributed RL training system called TianJi. It\nrelaxes assignment dependencies between subtask components and enables\nevent-driven asynchronous communication. Meanwhile, TianJi maintains clear\nboundaries between subtask components. To address convergence uncertainty from\nrelaxed assignment dependencies, TianJi proposes a distributed strategy based\non the balance of sample production and consumption. The strategy controls the\nstaleness of samples to correct their quality, ensuring convergence. We\nconducted extensive experiments. TianJi achieves a convergence time\nacceleration ratio of up to 4.37 compared to related comparison systems. When\nscaled to eight computational nodes, TianJi shows a convergence time speedup of\n1.6 and a throughput speedup of 7.13 relative to XingTian, demonstrating its\ncapability to accelerate training and scalability. In data transmission\nefficiency experiments, TianJi significantly outperforms other systems,\napproaching hardware limits. TianJi also shows effectiveness in on-policy\nalgorithms, achieving convergence time acceleration ratios of 4.36 and 2.95\ncompared to RLlib and XingTian. TianJi is accessible at\nhttps://github.com/HiPRL/TianJi.git.",
      "pdf_url": "http://arxiv.org/pdf/2502.20190v1",
      "published": "2025-02-27T15:23:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20190v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs",
      "authors": [
        "Kaustubh Vyas",
        "Damien Graux",
        "Sébastien Montella",
        "Pavlos Vougiouklis",
        "Ruofei Lai",
        "Keshuang Li",
        "Yang Ren",
        "Jeff Z. Pan"
      ],
      "abstract": "In recent advancements, large language models (LLMs) have exhibited\nproficiency in code generation and chain-of-thought reasoning, laying the\ngroundwork for tackling automatic formal planning tasks. This study evaluates\nthe potential of LLMs to understand and generate Planning Domain Definition\nLanguage (PDDL), an essential representation in artificial intelligence\nplanning. We conduct an extensive analysis across 20 distinct models spanning 7\nmajor LLM families, both commercial and open-source. Our comprehensive\nevaluation sheds light on the zero-shot LLM capabilities of parsing,\ngenerating, and reasoning with PDDL. Our findings indicate that while some\nmodels demonstrate notable effectiveness in handling PDDL, others pose\nlimitations in more complex scenarios requiring nuanced planning knowledge.\nThese results highlight the promise and current limitations of LLMs in formal\nplanning tasks, offering insights into their application and guiding future\nefforts in AI-driven planning paradigms.",
      "pdf_url": "http://arxiv.org/pdf/2502.20175v1",
      "published": "2025-02-27T15:13:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20175v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Accelerating Model-Based Reinforcement Learning with State-Space World Models",
      "authors": [
        "Maria Krinner",
        "Elie Aljalbout",
        "Angel Romero",
        "Davide Scaramuzza"
      ],
      "abstract": "Reinforcement learning (RL) is a powerful approach for robot learning.\nHowever, model-free RL (MFRL) requires a large number of environment\ninteractions to learn successful control policies. This is due to the noisy RL\ntraining updates and the complexity of robotic systems, which typically involve\nhighly non-linear dynamics and noisy sensor signals. In contrast, model-based\nRL (MBRL) not only trains a policy but simultaneously learns a world model that\ncaptures the environment's dynamics and rewards. The world model can either be\nused for planning, for data collection, or to provide first-order policy\ngradients for training. Leveraging a world model significantly improves sample\nefficiency compared to model-free RL. However, training a world model alongside\nthe policy increases the computational complexity, leading to longer training\ntimes that are often intractable for complex real-world scenarios. In this\nwork, we propose a new method for accelerating model-based RL using state-space\nworld models. Our approach leverages state-space models (SSMs) to parallelize\nthe training of the dynamics model, which is typically the main computational\nbottleneck. Additionally, we propose an architecture that provides privileged\ninformation to the world model during training, which is particularly relevant\nfor partially observable environments. We evaluate our method in several\nreal-world agile quadrotor flight tasks, involving complex dynamics, for both\nfully and partially observable environments. We demonstrate a significant\nspeedup, reducing the world model training time by up to 10 times, and the\noverall MBRL training time by up to 4 times. This benefit comes without\ncompromising performance, as our method achieves similar sample efficiency and\ntask rewards to state-of-the-art MBRL methods.",
      "pdf_url": "http://arxiv.org/pdf/2502.20168v1",
      "published": "2025-02-27T15:05:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20168v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "stat.ML",
        "I.2.9; I.2.10; I.2.6"
      ]
    },
    {
      "title": "Adaptive H&E-IHC information fusion staining framework based on feature extra",
      "authors": [
        "Yifan Jia",
        "Xingda Yu",
        "Zhengyang Ji",
        "Songning Lai",
        "Yutao Yue"
      ],
      "abstract": "Immunohistochemistry (IHC) staining plays a significant role in the\nevaluation of diseases such as breast cancer. The H&E-to-IHC transformation\nbased on generative models provides a simple and cost-effective method for\nobtaining IHC images. Although previous models can perform digital coloring\nwell, they still suffer from (i) coloring only through the pixel features that\nare not prominent in HE, which is easy to cause information loss in the\ncoloring process; (ii) The lack of pixel-perfect H&E-IHC groundtruth pairs\nposes a challenge to the classical L1 loss.To address the above challenges, we\npropose an adaptive information enhanced coloring framework based on feature\nextractors. We first propose the VMFE module to effectively extract the color\ninformation features using multi-scale feature extraction and wavelet transform\nconvolution, while combining the shared decoder for feature fusion. The\nhigh-performance dual feature extractor of H&E-IHC is trained by contrastive\nlearning, which can effectively perform feature alignment of HE-IHC in high\nlatitude space. At the same time, the trained feature encoder is used to\nenhance the features and adaptively adjust the loss in the HE section staining\nprocess to solve the problems related to unclear and asymmetric information. We\nhave tested on different datasets and achieved excellent performance.Our code\nis available at https://github.com/babyinsunshine/CEFF",
      "pdf_url": "http://arxiv.org/pdf/2502.20156v1",
      "published": "2025-02-27T14:55:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20156v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "QPM: Discrete Optimization for Globally Interpretable Image Classification",
      "authors": [
        "Thomas Norrenbrock",
        "Timo Kaiser",
        "Sovan Biswas",
        "Ramesh Manuvinakurike",
        "Bodo Rosenhahn"
      ],
      "abstract": "Understanding the classifications of deep neural networks, e.g. used in\nsafety-critical situations, is becoming increasingly important. While recent\nmodels can locally explain a single decision, to provide a faithful global\nexplanation about an accurate model's general behavior is a more challenging\nopen task. Towards that goal, we introduce the Quadratic Programming Enhanced\nModel (QPM), which learns globally interpretable class representations. QPM\nrepresents every class with a binary assignment of very few, typically 5,\nfeatures, that are also assigned to other classes, ensuring easily comparable\ncontrastive class representations. This compact binary assignment is found\nusing discrete optimization based on predefined similarity measures and\ninterpretability constraints. The resulting optimal assignment is used to\nfine-tune the diverse features, so that each of them becomes the shared general\nconcept between the assigned classes. Extensive evaluations show that QPM\ndelivers unprecedented global interpretability across small and large-scale\ndatasets while setting the state of the art for the accuracy of interpretable\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2502.20130v1",
      "published": "2025-02-27T14:25:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20130v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
      "authors": [
        "Zexiong Ma",
        "Chao Peng",
        "Pengfei Gao",
        "Xiangxin Meng",
        "Yanzhen Zou",
        "Bing Xie"
      ],
      "abstract": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models.",
      "pdf_url": "http://arxiv.org/pdf/2502.20127v1",
      "published": "2025-02-27T14:19:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20127v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Exploring Open-world Continual Learning with Knowns-Unknowns Knowledge Transfer",
      "authors": [
        "Yujie Li",
        "Guannan Lai",
        "Xin Yang",
        "Yonghao Li",
        "Marcello Bonsangue",
        "Tianrui Li"
      ],
      "abstract": "Open-World Continual Learning (OWCL) is a challenging paradigm where models\nmust incrementally learn new knowledge without forgetting while operating under\nan open-world assumption. This requires handling incomplete training data and\nrecognizing unknown samples during inference. However, existing OWCL methods\noften treat open detection and continual learning as separate tasks, limiting\ntheir ability to integrate open-set detection and incremental classification in\nOWCL. Moreover, current approaches primarily focus on transferring knowledge\nfrom known samples, neglecting the insights derived from unknown/open samples.\nTo address these limitations, we formalize four distinct OWCL scenarios and\nconduct comprehensive empirical experiments to explore potential challenges in\nOWCL. Our findings reveal a significant interplay between the open detection of\nunknowns and incremental classification of knowns, challenging a widely held\nassumption that unknown detection and known classification are orthogonal\nprocesses. Building on our insights, we propose \\textbf{HoliTrans} (Holistic\nKnowns-Unknowns Knowledge Transfer), a novel OWCL framework that integrates\nnonlinear random projection (NRP) to create a more linearly separable embedding\nspace and distribution-aware prototypes (DAPs) to construct an adaptive\nknowledge space. Particularly, our HoliTrans effectively supports knowledge\ntransfer for both known and unknown samples while dynamically updating\nrepresentations of open samples during OWCL. Extensive experiments across\nvarious OWCL scenarios demonstrate that HoliTrans outperforms 22 competitive\nbaselines, bridging the gap between OWCL theory and practice and providing a\nrobust, scalable framework for advancing open-world learning paradigms.",
      "pdf_url": "http://arxiv.org/pdf/2502.20124v1",
      "published": "2025-02-27T14:16:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20124v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Self-Training Elicits Concise Reasoning in Large Language Models",
      "authors": [
        "Tergel Munkhbat",
        "Namgyu Ho",
        "Seohyun Kim",
        "Yongjin Yang",
        "Yujin Kim",
        "Se-Young Yun"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning",
      "pdf_url": "http://arxiv.org/pdf/2502.20122v1",
      "published": "2025-02-27T14:14:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20122v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Forward-Cooperation-Backward (FCB) learning in a Multi-Encoding Uni-Decoding neural network architecture",
      "authors": [
        "Prasun Dutta",
        "Koustab Ghosh",
        "Rajat K. De"
      ],
      "abstract": "The most popular technique to train a neural network is backpropagation.\nRecently, the Forward-Forward technique has also been introduced for certain\nlearning tasks. However, in real life, human learning does not follow any of\nthese techniques exclusively. The way a human learns is basically a combination\nof forward learning, backward propagation and cooperation. Humans start\nlearning a new concept by themselves and try to refine their understanding\nhierarchically during which they might come across several doubts. The most\ncommon approach to doubt solving is a discussion with peers, which can be\ncalled cooperation. Cooperation/discussion/knowledge sharing among peers is one\nof the most important steps of learning that humans follow. However, there\nmight still be a few doubts even after the discussion. Then the difference\nbetween the understanding of the concept and the original literature is\nidentified and minimized over several revisions. Inspired by this, the paper\nintroduces Forward-Cooperation-Backward (FCB) learning in a deep neural network\nframework mimicking the human nature of learning a new concept. A novel deep\nneural network architecture, called Multi Encoding Uni Decoding neural network\nmodel, has been designed which learns using the notion of FCB. A special\nlateral synaptic connection has also been introduced to realize cooperation.\nThe models have been justified in terms of their performance in dimension\nreduction on four popular datasets. The ability to preserve the granular\nproperties of data in low-rank embedding has been tested to justify the quality\nof dimension reduction. For downstream analyses, classification has also been\nperformed. An experimental study on convergence analysis has been performed to\nestablish the efficacy of the FCB learning strategy.",
      "pdf_url": "http://arxiv.org/pdf/2502.20113v1",
      "published": "2025-02-27T14:04:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20113v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ]
    },
    {
      "title": "MITracker: Multi-View Integration for Visual Object Tracking",
      "authors": [
        "Mengjie Xu",
        "Yitao Zhu",
        "Haotian Jiang",
        "Jiaming Li",
        "Zhenrong Shen",
        "Sheng Wang",
        "Haolin Huang",
        "Xinyu Wang",
        "Qing Yang",
        "Han Zhang",
        "Qian Wang"
      ],
      "abstract": "Multi-view object tracking (MVOT) offers promising solutions to challenges\nsuch as occlusion and target loss, which are common in traditional single-view\ntracking. However, progress has been limited by the lack of comprehensive\nmulti-view datasets and effective cross-view integration methods. To overcome\nthese limitations, we compiled a Multi-View object Tracking (MVTrack) dataset\nof 234K high-quality annotated frames featuring 27 distinct objects across\nvarious scenes. In conjunction with this dataset, we introduce a novel MVOT\nmethod, Multi-View Integration Tracker (MITracker), to efficiently integrate\nmulti-view object features and provide stable tracking outcomes. MITracker can\ntrack any object in video frames of arbitrary length from arbitrary viewpoints.\nThe key advancements of our method over traditional single-view approaches come\nfrom two aspects: (1) MITracker transforms 2D image features into a 3D feature\nvolume and compresses it into a bird's eye view (BEV) plane, facilitating\ninter-view information fusion; (2) we propose an attention mechanism that\nleverages geometric information from fused 3D feature volume to refine the\ntracking results at each view. MITracker outperforms existing methods on the\nMVTrack and GMTD datasets, achieving state-of-the-art performance. The code and\nthe new dataset will be available at\nhttps://mii-laboratory.github.io/MITracker/.",
      "pdf_url": "http://arxiv.org/pdf/2502.20111v1",
      "published": "2025-02-27T14:03:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20111v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Sanity Checking Causal Representation Learning on a Simple Real-World System",
      "authors": [
        "Juan L. Gamella",
        "Simon Bing",
        "Jakob Runge"
      ],
      "abstract": "We evaluate methods for causal representation learning (CRL) on a simple,\nreal-world system where these methods are expected to work. The system consists\nof a controlled optical experiment specifically built for this purpose, which\nsatisfies the core assumptions of CRL and where the underlying causal factors\n(the inputs to the experiment) are known, providing a ground truth. We select\nmethods representative of different approaches to CRL and find that they all\nfail to recover the underlying causal factors. To understand the failure modes\nof the evaluated algorithms, we perform an ablation on the data by substituting\nthe real data-generating process with a simpler synthetic equivalent. The\nresults reveal a reproducibility problem, as most methods already fail on this\nsynthetic ablation despite its simple data-generating process. Additionally, we\nobserve that common assumptions on the mixing function are crucial for the\nperformance of some of the methods but do not hold in the real data. Our\nefforts highlight the contrast between the theoretical promise of the state of\nthe art and the challenges in its application. We hope the benchmark serves as\na simple, real-world sanity check to further develop and validate methodology,\nbridging the gap towards CRL methods that work in practice. We make all code\nand datasets publicly available at github.com/simonbing/CRLSanityCheck",
      "pdf_url": "http://arxiv.org/pdf/2502.20099v1",
      "published": "2025-02-27T13:56:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20099v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ]
    },
    {
      "title": "WalnutData: A UAV Remote Sensing Dataset of Green Walnuts and Model Evaluation",
      "authors": [
        "Mingjie Wu",
        "Chenggui Yang",
        "Huihua Wang",
        "Chen Xue",
        "Yibo Wang",
        "Haoyu Wang",
        "Yansong Wang",
        "Can Peng",
        "Yuqi Han",
        "Ruoyu Li",
        "Lijun Yun",
        "Zaiqing Chen",
        "Songfan Shi",
        "Luhao Fang",
        "Shuyi Wan",
        "Tingfeng Li",
        "Shuangyao Liu",
        "Haotian Feng"
      ],
      "abstract": "The UAV technology is gradually maturing and can provide extremely powerful\nsupport for smart agriculture and precise monitoring. Currently, there is no\ndataset related to green walnuts in the field of agricultural computer vision.\nThus, in order to promote the algorithm design in the field of agricultural\ncomputer vision, we used UAV to collect remote-sensing data from 8 walnut\nsample plots. Considering that green walnuts are subject to various lighting\nconditions and occlusion, we constructed a large-scale dataset with a\nhigher-granularity of target features - WalnutData. This dataset contains a\ntotal of 30,240 images and 706,208 instances, and there are 4 target\ncategories: being illuminated by frontal light and unoccluded (A1), being\nbacklit and unoccluded (A2), being illuminated by frontal light and occluded\n(B1), and being backlit and occluded (B2). Subsequently, we evaluated many\nmainstream algorithms on WalnutData and used these evaluation results as the\nbaseline standard. The dataset and all evaluation results can be obtained at\nhttps://github.com/1wuming/WalnutData.",
      "pdf_url": "http://arxiv.org/pdf/2502.20092v1",
      "published": "2025-02-27T13:51:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20092v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning",
      "authors": [
        "Adib Karimi",
        "Mohammad Mehdi Ebadzadeh"
      ],
      "abstract": "We introduce a novel Inverse Reinforcement Learning (IRL) approach that\novercomes limitations of fixed reward assignments and constrained flexibility\nin implicit reward regularization. By extending the Maximum Entropy IRL\nframework with a squared temporal-difference (TD) regularizer and adaptive\ntargets, dynamically adjusted during training, our method indirectly optimizes\na reward function while incorporating reinforcement learning principles.\nFurthermore, we integrate distributional RL to capture richer return\ninformation. Our approach achieves state-of-the-art performance on challenging\nMuJoCo tasks, demonstrating expert-level results on the Humanoid task with only\n3 demonstrations. Extensive experiments and ablation studies validate the\neffectiveness of our method, providing insights into adaptive targets and\nreward dynamics in imitation learning.",
      "pdf_url": "http://arxiv.org/pdf/2502.20089v1",
      "published": "2025-02-27T13:47:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20089v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Minds on the Move: Decoding Trajectory Prediction in Autonomous Driving with Cognitive Insights",
      "authors": [
        "Haicheng Liao",
        "Chengyue Wang",
        "Kaiqun Zhu",
        "Yilong Ren",
        "Bolin Gao",
        "Shengbo Eben Li",
        "Chengzhong Xu",
        "Zhenning Li"
      ],
      "abstract": "In mixed autonomous driving environments, accurately predicting the future\ntrajectories of surrounding vehicles is crucial for the safe operation of\nautonomous vehicles (AVs). In driving scenarios, a vehicle's trajectory is\ndetermined by the decision-making process of human drivers. However, existing\nmodels primarily focus on the inherent statistical patterns in the data, often\nneglecting the critical aspect of understanding the decision-making processes\nof human drivers. This oversight results in models that fail to capture the\ntrue intentions of human drivers, leading to suboptimal performance in\nlong-term trajectory prediction. To address this limitation, we introduce a\nCognitive-Informed Transformer (CITF) that incorporates a cognitive concept,\nPerceived Safety, to interpret drivers' decision-making mechanisms. Perceived\nSafety encapsulates the varying risk tolerances across drivers with different\ndriving behaviors. Specifically, we develop a Perceived Safety-aware Module\nthat includes a Quantitative Safety Assessment for measuring the subject risk\nlevels within scenarios, and Driver Behavior Profiling for characterizing\ndriver behaviors. Furthermore, we present a novel module, Leanformer, designed\nto capture social interactions among vehicles. CITF demonstrates significant\nperformance improvements on three well-established datasets. In terms of\nlong-term prediction, it surpasses existing benchmarks by 12.0% on the NGSIM,\n28.2% on the HighD, and 20.8% on the MoCAD dataset. Additionally, its\nrobustness in scenarios with limited or missing data is evident, surpassing\nmost state-of-the-art (SOTA) baselines, and paving the way for real-world\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2502.20084v1",
      "published": "2025-02-27T13:43:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20084v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents",
      "authors": [
        "Haochen Sun",
        "Shuwen Zhang",
        "Lei Ren",
        "Hao Xu",
        "Hao Fu",
        "Caixia Yuan",
        "Xiaojie Wang"
      ],
      "abstract": "Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 10 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaption that are critical\nfor efficiently fulfilling complicated tasks. Notably, we highlight the\nstrengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30\nopen-ended tasks, and an integrated evaluation package are now publicly\navailable at https://github.com/YusaeMeow/Collab-Overcooked.",
      "pdf_url": "http://arxiv.org/pdf/2502.20073v1",
      "published": "2025-02-27T13:31:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20073v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation",
      "authors": [
        "Kang Liu",
        "Zhuoqi Ma",
        "Xiaolu Kang",
        "Yunan Li",
        "Kun Xie",
        "Zhicheng Jiao",
        "Qiguang Miao"
      ],
      "abstract": "Automated radiology report generation offers an effective solution to\nalleviate radiologists' workload. However, most existing methods focus\nprimarily on single or fixed-view images to model current disease conditions,\nwhich limits diagnostic accuracy and overlooks disease progression. Although\nsome approaches utilize longitudinal data to track disease progression, they\nstill rely on single images to analyze current visits. To address these issues,\nwe propose enhanced contrastive learning with Multi-view Longitudinal data to\nfacilitate chest X-ray Report Generation, named MLRG. Specifically, we\nintroduce a multi-view longitudinal contrastive learning method that integrates\nspatial information from current multi-view images and temporal information\nfrom longitudinal data. This method also utilizes the inherent spatiotemporal\ninformation of radiology reports to supervise the pre-training of visual and\ntextual representations. Subsequently, we present a tokenized absence encoding\ntechnique to flexibly handle missing patient-specific prior knowledge, allowing\nthe model to produce more accurate radiology reports based on available prior\nknowledge. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view CXR\ndatasets demonstrate that our MLRG outperforms recent state-of-the-art methods,\nachieving a 2.3% BLEU-4 improvement on MIMIC-CXR, a 5.5% F1 score improvement\non MIMIC-ABN, and a 2.7% F1 RadGraph improvement on Two-view CXR.",
      "pdf_url": "http://arxiv.org/pdf/2502.20056v1",
      "published": "2025-02-27T12:59:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20056v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Polish-ASTE: Aspect-Sentiment Triplet Extraction Datasets for Polish",
      "authors": [
        "Marta Lango",
        "Borys Naglik",
        "Mateusz Lango",
        "Iwo Naglik"
      ],
      "abstract": "Aspect-Sentiment Triplet Extraction (ASTE) is one of the most challenging and\ncomplex tasks in sentiment analysis. It concerns the construction of triplets\nthat contain an aspect, its associated sentiment polarity, and an opinion\nphrase that serves as a rationale for the assigned polarity. Despite the\ngrowing popularity of the task and the many machine learning methods being\nproposed to address it, the number of datasets for ASTE is very limited. In\nparticular, no dataset is available for any of the Slavic languages. In this\npaper, we present two new datasets for ASTE containing customer opinions about\nhotels and purchased products expressed in Polish. We also perform experiments\nwith two ASTE techniques combined with two large language models for Polish to\ninvestigate their performance and the difficulty of the assembled datasets. The\nnew datasets are available under a permissive licence and have the same file\nformat as the English datasets, facilitating their use in future research.",
      "pdf_url": "http://arxiv.org/pdf/2502.20046v1",
      "published": "2025-02-27T12:38:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.20046v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}