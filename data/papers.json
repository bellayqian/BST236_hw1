{
  "last_updated": "2025-03-21T00:47:16.023184",
  "papers": [
    {
      "title": "TULIP: Towards Unified Language-Image Pretraining",
      "authors": [
        "Zineng Tang",
        "Long Lian",
        "Seun Eisape",
        "XuDong Wang",
        "Roei Herzig",
        "Adam Yala",
        "Alane Suhr",
        "Trevor Darrell",
        "David M. Chan"
      ],
      "abstract": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.15485v1",
      "published": "2025-03-19T17:58:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15485v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Value Profiles for Encoding Human Variation",
      "authors": [
        "Taylor Sorensen",
        "Pushkar Mishra",
        "Roma Patel",
        "Michael Henry Tessler",
        "Michiel Bakker",
        "Georgina Evans",
        "Iason Gabriel",
        "Noah Goodman",
        "Verena Rieser"
      ],
      "abstract": "Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information.",
      "pdf_url": "http://arxiv.org/pdf/2503.15484v1",
      "published": "2025-03-19T17:57:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15484v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Learning to Play Piano in the Real World",
      "authors": [
        "Yves-Simon Zeulner",
        "Sandeep Selvaraj",
        "Roberto Calandra"
      ],
      "abstract": "Towards the grand challenge of achieving human-level manipulation in robots,\nplaying piano is a compelling testbed that requires strategic, precise, and\nflowing movements. Over the years, several works demonstrated hand-designed\ncontrollers on real world piano playing, while other works evaluated robot\nlearning approaches on simulated piano scenarios. In this paper, we develop the\nfirst piano playing robotic system that makes use of learning approaches while\nalso being deployed on a real world dexterous robot. Specifically, we make use\nof Sim2Real to train a policy in simulation using reinforcement learning before\ndeploying the learned policy on a real world dexterous robot. In our\nexperiments, we thoroughly evaluate the interplay between domain randomization\nand the accuracy of the dynamics model used in simulation. Moreover, we\nevaluate the robot's performance across multiple songs with varying complexity\nto study the generalization of our learned policy. By providing a\nproof-of-concept of learning to play piano in the real world, we want to\nencourage the community to adopt piano playing as a compelling benchmark\ntowards human-level manipulation. We open-source our code and show additional\nvideos at https://lasr.org/research/learning-to-play-piano .",
      "pdf_url": "http://arxiv.org/pdf/2503.15481v1",
      "published": "2025-03-19T17:56:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15481v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective",
      "authors": [
        "Noam Razin",
        "Zixuan Wang",
        "Hubert Strauss",
        "Stanley Wei",
        "Jason D. Lee",
        "Sanjeev Arora"
      ],
      "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.",
      "pdf_url": "http://arxiv.org/pdf/2503.15477v1",
      "published": "2025-03-19T17:54:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15477v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ]
    },
    {
      "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining",
      "authors": [
        "Boshen Xu",
        "Yuting Mei",
        "Xinbi Liu",
        "Sipeng Zheng",
        "Qin Jin"
      ],
      "abstract": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM.",
      "pdf_url": "http://arxiv.org/pdf/2503.15470v1",
      "published": "2025-03-19T17:45:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15470v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional Context-Aware Representation Learning for Enhanced Text Classification",
      "authors": [
        "ZhengLin Lai",
        "MengYao Liao",
        "Dong Xu"
      ],
      "abstract": "Text classification, a fundamental task in natural language processing (NLP),\naims to categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies. The\nadvent of deep learning, particularly recurrent neural networks (RNNs) and\nTransformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite improvements,\nexisting models exhibit limitations in balancing interpretability,\ncomputational efficiency, and long-range contextual understanding. This paper\nproposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which\nintegrates bidirectional temporal modelling with self-attention mechanisms.\nDBEAN dynamically assigns weights to critical segments of input, improving\ncontextual representation while maintaining computational efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2503.15469v2",
      "published": "2025-03-19T17:45:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15469v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment",
      "authors": [
        "Jia-Nan Li",
        "Jian Guan",
        "Songhao Wu",
        "Wei Wu",
        "Rui Yan"
      ],
      "abstract": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2503.15463v1",
      "published": "2025-03-19T17:41:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15463v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator",
      "authors": [
        "Yuanzhi Zhu",
        "Xi Wang",
        "Stéphane Lathuilière",
        "Vicky Kalogeiton"
      ],
      "abstract": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.",
      "pdf_url": "http://arxiv.org/pdf/2503.15457v1",
      "published": "2025-03-19T17:36:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15457v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning",
      "authors": [
        "Yang Tan",
        "Chen Liu",
        "Jingyuan Gao",
        "Banghao Wu",
        "Mingchen Li",
        "Ruilin Wang",
        "Lingrong Zhang",
        "Huiqun Yu",
        "Guisheng Fan",
        "Liang Hong",
        "Bingxin Zhou"
      ],
      "abstract": "Natural language processing (NLP) has significantly influenced scientific\ndomains beyond human language, including protein engineering, where pre-trained\nprotein language models (PLMs) have demonstrated remarkable success. However,\ninterdisciplinary adoption remains limited due to challenges in data\ncollection, task benchmarking, and application. This work presents\nVenusFactory, a versatile engine that integrates biological data retrieval,\nstandardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory\nsupports both computer science and biology communities with choices of both a\ncommand-line execution and a Gradio-based no-code interface, integrating $40+$\nprotein-related datasets and $40+$ popular PLMs. All implementations are\nopen-sourced on https://github.com/tyang816/VenusFactory.",
      "pdf_url": "http://arxiv.org/pdf/2503.15438v1",
      "published": "2025-03-19T17:19:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15438v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.QM"
      ]
    },
    {
      "title": "An extensive simulation study evaluating the interaction of resampling techniques across multiple causal discovery contexts",
      "authors": [
        "Ritwick Banerjee",
        "Bryan Andrews",
        "Erich Kummerfeld"
      ],
      "abstract": "Despite the accelerating presence of exploratory causal analysis in modern\nscience and medicine, the available non-experimental methods for validating\ncausal models are not well characterized. One of the most popular methods is to\nevaluate the stability of model features after resampling the data, similar to\nresampling methods for estimating confidence intervals in statistics. Many\naspects of this approach have received little to no attention, however, such as\nwhether the choice of resampling method should depend on the sample size,\nalgorithms being used, or algorithm tuning parameters. We present theoretical\nresults proving that certain resampling methods closely emulate the assignment\nof specific values to algorithm tuning parameters. We also report the results\nof extensive simulation experiments, which verify the theoretical result and\nprovide substantial data to aid researchers in further characterizing\nresampling in the context of causal discovery analysis. Together, the\ntheoretical work and simulation results provide specific guidance on how\nresampling methods and tuning parameters should be selected in practice.",
      "pdf_url": "http://arxiv.org/pdf/2503.15436v1",
      "published": "2025-03-19T17:18:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15436v1",
      "categories": [
        "stat.ME",
        "cs.AI"
      ]
    },
    {
      "title": "Visual Position Prompt for MLLM based Visual Grounding",
      "authors": [
        "Wei Tang",
        "Yanpeng Sun",
        "Qinying Gu",
        "Zechao Li"
      ],
      "abstract": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2503.15426v1",
      "published": "2025-03-19T17:08:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15426v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Probing the topology of the space of tokens with structured prompts",
      "authors": [
        "Michael Robinson",
        "Sourya Dey",
        "Taisa Kushner"
      ],
      "abstract": "This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes.",
      "pdf_url": "http://arxiv.org/pdf/2503.15421v1",
      "published": "2025-03-19T17:01:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15421v1",
      "categories": [
        "math.DG",
        "cs.AI",
        "53Z50, 58Z05",
        "I.2.7"
      ]
    },
    {
      "title": "Temporal Regularization Makes Your Video Generator Stronger",
      "authors": [
        "Harold Haodong Chen",
        "Haojian Huang",
        "Xianfeng Wu",
        "Yexin Liu",
        "Yajing Bai",
        "Wen-Jie Shu",
        "Harry Yang",
        "Ser-Nam Lim"
      ],
      "abstract": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
      "pdf_url": "http://arxiv.org/pdf/2503.15417v1",
      "published": "2025-03-19T16:59:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15417v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures",
      "authors": [
        "Giovanni Floreale",
        "Piero Baraldi",
        "Enrico Zio",
        "Olga Fink"
      ],
      "abstract": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.",
      "pdf_url": "http://arxiv.org/pdf/2503.15415v1",
      "published": "2025-03-19T16:57:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15415v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Towards efficient keyword spotting using spike-based time difference encoders",
      "authors": [
        "Alejandro Pequeño-Zurro",
        "Lyes Khacef",
        "Stefano Panzeri",
        "Elisabetta Chicca"
      ],
      "abstract": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns.",
      "pdf_url": "http://arxiv.org/pdf/2503.15402v1",
      "published": "2025-03-19T16:43:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15402v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.ET"
      ]
    },
    {
      "title": "CCDP: Composition of Conditional Diffusion Policies with Guided Sampling",
      "authors": [
        "Amirreza Razmjoo",
        "Sylvain Calinon",
        "Michael Gienger",
        "Fan Zhang"
      ],
      "abstract": "Imitation Learning offers a promising approach to learn directly from data\nwithout requiring explicit models, simulations, or detailed task definitions.\nDuring inference, actions are sampled from the learned distribution and\nexecuted on the robot. However, sampled actions may fail for various reasons,\nand simply repeating the sampling step until a successful action is obtained\ncan be inefficient. In this work, we propose an enhanced sampling strategy that\nrefines the sampling distribution to avoid previously unsuccessful actions. We\ndemonstrate that by solely utilizing data from successful demonstrations, our\nmethod can infer recovery actions without the need for additional exploratory\nbehavior or a high-level controller. Furthermore, we leverage the concept of\ndiffusion model decomposition to break down the primary problem (which may\nrequire long-horizon history to manage failures) into multiple smaller, more\nmanageable sub-problems in learning, data collection, and inference, thereby\nenabling the system to adapt to variable failure counts. Our approach yields a\nlow-level controller that dynamically adjusts its sampling space to improve\nefficiency when prior samples fall short. We validate our method across several\ntasks, including door opening with unknown directions, object manipulation, and\nbutton-searching scenarios, demonstrating that our approach outperforms\ntraditional baselines.",
      "pdf_url": "http://arxiv.org/pdf/2503.15386v1",
      "published": "2025-03-19T16:24:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15386v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
      "authors": [
        "Anatole Callies",
        "Quentin Bodinier",
        "Philippe Ravaud",
        "Kourosh Davarpanah"
      ],
      "abstract": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.",
      "pdf_url": "http://arxiv.org/pdf/2503.15374v1",
      "published": "2025-03-19T16:12:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15374v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Optimizing Decomposition for Optimal Claim Verification",
      "authors": [
        "Yining Lu",
        "Noah Ziems",
        "Hy Dang",
        "Meng Jiang"
      ],
      "abstract": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
      "pdf_url": "http://arxiv.org/pdf/2503.15354v1",
      "published": "2025-03-19T15:56:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15354v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer",
      "authors": [
        "Abhi Kamboj",
        "Minh N. Do"
      ],
      "abstract": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.",
      "pdf_url": "http://arxiv.org/pdf/2503.15352v1",
      "published": "2025-03-19T15:51:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15352v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP"
      ]
    },
    {
      "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection",
      "authors": [
        "Ritabrata Chakraborty",
        "Rajatsubhra Chakraborty",
        "Ali Khaleghi Rahimian",
        "Thomas MacDougall"
      ],
      "abstract": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.",
      "pdf_url": "http://arxiv.org/pdf/2503.15342v1",
      "published": "2025-03-19T15:41:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15342v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Challenges and Trends in Egocentric Vision: A Survey",
      "authors": [
        "Xiang Li",
        "Heqian Qiu",
        "Lanxiao Wang",
        "Hanwen Zhang",
        "Chenghao Qi",
        "Linfeng Han",
        "Huiyu Xiong",
        "Hongliang Li"
      ],
      "abstract": "With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield.",
      "pdf_url": "http://arxiv.org/pdf/2503.15275v1",
      "published": "2025-03-19T14:51:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15275v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration",
      "authors": [
        "David Wan",
        "Justin Chih-Yao Chen",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.",
      "pdf_url": "http://arxiv.org/pdf/2503.15272v1",
      "published": "2025-03-19T14:46:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15272v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?",
      "authors": [
        "Roberto Araya"
      ],
      "abstract": "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.",
      "pdf_url": "http://arxiv.org/pdf/2503.15268v1",
      "published": "2025-03-19T14:44:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15268v1",
      "categories": [
        "cs.AI",
        "I.2.0"
      ]
    },
    {
      "title": "Automated Non-Functional Requirements Generation in Software Engineering with Large Language Models: A Comparative Study",
      "authors": [
        "Jomar Thomas Almonte",
        "Santhosh Anitha Boominathan",
        "Nathalia Nascimento"
      ],
      "abstract": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering.",
      "pdf_url": "http://arxiv.org/pdf/2503.15248v1",
      "published": "2025-03-19T14:23:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15248v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?",
      "authors": [
        "Pierre Chambon",
        "Baptiste Roziere",
        "Benoit Sagot",
        "Gabriel Synnaeve"
      ],
      "abstract": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
      "pdf_url": "http://arxiv.org/pdf/2503.15242v1",
      "published": "2025-03-19T14:19:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15242v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CC"
      ]
    },
    {
      "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification",
      "authors": [
        "Wenlong Yu",
        "Qilong Wang",
        "Chuang Liu",
        "Dong Li",
        "Qinghua Hu"
      ],
      "abstract": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores.",
      "pdf_url": "http://arxiv.org/pdf/2503.15234v1",
      "published": "2025-03-19T14:13:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15234v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Large Language Models for Word Games:Who is the Spy?",
      "authors": [
        "Chentian Wei",
        "Jiewei Chen",
        "Jinzhu Xu"
      ],
      "abstract": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.",
      "pdf_url": "http://arxiv.org/pdf/2503.15235v1",
      "published": "2025-03-19T14:13:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15235v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "A Personalized Data-Driven Generative Model of Human Motion",
      "authors": [
        "Angelo Di Porzio",
        "Marco Coraggio"
      ],
      "abstract": "The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities - such as rehabilitation therapy, sports, and\nmanufacturing - is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. However, existing\nmodels only provide simplified descriptions of human motor behavior. In this\nwork, we propose a fully data-driven approach, based on Long Short-Term Memory\nneural networks, to generate original motion that captures the unique\ncharacteristics of specific individuals. We validate the architecture using\nreal data of scalar oscillatory motion. Extensive analyses show that our model\neffectively replicates the velocity distribution and amplitude envelopes of the\nindividual it was trained on, remaining different from other individuals, and\noutperforming state-of-the-art models in terms of similarity to human data.",
      "pdf_url": "http://arxiv.org/pdf/2503.15225v1",
      "published": "2025-03-19T14:03:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15225v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection",
      "authors": [
        "Tittaya Mairittha",
        "Tanakon Sawanglok",
        "Panuwit Raden",
        "Sorrawit Treesuk"
      ],
      "abstract": "Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.",
      "pdf_url": "http://arxiv.org/pdf/2503.15204v1",
      "published": "2025-03-19T13:47:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15204v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.MA"
      ]
    },
    {
      "title": "A Unified Framework for Real-Time Failure Handling in Robotics Using Vision-Language Models, Reactive Planner and Behavior Trees",
      "authors": [
        "Faseeh Ahmad",
        "Hashim Ismail",
        "Jonathan Styrud",
        "Maj Stenmark",
        "Volker Krueger"
      ],
      "abstract": "Robotic systems often face execution failures due to unexpected obstacles,\nsensor errors, or environmental changes. Traditional failure recovery methods\nrely on predefined strategies or human intervention, making them less\nadaptable. This paper presents a unified failure recovery framework that\ncombines Vision-Language Models (VLMs), a reactive planner, and Behavior Trees\n(BTs) to enable real-time failure handling. Our approach includes pre-execution\nverification, which checks for potential failures before execution, and\nreactive failure handling, which detects and corrects failures during execution\nby verifying existing BT conditions, adding missing preconditions and, when\nnecessary, generating new skills. The framework uses a scene graph for\nstructured environmental perception and an execution history for continuous\nmonitoring, enabling context-aware and adaptive failure handling. We evaluate\nour framework through real-world experiments with an ABB YuMi robot on tasks\nlike peg insertion, object sorting, and drawer placement, as well as in\nAI2-THOR simulator. Compared to using pre-execution and reactive methods\nseparately, our approach achieves higher task success rates and greater\nadaptability. Ablation studies highlight the importance of VLM-based reasoning,\nstructured scene representation, and execution history tracking for effective\nfailure recovery in robotics.",
      "pdf_url": "http://arxiv.org/pdf/2503.15202v1",
      "published": "2025-03-19T13:40:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15202v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation",
      "authors": [
        "Gyeongrok Oh",
        "Sungjune Kim",
        "Heeju Ko",
        "Hyung-gun Chi",
        "Jinkyu Kim",
        "Dongwook Lee",
        "Daehyun Ji",
        "Sungjoon Choi",
        "Sujin Jang",
        "Sangpil Kim"
      ],
      "abstract": "The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution.",
      "pdf_url": "http://arxiv.org/pdf/2503.15185v1",
      "published": "2025-03-19T13:14:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15185v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Foundation models may exhibit staged progression in novel CBRN threat disclosure",
      "authors": [
        "Kevin M Esvelt"
      ],
      "abstract": "The extent to which foundation models can disclose novel chemical,\nbiological, radiation, and nuclear (CBRN) threats to expert users is unclear\ndue to a lack of test cases. I leveraged the unique opportunity presented by an\nupcoming publication describing a novel catastrophic biothreat - \"Technical\nReport on Mirror Bacteria: Feasibility and Risks\" - to conduct a small\ncontrolled study before it became public. Graduate-trained biologists tasked\nwith predicting the consequences of releasing mirror E. coli showed no\nsignificant differences in rubric-graded accuracy using Claude Sonnet 3.5 new\n(n=10) or web search only (n=2); both groups scored comparably to a web\nbaseline (28 and 43 versus 36). However, Sonnet reasoned correctly when\nprompted by a report author, but a smaller model, Haiku 3.5, failed even with\nauthor guidance (80 versus 5). These results suggest distinct stages of model\ncapability: Haiku is unable to reason about mirror life even with threat-aware\nexpert guidance (Stage 1), while Sonnet correctly reasons only with\nthreat-aware prompting (Stage 2). Continued advances may allow future models to\ndisclose novel CBRN threats to naive experts (Stage 3) or unskilled users\n(Stage 4). While mirror life represents only one case study, monitoring new\nmodels' ability to reason about privately known threats may allow protective\nmeasures to be implemented before widespread disclosure.",
      "pdf_url": "http://arxiv.org/pdf/2503.15182v1",
      "published": "2025-03-19T13:08:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15182v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "q-bio.OT"
      ]
    },
    {
      "title": "Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum Access Systems",
      "authors": [
        "George Stamatelis",
        "Angelos-Nikolaos Kanatas",
        "George C. Alexandropoulos"
      ],
      "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) has emerged as a powerful\ntool for optimizing decentralized decision-making systems in complex settings,\nsuch as Dynamic Spectrum Access (DSA). However, deploying deep learning models\non resource-constrained edge devices remains challenging due to their high\ncomputational cost. To address this challenge, in this paper, we present a\nnovel sparse recurrent MARL framework integrating gradual neural network\npruning into the independent actor global critic paradigm. Additionally, we\nintroduce a harmonic annealing sparsity scheduler, which achieves comparable,\nand in certain cases superior, performance to standard linear and polynomial\npruning schedulers at large sparsities. Our experimental investigation\ndemonstrates that the proposed DSA framework can discover superior policies,\nunder diverse training conditions, outperforming conventional DSA, MADRL\nbaselines, and state-of-the-art pruning techniques.",
      "pdf_url": "http://arxiv.org/pdf/2503.15172v1",
      "published": "2025-03-19T12:56:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15172v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks",
      "authors": [
        "Yuting Guo",
        "Abeed Sarker"
      ],
      "abstract": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2503.15169v1",
      "published": "2025-03-19T12:51:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15169v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child",
      "authors": [
        "Javier Del Ser",
        "Jesus L. Lobo",
        "Heimo Müller",
        "Andreas Holzinger"
      ],
      "abstract": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2503.15168v1",
      "published": "2025-03-19T12:50:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15168v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "cs.LG",
        "68T05"
      ]
    },
    {
      "title": "Volumetric Reconstruction From Partial Views for Task-Oriented Grasping",
      "authors": [
        "Fujian Yan",
        "Hui Li",
        "Hongsheng He"
      ],
      "abstract": "Object affordance and volumetric information are essential in devising\neffective grasping strategies under task-specific constraints. This paper\npresents an approach for inferring suitable grasping strategies from limited\npartial views of an object. To achieve this, a recurrent generative adversarial\nnetwork (R-GAN) was proposed by incorporating a recurrent generator with long\nshort-term memory (LSTM) units for it to process a variable number of depth\nscans. To determine object affordances, the AffordPose knowledge dataset is\nutilized as prior knowledge. Affordance retrieving is defined by the volume\nsimilarity measured via Chamfer Distance and action similarities. A Proximal\nPolicy Optimization (PPO) reinforcement learning model is further implemented\nto refine the retrieved grasp strategies for task-oriented grasping. The\nretrieved grasp strategies were evaluated on a dual-arm mobile manipulation\nrobot with an overall grasping accuracy of 89% for four tasks: lift, handle\ngrasp, wrap grasp, and press.",
      "pdf_url": "http://arxiv.org/pdf/2503.15167v1",
      "published": "2025-03-19T12:47:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15167v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU",
      "authors": [
        "Àlex Pujol Vidal",
        "Sergio Escalera",
        "Kamal Nasrollahi",
        "Thomas B. Moeslund"
      ],
      "abstract": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC",
      "pdf_url": "http://arxiv.org/pdf/2503.15166v1",
      "published": "2025-03-19T12:47:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15166v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "A Foundational Theory for Decentralized Sensory Learning",
      "authors": [
        "Linus Mårtensson",
        "Jonas M. D. Enander",
        "Udaya B. Rongala",
        "Henrik Jörntell"
      ],
      "abstract": "In both neuroscience and artificial intelligence, popular functional\nframeworks and neural network formulations operate by making use of extrinsic\nerror measurements and global learning algorithms. Through a set of conjectures\nbased on evolutionary insights on the origin of cellular adaptive mechanisms,\nwe reinterpret the core meaning of sensory signals to allow the brain to be\ninterpreted as a negative feedback control system, and show how this could lead\nto local learning algorithms without the need for global error correction\nmetrics. Thereby, a sufficiently good minima in sensory activity can be the\ncomplete reward signal of the network, as well as being both necessary and\nsufficient for biological learning to arise. We show that this method of\nlearning was likely already present in the earliest unicellular life forms on\nearth. We show evidence that the same principle holds and scales to\nmulticellular organisms where it in addition can lead to division of labour\nbetween cells. Available evidence shows that the evolution of the nervous\nsystem likely was an adaptation to more effectively communicate intercellular\nsignals to support such division of labour. We therefore propose that the same\nlearning principle that evolved already in the earliest unicellular life forms,\ni.e. negative feedback control of externally and internally generated sensor\nsignals, has simply been scaled up to become a fundament of the learning we see\nin biological brains today. We illustrate diverse biological settings, from the\nearliest unicellular organisms to humans, where this operational principle\nappears to be a plausible interpretation of the meaning of sensor signals in\nbiology, and how this relates to current neuroscientific theories and findings.",
      "pdf_url": "http://arxiv.org/pdf/2503.15130v1",
      "published": "2025-03-19T11:44:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15130v1",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ]
    },
    {
      "title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models",
      "authors": [
        "Man Fai Wong",
        "Chee Wei Tan"
      ],
      "abstract": "This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation.",
      "pdf_url": "http://arxiv.org/pdf/2503.15129v1",
      "published": "2025-03-19T11:44:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15129v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors",
      "authors": [
        "Dominik Macko",
        "Robert Moro",
        "Ivan Srba"
      ],
      "abstract": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data.",
      "pdf_url": "http://arxiv.org/pdf/2503.15128v1",
      "published": "2025-03-19T11:42:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15128v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation",
      "authors": [
        "Haoyu Ji",
        "Bowen Chen",
        "Weihong Ren",
        "Wenze Huang",
        "Zhihao Yang",
        "Zhiyong Wang",
        "Honghai Liu"
      ],
      "abstract": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results.",
      "pdf_url": "http://arxiv.org/pdf/2503.15126v1",
      "published": "2025-03-19T11:38:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15126v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
      "authors": [
        "Benjamin Estermann",
        "Roger Wattenhofer"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
      "pdf_url": "http://arxiv.org/pdf/2503.15113v1",
      "published": "2025-03-19T11:13:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15113v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "VIPER: Visual Perception and Explainable Reasoning for Sequential Decision-Making",
      "authors": [
        "Mohamed Salim Aissi",
        "Clemence Grislain",
        "Mohamed Chetouani",
        "Olivier Sigaud",
        "Laure Soulier",
        "Nicolas Thome"
      ],
      "abstract": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.",
      "pdf_url": "http://arxiv.org/pdf/2503.15108v1",
      "published": "2025-03-19T11:05:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15108v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive Control",
      "authors": [
        "Stelios Zarifis",
        "Ioannis Kordonis",
        "Petros Maragos"
      ],
      "abstract": "We propose Diffusion-Informed Model Predictive Control (D-I MPC), a generic\nframework for uncertainty-aware prediction and decision-making in partially\nobservable stochastic systems by integrating diffusion-based time series\nforecasting models in Model Predictive Control algorithms. In our approach, a\ndiffusion-based time series forecasting model is used to probabilistically\nestimate the evolution of the system's stochastic components. These forecasts\nare then incorporated into MPC algorithms to estimate future trajectories and\noptimize action selection under the uncertainty of the future. We evaluate the\nframework on the task of energy arbitrage, where a Battery Energy Storage\nSystem participates in the day-ahead electricity market of the New York state.\nExperimental results indicate that our model-based approach with a\ndiffusion-based forecaster significantly outperforms both implementations with\nclassical forecasting methods and model-free reinforcement learning baselines.",
      "pdf_url": "http://arxiv.org/pdf/2503.15095v1",
      "published": "2025-03-19T10:48:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15095v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "I.2.6; I.5.1"
      ]
    },
    {
      "title": "Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings",
      "authors": [
        "Zonghao Ying",
        "Guangyi Zheng",
        "Yongxin Huang",
        "Deyue Zhang",
        "Wenxin Zhang",
        "Quanchen Zou",
        "Aishan Liu",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "abstract": "This study presents the first comprehensive safety evaluation of the DeepSeek\nmodels, focusing on evaluating the safety risks associated with their generated\ncontent. Our evaluation encompasses DeepSeek's latest generation of large\nlanguage models, multimodal large language models, and text-to-image models,\nsystematically examining their performance regarding unsafe content generation.\nNotably, we developed a bilingual (Chinese-English) safety evaluation dataset\ntailored to Chinese sociocultural contexts, enabling a more thorough evaluation\nof the safety capabilities of Chinese-developed models. Experimental results\nindicate that despite their strong general capabilities, DeepSeek models\nexhibit significant safety vulnerabilities across multiple risk dimensions,\nincluding algorithmic discrimination and sexual content. These findings provide\ncrucial insights for understanding and improving the safety of large foundation\nmodels. Our code is available at\nhttps://github.com/NY1024/DeepSeek-Safety-Eval.",
      "pdf_url": "http://arxiv.org/pdf/2503.15092v1",
      "published": "2025-03-19T10:44:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15092v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion",
      "authors": [
        "Le Ma",
        "Ziyu Meng",
        "Tengyu Liu",
        "Yuhan Li",
        "Ran Song",
        "Wei Zhang",
        "Siyuan Huang"
      ],
      "abstract": "Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs.",
      "pdf_url": "http://arxiv.org/pdf/2503.15082v1",
      "published": "2025-03-19T10:27:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15082v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis",
      "authors": [
        "Imanol G. Estepa",
        "Jesús M. Rodríguez-de-Vera",
        "Ignacio Sarasúa",
        "Bhalaji Nagarajan",
        "Petia Radeva"
      ],
      "abstract": "While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models.",
      "pdf_url": "http://arxiv.org/pdf/2503.15060v1",
      "published": "2025-03-19T09:53:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15060v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.5.4; I.5.1; I.2.10"
      ]
    },
    {
      "title": "Texture-Aware StarGAN for CT data harmonisation",
      "authors": [
        "Francesco Di Feola",
        "Ludovica Pompilio",
        "Cecilia Assolito",
        "Valerio Guarrasi",
        "Paolo Soda"
      ],
      "abstract": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.",
      "pdf_url": "http://arxiv.org/pdf/2503.15058v1",
      "published": "2025-03-19T09:50:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15058v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation",
      "authors": [
        "Cheng Wang",
        "Lingxin Kong",
        "Massimiliano Tamborski",
        "Stefano V. Albrecht"
      ],
      "abstract": "Simulation-based testing has emerged as an essential tool for verifying and\nvalidating autonomous vehicles (AVs). However, contemporary methodologies, such\nas deterministic and imitation learning-based driver models, struggle to\ncapture the variability of human-like driving behavior. Given these challenges,\nwe propose HAD-Gen, a general framework for realistic traffic scenario\ngeneration that simulates diverse human-like driving behaviors. The framework\nfirst clusters the vehicle trajectory data into different driving styles\naccording to safety features. It then employs maximum entropy inverse\nreinforcement learning on each of the clusters to learn the reward function\ncorresponding to each driving style. Using these reward functions, the method\nintegrates offline reinforcement learning pre-training and multi-agent\nreinforcement learning algorithms to obtain general and robust driving\npolicies. Multi-perspective simulation results show that our proposed scenario\ngeneration framework can simulate diverse, human-like driving behaviors with\nstrong generalization capability. The proposed framework achieves a 90.96%\ngoal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in\nthe generalization test, outperforming prior approaches by over 20% in\ngoal-reaching performance. The source code is released at\nhttps://github.com/RoboSafe-Lab/Sim4AD.",
      "pdf_url": "http://arxiv.org/pdf/2503.15049v1",
      "published": "2025-03-19T09:38:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15049v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback",
      "authors": [
        "Sungjae Lee",
        "Yeonjoo Hong",
        "Kwang In Kim"
      ],
      "abstract": "Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.",
      "pdf_url": "http://arxiv.org/pdf/2503.15035v1",
      "published": "2025-03-19T09:25:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2503.15035v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    }
  ]
}