{
  "last_updated": "2025-07-30T00:57:50.519135",
  "papers": [
    {
      "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence",
      "authors": [
        "Huan-ang Gao",
        "Jiayi Geng",
        "Wenyue Hua",
        "Mengkang Hu",
        "Xinzhe Juan",
        "Hongzhang Liu",
        "Shilong Liu",
        "Jiahao Qiu",
        "Xuan Qi",
        "Yiran Wu",
        "Hongru Wang",
        "Han Xiao",
        "Yuhang Zhou",
        "Shaokun Zhang",
        "Jiayi Zhang",
        "Jinyu Xiang",
        "Yixiong Fang",
        "Qiwen Zhao",
        "Dongrui Liu",
        "Qihan Ren",
        "Cheng Qian",
        "Zhenghailong Wang",
        "Minda Hu",
        "Huazheng Wang",
        "Qingyun Wu",
        "Heng Ji",
        "Mengdi Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
      "pdf_url": "http://arxiv.org/pdf/2507.21046v1",
      "published": "2025-07-28T17:59:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21046v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis",
      "authors": [
        "Haoyang Liu",
        "Yijiang Li",
        "Haohan Wang"
      ],
      "abstract": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
      "pdf_url": "http://arxiv.org/pdf/2507.21035v1",
      "published": "2025-07-28T17:55:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21035v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "q-bio.GN"
      ]
    },
    {
      "title": "Smart Expansion Techniques for ASP-based Interactive Configuration",
      "authors": [
        "Lucia Balážová",
        "Richard Comploi-Taupe",
        "Susana Hahn",
        "Nicolas Rühling",
        "Gottfried Schenner"
      ],
      "abstract": "Product configuration is a successful application of Answer Set Programming\n(ASP). However, challenges are still open for interactive systems to\neffectively guide users through the configuration process. The aim of our work\nis to provide an ASP-based solver for interactive configuration that can deal\nwith large-scale industrial configuration problems and that supports intuitive\nuser interfaces via an API. In this paper, we focus on improving the\nperformance of automatically completing a partial configuration. Our main\ncontribution enhances the classical incremental approach for multi-shot solving\nby four different smart expansion functions. The core idea is to determine and\nadd specific objects or associations to the partial configuration by exploiting\ncautious and brave consequences before checking for the existence of a complete\nconfiguration with the current objects in each iteration. This approach limits\nthe number of costly unsatisfiability checks and reduces the search space,\nthereby improving solving performance. In addition, we present a user interface\nthat uses our API and is implemented in ASP.",
      "pdf_url": "http://arxiv.org/pdf/2507.21027v1",
      "published": "2025-07-28T17:46:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21027v1",
      "categories": [
        "cs.AI",
        "cs.SE",
        "D.1.6; I.2.1"
      ]
    },
    {
      "title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them",
      "authors": [
        "Weichen Zhang",
        "Yiyou Sun",
        "Pohao Huang",
        "Jiayue Pu",
        "Heyue Lin",
        "Dawn Song"
      ],
      "abstract": "Hallucinations pose critical risks for large language model (LLM)-based\nagents, often manifesting as hallucinative actions resulting from fabricated or\nmisinterpreted information within the cognitive context. While recent studies\nhave exposed such failures, existing evaluations remain fragmented and lack a\nprincipled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions\nin Risky AGEnt settings--the first unified benchmark for eliciting and\nevaluating hallucinations in interactive LLM-agent scenarios. We begin by\nintroducing a three-part taxonomy to address agentic hallucinations: actions\nthat are unfaithful to (i) task instructions, (ii) execution history, or (iii)\nenvironment observations. To analyze, we first elicit such failures by\nperforming a systematic audit of existing agent benchmarks, then synthesize\ntest cases using a snapshot strategy that isolates decision points in\ndeterministic and reproducible manners. To evaluate hallucination behaviors, we\nadopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware\nprompts, enabling scalable, high-fidelity assessment of agent actions without\nenumerating full action spaces. MIRAGE-Bench provides actionable insights on\nfailure modes of LLM agents and lays the groundwork for principled progress in\nmitigating hallucinations in interactive environments.",
      "pdf_url": "http://arxiv.org/pdf/2507.21017v1",
      "published": "2025-07-28T17:38:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21017v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Memorization in Fine-Tuned Large Language Models",
      "authors": [
        "Danil Savine",
        "Muni Sreenivas Pydi",
        "Jamal Atif",
        "Olivier Cappé"
      ],
      "abstract": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns.",
      "pdf_url": "http://arxiv.org/pdf/2507.21009v1",
      "published": "2025-07-28T17:22:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21009v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability",
      "authors": [
        "Fang Li"
      ],
      "abstract": "Deep Neural Networks (DNNs) deliver impressive performance but their\nblack-box nature limits deployment in high-stakes domains requiring\ntransparency. We introduce Compositional Function Networks (CFNs), a novel\nframework that builds inherently interpretable models by composing elementary\nmathematical functions with clear semantics. Unlike existing interpretable\napproaches that are limited to simple additive structures, CFNs support diverse\ncompositional patterns -- sequential, parallel, and conditional -- enabling\ncomplex feature interactions while maintaining transparency. A key innovation\nis that CFNs are fully differentiable, allowing efficient training through\nstandard gradient descent. We demonstrate CFNs' versatility across multiple\ndomains, from symbolic regression to image classification with deep\nhierarchical networks. Our empirical evaluation shows CFNs achieve competitive\nperformance against black-box models (96.24% accuracy on CIFAR-10) while\noutperforming state-of-the-art interpretable models like Explainable Boosting\nMachines. By combining the hierarchical expressiveness and efficient training\nof deep learning with the intrinsic interpretability of well-defined\nmathematical functions, CFNs offer a powerful framework for applications where\nboth performance and accountability are paramount.",
      "pdf_url": "http://arxiv.org/pdf/2507.21004v1",
      "published": "2025-07-28T17:18:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21004v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition",
      "authors": [
        "Haris Khan",
        "Shumaila Asif",
        "Sadia Asif"
      ],
      "abstract": "In real-world machine learning deployments, models must be continually\nupdated, composed, and when required, selectively undone. However, existing\napproaches to model merging and continual learning often suffer from task\ninterference, catastrophic forgetting, or lack of reversibility. We propose\nModular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework\nthat enables scalable, interference-free, and reversible composition of\nfine-tuned models. Each task-specific model is encoded as a delta from a shared\nbase and projected into an orthogonal subspace to eliminate conflict. These\nprojected deltas are then merged via gradient-based optimization to form a\nunified model that retains performance across tasks. Our approach supports\ncontinual integration of new models, structured unmerging for compliance such\nas GDPR requirements, and model stability via elastic weight consolidation and\nsynthetic replay. Extensive experiments on vision and natural language\nprocessing benchmarks demonstrate that MDM-OC outperforms prior baselines in\naccuracy, backward transfer, and unmerge fidelity, while remaining\nmemory-efficient and computationally tractable. This framework offers a\nprincipled solution for modular and compliant AI system design.",
      "pdf_url": "http://arxiv.org/pdf/2507.20997v1",
      "published": "2025-07-28T17:08:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20997v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM",
      "authors": [
        "Shen Li",
        "Liuyi Yao",
        "Wujia Niu",
        "Lan Zhang",
        "Yaliang Li"
      ],
      "abstract": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality.",
      "pdf_url": "http://arxiv.org/pdf/2507.20994v1",
      "published": "2025-07-28T16:59:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20994v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Personalized Treatment Effect Estimation from Unstructured Data",
      "authors": [
        "Henri Arno",
        "Thomas Demeester"
      ],
      "abstract": "Existing methods for estimating personalized treatment effects typically rely\non structured covariates, limiting their applicability to unstructured data.\nYet, leveraging unstructured data for causal inference has considerable\napplication potential, for instance in healthcare, where clinical notes or\nmedical images are abundant. To this end, we first introduce an approximate\n'plug-in' method trained directly on the neural representations of unstructured\ndata. However, when these fail to capture all confounding information, the\nmethod may be subject to confounding bias. We therefore introduce two\ntheoretically grounded estimators that leverage structured measurements of the\nconfounders during training, but allow estimating personalized treatment\neffects purely from unstructured inputs, while avoiding confounding bias. When\nthese structured measurements are only available for a non-representative\nsubset of the data, these estimators may suffer from sampling bias. To address\nthis, we further introduce a regression-based correction that accounts for the\nnon-uniform sampling, assuming the sampling mechanism is known or can be\nwell-estimated. Our experiments on two benchmark datasets show that the plug-in\nmethod, directly trainable on large unstructured datasets, achieves strong\nempirical performance across all settings, despite its simplicity.",
      "pdf_url": "http://arxiv.org/pdf/2507.20993v1",
      "published": "2025-07-28T16:52:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20993v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1",
      "authors": [
        "Xinhan Di",
        "Kristin Qi",
        "Pengqian Yu"
      ],
      "abstract": "Recent advances in diffusion-based video generation have enabled\nphoto-realistic short clips, but current methods still struggle to achieve\nmulti-modal consistency when jointly generating whole-body motion and natural\nspeech. Current approaches lack comprehensive evaluation frameworks that assess\nboth visual and audio quality, and there are insufficient benchmarks for\nregion-specific performance analysis. To address these gaps, we introduce the\nJoint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1),\ncomprising a large-scale multi-modal dataset with 10,000 unique identities\nacross 2 million video samples, and an evaluation protocol for assessing joint\naudio-video generation of whole-body animatable avatars. Our evaluation of SOTA\nmodels reveals consistent performance disparities between face/hand-centric and\nwhole-body performance, which incidates essential areas for future research.\nThe dataset and evaluation tools are publicly available at\nhttps://github.com/deepreasonings/WholeBodyBenchmark.",
      "pdf_url": "http://arxiv.org/pdf/2507.20987v2",
      "published": "2025-07-28T16:47:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20987v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment",
      "authors": [
        "Yixin Song",
        "Zhenliang Xue",
        "Dongliang Wei",
        "Feiyang Chen",
        "Jianxiang Gao",
        "Junchen Liu",
        "Hangyu Liang",
        "Guangshuo Qin",
        "Chengrong Tian",
        "Bo Wen",
        "Longyu Zhao",
        "Xinrui Zheng",
        "Zeyu Mi",
        "Haibo Chen"
      ],
      "abstract": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
      "pdf_url": "http://arxiv.org/pdf/2507.20984v1",
      "published": "2025-07-28T16:45:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20984v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation",
      "authors": [
        "Rongyao Cai",
        "Ming Jin",
        "Qingsong Wen",
        "Kexin Zhang"
      ],
      "abstract": "Domain shift poses a fundamental challenge in time series analysis, where\nmodels trained on source domain often fail dramatically when applied in target\ndomain with different yet similar distributions. While current unsupervised\ndomain adaptation (UDA) methods attempt to align cross-domain feature\ndistributions, they typically treat features as indivisible entities, ignoring\ntheir intrinsic compositions that governs domain adaptation. We introduce\nDARSD, a novel UDA framework with theoretical explainability that explicitly\nrealizes UDA tasks from the perspective of representation space decomposition.\nOur core insight is that effective domain adaptation requires not just\nalignment, but principled disentanglement of transferable knowledge from mixed\nrepresentations. DARSD consists three synergistic components: (I) An\nadversarial learnable common invariant basis that projects original features\ninto a domain-invariant subspace while preserving semantic content; (II) A\nprototypical pseudo-labeling mechanism that dynamically separates target\nfeatures based on confidence, hindering error accumulation; (III) A hybrid\ncontrastive optimization strategy that simultaneously enforces feature\nclustering and consistency while mitigating emerging distribution gaps.\nComprehensive experiments conducted on four benchmark datasets (WISDM, HAR,\nHHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms,\nachieving optimal performance in 35 out of 53 cross-domain scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2507.20968v1",
      "published": "2025-07-28T16:26:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20968v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL",
      "authors": [
        "Hussein A. Ammar",
        "Raviraj Adve",
        "Shahram Shahbazpanahi",
        "Gary Boudreau",
        "Israfil Bahceci"
      ],
      "abstract": "In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user\nmobility necessitates updating the set of serving access points to maintain the\nuser-centric clustering. Such updates are typically performed through handoff\n(HO) operations; however, frequent HOs lead to overheads associated with the\nallocation and release of resources. This paper presents a deep reinforcement\nlearning (DRL)-based solution to predict and manage these connections for\nmobile users. Our solution employs the Soft Actor-Critic algorithm, with\ncontinuous action space representation, to train a deep neural network to serve\nas the HO policy. We present a novel proposition for a reward function that\nintegrates a HO penalty in order to balance the attainable rate and the\nassociated overhead related to HOs. We develop two variants of our system; the\nfirst one uses mobility direction-assisted (DA) observations that are based on\nthe user movement pattern, while the second one uses history-assisted (HA)\nobservations that are based on the history of the large-scale fading (LSF).\nSimulation results show that our DRL-based continuous action space approach is\nmore scalable than discrete space counterpart, and that our derived HO policy\nautomatically learns to gather HOs in specific time slots to minimize the\noverhead of initiating HOs. Our solution can also operate in real time with a\nresponse time less than 0.4 ms.",
      "pdf_url": "http://arxiv.org/pdf/2507.20966v1",
      "published": "2025-07-28T16:21:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20966v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "cs.NI",
        "eess.SP",
        "math.IT"
      ]
    },
    {
      "title": "Core Safety Values for Provably Corrigible Agents",
      "authors": [
        "Aran Nayebi"
      ],
      "abstract": "We introduce the first implementable framework for corrigibility, with\nprovable guarantees in multi-step, partially observed environments. Our\nframework replaces a single opaque reward with five *structurally separate*\nutility heads -- deference, switch-access preservation, truthfulness,\nlow-impact behavior via a belief-based extension of Attainable Utility\nPreservation, and bounded task reward -- combined lexicographically by strict\nweight gaps. Theorem 1 proves exact single-round corrigibility in the partially\nobservable off-switch game; Theorem 3 extends the guarantee to multi-step,\nself-spawning agents, showing that even if each head is \\emph{learned} to\nmean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal,\nthe probability of violating \\emph{any} safety property is bounded while still\nensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,\nwhich merge all norms into one learned scalar, our separation makes obedience\nand impact-limits dominate even when incentives conflict. For open-ended\nsettings where adversaries can modify the agent, we prove that deciding whether\nan arbitrary post-hack agent will ever violate corrigibility is undecidable by\nreduction to the halting problem, then carve out a finite-horizon ``decidable\nisland'' where safety can be certified in randomized polynomial time and\nverified with privacy-preserving, constant-round zero-knowledge proofs.\nConsequently, the remaining challenge is the ordinary ML task of data coverage\nand generalization: reward-hacking risk is pushed into evaluation quality\nrather than hidden incentive leak-through, giving clearer implementation\nguidance for today's LLM assistants and future autonomous systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.20964v1",
      "published": "2025-07-28T16:19:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20964v1",
      "categories": [
        "cs.AI",
        "cs.CC",
        "cs.GT",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "On the Limits of Hierarchically Embedded Logic in Classical Neural Networks",
      "authors": [
        "Bill Cochran"
      ],
      "abstract": "We propose a formal model of reasoning limitations in large neural net models\nfor language, grounded in the depth of their neural architecture. By treating\nneural networks as linear operators over logic predicate space we show that\neach layer can encode at most one additional level of logical reasoning. We\nprove that a neural network of depth a particular depth cannot faithfully\nrepresent predicates in a one higher order logic, such as simple counting over\ncomplex predicates, implying a strict upper bound on logical expressiveness.\nThis structure induces a nontrivial null space during tokenization and\nembedding, excluding higher-order predicates from representability. Our\nframework offers a natural explanation for phenomena such as hallucination,\nrepetition, and limited planning, while also providing a foundation for\nunderstanding how approximations to higher-order logic may emerge. These\nresults motivate architectural extensions and interpretability strategies in\nfuture development of language models.",
      "pdf_url": "http://arxiv.org/pdf/2507.20960v1",
      "published": "2025-07-28T16:13:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20960v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis",
      "authors": [
        "Hoyoung Lee",
        "Junhyuk Seo",
        "Suhwan Park",
        "Junhyeong Lee",
        "Wonbin Ahn",
        "Chanyeol Choi",
        "Alejandro Lopez-Lira",
        "Yongjae Lee"
      ],
      "abstract": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract models' latent preferences and measure\ntheir persistence. Focusing on sector, size, and momentum, our analysis reveals\ndistinct, model-specific tendencies. In particular, we observe a consistent\npreference for large-cap stocks and contrarian strategies across most models.\nThese preferences often harden into confirmation bias, with models clinging to\ninitial judgments despite counter-evidence.",
      "pdf_url": "http://arxiv.org/pdf/2507.20957v1",
      "published": "2025-07-28T16:09:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20957v1",
      "categories": [
        "q-fin.PM",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models",
      "authors": [
        "Max Peeperkorn",
        "Tom Kouwenhoven",
        "Dan Brown",
        "Anna Jordanous"
      ],
      "abstract": "Instruction-tuning large language models (LLMs) reduces the diversity of\ntheir outputs, which has implications for many tasks, particularly for creative\ntasks. This paper investigates the ``diversity gap'' for a writing prompt\nnarrative generation task. This gap emerges as measured by current diversity\nmetrics for various open-weight and open-source LLMs. The results show\nsignificant decreases in diversity due to instruction-tuning. We explore the\ndiversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to\nfurther understand how output diversity is affected. The results indicate that\nDPO has the most substantial impact on diversity. Motivated by these findings,\nwe present a new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases diversity and\neven maintains or improves quality.",
      "pdf_url": "http://arxiv.org/pdf/2507.20956v1",
      "published": "2025-07-28T16:04:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20956v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Partially Observable Monte-Carlo Graph Search",
      "authors": [
        "Yang You",
        "Vincent Thomas",
        "Alex Schutz",
        "Robert Skilton",
        "Nick Hawes",
        "Olivier Buffet"
      ],
      "abstract": "Currently, large partially observable Markov decision processes (POMDPs) are\noften solved by sampling-based online methods which interleave planning and\nexecution phases. However, a pre-computed offline policy is more desirable in\nPOMDP applications with time or energy constraints. But previous offline\nalgorithms are not able to scale up to large POMDPs. In this article, we\npropose a new sampling-based algorithm, the partially observable Monte-Carlo\ngraph search (POMCGS) to solve large POMDPs offline. Different from many online\nPOMDP methods, which progressively develop a tree while performing\n(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to\nconstruct a policy graph, so that computations can be drastically reduced, and\nusers can analyze and validate the policy prior to embedding and executing it.\nMoreover, POMCGS, together with action progressive widening and observation\nclustering methods provided in this article, is able to address certain\ncontinuous POMDPs. Through experiments, we demonstrate that POMCGS can generate\npolicies on the most challenging POMDPs, which cannot be computed by previous\noffline algorithms, and these policies' values are competitive compared with\nthe state-of-the-art online POMDP algorithms.",
      "pdf_url": "http://arxiv.org/pdf/2507.20951v1",
      "published": "2025-07-28T16:02:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20951v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Multivariate Conformal Prediction via Conformalized Gaussian Scoring",
      "authors": [
        "Sacha Braun",
        "Eugène Berta",
        "Michael I. Jordan",
        "Francis Bach"
      ],
      "abstract": "While achieving exact conditional coverage in conformal prediction is\nunattainable without making strong, untestable regularity assumptions, the\npromise of conformal prediction hinges on finding approximations to conditional\nguarantees that are realizable in practice. A promising direction for obtaining\nconditional dependence for conformal sets--in particular capturing\nheteroskedasticity--is through estimating the conditional density\n$\\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this\nvein has focused on nonconformity scores based on the empirical cumulative\ndistribution function (CDF). Such scores are, however, computationally costly,\ntypically requiring expensive sampling methods. To avoid the need for sampling,\nwe observe that the CDF-based score reduces to a Mahalanobis distance in the\ncase of Gaussian scores, yielding a closed-form expression that can be directly\nconformalized. Moreover, the use of a Gaussian-based score opens the door to a\nnumber of extensions of the basic conformal method; in particular, we show how\nto construct conformal sets with missing output values, refine conformal sets\nas partial information about $Y$ becomes available, and construct conformal\nsets on transformations of the output space. Finally, empirical results\nindicate that our approach produces conformal sets that more closely\napproximate conditional coverage in multivariate settings compared to\nalternative methods.",
      "pdf_url": "http://arxiv.org/pdf/2507.20941v1",
      "published": "2025-07-28T15:55:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20941v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME",
        "stat.OT"
      ]
    },
    {
      "title": "Dissecting Persona-Driven Reasoning in Language Models via Activation Patching",
      "authors": [
        "Ansh Poonia",
        "Maeghal Jain"
      ],
      "abstract": "Large language models (LLMs) exhibit remarkable versatility in adopting\ndiverse personas. In this study, we examine how assigning a persona influences\na model's reasoning on an objective task. Using activation patching, we take a\nfirst step toward understanding how key components of the model encode\npersona-specific information. Our findings reveal that the early Multi-Layer\nPerceptron (MLP) layers attend not only to the syntactic structure of the input\nbut also process its semantic content. These layers transform persona tokens\ninto richer representations, which are then used by the middle Multi-Head\nAttention (MHA) layers to shape the model's output. Additionally, we identify\nspecific attention heads that disproportionately attend to racial and\ncolor-based identities.",
      "pdf_url": "http://arxiv.org/pdf/2507.20936v1",
      "published": "2025-07-28T15:45:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20936v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models",
      "authors": [
        "Likun Tan",
        "Kuan-Wei Huang",
        "Kevin Wu"
      ],
      "abstract": "Hallucinations in large language models pose a critical challenge for\napplications requiring factual reliability, particularly in high-stakes domains\nsuch as finance. This work presents an effective approach for detecting and\nediting factually incorrect content in model-generated responses based on the\nprovided context. Given a user-defined domain-specific error taxonomy, we\nconstruct a synthetic dataset by inserting tagged errors into financial\nquestion-answering corpora and then fine-tune four language models, Phi-4,\nPhi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual\ninaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%\nimprovement in binary F1 score and a 30% gain in overall detection performance\ncompared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having\nonly 4 billion parameters, maintains competitive performance with just a 2%\ndrop in binary detection and a 0.1% decline in overall detection compared to\nOpenAI-o3. Our work provides a practical solution for detecting and editing\nfactual inconsistencies in financial text generation while introducing a\ngeneralizable framework that can enhance the trustworthiness and alignment of\nlarge language models across diverse applications beyond finance. Our code and\ndata are available at https://github.com/pegasi-ai/fine-grained-editting.",
      "pdf_url": "http://arxiv.org/pdf/2507.20930v1",
      "published": "2025-07-28T15:41:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20930v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models",
      "authors": [
        "Roberto Labadie-Tamayo",
        "Adrian Jaques Böck",
        "Djordje Slijepčević",
        "Xihui Chen",
        "Andreas Babic",
        "Matthias Zeppelzauer"
      ],
      "abstract": "Sexism has become widespread on social media and in online conversation. To\nhelp address this issue, the fifth Sexism Identification in Social Networks\n(EXIST) challenge is initiated at CLEF 2025. Among this year's international\nbenchmarks, we concentrate on solving the first task aiming to identify and\nclassify sexism in social media textual posts. In this paper, we describe our\nsolutions and report results for three subtasks: Subtask 1.1 - Sexism\nIdentification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask\n1.3 - Sexism Categorization in Tweets. We implement three models to address\neach subtask which constitute three individual runs: Speech Concept Bottleneck\nModel (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a\nfine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to encode input texts into a human-interpretable representation of\nadjectives, then used to train a lightweight classifier for downstream tasks.\nSCBMT extends SCBM by fusing adjective-based representation with contextual\nembeddings from transformers to balance interpretability and classification\nperformance. Beyond competitive results, these two models offer fine-grained\nexplanations at both instance (local) and class (global) levels. We also\ninvestigate how additional metadata, e.g., annotators' demographic profiles,\ncan be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data\naugmented with prior datasets, ranks 6th for English and Spanish and 4th for\nEnglish in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and\nSpanish and 6th for Spanish.",
      "pdf_url": "http://arxiv.org/pdf/2507.20924v1",
      "published": "2025-07-28T15:30:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20924v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SI",
        "I.2"
      ]
    },
    {
      "title": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization",
      "authors": [
        "Minh Hieu Ha",
        "Hung Phan",
        "Tung Duy Doan",
        "Tung Dao",
        "Dao Tran",
        "Huynh Thi Thanh Binh"
      ],
      "abstract": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE.",
      "pdf_url": "http://arxiv.org/pdf/2507.20923v1",
      "published": "2025-07-28T15:26:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20923v1",
      "categories": [
        "cs.NE",
        "cs.AI"
      ]
    },
    {
      "title": "Modeling User Behavior from Adaptive Surveys with Supplemental Context",
      "authors": [
        "Aman Shukla",
        "Daniel Patrick Scantlebury",
        "Rishabh Kumar"
      ],
      "abstract": "Modeling user behavior is critical across many industries where understanding\npreferences, intent, or decisions informs personalization, targeting, and\nstrategic outcomes. Surveys have long served as a classical mechanism for\ncollecting such behavioral data due to their interpretability, structure, and\nease of deployment. However, surveys alone are inherently limited by user\nfatigue, incomplete responses, and practical constraints on their length making\nthem insufficient for capturing user behavior. In this work, we present LANTERN\n(Late-Attentive Network for Enriched Response Modeling), a modular architecture\nfor modeling user behavior by fusing adaptive survey responses with\nsupplemental contextual signals. We demonstrate the architectural value of\nmaintaining survey primacy through selective gating, residual connections and\nlate fusion via cross-attention, treating survey data as the primary signal\nwhile incorporating external modalities only when relevant. LANTERN outperforms\nstrong survey-only baselines in multi-label prediction of survey responses. We\nfurther investigate threshold sensitivity and the benefits of selective\nmodality reliance through ablation and rare/frequent attribute analysis.\nLANTERN's modularity supports scalable integration of new encoders and evolving\ndatasets. This work provides a practical and extensible blueprint for behavior\nmodeling in survey-centric applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.20919v1",
      "published": "2025-07-28T15:19:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20919v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation",
      "authors": [
        "Adrien Bazoge"
      ],
      "abstract": "This work introduces MediQAl, a French medical question answering dataset\ndesigned to evaluate the capabilities of language models in factual medical\nrecall and reasoning over real-world clinical scenarios. MediQAl contains\n32,603 questions sourced from French medical examinations across 41 medical\nsubjects. The dataset includes three tasks: (i) Multiple-Choice Question with\nUnique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii)\nOpen-Ended Question with Short-Answer. Each question is labeled as\nUnderstanding or Reasoning, enabling a detailed analysis of models' cognitive\ncapabilities. We validate the MediQAl dataset through extensive evaluation with\n14 large language models, including recent reasoning-augmented models, and\nobserve a significant performance gap between factual recall and reasoning\ntasks. Our evaluation provides a comprehensive benchmark for assessing language\nmodels' performance on French medical question answering, addressing a crucial\ngap in multilingual resources for the medical domain.",
      "pdf_url": "http://arxiv.org/pdf/2507.20917v1",
      "published": "2025-07-28T15:17:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20917v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection",
      "authors": [
        "Jialei Cui",
        "Jianwei Du",
        "Yanzhe Li",
        "Lei Gao",
        "Hui Jiang",
        "Chenfu Bao"
      ],
      "abstract": "The rapid evolution of face manipulation techniques poses a critical\nchallenge for face forgery detection: cross-domain generalization. Conventional\nmethods, which rely on simple classification objectives, often fail to learn\ndomain-invariant representations. We propose HAMLET-FFD, a cognitively inspired\nHierarchical Adaptive Multi-modal Learning framework that tackles this\nchallenge via bidirectional cross-modal reasoning. Building on contrastive\nvision-language models such as CLIP, HAMLET-FFD introduces a knowledge\nrefinement loop that iteratively assesses authenticity by integrating visual\nevidence with conceptual cues, emulating expert forensic analysis. A key\ninnovation is a bidirectional fusion mechanism in which textual authenticity\nembeddings guide the aggregation of hierarchical visual features, while\nmodulated visual features refine text embeddings to generate image-adaptive\nprompts. This closed-loop process progressively aligns visual observations with\nsemantic priors to enhance authenticity assessment. By design, HAMLET-FFD\nfreezes all pretrained parameters, serving as an external plugin that preserves\nCLIP's original capabilities. Extensive experiments demonstrate its superior\ngeneralization to unseen manipulations across multiple benchmarks, and visual\nanalyses reveal a division of labor among embeddings, with distinct\nrepresentations specializing in fine-grained artifact recognition.",
      "pdf_url": "http://arxiv.org/pdf/2507.20913v1",
      "published": "2025-07-28T15:09:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20913v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SCORPION: Addressing Scanner-Induced Variability in Histopathology",
      "authors": [
        "Jeongun Ryu",
        "Heon Song",
        "Seungeun Lee",
        "Soo Ick Cho",
        "Jiwon Shin",
        "Kyunghyun Paeng",
        "Sérgio Pereira"
      ],
      "abstract": "Ensuring reliable model performance across diverse domains is a critical\nchallenge in computational pathology. A particular source of variability in\nWhole-Slide Images is introduced by differences in digital scanners, thus\ncalling for better scanner generalization. This is critical for the real-world\nadoption of computational pathology, where the scanning devices may differ per\ninstitution or hospital, and the model should not be dependent on\nscanner-induced details, which can ultimately affect the patient's diagnosis\nand treatment planning. However, past efforts have primarily focused on\nstandard domain generalization settings, evaluating on unseen scanners during\ntraining, without directly evaluating consistency across scanners for the same\ntissue. To overcome this limitation, we introduce SCORPION, a new dataset\nexplicitly designed to evaluate model reliability under scanner variability.\nSCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding\n2,400 spatially aligned patches. This scanner-paired design allows for the\nisolation of scanner-induced variability, enabling a rigorous evaluation of\nmodel consistency while controlling for differences in tissue composition.\nFurthermore, we propose SimCons, a flexible framework that combines\naugmentation-based domain generalization techniques with a consistency loss to\nexplicitly address scanner generalization. We empirically show that SimCons\nimproves model consistency on varying scanners without compromising\ntask-specific performance. By releasing the SCORPION dataset and proposing\nSimCons, we provide the research community with a crucial resource for\nevaluating and improving model consistency across diverse scanners, setting a\nnew standard for reliability testing.",
      "pdf_url": "http://arxiv.org/pdf/2507.20907v1",
      "published": "2025-07-28T15:00:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20907v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Music Arena: Live Evaluation for Text-to-Music",
      "authors": [
        "Yonghyun Kim",
        "Wayne Chi",
        "Anastasios N. Angelopoulos",
        "Wei-Lin Chiang",
        "Koichi Saito",
        "Shinji Watanabe",
        "Yuki Mitsufuji",
        "Chris Donahue"
      ],
      "abstract": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org",
      "pdf_url": "http://arxiv.org/pdf/2507.20900v1",
      "published": "2025-07-28T14:52:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20900v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment",
      "authors": [
        "Renhang Liu",
        "Chia-Yu Hung",
        "Navonil Majumder",
        "Taylor Gautreaux",
        "Amir Ali Bagherzadeh",
        "Chuan Li",
        "Dorien Herremans",
        "Soujanya Poria"
      ],
      "abstract": "Diffusion and flow-matching models have revolutionized automatic\ntext-to-audio generation in recent times. These models are increasingly capable\nof generating high quality and faithful audio outputs capturing to speech and\nacoustic events. However, there is still much room for improvement in creative\naudio generation that primarily involves music and songs. Recent open\nlyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an\nacceptable standard in automatic song generation for recreational use. However,\nthese models lack fine-grained word-level controllability often desired by\nmusicians in their workflows. To the best of our knowledge, our\nflow-matching-based JAM is the first effort toward endowing word-level timing\nand duration control in song generation, allowing fine-grained vocal control.\nTo enhance the quality of generated songs to better align with human\npreferences, we implement aesthetic alignment through Direct Preference\nOptimization, which iteratively refines the model using a synthetic dataset,\neliminating the need or manual data annotations. Furthermore, we aim to\nstandardize the evaluation of such lyrics-to-song models through our public\nevaluation dataset JAME. We show that JAM outperforms the existing models in\nterms of the music-specific attributes.",
      "pdf_url": "http://arxiv.org/pdf/2507.20880v1",
      "published": "2025-07-28T14:34:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20880v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease",
      "authors": [
        "Ahmed Sharshar",
        "Yasser Ashraf",
        "Tameem Bakr",
        "Salma Hassan",
        "Hosam Elgendy",
        "Mohammad Yaqub",
        "Mohsen Guizani"
      ],
      "abstract": "Alzheimer's disease affects over 55 million people worldwide and is projected\nto more than double by 2050, necessitating rapid, accurate, and scalable\ndiagnostics. However, existing approaches are limited because they cannot\nachieve clinically acceptable accuracy, generalization across datasets,\nrobustness to missing modalities, and explainability all at the same time. This\ninability to satisfy all these requirements simultaneously undermines their\nreliability in clinical settings. We propose OmniBrain, a multimodal framework\nthat integrates brain MRI, radiomics, gene expression, and clinical data using\na unified model with cross-attention and modality dropout. OmniBrain achieves\n$92.2 \\pm 2.4\\%$accuracy on the ANMerge dataset and generalizes to the MRI-only\nADNI dataset with $70.4 \\pm 2.7\\%$ accuracy, outperforming unimodal and prior\nmultimodal approaches. Explainability analyses highlight neuropathologically\nrelevant brain regions and genes, enhancing clinical trust. OmniBrain offers a\nrobust, interpretable, and practical solution for real-world Alzheimer's\ndiagnosis.",
      "pdf_url": "http://arxiv.org/pdf/2507.20872v1",
      "published": "2025-07-28T14:24:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20872v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces",
      "authors": [
        "Saket Tiwari",
        "Omer Gottesman",
        "George Konidaris"
      ],
      "abstract": "Advances in reinforcement learning (RL) have led to its successful\napplication in complex tasks with continuous state and action spaces. Despite\nthese advances in practice, most theoretical work pertains to finite state and\naction spaces. We propose building a theoretical understanding of continuous\nstate and action spaces by employing a geometric lens to understand the locally\nattained set of states. The set of all parametrised policies learnt through a\nsemi-gradient based approach induces a set of attainable states in RL. We show\nthat the training dynamics of a two-layer neural policy induce a low\ndimensional manifold of attainable states embedded in the high-dimensional\nnominal state space trained using an actor-critic algorithm. We prove that,\nunder certain conditions, the dimensionality of this manifold is of the order\nof the dimensionality of the action space. This is the first result of its\nkind, linking the geometry of the state space to the dimensionality of the\naction space. We empirically corroborate this upper bound for four MuJoCo\nenvironments and also demonstrate the results in a toy environment with varying\ndimensionality. We also show the applicability of this theoretical result by\nintroducing a local manifold learning layer to the policy and value function\nnetworks to improve the performance in control environments with very high\ndegrees of freedom by changing one layer of the neural network to learn sparse\nrepresentations.",
      "pdf_url": "http://arxiv.org/pdf/2507.20853v1",
      "published": "2025-07-28T14:06:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20853v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments",
      "authors": [
        "Meiting Dang",
        "Yanping Wu",
        "Yafei Wang",
        "Dezong Zhao",
        "David Flynn",
        "Chongfeng Wei"
      ],
      "abstract": "Recent advances in autonomous vehicle (AV) behavior planning have shown\nimpressive social interaction capabilities when interacting with other road\nusers. However, achieving human-like prediction and decision-making in\ninteractions with vulnerable road users remains a key challenge in complex\nmulti-agent interactive environments. Existing research focuses primarily on\ncrowd navigation for small mobile robots, which cannot be directly applied to\nAVs due to inherent differences in their decision-making strategies and dynamic\nboundaries. Moreover, pedestrians in these multi-agent simulations follow fixed\nbehavior patterns that cannot dynamically respond to AV actions. To overcome\nthese limitations, this paper proposes a novel framework for modeling\ninteractions between the AV and multiple pedestrians. In this framework, a\ncognitive process modeling approach inspired by the Free Energy Principle is\nintegrated into both the AV and pedestrian models to simulate more realistic\ninteraction dynamics. Specifically, the proposed pedestrian Cognitive-Risk\nSocial Force Model adjusts goal-directed and repulsive forces using a fused\nmeasure of cognitive uncertainty and physical risk to produce human-like\ntrajectories. Meanwhile, the AV leverages this fused risk to construct a\ndynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a\nSoft Actor-Critic architecture, allowing it to make more reasonable and\ninformed decisions. Simulation results indicate that our proposed framework\neffectively improves safety, efficiency, and smoothness of AV navigation\ncompared to the state-of-the-art method.",
      "pdf_url": "http://arxiv.org/pdf/2507.20850v1",
      "published": "2025-07-28T14:02:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20850v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "First Hallucination Tokens Are Different from Conditional Ones",
      "authors": [
        "Jakob Snel",
        "Seong Joon Oh"
      ],
      "abstract": "Hallucination, the generation of untruthful content, is one of the major\nconcerns regarding foundational models. Detecting hallucinations at the token\nlevel is vital for real-time filtering and targeted correction, yet the\nvariation of hallucination signals within token sequences is not fully\nunderstood. Leveraging the RAGTruth corpus with token-level annotations and\nreproduced logits, we analyse how these signals depend on a token's position\nwithin hallucinated spans, contributing to an improved understanding of\ntoken-level hallucination. Our results show that the first hallucinated token\ncarries a stronger signal and is more detectable than conditional tokens. We\nrelease our analysis framework, along with code for logit reproduction and\nmetric computation at https://github.com/jakobsnl/RAGTruth_Xtended.",
      "pdf_url": "http://arxiv.org/pdf/2507.20836v1",
      "published": "2025-07-28T13:44:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20836v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Why Flow Matching is Particle Swarm Optimization?",
      "authors": [
        "Kaichen Ouyang"
      ],
      "abstract": "This paper preliminarily investigates the duality between flow matching in\ngenerative models and particle swarm optimization (PSO) in evolutionary\ncomputation. Through theoretical analysis, we reveal the intrinsic connections\nbetween these two approaches in terms of their mathematical formulations and\noptimization mechanisms: the vector field learning in flow matching shares\nsimilar mathematical expressions with the velocity update rules in PSO; both\nmethods follow the fundamental framework of progressive evolution from initial\nto target distributions; and both can be formulated as dynamical systems\ngoverned by ordinary differential equations. Our study demonstrates that flow\nmatching can be viewed as a continuous generalization of PSO, while PSO\nprovides a discrete implementation of swarm intelligence principles. This\nduality understanding establishes a theoretical foundation for developing novel\nhybrid algorithms and creates a unified framework for analyzing both methods.\nAlthough this paper only presents preliminary discussions, the revealed\ncorrespondences suggest several promising research directions, including\nimproving swarm intelligence algorithms based on flow matching principles and\nenhancing generative models using swarm intelligence concepts.",
      "pdf_url": "http://arxiv.org/pdf/2507.20810v1",
      "published": "2025-07-28T13:21:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20810v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs",
      "authors": [
        "Xueyao Wan",
        "Hang Yu"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances language model generation by\nretrieving relevant information from external knowledge bases. However,\nconventional RAG methods face the issue of missing multimodal information.\nMultimodal RAG methods address this by fusing images and text through mapping\nthem into a shared embedding space, but they fail to capture the structure of\nknowledge and logical chains between modalities. Moreover, they also require\nlarge-scale training for specific tasks, resulting in limited generalizing\nability. To address these limitations, we propose MMGraphRAG, which refines\nvisual content through scene graphs and constructs a multimodal knowledge graph\n(MMKG) in conjunction with text-based KG. It employs spectral clustering to\nachieve cross-modal entity linking and retrieves context along reasoning paths\nto guide the generative process. Experimental results show that MMGraphRAG\nachieves state-of-the-art performance on the DocBench and MMLongBench datasets,\ndemonstrating strong domain adaptability and clear reasoning paths.",
      "pdf_url": "http://arxiv.org/pdf/2507.20804v1",
      "published": "2025-07-28T13:16:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20804v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations",
      "authors": [
        "Vinil Polepalli"
      ],
      "abstract": "The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2507.20800v1",
      "published": "2025-07-28T13:08:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20800v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach",
      "authors": [
        "Wei Lu",
        "Daniel L. Chen",
        "Christian B. Hansen"
      ],
      "abstract": "Understanding how large language model (LLM) agents behave in strategic\ninteractions is essential as these systems increasingly participate\nautonomously in economically and morally consequential decisions. We evaluate\nLLM preferences using canonical economic games, finding substantial deviations\nfrom human behavior. Models like GPT-4o show excessive cooperation and limited\nincentive sensitivity, while reasoning models, such as o3-mini, align more\nconsistently with payoff-maximizing strategies. We propose a supervised\nfine-tuning pipeline that uses synthetic datasets derived from economic\nreasoning to align LLM agents with economic preferences, focusing on two\nstylized preference structures. In the first, utility depends only on\nindividual payoffs (homo economicus), while utility also depends on a notion of\nKantian universalizability in the second preference structure (homo moralis).\nWe find that fine-tuning based on small datasets shifts LLM agent behavior\ntoward the corresponding economic agent. We further assess the fine-tuned\nagents' behavior in two applications: Moral dilemmas involving autonomous\nvehicles and algorithmic pricing in competitive markets. These examples\nillustrate how different normative objectives embedded via realizations from\nstructured preference structures can influence market and moral outcomes. This\nwork contributes a replicable, cost-efficient, and economically grounded\npipeline to align AI preferences using moral-economic principles.",
      "pdf_url": "http://arxiv.org/pdf/2507.20796v1",
      "published": "2025-07-28T13:05:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20796v1",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.LG",
        "q-fin.EC"
      ]
    },
    {
      "title": "Investigation of Accuracy and Bias in Face Recognition Trained with Synthetic Data",
      "authors": [
        "Pavel Korshunov",
        "Ketan Kotwal",
        "Christophe Ecabert",
        "Vidit Vidit",
        "Amir Mohammadi",
        "Sebastien Marcel"
      ],
      "abstract": "Synthetic data has emerged as a promising alternative for training face\nrecognition (FR) models, offering advantages in scalability, privacy\ncompliance, and potential for bias mitigation. However, critical questions\nremain on whether both high accuracy and fairness can be achieved with\nsynthetic data. In this work, we evaluate the impact of synthetic data on bias\nand performance of FR systems. We generate balanced face dataset, FairFaceGen,\nusing two state of the art text-to-image generators, Flux.1-dev and Stable\nDiffusion v3.5 (SD35), and combine them with several identity augmentation\nmethods, including Arc2Face and four IP-Adapters. By maintaining equal identity\ncount across synthetic and real datasets, we ensure fair comparisons when\nevaluating FR performance on standard (LFW, AgeDB-30, etc.) and challenging\nIJB-B/C benchmarks and FR bias on Racial Faces in-the-Wild (RFW) dataset. Our\nresults demonstrate that although synthetic data still lags behind the real\ndatasets in the generalization on IJB-B/C, demographically balanced synthetic\ndatasets, especially those generated with SD35, show potential for bias\nmitigation. We also observe that the number and quality of intra-class\naugmentations significantly affect FR accuracy and fairness. These findings\nprovide practical guidelines for constructing fairer FR systems using synthetic\ndata.",
      "pdf_url": "http://arxiv.org/pdf/2507.20782v1",
      "published": "2025-07-28T12:52:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20782v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments",
      "authors": [
        "Fatou Ndiaye Mbodji"
      ],
      "abstract": "Smart contract comment generation has gained traction as a means to improve\ncode comprehension and maintainability in blockchain systems. However,\nevaluating the quality of generated comments remains a challenge. Traditional\nmetrics such as BLEU and ROUGE fail to capture domain-specific nuances, while\nhuman evaluation is costly and unscalable. In this paper, we present\n\\texttt{evalSmarT}, a modular and extensible framework that leverages large\nlanguage models (LLMs) as evaluators. The system supports over 400 evaluator\nconfigurations by combining approximately 40 LLMs with 10 prompting strategies.\nWe demonstrate its application in benchmarking comment generation tools and\nselecting the most informative outputs. Our results show that prompt design\nsignificantly impacts alignment with human judgment, and that LLM-based\nevaluation offers a scalable and semantically rich alternative to existing\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2507.20774v1",
      "published": "2025-07-28T12:37:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20774v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation",
      "authors": [
        "Hao Yang",
        "Qinghua Zhao",
        "Lei Li"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet\nits internal mechanisms remain poorly understood. We analyze CoT's operational\nprinciples by reversely tracing information flow across decoding, projection,\nand activation phases. Our quantitative analysis suggests that CoT may serve as\na decoding space pruner, leveraging answer templates to guide output\ngeneration, with higher template adherence strongly correlating with improved\nperformance. Furthermore, we surprisingly find that CoT modulates neuron\nengagement in a task-dependent manner: reducing neuron activation in\nopen-domain tasks, yet increasing it in closed-domain scenarios. These findings\noffer a novel mechanistic interpretability framework and critical insights for\nenabling targeted CoT interventions to design more efficient and robust\nprompts. We released our code and data at\nhttps://anonymous.4open.science/r/cot-D247.",
      "pdf_url": "http://arxiv.org/pdf/2507.20758v1",
      "published": "2025-07-28T12:11:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20758v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry",
      "authors": [
        "Matan Kichler",
        "Shai Bagon",
        "Mark Sheinin"
      ],
      "abstract": "Computer vision seeks to infer a wide range of information about objects and\nevents. However, vision systems based on conventional imaging are limited to\nextracting information only from the visible surfaces of scene objects. For\ninstance, a vision system can detect and identify a Coke can in the scene, but\nit cannot determine whether the can is full or empty. In this paper, we aim to\nexpand the scope of computer vision to include the novel task of inferring the\nhidden liquid levels of opaque containers by sensing the tiny vibrations on\ntheir surfaces. Our method provides a first-of-a-kind way to inspect the fill\nlevel of multiple sealed containers remotely, at once, without needing physical\nmanipulation and manual weighing. First, we propose a novel speckle-based\nvibration sensing system for simultaneously capturing scene vibrations on a 2D\ngrid of points. We use our system to efficiently and remotely capture a dataset\nof vibration responses for a variety of everyday liquid containers. Then, we\ndevelop a transformer-based approach for analyzing the captured vibrations and\nclassifying the container type and its hidden liquid level at the time of\nmeasurement. Our architecture is invariant to the vibration source, yielding\ncorrect liquid level estimates for controlled and ambient scene sound sources.\nMoreover, our model generalizes to unseen container instances within known\nclasses (e.g., training on five Coke cans of a six-pack, testing on a sixth)\nand fluid levels. We demonstrate our method by recovering liquid levels from\nvarious everyday containers.",
      "pdf_url": "http://arxiv.org/pdf/2507.20757v1",
      "published": "2025-07-28T12:11:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20757v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours",
      "authors": [
        "Arpan Dasgupta",
        "Sarvesh Gharat",
        "Neha Madhiwalla",
        "Aparna Hegde",
        "Milind Tambe",
        "Aparna Taneja"
      ],
      "abstract": "Automated voice calls with health information are a proven method for\ndisseminating maternal and child health information among beneficiaries and are\ndeployed in several programs around the world. However, these programs often\nsuffer from beneficiary dropoffs and poor engagement. In previous work, through\nreal-world trials, we showed that an AI model, specifically a restless bandit\nmodel, could identify beneficiaries who would benefit most from live service\ncall interventions, preventing dropoffs and boosting engagement. However, one\nkey question has remained open so far: does such improved listenership via\nAI-targeted interventions translate into beneficiaries' improved knowledge and\nhealth behaviors? We present a first study that shows not only listenership\nimprovements due to AI interventions, but also simultaneously links these\nimprovements to health behavior changes. Specifically, we demonstrate that\nAI-scheduled interventions, which enhance listenership, lead to statistically\nsignificant improvements in beneficiaries' health behaviors such as taking iron\nor calcium supplements in the postnatal period, as well as understanding of\ncritical health topics during pregnancy and infancy. This underscores the\npotential of AI to drive meaningful improvements in maternal and child health.",
      "pdf_url": "http://arxiv.org/pdf/2507.20755v1",
      "published": "2025-07-28T12:06:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20755v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Industry Insights from Comparing Deep Learning and GBDT Models for E-Commerce Learning-to-Rank",
      "authors": [
        "Yunus Lutz",
        "Timo Wilm",
        "Philipp Duwe"
      ],
      "abstract": "In e-commerce recommender and search systems, tree-based models, such as\nLambdaMART, have set a strong baseline for Learning-to-Rank (LTR) tasks.\nDespite their effectiveness and widespread adoption in industry, the debate\ncontinues whether deep neural networks (DNNs) can outperform traditional\ntree-based models in this domain. To contribute to this discussion, we\nsystematically benchmark DNNs against our production-grade LambdaMART model. We\nevaluate multiple DNN architectures and loss functions on a proprietary dataset\nfrom OTTO and validate our findings through an 8-week online A/B test. The\nresults show that a simple DNN architecture outperforms a strong tree-based\nbaseline in terms of total clicks and revenue, while achieving parity in total\nunits sold.",
      "pdf_url": "http://arxiv.org/pdf/2507.20753v1",
      "published": "2025-07-28T12:02:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20753v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AR-LIF: Adaptive reset leaky-integrate and fire neuron for spiking neural networks",
      "authors": [
        "Zeyu Huang",
        "Wei Meng",
        "Quan Liu",
        "Kun Chen",
        "Li Ma"
      ],
      "abstract": "Spiking neural networks possess the advantage of low energy consumption due\nto their event-driven nature. Compared with binary spike outputs, their\ninherent floating-point dynamics are more worthy of attention. The threshold\nlevel and reset mode of neurons play a crucial role in determining the number\nand timing of spikes. The existing hard reset method causes information loss,\nwhile the improved soft reset method adopts a uniform treatment for neurons. In\nresponse to this, this paper designs an adaptive reset neuron, establishing the\ncorrelation between input, output and reset, and integrating a simple yet\neffective threshold adjustment strategy. It achieves excellent performance on\nvarious datasets while maintaining the advantage of low energy consumption.",
      "pdf_url": "http://arxiv.org/pdf/2507.20746v1",
      "published": "2025-07-28T11:54:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20746v1",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Regularizing Subspace Redundancy of Low-Rank Adaptation",
      "authors": [
        "Yue Zhu",
        "Haiwen Diao",
        "Shang Gao",
        "Jiazuo Yu",
        "Jiawen Zhu",
        "Yunzhi Zhuge",
        "Shuai Hao",
        "Xu Jia",
        "Lu Zhang",
        "Ying Zhang",
        "Huchuan Lu"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) and its variants have delivered strong capability\nin Parameter-Efficient Transfer Learning (PETL) by minimizing trainable\nparameters and benefiting from reparameterization. However, their projection\nmatrices remain unrestricted during training, causing high representation\nredundancy and diminishing the effectiveness of feature adaptation in the\nresulting subspaces. While existing methods mitigate this by manually adjusting\nthe rank or implicitly applying channel-wise masks, they lack flexibility and\ngeneralize poorly across various datasets and architectures. Hence, we propose\nReSoRA, a method that explicitly models redundancy between mapping subspaces\nand adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.\nSpecifically, it theoretically decomposes the low-rank submatrices into\nmultiple equivalent subspaces and systematically applies de-redundancy\nconstraints to the feature distributions across different projections.\nExtensive experiments validate that our proposed method consistently\nfacilitates existing state-of-the-art PETL methods across various backbones and\ndatasets in vision-language retrieval and standard visual classification\nbenchmarks. Besides, as a training supervision, ReSoRA can be seamlessly\nintegrated into existing approaches in a plug-and-play manner, with no\nadditional inference costs. Code is publicly available at:\nhttps://github.com/Lucenova/ReSoRA.",
      "pdf_url": "http://arxiv.org/pdf/2507.20745v1",
      "published": "2025-07-28T11:52:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20745v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals",
      "authors": [
        "Geng-Xin Xu",
        "Xiang Zuo",
        "Ye Li"
      ],
      "abstract": "Emotion recognition from physiological data is crucial for mental health\nassessment, yet it faces two significant challenges: incomplete multi-modal\nsignals and interference from body movements and artifacts. This paper presents\na novel Multi-Masked Querying Network (MMQ-Net) to address these issues by\nintegrating multiple querying mechanisms into a unified framework.\nSpecifically, it uses modality queries to reconstruct missing data from\nincomplete signals, category queries to focus on emotional state features, and\ninterference queries to separate relevant information from noise. Extensive\nexperiment results demonstrate the superior emotion recognition performance of\nMMQ-Net compared to existing approaches, particularly under high levels of data\nincompleteness.",
      "pdf_url": "http://arxiv.org/pdf/2507.20737v1",
      "published": "2025-07-28T11:41:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20737v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Learning the Value Systems of Societies from Preferences",
      "authors": [
        "Andrés Holgado-Sánchez",
        "Holger Billhardt",
        "Sascha Ossowski",
        "Sara Degli-Esposti"
      ],
      "abstract": "Aligning AI systems with human values and the value-based preferences of\nvarious stakeholders (their value systems) is key in ethical AI. In value-aware\nAI systems, decision-making draws upon explicit computational representations\nof individual values (groundings) and their aggregation into value systems. As\nthese are notoriously difficult to elicit and calibrate manually, value\nlearning approaches aim to automatically derive computational models of an\nagent's values and value system from demonstrations of human behaviour.\nNonetheless, social science and humanities literature suggest that it is more\nadequate to conceive the value system of a society as a set of value systems of\ndifferent groups, rather than as the simple aggregation of individual value\nsystems. Accordingly, here we formalize the problem of learning the value\nsystems of societies and propose a method to address it based on heuristic deep\nclustering. The method learns socially shared value groundings and a set of\ndiverse value systems representing a given society by observing qualitative\nvalue-based preferences from a sample of agents. We evaluate the proposal in a\nuse case with real data about travelling decisions.",
      "pdf_url": "http://arxiv.org/pdf/2507.20728v1",
      "published": "2025-07-28T11:25:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20728v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ]
    },
    {
      "title": "Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI",
      "authors": [
        "Asma Sadia Khan",
        "Fariba Tasnia Khan",
        "Tanjim Mahmud",
        "Salman Karim Khan",
        "Rishita Chakma",
        "Nahed Sharmen",
        "Mohammad Shahadat Hossain",
        "Karl Andersson"
      ],
      "abstract": "Prostate cancer, the second most prevalent male malignancy, requires advanced\ndiagnostic tools. We propose an explainable AI system combining BERT (for\ntextual clinical notes) and Random Forest (for numerical lab data) through a\nnovel multimodal fusion strategy, achieving superior classification performance\non PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is\nestablished, our work demonstrates that a simple yet interpretable BERT+RF\npipeline delivers clinically significant improvements - particularly for\nintermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824\nnumerical/0.725 textual). SHAP analysis provides transparent feature importance\nrankings, while ablation studies prove textual features' complementary value.\nThis accessible approach offers hospitals a balance of high performance\n(F1=89%), computational efficiency, and clinical interpretability - addressing\ncritical needs in prostate cancer diagnostics.",
      "pdf_url": "http://arxiv.org/pdf/2507.20714v1",
      "published": "2025-07-28T11:07:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20714v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM",
        "stat.AP"
      ]
    },
    {
      "title": "Algorithmic Fairness: A Runtime Perspective",
      "authors": [
        "Filip Cano",
        "Thomas A. Henzinger",
        "Konstantin Kueffner"
      ],
      "abstract": "Fairness in AI is traditionally studied as a static property evaluated once,\nover a fixed dataset. However, real-world AI systems operate sequentially, with\noutcomes and environments evolving over time. This paper proposes a framework\nfor analysing fairness as a runtime property. Using a minimal yet expressive\nmodel based on sequences of coin tosses with possibly evolving biases, we study\nthe problems of monitoring and enforcing fairness expressed in either toss\noutcomes or coin biases. Since there is no one-size-fits-all solution for\neither problem, we provide a summary of monitoring and enforcement strategies,\nparametrised by environment dynamics, prediction horizon, and confidence\nthresholds. For both problems, we present general results under simple or\nminimal assumptions. We survey existing solutions for the monitoring problem\nfor Markovian and additive dynamics, and existing solutions for the enforcement\nproblem in static settings with known dynamics.",
      "pdf_url": "http://arxiv.org/pdf/2507.20711v1",
      "published": "2025-07-28T11:04:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20711v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models",
      "authors": [
        "Gabriel Downer",
        "Sean Craven",
        "Damian Ruck",
        "Jake Thomas"
      ],
      "abstract": "The increasing integration of Visual Language Models (VLMs) into AI systems\nnecessitates robust model alignment, especially when handling multimodal\ncontent that combines text and images. Existing evaluation datasets heavily\nlean towards text-only prompts, leaving visual vulnerabilities under evaluated.\nTo address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline\nthat adapts text-only datasets into multimodal formats, specifically designed\nto evaluate the resilience of VLMs against typographic prompt injection\nattacks. The Text2VLM pipeline identifies harmful content in the original text\nand converts it into a typographic image, creating a multimodal prompt for\nVLMs. Also, our evaluation of open-source VLMs highlights their increased\nsusceptibility to prompt injection when visual inputs are introduced, revealing\ncritical weaknesses in the current models' alignment. This is in addition to a\nsignificant performance gap compared to closed-source frontier models. We\nvalidate Text2VLM through human evaluations, ensuring the alignment of\nextracted salient concepts; text summarization and output classification align\nwith human expectations. Text2VLM provides a scalable tool for comprehensive\nsafety assessment, contributing to the development of more robust safety\nmechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,\nText2VLM plays a role in advancing the safe deployment of VLMs in diverse,\nreal-world applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.20704v1",
      "published": "2025-07-28T10:57:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.20704v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ]
    }
  ]
}