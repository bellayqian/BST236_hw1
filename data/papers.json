{
  "last_updated": "2025-11-12T00:52:40.227841",
  "papers": [
    {
      "title": "Alpha MAML: Adaptive Model-Agnostic Meta-Learning",
      "authors": [
        "Harkirat Singh Behl",
        "Atılım Güneş Baydin",
        "Philip H. S. Torr"
      ],
      "abstract": "Model-agnostic meta-learning (MAML) is a meta-learning technique to train a model on a multitude of learning tasks in a way that primes the model for few-shot learning of new tasks. The MAML algorithm performs well on few-shot learning problems in classification, regression, and fine-tuning of policy gradients in reinforcement learning, but comes with the need for costly hyperparameter tuning for training stability. We address this shortcoming by introducing an extension to MAML, called Alpha MAML, to incorporate an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates. Our results with the Omniglot database demonstrate a substantial reduction in the need to tune MAML training hyperparameters and improvement to training stability with less sensitivity to hyperparameter choice.",
      "pdf_url": null,
      "published": "2019-05-17T18:45:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/1905.07435v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Emotion in Reinforcement Learning Agents and Robots: A Survey",
      "authors": [
        "Thomas M. Moerland",
        "Joost Broekens",
        "Catholijn M. Jonker"
      ],
      "abstract": "This article provides the first survey of computational models of emotion in reinforcement learning (RL) agents. The survey focuses on agent/robot emotions, and mostly ignores human user emotions. Emotions are recognized as functional in decision-making by influencing motivation and action selection. Therefore, computational emotion models are usually grounded in the agent's decision making architecture, of which RL is an important subclass. Studying emotions in RL-based agents is useful for three research fields. For machine learning (ML) researchers, emotion models may improve learning efficiency. For the interactive ML and human-robot interaction (HRI) community, emotions can communicate state and enhance user investment. Lastly, it allows affective modelling (AM) researchers to investigate their emotion theories in a successful AI agent class. This survey provides background on emotion theory and RL. It systematically addresses 1) from what underlying dimensions (e.g., homeostasis, appraisal) emotions can be derived and how these can be modelled in RL-agents, 2) what types of emotions have been derived from these dimensions, and 3) how these emotions may either influence the learning efficiency of the agent or be useful as social signals. We also systematically compare evaluation criteria, and draw connections to important RL sub-domains like (intrinsic) motivation and model-based RL. In short, this survey provides both a practical overview for engineers wanting to implement emotions in their RL agents, and identifies challenges and directions for future emotion-RL research.",
      "pdf_url": null,
      "published": "2017-05-15T11:49:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/1705.05172v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.RO",
        "stat.ML"
      ]
    },
    {
      "title": "Transfer Learning with Pre-trained Conditional Generative Models",
      "authors": [
        "Shin'ya Yamaguchi",
        "Sekitoshi Kanai",
        "Atsutoshi Kumagai",
        "Daiki Chijiwa",
        "Hisashi Kashima"
      ],
      "abstract": "Transfer learning is crucial in training deep neural networks on new target tasks. Current transfer learning methods always assume at least one of (i) source and target task label spaces overlap, (ii) source datasets are available, and (iii) target network architectures are consistent with source ones. However, holding these assumptions is difficult in practical settings because the target task rarely has the same labels as the source task, the source dataset access is restricted due to storage costs and privacy, and the target architecture is often specialized to each task. To transfer source knowledge without these assumptions, we propose a transfer learning method that uses deep generative models and is composed of the following two stages: pseudo pre-training (PP) and pseudo semi-supervised learning (P-SSL). PP trains a target architecture with an artificial dataset synthesized by using conditional source generative models. P-SSL applies SSL algorithms to labeled target data and unlabeled pseudo samples, which are generated by cascading the source classifier and generative models to condition them with target samples. Our experimental results indicate that our method can outperform the baselines of scratch training and knowledge distillation.",
      "pdf_url": null,
      "published": "2022-04-27T10:36:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2204.12833v3",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "ChainerRL: A Deep Reinforcement Learning Library",
      "authors": [
        "Yasuhiro Fujita",
        "Prabhat Nagarajan",
        "Toshiki Kataoka",
        "Takahiro Ishikawa"
      ],
      "abstract": "In this paper, we introduce ChainerRL, an open-source deep reinforcement learning (DRL) library built using Python and the Chainer deep learning framework. ChainerRL implements a comprehensive set of DRL algorithms and techniques drawn from state-of-the-art research in the field. To foster reproducible research, and for instructional purposes, ChainerRL provides scripts that closely replicate the original papers' experimental settings and reproduce published benchmark results for several algorithms. Lastly, ChainerRL offers a visualization tool that enables the qualitative inspection of trained agents. The ChainerRL source code can be found on GitHub: https://github.com/chainer/chainerrl.",
      "pdf_url": null,
      "published": "2019-12-09T08:59:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/1912.03905v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Temporal Supervised Contrastive Learning for Modeling Patient Risk Progression",
      "authors": [
        "Shahriar Noroozizadeh",
        "Jeremy C. Weiss",
        "George H. Chen"
      ],
      "abstract": "We consider the problem of predicting how the likelihood of an outcome of interest for a patient changes over time as we observe more of the patient data. To solve this problem, we propose a supervised contrastive learning framework that learns an embedding representation for each time step of a patient time series. Our framework learns the embedding space to have the following properties: (1) nearby points in the embedding space have similar predicted class probabilities, (2) adjacent time steps of the same time series map to nearby points in the embedding space, and (3) time steps with very different raw feature vectors map to far apart regions of the embedding space. To achieve property (3), we employ a nearest neighbor pairing mechanism in the raw feature space. This mechanism also serves as an alternative to data augmentation, a key ingredient of contrastive learning, which lacks a standard procedure that is adequately realistic for clinical tabular data, to our knowledge. We demonstrate that our approach outperforms state-of-the-art baselines in predicting mortality of septic patients (MIMIC-III dataset) and tracking progression of cognitive impairment (ADNI dataset). Our method also consistently recovers the correct synthetic dataset embedding structure across experiments, a feat not achieved by baselines. Our ablation experiments show the pivotal role of our nearest neighbor pairing.",
      "pdf_url": null,
      "published": "2023-12-10T16:43:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2312.05933v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Energy-Based Models for Continual Learning",
      "authors": [
        "Shuang Li",
        "Yilun Du",
        "Gido M. van de Ven",
        "Igor Mordatch"
      ],
      "abstract": "We motivate Energy-Based Models (EBMs) as a promising model class for continual learning problems. Instead of tackling continual learning via the use of external memory, growing models, or regularization, EBMs change the underlying training objective to cause less interference with previously learned information. Our proposed version of EBMs for continual learning is simple, efficient, and outperforms baseline methods by a large margin on several benchmarks. Moreover, our proposed contrastive divergence-based training objective can be combined with other continual learning methods, resulting in substantial boosts in their performance. We further show that EBMs are adaptable to a more general continual learning setting where the data distribution changes without the notion of explicitly delineated tasks. These observations point towards EBMs as a useful building block for future continual learning methods.",
      "pdf_url": null,
      "published": "2020-11-24T17:08:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2011.12216v3",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Unsupervised Representation Learning with Minimax Distance Measures",
      "authors": [
        "Morteza Haghir Chehreghani"
      ],
      "abstract": "We investigate the use of Minimax distances to extract in a nonparametric way the features that capture the unknown underlying patterns and structures in the data. We develop a general-purpose and computationally efficient framework to employ Minimax distances with many machine learning methods that perform on numerical data. We study both computing the pairwise Minimax distances for all pairs of objects and as well as computing the Minimax distances of all the objects to/from a fixed (test) object. We first efficiently compute the pairwise Minimax distances between the objects, using the equivalence of Minimax distances over a graph and over a minimum spanning tree constructed on that. Then, we perform an embedding of the pairwise Minimax distances into a new vector space, such that their squared Euclidean distances in the new space equal to the pairwise Minimax distances in the original space. We also study the case of having multiple pairwise Minimax matrices, instead of a single one. Thereby, we propose an embedding via first summing up the centered matrices and then performing an eigenvalue decomposition to obtain the relevant features. In the following, we study computing Minimax distances from a fixed (test) object which can be used for instance in K-nearest neighbor search. Similar to the case of all-pair pairwise Minimax distances, we develop an efficient and general-purpose algorithm that is applicable with any arbitrary base distance measure. Moreover, we investigate in detail the edges selected by the Minimax distances and thereby explore the ability of Minimax distances in detecting outlier objects. Finally, for each setting, we perform several experiments to demonstrate the effectiveness of our framework.",
      "pdf_url": null,
      "published": "2019-04-27T16:13:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/1904.13223v3",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Open-Ended Learning Leads to Generally Capable Agents",
      "authors": [
        "Open Ended Learning Team",
        "Adam Stooke",
        "Anuj Mahajan",
        "Catarina Barros",
        "Charlie Deck",
        "Jakob Bauer",
        "Jakub Sygnowski",
        "Maja Trebacz",
        "Max Jaderberg",
        "Michael Mathieu",
        "Nat McAleese",
        "Nathalie Bradley-Schmieg",
        "Nathaniel Wong",
        "Nicolas Porcel",
        "Roberta Raileanu",
        "Steph Hughes-Fitt",
        "Valentin Dalibard",
        "Wojciech Marian Czarnecki"
      ],
      "abstract": "In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",
      "pdf_url": null,
      "published": "2021-07-27T13:30:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2107.12808v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Learning and Improving Backgammon Strategy",
      "authors": [
        "Gregory R. Galperin"
      ],
      "abstract": "A novel approach to learning is presented, combining features of on-line and off-line methods to achieve considerable performance in the task of learning a backgammon value function in a process that exploits the processing power of parallel supercomputers. The off-line methods comprise a set of techniques for parallelizing neural network training and $TD(λ)$ reinforcement learning; here Monte-Carlo ``Rollouts'' are introduced as a massively parallel on-line policy improvement technique which applies resources to the decision points encountered during the search of the game tree to further augment the learned value function estimate. A level of play roughly as good as, or possibly better than, the current champion human and computer backgammon players has been achieved in a short period of learning.",
      "pdf_url": null,
      "published": "2025-04-03T02:27:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.02221v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Explanatory machine learning for sequential human teaching",
      "authors": [
        "Lun Ai",
        "Johannes Langer",
        "Stephen H. Muggleton",
        "Ute Schmid"
      ],
      "abstract": "The topic of comprehensibility of machine-learned theories has recently drawn increasing attention. Inductive Logic Programming (ILP) uses logic programming to derive logic theories from small data based on abduction and induction techniques. Learned theories are represented in the form of rules as declarative descriptions of obtained knowledge. In earlier work, the authors provided the first evidence of a measurable increase in human comprehension based on machine-learned logic rules for simple classification tasks. In a later study, it was found that the presentation of machine-learned explanations to humans can produce both beneficial and harmful effects in the context of game learning. We continue our investigation of comprehensibility by examining the effects of the ordering of concept presentations on human comprehension. In this work, we examine the explanatory effects of curriculum order and the presence of machine-learned explanations for sequential problem-solving. We show that 1) there exist tasks A and B such that learning A before B has a better human comprehension with respect to learning B before A and 2) there exist tasks A and B such that the presence of explanations when learning A contributes to improved human comprehension when subsequently learning B. We propose a framework for the effects of sequential teaching on comprehension based on an existing definition of comprehensibility and provide evidence for support from data collected in human trials. Empirical results show that sequential teaching of concepts with increasing complexity a) has a beneficial effect on human comprehension and b) leads to human re-discovery of divide-and-conquer problem-solving strategies, and c) studying machine-learned explanations allows adaptations of human problem-solving strategy with better performance.",
      "pdf_url": null,
      "published": "2022-05-20T15:23:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2205.10250v2",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing",
      "authors": [
        "Amin Jalali",
        "Milad Soltany",
        "Michael Greenspan",
        "Ali Etemad"
      ],
      "abstract": "We propose TimeHUT, a novel method for learning time-series representations by hierarchical uniformity-tolerance balancing of contrastive representations. Our method uses two distinct losses to learn strong representations with the aim of striking an effective balance between uniformity and tolerance in the embedding space. First, TimeHUT uses a hierarchical setup to learn both instance-wise and temporal information from input time-series. Next, we integrate a temperature scheduler within the vanilla contrastive loss to balance the uniformity and tolerance characteristics of the embeddings. Additionally, a hierarchical angular margin loss enforces instance-wise and temporal contrast losses, creating geometric margins between positive and negative pairs of temporal sequences. This approach improves the coherence of positive pairs and their separation from the negatives, enhancing the capture of temporal dependencies within a time-series sample. We evaluate our approach on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and multivariate classification, as well as Yahoo and KPI datasets for anomaly detection. The results demonstrate that TimeHUT outperforms prior methods by considerable margins on classification, while obtaining competitive results for anomaly detection. Finally, detailed sensitivity and ablation studies are performed to evaluate different components and hyperparameters of our method.",
      "pdf_url": null,
      "published": "2025-10-02T04:30:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2510.01658v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Uncertain Bayesian Networks: Learning from Incomplete Data",
      "authors": [
        "Conrad D. Hougen",
        "Lance M. Kaplan",
        "Federico Cerutti",
        "Alfred O. Hero"
      ],
      "abstract": "When the historical data are limited, the conditional probabilities associated with the nodes of Bayesian networks are uncertain and can be empirically estimated. Second order estimation methods provide a framework for both estimating the probabilities and quantifying the uncertainty in these estimates. We refer to these cases as uncer tain or second-order Bayesian networks. When such data are complete, i.e., all variable values are observed for each instantiation, the conditional probabilities are known to be Dirichlet-distributed. This paper improves the current state-of-the-art approaches for handling uncertain Bayesian networks by enabling them to learn distributions for their parameters, i.e., conditional probabilities, with incomplete data. We extensively evaluate various methods to learn the posterior of the parameters through the desired and empirically derived strength of confidence bounds for various queries.",
      "pdf_url": null,
      "published": "2022-08-08T15:46:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2208.04221v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Concave Utility Reinforcement Learning with Zero-Constraint Violations",
      "authors": [
        "Mridul Agarwal",
        "Qinbo Bai",
        "Vaneet Aggarwal"
      ],
      "abstract": "We consider the problem of tabular infinite horizon concave utility reinforcement learning (CURL) with convex constraints. For this, we propose a model-based learning algorithm that also achieves zero constraint violations. Assuming that the concave objective and the convex constraints have a solution interior to the set of feasible occupation measures, we solve a tighter optimization problem to ensure that the constraints are never violated despite the imprecise model knowledge and model stochasticity. We use Bellman error-based analysis for tabular infinite-horizon setups which allows analyzing stochastic policies. Combining the Bellman error-based analysis and tighter optimization equation, for $T$ interactions with the environment, we obtain a high-probability regret guarantee for objective which grows as $\\Tilde{O}(1/\\sqrt{T})$, excluding other factors. The proposed method can be applied for optimistic algorithms to obtain high-probability regret bounds and also be used for posterior sampling algorithms to obtain a loose Bayesian regret bounds but with significant improvement in computational complexity.",
      "pdf_url": null,
      "published": "2021-09-12T06:13:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2109.05439v3",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation",
      "authors": [
        "Michelle Pan",
        "Mariah Schrum",
        "Vivek Myers",
        "Erdem Bıyık",
        "Anca Dragan"
      ],
      "abstract": "Adaptive brain stimulation can treat neurological conditions such as Parkinson's disease and post-stroke motor deficits by influencing abnormal neural activity. Because of patient heterogeneity, each patient requires a unique stimulation policy to achieve optimal neural responses. Model-free reinforcement learning (MFRL) holds promise in learning effective policies for a variety of similar control tasks, but is limited in domains like brain stimulation by a need for numerous costly environment interactions. In this work we introduce Coprocessor Actor Critic, a novel, model-based reinforcement learning (MBRL) approach for learning neural coprocessor policies for brain stimulation. Our key insight is that coprocessor policy learning is a combination of learning how to act optimally in the world and learning how to induce optimal actions in the world through stimulation of an injured brain. We show that our approach overcomes the limitations of traditional MFRL methods in terms of sample efficiency and task success and outperforms baseline MBRL approaches in a neurologically realistic model of an injured brain.",
      "pdf_url": null,
      "published": "2024-06-10T18:23:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2406.06714v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Multi-objective Model-based Policy Search for Data-efficient Learning with Sparse Rewards",
      "authors": [
        "Rituraj Kaushik",
        "Konstantinos Chatzilygeroudis",
        "Jean-Baptiste Mouret"
      ],
      "abstract": "The most data-efficient algorithms for reinforcement learning in robotics are model-based policy search algorithms, which alternate between learning a dynamical model of the robot and optimizing a policy to maximize the expected return given the model and its uncertainties. However, the current algorithms lack an effective exploration strategy to deal with sparse or misleading reward scenarios: if they do not experience any state with a positive reward during the initial random exploration, it is very unlikely to solve the problem. Here, we propose a novel model-based policy search algorithm, Multi-DEX, that leverages a learned dynamical model to efficiently explore the task space and solve tasks with sparse rewards in a few episodes. To achieve this, we frame the policy search problem as a multi-objective, model-based policy optimization problem with three objectives: (1) generate maximally novel state trajectories, (2) maximize the expected return and (3) keep the system in state-space regions for which the model is as accurate as possible. We then optimize these objectives using a Pareto-based multi-objective optimization algorithm. The experiments show that Multi-DEX is able to solve sparse reward scenarios (with a simulated robotic arm) in much lower interaction time than VIME, TRPO, GEP-PG, CMA-ES and Black-DROPS.",
      "pdf_url": null,
      "published": "2018-06-25T09:46:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/1806.09351v3",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.RO",
        "stat.ML"
      ]
    },
    {
      "title": "On-Policy Robot Imitation Learning from a Converging Supervisor",
      "authors": [
        "Ashwin Balakrishna",
        "Brijen Thananjeyan",
        "Jonathan Lee",
        "Felix Li",
        "Arsh Zahed",
        "Joseph E. Gonzalez",
        "Ken Goldberg"
      ],
      "abstract": "Existing on-policy imitation learning algorithms, such as DAgger, assume access to a fixed supervisor. However, there are many settings where the supervisor may evolve during policy learning, such as a human performing a novel task or an improving algorithmic controller. We formalize imitation learning from a \"converging supervisor\" and provide sublinear static and dynamic regret guarantees against the best policy in hindsight with labels from the converged supervisor, even when labels during learning are only from intermediate supervisors. We then show that this framework is closely connected to a class of reinforcement learning (RL) algorithms known as dual policy iteration (DPI), which alternate between training a reactive learner with imitation learning and a model-based supervisor with data from the learner. Experiments suggest that when this framework is applied with the state-of-the-art deep model-based RL algorithm PETS as an improving supervisor, it outperforms deep RL baselines on continuous control tasks and provides up to an 80-fold speedup in policy evaluation.",
      "pdf_url": null,
      "published": "2019-07-08T07:02:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/1907.03423v7",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models",
      "authors": [
        "Mianchu Wang",
        "Rui Yang",
        "Xi Chen",
        "Hao Sun",
        "Meng Fang",
        "Giovanni Montana"
      ],
      "abstract": "Offline Goal-Conditioned RL (GCRL) offers a feasible paradigm for learning general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods, mainly model-free, face constraints in handling limited data and generalizing to unseen goals. In this work, we propose Goal-conditioned Offline Planning (GOPlan), a novel model-based framework that contains two key phases: (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, we base the prior policy on an advantage-weighted conditioned generative adversarial network, which facilitates distinct mode separation, mitigating the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by planning with learned models for both intra-trajectory and inter-trajectory goals. With thorough experimental evaluations, we demonstrate that GOPlan achieves state-of-the-art performance on various offline multi-goal navigation and manipulation tasks. Moreover, our results highlight the superior ability of GOPlan to handle small data budgets and generalize to OOD goals.",
      "pdf_url": null,
      "published": "2023-10-30T21:19:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2310.20025v3",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Evolutionary Reinforcement Learning for Sample-Efficient Multiagent Coordination",
      "authors": [
        "Shauharda Khadka",
        "Somdeb Majumdar",
        "Santiago Miret",
        "Stephen McAleer",
        "Kagan Tumer"
      ],
      "abstract": "Many cooperative multiagent reinforcement learning environments provide agents with a sparse team-based reward, as well as a dense agent-specific reward that incentivizes learning basic skills. Training policies solely on the team-based reward is often difficult due to its sparsity. Furthermore, relying solely on the agent-specific reward is sub-optimal because it usually does not capture the team coordination objective. A common approach is to use reward shaping to construct a proxy reward by combining the individual rewards. However, this requires manual tuning for each environment. We introduce Multiagent Evolutionary Reinforcement Learning (MERL), a split-level training platform that handles the two objectives separately through two optimization processes. An evolutionary algorithm maximizes the sparse team-based objective through neuroevolution on a population of teams. Concurrently, a gradient-based optimizer trains policies to only maximize the dense agent-specific rewards. The gradient-based policies are periodically added to the evolutionary population as a way of information transfer between the two optimization processes. This enables the evolutionary algorithm to use skills learned via the agent-specific rewards toward optimizing the global objective. Results demonstrate that MERL significantly outperforms state-of-the-art methods, such as MADDPG, on a number of difficult coordination benchmarks.",
      "pdf_url": null,
      "published": "2019-06-18T00:25:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/1906.07315v3",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "stat.ML"
      ]
    },
    {
      "title": "Stabilizing Extreme Q-learning by Maclaurin Expansion",
      "authors": [
        "Motoki Omura",
        "Takayuki Osa",
        "Yusuke Mukuta",
        "Tatsuya Harada"
      ],
      "abstract": "In offline reinforcement learning, in-sample learning methods have been widely used to prevent performance degradation caused by evaluating out-of-distribution actions from the dataset. Extreme Q-learning (XQL) employs a loss function based on the assumption that Bellman error follows a Gumbel distribution, enabling it to model the soft optimal value function in an in-sample manner. It has demonstrated strong performance in both offline and online reinforcement learning settings. However, issues remain, such as the instability caused by the exponential term in the loss function and the risk of the error distribution deviating from the Gumbel distribution. Therefore, we propose Maclaurin Expanded Extreme Q-learning to enhance stability. In this method, applying Maclaurin expansion to the loss function in XQL enhances stability against large errors. This approach involves adjusting the modeled value function between the value function under the behavior policy and the soft optimal value function, thus achieving a trade-off between stability and optimality depending on the order of expansion. It also enables adjustment of the error distribution assumption from a normal distribution to a Gumbel distribution. Our method significantly stabilizes learning in online RL tasks from DM Control, where XQL was previously unstable. Additionally, it improves performance in several offline RL tasks from D4RL.",
      "pdf_url": null,
      "published": "2024-06-07T12:43:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2406.04896v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Meta-Learning Representations for Continual Learning",
      "authors": [
        "Khurram Javed",
        "Martha White"
      ],
      "abstract": "A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite---they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. We release an implementation of our method at https://github.com/khurramjaved96/mrcl .",
      "pdf_url": null,
      "published": "2019-05-29T17:09:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/1905.12588v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "d3rlpy: An Offline Deep Reinforcement Learning Library",
      "authors": [
        "Takuma Seno",
        "Michita Imai"
      ],
      "abstract": "In this paper, we introduce d3rlpy, an open-sourced offline deep reinforcement learning (RL) library for Python. d3rlpy supports a set of offline deep RL algorithms as well as off-policy online algorithms via a fully documented plug-and-play API. To address a reproducibility issue, we conduct a large-scale benchmark with D4RL and Atari 2600 dataset to ensure implementation quality and provide experimental scripts and full tables of results. The d3rlpy source code can be found on GitHub: \\url{https://github.com/takuseno/d3rlpy}.",
      "pdf_url": null,
      "published": "2021-11-06T03:09:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2111.03788v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Bootstrapping Intrinsically Motivated Learning with Human Demonstrations",
      "authors": [
        "Sao Mai Nguyen",
        "Adrien Baranes",
        "Pierre-Yves Oudeyer"
      ],
      "abstract": "This paper studies the coupling of internally guided learning and social interaction, and more specifically the improvement owing to demonstrations of the learning by intrinsic motivation. We present Socially Guided Intrinsic Motivation by Demonstration (SGIM-D), an algorithm for learning in continuous, unbounded and non-preset environments. After introducing social learning and intrinsic motivation, we describe the design of our algorithm, before showing through a fishing experiment that SGIM-D efficiently combines the advantages of social learning and intrinsic motivation to gain a wide repertoire while being specialised in specific subspaces.",
      "pdf_url": null,
      "published": "2011-12-08T20:27:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/1112.1937v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Robust Offline Reinforcement Learning with Linearly Structured f-Divergence Regularization",
      "authors": [
        "Cheng Tang",
        "Zhishuai Liu",
        "Pan Xu"
      ],
      "abstract": "The Robust Regularized Markov Decision Process (RRMDP) is proposed to learn policies robust to dynamics shifts by adding regularization to the transition dynamics in the value function. Existing methods mostly use unstructured regularization, potentially leading to conservative policies under unrealistic transitions. To address this limitation, we propose a novel framework, the $d$-rectangular linear RRMDP ($d$-RRMDP), which introduces latent structures into both transition kernels and regularization. We focus on offline reinforcement learning, where an agent learns policies from a precollected dataset in the nominal environment. We develop the Robust Regularized Pessimistic Value Iteration (R2PVI) algorithm that employs linear function approximation for robust policy learning in $d$-RRMDPs with $f$-divergence based regularization terms on transition kernels. We provide instance-dependent upper bounds on the suboptimality gap of R2PVI policies, demonstrating that these bounds are influenced by how well the dataset covers state-action spaces visited by the optimal robust policy under robustly admissible transitions. We establish information-theoretic lower bounds to verify that our algorithm is near-optimal. Finally, numerical experiments validate that R2PVI learns robust policies and exhibits superior computational efficiency compared to baseline methods.",
      "pdf_url": null,
      "published": "2024-11-27T18:57:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2411.18612v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ]
    },
    {
      "title": "Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms",
      "authors": [
        "Ehsan Hallaji",
        "Roozbeh Razavi-Far",
        "Mehrdad Saif"
      ],
      "abstract": "The advent of federated learning has facilitated large-scale data exchange amongst machine learning models while maintaining privacy. Despite its brief history, federated learning is rapidly evolving to make wider use more practical. One of the most significant advancements in this domain is the incorporation of transfer learning into federated learning, which overcomes fundamental constraints of primary federated learning, particularly in terms of security. This chapter performs a comprehensive survey on the intersection of federated and transfer learning from a security point of view. The main goal of this study is to uncover potential vulnerabilities and defense mechanisms that might compromise the privacy and performance of systems that use federated and transfer learning.",
      "pdf_url": null,
      "published": "2022-07-05T22:07:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2207.02337v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV",
        "cs.DC"
      ]
    },
    {
      "title": "Physics-Inspired Interpretability Of Machine Learning Models",
      "authors": [
        "Maximilian P Niroomand",
        "David J Wales"
      ],
      "abstract": "The ability to explain decisions made by machine learning models remains one of the most significant hurdles towards widespread adoption of AI in highly sensitive areas such as medicine, cybersecurity or autonomous driving. Great interest exists in understanding which features of the input data prompt model decision making. In this contribution, we propose a novel approach to identify relevant features of the input data, inspired by methods from the energy landscapes field, developed in the physical sciences. By identifying conserved weights within groups of minima of the loss landscapes, we can identify the drivers of model decision making. Analogues to this idea exist in the molecular sciences, where coordinate invariants or order parameters are employed to identify critical features of a molecule. However, no such approach exists for machine learning loss landscapes. We will demonstrate the applicability of energy landscape methods to machine learning models and give examples, both synthetic and from the real world, for how these methods can help to make models more interpretable.",
      "pdf_url": null,
      "published": "2023-04-05T11:35:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2304.02381v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Gradient Boosting Reinforcement Learning",
      "authors": [
        "Benjamin Fuhrer",
        "Chen Tessler",
        "Gal Dalal"
      ],
      "abstract": "We present Gradient Boosting Reinforcement Learning (GBRL), a framework that adapts the strengths of gradient boosting trees (GBT) to reinforcement learning (RL) tasks. While neural networks (NNs) have become the de facto choice for RL, they face significant challenges with structured and categorical features and tend to generalize poorly to out-of-distribution samples. These are challenges for which GBTs have traditionally excelled in supervised learning. However, GBT's application in RL has been limited. The design of traditional GBT libraries is optimized for static datasets with fixed labels, making them incompatible with RL's dynamic nature, where both state distributions and reward signals evolve during training. GBRL overcomes this limitation by continuously interleaving tree construction with environment interaction. Through extensive experiments, we demonstrate that GBRL outperforms NNs in domains with structured observations and categorical features while maintaining competitive performance on standard continuous control benchmarks. Like its supervised learning counterpart, GBRL demonstrates superior robustness to out-of-distribution samples and better handles irregular state-action relationships.",
      "pdf_url": null,
      "published": "2024-07-11T07:52:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2407.08250v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "An Optimistic Perspective on Offline Reinforcement Learning",
      "authors": [
        "Rishabh Agarwal",
        "Dale Schuurmans",
        "Mohammad Norouzi"
      ],
      "abstract": "Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.",
      "pdf_url": null,
      "published": "2019-07-10T07:23:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/1907.04543v4",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction",
      "authors": [
        "Yiting He",
        "Zhishuai Liu",
        "Weixin Wang",
        "Pan Xu"
      ],
      "abstract": "Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.",
      "pdf_url": null,
      "published": "2025-11-07T16:24:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2511.05396v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ]
    },
    {
      "title": "XtarNet: Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning",
      "authors": [
        "Sung Whan Yoon",
        "Do-Yeon Kim",
        "Jun Seo",
        "Jaekyun Moon"
      ],
      "abstract": "Learning novel concepts while preserving prior knowledge is a long-standing challenge in machine learning. The challenge gets greater when a novel task is given with only a few labeled examples, a problem known as incremental few-shot learning. We propose XtarNet, which learns to extract task-adaptive representation (TAR) for facilitating incremental few-shot learning. The method utilizes a backbone network pretrained on a set of base categories while also employing additional modules that are meta-trained across episodes. Given a new task, the novel feature extracted from the meta-trained modules is mixed with the base feature obtained from the pretrained model. The process of combining two different features provides TAR and is also controlled by meta-trained modules. The TAR contains effective information for classifying both novel and base categories. The base and novel classifiers quickly adapt to a given task by utilizing the TAR. Experiments on standard image datasets indicate that XtarNet achieves state-of-the-art incremental few-shot learning performance. The concept of TAR can also be used in conjunction with existing incremental few-shot learning methods; extensive simulation results in fact show that applying TAR enhances the known methods significantly.",
      "pdf_url": null,
      "published": "2020-03-19T04:02:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2003.08561v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation",
      "authors": [
        "Shani Gamrian",
        "Yoav Goldberg"
      ],
      "abstract": "Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning---the common transfer learning paradigm---fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in https://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo .",
      "pdf_url": null,
      "published": "2018-05-31T09:25:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/1806.07377v6",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts",
      "authors": [
        "Rohit Agarwal",
        "Deepak Gupta",
        "Alexander Horsch",
        "Dilip K. Prasad"
      ],
      "abstract": "Many real-world applications based on online learning produce streaming data that is haphazard in nature, i.e., contains missing features, features becoming obsolete in time, the appearance of new features at later points in time and a lack of clarity on the total number of input features. These challenges make it hard to build a learnable system for such applications, and almost no work exists in deep learning that addresses this issue. In this paper, we present Aux-Drop, an auxiliary dropout regularization strategy for online learning that handles the haphazard input features in an effective manner. Aux-Drop adapts the conventional dropout regularization scheme for the haphazard input feature space ensuring that the final output is minimally impacted by the chaotic appearance of such features. It helps to prevent the co-adaptation of especially the auxiliary and base features, as well as reduces the strong dependence of the output on any of the auxiliary inputs of the model. This helps in better learning for scenarios where certain features disappear in time or when new features are to be modelled. The efficacy of Aux-Drop has been demonstrated through extensive numerical experiments on SOTA benchmarking datasets that include Italy Power Demand, HIGGS, SUSY and multiple UCI datasets. The code is available at https://github.com/Rohit102497/Aux-Drop.",
      "pdf_url": null,
      "published": "2023-03-09T10:15:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2303.05155v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays",
      "authors": [
        "Rogier van der Sluijs",
        "Nandita Bhaskhar",
        "Daniel Rubin",
        "Curtis Langlotz",
        "Akshay Chaudhari"
      ],
      "abstract": "Image augmentations are quintessential for effective visual representation learning across self-supervised learning techniques. While augmentation strategies for natural imaging have been studied extensively, medical images are vastly different from their natural counterparts. Thus, it is unknown whether common augmentation strategies employed in Siamese representation learning generalize to medical images and to what extent. To address this challenge, in this study, we systematically assess the effect of various augmentations on the quality and robustness of the learned representations. We train and evaluate Siamese Networks for abnormality detection on chest X-Rays across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate the efficacy of the learned representations through experiments involving linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally, we identify a set of augmentations that yield robust representations that generalize well to both out-of-distribution data and diseases, while outperforming supervised baselines using just zero-shot transfer and linear probes by up to 20%. Our code is available at https://github.com/StanfordMIMI/siaug.",
      "pdf_url": null,
      "published": "2023-01-30T03:42:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2301.12636v2",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "A deep cut into Split Federated Self-supervised Learning",
      "authors": [
        "Marcin Przewięźlikowski",
        "Marcin Osial",
        "Bartosz Zieliński",
        "Marek Śmieja"
      ],
      "abstract": "Collaborative self-supervised learning has recently become feasible in highly distributed environments by dividing the network layers between client devices and a central server. However, state-of-the-art methods, such as MocoSFL, are optimized for network division at the initial layers, which decreases the protection of the client data and increases communication overhead. In this paper, we demonstrate that splitting depth is crucial for maintaining privacy and communication efficiency in distributed training. We also show that MocoSFL suffers from a catastrophic quality deterioration for the minimal communication overhead. As a remedy, we introduce Momentum-Aligned contrastive Split Federated Learning (MonAcoSFL), which aligns online and momentum client models during training procedure. Consequently, we achieve state-of-the-art accuracy while significantly reducing the communication overhead, making MonAcoSFL more practical in real-world scenarios.",
      "pdf_url": null,
      "published": "2024-06-12T14:35:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2406.08267v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "Learning to Navigate in Mazes with Novel Layouts using Abstract Top-down Maps",
      "authors": [
        "Linfeng Zhao",
        "Lawson L. S. Wong"
      ],
      "abstract": "Learning navigation capabilities in different environments has long been one of the major challenges in decision-making. In this work, we focus on zero-shot navigation ability using given abstract $2$-D top-down maps. Like human navigation by reading a paper map, the agent reads the map as an image when navigating in a novel layout, after learning to navigate on a set of training maps. We propose a model-based reinforcement learning approach for this multi-task learning problem, where it jointly learns a hypermodel that takes top-down maps as input and predicts the weights of the transition network. We use the DeepMind Lab environment and customize layouts using generated maps. Our method can adapt better to novel environments in zero-shot and is more robust to noise.",
      "pdf_url": null,
      "published": "2024-12-16T17:51:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2412.12024v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Continual Learning of Diffusion Models with Generative Distillation",
      "authors": [
        "Sergi Masip",
        "Pau Rodriguez",
        "Tinne Tuytelaars",
        "Gido M. van de Ven"
      ],
      "abstract": "Diffusion models are powerful generative models that achieve state-of-the-art performance in image synthesis. However, training them demands substantial amounts of data and computational resources. Continual learning would allow for incrementally learning new tasks and accumulating knowledge, thus enabling the reuse of trained models for further learning. One potentially suitable continual learning approach is generative replay, where a copy of a generative model trained on previous tasks produces synthetic data that are interleaved with data from the current task. However, standard generative replay applied to diffusion models results in a catastrophic loss in denoising capabilities. In this paper, we propose generative distillation, an approach that distils the entire reverse process of a diffusion model. We demonstrate that our approach substantially improves the continual learning performance of generative replay with only a modest increase in the computational costs.",
      "pdf_url": null,
      "published": "2023-11-23T14:33:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2311.14028v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Learning Temporal Distances: Contrastive Successor Features Can Provide a Metric Structure for Decision-Making",
      "authors": [
        "Vivek Myers",
        "Chongyi Zheng",
        "Anca Dragan",
        "Sergey Levine",
        "Benjamin Eysenbach"
      ],
      "abstract": "Temporal distances lie at the heart of many algorithms for planning, control, and reinforcement learning that involve reaching goals, allowing one to estimate the transit time between two states. However, prior attempts to define such temporal distances in stochastic settings have been stymied by an important limitation: these prior approaches do not satisfy the triangle inequality. This is not merely a definitional concern, but translates to an inability to generalize and find shortest paths. In this paper, we build on prior work in contrastive learning and quasimetrics to show how successor features learned by contrastive learning (after a change of variables) form a temporal distance that does satisfy the triangle inequality, even in stochastic settings. Importantly, this temporal distance is computationally efficient to estimate, even in high-dimensional and stochastic settings. Experiments in controlled settings and benchmark suites demonstrate that an RL algorithm based on these new temporal distances exhibits combinatorial generalization (i.e., \"stitching\") and can sometimes learn more quickly than prior methods, including those based on quasimetrics.",
      "pdf_url": null,
      "published": "2024-06-24T19:36:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2406.17098v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Untangling Dense Knots by Learning Task-Relevant Keypoints",
      "authors": [
        "Jennifer Grannen",
        "Priya Sundaresan",
        "Brijen Thananjeyan",
        "Jeffrey Ichnowski",
        "Ashwin Balakrishna",
        "Minho Hwang",
        "Vainavi Viswanath",
        "Michael Laskey",
        "Joseph E. Gonzalez",
        "Ken Goldberg"
      ],
      "abstract": "Untangling ropes, wires, and cables is a challenging task for robots due to the high-dimensional configuration space, visual homogeneity, self-occlusions, and complex dynamics. We consider dense (tight) knots that lack space between self-intersections and present an iterative approach that uses learned geometric structure in configurations. We instantiate this into an algorithm, HULK: Hierarchical Untangling from Learned Keypoints, which combines learning-based perception with a geometric planner into a policy that guides a bilateral robot to untangle knots. To evaluate the policy, we perform experiments both in a novel simulation environment modelling cables with varied knot types and textures and in a physical system using the da Vinci surgical robot. We find that HULK is able to untangle cables with dense figure-eight and overhand knots and generalize to varied textures and appearances. We compare two variants of HULK to three baselines and observe that HULK achieves 43.3% higher success rates on a physical system compared to the next best baseline. HULK successfully untangles a cable from a dense initial configuration containing up to two overhand and figure-eight knots in 97.9% of 378 simulation experiments with an average of 12.1 actions per trial. In physical experiments, HULK achieves 61.7% untangling success, averaging 8.48 actions per trial. Supplementary material, code, and videos can be found at https://tinyurl.com/y3a88ycu.",
      "pdf_url": null,
      "published": "2020-11-10T09:29:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2011.04999v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "PePR: Performance Per Resource Unit as a Metric to Promote Small-Scale Deep Learning in Medical Image Analysis",
      "authors": [
        "Raghavendra Selvan",
        "Bob Pepin",
        "Christian Igel",
        "Gabrielle Samuel",
        "Erik B Dam"
      ],
      "abstract": "The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for medical image analysis tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-resource trade-offs. In applications like medical image analysis, we argue that small-scale, specialized models are better than striving for large-scale models. Furthermore, we show that using existing pretrained models that are fine-tuned on new data can significantly reduce the computational resources and data required compared to training models from scratch. We hope this work will encourage the community to focus on improving AI equity by developing methods and models with smaller resource footprints.",
      "pdf_url": null,
      "published": "2024-03-19T09:17:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2403.12562v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "EEC: Learning to Encode and Regenerate Images for Continual Learning",
      "authors": [
        "Ali Ayub",
        "Alan R. Wagner"
      ],
      "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. During training on a new task, reconstructed images from encoded episodes are replayed in order to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable while using less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.",
      "pdf_url": null,
      "published": "2021-01-13T06:43:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2101.04904v4",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Preferential Temporal Difference Learning",
      "authors": [
        "Nishanth Anand",
        "Doina Precup"
      ],
      "abstract": "Temporal-Difference (TD) learning is a general and very useful tool for estimating the value function of a given policy, which in turn is required to find good policies. Generally speaking, TD learning updates states whenever they are visited. When the agent lands in a state, its value can be used to compute the TD-error, which is then propagated to other states. However, it may be interesting, when computing updates, to take into account other information than whether a state is visited or not. For example, some states might be more important than others (such as states which are frequently seen in a successful trajectory). Or, some states might have unreliable value estimates (for example, due to partial observability or lack of data), making their values less desirable as targets. We propose an approach to re-weighting states used in TD updates, both when they are the input and when they provide the target for the update. We prove that our approach converges with linear function approximation and illustrate its desirable empirical behaviour compared to other TD-style methods.",
      "pdf_url": null,
      "published": "2021-06-11T17:05:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2106.06508v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning Geometry",
      "authors": [
        "Martin Lindström",
        "Borja Rodríguez-Gálvez",
        "Ragnar Thobaben",
        "Mikael Skoglund"
      ],
      "abstract": "Hyperspherical Prototypical Learning (HPL) is a supervised approach to representation learning that designs class prototypes on the unit hypersphere. The prototypes bias the representations to class separation in a scale invariant and known geometry. Previous approaches to HPL have either of the following shortcomings: (i) they follow an unprincipled optimisation procedure; or (ii) they are theoretically sound, but are constrained to only one possible latent dimension. In this paper, we address both shortcomings. To address (i), we present a principled optimisation procedure whose solution we show is optimal. To address (ii), we construct well-separated prototypes in a wide range of dimensions using linear block codes. Additionally, we give a full characterisation of the optimal prototype placement in terms of achievable and converse bounds, showing that our proposed methods are near-optimal.",
      "pdf_url": null,
      "published": "2024-07-10T13:44:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2407.07664v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP",
        "stat.ML"
      ]
    },
    {
      "title": "Execution-based Code Generation using Deep Reinforcement Learning",
      "authors": [
        "Parshin Shojaee",
        "Aneesh Jain",
        "Sindhu Tipirneni",
        "Chandan K. Reddy"
      ],
      "abstract": "The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs.",
      "pdf_url": null,
      "published": "2023-01-31T18:02:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2301.13816v4",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ]
    },
    {
      "title": "Multi-agent assignment via state augmented reinforcement learning",
      "authors": [
        "Leopoldo Agorio",
        "Sean Van Alen",
        "Miguel Calvo-Fullana",
        "Santiago Paternain",
        "Juan Andres Bazerque"
      ],
      "abstract": "We address the conflicting requirements of a multi-agent assignment problem through constrained reinforcement learning, emphasizing the inadequacy of standard regularization techniques for this purpose. Instead, we recur to a state augmentation approach in which the oscillation of dual variables is exploited by agents to alternate between tasks. In addition, we coordinate the actions of the multiple agents acting on their local states through these multipliers, which are gossiped through a communication network, eliminating the need to access other agent states. By these means, we propose a distributed multi-agent assignment protocol with theoretical feasibility guarantees that we corroborate in a monitoring numerical experiment.",
      "pdf_url": null,
      "published": "2024-06-03T20:56:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2406.01782v1",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "Intelligent Offloading in Vehicular Edge Computing: A Comprehensive Review of Deep Reinforcement Learning Approaches and Architectures",
      "authors": [
        "Ashab Uddin",
        "Ahmed Hamdi Sakr",
        "Ning Zhang"
      ],
      "abstract": "The increasing complexity of Intelligent Transportation Systems (ITS) has led to significant interest in computational offloading to external infrastructures such as edge servers, vehicular nodes, and UAVs. These dynamic and heterogeneous environments pose challenges for traditional offloading strategies, prompting the exploration of Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) as adaptive decision-making frameworks. This survey presents a comprehensive review of recent advances in DRL-based offloading for vehicular edge computing (VEC). We classify and compare existing works based on learning paradigms (e.g., single-agent, multi-agent), system architectures (e.g., centralized, distributed, hierarchical), and optimization objectives (e.g., latency, energy, fairness). Furthermore, we analyze how Markov Decision Process (MDP) formulations are applied and highlight emerging trends in reward design, coordination mechanisms, and scalability. Finally, we identify open challenges and outline future research directions to guide the development of robust and intelligent offloading strategies for next-generation ITS.",
      "pdf_url": null,
      "published": "2025-02-10T19:02:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2502.06963v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.MA"
      ]
    },
    {
      "title": "Learning Neural Parsers with Deterministic Differentiable Imitation Learning",
      "authors": [
        "Tanmay Shankar",
        "Nicholas Rhinehart",
        "Katharina Muelling",
        "Kris M. Kitani"
      ],
      "abstract": "We explore the problem of learning to decompose spatial tasks into segments, as exemplified by the problem of a painting robot covering a large object. Inspired by the ability of classical decision tree algorithms to construct structured partitions of their input spaces, we formulate the problem of decomposing objects into segments as a parsing approach. We make the insight that the derivation of a parse-tree that decomposes the object into segments closely resembles a decision tree constructed by ID3, which can be done when the ground-truth available. We learn to imitate an expert parsing oracle, such that our neural parser can generalize to parse natural images without ground truth. We introduce a novel deterministic policy gradient update, DRAG (i.e., DeteRministically AGgrevate) in the form of a deterministic actor-critic variant of AggreVaTeD, to train our neural parser. From another perspective, our approach is a variant of the Deterministic Policy Gradient suitable for the imitation learning setting. The deterministic policy representation offered by training our neural parser with DRAG allows it to outperform state of the art imitation and reinforcement learning approaches.",
      "pdf_url": null,
      "published": "2018-06-20T16:15:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/1806.07822v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO",
        "stat.ML"
      ]
    },
    {
      "title": "Causal-Paced Deep Reinforcement Learning",
      "authors": [
        "Geonwoo Cho",
        "Jaegyun Im",
        "Doyoon Kim",
        "Sundong Kim"
      ],
      "abstract": "Designing effective task sequences is crucial for curriculum reinforcement learning (CRL), where agents must gradually acquire skills by training on intermediate tasks. A key challenge in CRL is to identify tasks that promote exploration, yet are similar enough to support effective transfer. While recent approach suggests comparing tasks via their Structural Causal Models (SCMs), the method requires access to ground-truth causal structures, an unrealistic assumption in most RL settings. In this work, we propose Causal-Paced Deep Reinforcement Learning (CP-DRL), a curriculum learning framework aware of SCM differences between tasks based on interaction data approximation. This signal captures task novelty, which we combine with the agent's learnability, measured by reward gain, to form a unified objective. Empirically, CP-DRL outperforms existing curriculum methods on the Point Mass benchmark, achieving faster convergence and higher returns. CP-DRL demonstrates reduced variance with comparable final returns in the Bipedal Walker-Trivial setting, and achieves the highest average performance in the Infeasible variant. These results indicate that leveraging causal relationships between tasks can improve the structure-awareness and sample efficiency of curriculum reinforcement learning. We provide the full implementation of CP-DRL to facilitate the reproduction of our main results at https://github.com/Cho-Geonwoo/CP-DRL.",
      "pdf_url": null,
      "published": "2025-06-24T20:15:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.02910v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Model-Agnostic Learning to Meta-Learn",
      "authors": [
        "Arnout Devos",
        "Yatin Dandi"
      ],
      "abstract": "In this paper, we propose a learning algorithm that enables a model to quickly exploit commonalities among related tasks from an unseen task distribution, before quickly adapting to specific tasks from that same distribution. We investigate how learning with different task distributions can first improve adaptability by meta-finetuning on related tasks before improving goal task generalization with finetuning. Synthetic regression experiments validate the intuition that learning to meta-learn improves adaptability and consecutively generalization. Experiments on more complex image classification, continual regression, and reinforcement learning tasks demonstrate that learning to meta-learn generally improves task-specific adaptation. The methodology, setup, and hypotheses in this proposal were positively evaluated by peer review before conclusive experiments were carried out.",
      "pdf_url": null,
      "published": "2020-12-04T15:55:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2012.02684v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Learning Multi-Objective Curricula for Robotic Policy Learning",
      "authors": [
        "Jikun Kang",
        "Miao Liu",
        "Abhinav Gupta",
        "Chris Pal",
        "Xue Liu",
        "Jie Fu"
      ],
      "abstract": "Various automatic curriculum learning (ACL) methods have been proposed to improve the sample efficiency and final performance of deep reinforcement learning (DRL). They are designed to control how a DRL agent collects data, which is inspired by how humans gradually adapt their learning processes to their capabilities. For example, ACL can be used for subgoal generation, reward shaping, environment generation, or initial state generation. However, prior work only considers curriculum learning following one of the aforementioned predefined paradigms. It is unclear which of these paradigms are complementary, and how the combination of them can be learned from interactions with the environment. Therefore, in this paper, we propose a unified automatic curriculum learning framework to create multi-objective but coherent curricula that are generated by a set of parametric curriculum modules. Each curriculum module is instantiated as a neural network and is responsible for generating a particular curriculum. In order to coordinate those potentially conflicting modules in unified parameter space, we propose a multi-task hyper-net learning framework that uses a single hyper-net to parameterize all those curriculum modules. In addition to existing hand-designed curricula paradigms, we further design a flexible memory mechanism to learn an abstract curriculum, which may otherwise be difficult to design manually. We evaluate our method on a series of robotic manipulation tasks and demonstrate its superiority over other state-of-the-art ACL methods in terms of sample efficiency and final performance.",
      "pdf_url": null,
      "published": "2021-10-06T19:30:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2110.03032v3",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "eess.SY",
        "stat.ML"
      ]
    },
    {
      "title": "Learning 2-opt Heuristics for the Traveling Salesman Problem via Deep Reinforcement Learning",
      "authors": [
        "Paulo R. de O. da Costa",
        "Jason Rhuggenaath",
        "Yingqian Zhang",
        "Alp Akcay"
      ],
      "abstract": "Recent works using deep learning to solve the Traveling Salesman Problem (TSP) have focused on learning construction heuristics. Such approaches find TSP solutions of good quality but require additional procedures such as beam search and sampling to improve solutions and achieve state-of-the-art performance. However, few studies have focused on improvement heuristics, where a given solution is improved until reaching a near-optimal one. In this work, we propose to learn a local search heuristic based on 2-opt operators via deep reinforcement learning. We propose a policy gradient algorithm to learn a stochastic policy that selects 2-opt operations given a current solution. Moreover, we introduce a policy neural network that leverages a pointing attention mechanism, which unlike previous works, can be easily extended to more general k-opt moves. Our results show that the learned policies can improve even over random initial solutions and approach near-optimal solutions at a faster rate than previous state-of-the-art deep learning methods.",
      "pdf_url": null,
      "published": "2020-04-03T14:51:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2004.01608v3",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "One-Shot Reinforcement Learning for Robot Navigation with Interactive Replay",
      "authors": [
        "Jake Bruce",
        "Niko Suenderhauf",
        "Piotr Mirowski",
        "Raia Hadsell",
        "Michael Milford"
      ],
      "abstract": "Recently, model-free reinforcement learning algorithms have been shown to solve challenging problems by learning from extensive interaction with the environment. A significant issue with transferring this success to the robotics domain is that interaction with the real world is costly, but training on limited experience is prone to overfitting. We present a method for learning to navigate, to a fixed goal and in a known environment, on a mobile robot. The robot leverages an interactive world model built from a single traversal of the environment, a pre-trained visual feature encoder, and stochastic environmental augmentation, to demonstrate successful zero-shot transfer under real-world environmental variations without fine-tuning.",
      "pdf_url": null,
      "published": "2017-11-28T06:03:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/1711.10137v2",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    }
  ]
}