{
  "last_updated": "2025-07-17T00:56:50.535773",
  "papers": [
    {
      "title": "Streaming 4D Visual Geometry Transformer",
      "authors": [
        "Dong Zhuo",
        "Wenzhao Zheng",
        "Jiahe Guo",
        "Yuqi Wu",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
      "pdf_url": "http://arxiv.org/pdf/2507.11539v1",
      "published": "2025-07-15T17:59:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11539v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "How Many Instructions Can LLMs Follow at Once?",
      "authors": [
        "Daniel Jaroslawicz",
        "Brendan Whiting",
        "Parth Shah",
        "Karime Maamari"
      ],
      "abstract": "Production-grade LLM systems require robust adherence to dozens or even\nhundreds of instructions simultaneously. However, the instruction-following\ncapabilities of LLMs at high instruction densities have not yet been\ncharacterized, as existing benchmarks only evaluate models on tasks with a\nsingle or few instructions. We introduce IFScale, a simple benchmark of 500\nkeyword-inclusion instructions for a business report writing task to measure\nhow instruction-following performance degrades as instruction density\nincreases. We evaluate 20 state-of-the-art models across seven major providers\nand find that even the best frontier models only achieve 68% accuracy at the\nmax density of 500 instructions. Our analysis reveals model size and reasoning\ncapability to correlate with 3 distinct performance degradation patterns, bias\ntowards earlier instructions, and distinct categories of instruction-following\nerrors. Our insights can help inform design of instruction-dense prompts in\nreal-world applications and highlight important performance-latency tradeoffs.\nWe open-source the benchmark and all results for further analysis at\nhttps://distylai.github.io/IFScale.",
      "pdf_url": "http://arxiv.org/pdf/2507.11538v1",
      "published": "2025-07-15T17:59:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11538v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering",
      "authors": [
        "Yinsheng Li",
        "Zhen Dong",
        "Yi Shao"
      ],
      "abstract": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.",
      "pdf_url": "http://arxiv.org/pdf/2507.11527v1",
      "published": "2025-07-15T17:56:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11527v1",
      "categories": [
        "cs.AI",
        "cs.CE"
      ]
    },
    {
      "title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air",
      "authors": [
        "Shiyi Yang",
        "Xiaoxue Yu",
        "Rongpeng Li",
        "Jianhang Zhu",
        "Zhifeng Zhao",
        "Honggang Zhang"
      ],
      "abstract": "Operating Large Language Models (LLMs) on edge devices is increasingly\nchallenged by limited communication bandwidth and strained computational and\nmemory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.\nNevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ\nfixed or heuristic rank configurations, and the subsequent over-the-air\ntransmission of all LoRA parameters could be rather inefficient. To address\nthis limitation, we develop AirLLM, a hierarchical diffusion policy framework\nfor communication-aware LoRA adaptation. Specifically, AirLLM models the rank\nconfiguration as a structured action vector that spans all LoRA-inserted\nprojections. To solve the underlying high-dimensional sequential\ndecision-making problem, a Proximal Policy Optimization (PPO) agent generates\ncoarse-grained decisions by jointly observing wireless states and linguistic\ncomplexity, which are then refined via Denoising Diffusion Implicit Models\n(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The\ntwo modules are optimized alternatively, with the DDIM trained under the\nClassifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.\nExperiments under varying signal-to-noise ratios demonstrate that AirLLM\nconsistently enhances fine-tuning performance while significantly reducing\ntransmission costs, highlighting the effectiveness of reinforcement-driven,\ndiffusion-refined rank adaptation for scalable and efficient remote fine-tuning\nover the air.",
      "pdf_url": "http://arxiv.org/pdf/2507.11515v1",
      "published": "2025-07-15T17:36:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11515v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Recursive Bound-Constrained AdaGrad with Applications to Multilevel and Domain Decomposition Minimization",
      "authors": [
        "Serge Gratton",
        "Alena Kopaničáková",
        "Philippe Toint"
      ],
      "abstract": "Two OFFO (Objective-Function Free Optimization) noise tolerant algorithms are\npresented that handle bound constraints, inexact gradients and use second-order\ninformation when available.The first is a multi-level method exploiting a\nhierarchical description of the problem and the second is a\ndomain-decomposition method covering the standard addditive Schwarz\ndecompositions. Both are generalizations of the first-order AdaGrad algorithm\nfor unconstrained optimization. Because these algorithms share a common\ntheoretical framework, a single convergence/complexity theory is provided which\ncovers them both. Its main result is that, with high probability, both methods\nneed at most $O(\\epsilon^{-2})$ iterations and noisy gradient evaluations to\ncompute an $\\epsilon$-approximate first-order critical point of the\nbound-constrained problem. Extensive numerical experiments are discussed on\napplications ranging from PDE-based problems to deep neural network training,\nillustrating their remarkable computational efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2507.11513v1",
      "published": "2025-07-15T17:32:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11513v1",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.NA",
        "math.NA",
        "49K20, 65M55, 65Y20, 68Q25, 68T05, 90C26, 90C30",
        "F.2.1; G.1.8; I.2.5"
      ]
    },
    {
      "title": "COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation",
      "authors": [
        "Pakizar Shamoi",
        "Nuray Toganas",
        "Muragul Muratbekova",
        "Elnara Kadyrgali",
        "Adilet Yerkin",
        "Ayan Igali",
        "Malika Ziyada",
        "Ayana Adilova",
        "Aron Karatayev",
        "Yerdauit Torekhan"
      ],
      "abstract": "Colors are omnipresent in today's world and play a vital role in how humans\nperceive and interact with their surroundings. However, it is challenging for\ncomputers to imitate human color perception. This paper introduces the Human\nPerception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based\nRepresentation and Interpretation), designed to bridge the gap between\ncomputational color representations and human visual perception. The proposed\nmodel uses fuzzy sets and logic to create a framework for color categorization.\nUsing a three-phase experimental approach, the study first identifies\ndistinguishable color stimuli for hue, saturation, and intensity through\npreliminary experiments, followed by a large-scale human categorization survey\ninvolving more than 1000 human subjects. The resulting data are used to extract\nfuzzy partitions and generate membership functions that reflect real-world\nperceptual uncertainty. The model incorporates a mechanism for adaptation that\nallows refinement based on feedback and contextual changes. Comparative\nevaluations demonstrate the model's alignment with human perception compared to\ntraditional color models, such as RGB, HSV, and LAB. To the best of our\nknowledge, no previous research has documented the construction of a model for\ncolor attribute specification based on a sample of this size or a comparable\nsample of the human population (n = 2496). Our findings are significant for\nfields such as design, artificial intelligence, marketing, and human-computer\ninteraction, where perceptually relevant color representation is critical.",
      "pdf_url": "http://arxiv.org/pdf/2507.11488v1",
      "published": "2025-07-15T17:01:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11488v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light",
      "authors": [
        "Mani Hamidi",
        "Terrence W. Deacon"
      ],
      "abstract": "Three core tenets of reinforcement learning (RL)--concerning the definition\nof agency, the objective of learning, and the scope of the reward\nhypothesis--have been highlighted as key targets for conceptual revision, with\nmajor implications for theory and application. We propose a framework, inspired\nby open-ended evolutionary theory, to reconsider these three \"dogmas.\" We\nrevisit each assumption and address related concerns raised alongside them. To\nmake our arguments relevant to RL as a model of biological learning, we first\nestablish that evolutionary dynamics can plausibly operate within living brains\nover an individual's lifetime, and are not confined to cross-generational\nprocesses. We begin by revisiting the second dogma, drawing on evolutionary\ninsights to enrich the \"adaptation-rather-than-search\" view of learning. We\nthen address the third dogma regarding the limits of the reward hypothesis,\nusing analogies from evolutionary fitness to illuminate the scalar reward vs.\nmulti-objective debate. After discussing practical implications for exploration\nin RL, we turn to the first--and arguably most fundamental--issue: the absence\nof a formal account of agency. We argue that unlike the other two problems, the\nevolutionary paradigm alone cannot resolve the agency question, though it\ngestures in a productive direction. We advocate integrating ideas from\norigins-of-life theory, where the thermodynamics of sustenance and replication\noffer promising foundations for understanding agency and resource-constrained\nreinforcement learning in biological systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.11482v1",
      "published": "2025-07-15T16:53:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11482v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety",
      "authors": [
        "Tomek Korbak",
        "Mikita Balesni",
        "Elizabeth Barnes",
        "Yoshua Bengio",
        "Joe Benton",
        "Joseph Bloom",
        "Mark Chen",
        "Alan Cooney",
        "Allan Dafoe",
        "Anca Dragan",
        "Scott Emmons",
        "Owain Evans",
        "David Farhi",
        "Ryan Greenblatt",
        "Dan Hendrycks",
        "Marius Hobbhahn",
        "Evan Hubinger",
        "Geoffrey Irving",
        "Erik Jenner",
        "Daniel Kokotajlo",
        "Victoria Krakovna",
        "Shane Legg",
        "David Lindner",
        "David Luan",
        "Aleksander Mądry",
        "Julian Michael",
        "Neel Nanda",
        "Dave Orr",
        "Jakub Pachocki",
        "Ethan Perez",
        "Mary Phuong",
        "Fabien Roger",
        "Joshua Saxe",
        "Buck Shlegeris",
        "Martín Soto",
        "Eric Steinberger",
        "Jasmine Wang",
        "Wojciech Zaremba",
        "Bowen Baker",
        "Rohin Shah",
        "Vlad Mikulik"
      ],
      "abstract": "AI systems that \"think\" in human language offer a unique opportunity for AI\nsafety: we can monitor their chains of thought (CoT) for the intent to\nmisbehave. Like all other known AI oversight methods, CoT monitoring is\nimperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows\npromise and we recommend further research into CoT monitorability and\ninvestment in CoT monitoring alongside existing safety methods. Because CoT\nmonitorability may be fragile, we recommend that frontier model developers\nconsider the impact of development decisions on CoT monitorability.",
      "pdf_url": "http://arxiv.org/pdf/2507.11473v1",
      "published": "2025-07-15T16:43:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11473v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Modeling Code: Is Text All You Need?",
      "authors": [
        "Daniel Nichols",
        "Konstantinos Parasyris",
        "Harshitha Menon",
        "Brian R. Bartoldson",
        "Giorgis Georgakoudis",
        "Tal Ben-Nun",
        "Abhinav Bhatele"
      ],
      "abstract": "Code LLMs have become extremely popular recently for modeling source code\nacross a variety of tasks, such as generation, translation, and summarization.\nHowever, transformer-based models are limited in their capabilities to reason\nthrough structured, analytical properties of code, such as control and data\nflow. Previous work has explored the modeling of these properties with\nstructured data and graph neural networks. However, these approaches lack the\ngenerative capabilities and scale of modern LLMs. In this work, we introduce a\nnovel approach to combine the strengths of modeling both code as text and more\nstructured forms.",
      "pdf_url": "http://arxiv.org/pdf/2507.11467v1",
      "published": "2025-07-15T16:39:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11467v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "COLI: A Hierarchical Efficient Compressor for Large Images",
      "authors": [
        "Haoran Wang",
        "Hanyu Pei",
        "Yang Lyu",
        "Kai Zhang",
        "Li Li",
        "Feng-Lei Fan"
      ],
      "abstract": "The escalating adoption of high-resolution, large-field-of-view imagery\namplifies the need for efficient compression methodologies. Conventional\ntechniques frequently fail to preserve critical image details, while\ndata-driven approaches exhibit limited generalizability. Implicit Neural\nRepresentations (INRs) present a promising alternative by learning continuous\nmappings from spatial coordinates to pixel intensities for individual images,\nthereby storing network weights rather than raw pixels and avoiding the\ngeneralization problem. However, INR-based compression of large images faces\nchallenges including slow compression speed and suboptimal compression ratios.\nTo address these limitations, we introduce COLI (Compressor for Large Images),\na novel framework leveraging Neural Representations for Videos (NeRV). First,\nrecognizing that INR-based compression constitutes a training process, we\naccelerate its convergence through a pretraining-finetuning paradigm,\nmixed-precision training, and reformulation of the sequential loss into a\nparallelizable objective. Second, capitalizing on INRs' transformation of image\nstorage constraints into weight storage, we implement Hyper-Compression, a\nnovel post-training technique to substantially enhance compression ratios while\nmaintaining minimal output distortion. Evaluations across two medical imaging\ndatasets demonstrate that COLI consistently achieves competitive or superior\nPSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while\naccelerating NeRV training by up to 4 times.",
      "pdf_url": "http://arxiv.org/pdf/2507.11443v1",
      "published": "2025-07-15T16:07:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11443v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures",
      "authors": [
        "Behtom Adeli",
        "John McLinden",
        "Pankaj Pandey",
        "Ming Shao",
        "Yalda Shahriari"
      ],
      "abstract": "Activation functions are critical to the performance of deep neural networks,\nparticularly in domains such as functional near-infrared spectroscopy (fNIRS),\nwhere nonlinearity, low signal-to-noise ratio (SNR), and signal variability\nposes significant challenges to model accuracy. However, the impact of\nactivation functions on deep learning (DL) performance in the fNIRS domain\nremains underexplored and lacks systematic investigation in the current\nliterature. This study evaluates a range of conventional and field-specific\nactivation functions for fNIRS classification tasks using multiple deep\nlearning architectures, including the domain-specific fNIRSNet, AbsoluteNet,\nMDNN, and shallowConvNet (as the baseline), all tested on a single dataset\nrecorded during an auditory task. To ensure fair a comparison, all networks\nwere trained and tested using standardized preprocessing and consistent\ntraining parameters. The results show that symmetrical activation functions\nsuch as Tanh and the Absolute value function Abs(x) can outperform commonly\nused functions like the Rectified Linear Unit (ReLU), depending on the\narchitecture. Additionally, a focused analysis of the role of symmetry was\nconducted using a Modified Absolute Function (MAF), with results further\nsupporting the effectiveness of symmetrical activation functions on performance\ngains. These findings underscore the importance of selecting proper activation\nfunctions that align with the signal characteristics of fNIRS data.",
      "pdf_url": "http://arxiv.org/pdf/2507.11436v1",
      "published": "2025-07-15T15:58:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11436v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV",
      "authors": [
        "Hongbo Ye",
        "Fenghe Tang",
        "Peiang Zhao",
        "Zhen Huang",
        "Dexin Zhao",
        "Minghao Bian",
        "S. Kevin Zhou"
      ],
      "abstract": "Achieving equity in healthcare accessibility requires lightweight yet\nhigh-performance solutions for medical image segmentation, particularly in\nresource-limited settings. Existing methods like U-Net and its variants often\nsuffer from limited global Effective Receptive Fields (ERFs), hindering their\nability to capture long-range dependencies. To address this, we propose U-RWKV,\na novel framework leveraging the Recurrent Weighted Key-Value(RWKV)\narchitecture, which achieves efficient long-range modeling at O(N)\ncomputational cost. The framework introduces two key innovations: the\nDirection-Adaptive RWKV Module(DARM) and the Stage-Adaptive\nSqueeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan\nmechanisms to aggregate contextual cues across images, mitigating directional\nbias while preserving global context and maintaining high computational\nefficiency. SASE dynamically adapts its architecture to different feature\nextraction stages, balancing high-resolution detail preservation and semantic\nrelationship capture. Experiments demonstrate that U-RWKV achieves\nstate-of-the-art segmentation performance with high computational efficiency,\noffering a practical solution for democratizing advanced medical imaging\ntechnologies in resource-constrained environments. The code is available at\nhttps://github.com/hbyecoding/U-RWKV.",
      "pdf_url": "http://arxiv.org/pdf/2507.11415v1",
      "published": "2025-07-15T15:40:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11415v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?",
      "authors": [
        "Soumadeep Saha",
        "Akshay Chaturvedi",
        "Saptarshi Saha",
        "Utpal Garain",
        "Nicholas Asher"
      ],
      "abstract": "Chain-of-thought traces have been shown to improve performance of large\nlanguage models in a plethora of reasoning tasks, yet there is no consensus on\nthe mechanism through which this performance boost is achieved. To shed more\nlight on this, we introduce Causal CoT Graphs (CCGs), which are directed\nacyclic graphs automatically extracted from reasoning traces that model\nfine-grained causal dependencies in the language model output. A collection of\n$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their\nassociated CCGs are compiled into our dataset -- \\textbf{KisMATH}. Our detailed\nempirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in\nthe CCG are mediators for the final answer, a condition necessary for\nreasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating\nthat models internally realise structures akin to our graphs. KisMATH enables\ncontrolled, graph-aligned interventions and opens up avenues for further\ninvestigation into the role of chain-of-thought in LLM reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2507.11408v1",
      "published": "2025-07-15T15:28:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11408v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ]
    },
    {
      "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes",
      "authors": [
        "LG AI Research",
        ":",
        "Kyunghoon Bae",
        "Eunbi Choi",
        "Kibong Choi",
        "Stanley Jungkyu Choi",
        "Yemuk Choi",
        "Kyubeen Han",
        "Seokhee Hong",
        "Junwon Hwang",
        "Taewan Hwang",
        "Joonwon Jang",
        "Hyojin Jeon",
        "Kijeong Jeon",
        "Gerrard Jeongwon Jo",
        "Hyunjik Jo",
        "Jiyeon Jung",
        "Euisoon Kim",
        "Hyosang Kim",
        "Jihoon Kim",
        "Joonkee Kim",
        "Seonghwan Kim",
        "Soyeon Kim",
        "Sunkyoung Kim",
        "Yireun Kim",
        "Yongil Kim",
        "Youchul Kim",
        "Edward Hwayoung Lee",
        "Gwangho Lee",
        "Haeju Lee",
        "Honglak Lee",
        "Jinsik Lee",
        "Kyungmin Lee",
        "Sangha Park",
        "Young Min Paik",
        "Yongmin Park",
        "Youngyong Park",
        "Sanghyun Seo",
        "Sihoon Yang",
        "Heuiyeen Yeen",
        "Sihyuk Yi",
        "Hyeongu Yun"
      ],
      "abstract": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning\nmode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5\nand the advanced reasoning abilities of EXAONE Deep. To pave the way for the\nagentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool\nuse, and its multilingual capabilities are extended to support Spanish in\naddition to English and Korean. The EXAONE 4.0 model series consists of two\nsizes: a mid-size 32B model optimized for high performance, and a small-size\n1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates\nsuperior performance compared to open-weight models in its class and remains\ncompetitive even against frontier-class models. The models are publicly\navailable for research purposes and can be easily downloaded via\nhttps://huggingface.co/LGAI-EXAONE.",
      "pdf_url": "http://arxiv.org/pdf/2507.11407v1",
      "published": "2025-07-15T15:24:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11407v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From Kinetic Theory to AI: a Rediscovery of High-Dimensional Divergences and Their Properties",
      "authors": [
        "Gennaro Auricchio",
        "Giovanni Brigati",
        "Paolo Giudici",
        "Giuseppe Toscani"
      ],
      "abstract": "Selecting an appropriate divergence measure is a critical aspect of machine\nlearning, as it directly impacts model performance. Among the most widely used,\nwe find the Kullback-Leibler (KL) divergence, originally introduced in kinetic\ntheory as a measure of relative entropy between probability distributions. Just\nas in machine learning, the ability to quantify the proximity of probability\ndistributions plays a central role in kinetic theory. In this paper, we present\na comparative review of divergence measures rooted in kinetic theory,\nhighlighting their theoretical foundations and exploring their potential\napplications in machine learning and artificial intelligence.",
      "pdf_url": "http://arxiv.org/pdf/2507.11387v1",
      "published": "2025-07-15T14:56:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11387v1",
      "categories": [
        "math-ph",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "math.MP",
        "35B40, 35L60, 35K55, 35Q70, 35Q91, 35Q92"
      ]
    },
    {
      "title": "Attributes Shape the Embedding Space of Face Recognition Models",
      "authors": [
        "Pierrick Leroy",
        "Antonio Mastropietro",
        "Marco Nurisso",
        "Francesco Vaccarino"
      ],
      "abstract": "Face Recognition (FR) tasks have made significant progress with the advent of\nDeep Neural Networks, particularly through margin-based triplet losses that\nembed facial images into high-dimensional feature spaces. During training,\nthese contrastive losses focus exclusively on identity information as labels.\nHowever, we observe a multiscale geometric structure emerging in the embedding\nspace, influenced by interpretable facial (e.g., hair color) and image\nattributes (e.g., contrast). We propose a geometric approach to describe the\ndependence or invariance of FR models to these attributes and introduce a\nphysics-inspired alignment metric. We evaluate the proposed metric on\ncontrolled, simplified models and widely used FR models fine-tuned with\nsynthetic data for targeted attribute augmentation. Our findings reveal that\nthe models exhibit varying degrees of invariance across different attributes,\nproviding insight into their strengths and weaknesses and enabling deeper\ninterpretability. Code available here:\nhttps://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs",
      "pdf_url": "http://arxiv.org/pdf/2507.11372v1",
      "published": "2025-07-15T14:44:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11372v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning",
      "authors": [
        "Daniel Tanneberg"
      ],
      "abstract": "Training neural networks with reinforcement learning (RL) typically relies on\nbackpropagation (BP), necessitating storage of activations from the forward\npass for subsequent backward updates. Furthermore, backpropagating error\nsignals through multiple layers often leads to vanishing or exploding\ngradients, which can degrade learning performance and stability. We propose a\nnovel approach that trains each layer of the neural network using local signals\nduring the forward pass in RL settings. Our approach introduces local,\nlayer-wise losses leveraging the principle of matching pairwise distances from\nmulti-dimensional scaling, enhanced with optional reward-driven guidance. This\nmethod allows each hidden layer to be trained using local signals computed\nduring forward propagation, thus eliminating the need for backward passes and\nstoring intermediate activations. Our experiments, conducted with policy\ngradient methods across common RL benchmarks, demonstrate that this\nbackpropagation-free method achieves competitive performance compared to their\nclassical BP-based counterpart. Additionally, the proposed method enhances\nstability and consistency within and across runs, and improves performance\nespecially in challenging environments.",
      "pdf_url": "http://arxiv.org/pdf/2507.11367v1",
      "published": "2025-07-15T14:39:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11367v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces",
      "authors": [
        "Yunhao Yang",
        "Neel P. Bhatt",
        "Christian Ellis",
        "Alvaro Velasquez",
        "Zhangyang Wang",
        "Ufuk Topcu"
      ],
      "abstract": "Logistics operators, from battlefield coordinators rerouting airlifts ahead\nof a storm to warehouse managers juggling late trucks, often face life-critical\ndecisions that demand both domain expertise and rapid and continuous\nreplanning. While popular methods like integer programming yield logistics\nplans that satisfy user-defined logical constraints, they are slow and assume\nan idealized mathematical model of the environment that does not account for\nuncertainty. On the other hand, large language models (LLMs) can handle\nuncertainty and promise to accelerate replanning while lowering the barrier to\nentry by translating free-form utterances into executable plans, yet they\nremain prone to misinterpretations and hallucinations that jeopardize safety\nand cost. We introduce a neurosymbolic framework that pairs the accessibility\nof natural-language dialogue with verifiable guarantees on goal interpretation.\nIt converts user requests into structured planning specifications, quantifies\nits own uncertainty at the field and token level, and invokes an interactive\nclarification loop whenever confidence falls below an adaptive threshold. A\nlightweight model, fine-tuned on just 100 uncertainty-filtered examples,\nsurpasses the zero-shot performance of GPT-4.1 while cutting inference latency\nby nearly 50%. These preliminary results highlight a practical path toward\ncertifiable, real-time, and user-aligned decision-making for complex logistics.",
      "pdf_url": "http://arxiv.org/pdf/2507.11352v1",
      "published": "2025-07-15T14:24:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11352v1",
      "categories": [
        "cs.AI",
        "cs.FL"
      ]
    },
    {
      "title": "Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM",
      "authors": [
        "Oscar Lima",
        "Marc Vinci",
        "Sunandita Patra",
        "Sebastian Stock",
        "Joachim Hertzberg",
        "Martin Atzmueller",
        "Malik Ghallab",
        "Dana Nau",
        "Paolo Traverso"
      ],
      "abstract": "Robotic task execution faces challenges due to the inconsistency between\nsymbolic planner models and the rich control structures actually running on the\nrobot. In this paper, we present the first physical deployment of an integrated\nactor-planner system that shares hierarchical operational models for both\nacting and planning, interleaving the Reactive Acting Engine (RAE) with an\nanytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile\nmanipulator in a real-world deployment for an object collection task. Our\nexperiments demonstrate robust task execution under action failures and sensor\nnoise, and provide empirical insights into the interleaved acting-and-planning\ndecision making process.",
      "pdf_url": "http://arxiv.org/pdf/2507.11345v1",
      "published": "2025-07-15T14:20:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11345v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking",
      "authors": [
        "Yuehao Huang",
        "Liang Liu",
        "Shuangming Lei",
        "Yukai Ma",
        "Hao Su",
        "Jianbiao Mei",
        "Pengxiang Zhao",
        "Yaqing Gu",
        "Yong Liu",
        "Jiajun Lv"
      ],
      "abstract": "Mobile robots are increasingly required to navigate and interact within\nunknown and unstructured environments to meet human demands. Demand-driven\nnavigation (DDN) enables robots to identify and locate objects based on\nimplicit human intent, even when object locations are unknown. However,\ntraditional data-driven DDN methods rely on pre-collected data for model\ntraining and decision-making, limiting their generalization capability in\nunseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that\nemulates the human cognitive and learning mechanisms by integrating fast and\nslow thinking systems and selectively identifying key objects essential to\nfulfilling user demands. CogDDN identifies appropriate target objects by\nsemantically aligning detected objects with the given instructions.\nFurthermore, it incorporates a dual-process decision-making module, comprising\na Heuristic Process for rapid, efficient decisions and an Analytic Process that\nanalyzes past errors, accumulates them in a knowledge base, and continuously\nimproves performance. Chain of Thought (CoT) reasoning strengthens the\ndecision-making process. Extensive closed-loop evaluations on the AI2Thor\nsimulator with the ProcThor dataset show that CogDDN outperforms single-view\ncamera-only methods by 15%, demonstrating significant improvements in\nnavigation accuracy and adaptability. The project page is available at\nhttps://yuehaohuang.github.io/CogDDN/.",
      "pdf_url": "http://arxiv.org/pdf/2507.11334v1",
      "published": "2025-07-15T14:06:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11334v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "SystolicAttention: Fusing FlashAttention within a Single Systolic Array",
      "authors": [
        "Jiawei Lin",
        "Guokai Chen",
        "Yuanlong Li",
        "Thomas Bourgeat"
      ],
      "abstract": "Transformer models rely heavily on scaled dot-product attention (SDPA),\ntypically implemented using the FlashAttention algorithm. However, current\nsystolic-array-based accelerators face significant challenges when executing\nFlashAttention. Systolic arrays can only achieve high utilization for\nconsecutive and large matrix multiplications. In contrast, FlashAttention\nrequires frequently interleaved matrix multiplications and softmax operations.\n  The frequent data swaps between the systolic array and external vector units\nresult in low systolic array utilization. This is further exacerbated by the\nfact that softmax involves numerous non-matrix operations, which are not\nwell-suited for systolic arrays. Moreover, the concurrent execution of matrix\nmultiplication on systolic arrays and softmax on vector units leads to register\nfile and SRAM port contention, further degrading performance.\n  To overcome these limitations, we propose FSA, an enhanced systolic array\narchitecture that enables the entire FlashAttention algorithm to run entirely\nwithin a single systolic array, eliminating the need for external vector units.\nAt the core of FSA is SystolicAttention, a novel scheduling algorithm that maps\nFlashAttention operations onto systolic arrays with fine-grained, element-wise\noverlap. This significantly improves array utilization while preserving the\noriginal floating-point operation order to maintain numerical stability.\n  We implement FSA in synthesizable RTL and evaluate its performance against\nstate-of-the-art commercial accelerators. Our results show that FSA achieves\n1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS\nNeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area\noverhead.",
      "pdf_url": "http://arxiv.org/pdf/2507.11331v2",
      "published": "2025-07-15T14:04:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11331v2",
      "categories": [
        "cs.AR",
        "cs.AI"
      ]
    },
    {
      "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge",
      "authors": [
        "Wenqing Wu",
        "Chengzhi Zhang",
        "Yi Zhao"
      ],
      "abstract": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. One of the most common types of\nnovelty in academic papers is the introduction of new methods. In this paper,\nwe propose leveraging human knowledge and LLM to assist pretrained language\nmodels (PLMs, e.g. BERT etc.) in predicting the method novelty of papers.\nSpecifically, we extract sentences related to the novelty of the academic paper\nfrom peer review reports and use LLM to summarize the methodology section of\nthe academic paper, which are then used to fine-tune PLMs. In addition, we have\ndesigned a text-guided fusion module with novel Sparse-Attention to better\nintegrate human and LLM knowledge. We compared the method we proposed with a\nlarge number of baselines. Extensive experiments demonstrate that our method\nachieves superior performance.",
      "pdf_url": "http://arxiv.org/pdf/2507.11330v2",
      "published": "2025-07-15T14:03:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11330v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL",
        "cs.HC"
      ]
    },
    {
      "title": "Quantitative multi-metabolite imaging of Parkinson's disease using AI boosted molecular MRI",
      "authors": [
        "Hagar Shmuely",
        "Michal Rivlin",
        "Or Perlman"
      ],
      "abstract": "Traditional approaches for molecular imaging of Parkinson's disease (PD) in\nvivo require radioactive isotopes, lengthy scan times, or deliver only low\nspatial resolution. Recent advances in saturation transfer-based PD magnetic\nresonance imaging (MRI) have provided biochemical insights, although the image\ncontrast is semi-quantitative and nonspecific. Here, we combined a rapid\nmolecular MRI acquisition paradigm with deep learning based reconstruction for\nmulti-metabolite quantification of glutamate, mobile proteins, semisolid, and\nmobile macromolecules in an acute MPTP\n(1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine) mouse model. The quantitative\nparameter maps are in general agreement with the histology and MR spectroscopy,\nand demonstrate that semisolid magnetization transfer (MT), amide, and\naliphatic relayed nuclear Overhauser effect (rNOE) proton volume fractions may\nserve as PD biomarkers.",
      "pdf_url": "http://arxiv.org/pdf/2507.11329v1",
      "published": "2025-07-15T14:01:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11329v1",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ]
    },
    {
      "title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging",
      "authors": [
        "Arefin Ittesafun Abian",
        "Ripon Kumar Debnath",
        "Md. Abdur Rahman",
        "Mohaimenul Azam Khan Raiaan",
        "Md Rafiqul Islam",
        "Asif Karim",
        "Reem E. Mohamed",
        "Sami Azam"
      ],
      "abstract": "Accurate liver and tumor segmentation on abdominal CT images is critical for\nreliable diagnosis and treatment planning, but remains challenging due to\ncomplex anatomical structures, variability in tumor appearance, and limited\nannotated data. To address these issues, we introduce Hyperbolic-convolutions\nAdaptive-temporal-attention with Neural-representation and Synaptic-plasticity\nNetwork (HANS-Net), a novel segmentation framework that synergistically\ncombines hyperbolic convolutions for hierarchical geometric representation, a\nwavelet-inspired decomposition module for multi-scale texture learning, a\nbiologically motivated synaptic plasticity mechanism for adaptive feature\nenhancement, and an implicit neural representation branch to model fine-grained\nand continuous anatomical boundaries. Additionally, we incorporate\nuncertainty-aware Monte Carlo dropout to quantify prediction confidence and\nlightweight temporal attention to improve inter-slice consistency without\nsacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate\nthat HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an\naverage symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap\nerror (VOE) of 11.91%. Furthermore, cross-dataset validation on the\n3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of\n1.525 mm, and VOE of 19.71%, indicating strong generalization across different\ndatasets. These results confirm the effectiveness and robustness of HANS-Net in\nproviding anatomically consistent, accurate, and confident liver and tumor\nsegmentation.",
      "pdf_url": "http://arxiv.org/pdf/2507.11325v1",
      "published": "2025-07-15T13:56:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11325v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Contestability in Quantitative Argumentation",
      "authors": [
        "Xiang Yin",
        "Nico Potyka",
        "Antonio Rago",
        "Timotheus Kampik",
        "Francesca Toni"
      ],
      "abstract": "Contestable AI requires that AI-driven decisions align with human\npreferences. While various forms of argumentation have been shown to support\ncontestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks\n(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs\ncan be deployed for this purpose. Specifically, we introduce the contestability\nproblem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)\nto achieve a desired strength for a specific argument of interest (i.e., a\ntopic argument). To address this problem, we propose gradient-based relation\nattribution explanations (G-RAEs), which quantify the sensitivity of the topic\nargument's strength to changes in individual edge weights, thus providing\ninterpretable guidance for weight adjustments towards contestability. Building\non G-RAEs, we develop an iterative algorithm that progressively adjusts the\nedge weights to attain the desired strength. We evaluate our approach\nexperimentally on synthetic EW-QBAFs that simulate the structural\ncharacteristics of personalised recommender systems and multi-layer\nperceptrons, and demonstrate that it can solve the problem effectively.",
      "pdf_url": "http://arxiv.org/pdf/2507.11323v1",
      "published": "2025-07-15T13:54:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11323v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation",
      "authors": [
        "Haoran Jin",
        "Meng Li",
        "Xiting Wang",
        "Zhihao Xu",
        "Minlie Huang",
        "Yantao Jia",
        "Defu Lian"
      ],
      "abstract": "Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA.",
      "pdf_url": "http://arxiv.org/pdf/2507.11316v1",
      "published": "2025-07-15T13:48:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11316v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Opus: A Prompt Intention Framework for Complex Workflow Generation",
      "authors": [
        "Théo Fagnoni",
        "Mahsun Altin",
        "Chia En Chung",
        "Phillip Kingston",
        "Alan Tuning",
        "Dana O. Mohamed",
        "Inès Adnani"
      ],
      "abstract": "This paper introduces the Opus Prompt Intention Framework, designed to\nimprove complex Workflow Generation with instruction-tuned Large Language\nModels (LLMs). We propose an intermediate Intention Capture layer between user\nqueries and Workflow Generation, implementing the Opus Workflow Intention\nFramework, which consists of extracting Workflow Signals from user queries,\ninterpreting them into structured Workflow Intention objects, and generating\nWorkflows based on these Intentions. Our results show that this layer enables\nLLMs to produce logical and meaningful outputs that scale reliably as query\ncomplexity increases. On a synthetic benchmark of 1,000 multi-intent\nquery-Workflow(s) pairs, applying the Opus Prompt Intention Framework to\nWorkflow Generation yields consistent improvements in semantic Workflow\nsimilarity metrics. In this paper, we introduce the Opus Prompt Intention\nFramework by applying the concepts of Workflow Signal and Workflow Intention to\nLLM-driven Workflow Generation. We present a reproducible, customizable\nLLM-based Intention Capture system to extract Workflow Signals and Workflow\nIntentions from user queries. Finally, we provide empirical evidence that the\nproposed system significantly improves Workflow Generation quality compared to\ndirect generation from user queries, particularly in cases of Mixed Intention\nElicitation.",
      "pdf_url": "http://arxiv.org/pdf/2507.11288v1",
      "published": "2025-07-15T13:13:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11288v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems",
      "authors": [
        "Dany Moshkovich",
        "Sergey Zeltyn"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed within agentic\nsystems-collections of interacting, LLM-powered agents that execute complex,\nadaptive workflows using memory, tools, and dynamic planning. While enabling\npowerful new capabilities, these systems also introduce unique forms of\nuncertainty stemming from probabilistic reasoning, evolving memory states, and\nfluid execution paths. Traditional software observability and operations\npractices fall short in addressing these challenges.\n  This paper introduces AgentOps: a comprehensive framework for observing,\nanalyzing, optimizing, and automating operation of agentic AI systems. We\nidentify distinct needs across four key roles-developers, testers, site\nreliability engineers (SREs), and business users-each of whom engages with the\nsystem at different points in its lifecycle. We present the AgentOps Automation\nPipeline, a six-stage process encompassing behavior observation, metric\ncollection, issue detection, root cause analysis, optimized recommendations,\nand runtime automation. Throughout, we emphasize the critical role of\nautomation in managing uncertainty and enabling self-improving AI systems-not\nby eliminating uncertainty, but by taming it to ensure safe, adaptive, and\neffective operation.",
      "pdf_url": "http://arxiv.org/pdf/2507.11277v1",
      "published": "2025-07-15T12:54:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11277v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound",
      "authors": [
        "Tal Fiskus",
        "Uri Shaham"
      ],
      "abstract": "Deep reinforcement learning (DRL) agents excel in solving complex\ndecision-making tasks across various domains. However, they often require a\nsubstantial number of training steps and a vast experience replay buffer,\nleading to significant computational and resource demands. To address these\nchallenges, we introduce a novel theoretical result that leverages the\nNeyman-Rubin potential outcomes framework into DRL. Unlike most methods that\nfocus on bounding the counterfactual loss, we establish a causal bound on the\nfactual loss, which is analogous to the on-policy loss in DRL. This bound is\ncomputed by storing past value network outputs in the experience replay buffer,\neffectively utilizing data that is usually discarded. Extensive experiments\nacross the Atari 2600 and MuJoCo domains on various agents, such as DQN and\nSAC, achieve up to 2,427% higher reward ratio, outperforming the same agents\nwithout our proposed term, and reducing the experience replay buffer size by up\nto 96%, significantly improving sample efficiency at negligible cost.",
      "pdf_url": "http://arxiv.org/pdf/2507.11269v1",
      "published": "2025-07-15T12:46:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11269v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery",
      "authors": [
        "Aon Safdar",
        "Usman Akram",
        "Waseem Anwar",
        "Basit Malik",
        "Mian Ibad Ali"
      ],
      "abstract": "Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared\n(TI) imagery in the defense and surveillance domain is a challenging computer\nvision (CV) task in comparison to the commercial autonomous vehicle perception\ndomain. Limited datasets, peculiar domain-specific and TI modality-specific\nchallenges, i.e., limited hardware, scale invariance issues due to greater\ndistances, deliberate occlusion by tactical vehicles, lower sensor resolution\nand resultant lack of structural information in targets, effects of weather,\ntemperature, and time of day variations, and varying target to clutter ratios\nall result in increased intra-class variability and higher inter-class\nsimilarity, making accurate real-time ATR a challenging CV task. Resultantly,\ncontemporary state-of-the-art (SOTA) deep learning architectures underperform\nin the ATR domain. We propose a modified anchor-based single-stage detector,\ncalled YOLOatr, based on a modified YOLOv5s, with optimal modifications to the\ndetection heads, feature fusion in the neck, and a custom augmentation profile.\nWe evaluate the performance of our proposed model on a comprehensive DSIAC MWIR\ndataset for real-time ATR over both correlated and decorrelated testing\nprotocols. The results demonstrate that our proposed model achieves\nstate-of-the-art ATR performance of up to 99.6%.",
      "pdf_url": "http://arxiv.org/pdf/2507.11267v1",
      "published": "2025-07-15T12:41:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11267v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion",
      "authors": [
        "Jin Li",
        "Zezhong Ding",
        "Xike Xie"
      ],
      "abstract": "Knowledge graphs (KGs) are vital for enabling knowledge reasoning across\nvarious domains. Recent KG reasoning methods that integrate both global and\nlocal information have achieved promising results. However, existing methods\noften suffer from score over-smoothing, which blurs the distinction between\ncorrect and incorrect answers and hinders reasoning effectiveness. To address\nthis, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with\ndual-pathway global-local fusion. DuetGraph tackles over-smoothing by\nsegregating -- rather than stacking -- the processing of local (via message\npassing) and global (via attention) information into two distinct pathways,\npreventing mutual interference and preserving representational discrimination.\nIn addition, DuetGraph introduces a coarse-to-fine optimization, which\npartitions entities into high- and low-score subsets. This strategy narrows the\ncandidate space and sharpens the score gap between the two subsets, which\nalleviates over-smoothing and enhances inference quality. Extensive experiments\non various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)\nperformance, with up to an 8.7% improvement in reasoning quality and a\n1.8$\\times$ acceleration in training efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2507.11229v1",
      "published": "2025-07-15T11:59:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11229v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "An Agentic Flow for Finite State Machine Extraction using Prompt Chaining",
      "authors": [
        "Fares Wael",
        "Youssef Maklad",
        "Ali Hamdi",
        "Wael Elsersy"
      ],
      "abstract": "Finite-State Machines (FSMs) are critical for modeling the operational logic\nof network protocols, enabling verification, analysis, and vulnerability\ndiscovery. However, existing FSM extraction techniques face limitations such as\nscalability, incomplete coverage, and ambiguity in natural language\nspecifications. In this paper, we propose FlowFSM, a novel agentic framework\nthat leverages Large Language Models (LLMs) combined with prompt chaining and\nchain-of-thought reasoning to extract accurate FSMs from raw RFC documents.\nFlowFSM systematically processes protocol specifications, identifies state\ntransitions, and constructs structured rule-books by chaining agent outputs.\nExperimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM\nachieves high extraction precision while minimizing hallucinated transitions,\nshowing promising results. Our findings highlight the potential of agent-based\nLLM systems in the advancement of protocol analysis and FSM inference for\ncybersecurity and reverse engineering applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.11222v1",
      "published": "2025-07-15T11:50:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11222v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias",
      "authors": [
        "Rushia Harada",
        "Yuken Kimura",
        "Keito Inoshita"
      ],
      "abstract": "Well-being in family settings involves subtle psychological dynamics that\nconventional metrics often overlook. In particular, unconscious parental\nexpectations, termed ideal parent bias, can suppress children's emotional\nexpression and autonomy. This suppression, referred to as suppressed emotion,\noften stems from well-meaning but value-driven communication, which is\ndifficult to detect or address from outside the family. Focusing on these\nlatent dynamics, this study explores Large Language Model (LLM)-based support\nfor psychologically safe family communication. We constructed a Japanese\nparent-child dialogue corpus of 30 scenarios, each annotated with metadata on\nideal parent bias and suppressed emotion. Based on this corpus, we developed a\nRole-Playing LLM-based multi-agent dialogue support framework that analyzes\ndialogue and generates feedback. Specialized agents detect suppressed emotion,\ndescribe implicit ideal parent bias in parental speech, and infer contextual\nattributes such as the child's age and background. A meta-agent compiles these\noutputs into a structured report, which is then passed to five selected expert\nagents. These agents collaboratively generate empathetic and actionable\nfeedback through a structured four-step discussion process. Experiments show\nthat the system can detect categories of suppressed emotion with moderate\naccuracy and produce feedback rated highly in empathy and practicality.\nMoreover, simulated follow-up dialogues incorporating this feedback exhibited\nsigns of improved emotional expression and mutual understanding, suggesting the\nframework's potential in supporting positive transformation in family\ninteractions.",
      "pdf_url": "http://arxiv.org/pdf/2507.11210v1",
      "published": "2025-07-15T11:27:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11210v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding",
      "authors": [
        "Conrad Borchers",
        "Bahar Shahrokhian",
        "Francesco Balzan",
        "Elham Tajik",
        "Sreecharan Sankaranarayanan",
        "Sebastian Simon"
      ],
      "abstract": "Large Language Models (LLMs) enable new possibilities for qualitative\nresearch at scale, including coding and data annotation. While multi-agent\nsystems (MAS) can emulate human coding workflows, their benefits over\nsingle-agent coding remain poorly understood. We conducted an experimental\nstudy of how agent persona and temperature shape consensus-building and coding\naccuracy of dialog segments based on a codebook with 8 codes. Our open-source\nMAS mirrors deductive human coding through structured agent discussion and\nconsensus arbitration. Using six open-source LLMs (with 3 to 32 billion\nparameters) and 18 experimental configurations, we analyze over 77,000 coding\ndecisions against a gold-standard dataset of human-annotated transcripts from\nonline math tutoring sessions. Temperature significantly impacted whether and\nwhen consensus was reached across all six LLMs. MAS with multiple personas\n(including neutral, assertive, or empathetic), significantly delayed consensus\nin four out of six LLMs compared to uniform personas. In three of those LLMs,\nhigher temperatures significantly diminished the effects of multiple personas\non consensus. However, neither temperature nor persona pairing lead to robust\nimprovements in coding accuracy. Single agents matched or outperformed MAS\nconsensus in most conditions. Only one model (OpenHermesV2:7B) and code\ncategory showed above-chance gains from MAS deliberation when temperature was\n0.5 or lower and especially when the agents included at least one assertive\npersona. Qualitative analysis of MAS collaboration for these configurations\nsuggests that MAS may nonetheless aid in narrowing ambiguous code applications\nthat could improve codebooks and human-AI coding. We contribute new insight\ninto the limits of LLM-based qualitative methods, challenging the notion that\ndiverse MAS personas lead to better outcomes. We open-source our MAS and\nexperimentation code.",
      "pdf_url": "http://arxiv.org/pdf/2507.11198v1",
      "published": "2025-07-15T11:06:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11198v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment",
      "authors": [
        "Md. Emon Akter Sourov",
        "Md. Sabbir Hossen",
        "Pabon Shaha",
        "Mohammad Minoar Hossain",
        "Md Sadiq Iqbal"
      ],
      "abstract": "Heart disease remains a major global health concern, particularly in regions\nwith limited access to medical resources and diagnostic facilities. Traditional\ndiagnostic methods often fail to accurately identify and manage heart disease\nrisks, leading to adverse outcomes. Machine learning has the potential to\nsignificantly enhance the accuracy, efficiency, and speed of heart disease\ndiagnosis. In this study, we proposed a comprehensive framework that combines\nclassification models for heart disease detection and regression models for\nrisk prediction. We employed the Heart Disease dataset, which comprises 1,035\ncases. To address the issue of class imbalance, the Synthetic Minority\nOversampling Technique (SMOTE) was applied, resulting in the generation of an\nadditional 100,000 synthetic data points. Performance metrics, including\naccuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to\nevaluate the model's effectiveness. Among the classification models, Random\nForest emerged as the standout performer, achieving an accuracy of 97.2% on\nreal data and 97.6% on synthetic data. For regression tasks, Linear Regression\ndemonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic\ndatasets, respectively, with the lowest error metrics. Additionally,\nExplainable AI techniques were employed to enhance the interpretability of the\nmodels. This study highlights the potential of machine learning to\nrevolutionize heart disease diagnosis and risk prediction, thereby facilitating\nearly intervention and enhancing clinical decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2507.11185v1",
      "published": "2025-07-15T10:38:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11185v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Mixture of Experts in Large Language Models",
      "authors": [
        "Danyang Zhang",
        "Junhao Song",
        "Ziqian Bi",
        "Yingfang Yuan",
        "Tianyang Wang",
        "Joe Yeong",
        "Junfeng Hao"
      ],
      "abstract": "This paper presents a comprehensive review of the Mixture-of-Experts (MoE)\narchitecture in large language models, highlighting its ability to\nsignificantly enhance model performance while maintaining minimal computational\noverhead. Through a systematic analysis spanning theoretical foundations, core\narchitectural designs, and large language model (LLM) applications, we examine\nexpert gating and routing mechanisms, hierarchical and sparse MoE\nconfigurations, meta-learning approaches, multimodal and multitask learning\nscenarios, real-world deployment cases, and recent advances and challenges in\ndeep learning. Our analysis identifies key advantages of MoE, including\nsuperior model capacity compared to equivalent Bayesian approaches, improved\ntask-specific performance, and the ability to scale model capacity efficiently.\nWe also underscore the importance of ensuring expert diversity, accurate\ncalibration, and reliable inference aggregation, as these are essential for\nmaximizing the effectiveness of MoE architectures. Finally, this review\noutlines current research limitations, open challenges, and promising future\ndirections, providing a foundation for continued innovation in MoE architecture\nand its applications.",
      "pdf_url": "http://arxiv.org/pdf/2507.11181v1",
      "published": "2025-07-15T10:36:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11181v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Gradient Regularization-based Neural Granger Causality",
      "authors": [
        "Meiliang Liu",
        "Huiwen Dong",
        "Xiaoxiao Yang",
        "Yunfang Xu",
        "Zijin Li",
        "Zhengye Si",
        "Xinyue Yang",
        "Zhiwen Zhao"
      ],
      "abstract": "With the advancement of deep learning technologies, various neural\nnetwork-based Granger causality models have been proposed. Although these\nmodels have demonstrated notable improvements, several limitations remain. Most\nexisting approaches adopt the component-wise architecture, necessitating the\nconstruction of a separate model for each time series, which results in\nsubstantial computational costs. In addition, imposing the sparsity-inducing\npenalty on the first-layer weights of the neural network to extract causal\nrelationships weakens the model's ability to capture complex interactions. To\naddress these limitations, we propose Gradient Regularization-based Neural\nGranger Causality (GRNGC), which requires only one time series prediction model\nand applies $L_{1}$ regularization to the gradient between model's input and\noutput to infer Granger causality. Moreover, GRNGC is not tied to a specific\ntime series forecasting model and can be implemented with diverse architectures\nsuch as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical\nsimulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC\noutperforms existing baselines and significantly reduces computational\noverhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder\nurothelial carcinoma datasets further validate the model's effectiveness in\nreconstructing gene regulatory networks.",
      "pdf_url": "http://arxiv.org/pdf/2507.11178v1",
      "published": "2025-07-15T10:35:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11178v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Improving Wi-Fi Network Performance Prediction with Deep Learning Models",
      "authors": [
        "Gabriele Formis",
        "Amanda Ericson",
        "Stefan Forsstrom",
        "Kyi Thar",
        "Gianluca Cena",
        "Stefano Scanzio"
      ],
      "abstract": "The increasing need for robustness, reliability, and determinism in wireless\nnetworks for industrial and mission-critical applications is the driver for the\ngrowth of new innovative methods. The study presented in this work makes use of\nmachine learning techniques to predict channel quality in a Wi-Fi network in\nterms of the frame delivery ratio. Predictions can be used proactively to\nadjust communication parameters at runtime and optimize network operations for\nindustrial applications. Methods including convolutional neural networks and\nlong short-term memory were analyzed on datasets acquired from a real Wi-Fi\nsetup across multiple channels. The models were compared in terms of prediction\naccuracy and computational complexity. Results show that the frame delivery\nratio can be reliably predicted, and convolutional neural networks, although\nslightly less effective than other models, are more efficient in terms of CPU\nusage and memory consumption. This enhances the model's usability on embedded\nand industrial systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.11168v1",
      "published": "2025-07-15T10:18:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11168v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "Assessing Color Vision Test in Large Vision-language Models",
      "authors": [
        "Hongfei Ye",
        "Bin Chen",
        "Wenxi Liu",
        "Yu Zhang",
        "Zhao Li",
        "Dandan Ni",
        "Hongyang Chen"
      ],
      "abstract": "With the widespread adoption of large vision-language models, the capacity\nfor color vision in these models is crucial. However, the color vision\nabilities of large visual-language models have not yet been thoroughly\nexplored. To address this gap, we define a color vision testing task for large\nvision-language models and construct a dataset \\footnote{Anonymous Github\nShowing some of the data\nhttps://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers\nmultiple categories of test questions and tasks of varying difficulty levels.\nFurthermore, we analyze the types of errors made by large vision-language\nmodels and propose fine-tuning strategies to enhance their performance in color\nvision tests.",
      "pdf_url": "http://arxiv.org/pdf/2507.11153v1",
      "published": "2025-07-15T10:03:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11153v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Latent Space Consistency for Sparse-View CT Reconstruction",
      "authors": [
        "Duoyou Chen",
        "Yunqing Chen",
        "Can Zhang",
        "Zhou Wang",
        "Cheng Chen",
        "Ruoxiu Xiao"
      ],
      "abstract": "Computed Tomography (CT) is a widely utilized imaging modality in clinical\nsettings. Using densely acquired rotational X-ray arrays, CT can capture 3D\nspatial features. However, it is confronted with challenged such as significant\ntime consumption and high radiation exposure. CT reconstruction methods based\non sparse-view X-ray images have garnered substantial attention from\nresearchers as they present a means to mitigate costs and risks. In recent\nyears, diffusion models, particularly the Latent Diffusion Model (LDM), have\ndemonstrated promising potential in the domain of 3D CT reconstruction.\nNonetheless, due to the substantial differences between the 2D latent\nrepresentation of X-ray modalities and the 3D latent representation of CT\nmodalities, the vanilla LDM is incapable of achieving effective alignment\nwithin the latent space. To address this issue, we propose the Consistent\nLatent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature\ncontrastive learning to efficiently extract latent 3D information from 2D X-ray\nimages and achieve latent space alignment between modalities. Experimental\nresults indicate that CLS-DM outperforms classical and state-of-the-art\ngenerative models in terms of standard voxel-level metrics (PSNR, SSIM) on the\nLIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing\nthe effectiveness and economic viability of sparse X-ray reconstructed CT but\ncan also be generalized to other cross-modal transformation tasks, such as\ntext-to-image synthesis. We have made our code publicly available at\nhttps://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research\nand applications in other domains.",
      "pdf_url": "http://arxiv.org/pdf/2507.11152v1",
      "published": "2025-07-15T10:02:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11152v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming",
      "authors": [
        "Alessandro Bertagnon",
        "Marcello Dalpasso",
        "Michele Favalli",
        "Marco Gavanelli"
      ],
      "abstract": "In the design of integrated circuits, one critical metric is the maximum\ndelay introduced by combinational modules within the circuit. This delay is\ncrucial because it represents the time required to perform a computation: in an\nArithmetic-Logic Unit it represents the maximum time taken by the circuit to\nperform an arithmetic operation. When such a circuit is part of a larger,\nsynchronous system, like a CPU, the maximum delay directly impacts the maximum\nclock frequency of the entire system. Typically, hardware designers use Static\nTiming Analysis to compute an upper bound of the maximum delay because it can\nbe determined in polynomial time. However, relying on this upper bound can lead\nto suboptimal processor speeds, thereby missing performance opportunities. In\nthis work, we tackle the challenging task of computing the actual maximum\ndelay, rather than an approximate value. Since the problem is computationally\nhard, we model it in Answer Set Programming (ASP), a logic language featuring\nextremely efficient solvers. We propose non-trivial encodings of the problem\ninto ASP. Experimental results show that ASP is a viable solution to address\ncomplex problems in hardware design.",
      "pdf_url": "http://arxiv.org/pdf/2507.11150v1",
      "published": "2025-07-15T09:57:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11150v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ]
    },
    {
      "title": "Collaborative Trustworthiness for Good Decision Making in Autonomous Systems",
      "authors": [
        "Selma Saidi",
        "Omar Laimona",
        "Christoph Schmickler",
        "Dirk Ziegenbein"
      ],
      "abstract": "Autonomous systems are becoming an integral part of many application domains,\nlike in the mobility sector. However, ensuring their safe and correct behaviour\nin dynamic and complex environments remains a significant challenge, where\nsystems should autonomously make decisions e.g., about manoeuvring. We propose\nin this paper a general collaborative approach for increasing the level of\ntrustworthiness in the environment of operation and improve reliability and\ngood decision making in autonomous system. In the presence of conflicting\ninformation, aggregation becomes a major issue for trustworthy decision making\nbased on collaborative data sharing. Unlike classical approaches in the\nliterature that rely on consensus or majority as aggregation rule, we exploit\nthe fact that autonomous systems have different quality attributes like\nperception quality. We use this criteria to determine which autonomous systems\nare trustworthy and borrow concepts from social epistemology to define\naggregation and propagation rules, used for automated decision making. We use\nBinary Decision Diagrams (BDDs) as formal models for beliefs aggregation and\npropagation, and formulate reduction rules to reduce the size of the BDDs and\nallow efficient computation structures for collaborative automated reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2507.11135v1",
      "published": "2025-07-15T09:37:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11135v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MMOne: Representing Multiple Modalities in One Scene",
      "authors": [
        "Zhifeng Gu",
        "Bing Wang"
      ],
      "abstract": "Humans perceive the world through multimodal cues to understand and interact\nwith the environment. Learning a scene representation for multiple modalities\nenhances comprehension of the physical world. However, modality conflicts,\narising from inherent distinctions among different modalities, present two\ncritical challenges: property disparity and granularity disparity. To address\nthese challenges, we propose a general framework, MMOne, to represent multiple\nmodalities in one scene, which can be readily extended to additional\nmodalities. Specifically, a modality modeling module with a novel modality\nindicator is proposed to capture the unique properties of each modality.\nAdditionally, we design a multimodal decomposition mechanism to separate\nmulti-modal Gaussians into single-modal Gaussians based on modality\ndifferences. We address the essential distinctions among modalities by\ndisentangling multimodal information into shared and modality-specific\ncomponents, resulting in a more compact and efficient multimodal scene\nrepresentation. Extensive experiments demonstrate that our method consistently\nenhances the representation capability for each modality and is scalable to\nadditional modalities. The code is available at\nhttps://github.com/Neal2020GitHub/MMOne.",
      "pdf_url": "http://arxiv.org/pdf/2507.11129v1",
      "published": "2025-07-15T09:29:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11129v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Defining neurosymbolic AI",
      "authors": [
        "Lennert De Smet",
        "Luc De Raedt"
      ],
      "abstract": "Neurosymbolic AI focuses on integrating learning and reasoning, in\nparticular, on unifying logical and neural representations. Despite the\nexistence of an alphabet soup of neurosymbolic AI systems, the field is lacking\na generally accepted formal definition of what neurosymbolic models and\ninference really are. We introduce a formal definition for neurosymbolic AI\nthat makes abstraction of its key ingredients. More specifically, we define\nneurosymbolic inference as the computation of an integral over a product of a\nlogical and a belief function. We show that our neurosymbolic AI definition\nmakes abstraction of key representative neurosymbolic AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.11127v1",
      "published": "2025-07-15T09:23:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11127v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AI Agent Architecture for Decentralized Trading of Alternative Assets",
      "authors": [
        "Ailiya Borjigin",
        "Cong He",
        "Charles CC Lee",
        "Wei Zhou"
      ],
      "abstract": "Decentralized trading of real-world alternative assets (e.g., gold) requires\nbridging physical asset custody with blockchain systems while meeting strict\nrequirements for compliance, liquidity, and risk management. We present\nGoldMine OS, a research oriented architecture that employs multiple specialized\nAI agents to automate and secure the tokenization and exchange of physical gold\ninto a blockchain based stablecoin (\"OZ\"). Our approach combines on chain smart\ncontracts for critical risk controls with off chain AI agents for decision\nmaking, blending the transparency and reliability of blockchains with the\nflexibility of AI driven automation. We describe four cooperative agents\n(Compliance, Token Issuance, Market Making, and Risk Control) and a\ncoordinating core, and evaluate the system through simulation and a controlled\npilot deployment. In experiments the prototype delivers on demand token\nissuance in under 1.2 s, more than 100 times faster than manual workflows. The\nMarket Making agent maintains tight liquidity with spreads often below 0.5\npercent even under volatile conditions. Fault injection tests show resilience:\nan oracle price spoofing attack is detected and mitigated within 10 s, and a\nsimulated vault mis reporting halts issuance immediately with minimal user\nimpact. The architecture scales to 5000 transactions per second with 10000\nconcurrent users in benchmarks. These results indicate that an AI agent based\ndecentralized exchange for alternative assets can satisfy rigorous performance\nand safety requirements. We discuss broader implications for democratizing\naccess to traditionally illiquid assets and explain how our governance model --\nmulti signature agent updates and on chain community voting on risk parameters\n-- provides ongoing transparency, adaptability, and formal assurance of system\nintegrity.",
      "pdf_url": "http://arxiv.org/pdf/2507.11117v1",
      "published": "2025-07-15T09:11:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11117v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing",
      "authors": [
        "Vassilis Sioros",
        "Alexandros Potamianos",
        "Giorgos Paraskevopoulos"
      ],
      "abstract": "In this study, we investigate leveraging cross-attention control for\nefficient audio editing within auto-regressive models. Inspired by image\nediting methodologies, we develop a Prompt-to-Prompt-like approach that guides\nedits through cross and self-attention mechanisms. Integrating a\ndiffusion-based strategy, influenced by Auffusion, we extend the model's\nfunctionality to support refinement edits, establishing a baseline for\nprompt-guided audio editing. Additionally, we introduce an alternative approach\nby incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and\npropose three editing mechanisms, based on Replacement, Reweighting, and\nRefinement of the attention scores. We employ commonly-used music-specific\nevaluation metrics and a human study, to gauge time-varying controllability,\nadherence to global text cues, and overall audio realism. The automatic and\nhuman evaluations indicate that the proposed combination of prompt-to-prompt\nguidance with autoregressive generation models significantly outperforms the\ndiffusion-based baseline in terms of melody, dynamics, and tempo of the\ngenerated audio. Our code is available at https://github.com/billsioros/EditGen",
      "pdf_url": "http://arxiv.org/pdf/2507.11096v1",
      "published": "2025-07-15T08:44:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11096v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Function-to-Style Guidance of LLMs for Code Translation",
      "authors": [
        "Longhui Zhang",
        "Bin Wang",
        "Jiahao Wang",
        "Xiaofeng Zhao",
        "Min Zhang",
        "Hao Yang",
        "Meishan Zhang",
        "Yu Li",
        "Jing Li",
        "Jun Yu",
        "Min Zhang"
      ],
      "abstract": "Large language models (LLMs) have made significant strides in code\ntranslation tasks. However, ensuring both the correctness and readability of\ntranslated code remains a challenge, limiting their effective adoption in\nreal-world software development. In this work, we propose F2STrans, a\nfunction-to-style guiding paradigm designed to progressively improve the\nperformance of LLMs in code translation. Our approach comprises two key stages:\n(1) Functional learning, which optimizes translation correctness using\nhigh-quality source-target code pairs mined from online programming platforms,\nand (2) Style learning, which improves translation readability by incorporating\nboth positive and negative style examples. Additionally, we introduce a novel\ncode translation benchmark that includes up-to-date source code, extensive test\ncases, and manually annotated ground-truth translations, enabling comprehensive\nfunctional and stylistic evaluations. Experiments on both our new benchmark and\nexisting datasets demonstrate that our approach significantly improves code\ntranslation performance. Notably, our approach enables Qwen-1.5B to outperform\nprompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code\ntranslation scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2507.11083v1",
      "published": "2025-07-15T08:25:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11083v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification",
      "authors": [
        "Chang Peng",
        "Bao Yang",
        "Meiqi Li",
        "Ge Zhang",
        "Hui Sun",
        "Zhenyu Jiang"
      ],
      "abstract": "Ground penetrating radar (GPR) has become a rapid and non-destructive\nsolution for road subsurface distress (RSD) detection. However, RSD recognition\nfrom GPR images is labor-intensive and heavily relies on inspectors' expertise.\nDeep learning offers the possibility for automatic RSD recognition, but its\ncurrent performance is limited by two factors: Scarcity of high-quality dataset\nfor network training and insufficient capability of network to distinguish RSD.\nIn this study, a rigorously validated 3D GPR dataset containing 2134 samples of\ndiverse types was constructed through field scanning. Based on the finding that\nthe YOLO model trained with one of the three scans of GPR images exhibits\nvarying sensitivity to specific type of RSD, we proposed a novel\ncross-verification strategy with outstanding accuracy in RSD recognition,\nachieving recall over 98.6% in field tests. The approach, integrated into an\nonline RSD detection system, can reduce the labor of inspection by around 90%.",
      "pdf_url": "http://arxiv.org/pdf/2507.11081v1",
      "published": "2025-07-15T08:23:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11081v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4.9; I.5.4; J.2"
      ]
    },
    {
      "title": "Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander",
      "authors": [
        "Li Wang",
        "Qizhen Wu",
        "Lei Chen"
      ],
      "abstract": "In multiple unmanned ground vehicle confrontations, autonomously evolving\nmulti-agent tactical decisions from situational awareness remain a significant\nchallenge. Traditional handcraft rule-based methods become vulnerable in the\ncomplicated and transient battlefield environment, and current reinforcement\nlearning methods mainly focus on action manipulation instead of strategic\ndecisions due to lack of interpretability. Here, we propose a vision-language\nmodel-based commander to address the issue of intelligent\nperception-to-decision reasoning in autonomous confrontations. Our method\nintegrates a vision language model for scene understanding and a lightweight\nlarge language model for strategic reasoning, achieving unified perception and\ndecision within a shared semantic space, with strong adaptability and\ninterpretability. Unlike rule-based search and reinforcement learning methods,\nthe combination of the two modules establishes a full-chain process, reflecting\nthe cognitive process of human commanders. Simulation and ablation experiments\nvalidate that the proposed approach achieves a win rate of over 80% compared\nwith baseline models.",
      "pdf_url": "http://arxiv.org/pdf/2507.11079v1",
      "published": "2025-07-15T08:22:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11079v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Joint angle model based learning to refine kinematic human pose estimation",
      "authors": [
        "Chang Peng",
        "Yifei Zhou",
        "Huifeng Xi",
        "Shiqing Huang",
        "Chuangye Chen",
        "Jianming Yang",
        "Bao Yang",
        "Zhenyu Jiang"
      ],
      "abstract": "Marker-free human pose estimation (HPE) has found increasing applications in\nvarious fields. Current HPE suffers from occasional errors in keypoint\nrecognition and random fluctuation in keypoint trajectories when analyzing\nkinematic human poses. The performance of existing deep learning-based models\nfor HPE refinement is considerably limited by inaccurate training datasets in\nwhich the keypoints are manually annotated. This paper proposed a novel method\nto overcome the difficulty through joint angle-based modeling. The key\ntechniques include: (i) A joint angle-based model of human pose, which is\nrobust to describe kinematic human poses; (ii) Approximating temporal variation\nof joint angles through high order Fourier series to get reliable \"ground\ntruth\"; (iii) A bidirectional recurrent network is designed as a\npost-processing module to refine the estimation of well-established HRNet.\nTrained with the high-quality dataset constructed using our method, the network\ndemonstrates outstanding performance to correct wrongly recognized joints and\nsmooth their spatiotemporal trajectories. Tests show that joint angle-based\nrefinement (JAR) outperforms the state-of-the-art HPE refinement network in\nchallenging cases like figure skating and breaking.",
      "pdf_url": "http://arxiv.org/pdf/2507.11075v1",
      "published": "2025-07-15T08:16:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.11075v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4.9; I.5.4; J.3"
      ]
    }
  ]
}