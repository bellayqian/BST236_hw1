{
  "last_updated": "2025-05-28T00:52:39.789212",
  "papers": [
    {
      "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models",
      "authors": [
        "Kai Sun",
        "Yushi Bai",
        "Zhen Yang",
        "Jiajie Zhang",
        "Ji Qi",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.",
      "pdf_url": "http://arxiv.org/pdf/2505.20152v1",
      "published": "2025-05-26T15:55:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20152v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "On the (Non) Injectivity of Piecewise Linear Janossy Pooling",
      "authors": [
        "Ilai Reshef",
        "Nadav Dym"
      ],
      "abstract": "Multiset functions, which are functions that map multisets to vectors, are a\nfundamental tool in the construction of neural networks for multisets and\ngraphs. To guarantee that the vector representation of the multiset is\nfaithful, it is often desirable to have multiset mappings that are both\ninjective and bi-Lipschitz. Currently, there are several constructions of\nmultiset functions achieving both these guarantees, leading to improved\nperformance in some tasks but often also to higher compute time than standard\nconstructions. Accordingly, it is natural to inquire whether simpler multiset\nfunctions achieving the same guarantees are available. In this paper, we make a\nlarge step towards giving a negative answer to this question. We consider the\nfamily of k-ary Janossy pooling, which includes many of the most popular\nmultiset models, and prove that no piecewise linear Janossy pooling function\ncan be injective. On the positive side, we show that when restricted to\nmultisets without multiplicities, even simple deep-sets models suffice for\ninjectivity and bi-Lipschitzness.",
      "pdf_url": "http://arxiv.org/pdf/2505.20150v1",
      "published": "2025-05-26T15:53:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20150v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases",
      "authors": [
        "Cheng-Yu Tai",
        "Ching-Wen Chen",
        "Chi-Chin Wu",
        "Bo-Chen Chiu",
        "Cheng-Hung",
        "Lin",
        "Cheng-Kai Lu",
        "Jia-Kang Wang",
        "Tzu-Lun Huang"
      ],
      "abstract": "This paper focuses on using few-shot learning to improve the accuracy of\nclassifying OCT diagnosis images with major and rare classes. We used the\nGAN-based augmentation strategy as a baseline and introduced several novel\nmethods to further enhance our model. The proposed strategy contains U-GAT-IT\nfor improving the generative part and uses the data balance technique to narrow\ndown the skew of accuracy between all categories. The best model obtained was\nbuilt with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an\noverall accuracy of 97.85%, representing a significant improvement over the\noriginal baseline.",
      "pdf_url": "http://arxiv.org/pdf/2505.20149v1",
      "published": "2025-05-26T15:49:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20149v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents",
      "authors": [
        "Ziming Wei",
        "Bingqian Lin",
        "Zijian Jiao",
        "Yunshuang Nie",
        "Liang Ma",
        "Yuecheng Liu",
        "Yuzheng Zhuang",
        "Xiaodan Liang"
      ],
      "abstract": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.",
      "pdf_url": "http://arxiv.org/pdf/2505.20148v1",
      "published": "2025-05-26T15:48:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20148v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs",
      "authors": [
        "Jialin Yang",
        "Dongfu Jiang",
        "Lipeng He",
        "Sherman Siu",
        "Yuxuan Zhang",
        "Disen Liao",
        "Zhuofeng Li",
        "Huaye Zeng",
        "Yiming Jia",
        "Haozhe Wang",
        "Benjamin Schneider",
        "Chi Ruan",
        "Wentao Ma",
        "Zhiheng Lyu",
        "Yifei Wang",
        "Yi Lu",
        "Quy Duc Do",
        "Ziyan Jiang",
        "Ping Nie",
        "Wenhu Chen"
      ],
      "abstract": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures.",
      "pdf_url": "http://arxiv.org/pdf/2505.20139v1",
      "published": "2025-05-26T15:40:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20139v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks",
      "authors": [
        "Cédric Goemaere",
        "Gaspard Oliviers",
        "Rafal Bogacz",
        "Thomas Demeester"
      ],
      "abstract": "Predictive Coding (PC) offers a biologically plausible alternative to\nbackpropagation for neural network training, yet struggles with deeper\narchitectures. This paper identifies the root cause: an inherent signal decay\nproblem where gradients attenuate exponentially with depth, becoming\ncomputationally negligible due to numerical precision constraints. To address\nthis fundamental limitation, we introduce Error Optimization (EO), a novel\nreparameterization that preserves PC's theoretical properties while eliminating\nsignal decay. By optimizing over prediction errors rather than states, EO\nenables signals to reach all layers simultaneously and without attenuation,\nconverging orders of magnitude faster than standard PC. Experiments across\nmultiple architectures and datasets demonstrate that EO matches\nbackpropagation's performance even for deeper models where conventional PC\nstruggles. Besides practical improvements, our work provides theoretical\ninsight into PC dynamics and establishes a foundation for scaling\nbiologically-inspired learning to deeper architectures on digital hardware and\nbeyond.",
      "pdf_url": "http://arxiv.org/pdf/2505.20137v1",
      "published": "2025-05-26T15:39:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20137v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks",
      "authors": [
        "Safa Hamreras",
        "Sukhbinder Singh",
        "Román Orús"
      ],
      "abstract": "Tensorizing a neural network involves reshaping some or all of its dense\nweight matrices into higher-order tensors and approximating them using low-rank\ntensor network decompositions. This technique has shown promise as a model\ncompression strategy for large-scale neural networks. However, despite\nencouraging empirical results, tensorized neural networks (TNNs) remain\nunderutilized in mainstream deep learning. In this position paper, we offer a\nperspective on both the potential and current limitations of TNNs. We argue\nthat TNNs represent a powerful yet underexplored framework for deep\nlearning--one that deserves greater attention from both engineering and\ntheoretical communities. Beyond compression, we highlight the value of TNNs as\na flexible class of architectures with distinctive scaling properties and\nincreased interpretability. A central feature of TNNs is the presence of bond\nindices, which introduce new latent spaces not found in conventional networks.\nThese internal representations may provide deeper insight into the evolution of\nfeatures across layers, potentially advancing the goals of mechanistic\ninterpretability. We conclude by outlining several key research directions\naimed at overcoming the practical barriers to scaling and adopting TNNs in\nmodern deep learning workflows.",
      "pdf_url": "http://arxiv.org/pdf/2505.20132v1",
      "published": "2025-05-26T15:32:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20132v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ]
    },
    {
      "title": "Agentic AI Process Observability: Discovering Behavioral Variability",
      "authors": [
        "Fabiana Fournier",
        "Lior Limonad",
        "Yuval David"
      ],
      "abstract": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.",
      "pdf_url": "http://arxiv.org/pdf/2505.20127v1",
      "published": "2025-05-26T15:26:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20127v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets",
      "authors": [
        "Simpson Zhang",
        "Tennison Liu",
        "Mihaela van der Schaar"
      ],
      "abstract": "Current labor markets are strongly affected by the economic forces of adverse\nselection, moral hazard, and reputation, each of which arises due to\n$\\textit{incomplete information}$. These economic forces will still be\ninfluential after AI agents are introduced, and thus, agents must use\nmetacognitive and strategic reasoning to perform effectively. Metacognition is\na form of $\\textit{internal reasoning}$ that includes the capabilities for\nself-assessment, task understanding, and evaluation of strategies. Strategic\nreasoning is $\\textit{external reasoning}$ that covers holding beliefs about\nother participants in the labor market (e.g., competitors, colleagues), making\nstrategic decisions, and learning about others over time. Both types of\nreasoning are required by agents as they decide among the many\n$\\textit{actions}$ they can take in labor markets, both within and outside\ntheir jobs. We discuss current research into metacognitive and strategic\nreasoning and the areas requiring further development.",
      "pdf_url": "http://arxiv.org/pdf/2505.20120v1",
      "published": "2025-05-26T15:22:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20120v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Spatiotemporal Causal Decoupling Model for Air Quality Forecasting",
      "authors": [
        "Jiaming Ma",
        "Guanjun Wang",
        "Sheng Huang",
        "Kuo Yang",
        "Binwu Wang",
        "Pengkun Wang",
        "Yang Wang"
      ],
      "abstract": "Due to the profound impact of air pollution on human health, livelihoods, and\neconomic development, air quality forecasting is of paramount significance.\nInitially, we employ the causal graph method to scrutinize the constraints of\nexisting research in comprehensively modeling the causal relationships between\nthe air quality index (AQI) and meteorological features. In order to enhance\nprediction accuracy, we introduce a novel air quality forecasting model,\nAirCade, which incorporates a causal decoupling approach. AirCade leverages a\nspatiotemporal module in conjunction with knowledge embedding techniques to\ncapture the internal dynamics of AQI. Subsequently, a causal decoupling module\nis proposed to disentangle synchronous causality from past AQI and\nmeteorological features, followed by the dissemination of acquired knowledge to\nfuture time steps to enhance performance. Additionally, we introduce a causal\nintervention mechanism to explicitly represent the uncertainty of future\nmeteorological features, thereby bolstering the model's robustness. Our\nevaluation of AirCade on an open-source air quality dataset demonstrates over\n20\\% relative improvement over state-of-the-art models.",
      "pdf_url": "http://arxiv.org/pdf/2505.20119v1",
      "published": "2025-05-26T15:21:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20119v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone",
      "authors": [
        "Cristian Santini",
        "Laura Melosi",
        "Emanuele Frontoni"
      ],
      "abstract": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences.",
      "pdf_url": "http://arxiv.org/pdf/2505.20113v1",
      "published": "2025-05-26T15:16:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20113v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression",
      "authors": [
        "Haolei Bai",
        "Siyong Jian",
        "Tuo Liang",
        "Yu Yin",
        "Huan Wang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models.Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness.",
      "pdf_url": "http://arxiv.org/pdf/2505.20112v1",
      "published": "2025-05-26T15:14:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20112v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Proxy-Free GFlowNet",
      "authors": [
        "Ruishuo Chen",
        "Xun Wang",
        "Rui Hu",
        "Zhuoran Li",
        "Longbo Huang"
      ],
      "abstract": "Generative Flow Networks (GFlowNets) are a promising class of generative\nmodels designed to sample diverse, high-reward structures by modeling\ndistributions over compositional objects. In many real-world applications,\nobtaining the reward function for such objects is expensive, time-consuming, or\nrequires human input, making it necessary to train GFlowNets from historical\ndatasets. Most existing methods adopt a model-based approach, learning a proxy\nmodel from the dataset to approximate the reward function. However, this\nstrategy inherently ties the quality of the learned policy to the accuracy of\nthe proxy, introducing additional complexity and uncertainty into the training\nprocess. To overcome these limitations, we propose \\textbf{Trajectory-Distilled\nGFlowNet (TD-GFN)}, a \\emph{proxy-free} training framework that eliminates the\nneed for out-of-dataset reward queries. Our method is motivated by the key\nobservation that different edges in the associated directed acyclic graph (DAG)\ncontribute unequally to effective policy learning. TD-GFN leverages inverse\nreinforcement learning to estimate edge-level rewards from the offline dataset,\nwhich are then used to ingeniously prune the DAG and guide backward trajectory\nsampling during training. This approach directs the policy toward high-reward\nregions while reducing the complexity of model fitting. Empirical results\nacross multiple tasks show that TD-GFN trains both efficiently and reliably,\nsignificantly outperforming existing baselines in convergence speed and sample\nquality.",
      "pdf_url": "http://arxiv.org/pdf/2505.20110v1",
      "published": "2025-05-26T15:12:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20110v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models",
      "authors": [
        "June-Woo Kim",
        "Wonkyo Oh",
        "Haram Yoon",
        "Sung-Hoon Yoon",
        "Dae-Jin Kim",
        "Dong-Ho Lee",
        "Sang-Yeol Lee",
        "Chan-Mo Yang"
      ],
      "abstract": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment.",
      "pdf_url": "http://arxiv.org/pdf/2505.20109v1",
      "published": "2025-05-26T15:12:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20109v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models",
      "authors": [
        "Fengyuan Sun",
        "Leqi Shen",
        "Hui Chen",
        "Sicheng Zhao",
        "Jungong Han",
        "Guiguang Ding"
      ],
      "abstract": "Video Large Language Models (Video LLMs) have achieved remarkable results in\nvideo understanding tasks. However, they often suffer from heavy computational\noverhead due to the large number of visual tokens generated from multiple video\nframes. Existing visual token compression methods often rely on attention\nscores from language models as guidance. However, these scores exhibit inherent\nbiases: global bias reflects a tendency to focus on the two ends of the visual\ntoken sequence, while local bias leads to an over-concentration on the same\nspatial positions across different frames. To address the issue of attention\nbias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed\n$\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models\n($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP\nintegrates two dedicated debiasing modules into the pipeline, targeting global\nattention bias and local attention bias, respectively. Without the need for\nadditional training, our method significantly reduces the computational\noverhead of Video LLMs while retaining the performance of vanilla models.\nExtensive evaluation shows that AdaTP achieves state-of-the-art performance in\nvarious commonly used video understanding benchmarks. In particular, on\nLLaVA-OneVision-7B, AdaTP maintains performance without degradation while using\nonly up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be\nreleased soon.",
      "pdf_url": "http://arxiv.org/pdf/2505.20100v1",
      "published": "2025-05-26T15:08:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20100v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities",
      "authors": [
        "Chuangtao Ma",
        "Yongrui Chen",
        "Tianxing Wu",
        "Arijit Khan",
        "Haofen Wang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.",
      "pdf_url": "http://arxiv.org/pdf/2505.20099v1",
      "published": "2025-05-26T15:08:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20099v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning",
      "authors": [
        "Thang Nguyen",
        "Peter Chin",
        "Yu-Wing Tai"
      ],
      "abstract": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.",
      "pdf_url": "http://arxiv.org/pdf/2505.20096v1",
      "published": "2025-05-26T15:05:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20096v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale",
      "authors": [
        "Qi Li",
        "Kun Li",
        "Haozhi Han",
        "Honghui Shang",
        "Xinfu He",
        "Yunquan Zhang",
        "Hong An",
        "Ting Cao",
        "Mao Yang"
      ],
      "abstract": "Can a scientific simulation system be physically consistent, interpretable by\ndesign, and scalable across regimes--all at once? Despite decades of progress,\nthis trifecta remains elusive. Classical methods like Kinetic Monte Carlo\nensure thermodynamic accuracy but scale poorly; learning-based methods offer\nefficiency but often sacrifice physical consistency and interpretability. We\npresent SwarmThinkers, a reinforcement learning framework that recasts\natomic-scale simulation as a physically grounded swarm intelligence system.\nEach diffusing particle is modeled as a local decision-making agent that\nselects transitions via a shared policy network trained under thermodynamic\nconstraints. A reweighting mechanism fuses learned preferences with transition\nrates, preserving statistical fidelity while enabling interpretable, step-wise\ndecision making. Training follows a centralized-training,\ndecentralized-execution paradigm, allowing the policy to generalize across\nsystem sizes, concentrations, and temperatures without retraining. On a\nbenchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers\nis the first system to achieve full-scale, physically consistent simulation on\na single A100 GPU, previously attainable only via OpenKMC on a supercomputer.\nIt delivers up to 4963x (3185x on average) faster computation with 485x lower\nmemory usage. By treating particles as decision-makers, not passive samplers,\nSwarmThinkers marks a paradigm shift in scientific simulation--one that unifies\nphysical consistency, interpretability, and scalability through agent-driven\nintelligence.",
      "pdf_url": "http://arxiv.org/pdf/2505.20094v1",
      "published": "2025-05-26T15:04:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20094v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Homophily Enhanced Graph Domain Adaptation",
      "authors": [
        "Ruiyi Fang",
        "Bingheng Li",
        "Jingyu Zhao",
        "Ruizhi Pu",
        "Qiuhao Zeng",
        "Gezheng Xu",
        "Charles Ling",
        "Boyu Wang"
      ],
      "abstract": "Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs\nto unlabeled target graphs, addressing the challenge of label scarcity. In this\npaper, we highlight the significance of graph homophily, a pivotal factor for\ngraph domain alignment, which, however, has long been overlooked in existing\napproaches. Specifically, our analysis first reveals that homophily\ndiscrepancies exist in benchmarks. Moreover, we also show that homophily\ndiscrepancies degrade GDA performance from both empirical and theoretical\naspects, which further underscores the importance of homophily alignment in\nGDA. Inspired by this finding, we propose a novel homophily alignment algorithm\nthat employs mixed filters to smooth graph signals, thereby effectively\ncapturing and mitigating homophily discrepancies between graphs. Experimental\nresults on a variety of benchmarks verify the effectiveness of our method.",
      "pdf_url": "http://arxiv.org/pdf/2505.20089v2",
      "published": "2025-05-26T15:02:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20089v2",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models",
      "authors": [
        "Makesh Narsimhan Sreedhar",
        "Traian Rebedea",
        "Christopher Parisien"
      ],
      "abstract": "Reasoning-based language models have demonstrated strong performance across\nvarious domains, with the most notable gains seen in mathematical and coding\ntasks. Recent research has shown that reasoning also offers significant\nbenefits for LLM safety and guardrail applications. In this work, we conduct a\ncomprehensive analysis of training reasoning-based guardrail models for content\nmoderation, with an emphasis on generalization to custom safety policies at\ninference time. Our study focuses on two key dimensions: data efficiency and\ninference efficiency. On the data front, we find that reasoning-based models\nexhibit strong sample efficiency, achieving competitive performance with\nsignificantly fewer training examples than their non-reasoning counterparts.\nThis unlocks the potential to repurpose the remaining data for mining\nhigh-value, difficult samples that further enhance model performance. On the\ninference side, we evaluate practical trade-offs by introducing reasoning\nbudgets, examining the impact of reasoning length on latency and accuracy, and\nexploring dual-mode training to allow runtime control over reasoning behavior.\nOur findings will provide practical insights for researchers and developers to\neffectively and efficiently train and deploy reasoning-based guardrails models\nin real-world systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.20087v1",
      "published": "2025-05-26T15:01:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20087v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Explanation User Interfaces: A Systematic Literature Review",
      "authors": [
        "Eleonora Cappuccio",
        "Andrea Esposito",
        "Francesco Greco",
        "Giuseppe Desolda",
        "Rosa Lanzilotti",
        "Salvatore Rinzivillo"
      ],
      "abstract": "Artificial Intelligence (AI) is one of the major technological advancements\nof this century, bearing incredible potential for users through AI-powered\napplications and tools in numerous domains. Being often black-box (i.e., its\ndecision-making process is unintelligible), developers typically resort to\neXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour\nof AI models to produce systems that are transparent, fair, reliable, and\ntrustworthy. However, presenting explanations to the user is not trivial and is\noften left as a secondary aspect of the system's design process, leading to AI\nsystems that are not useful to end-users. This paper presents a Systematic\nLiterature Review on Explanation User Interfaces (XUIs) to gain a deeper\nunderstanding of the solutions and design guidelines employed in the academic\nliterature to effectively present explanations to users. To improve the\ncontribution and real-world impact of this survey, we also present a framework\nfor Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide\npractitioners and academics in the design and evaluation of XUIs.",
      "pdf_url": "http://arxiv.org/pdf/2505.20085v1",
      "published": "2025-05-26T15:00:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20085v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "A.1"
      ]
    },
    {
      "title": "Inference-time Alignment in Continuous Space",
      "authors": [
        "Yige Yuan",
        "Teng Xiao",
        "Li Yunfan",
        "Bingbing Xu",
        "Shuchang Tao",
        "Yunqi Qiu",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA",
      "pdf_url": "http://arxiv.org/pdf/2505.20081v1",
      "published": "2025-05-26T14:58:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20081v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback",
      "authors": [
        "Mengdi Li",
        "Jiaye Lin",
        "Xufeng Zhao",
        "Wenhao Lu",
        "Peilin Zhao",
        "Stefan Wermter",
        "Di Wang"
      ],
      "abstract": "Reward models trained with conventional Reinforcement Learning from AI\nFeedback (RLAIF) methods suffer from limited generalizability, which hinders\nthe alignment performance of the policy model during reinforcement learning\n(RL). This challenge stems from various issues, including distribution shift,\npreference label noise, and mismatches between overly challenging samples and\nmodel capacity. In this paper, we attempt to enhance the generalizability of\nreward models through a data-centric approach, driven by the insight that these\nissues are inherently intertwined from the perspective of data difficulty. To\naddress this, we propose a novel framework, $\\textit{Curriculum-RLAIF}$, which\nconstructs preference pairs with varying difficulty levels and produces a\ncurriculum that progressively incorporates preference pairs of increasing\ndifficulty for reward model training. Our experimental results suggest that\nreward models trained with Curriculum-RLAIF achieve improved generalizability,\nsignificantly increasing the alignment performance of the policy model by a\nlarge margin without incurring additional inference costs compared to various\nnon-curriculum baselines. Detailed analysis and comparisons with alternative\napproaches, including data selection via external pretrained reward models or\ninternal self-selection mechanisms, as well as other curriculum strategies,\nfurther demonstrate the superiority of our approach in terms of simplicity,\nefficiency, and effectiveness.",
      "pdf_url": "http://arxiv.org/pdf/2505.20075v1",
      "published": "2025-05-26T14:53:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20075v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Incentivizing Reasoning from Weak Supervision",
      "authors": [
        "Yige Yuan",
        "Teng Xiao",
        "Shuchang Tao",
        "Xue Wang",
        "Jinyang Gao",
        "Bolin Ding",
        "Bingbing Xu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR.",
      "pdf_url": "http://arxiv.org/pdf/2505.20072v1",
      "published": "2025-05-26T14:51:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20072v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction",
      "authors": [
        "Qingyu Liang",
        "Jaime Banks"
      ],
      "abstract": "Shared understanding plays a key role in the effective communication in and\nperformance of human-human interactions. With the increasingly common\nintegration of AI into human contexts, the future of personal and workplace\ninteractions will likely see human-AI interaction (HAII) in which the\nperception of shared understanding is important. Existing literature has\naddressed the processes and effects of PSU in human-human interactions, but the\nconstrual remains underexplored in HAII. To better understand PSU in HAII, we\nconducted an online survey to collect user reflections on interactions with a\nlarge language model when it sunderstanding of a situation was thought to be\nsimilar to or different from the participant's. Through inductive thematic\nanalysis, we identified eight dimensions comprising PSU in human-AI\ninteractions: Fluency, aligned operation, fluidity, outcome satisfaction,\ncontextual awareness, lack of humanlike abilities, computational limits, and\nsuspicion.",
      "pdf_url": "http://arxiv.org/pdf/2505.20068v1",
      "published": "2025-05-26T14:50:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20068v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Community Moderation and the New Epistemology of Fact Checking on Social Media",
      "authors": [
        "Isabelle Augenstein",
        "Michiel Bakker",
        "Tanmoy Chakraborty",
        "David Corney",
        "Emilio Ferrara",
        "Iryna Gurevych",
        "Scott Hale",
        "Eduard Hovy",
        "Heng Ji",
        "Irene Larraz",
        "Filippo Menczer",
        "Preslav Nakov",
        "Paolo Papotti",
        "Dhruv Sahnan",
        "Greta Warren",
        "Giovanni Zagni"
      ],
      "abstract": "Social media platforms have traditionally relied on internal moderation teams\nand partnerships with independent fact-checking organizations to identify and\nflag misleading content. Recently, however, platforms including X (formerly\nTwitter) and Meta have shifted towards community-driven content moderation by\nlaunching their own versions of crowd-sourced fact-checking -- Community Notes.\nIf effectively scaled and governed, such crowd-checking initiatives have the\npotential to combat misinformation with increased scale and speed as\nsuccessfully as community-driven efforts once did with spam. Nevertheless,\ngeneral content moderation, especially for misinformation, is inherently more\ncomplex. Public perceptions of truth are often shaped by personal biases,\npolitical leanings, and cultural contexts, complicating consensus on what\nconstitutes misleading content. This suggests that community efforts, while\nvaluable, cannot replace the indispensable role of professional fact-checkers.\nHere we systemically examine the current approaches to misinformation detection\nacross major platforms, explore the emerging role of community-driven\nmoderation, and critically evaluate both the promises and challenges of\ncrowd-checking at scale.",
      "pdf_url": "http://arxiv.org/pdf/2505.20067v1",
      "published": "2025-05-26T14:50:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20067v1",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Automated data curation for self-supervised learning in underwater acoustic analysis",
      "authors": [
        "Hilde I Hummel",
        "Sandjai Bhulai",
        "Burooj Ghani",
        "Rob van der Mei"
      ],
      "abstract": "The sustainability of the ocean ecosystem is threatened by increased levels\nof sound pollution, making monitoring crucial to understand its variability and\nimpact. Passive acoustic monitoring (PAM) systems collect a large amount of\nunderwater sound recordings, but the large volume of data makes manual analysis\nimpossible, creating the need for automation. Although machine learning offers\na potential solution, most underwater acoustic recordings are unlabeled.\nSelf-supervised learning models have demonstrated success in learning from\nlarge-scale unlabeled data in various domains like computer vision, Natural\nLanguage Processing, and audio. However, these models require large, diverse,\nand balanced datasets for training in order to generalize well. To address\nthis, a fully automated self-supervised data curation pipeline is proposed to\ncreate a diverse and balanced dataset from raw PAM data. It integrates\nAutomatic Identification System (AIS) data with recordings from various\nhydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw\naudio data is sampled and then combined with AIS samples to create a balanced\nand diverse dataset. The resulting curated dataset enables the development of\nself-supervised learning models, facilitating various tasks such as monitoring\nmarine mammals and assessing sound pollution.",
      "pdf_url": "http://arxiv.org/pdf/2505.20066v1",
      "published": "2025-05-26T14:50:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20066v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety",
      "authors": [
        "Geon-Hyeong Kim",
        "Youngsoo Jang",
        "Yu Jin Kim",
        "Byoungjip Kim",
        "Honglak Lee",
        "Kyunghoon Bae",
        "Moontae Lee"
      ],
      "abstract": "As Large Language Models (LLMs) continue to advance and find applications\nacross a growing number of fields, ensuring the safety of LLMs has become\nincreasingly critical. To address safety concerns, recent studies have proposed\nintegrating safety constraints into Reinforcement Learning from Human Feedback\n(RLHF). However, these approaches tend to be complex, as they encompass\ncomplicated procedures in RLHF along with additional steps required by the\nsafety constraints. Inspired by Direct Preference Optimization (DPO), we\nintroduce a new algorithm called SafeDPO, which is designed to directly\noptimize the safety alignment objective in a single stage of policy learning,\nwithout requiring relaxation. SafeDPO introduces only one additional\nhyperparameter to further enhance safety and requires only minor modifications\nto standard DPO. As a result, it eliminates the need to fit separate reward and\ncost models or to sample from the language model during fine-tuning, while\nstill enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO\nachieves competitive performance compared to state-of-the-art safety alignment\nalgorithms, both in terms of aligning with human preferences and improving\nsafety.",
      "pdf_url": "http://arxiv.org/pdf/2505.20065v1",
      "published": "2025-05-26T14:50:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20065v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SAEs Are Good for Steering -- If You Select the Right Features",
      "authors": [
        "Dana Arad",
        "Aaron Mueller",
        "Yonatan Belinkov"
      ],
      "abstract": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to\nlearn a decomposition of a model's latent space. This enables useful\napplications such as steering - influencing the output of a model towards a\ndesired concept - without requiring labeled data. Current methods identify SAE\nfeatures to steer by analyzing the input tokens that activate them. However,\nrecent work has highlighted that activations alone do not fully describe the\neffect of a feature on the model's output. In this work, we draw a distinction\nbetween two types of features: input features, which mainly capture patterns in\nthe model's input, and output features, which have a human-understandable\neffect on the model's output. We propose input and output scores to\ncharacterize and locate these types of features, and show that high values for\nboth scores rarely co-occur in the same features. These findings have practical\nimplications: after filtering out features with low output scores, we obtain\n2-3x improvements when steering with SAEs, making them competitive with\nsupervised methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.20063v1",
      "published": "2025-05-26T14:47:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20063v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion",
      "authors": [
        "Zheqi Lv",
        "Junhao Chen",
        "Qi Tian",
        "Keting Yin",
        "Shengyu Zhang",
        "Fei Wu"
      ],
      "abstract": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements.",
      "pdf_url": "http://arxiv.org/pdf/2505.20053v1",
      "published": "2025-05-26T14:42:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20053v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ]
    },
    {
      "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks",
      "authors": [
        "Debargha Ganguly",
        "Vikash Singh",
        "Sreehari Sankar",
        "Biyao Zhang",
        "Xuecen Zhang",
        "Srinivasan Iyengar",
        "Xiaotian Han",
        "Amit Sharma",
        "Shivkumar Kalyanaraman",
        "Vipin Chaudhary"
      ],
      "abstract": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline.",
      "pdf_url": "http://arxiv.org/pdf/2505.20047v1",
      "published": "2025-05-26T14:34:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20047v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LO",
        "cs.SE"
      ]
    },
    {
      "title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition",
      "authors": [
        "Christoph Schuhmann",
        "Robert Kaczmarczyk",
        "Gollam Rabby",
        "Felix Friedrich",
        "Maurice Kraus",
        "Krishna Kalyan",
        "Kourosh Nadi",
        "Huu Nguyen",
        "Kristian Kersting",
        "Sören Auer"
      ],
      "abstract": "Effective human-AI interaction relies on AI's ability to accurately perceive\nand interpret human emotions. Current benchmarks for vision and vision-language\nmodels are severely limited, offering a narrow emotional spectrum that\noverlooks nuanced states (e.g., bitterness, intoxication) and fails to\ndistinguish subtle differences between related feelings (e.g., shame vs.\nembarrassment). Existing datasets also often use uncontrolled imagery with\noccluded faces and lack demographic diversity, risking significant bias. To\naddress these critical gaps, we introduce EmoNet Face, a comprehensive\nbenchmark suite. EmoNet Face features: (1) A novel 40-category emotion\ntaxonomy, meticulously derived from foundational research to capture finer\ndetails of human emotional experiences. (2) Three large-scale, AI-generated\ndatasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and\ncontrolled demographic balance across ethnicity, age, and gender. (3) Rigorous,\nmulti-expert annotations for training and high-fidelity evaluation. (4) We\nbuilt EmpathicInsight-Face, a model achieving human-expert-level performance on\nour benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,\nand model - provides a robust foundation for developing and evaluating AI\nsystems with a deeper understanding of human emotions.",
      "pdf_url": "http://arxiv.org/pdf/2505.20033v2",
      "published": "2025-05-26T14:19:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20033v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions",
      "authors": [
        "Wenbo Wei",
        "Nicholas Chong Jia Le",
        "Choy Heng Lai",
        "Ling Feng"
      ],
      "abstract": "We observe a novel 'multiple-descent' phenomenon during the training process\nof LSTM, in which the test loss goes through long cycles of up and down trend\nmultiple times after the model is overtrained. By carrying out asymptotic\nstability analysis of the models, we found that the cycles in test loss are\nclosely associated with the phase transition process between order and chaos,\nand the local optimal epochs are consistently at the critical transition point\nbetween the two phases. More importantly, the global optimal epoch occurs at\nthe first transition from order to chaos, where the 'width' of the 'edge of\nchaos' is the widest, allowing the best exploration of better weight\nconfigurations for learning.",
      "pdf_url": "http://arxiv.org/pdf/2505.20030v1",
      "published": "2025-05-26T14:18:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20030v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "nlin.CD",
        "physics.comp-ph"
      ]
    },
    {
      "title": "Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)",
      "authors": [
        "Subba Reddy Oota",
        "Akshett Jindal",
        "Ishani Mondal",
        "Khushbu Pahwa",
        "Satya Sai Srinath Namburi",
        "Manish Shrivastava",
        "Maneesh Singh",
        "Bapi S. Raju",
        "Manish Gupta"
      ],
      "abstract": "Transformer-based language models, though not explicitly trained to mimic\nbrain recordings, have demonstrated surprising alignment with brain activity.\nProgress in these models-through increased size, instruction-tuning, and\nmultimodality-has led to better representational alignment with neural data.\nRecently, a new class of instruction-tuned multimodal LLMs (MLLMs) have\nemerged, showing remarkable zero-shot capabilities in open-ended multimodal\nvision tasks. However, it is unknown whether MLLMs, when prompted with natural\ninstructions, lead to better brain alignment and effectively capture\ninstruction-specific representations. To address this, we first investigate\nbrain alignment, i.e., measuring the degree of predictivity of neural visual\nactivity using text output response embeddings from MLLMs as participants\nengage in watching natural scenes. Experiments with 10 different instructions\nshow that MLLMs exhibit significantly better brain alignment than vision-only\nmodels and perform comparably to non-instruction-tuned multimodal models like\nCLIP. We also find that while these MLLMs are effective at generating\nhigh-quality responses suitable to the task-specific instructions, not all\ninstructions are relevant for brain alignment. Further, by varying\ninstructions, we make the MLLMs encode instruction-specific visual concepts\nrelated to the input image. This analysis shows that MLLMs effectively capture\ncount-related and recognition-related concepts, demonstrating strong alignment\nwith brain activity. Notably, the majority of the explained variance of the\nbrain encoding models is shared between MLLM embeddings of image captioning and\nother instructions. These results suggest that enhancing MLLMs' ability to\ncapture task-specific information could lead to better differentiation between\nvarious types of instructions, and thereby improving their precision in\npredicting brain responses.",
      "pdf_url": "http://arxiv.org/pdf/2505.20029v1",
      "published": "2025-05-26T14:18:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20029v1",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Multi-modal brain encoding models for multi-modal stimuli",
      "authors": [
        "Subba Reddy Oota",
        "Khushbu Pahwa",
        "Mounika Marreddy",
        "Maneesh Singh",
        "Manish Gupta",
        "Bapi S. Raju"
      ],
      "abstract": "Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain.",
      "pdf_url": "http://arxiv.org/pdf/2505.20027v1",
      "published": "2025-05-26T14:17:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20027v1",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS",
        "eess.IV"
      ]
    },
    {
      "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage",
      "authors": [
        "Xinping Chen",
        "Chen Liu"
      ],
      "abstract": "We propose Gradient Inversion Transcript (GIT), a novel generative approach\nfor reconstructing training data from leaked gradients. GIT employs a\ngenerative attack model, whose architecture is tailored to align with the\nstructure of the leaked model based on theoretical analysis. Once trained\noffline, GIT can be deployed efficiently and only relies on the leaked\ngradients to reconstruct the input data, rendering it applicable under various\ndistributed learning environments. When used as a prior for other iterative\noptimization-based methods, GIT not only accelerates convergence but also\nenhances the overall reconstruction quality. GIT consistently outperforms\nexisting methods across multiple datasets and demonstrates strong robustness\nunder challenging conditions, including inaccurate gradients, data distribution\nshifts and discrepancies in model parameters.",
      "pdf_url": "http://arxiv.org/pdf/2505.20026v1",
      "published": "2025-05-26T14:17:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20026v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving",
      "authors": [
        "Xueyi Liu",
        "Zuodong Zhong",
        "Yuxin Guo",
        "Yun-Fu Liu",
        "Zhiguo Su",
        "Qichao Zhang",
        "Junli Wang",
        "Yinfeng Gao",
        "Yupeng Zheng",
        "Qiao Lin",
        "Huiyong Chen",
        "Dongbin Zhao"
      ],
      "abstract": "Due to the powerful vision-language reasoning and generalization abilities,\nmultimodal large language models (MLLMs) have garnered significant attention in\nthe field of end-to-end (E2E) autonomous driving. However, their application to\nclosed-loop systems remains underexplored, and current MLLM-based methods have\nnot shown clear superiority to mainstream E2E imitation learning approaches. In\nthis work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed\nfor closed-loop driving through holistic reasoning with a self-supervised Next\nScene Prediction task and supervised Decision Chain-of-Thought process. This\ndual mechanism encourages the model to align visual representations with\nactionable driving context, while promoting interpretable and causally grounded\ndecision making. We curate a planning-oriented decision reasoning dataset,\nnamely PDR, comprising 210k diverse and high-quality samples. Our method\noutperforms the mainstream E2E imitation learning method by a large margin of\n19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan\ndemonstrates strong zero-shot generalization on unseen DOS benchmark,\nhighlighting its adaptability in handling zero-shot corner cases. Code and\ndataset will be found in https://github.com/Liuxueyi/ReasonPlan.",
      "pdf_url": "http://arxiv.org/pdf/2505.20024v1",
      "published": "2025-05-26T14:12:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20024v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO",
        "68T40(Primary), 68T45, 68T50(Secondary)",
        "I.2.9; I.2.10; I.5.1"
      ]
    },
    {
      "title": "Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models",
      "authors": [
        "Hyunsik Chae",
        "Seungwoo Yoon",
        "Jaden Park",
        "Chloe Yewon Chun",
        "Yongin Cho",
        "Mu Cai",
        "Yong Jae Lee",
        "Ernest K. Ryu"
      ],
      "abstract": "Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal\ncomprehension and reasoning capabilities, yet they often struggle with\ntrivially simple visual tasks. In this work, we focus on the domain of basic 2D\nEuclidean geometry and systematically categorize the fundamental, indivisible\nvisual perception skills, which we refer to as atomic visual skills. We then\nintroduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the\natomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find\nthat they struggle with these tasks, despite being trivial for adult humans.\nOur findings highlight the need for purpose-built datasets to train and\nevaluate VLMs on atomic, rather than composite, visual perception tasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.20021v1",
      "published": "2025-05-26T14:09:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20021v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments",
      "authors": [
        "Maciej Świechowski",
        "Dominik Ślęzak"
      ],
      "abstract": "Human-like agents are an increasingly important topic in games and beyond.\nBelievable non-player characters enhance the gaming experience by improving\nimmersion and providing entertainment. They also offer players the opportunity\nto engage with AI entities that can function as opponents, teachers, or\ncooperating partners. Additionally, in games where bots are prohibited -- and\neven more so in non-game environments -- there is a need for methods capable of\nidentifying whether digital interactions occur with bots or humans. This leads\nto two fundamental research questions: (1) how to model and implement\nhuman-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most\nsignificant challenges in implementing human-like AI in games (or any virtual\nenvironment featuring simulated agents, although this article specifically\nfocuses on games). Thirteen such challenges, both conceptual and technical, are\ndiscussed in detail. The second is an empirical study performed in a tactical\nvideo game that addresses the research question: \"Is it possible to distinguish\nhuman players from bots (AI agents) based on empirical data?\" A\nmachine-learning approach using a custom deep recurrent convolutional neural\nnetwork is presented. We hypothesize that the more challenging it is to create\nhuman-like AI for a given game, the easier it becomes to develop a method for\ndistinguishing humans from AI-driven players.",
      "pdf_url": "http://arxiv.org/pdf/2505.20011v1",
      "published": "2025-05-26T14:00:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.20011v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MM",
        "68T01",
        "I.2; I.6.0; H.1.2"
      ]
    },
    {
      "title": "ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications",
      "authors": [
        "Tong Wu",
        "Zhiyong Chen",
        "Dazhi He",
        "Feng Yang",
        "Meixia Tao",
        "Xiaodong Xu",
        "Wenjun Zhang",
        "Ping Zhang"
      ],
      "abstract": "Diffusion models (DMs) have recently achieved significant success in wireless\ncommunications systems due to their denoising capabilities. The broadcast\nnature of wireless signals makes them susceptible not only to Gaussian noise,\nbut also to unaware interference. This raises the question of whether DMs can\neffectively mitigate interference in wireless semantic communication systems.\nIn this paper, we model the interference cancellation problem as a maximum a\nposteriori (MAP) problem over the joint posterior probability of the signal and\ninterference, and theoretically prove that the solution provides excellent\nestimates for the signal and interference. To solve this problem, we develop an\ninterference cancellation diffusion model (ICDM), which decomposes the joint\nposterior into independent prior probabilities of the signal and interference,\nalong with the channel transition probablity. The log-gradients of these\ndistributions at each time step are learned separately by DMs and accurately\nestimated through deriving. ICDM further integrates these gradients with\nadvanced numerical iteration method, achieving accurate and rapid interference\ncancellation. Extensive experiments demonstrate that ICDM significantly reduces\nthe mean square error (MSE) and enhances perceptual quality compared to schemes\nwithout ICDM. For example, on the CelebA dataset under the Rayleigh fading\nchannel with a signal-to-noise ratio (SNR) of $20$ dB and signal to\ninterference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB\nand improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.",
      "pdf_url": "http://arxiv.org/pdf/2505.19983v1",
      "published": "2025-05-26T13:41:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19983v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.CV",
        "math.IT"
      ]
    },
    {
      "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response",
      "authors": [
        "Bilel Cherif",
        "Tamas Bisztray",
        "Richard A. Dubniczky",
        "Aaesha Aldahmani",
        "Saeed Alshehhi",
        "Norbert Tihanyi"
      ],
      "abstract": "Digital Forensics and Incident Response (DFIR) involves analyzing digital\nevidence to support legal investigations. Large Language Models (LLMs) offer\nnew opportunities in DFIR tasks such as log analysis and memory forensics, but\ntheir susceptibility to errors and hallucinations raises concerns in\nhigh-stakes contexts. Despite growing interest, there is no comprehensive\nbenchmark to evaluate LLMs across both theoretical and practical DFIR domains.\nTo address this gap, we present DFIR-Metric, a benchmark with three components:\n(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice\nquestions sourced from industry-standard certifications and official\ndocumentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing\nmulti-step reasoning and evidence correlation; and (3) Practical Analysis: 500\ndisk and memory forensics cases from the NIST Computer Forensics Tool Testing\nProgram (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their\naccuracy and consistency across trials. We also introduce a new metric, the\nTask Understanding Score (TUS), designed to more effectively evaluate models in\nscenarios where they achieve near-zero accuracy. This benchmark offers a\nrigorous, reproducible foundation for advancing AI in digital forensics. All\nscripts, artifacts, and results are available on the project website at\nhttps://github.com/DFIR-Metric.",
      "pdf_url": "http://arxiv.org/pdf/2505.19973v1",
      "published": "2025-05-26T13:35:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19973v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Learning to Select In-Context Demonstration Preferred by Large Language Model",
      "authors": [
        "Zheng Zhang",
        "Shaocheng Lan",
        "Lei Song",
        "Jiang Bian",
        "Yexin Li",
        "Kan Ren"
      ],
      "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks during inference using only a few demonstrations. However, ICL\nperformance is highly dependent on the selection of these demonstrations.\nRecent work explores retrieval-based methods for selecting query-specific\ndemonstrations, but these approaches often rely on surrogate objectives such as\nmetric learning, failing to directly optimize ICL performance. Consequently,\nthey struggle to identify truly beneficial demonstrations. Moreover, their\ndiscriminative retrieval paradigm is ineffective when the candidate pool lacks\nsufficient high-quality demonstrations. To address these challenges, we propose\nGenICL, a novel generative preference learning framework that leverages LLM\nfeedback to directly optimize demonstration selection for ICL. Experiments on\n19 datasets across 11 task categories demonstrate that GenICL achieves superior\nperformance than existing methods in selecting the most effective\ndemonstrations, leading to better ICL performance.",
      "pdf_url": "http://arxiv.org/pdf/2505.19966v1",
      "published": "2025-05-26T13:26:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19966v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction",
      "authors": [
        "Yu Wang",
        "Junshu Dai",
        "Yuchen Ying",
        "Yuxuan Liang",
        "Tongya Zheng",
        "Mingli Song"
      ],
      "abstract": "Human mobility prediction is crucial for applications ranging from\nlocation-based recommendations to urban planning, which aims to forecast users'\nnext location visits based on historical trajectories. Despite the severe\nlong-tailed distribution of locations, the problem of long-tailed mobility\nprediction remains largely underexplored. Existing long-tailed learning methods\nprimarily focus on rebalancing the skewed distribution at the data, model, or\nclass level, neglecting to exploit the spatiotemporal semantics of locations.\nTo address this gap, we propose the first plug-and-play framework for\nlong-tailed mobility prediction in an exploitation and exploration manner,\nnamed \\textbf{A}daptive \\textbf{LO}cation \\textbf{H}ier\\textbf{A}rchy learning\n(ALOHA). First, we construct city-tailored location hierarchy based on Large\nLanguage Models (LLMs) by exploiting Maslow's theory of human motivation to\ndesign Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics.\nSecond, we optimize the location hierarchy predictions by Gumbel disturbance\nand node-wise adaptive weights within the hierarchical tree structure.\nExperiments on state-of-the-art models across six datasets demonstrate the\nframework's consistent effectiveness and generalizability, which strikes a well\nbalance between head and tail locations. Weight analysis and ablation studies\nreveal the optimization differences of each component for head and tail\nlocations. Furthermore, in-depth analyses of hierarchical distance and case\nstudy demonstrate the effective semantic guidance from the location hierarchy.\nOur code will be made publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2505.19965v1",
      "published": "2025-05-26T13:26:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19965v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Limits of Preference Data for Post-Training",
      "authors": [
        "Eric Zhao",
        "Jessica Dai",
        "Pranjal Awasthi"
      ],
      "abstract": "Recent progress in strengthening the capabilities of large language models\nhas stemmed from applying reinforcement learning to domains with automatically\nverifiable outcomes. A key question is whether we can similarly use RL to\noptimize for outcomes in domains where evaluating outcomes inherently requires\nhuman feedback; for example, in tasks like deep research and trip planning,\noutcome evaluation is qualitative and there are many possible degrees of\nsuccess. One attractive and scalable modality for collecting human feedback is\npreference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$\ngiven outcomes, which one is preferred. In this work, we study a critical\nroadblock: preference data fundamentally and significantly limits outcome-based\noptimization. Even with idealized preference data (infinite, noiseless, and\nonline), the use of ordinal feedback can prevent obtaining even approximately\noptimal solutions. We formalize this impossibility using voting theory, drawing\nan analogy between how a model chooses to answer a query with how voters choose\na candidate to elect. This indicates that grounded human scoring and\nalgorithmic innovations are necessary for extending the success of RL\npost-training to domains demanding human feedback. We also explore why these\nlimitations have disproportionately impacted RLHF when it comes to eliciting\nreasoning behaviors (e.g., backtracking) versus situations where RLHF has been\nhistorically successful (e.g., instruction-tuning and safety training), finding\nthat the limitations of preference data primarily suppress RLHF's ability to\nelicit robust strategies -- a class that encompasses most reasoning behaviors.",
      "pdf_url": "http://arxiv.org/pdf/2505.19964v1",
      "published": "2025-05-26T13:26:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19964v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.GT"
      ]
    },
    {
      "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph",
      "authors": [
        "Jihyung Lee",
        "Jin-Seop Lee",
        "Jaehoon Lee",
        "YunSeok Choi",
        "Jee-Hyong Lee"
      ],
      "abstract": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. Our code will be\nreleased.",
      "pdf_url": "http://arxiv.org/pdf/2505.19956v1",
      "published": "2025-05-26T13:19:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19956v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
      "authors": [
        "Hui Chen",
        "Miao Xiong",
        "Yujie Lu",
        "Wei Han",
        "Ailin Deng",
        "Yufei He",
        "Jiaying Wu",
        "Yibo Li",
        "Yue Liu",
        "Bryan Hooi"
      ],
      "abstract": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
      "pdf_url": "http://arxiv.org/pdf/2505.19955v1",
      "published": "2025-05-26T13:18:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19955v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy",
      "authors": [
        "Elvir Karimov",
        "Alexander Varlamov",
        "Danil Ivanov",
        "Dmitrii Korzh",
        "Oleg Y. Rogov"
      ],
      "abstract": "Deep learning voice models are commonly used nowadays, but the safety\nprocessing of personal data, such as human identity and speech content, remains\nsuspicious. To prevent malicious user identification, speaker anonymization\nmethods were proposed. Current methods, particularly based on universal\nadversarial patch (UAP) applications, have drawbacks such as significant\ndegradation of audio quality, decreased speech recognition quality, low\ntransferability across different voice biometrics models, and performance\ndependence on the input audio length. To mitigate these drawbacks, in this\nwork, we introduce and leverage the novel Exponential Total Variance (TV) loss\nfunction and provide experimental evidence that it positively affects UAP\nstrength and imperceptibility. Moreover, we present a novel scalable UAP\ninsertion procedure and demonstrate its uniformly high performance for various\naudio lengths.",
      "pdf_url": "http://arxiv.org/pdf/2505.19951v1",
      "published": "2025-05-26T13:16:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19951v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "eess.AS"
      ]
    },
    {
      "title": "SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection",
      "authors": [
        "Gokul Adethya",
        "Bhanu Pratyush Mantha",
        "Tianyang Wang",
        "Xingjian Li",
        "Min Xu"
      ],
      "abstract": "Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for\nimaging macromolecular complexes in their near-native states. However, the\nlocalization of 3D particles in cellular environments still presents a\nsignificant challenge due to low signal-to-noise ratios and missing wedge\nartifacts. Deep learning approaches have shown great potential, but they need\nhuge amounts of data, which can be a challenge in cryo-ET scenarios where\nlabeled data is often scarce. In this paper, we propose a novel Self-augmented\nand Self-interpreted (SaSi) deep learning approach towards few-shot particle\ndetection in 3D cryo-ET images. Our method builds upon self-augmentation\ntechniques to further boost data utilization and introduces a self-interpreted\nsegmentation strategy for alleviating dependency on labeled data, hence\nimproving generalization and robustness. As demonstrated by experiments\nconducted on both simulated and real-world cryo-ET datasets, the SaSi approach\nsignificantly outperforms existing state-of-the-art methods for particle\nlocalization. This research increases understanding of how to detect particles\nwith very few labels in cryo-ET and thus sets a new benchmark for few-shot\nlearning in structural biology.",
      "pdf_url": "http://arxiv.org/pdf/2505.19948v1",
      "published": "2025-05-26T13:14:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19948v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees",
      "authors": [
        "Herbert Woisetschläger",
        "Ryan Zhang",
        "Shiqiang Wang",
        "Hans-Arno Jacobsen"
      ],
      "abstract": "Open-weight LLM zoos provide access to numerous high-quality models, but\nselecting the appropriate model for specific tasks remains challenging and\nrequires technical expertise. Most users simply want factually correct, safe,\nand satisfying responses without concerning themselves with model\ntechnicalities, while inference service providers prioritize minimizing\noperating costs. These competing interests are typically mediated through\nservice level agreements (SLAs) that guarantee minimum service quality. We\nintroduce MESS+, a stochastic optimization algorithm for cost-optimal LLM\nrequest routing while providing rigorous SLA compliance guarantees. MESS+\nlearns request satisfaction probabilities of LLMs in real-time as users\ninteract with the system, based on which model selection decisions are made by\nsolving a per-request optimization problem. Our algorithm includes a novel\ncombination of virtual queues and request satisfaction prediction, along with a\ntheoretical analysis of cost optimality and constraint satisfaction. Across a\nwide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x\ncost savings compared to existing LLM routing techniques.",
      "pdf_url": "http://arxiv.org/pdf/2505.19947v1",
      "published": "2025-05-26T13:11:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19947v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "I.2; I.2.7; I.2.8"
      ]
    },
    {
      "title": "Can Visual Encoder Learn to See Arrows?",
      "authors": [
        "Naoyuki Terashita",
        "Yusuke Tozaki",
        "Hideaki Omote",
        "Congkha Nguyen",
        "Ryosuke Nakamoto",
        "Yuta Koreeda",
        "Hiroaki Ozaki"
      ],
      "abstract": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding.",
      "pdf_url": "http://arxiv.org/pdf/2505.19944v1",
      "published": "2025-05-26T13:09:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.19944v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    }
  ]
}