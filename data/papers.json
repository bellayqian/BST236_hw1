{
  "last_updated": "2025-06-30T00:57:20.515224",
  "papers": [
    {
      "title": "Whole-Body Conditioned Egocentric Video Prediction",
      "authors": [
        "Yutong Bai",
        "Danny Tran",
        "Amir Bar",
        "Yann LeCun",
        "Trevor Darrell",
        "Jitendra Malik"
      ],
      "abstract": "We train models to Predict Ego-centric Video from human Actions (PEVA), given\nthe past video and an action represented by the relative 3D body pose. By\nconditioning on kinematic pose trajectories, structured by the joint hierarchy\nof the body, our model learns to simulate how physical human actions shape the\nenvironment from a first-person point of view. We train an auto-regressive\nconditional diffusion transformer on Nymeria, a large-scale dataset of\nreal-world egocentric video and body pose capture. We further design a\nhierarchical evaluation protocol with increasingly challenging tasks, enabling\na comprehensive analysis of the model's embodied prediction and control\nabilities. Our work represents an initial attempt to tackle the challenges of\nmodeling complex real-world environments and embodied agent behaviors with\nvideo prediction from the perspective of a human.",
      "pdf_url": "http://arxiv.org/pdf/2506.21552v1",
      "published": "2025-06-26T17:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21552v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.RO"
      ]
    },
    {
      "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale",
      "authors": [
        "Xiaona Zhou",
        "Constantin Brif",
        "Ismini Lourentzou"
      ],
      "abstract": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection.",
      "pdf_url": "http://arxiv.org/pdf/2506.21550v1",
      "published": "2025-06-26T17:59:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21550v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation",
      "authors": [
        "Xinzhuo Li",
        "Adheesh Juvekar",
        "Xingyou Liu",
        "Muntasir Wahed",
        "Kiet A. Nguyen",
        "Ismini Lourentzou"
      ],
      "abstract": "Recent progress in vision-language segmentation has significantly advanced\ngrounded visual understanding. However, these models often exhibit\nhallucinations by producing segmentation masks for objects not grounded in the\nimage content or by incorrectly labeling irrelevant regions. Existing\nevaluation protocols for segmentation hallucination primarily focus on label or\ntextual hallucinations without manipulating the visual context, limiting their\ncapacity to diagnose critical failures. In response, we introduce\nHalluSegBench, the first benchmark specifically designed to evaluate\nhallucinations in visual grounding through the lens of counterfactual visual\nreasoning. Our benchmark consists of a novel dataset of 1340 counterfactual\ninstance pairs spanning 281 unique object classes, and a set of newly\nintroduced metrics that quantify hallucination sensitivity under visually\ncoherent scene edits. Experiments on HalluSegBench with state-of-the-art\nvision-language segmentation models reveal that vision-driven hallucinations\nare significantly more prevalent than label-driven ones, with models often\npersisting in false segmentation, highlighting the need for counterfactual\nreasoning to diagnose grounding fidelity.",
      "pdf_url": "http://arxiv.org/pdf/2506.21546v1",
      "published": "2025-06-26T17:59:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21546v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "WorldVLA: Towards Autoregressive Action World Model",
      "authors": [
        "Jun Cen",
        "Chaohui Yu",
        "Hangjie Yuan",
        "Yuming Jiang",
        "Siteng Huang",
        "Jiayan Guo",
        "Xin Li",
        "Yibing Song",
        "Hao Luo",
        "Fan Wang",
        "Deli Zhao",
        "Hao Chen"
      ],
      "abstract": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.",
      "pdf_url": "http://arxiv.org/pdf/2506.21539v1",
      "published": "2025-06-26T17:55:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21539v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "PsyLite Technical Report",
      "authors": [
        "Fangjun Ding",
        "Renyu Zhang",
        "Xinyu Feng",
        "Chengye Xie",
        "Zheng Zhang",
        "Yanting Zhang"
      ],
      "abstract": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments.",
      "pdf_url": "http://arxiv.org/pdf/2506.21536v1",
      "published": "2025-06-26T17:54:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21536v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets",
      "authors": [
        "Akshay Paruchuri",
        "Maryam Aziz",
        "Rohit Vartak",
        "Ayman Ali",
        "Best Uchehara",
        "Xin Liu",
        "Ishan Chatterjee",
        "Monica Agrawal"
      ],
      "abstract": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat",
      "pdf_url": "http://arxiv.org/pdf/2506.21532v1",
      "published": "2025-06-26T17:52:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21532v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Potemkin Understanding in Large Language Models",
      "authors": [
        "Marina Mancoridis",
        "Bec Weeks",
        "Keyon Vafa",
        "Sendhil Mullainathan"
      ],
      "abstract": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.",
      "pdf_url": "http://arxiv.org/pdf/2506.21521v1",
      "published": "2025-06-26T17:41:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21521v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "skLEP: A Slovak General Language Understanding Benchmark",
      "authors": [
        "Marek Šuppa",
        "Andrej Ridzik",
        "Daniel Hládek",
        "Tomáš Javůrek",
        "Viktória Ondrejová",
        "Kristína Sásiková",
        "Martin Tamajka",
        "Marián Šimko"
      ],
      "abstract": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU.",
      "pdf_url": "http://arxiv.org/pdf/2506.21508v1",
      "published": "2025-06-26T17:35:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21508v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "68T50",
        "I.2.7"
      ]
    },
    {
      "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
      "authors": [
        "Boyu Gou",
        "Zanming Huang",
        "Yuting Ning",
        "Yu Gu",
        "Michael Lin",
        "Weijian Qi",
        "Andrei Kopanev",
        "Botao Yu",
        "Bernal Jiménez Gutiérrez",
        "Yiheng Shu",
        "Chan Hee Song",
        "Jiaman Wu",
        "Shijie Chen",
        "Hanane Nour Moussa",
        "Tianshu Zhang",
        "Jian Xie",
        "Yifei Li",
        "Tianci Xue",
        "Zeyi Liao",
        "Kai Zhang",
        "Boyuan Zheng",
        "Zhaowei Cai",
        "Viktor Rozgic",
        "Morteza Ziyadi",
        "Huan Sun",
        "Yu Su"
      ],
      "abstract": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.21506v1",
      "published": "2025-06-26T17:32:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21506v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems",
      "authors": [
        "Francesco Vitale",
        "Nicola Dall'Ora",
        "Sebastiano Gaiardelli",
        "Enrico Fraccaroli",
        "Nicola Mazzocca",
        "Franco Fummi"
      ],
      "abstract": "Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring\nsystem dependability and operational efficiency by accurately detecting\nanomalies and identifying their root causes. However, the manual modeling of\nfaulty behaviors often demands extensive domain expertise and produces models\nthat are complex, error-prone, and difficult to interpret. To address this\nchallenge, we present a novel unsupervised fault diagnosis methodology that\nintegrates collective anomaly detection in multivariate time series, process\nmining, and stochastic simulation. Initially, collective anomalies are detected\nfrom low-level sensor data using multivariate time-series analysis. These\nanomalies are then transformed into structured event logs, enabling the\ndiscovery of interpretable process models through process mining. By\nincorporating timing distributions into the extracted Petri nets, the approach\nsupports stochastic simulation of faulty behaviors, thereby enhancing root\ncause analysis and behavioral understanding. The methodology is validated using\nthe Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart\nmanufacturing. Experimental results demonstrate its effectiveness in modeling,\nsimulating, and classifying faulty behaviors in CPSs. This enables the creation\nof comprehensive fault dictionaries that support predictive maintenance and the\ndevelopment of digital twins for industrial environments.",
      "pdf_url": "http://arxiv.org/pdf/2506.21502v1",
      "published": "2025-06-26T17:29:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21502v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Ad-Hoc Human-AI Coordination Challenge",
      "authors": [
        "Tin Dizdarević",
        "Ravi Hammond",
        "Tobias Gessler",
        "Anisoara Calinescu",
        "Jonathan Cook",
        "Matteo Gallici",
        "Andrei Lupu",
        "Jakob Nicolaus Foerster"
      ],
      "abstract": "Achieving seamless coordination between AI agents and humans is crucial for\nreal-world applications, yet it remains a significant open challenge. Hanabi is\na cooperative card game featuring imperfect information, constrained\ncommunication, theory of mind requirements, and coordinated action -- making it\nan ideal testbed for human-AI coordination. However, its use for human-AI\ninteraction has been limited by the challenges of human evaluation. In this\nwork, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to\novercome the constraints of costly and difficult-to-reproduce human\nevaluations. We develop \\textit{human proxy agents} on a large-scale human\ndataset that serve as robust, cheap, and reproducible human-like evaluation\npartners in AH2AC2. To encourage the development of data-efficient methods, we\nopen-source a dataset of 3,079 games, deliberately limiting the amount of\navailable human gameplay data. We present baseline results for both two- and\nthree- player Hanabi scenarios. To ensure fair evaluation, we host the proxy\nagents through a controlled evaluation system rather than releasing them\npublicly. The code is available at\n\\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.",
      "pdf_url": "http://arxiv.org/pdf/2506.21490v1",
      "published": "2025-06-26T17:19:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21490v1",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ]
    },
    {
      "title": "TITAN: Query-Token based Domain Adaptive Adversarial Learning",
      "authors": [
        "Tajamul Ashraf",
        "Janibul Bashir"
      ],
      "abstract": "We focus on the source-free domain adaptive object detection (SF-DAOD)\nproblem when source data is unavailable during adaptation and the model must\nadapt to an unlabeled target domain. The majority of approaches for the problem\nemploy a self-supervised approach using a student-teacher (ST) framework where\npseudo-labels are generated via a source-pretrained model for further\nfine-tuning. We observe that the performance of a student model often degrades\ndrastically, due to the collapse of the teacher model, primarily caused by high\nnoise in pseudo-labels, resulting from domain bias, discrepancies, and a\nsignificant domain shift across domains. To obtain reliable pseudo-labels, we\npropose a Target-based Iterative Query-Token Adversarial Network (TITAN), which\nseparates the target images into two subsets: those similar to the source\n(easy) and those dissimilar (hard). We propose a strategy to estimate variance\nto partition the target domain. This approach leverages the insight that higher\ndetection variances correspond to higher recall and greater similarity to the\nsource domain. Also, we incorporate query-token-based adversarial modules into\na student-teacher baseline framework to reduce the domain gaps between two\nfeature representations. Experiments conducted on four natural imaging datasets\nand two challenging medical datasets have substantiated the superior\nperformance of TITAN compared to existing state-of-the-art (SOTA)\nmethodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7\npercent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,\nrespectively.",
      "pdf_url": "http://arxiv.org/pdf/2506.21484v1",
      "published": "2025-06-26T17:12:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21484v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture",
      "authors": [
        "Kehan Sui",
        "Jinxu Xiang",
        "Fang Jin"
      ],
      "abstract": "Singing voice synthesis (SVS) aims to generate expressive and high-quality\nvocals from musical scores, requiring precise modeling of pitch, duration, and\narticulation. While diffusion-based models have achieved remarkable success in\nimage and video generation, their application to SVS remains challenging due to\nthe complex acoustic and musical characteristics of singing, often resulting in\nartifacts that degrade naturalness. In this work, we propose SmoothSinger, a\nconditional diffusion model designed to synthesize high quality and natural\nsinging voices. Unlike prior methods that depend on vocoders as a final stage\nand often introduce distortion, SmoothSinger refines low-quality synthesized\naudio directly in a unified framework, mitigating the degradation associated\nwith two-stage pipelines. The model adopts a reference-guided dual-branch\narchitecture, using low-quality audio from any baseline system as a reference\nto guide the denoising process, enabling more expressive and context-aware\nsynthesis. Furthermore, it enhances the conventional U-Net with a parallel\nlow-frequency upsampling path, allowing the model to better capture pitch\ncontours and long term spectral dependencies. To improve alignment during\ntraining, we replace reference audio with degraded ground truth audio,\naddressing temporal mismatch between reference and target signals. Experiments\non the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that\nSmoothSinger achieves state-of-the-art results in both objective and subjective\nevaluations. Extensive ablation studies confirm its effectiveness in reducing\nartifacts and improving the naturalness of synthesized voices.",
      "pdf_url": "http://arxiv.org/pdf/2506.21478v1",
      "published": "2025-06-26T17:07:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21478v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage",
      "authors": [
        "Gavin Lee Goodship",
        "Luis Miralles-Pechuan",
        "Stephen O'Sullivan"
      ],
      "abstract": "Extended Stability Runge-Kutta (ESRK) methods are crucial for solving\nlarge-scale computational problems in science and engineering, including\nweather forecasting, aerodynamic analysis, and complex biological modelling.\nHowever, balancing accuracy, stability, and computational efficiency remains\nchallenging, particularly for high-order, low-storage schemes. This study\nintroduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)\napproach for automated heuristic discovery, optimising low-storage ESRK\nmethods. Unlike traditional approaches that rely on manually designed\nheuristics or exhaustive numerical searches, our method leverages GA-driven\nmutations for search-space exploration and an RL-inspired state transition\nmechanism to refine heuristic selection dynamically. This enables systematic\nparameter reduction, preserving fourth-order accuracy while significantly\nimproving computational efficiency.The proposed GA-RL heuristic optimisation\nframework is validated through rigorous testing on benchmark problems,\nincluding the 1D and 2D Brusselator systems and the steady-state Navier-Stokes\nequations. The best-performing heuristic achieves a 25\\% reduction in IPOPT\nruntime compared to traditional ESRK optimisation processes while maintaining\nnumerical stability and accuracy. These findings demonstrate the potential of\nadaptive heuristic discovery to improve resource efficiency in high-fidelity\nsimulations and broaden the applicability of low-storage Runge-Kutta methods in\nreal-world computational fluid dynamics, physics simulations, and other\ndemanding fields. This work establishes a new paradigm in heuristic\noptimisation for numerical methods, opening pathways for further exploration\nusing Deep RL and AutoML-based heuristic search",
      "pdf_url": "http://arxiv.org/pdf/2506.21465v1",
      "published": "2025-06-26T16:51:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21465v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Spatial Mental Modeling from Limited Views",
      "authors": [
        "Baiqiao Yin",
        "Qineng Wang",
        "Pingyue Zhang",
        "Jianshu Zhang",
        "Kangrui Wang",
        "Zihan Wang",
        "Jieyu Zhang",
        "Keshigeyan Chandrasegaran",
        "Han Liu",
        "Ranjay Krishna",
        "Saining Xie",
        "Manling Li",
        "Jiajun Wu",
        "Li Fei-Fei"
      ],
      "abstract": "Can Vision Language Models (VLMs) imagine the full scene from just a few\nviews, like humans do? Humans form spatial mental models, internal\nrepresentations of unseen space, to reason about layout, perspective, and\nmotion. Our new MindCube benchmark with 21,154 questions across 3,268 images\nexposes this critical gap, where existing VLMs exhibit near-random performance.\nUsing MindCube, we systematically evaluate how well VLMs build robust spatial\nmental models through representing positions (cognitive mapping), orientations\n(perspective-taking), and dynamics (mental simulation for \"what-if\" movements).\nWe then explore three approaches to help VLMs approximate spatial mental\nmodels, including unseen intermediate views, natural language reasoning chains,\nand cognitive maps. The significant improvement comes from a synergistic\napproach, \"map-then-reason\", that jointly trains the model to first generate a\ncognitive map and then reason upon it. By training models to reason over these\ninternal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding\nreinforcement learning pushed performance even further to 70.7% (+32.9%). Our\nkey insight is that such scaffolding of spatial mental models, actively\nconstructing and utilizing internal structured spatial representations with\nflexible reasoning processes, significantly improves understanding of\nunobservable space.",
      "pdf_url": "http://arxiv.org/pdf/2506.21458v1",
      "published": "2025-06-26T16:38:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21458v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection",
      "authors": [
        "Ali Şenol",
        "Garima Agrawal",
        "Huan Liu"
      ],
      "abstract": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.21443v1",
      "published": "2025-06-26T16:29:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21443v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference",
      "authors": [
        "Colin Samplawski",
        "Adam D. Cobb",
        "Manoj Acharya",
        "Ramneet Kaur",
        "Susmit Jha"
      ],
      "abstract": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work.",
      "pdf_url": "http://arxiv.org/pdf/2506.21408v1",
      "published": "2025-06-26T15:54:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21408v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding",
      "authors": [
        "Junwen Zhang",
        "Pu Chen",
        "Yin Zhang"
      ],
      "abstract": "Multimodal understanding of tables in real-world contexts is challenging due\nto the complexity of structure, symbolic density, and visual degradation (blur,\nskew, watermarking, incomplete structures or fonts, multi-span or\nhierarchically nested layouts). Existing multimodal large language models\n(MLLMs) struggle with such WildStruct conditions, resulting in limited\nperformance and poor generalization. To address these challenges, we propose\nTableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture\nspecifically designed for robust, structured reasoning over multimodal table\ndata. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which\npredicts latent semantic token roles (e.g., header, data cell, axis, formula)\nand dynamically routes table elements to specialized experts (Table-to-HTML,\nTable-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed\nby symbolic reasoning graphs. To facilitate effective alignment-driven\npretraining, we introduce the large-scale TableMoE-Align dataset, consisting of\n1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and\nindustry, utilized exclusively for model pretraining. For evaluation, we curate\nand release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,\nWMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models\nunder real-world multimodal degradation and structural complexity. Experimental\nresults demonstrate that TableMoE significantly surpasses existing\nstate-of-the-art models. Extensive ablation studies validate each core\ncomponent, emphasizing the critical role of Neuro-Symbolic Routing and\nstructured expert alignment. Through qualitative analyses, we further showcase\nTableMoE's interpretability and enhanced robustness, underscoring the\neffectiveness of integrating neuro-symbolic reasoning for multimodal table\nunderstanding.",
      "pdf_url": "http://arxiv.org/pdf/2506.21393v1",
      "published": "2025-06-26T15:41:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21393v1",
      "categories": [
        "cs.AI",
        "68T07 (Primary), 68T50, 68T30, 68T45 (Secondary)",
        "F.2.2; I.2.7; I.2.10"
      ]
    },
    {
      "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation",
      "authors": [
        "Guanting Dong",
        "Xiaoxi Li",
        "Yuyao Zhang",
        "Mengjie Deng"
      ],
      "abstract": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.",
      "pdf_url": "http://arxiv.org/pdf/2506.21384v1",
      "published": "2025-06-26T15:35:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21384v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection",
      "authors": [
        "Zhi Zheng",
        "Bochuan Zhou",
        "Yuping Song"
      ],
      "abstract": "Cryptocurrency transaction fraud detection faces the dual challenges of\nincreasingly complex transaction patterns and severe class imbalance.\nTraditional methods rely on manual feature engineering and struggle to capture\ntemporal and structural dependencies in transaction networks. This paper\nproposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that\nenhances detection performance through three modules: (1) designing an advanced\ntemporal embedding module that fuses multi-scale time difference features with\nperiodic position encoding; (2) constructing a temporal-aware triple attention\nmechanism that jointly optimizes structural, temporal, and global context\nattention; (3) employing weighted BCE loss to address class imbalance.\nExperiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT\nachieves an AUC of 0.9130, representing a 9.2% improvement over the best\ntraditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This\nmethod not only validates the enhancement effect of temporal awareness and\ntriple attention mechanisms on graph neural networks, but also provides\nfinancial institutions with more reliable fraud detection tools, with its\ndesign principles generalizable to other temporal graph anomaly detection\ntasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.21382v1",
      "published": "2025-06-26T15:34:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21382v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Pay Attention to Small Weights",
      "authors": [
        "Chao Zhou",
        "Tom Jacobs",
        "Advait Gadhikar",
        "Rebekka Burkholz"
      ],
      "abstract": "Finetuning large pretrained neural networks is known to be\nresource-intensive, both in terms of memory and computational cost. To mitigate\nthis, a common approach is to restrict training to a subset of the model\nparameters. By analyzing the relationship between gradients and weights during\nfinetuning, we observe a notable pattern: large gradients are often associated\nwith small-magnitude weights. This correlation is more pronounced in finetuning\nsettings than in training from scratch. Motivated by this observation, we\npropose NANOADAM, which dynamically updates only the small-magnitude weights\nduring finetuning and offers several practical advantages: first, this\ncriterion is gradient-free -- the parameter subset can be determined without\ngradient computation; second, it preserves large-magnitude weights, which are\nlikely to encode critical features learned during pretraining, thereby reducing\nthe risk of catastrophic forgetting; thirdly, it permits the use of larger\nlearning rates and consistently leads to better generalization performance in\nexperiments. We demonstrate this for both NLP and vision tasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.21374v1",
      "published": "2025-06-26T15:22:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21374v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Real-time and personalized product recommendations for large e-commerce platforms",
      "authors": [
        "Matteo Tolloso",
        "Davide Bacciu",
        "Shahab Mokarizadeh",
        "Marco Varesi"
      ],
      "abstract": "We present a methodology to provide real-time and personalized product\nrecommendations for large e-commerce platforms, specifically focusing on\nfashion retail. Our approach aims to achieve accurate and scalable\nrecommendations with minimal response times, ensuring user satisfaction,\nleveraging Graph Neural Networks and parsimonious learning methodologies.\nExtensive experimentation with datasets from one of the largest e-commerce\nplatforms demonstrates the effectiveness of our approach in forecasting\npurchase sequences and handling multi-interaction scenarios, achieving\nefficient personalized recommendations under real-world constraints.",
      "pdf_url": "http://arxiv.org/pdf/2506.21368v1",
      "published": "2025-06-26T15:16:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21368v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "rQdia: Regularizing Q-Value Distributions With Image Augmentation",
      "authors": [
        "Sam Lerman",
        "Jing Bi"
      ],
      "abstract": "rQdia regularizes Q-value distributions with augmented images in pixel-based\ndeep reinforcement learning. With a simple auxiliary loss, that equalizes these\ndistributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks\nrespectively in the MuJoCo Continuous Control Suite from pixels, and\nData-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured\nin both sample efficiency and longer-term training. Moreover, the addition of\nrQdia finally propels model-free continuous control from pixels over the state\nencoding baseline.",
      "pdf_url": "http://arxiv.org/pdf/2506.21367v1",
      "published": "2025-06-26T15:16:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21367v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection",
      "authors": [
        "Zhixin Cheng",
        "Jiacheng Deng",
        "Xinjun Li",
        "Xiaotian Yin",
        "Bohao Liao",
        "Baoqun Yin",
        "Wenfei Yang",
        "Tianzhu Zhang"
      ],
      "abstract": "Detection-free methods typically follow a coarse-to-fine pipeline, extracting\nimage and point cloud features for patch-level matching and refining dense\npixel-to-point correspondences. However, differences in feature channel\nattention between images and point clouds may lead to degraded matching\nresults, ultimately impairing registration accuracy. Furthermore, similar\nstructures in the scene could lead to redundant correspondences in cross-modal\nmatching. To address these issues, we propose Channel Adaptive Adjustment\nModule (CAA) and Global Optimal Selection Module (GOS). CAA enhances\nintra-modal features and suppresses cross-modal sensitivity, while GOS replaces\nlocal selection with global optimization. Experiments on RGB-D Scenes V2 and\n7-Scenes demonstrate the superiority of our method, achieving state-of-the-art\nperformance in image-to-point cloud registration.",
      "pdf_url": "http://arxiv.org/pdf/2506.21364v1",
      "published": "2025-06-26T15:15:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21364v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Systematic Review of Human-AI Co-Creativity",
      "authors": [
        "Saloni Singh",
        "Koen Hindriks",
        "Dirk Heylen",
        "Kim Baraka"
      ],
      "abstract": "The co creativity community is making significant progress in developing more\nsophisticated and tailored systems to support and enhance human creativity.\nDesign considerations from prior work can serve as a valuable and efficient\nfoundation for future systems. To support this effort, we conducted a\nsystematic literature review of 62 papers on co-creative systems. These papers\ncover a diverse range of applications, including visual arts, design, and\nwriting, where the AI acts not just as a tool but as an active collaborator in\nthe creative process. From this review, we identified several key dimensions\nrelevant to system design: phase of the creative process, creative task,\nproactive behavior of the system, user control, system embodiment, and AI model\ntype. Our findings suggest that systems offering high user control lead to\ngreater satisfaction, trust, and a stronger sense of ownership over creative\noutcomes. Furthermore, proactive systems, when adaptive and context sensitive,\ncan enhance collaboration. We also extracted 24 design considerations,\nhighlighting the value of encouraging users to externalize their thoughts and\nof increasing the system's social presence and transparency to foster trust.\nDespite recent advancements, important gaps remain, such as limited support for\nearly creative phases like problem clarification, and challenges related to\nuser adaptation to AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.21333v2",
      "published": "2025-06-26T14:44:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21333v2",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2.11"
      ]
    },
    {
      "title": "Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models",
      "authors": [
        "Haoyang Wu",
        "Tsun-Hsuan Wang",
        "Mathias Lechner",
        "Ramin Hasani",
        "Jennifer A. Eckhoff",
        "Paul Pak",
        "Ozanan R. Meireles",
        "Guy Rosman",
        "Yutong Ban",
        "Daniela Rus"
      ],
      "abstract": "Surgical workflow analysis is essential in robot-assisted surgeries, yet the\nlong duration of such procedures poses significant challenges for comprehensive\nvideo analysis. Recent approaches have predominantly relied on transformer\nmodels; however, their quadratic attention mechanism restricts efficient\nprocessing of lengthy surgical videos. In this paper, we propose a novel\nhierarchical input-dependent state space model that leverages the linear\nscaling property of state space models to enable decision making on full-length\nvideos while capturing both local and global dynamics. Our framework\nincorporates a temporally consistent visual feature extractor, which appends a\nstate space model head to a visual feature extractor to propagate temporal\ninformation. The proposed model consists of two key modules: a\nlocal-aggregation state space model block that effectively captures intricate\nlocal dynamics, and a global-relation state space model block that models\ntemporal dependencies across the entire video. The model is trained using a\nhybrid discrete-continuous supervision strategy, where both signals of discrete\nphase labels and continuous phase progresses are propagated through the\nnetwork. Experiments have shown that our method outperforms the current\nstate-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on\nMICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available\nafter paper acceptance.",
      "pdf_url": "http://arxiv.org/pdf/2506.21330v1",
      "published": "2025-06-26T14:43:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21330v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Active Inference AI Systems for Scientific Discovery",
      "authors": [
        "Karthik Duraisamy"
      ],
      "abstract": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component.",
      "pdf_url": "http://arxiv.org/pdf/2506.21329v1",
      "published": "2025-06-26T14:43:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21329v1",
      "categories": [
        "cs.AI",
        "physics.soc-ph",
        "68",
        "I.2"
      ]
    },
    {
      "title": "IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems",
      "authors": [
        "Pauline Speckmann",
        "Mario Nadj",
        "Christian Janiesch"
      ],
      "abstract": "Although several post-hoc methods for explainable AI have been developed,\nmost are static and neglect the user perspective, limiting their effectiveness\nfor the target audience. In response, we developed the interactive explainable\nintelligent system called IXAII that offers explanations from four explainable\nAI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored\nviews for five user groups and gives users agency over the explanations'\ncontent and their format. We evaluated IXAII through interviews with experts\nand lay users. Our results indicate that IXAII, which provides different\nexplanations with multiple visualization options, is perceived as helpful to\nincrease transparency. By bridging the gaps between explainable AI methods,\ninteractivity, and practical implementation, we provide a novel perspective on\nAI explanation practices and human-AI interaction.",
      "pdf_url": "http://arxiv.org/pdf/2506.21310v1",
      "published": "2025-06-26T14:28:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21310v1",
      "categories": [
        "cs.AI",
        "cs.SE",
        "K.6.3 Software Management"
      ]
    },
    {
      "title": "On Uniform Weighted Deep Polynomial approximation",
      "authors": [
        "Kingsley Yeon",
        "Steven B. Damelin"
      ],
      "abstract": "It is a classical result in rational approximation theory that certain\nnon-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be\nefficiently approximated using rational functions with root-exponential\nconvergence in terms of degrees of freedom \\cite{Sta, GN}. In contrast,\npolynomial approximations admit only algebraic convergence by Jackson's theorem\n\\cite{Lub2}. Recent work shows that composite polynomial architectures can\nrecover exponential approximation rates even without smoothness \\cite{KY}. In\nthis work, we introduce and analyze a class of weighted deep polynomial\napproximants tailored for functions with asymmetric behavior-growing unbounded\non one side and decaying on the other. By multiplying a learnable deep\npolynomial with a one-sided weight, we capture both local non-smoothness and\nglobal growth. We show numerically that this framework outperforms Taylor,\nChebyshev, and standard deep polynomial approximants, even when all use the\nsame number of parameters. To optimize these approximants in practice, we\npropose a stable graph-based parameterization strategy building on \\cite{Jar}.",
      "pdf_url": "http://arxiv.org/pdf/2506.21306v1",
      "published": "2025-06-26T14:25:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21306v1",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.LG",
        "cs.NA",
        "stat.ML"
      ]
    },
    {
      "title": "Exploring Adapter Design Tradeoffs for Low Resource Music Generation",
      "authors": [
        "Atharva Mehta",
        "Shivam Chauhan",
        "Monojit Choudhury"
      ],
      "abstract": "Fine-tuning large-scale music generation models, such as MusicGen and\nMustango, is a computationally expensive process, often requiring updates to\nbillions of parameters and, therefore, significant hardware resources.\nParameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based\nmethods, have emerged as a promising alternative, enabling adaptation with\nminimal trainable parameters while preserving model performance. However, the\ndesign choices for adapters, including their architecture, placement, and size,\nare numerous, and it is unclear which of these combinations would produce\noptimal adapters and why, for a given case of low-resource music genre. In this\npaper, we attempt to answer this question by studying various adapter\nconfigurations for two AI music models, MusicGen and Mustango, on two genres:\nHindustani Classical and Turkish Makam music.\n  Our findings reveal distinct trade-offs: convolution-based adapters excel in\ncapturing fine-grained local musical details such as ornamentations and short\nmelodic phrases, while transformer-based adapters better preserve long-range\ndependencies crucial for structured improvisation. Additionally, we analyze\ncomputational resource requirements across different adapter scales,\ndemonstrating how mid-sized adapters (40M parameters) achieve an optimal\nbalance between expressivity and quality. Furthermore, we find that Mustango, a\ndiffusion-based model, generates more diverse outputs with better adherence to\nthe description in the input prompt while lacking in providing stability in\nnotes, rhythm alignment, and aesthetics. Also, it is computationally intensive\nand requires significantly more time to train. In contrast, autoregressive\nmodels like MusicGen offer faster training and are more efficient, and can\nproduce better quality output in comparison, but have slightly higher\nredundancy in their generations.",
      "pdf_url": "http://arxiv.org/pdf/2506.21298v1",
      "published": "2025-06-26T14:18:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21298v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ]
    },
    {
      "title": "Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models",
      "authors": [
        "Bram Willemsen",
        "Gabriel Skantze"
      ],
      "abstract": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.",
      "pdf_url": "http://arxiv.org/pdf/2506.21294v1",
      "published": "2025-06-26T14:14:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21294v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
      "authors": [
        "Istabrak Abbes",
        "Gabriele Prato",
        "Quentin Fournier",
        "Fernando Rodriguez",
        "Alaa Boukhary",
        "Adam Elwood",
        "Sarath Chandar"
      ],
      "abstract": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less",
      "pdf_url": "http://arxiv.org/pdf/2506.21288v1",
      "published": "2025-06-26T14:09:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21288v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution",
      "authors": [
        "Lukas Sablica",
        "Kurt Hornik"
      ],
      "abstract": "We propose a novel variational autoencoder (VAE) architecture that employs a\nspherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian\nlatent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy\nprovides a more natural hyperspherical representation of latent variables,\nbetter capturing directional data while maintaining flexibility. Its\nheavy-tailed nature prevents over-regularization, ensuring efficient latent\nspace utilization while offering a more expressive representation.\nAdditionally, spCauchy circumvents the numerical instabilities inherent to vMF,\nwhich arise from computing normalization constants involving Bessel functions.\nInstead, it enables a fully differentiable and efficient reparameterization\ntrick via M\\\"obius transformations, allowing for stable and scalable training.\nThe KL divergence can be computed through a rapidly converging power series,\neliminating concerns of underflow or overflow associated with evaluation of\nratios of hypergeometric functions. These properties make spCauchy a compelling\nalternative for VAEs, offering both theoretical advantages and practical\nefficiency in high-dimensional generative modeling.",
      "pdf_url": "http://arxiv.org/pdf/2506.21278v1",
      "published": "2025-06-26T14:01:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21278v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou",
      "authors": [
        "Pengfei Fan",
        "Yuli Zhang",
        "Xinheng Wang",
        "Ruiyuan Jiang",
        "Hankang Gu",
        "Dongyao Jia",
        "Shangbo Wang"
      ],
      "abstract": "This study presents and publicly releases the Suzhou Urban Road Acoustic\nDataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive\ndata-acquisition protocols and annotation guidelines to ensure transparency and\nreproducibility of the experimental workflow. To model the coupling between\nvehicular noise and driving speed, we propose a bimodal-feature-fusion deep\nconvolutional neural network (BMCNN). During preprocessing, an adaptive\ndenoising and normalization strategy is applied to suppress environmental\nbackground interference; in the network architecture, parallel branches extract\nMel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,\nwhich are subsequently fused via a cross-modal attention mechanism in the\nintermediate feature space to fully exploit time-frequency information.\nExperimental results demonstrate that BMCNN achieves a classification accuracy\nof 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic\ndataset. Ablation studies and robustness tests on the Suzhou dataset further\nvalidate the contributions of each module to performance improvement and\noverfitting mitigation. The proposed acoustics-based speed classification\nmethod can be integrated into smart-city traffic management systems for\nreal-time noise monitoring and speed estimation, thereby optimizing traffic\nflow control, reducing roadside noise pollution, and supporting sustainable\nurban planning.",
      "pdf_url": "http://arxiv.org/pdf/2506.21269v1",
      "published": "2025-06-26T13:53:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21269v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster",
      "authors": [
        "Ji Qi",
        "WenPeng Zhu",
        "Li Li",
        "Ming Wu",
        "YingJun Wu",
        "Wu He",
        "Xun Gao",
        "Jason Zeng",
        "Michael Heinrich"
      ],
      "abstract": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters.",
      "pdf_url": "http://arxiv.org/pdf/2506.21263v1",
      "published": "2025-06-26T13:45:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21263v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents",
      "authors": [
        "Tianyi Men",
        "Zhuoran Jin",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show\npromise in real-world tasks like web navigation and embodied intelligence.\nHowever, due to limitations in a lack of external feedback, these agents\nstruggle with self-correction and generalization. A promising approach is to\nuse reward models as external feedback, but there is no clear on how to select\nreward models for agents. Thus, there is an urgent need to build a reward bench\ntargeted at agents. To address these challenges, we propose Agent-RewardBench,\na benchmark designed to evaluate reward modeling ability in MLLMs. The\nbenchmark is characterized by three key features: (1) Multiple dimensions and\nreal-world agent scenarios evaluation. It covers perception, planning, and\nsafety with 7 scenarios; (2) Step-level reward evaluation. It allows for the\nassessment of agent capabilities at the individual steps of a task, providing a\nmore granular view of performance during the planning process; and (3)\nAppropriately difficulty and high-quality. We carefully sample from 10 diverse\nmodels, difficulty control to maintain task challenges, and manual verification\nto ensure the integrity of the data. Experiments demonstrate that even\nstate-of-the-art multimodal models show limited performance, highlighting the\nneed for specialized training in agent reward modeling. Code is available at\ngithub.",
      "pdf_url": "http://arxiv.org/pdf/2506.21252v1",
      "published": "2025-06-26T13:36:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21252v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting",
      "authors": [
        "Giorgos Demosthenous",
        "Chryssis Georgiou",
        "Eliada Polydorou"
      ],
      "abstract": "This study investigates the impact of data source diversity on the\nperformance of cryptocurrency forecasting models by integrating various data\ncategories, including technical indicators, on-chain metrics, sentiment and\ninterest metrics, traditional market indices, and macroeconomic indicators. We\nintroduce the Crypto100 index, representing the top 100 cryptocurrencies by\nmarket capitalization, and propose a novel feature reduction algorithm to\nidentify the most impactful and resilient features from diverse data sources.\nOur comprehensive experiments demonstrate that data source diversity\nsignificantly enhances the predictive performance of forecasting models across\ndifferent time horizons. Key findings include the paramount importance of\non-chain metrics for both short-term and long-term predictions, the growing\nrelevance of traditional market indices and macroeconomic indicators for\nlonger-term forecasts, and substantial improvements in model accuracy when\ndiverse data sources are utilized. These insights help demystify the short-term\nand long-term driving factors of the cryptocurrency market and lay the\ngroundwork for developing more accurate and resilient forecasting models.",
      "pdf_url": "http://arxiv.org/pdf/2506.21246v1",
      "published": "2025-06-26T13:29:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21246v1",
      "categories": [
        "q-fin.PM",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "q-fin.ST"
      ]
    },
    {
      "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner",
      "authors": [
        "Junhao Shi",
        "Zhaoye Fei",
        "Siyin Wang",
        "Qipeng Guo",
        "Jingjing Gong",
        "Xipeng QIu"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) show promise for embodied planning tasks\nbut struggle with complex scenarios involving unfamiliar environments and\nmulti-step goals. Current approaches rely on environment-agnostic imitation\nlearning that disconnects instructions from environmental contexts, causing\nmodels to struggle with context-sensitive instructions and rely on\nsupplementary cues rather than visual reasoning during long-horizon\ninteractions. In this work, we propose World-Aware Planning Narrative\nEnhancement (WAP), a framework that infuses LVLMs with comprehensive\nenvironmental understanding through four cognitive capabilities (visual\nappearance modeling, spatial reasoning, functional abstraction, and syntactic\ngrounding) while developing and evaluating models using only raw visual\nobservations through curriculum learning. Evaluations on the EB-ALFRED\nbenchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a\n60.7 absolute improvement in task success rates, particularly in commonsense\nreasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced\nopen-source models outperform proprietary systems like GPT-4o and\nClaude-3.5-Sonnet by a large margin.",
      "pdf_url": "http://arxiv.org/pdf/2506.21230v1",
      "published": "2025-06-26T13:20:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21230v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?",
      "authors": [
        "Haoang Chi",
        "He Li",
        "Wenjing Yang",
        "Feng Liu",
        "Long Lan",
        "Xiaoguang Ren",
        "Tongliang Liu",
        "Bo Han"
      ],
      "abstract": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2.",
      "pdf_url": "http://arxiv.org/pdf/2506.21215v1",
      "published": "2025-06-26T13:11:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21215v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models",
      "authors": [
        "Quanming Liu",
        "Xupeng Bu",
        "Zhichao Yan",
        "Ru Li"
      ],
      "abstract": "Automatic Program Repair (APR) is a core technology in software development\nand maintenance, with aims to enable automated defect repair with minimal human\nintervention. In recent years, the substantial advancements in Large Language\nModels (LLMs) and the Chain-of-Thought (CoT) techniques have significantly\nenhanced the reasoning capabilities of these models. However, due to the\ncomplex logic and multi-step reasoning ability needed, the application of CoT\ntechniques in the APR domain remains insufficient. This study systematically\nevaluates the performance of several common CoT techniques in APR tasks and\nproposes an innovative framework $T^3$, which integrates the powerful reasoning\ncapabilities of LLMs with tree search, effectively improving the precision of\ngenerating candidate repair solutions. Furthermore, $T^3$ provides valuable\nguidance for optimizing sample selection and repair strategies in APR tasks,\nestablishing a robust framework for achieving efficient automated debugging.",
      "pdf_url": "http://arxiv.org/pdf/2506.21211v1",
      "published": "2025-06-26T13:04:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21211v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models",
      "authors": [
        "Louis Kerner",
        "Michel Meintz",
        "Bihe Zhao",
        "Franziska Boenisch",
        "Adam Dziedzic"
      ],
      "abstract": "State-of-the-art text-to-image models like Infinity generate photorealistic\nimages at an unprecedented speed. These models operate in a bitwise\nautoregressive manner over a discrete set of tokens that is practically\ninfinite in size. However, their impressive generative power comes with a\ngrowing risk: as their outputs increasingly populate the Internet, they are\nlikely to be scraped and reused as training data-potentially by the very same\nmodels. This phenomenon has been shown to lead to model collapse, where\nrepeated training on generated content, especially from the models' own\nprevious versions, causes a gradual degradation in performance. A promising\nmitigation strategy is watermarking, which embeds human-imperceptible yet\ndetectable signals into generated images-enabling the identification of\ngenerated content. In this work, we introduce BitMark, a robust bitwise\nwatermarking framework for Infinity. Our method embeds a watermark directly at\nthe bit level of the token stream across multiple scales (also referred to as\nresolutions) during Infinity's image generation process. Our bitwise watermark\nsubtly influences the bits to preserve visual fidelity and generation speed\nwhile remaining robust against a spectrum of removal techniques. Furthermore,\nit exhibits high radioactivity, i.e., when watermarked generated images are\nused to train another image generative model, this second model's outputs will\nalso carry the watermark. The radioactive traces remain detectable even when\nonly fine-tuning diffusion or image autoregressive models on images watermarked\nwith our BitMark. Overall, our approach provides a principled step toward\npreventing model collapse in image generative models by enabling reliable\ndetection of generated outputs.",
      "pdf_url": "http://arxiv.org/pdf/2506.21209v1",
      "published": "2025-06-26T13:03:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21209v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
      "authors": [
        "Minghao Qin",
        "Yan Shu",
        "Peitian Zhang",
        "Kun Lun",
        "Huaying Yuan",
        "Juenjie Zhou",
        "Shitao Xiao",
        "Bo Zhao",
        "Zheng Liu"
      ],
      "abstract": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
      "pdf_url": "http://arxiv.org/pdf/2506.21184v1",
      "published": "2025-06-26T12:43:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21184v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks",
      "authors": [
        "Isaac Chung",
        "Imene Kerboua",
        "Marton Kardos",
        "Roman Solomatin",
        "Kenneth Enevoldsen"
      ],
      "abstract": "The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation\nplatform for text embedding models. While previous work has established the\ncore benchmark methodology, this paper focuses on the engineering aspects that\nensure MTEB's continued reproducibility and extensibility. We present our\napproach to maintaining robust continuous integration pipelines that validate\ndataset integrity, automate test execution, and assess benchmark results'\ngeneralizability. We detail the design choices that collectively enhance\nreproducibility and usability. Furthermore, we discuss our strategies for\nhandling community contributions and extending the benchmark with new tasks and\ndatasets. These engineering practices have been instrumental in scaling MTEB to\nbecome more comprehensive while maintaining quality and, ultimately, relevance\nto the field. Our experiences offer valuable insights for benchmark maintainers\nfacing similar challenges in ensuring reproducibility and usability in machine\nlearning evaluation frameworks. The MTEB repository is available at:\nhttps://github.com/embeddings-benchmark/mteb",
      "pdf_url": "http://arxiv.org/pdf/2506.21182v1",
      "published": "2025-06-26T12:40:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21182v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "A Hierarchical Deep Learning Approach for Minority Instrument Detection",
      "authors": [
        "Dylan Sechet",
        "Francesca Bugiotti",
        "Matthieu Kowalski",
        "Edouard d'Hérouville",
        "Filip Langiewicz"
      ],
      "abstract": "Identifying instrument activities within audio excerpts is vital in music\ninformation retrieval, with significant implications for music cataloging and\ndiscovery. Prior deep learning endeavors in musical instrument recognition have\npredominantly emphasized instrument classes with ample data availability.\nRecent studies have demonstrated the applicability of hierarchical\nclassification in detecting instrument activities in orchestral music, even\nwith limited fine-grained annotations at the instrument level. Based on the\nHornbostel-Sachs classification, such a hierarchical classification system is\nevaluated using the MedleyDB dataset, renowned for its diversity and richness\nconcerning various instruments and music genres. This work presents various\nstrategies to integrate hierarchical structures into models and tests a new\nclass of models for hierarchical music prediction. This study showcases more\nreliable coarse-level instrument detection by bridging the gap between detailed\ninstrument identification and group-level recognition, paving the way for\nfurther advancements in this domain.",
      "pdf_url": "http://arxiv.org/pdf/2506.21167v1",
      "published": "2025-06-26T11:56:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21167v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation",
      "authors": [
        "Shuwei Xing",
        "Derek W. Cool",
        "David Tessier",
        "Elvis C. S. Chen",
        "Terry M. Peters",
        "Aaron Fenster"
      ],
      "abstract": "3D ultrasound (US) imaging has shown significant benefits in enhancing the\noutcomes of percutaneous liver tumour ablation. Its clinical integration is\ncrucial for transitioning 3D US into the therapeutic domain. However,\nchallenges of tumour identification in US images continue to hinder its broader\nadoption. In this work, we propose a novel framework for integrating 3D US into\nthe standard ablation workflow. We present a key component, a clinically viable\n2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to\nreduce registration complexity. To facilitate efficient verification of the\nregistration workflow, we also propose an intuitive multimodal image\nvisualization technique. In our study, 2D US-CT/MRI registration achieved a\nlandmark distance error of approximately 2-4 mm with a runtime of 0.22s per\nimage pair. Additionally, non-rigid registration reduced the mean alignment\nerror by approximately 40% compared to rigid registration. Results demonstrated\nthe efficacy of the proposed 2D US-CT/MRI registration workflow. Our\nintegration framework advanced the capabilities of 3D US imaging in improving\npercutaneous tumour ablation, demonstrating the potential to expand the\ntherapeutic role of 3D US in clinical interventions.",
      "pdf_url": "http://arxiv.org/pdf/2506.21162v1",
      "published": "2025-06-26T11:39:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21162v1",
      "categories": [
        "eess.IV",
        "cs.AI"
      ]
    },
    {
      "title": "Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation",
      "authors": [
        "He Li",
        "Haoang Chi",
        "Mingyu Liu",
        "Wanrong Huang",
        "Liyang Xu",
        "Wenjing Yang"
      ],
      "abstract": "The real world naturally has dimensions of time and space. Therefore,\nestimating the counterfactual outcomes with spatial-temporal attributes is a\ncrucial problem. However, previous methods are based on classical statistical\nmodels, which still have limitations in performance and generalization. This\npaper proposes a novel framework for estimating counterfactual outcomes with\nspatial-temporal attributes using the Transformer, exhibiting stronger\nestimation ability. Under mild assumptions, the proposed estimator within this\nframework is consistent and asymptotically normal. To validate the\neffectiveness of our approach, we conduct simulation experiments and real data\nexperiments. Simulation experiments show that our estimator has a stronger\nestimation capability than baseline methods. Real data experiments provide a\nvaluable conclusion to the causal effect of conflicts on forest loss in\nColombia. The source code is available at\nhttps://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.",
      "pdf_url": "http://arxiv.org/pdf/2506.21154v1",
      "published": "2025-06-26T11:24:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21154v1",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels",
      "authors": [
        "Aida Moafi",
        "Danial Moafi",
        "Evgeny M. Mirkes",
        "Gerry P. McCann",
        "Abbas S. Alatrany",
        "Jayanth R. Arnold",
        "Mostafa Mehdipour Ghazi"
      ],
      "abstract": "The accurate segmentation of myocardial scars from cardiac MRI is essential\nfor clinical assessment and treatment planning. In this study, we propose a\nrobust deep-learning pipeline for fully automated myocardial scar detection and\nsegmentation by fine-tuning state-of-the-art models. The method explicitly\naddresses challenges of label noise from semi-automatic annotations, data\nheterogeneity, and class imbalance through the use of Kullback-Leibler loss and\nextensive data augmentation. We evaluate the model's performance on both acute\nand chronic cases and demonstrate its ability to produce accurate and smooth\nsegmentations despite noisy labels. In particular, our approach outperforms\nstate-of-the-art models like nnU-Net and shows strong generalizability in an\nout-of-distribution test set, highlighting its robustness across various\nimaging conditions and clinical tasks. These results establish a reliable\nfoundation for automated myocardial scar quantification and support the broader\nclinical adoption of deep learning in cardiac imaging.",
      "pdf_url": "http://arxiv.org/pdf/2506.21151v1",
      "published": "2025-06-26T11:21:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21151v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Linearity-based neural network compression",
      "authors": [
        "Silas Dobler",
        "Florian Lemmerich"
      ],
      "abstract": "In neural network compression, most current methods reduce unnecessary\nparameters by measuring importance and redundancy. To augment already highly\noptimized existing solutions, we propose linearity-based compression as a novel\nway to reduce weights in a neural network. It is based on the intuition that\nwith ReLU-like activation functions, neurons that are almost always activated\nbehave linearly, allowing for merging of subsequent layers. We introduce the\ntheory underlying this compression and evaluate our approach experimentally.\nOur novel method achieves a lossless compression down to 1/4 of the original\nmodel size in over the majority of tested models. Applying our method on\nalready importance-based pruned models shows very little interference between\ndifferent types of compression, demonstrating the option of successful\ncombination of techniques. Overall, our work lays the foundation for a new type\nof compression method that enables smaller and ultimately more efficient neural\nnetwork models.",
      "pdf_url": "http://arxiv.org/pdf/2506.21146v1",
      "published": "2025-06-26T11:04:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21146v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding",
      "authors": [
        "Ziwei Wang",
        "Hongbin Wang",
        "Tianwang Jia",
        "Xingyi He",
        "Siyang Li",
        "Dongrui Wu"
      ],
      "abstract": "Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform\nspontaneous/evoked neural activity into control commands for external\ncommunication. While convolutional neural networks (CNNs) remain the mainstream\nbackbone for EEG decoding, their inherently short receptive field makes it\ndifficult to capture long-range temporal dependencies and global inter-channel\nrelationships. Recent CNN-Transformer (Conformers) hybrids partially address\nthis issue, but most adopt a serial design, resulting in suboptimal integration\nof local and global features, and often overlook explicit channel-wise\nmodeling. To address these limitations, we propose DBConformer, a dual-branch\nconvolutional Transformer network tailored for EEG decoding. It integrates a\ntemporal Conformer to model long-range temporal dependencies and a spatial\nConformer to extract inter-channel interactions, capturing both temporal\ndynamics and spatial patterns in EEG signals. A lightweight channel attention\nmodule further refines spatial representations by assigning data-driven\nimportance to EEG channels. Extensive experiments on five motor imagery (MI)\ndatasets and two seizure detection datasets under three evaluation settings\ndemonstrate that DBConformer consistently outperforms 10 competitive baseline\nmodels, with over eight times fewer parameters than the high-capacity EEG\nConformer baseline. Further, the visualization results confirm that the\nfeatures extracted by DBConformer are physiologically interpretable and aligned\nwith sensorimotor priors in MI. The superior performance and interpretability\nof DBConformer make it reliable for robust and explainable EEG decoding. Code\nis publicized at https://github.com/wzwvv/DBConformer.",
      "pdf_url": "http://arxiv.org/pdf/2506.21140v1",
      "published": "2025-06-26T10:53:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21140v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE",
      "authors": [
        "Abdelkarim El-Hajjami",
        "Camille Salinesi"
      ],
      "abstract": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation.",
      "pdf_url": "http://arxiv.org/pdf/2506.21138v1",
      "published": "2025-06-26T10:52:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2506.21138v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    }
  ]
}