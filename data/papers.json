{
  "last_updated": "2025-09-20T00:45:04.056552",
  "papers": [
    {
      "title": "Generalizable Geometric Image Caption Synthesis",
      "authors": [
        "Yue Xin",
        "Wenyuan Wang",
        "Rui Pan",
        "Ruida Wang",
        "Howard Meng",
        "Renjie Pi",
        "Shizhe Diao",
        "Tong Zhang"
      ],
      "abstract": "Multimodal large language models have various practical applications that\ndemand strong reasoning abilities. Despite recent advancements, these models\nstill struggle to solve complex geometric problems. A key challenge stems from\nthe lack of high-quality image-text pair datasets for understanding geometric\nimages. Furthermore, most template-based data synthesis pipelines typically\nfail to generalize to questions beyond their predefined templates. In this\npaper, we bridge this gap by introducing a complementary process of\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\npipeline. By adopting RLVR to refine captions for geometric images synthesized\nfrom 50 basic geometric relations and using reward signals derived from\nmathematical problem-solving tasks, our pipeline successfully captures the key\nfeatures of geometry problem-solving. This enables better task generalization\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\nscenarios, the generated dataset enhances the general reasoning capabilities of\nmultimodal large language models, yielding accuracy improvements of\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\nwith non-geometric input images of MathVista and MathVerse, along with\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\nin MMMU.",
      "pdf_url": "http://arxiv.org/pdf/2509.15217v1",
      "published": "2025-09-18T17:59:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15217v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation",
      "authors": [
        "Chen Si",
        "Qianyi Wu",
        "Chaitanya Amballa",
        "Romit Roy Choudhury"
      ],
      "abstract": "Realistic sound simulation plays a critical role in many applications. A key\nelement in sound simulation is the room impulse response (RIR), which\ncharacterizes how sound propagates from a source to a listener within a given\nspace. Recent studies have applied neural implicit methods to learn RIR using\ncontext information collected from the environment, such as scene images.\nHowever, these approaches do not effectively leverage explicit geometric\ninformation from the environment. To further exploit the potential of neural\nimplicit models with direct geometric features, we present Mesh-infused Neural\nAcoustic Field (MiNAF), which queries a rough room mesh at given locations and\nextracts distance distributions as an explicit representation of local context.\nOur approach demonstrates that incorporating explicit local geometric features\ncan better guide the neural network in generating more accurate RIR\npredictions. Through comparisons with conventional and state-of-the-art\nbaseline methods, we show that MiNAF performs competitively across various\nevaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets\nwith limited training samples, demonstrating an advance in high-fidelity sound\nsimulation.",
      "pdf_url": "http://arxiv.org/pdf/2509.15210v1",
      "published": "2025-09-18T17:57:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15210v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
      "authors": [
        "Xuekai Zhu",
        "Daixuan Cheng",
        "Dinghuai Zhang",
        "Hengli Li",
        "Kaiyan Zhang",
        "Che Jiang",
        "Youbang Sun",
        "Ermo Hua",
        "Yuxin Zuo",
        "Xingtai Lv",
        "Qizheng Zhang",
        "Lin Chen",
        "Fanghao Shao",
        "Bo Xue",
        "Yunchong Song",
        "Zhenjie Yang",
        "Ganqu Cui",
        "Ning Ding",
        "Jianfeng Gao",
        "Xiaodong Liu",
        "Bowen Zhou",
        "Hongyuan Mei",
        "Zhouhan Lin"
      ],
      "abstract": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
      "pdf_url": "http://arxiv.org/pdf/2509.15207v1",
      "published": "2025-09-18T17:56:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15207v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Orion: Fuzzing Workflow Automation",
      "authors": [
        "Max Bazalii",
        "Marius Fleischer"
      ],
      "abstract": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary.",
      "pdf_url": "http://arxiv.org/pdf/2509.15195v1",
      "published": "2025-09-18T17:52:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15195v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR",
        "D.4.6; I.2.2; D.2.5"
      ]
    },
    {
      "title": "TITAN: A Trajectory-Informed Technique for Adaptive Parameter Freezing in Large-Scale VQE",
      "authors": [
        "Yifeng Peng",
        "Xinyi Li",
        "Samuel Yen-Chi Chen",
        "Kaining Zhang",
        "Zhiding Liang",
        "Ying Wang",
        "Yuxuan Du"
      ],
      "abstract": "Variational quantum Eigensolver (VQE) is a leading candidate for harnessing\nquantum computers to advance quantum chemistry and materials simulations, yet\nits training efficiency deteriorates rapidly for large Hamiltonians. Two issues\nunderlie this bottleneck: (i) the no-cloning theorem imposes a linear growth in\ncircuit evaluations with the number of parameters per gradient step; and (ii)\ndeeper circuits encounter barren plateaus (BPs), leading to exponentially\nincreasing measurement overheads. To address these challenges, here we propose\na deep learning framework, dubbed Titan, which identifies and freezes inactive\nparameters of a given ansatze at initialization for a specific class of\nHamiltonians, reducing the optimization overhead without sacrificing accuracy.\nThe motivation of Titan starts with our empirical findings that a subset of\nparameters consistently has a negligible influence on training dynamics. Its\ndesign combines a theoretically grounded data construction strategy, ensuring\neach training example is informative and BP-resilient, with an adaptive neural\narchitecture that generalizes across ansatze of varying sizes. Across benchmark\ntransverse-field Ising models, Heisenberg models, and multiple molecule systems\nup to 30 qubits, Titan achieves up to 3 times faster convergence and 40% to 60%\nfewer circuit evaluations than state-of-the-art baselines, while matching or\nsurpassing their estimation accuracy. By proactively trimming parameter space,\nTitan lowers hardware demands and offers a scalable path toward utilizing VQE\nto advance practical quantum chemistry and materials science.",
      "pdf_url": "http://arxiv.org/pdf/2509.15193v1",
      "published": "2025-09-18T17:50:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15193v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning",
      "authors": [
        "Yeongbin Seo",
        "Dongha Lee",
        "Jaehyung Kim",
        "Jinyoung Yeo"
      ],
      "abstract": "Autoregressive (AR) language models generate text one token at a time, which\nlimits their inference speed. Diffusion-based language models offer a promising\nalternative, as they can decode multiple tokens in parallel. However, we\nidentify a key bottleneck in current diffusion LMs: the long decoding-window\nproblem, where tokens generated far from the input context often become\nirrelevant or repetitive. Previous solutions like semi-autoregressive address\nthis issue by splitting windows into blocks, but this sacrifices speed and\nbidirectionality, eliminating the main advantage of diffusion models. To\novercome this, we propose Convolutional decoding (Conv), a normalization-based\nmethod that narrows the decoding window without hard segmentation, leading to\nbetter fluency and flexibility. Additionally, we introduce Rejecting Rule-based\nFine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at\npositions far from context. Our methods achieve state-of-the-art results on\nopen-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM\nbaselines, with significantly lower step size than previous works,\ndemonstrating both speed and quality improvements.",
      "pdf_url": "http://arxiv.org/pdf/2509.15188v1",
      "published": "2025-09-18T17:48:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15188v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50",
        "I.2.7"
      ]
    },
    {
      "title": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models",
      "authors": [
        "Huy Nghiem",
        "Advik Sachdeva",
        "Hal Daumé III"
      ],
      "abstract": "WARNING: This paper contains examples of offensive materials. Toxic content\nhas become pervasive on social media platforms. We introduce SMARTER, a\ndata-efficient two-stage framework for explainable content moderation using\nLarge Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to\ngenerate synthetic explanations for both correct and incorrect labels, enabling\nalignment via preference optimization with minimal human supervision. In Stage\n2, we refine explanation quality through cross-model training, allowing weaker\nmodels to align stylistically and semantically with stronger ones. Experiments\non three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate --\ndemonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1\nimprovement over standard few-shot baselines while using only a fraction of the\nfull training data. Our framework offers a scalable strategy for low-resource\nsettings by harnessing LLMs' self-improving capabilities for both\nclassification and explanation.",
      "pdf_url": "http://arxiv.org/pdf/2509.15174v1",
      "published": "2025-09-18T17:30:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15174v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment",
      "authors": [
        "Ankur Samanta",
        "Akshayaa Magesh",
        "Youliang Yu",
        "Runzhe Wu",
        "Ayush Jain",
        "Daniel Jiang",
        "Boris Vidolov",
        "Paul Sajda",
        "Yonathan Efroni",
        "Kaveh Hassani"
      ],
      "abstract": "Language Models (LMs) are inconsistent reasoners, often generating\ncontradictory responses to identical prompts. While inference-time methods can\nmitigate these inconsistencies, they fail to address the core problem: LMs\nstruggle to reliably select reasoning pathways leading to consistent outcomes\nunder exploratory sampling. To address this, we formalize self-consistency as\nan intrinsic property of well-aligned reasoning models and introduce\nMulti-Agent Consensus Alignment (MACA), a reinforcement learning framework that\npost-trains models to favor reasoning trajectories aligned with their internal\nconsensus using majority/minority outcomes from multi-agent debate. These\ntrajectories emerge from deliberative exchanges where agents ground reasoning\nin peer arguments, not just aggregation of independent attempts, creating\nricher consensus signals than single-round majority voting. MACA enables agents\nto teach themselves to be more decisive and concise, and better leverage peer\ninsights in multi-agent settings without external supervision, driving\nsubstantial improvements across self-consistency (+27.6% on GSM8K),\nsingle-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%\nPass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).\nThese findings, coupled with strong generalization to unseen benchmarks (+16.3%\non GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more\nreliably unlocks latent reasoning potential of language models.",
      "pdf_url": "http://arxiv.org/pdf/2509.15172v1",
      "published": "2025-09-18T17:27:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15172v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting",
      "authors": [
        "Aarushi Mahajan",
        "Wayne Burleson"
      ],
      "abstract": "Radio frequency fingerprint identification (RFFI) distinguishes wireless\ndevices by the small variations in their analog circuits, avoiding heavy\ncryptographic authentication. While deep learning on spectrograms improves\naccuracy, models remain vulnerable to copying, tampering, and evasion. We\npresent a stronger RFFI system combining watermarking for ownership proof and\nanomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel\nspectrograms, we embed three watermarks: a simple trigger, an adversarially\ntrained trigger robust to noise and filtering, and a hidden gradient/weight\nsignature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler\n(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,\nour system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,\noffering verifiable, tamper-resistant authentication.",
      "pdf_url": "http://arxiv.org/pdf/2509.15170v1",
      "published": "2025-09-18T17:21:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15170v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "eess.SP"
      ]
    },
    {
      "title": "Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model",
      "authors": [
        "Pak-Hei Yeung",
        "Jayroop Ramesh",
        "Pengfei Lyu",
        "Ana Namburete",
        "Jagath Rajapakse"
      ],
      "abstract": "This paper explores the transfer of knowledge from general vision models\npretrained on 2D natural images to improve 3D medical image segmentation. We\nfocus on the semi-supervised setting, where only a few labeled 3D medical\nimages are available, along with a large set of unlabeled images. To tackle\nthis, we propose a model-agnostic framework that progressively distills\nknowledge from a 2D pretrained model to a 3D segmentation model trained from\nscratch. Our approach, M&N, involves iterative co-training of the two models\nusing pseudo-masks generated by each other, along with our proposed learning\nrate guided sampling that adaptively adjusts the proportion of labeled and\nunlabeled data in each training batch to align with the models' prediction\naccuracy and stability, minimizing the adverse effect caused by inaccurate\npseudo-masks. Extensive experiments on multiple publicly available datasets\ndemonstrate that M&N achieves state-of-the-art performance, outperforming\nthirteen existing semi-supervised segmentation approaches under all different\nsettings. Importantly, ablation studies show that M&N remains model-agnostic,\nallowing seamless integration with different architectures. This ensures its\nadaptability as more advanced models emerge. The code is available at\nhttps://github.com/pakheiyeung/M-N.",
      "pdf_url": "http://arxiv.org/pdf/2509.15167v1",
      "published": "2025-09-18T17:17:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15167v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models",
      "authors": [
        "Haobo Yang",
        "Minghao Guo",
        "Dequan Yang",
        "Wenyu Wang"
      ],
      "abstract": "Contemporary deep learning models have achieved impressive performance in\nimage classification by primarily leveraging statistical regularities within\nlarge datasets, but they rarely incorporate structured insights drawn directly\nfrom perceptual psychology. To explore the potential of perceptually motivated\ninductive biases, we propose integrating classic geometric visual illusions\nwell-studied phenomena from human perception into standard image-classification\ntraining pipelines. Specifically, we introduce a synthetic, parametric\ngeometric-illusion dataset and evaluate three multi-source learning strategies\nthat combine illusion recognition tasks with ImageNet classification\nobjectives. Our experiments reveal two key conceptual insights: (i)\nincorporating geometric illusions as auxiliary supervision systematically\nimproves generalization, especially in visually challenging cases involving\nintricate contours and fine textures; and (ii) perceptually driven inductive\nbiases, even when derived from synthetic stimuli traditionally considered\nunrelated to natural image recognition, can enhance the structural sensitivity\nof both CNN and transformer-based architectures. These results demonstrate a\nnovel integration of perceptual science and machine learning and suggest new\ndirections for embedding perceptual priors into vision model design.",
      "pdf_url": "http://arxiv.org/pdf/2509.15156v1",
      "published": "2025-09-18T17:00:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15156v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring How Audio Effects Alter Emotion with Foundation Models",
      "authors": [
        "Stelios Katsis",
        "Vassilis Lyberatos",
        "Spyridon Kantarelis",
        "Edmund Dervakos",
        "Giorgos Stamou"
      ],
      "abstract": "Audio effects (FX) such as reverberation, distortion, modulation, and dynamic\nrange processing play a pivotal role in shaping emotional responses during\nmusic listening. While prior studies have examined links between low-level\naudio features and affective perception, the systematic impact of audio FX on\nemotion remains underexplored. This work investigates how foundation models -\nlarge-scale neural architectures pretrained on multimodal data - can be\nleveraged to analyze these effects. Such models encode rich associations\nbetween musical structure, timbre, and affective meaning, offering a powerful\nframework for probing the emotional consequences of sound design techniques. By\napplying various probing methods to embeddings from deep learning models, we\nexamine the complex, nonlinear relationships between audio FX and estimated\nemotion, uncovering patterns tied to specific effects and evaluating the\nrobustness of foundation audio models. Our findings aim to advance\nunderstanding of the perceptual impact of audio production practices, with\nimplications for music cognition, performance, and affective computing.",
      "pdf_url": "http://arxiv.org/pdf/2509.15151v1",
      "published": "2025-09-18T16:57:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15151v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance",
      "authors": [
        "Chenxi Song",
        "Yanming Yang",
        "Tong Zhao",
        "Ruibo Li",
        "Chi Zhang"
      ],
      "abstract": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence.",
      "pdf_url": "http://arxiv.org/pdf/2509.15130v1",
      "published": "2025-09-18T16:40:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15130v1",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "The mechanization of science illustrated by the Lean formalization of the multi-graded Proj construction",
      "authors": [
        "Arnaud Mayeux",
        "Jujian Zhang"
      ],
      "abstract": "We formalize the multi-graded Proj construction in Lean4, illustrating\nmechanized mathematics and formalization.",
      "pdf_url": "http://arxiv.org/pdf/2509.15116v1",
      "published": "2025-09-18T16:19:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15116v1",
      "categories": [
        "cs.LO",
        "cs.AI",
        "math.AG"
      ]
    },
    {
      "title": "Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning",
      "authors": [
        "Simin Li",
        "Zheng Yuwei",
        "Zihao Mao",
        "Linhao Wang",
        "Ruixiao Xu",
        "Chengdong Ma",
        "Xin Yu",
        "Yuqing Ma",
        "Qi Dou",
        "Xin Wang",
        "Jie Luo",
        "Bo An",
        "Yaodong Yang",
        "Weifeng Lv",
        "Xianglong Liu"
      ],
      "abstract": "Partial agent failure becomes inevitable when systems scale up, making it\ncrucial to identify the subset of agents whose compromise would most severely\ndegrade overall performance. In this paper, we study this Vulnerable Agent\nIdentification (VAI) problem in large-scale multi-agent reinforcement learning\n(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field\nControl (HAD-MFC), where the upper level involves an NP-hard combinatorial task\nof selecting the most vulnerable agents, and the lower level learns worst-case\nadversarial policies for these agents using mean-field MARL. The two problems\nare coupled together, making HAD-MFC difficult to solve. To solve this, we\nfirst decouple the hierarchical process by Fenchel-Rockafellar transform,\nresulting a regularized mean-field Bellman operator for upper level that\nenables independent learning at each level, thus reducing computational\ncomplexity. We then reformulate the upper-level combinatorial problem as a MDP\nwith dense rewards from our regularized mean-field Bellman operator, enabling\nus to sequentially identify the most vulnerable agents by greedy and RL\nalgorithms. This decomposition provably preserves the optimal solution of the\noriginal HAD-MFC. Experiments show our method effectively identifies more\nvulnerable agents in large-scale MARL and the rule-based system, fooling system\ninto worse failures, and learns a value function that reveals the vulnerability\nof each agent.",
      "pdf_url": "http://arxiv.org/pdf/2509.15103v1",
      "published": "2025-09-18T16:03:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15103v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action",
      "authors": [
        "Chenyue Zhou",
        "Gürkan Solmaz",
        "Flavio Cirillo",
        "Kiril Gashteovski",
        "Jonathan Fürst"
      ],
      "abstract": "Humanitarian Mine Action has generated extensive best-practice knowledge, but\nmuch remains locked in unstructured reports. We introduce TextMine, an\nontology-guided pipeline that uses Large Language Models to extract knowledge\ntriples from HMA texts. TextMine integrates document chunking, domain-aware\nprompting, triple extraction, and both reference-based and LLM-as-a-Judge\nevaluation. We also create the first HMA ontology and a curated dataset of\nreal-world demining reports. Experiments show ontology-aligned prompts boost\nextraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format\nconformance by 20.9% over baselines. While validated on Cambodian reports,\nTextMine can adapt to global demining efforts or other domains, transforming\nunstructured data into structured knowledge.",
      "pdf_url": "http://arxiv.org/pdf/2509.15098v1",
      "published": "2025-09-18T15:55:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15098v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Listening, Imagining \\& Refining: A Heuristic Optimized ASR Correction Framework with LLMs",
      "authors": [
        "Yutong Liu",
        "Ziyue Zhang",
        "Yongbin Yu",
        "Xiangxiang Wang",
        "Yuqing Cai",
        "Nyima Tashi"
      ],
      "abstract": "Automatic Speech Recognition (ASR) systems remain prone to errors that affect\ndownstream applications. In this paper, we propose LIR-ASR, a heuristic\noptimized iterative correction framework using LLMs, inspired by human auditory\nperception. LIR-ASR applies a \"Listening-Imagining-Refining\" strategy,\ngenerating phonetic variants and refining them in context. A heuristic\noptimization with finite state machine (FSM) is introduced to prevent the\ncorrection process from being trapped in local optima and rule-based\nconstraints help maintain semantic fidelity. Experiments on both English and\nChinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of\nup to 1.5 percentage points compared to baselines, demonstrating substantial\naccuracy gains in transcription.",
      "pdf_url": "http://arxiv.org/pdf/2509.15095v1",
      "published": "2025-09-18T15:50:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15095v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support",
      "authors": [
        "Doreen Jirak",
        "Pieter Maes",
        "Armeen Saroukanoff",
        "Dirk van Rooy"
      ],
      "abstract": "As autonomous technologies increasingly shape maritime operations,\nunderstanding why an AI system makes a decision becomes as crucial as what it\ndecides. In complex and dynamic maritime environments, trust in AI depends not\nonly on performance but also on transparency and interpretability. This paper\nhighlights the importance of Explainable AI (XAI) as a foundation for effective\nhuman-machine teaming in the maritime domain, where informed oversight and\nshared understanding are essential. To support the user-centered integration of\nXAI, we propose a domain-specific survey designed to capture maritime\nprofessionals' perceptions of trust, usability, and explainability. Our aim is\nto foster awareness and guide the development of user-centric XAI systems\ntailored to the needs of seafarers and maritime teams.",
      "pdf_url": "http://arxiv.org/pdf/2509.15084v1",
      "published": "2025-09-18T15:42:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15084v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ]
    },
    {
      "title": "Communication Efficient Split Learning of ViTs with Attention-based Double Compression",
      "authors": [
        "Federico Alvetreti",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "abstract": "This paper proposes a novel communication-efficient Split Learning (SL)\nframework, named Attention-based Double Compression (ADC), which reduces the\ncommunication overhead required for transmitting intermediate Vision\nTransformers activations during the SL training process. ADC incorporates two\nparallel compression strategies. The first one merges samples' activations that\nare similar, based on the average attention score calculated in the last client\nlayer; this strategy is class-agnostic, meaning that it can also merge samples\nhaving different classes, without losing generalization ability nor decreasing\nfinal results. The second strategy follows the first and discards the least\nmeaningful tokens, further reducing the communication cost. Combining these\nstrategies not only allows for sending less during the forward pass, but also\nthe gradients are naturally compressed, allowing the whole model to be trained\nwithout additional tuning or approximations of the gradients. Simulation\nresults demonstrate that Attention-based Double Compression outperforms\nstate-of-the-art SL frameworks by significantly reducing communication\noverheads while maintaining high accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2509.15058v1",
      "published": "2025-09-18T15:22:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15058v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ]
    },
    {
      "title": "Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning",
      "authors": [
        "Quincy Hershey",
        "Randy Paffenroth"
      ],
      "abstract": "This paper develops alternative hyperparameters for specifying sparse\nRecurrent Neural Networks (RNNs). These hyperparameters allow for varying\nsparsity within the trainable weight matrices of the model while improving\noverall performance. This architecture enables the definition of a novel\nmetric, hidden proportion, which seeks to balance the distribution of unknowns\nwithin the model and provides significant explanatory power of model\nperformance. Together, the use of the varied sparsity RNN architecture combined\nwith the hidden proportion metric generates significant performance gains while\nimproving performance expectations on an a priori basis. This combined approach\nprovides a path forward towards generalized meta-learning applications and\nmodel optimization based on intrinsic characteristics of the data set,\nincluding input and output dimensions.",
      "pdf_url": "http://arxiv.org/pdf/2509.15057v1",
      "published": "2025-09-18T15:20:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15057v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Credit Card Fraud Detection",
      "authors": [
        "Iva Popova",
        "Hamza A. A. Gardi"
      ],
      "abstract": "Credit card fraud remains a significant challenge due to class imbalance and\nfraudsters mimicking legitimate behavior. This study evaluates five machine\nlearning models - Logistic Regression, Random Forest, XGBoost, K-Nearest\nNeighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using\nundersampling, SMOTE, and a hybrid approach. Our models are evaluated on the\noriginal imbalanced test set to better reflect real-world performance. Results\nshow that the hybrid method achieves the best balance between recall and\nprecision, especially improving MLP and KNN performance.",
      "pdf_url": "http://arxiv.org/pdf/2509.15044v1",
      "published": "2025-09-18T15:08:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15044v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Reinforcement Learning Agent for a 2D Shooter Game",
      "authors": [
        "Thomas Ackermann",
        "Moritz Spang",
        "Hamza A. A. Gardi"
      ],
      "abstract": "Reinforcement learning agents in complex game environments often suffer from\nsparse rewards, training instability, and poor sample efficiency. This paper\npresents a hybrid training approach that combines offline imitation learning\nwith online reinforcement learning for a 2D shooter game agent. We implement a\nmulti-head neural network with separate outputs for behavioral cloning and\nQ-learning, unified by shared feature extraction layers with attention\nmechanisms. Initial experiments using pure deep Q-Networks exhibited\nsignificant instability, with agents frequently reverting to poor policies\ndespite occasional good performance. To address this, we developed a hybrid\nmethodology that begins with behavioral cloning on demonstration data from\nrule-based agents, then transitions to reinforcement learning. Our hybrid\napproach achieves consistently above 70% win rate against rule-based opponents,\nsubstantially outperforming pure reinforcement learning methods which showed\nhigh variance and frequent performance degradation. The multi-head architecture\nenables effective knowledge transfer between learning modes while maintaining\ntraining stability. Results demonstrate that combining demonstration-based\ninitialization with reinforcement learning optimization provides a robust\nsolution for developing game AI agents in complex multi-agent environments\nwhere pure exploration proves insufficient.",
      "pdf_url": "http://arxiv.org/pdf/2509.15042v1",
      "published": "2025-09-18T15:07:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15042v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets",
      "authors": [
        "Juwon Kim",
        "Hyunwook Lee",
        "Hyotaek Jeon",
        "Seungmin Jin",
        "Sungahn Ko"
      ],
      "abstract": "Directional forecasting in financial markets requires both accuracy and\ninterpretability. Before the advent of deep learning, interpretable approaches\nbased on human-defined patterns were prevalent, but their structural vagueness\nand scale ambiguity hindered generalization. In contrast, deep learning models\ncan effectively capture complex dynamics, yet often offer limited transparency.\nTo bridge this gap, we propose a two-stage framework that integrates\nunsupervised pattern extracion with interpretable forecasting. (i) SIMPC\nsegments and clusters multivariate time series, extracting recurrent patterns\nthat are invariant to amplitude scaling and temporal distortion, even under\nvarying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses\nthe initial part of extracted patterns as input and forecasts subsequent\npartial sequences for short-term directional movement. Experiments on Bitcoin\nand three S&P 500 equities demonstrate that our method ranks first or second in\n11 out of 12 metric--dataset combinations, consistently outperforming\nbaselines. Unlike conventional deep learning models that output buy-or-sell\nsignals without interpretable justification, our approach enables transparent\ndecision-making by revealing the underlying pattern structures that drive\npredictive outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2509.15040v1",
      "published": "2025-09-18T15:05:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15040v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews",
      "authors": [
        "Gabriela C. Zapata",
        "Bill Cope",
        "Mary Kalantzis",
        "Duane Searsmith"
      ],
      "abstract": "This study investigates the use of generative AI to support formative\nassessment through machine generated reviews of peer reviews in graduate online\ncourses in a public university in the United States. Drawing on Systemic\nFunctional Linguistics and Appraisal Theory, we analyzed 120 metareviews to\nexplore how generative AI feedback constructs meaning across ideational,\ninterpersonal, and textual dimensions. The findings suggest that generative AI\ncan approximate key rhetorical and relational features of effective human\nfeedback, offering directive clarity while also maintaining a supportive\nstance. The reviews analyzed demonstrated a balance of praise and constructive\ncritique, alignment with rubric expectations, and structured staging that\nforegrounded student agency. By modeling these qualities, AI metafeedback has\nthe potential to scaffold feedback literacy and enhance leaner engagement with\npeer review.",
      "pdf_url": "http://arxiv.org/pdf/2509.15035v1",
      "published": "2025-09-18T15:00:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15035v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Sample Efficient Experience Replay in Non-stationary Environments",
      "authors": [
        "Tianyang Duan",
        "Zongyuan Zhang",
        "Songxiao Guo",
        "Yuanye Zhao",
        "Zheng Lin",
        "Zihan Fang",
        "Yi Liu",
        "Dianxin Luan",
        "Dong Huang",
        "Heming Cui",
        "Yong Cui"
      ],
      "abstract": "Reinforcement learning (RL) in non-stationary environments is challenging, as\nchanging dynamics and rewards quickly make past experiences outdated.\nTraditional experience replay (ER) methods, especially those using TD-error\nprioritization, struggle to distinguish between changes caused by the agent's\npolicy and those from the environment, resulting in inefficient learning under\ndynamic conditions. To address this challenge, we propose the Discrepancy of\nEnvironment Dynamics (DoE), a metric that isolates the effects of environment\nshifts on value functions. Building on this, we introduce Discrepancy of\nEnvironment Prioritized Experience Replay (DEER), an adaptive ER framework that\nprioritizes transitions based on both policy updates and environmental changes.\nDEER uses a binary classifier to detect environment changes and applies\ndistinct prioritization strategies before and after each shift, enabling more\nsample-efficient learning. Experiments on four non-stationary benchmarks\ndemonstrate that DEER further improves the performance of off-policy algorithms\nby 11.54 percent compared to the best-performing state-of-the-art ER methods.",
      "pdf_url": "http://arxiv.org/pdf/2509.15032v1",
      "published": "2025-09-18T14:57:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15032v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models",
      "authors": [
        "Thomas Huber",
        "Christina Niklaus"
      ],
      "abstract": "While LLMs have been extensively studied on general text generation tasks,\nthere is less research on text rewriting, a task related to general text\ngeneration, and particularly on the behavior of models on this task. In this\npaper we analyze what changes LLMs make in a text rewriting setting. We focus\nspecifically on argumentative texts and their improvement, a task named\nArgument Improvement (ArgImp). We present CLEAR: an evaluation pipeline\nconsisting of 57 metrics mapped to four linguistic levels: lexical, syntactic,\nsemantic and pragmatic. This pipeline is used to examine the qualities of\nLLM-rewritten arguments on a broad set of argumentation corpora and compare the\nbehavior of different LLMs on this task and analyze the behavior of different\nLLMs on this task in terms of linguistic levels. By taking all four linguistic\nlevels into consideration, we find that the models perform ArgImp by shortening\nthe texts while simultaneously increasing average word length and merging\nsentences. Overall we note an increase in the persuasion and coherence\ndimensions.",
      "pdf_url": "http://arxiv.org/pdf/2509.15027v1",
      "published": "2025-09-18T14:53:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15027v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering",
      "authors": [
        "Xuanting Xie",
        "Bingheng Li",
        "Erlin Pan",
        "Rui Hou",
        "Wenyu Chen",
        "Zhao Kang"
      ],
      "abstract": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.",
      "pdf_url": "http://arxiv.org/pdf/2509.15024v1",
      "published": "2025-09-18T14:51:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15024v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "title": "Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation",
      "authors": [
        "Vasiliki Ismiroglou",
        "Malte Pedersen",
        "Stefan H. Bengtson",
        "Andreas Aakerberg",
        "Thomas B. Moeslund"
      ],
      "abstract": "In recent years, the underwater image formation model has found extensive use\nin the generation of synthetic underwater data. Although many approaches focus\non scenes primarily affected by discoloration, they often overlook the model's\nability to capture the complex, distance-dependent visibility loss present in\nhighly turbid environments. In this work, we propose an improved synthetic data\ngeneration pipeline that includes the commonly omitted forward scattering term,\nwhile also considering a nonuniform medium. Additionally, we collected the\nBUCKET dataset under controlled turbidity conditions to acquire real turbid\nfootage with the corresponding reference images. Our results demonstrate\nqualitative improvements over the reference model, particularly under\nincreasing turbidity, with a selection rate of 82. 5\\% by survey participants.\nData and code can be accessed on the project page:\nvap.aau.dk/sea-ing-through-scattered-rays.",
      "pdf_url": "http://arxiv.org/pdf/2509.15011v1",
      "published": "2025-09-18T14:42:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.15011v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making",
      "authors": [
        "Xiao Wu",
        "Ting-Zhu Huang",
        "Liang-Jian Deng",
        "Yanyuan Qiao",
        "Imran Razzak",
        "Yutong Xie"
      ],
      "abstract": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC.",
      "pdf_url": "http://arxiv.org/pdf/2509.14998v1",
      "published": "2025-09-18T14:33:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14998v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Blockchain-Enabled Explainable AI for Trusted Healthcare Systems",
      "authors": [
        "Md Talha Mohsin"
      ],
      "abstract": "This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)\nfor healthcare systems to tackle two essential challenges confronting health\ninformation networks: safe data exchange and comprehensible AI-driven clinical\ndecision-making. Our architecture incorporates blockchain, ensuring patient\nrecords are immutable, auditable, and tamper-proof, alongside Explainable AI\n(XAI) methodologies that yield transparent and clinically relevant model\npredictions. By incorporating security assurances and interpretability\nrequirements into a unified optimization pipeline, BXHF ensures both data-level\ntrust (by verified and encrypted record sharing) and decision-level trust (with\nauditable and clinically aligned explanations). Its hybrid edge-cloud\narchitecture allows for federated computation across different institutions,\nenabling collaborative analytics while protecting patient privacy. We\ndemonstrate the framework's applicability through use cases such as\ncross-border clinical research networks, uncommon illness detection and\nhigh-risk intervention decision support. By ensuring transparency,\nauditability, and regulatory compliance, BXHF improves the credibility, uptake,\nand effectiveness of AI in healthcare, laying the groundwork for safer and more\nreliable clinical decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2509.14987v1",
      "published": "2025-09-18T14:17:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14987v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation",
      "authors": [
        "João Damião Almeida",
        "Egidio Falotico",
        "Cecilia Laschi",
        "José Santos-Victor"
      ],
      "abstract": "In-hand manipulation tasks, particularly in human-inspired robotic systems,\nmust rely on distributed tactile sensing to achieve precise control across a\nwide variety of tasks. However, the optimal configuration of this network of\nsensors is a complex problem, and while the fingertips are a common choice for\nplacing sensors, the contribution of tactile information from other regions of\nthe hand is often overlooked. This work investigates the impact of tactile\nfeedback from various regions of the fingers and palm in performing in-hand\nobject reorientation tasks. We analyze how sensory feedback from different\nparts of the hand influences the robustness of deep reinforcement learning\ncontrol policies and investigate the relationship between object\ncharacteristics and optimal sensor placement. We identify which tactile sensing\nconfigurations contribute to improving the efficiency and accuracy of\nmanipulation. Our results provide valuable insights for the design and use of\nanthropomorphic end-effectors with enhanced manipulation capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2509.14984v1",
      "published": "2025-09-18T14:13:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14984v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation",
      "authors": [
        "Ju Dong",
        "Lei Zhang",
        "Liding Zhang",
        "Yao Ling",
        "Yu Fu",
        "Kaixin Bai",
        "Zoltán-Csaba Márton",
        "Zhenshan Bing",
        "Zhaopeng Chen",
        "Alois Christian Knoll",
        "Jianwei Zhang"
      ],
      "abstract": "Mobile manipulation requires the coordinated control of a mobile base and a\nrobotic arm while simultaneously perceiving both global scene context and\nfine-grained object details. Existing single-view approaches often fail in\nunstructured environments due to limited fields of view, exploration, and\ngeneralization abilities. Moreover, classical controllers, although stable,\nstruggle with efficiency and manipulability near singularities. To address\nthese challenges, we propose M4Diffuser, a hybrid framework that integrates a\nMulti-View Diffusion Policy with a novel Reduced and Manipulability-aware QP\n(ReM-QP) controller for mobile manipulation. The diffusion policy leverages\nproprioceptive states and complementary camera perspectives with both\nclose-range object details and global scene context to generate task-relevant\nend-effector goals in the world frame. These high-level goals are then executed\nby the ReM-QP controller, which eliminates slack variables for computational\nefficiency and incorporates manipulability-aware preferences for robustness\nnear singularities. Comprehensive experiments in simulation and real-world\nenvironments show that M4Diffuser achieves 7 to 56 percent higher success rates\nand reduces collisions by 3 to 31 percent over baselines. Our approach\ndemonstrates robust performance for smooth whole-body coordination, and strong\ngeneralization to unseen tasks, paving the way for reliable mobile manipulation\nin unstructured environments. Details of the demo and supplemental material are\navailable on our project website https://sites.google.com/view/m4diffuser.",
      "pdf_url": "http://arxiv.org/pdf/2509.14980v1",
      "published": "2025-09-18T14:09:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14980v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching",
      "authors": [
        "Xingwu Zhang",
        "Guanxuan Li",
        "Zhuocheng Zhang",
        "Zijun Long"
      ],
      "abstract": "The rapidly growing number of product categories in large-scale e-commerce\nmakes accurate object identification for automated packing in warehouses\nsubstantially more difficult. As the catalog grows, intra-class variability and\na long tail of rare or visually similar items increase, and when combined with\ndiverse packaging, cluttered containers, frequent occlusion, and large\nviewpoint changes-these factors amplify discrepancies between query and\nreference images, causing sharp performance drops for methods that rely solely\non 2D appearance features. Thus, we propose RoboEye, a two-stage identification\nframework that dynamically augments 2D semantic features with domain-adapted 3D\nreasoning and lightweight adapters to bridge training deployment gaps. In the\nfirst stage, we train a large vision model to extract 2D features for\ngenerating candidate rankings. A lightweight 3D-feature-awareness module then\nestimates 3D feature quality and predicts whether 3D re-ranking is necessary,\npreventing performance degradation and avoiding unnecessary computation. When\ninvoked, the second stage uses our robot 3D retrieval transformer, comprising a\n3D feature extractor that produces geometry-aware dense features and a\nkeypoint-based matcher that computes keypoint-correspondence confidences\nbetween query and reference images instead of conventional cosine-similarity\nscoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior\nstate of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,\navoiding reliance on explicit 3D inputs and reducing deployment costs. The code\nused in this paper is publicly available at:\nhttps://github.com/longkukuhi/RoboEye.",
      "pdf_url": "http://arxiv.org/pdf/2509.14966v1",
      "published": "2025-09-18T13:59:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14966v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles",
      "authors": [
        "Filip Naudot",
        "Andreas Brännström",
        "Vicenç Torra",
        "Timotheus Kampik"
      ],
      "abstract": "We present functions that quantify the contribution of a set of arguments in\nquantitative bipolar argumentation graphs to (the final strength of) an\nargument of interest, a so-called topic. Our set contribution functions are\ngeneralizations of existing functions that quantify the contribution of a\nsingle contributing argument to a topic. Accordingly, we generalize existing\ncontribution function principles for set contribution functions and provide a\ncorresponding principle-based analysis. We introduce new principles specific to\nset-based functions that focus on properties pertaining to the interaction of\narguments within a set. Finally, we sketch how the principles play out across\ndifferent set contribution functions given a recommendation system application\nscenario.",
      "pdf_url": "http://arxiv.org/pdf/2509.14963v1",
      "published": "2025-09-18T13:52:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14963v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Discrete optimal transport is a strong audio adversarial attack",
      "authors": [
        "Anton Selitskiy",
        "Akib Shahriyar",
        "Jishnuraj Prakasan"
      ],
      "abstract": "In this paper, we show that discrete optimal transport (DOT) is an effective\nblack-box adversarial attack against modern audio anti-spoofing countermeasures\n(CMs). Our attack operates as a post-processing, distribution-alignment step:\nframe-level WavLM embeddings of generated speech are aligned to an unpaired\nbona fide pool via entropic OT and a top-$k$ barycentric projection, then\ndecoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with\nAASIST baselines, DOT yields consistently high equal error rate (EER) across\ndatasets and remains competitive after CM fine-tuning, outperforming several\nconventional attacks in cross-dataset transfer. Ablation analysis highlights\nthe practical impact of vocoder overlap. Results indicate that\ndistribution-level alignment is a powerful and stable attack surface for\ndeployed CMs.",
      "pdf_url": "http://arxiv.org/pdf/2509.14959v1",
      "published": "2025-09-18T13:46:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14959v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems",
      "authors": [
        "Diego Gosmar",
        "Deborah A. Dahl"
      ],
      "abstract": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time.",
      "pdf_url": "http://arxiv.org/pdf/2509.14956v1",
      "published": "2025-09-18T13:39:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14956v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening",
      "authors": [
        "Xiaolei Xu",
        "Chaoyue Niu",
        "Guy J. Brown",
        "Hector Romero",
        "Ning Ma"
      ],
      "abstract": "Obstructive sleep apnoea (OSA) is a prevalent condition with significant\nhealth consequences, yet many patients remain undiagnosed due to the complexity\nand cost of over-night polysomnography. Acoustic-based screening provides a\nscalable alternative, yet performance is limited by environmental noise and the\nlack of physiological context. Respiratory effort is a key signal used in\nclinical scoring of OSA events, but current approaches require additional\ncontact sensors that reduce scalability and patient comfort. This paper\npresents the first study to estimate respiratory effort directly from nocturnal\naudio, enabling physiological context to be recovered from sound alone. We\npropose a latent-space fusion framework that integrates the estimated effort\nembeddings with acoustic features for OSA detection. Using a dataset of 157\nnights from 103 participants recorded in home environments, our respiratory\neffort estimator achieves a concordance correlation coefficient of 0.48,\ncapturing meaningful respiratory dynamics. Fusing effort and audio improves\nsensitivity and AUC over audio-only baselines, especially at low\napnoea-hypopnoea index thresholds. The proposed approach requires only\nsmartphone audio at test time, which enables sensor-free, scalable, and\nlongitudinal OSA monitoring.",
      "pdf_url": "http://arxiv.org/pdf/2509.14944v1",
      "published": "2025-09-18T13:31:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14944v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers",
      "authors": [
        "Minh-Khoi Pham",
        "Tai Tan Mai",
        "Martin Crane",
        "Rob Brennan",
        "Marie E. Ward",
        "Una Geary",
        "Declan Byrne",
        "Brian O Connell",
        "Colm Bergin",
        "Donncha Creagh",
        "Nick McDonald",
        "Marija Bezbradica"
      ],
      "abstract": "Carbapenemase-Producing Enterobacteriace poses a critical concern for\ninfection prevention and control in hospitals. However, predictive modeling of\npreviously highlighted CPE-associated risks such as readmission, mortality, and\nextended length of stay (LOS) remains underexplored, particularly with modern\ndeep learning approaches. This study introduces an eXplainable AI modeling\nframework to investigate CPE impact on patient outcomes from Electronic Medical\nRecords data of an Irish hospital. We analyzed an inpatient dataset from an\nIrish acute hospital, incorporating diagnostic codes, ward transitions, patient\ndemographics, infection-related variables and contact network features. Several\nTransformer-based architectures were benchmarked alongside traditional machine\nlearning models. Clinical outcomes were predicted, and XAI techniques were\napplied to interpret model decisions. Our framework successfully demonstrated\nthe utility of Transformer-based models, with TabTransformer consistently\noutperforming baselines across multiple clinical prediction tasks, especially\nfor CPE acquisition (AUROC and sensitivity). We found infection-related\nfeatures, including historical hospital exposure, admission context, and\nnetwork centrality measures, to be highly influential in predicting patient\noutcomes and CPE acquisition risk. Explainability analyses revealed that\nfeatures like \"Area of Residence\", \"Admission Ward\" and prior admissions are\nkey risk factors. Network variables like \"Ward PageRank\" also ranked highly,\nreflecting the potential value of structural exposure information. This study\npresents a robust and explainable AI framework for analyzing complex EMR data\nto identify key risk factors and predict CPE-related outcomes. Our findings\nunderscore the superior performance of the Transformer models and highlight the\nimportance of diverse clinical and network features.",
      "pdf_url": "http://arxiv.org/pdf/2509.14942v1",
      "published": "2025-09-18T13:29:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14942v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Cross-Modal Knowledge Distillation for Speech Large Language Models",
      "authors": [
        "Enzhi Wang",
        "Qicheng Li",
        "Zhiyuan Tang",
        "Yuhang Jia"
      ],
      "abstract": "In this work, we present the first systematic evaluation of catastrophic\nforgetting and modality inequivalence in speech large language models, showing\nthat introducing speech capabilities can degrade knowledge and reasoning even\nwhen inputs remain textual, and performance further decreases with spoken\nqueries. To address these challenges, we propose a cross-modal knowledge\ndistillation framework that leverages both text-to-text and speech-to-text\nchannels to transfer knowledge from a text-based teacher model to a speech LLM.\nExtensive experiments on dialogue and audio understanding tasks validate the\neffectiveness of our approach in preserving textual knowledge, improving\ncross-modal alignment, and enhancing reasoning in speech-based interactions.",
      "pdf_url": "http://arxiv.org/pdf/2509.14930v1",
      "published": "2025-09-18T13:07:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14930v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Patent Language Model Pretraining with ModernBERT",
      "authors": [
        "Amirhossein Yousefiramandi",
        "Ciaran Cooney"
      ],
      "abstract": "Transformer-based language models such as BERT have become foundational in\nNLP, yet their performance degrades in specialized domains like patents, which\ncontain long, technical, and legally structured text. Prior approaches to\npatent NLP have primarily relied on fine-tuning general-purpose models or\ndomain-adapted variants pretrained with limited data. In this work, we pretrain\n3 domain-specific masked language models for patents, using the ModernBERT\narchitecture and a curated corpus of over 60 million patent records. Our\napproach incorporates architectural optimizations, including FlashAttention,\nrotary embeddings, and GLU feed-forward layers. We evaluate our models on four\ndownstream patent classification tasks. Our model, ModernBERT-base-PT,\nconsistently outperforms the general-purpose ModernBERT baseline on three out\nof four datasets and achieves competitive performance with a baseline\nPatentBERT. Additional experiments with ModernBERT-base-VX and\nMosaic-BERT-large demonstrate that scaling the model size and customizing the\ntokenizer further enhance performance on selected tasks. Notably, all\nModernBERT variants retain substantially faster inference over - 3x that of\nPatentBERT - underscoring their suitability for time-sensitive applications.\nThese results underscore the benefits of domain-specific pretraining and\narchitectural improvements for patent-focused NLP tasks.",
      "pdf_url": "http://arxiv.org/pdf/2509.14926v1",
      "published": "2025-09-18T13:04:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14926v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Back to Ear: Perceptually Driven High Fidelity Music Reconstruction",
      "authors": [
        "Kangdi Wang",
        "Zhiyue Wu",
        "Dinghao Zhou",
        "Rui Lin",
        "Junyu Dai",
        "Tao Jiang"
      ],
      "abstract": "Variational Autoencoders (VAEs) are essential for large-scale audio tasks\nlike diffusion-based generation. However, existing open-source models often\nneglect auditory perceptual aspects during training, leading to weaknesses in\nphase accuracy and stereophonic spatial representation. To address these\nchallenges, we propose {\\epsilon}ar-VAE, an open-source music signal\nreconstruction model that rethinks and optimizes the VAE training paradigm. Our\ncontributions are threefold: (i) A K-weighting perceptual filter applied prior\nto loss calculation to align the objective with auditory perception. (ii) Two\nnovel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss\nusing its derivatives--Instantaneous Frequency and Group Delay--for precision.\n(iii) A new spectral supervision paradigm where magnitude is supervised by all\nfour Mid/Side/Left/Right components, while phase is supervised only by the LR\ncomponents. Experiments show {\\epsilon}ar-VAE at 44.1kHz substantially\noutperforms leading open-source models across diverse metrics, showing\nparticular strength in reconstructing high-frequency harmonics and the spatial\ncharacteristics.",
      "pdf_url": "http://arxiv.org/pdf/2509.14912v1",
      "published": "2025-09-18T12:41:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14912v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation",
      "authors": [
        "Ye Shen",
        "Junying Wang",
        "Farong Wen",
        "Yijin Guo",
        "Qi Jia",
        "Zicheng Zhang",
        "Guangtao Zhai"
      ],
      "abstract": "The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred\nthe creation of numerous benchmarks. However, conventional full-coverage\nQuestion-Answering evaluations suffer from high redundancy and low efficiency.\nInspired by human interview processes, we propose a multi-to-one interview\nparadigm for efficient MLLM evaluation. Our framework consists of (i) a\ntwo-stage interview strategy with pre-interview and formal interview phases,\n(ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an\nadaptive mechanism for question difficulty-level chosen. Experiments on\ndifferent benchmarks show that the proposed paradigm achieves significantly\nhigher correlation with full-coverage results than random sampling, with\nimprovements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the\nnumber of required questions. These findings demonstrate that the proposed\nparadigm provides a reliable and efficient alternative for large-scale MLLM\nbenchmarking.",
      "pdf_url": "http://arxiv.org/pdf/2509.14886v1",
      "published": "2025-09-18T12:07:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14886v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "AI-Driven Multi-Agent Vehicular Planning for Battery Efficiency and QoS in 6G Smart Cities",
      "authors": [
        "Rohin Gillgallon",
        "Giacomo Bergami",
        "Reham Almutairi",
        "Graham Morgan"
      ],
      "abstract": "While simulators exist for vehicular IoT nodes communicating with the Cloud\nthrough Edge nodes in a fully-simulated osmotic architecture, they often lack\nsupport for dynamic agent planning and optimisation to minimise vehicular\nbattery consumption while ensuring fair communication times. Addressing these\nchallenges requires extending current simulator architectures with AI\nalgorithms for both traffic prediction and dynamic agent planning. This paper\npresents an extension of SimulatorOrchestrator (SO) to meet these requirements.\nPreliminary results over a realistic urban dataset show that utilising\nvehicular planning algorithms can lead to improved battery and QoS performance\ncompared with traditional shortest path algorithms. The additional inclusion of\ndesirability areas enabled more ambulances to be routed to their target\ndestinations while utilising less energy to do so, compared to traditional and\nweighted algorithms without desirability considerations.",
      "pdf_url": "http://arxiv.org/pdf/2509.14877v1",
      "published": "2025-09-18T11:46:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14877v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting",
      "authors": [
        "Qianyang Li",
        "Xingjun Zhang",
        "Shaoxun Wang",
        "Jia Wei"
      ],
      "abstract": "We conducted rigorous ablation studies to validate DPANet's key components\n(Table \\ref{tab:ablation-study}). The full model consistently outperforms all\nvariants. To test our dual-domain hypothesis, we designed two specialized\nversions: a Temporal-Only model (fusing two identical temporal pyramids) and a\nFrequency-Only model (fusing two spectral pyramids). Both variants\nunderperformed significantly, confirming that the fusion of heterogeneous\ntemporal and frequency information is critical. Furthermore, replacing the\ncross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the\nmost severe performance degradation. This result underscores that our\ninteractive fusion block is the most essential component.",
      "pdf_url": "http://arxiv.org/pdf/2509.14868v1",
      "published": "2025-09-18T11:35:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14868v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study",
      "authors": [
        "Zhengwei Wang",
        "Gang Wu"
      ],
      "abstract": "Graph Transformers (GTs) show considerable potential in graph representation\nlearning. The architecture of GTs typically integrates Graph Neural Networks\n(GNNs) with global attention mechanisms either in parallel or as a precursor to\nattention mechanisms, yielding a local-and-global or local-to-global attention\nscheme. However, as the global attention mechanism primarily captures\nlong-range dependencies between nodes, these integration schemes may suffer\nfrom information loss, where the local neighborhood information learned by GNN\ncould be diluted by the attention mechanism. Therefore, we propose G2LFormer,\nfeaturing a novel global-to-local attention scheme where the shallow network\nlayers use attention mechanisms to capture global information, while the deeper\nlayers employ GNN modules to learn local structural information, thereby\npreventing nodes from ignoring their immediate neighbors. An effective\ncross-layer information fusion strategy is introduced to allow local layers to\nretain beneficial information from global layers and alleviate information\nloss, with acceptable trade-offs in scalability. To validate the feasibility of\nthe global-to-local attention scheme, we compare G2LFormer with\nstate-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The\nresults indicate that G2LFormer exhibits excellent performance while keeping\nlinear complexity.",
      "pdf_url": "http://arxiv.org/pdf/2509.14863v1",
      "published": "2025-09-18T11:30:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14863v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MARIC: Multi-Agent Reasoning for Image Classification",
      "authors": [
        "Wonduk Seo",
        "Minhyeong Yu",
        "Hyunjin An",
        "Seunghyun Lee"
      ],
      "abstract": "Image classification has traditionally relied on parameter-intensive model\ntraining, requiring large-scale annotated datasets and extensive fine tuning to\nachieve competitive performance. While recent vision language models (VLMs)\nalleviate some of these constraints, they remain limited by their reliance on\nsingle pass representations, often failing to capture complementary aspects of\nvisual content. In this paper, we introduce Multi Agent based Reasoning for\nImage Classification (MARIC), a multi agent framework that reformulates image\nclassification as a collaborative reasoning process. MARIC first utilizes an\nOutliner Agent to analyze the global theme of the image and generate targeted\nprompts. Based on these prompts, three Aspect Agents extract fine grained\ndescriptions along distinct visual dimensions. Finally, a Reasoning Agent\nsynthesizes these complementary outputs through integrated reflection step,\nproducing a unified representation for classification. By explicitly\ndecomposing the task into multiple perspectives and encouraging reflective\nsynthesis, MARIC mitigates the shortcomings of both parameter-heavy training\nand monolithic VLM reasoning. Experiments on 4 diverse image classification\nbenchmark datasets demonstrate that MARIC significantly outperforms baselines,\nhighlighting the effectiveness of multi-agent visual reasoning for robust and\ninterpretable image classification.",
      "pdf_url": "http://arxiv.org/pdf/2509.14860v1",
      "published": "2025-09-18T11:27:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14860v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ]
    },
    {
      "title": "MeanFlowSE: one-step generative speech enhancement via conditional mean flow",
      "authors": [
        "Duojia Li",
        "Shenghui Lu",
        "Hongchen Pan",
        "Zongyi Zhan",
        "Qingyang Hong",
        "Lin Li"
      ],
      "abstract": "Multistep inference is a bottleneck for real-time generative speech\nenhancement because flow- and diffusion-based systems learn an instantaneous\nvelocity field and therefore rely on iterative ordinary differential equation\n(ODE) solvers. We introduce MeanFlowSE, a conditional generative model that\nlearns the average velocity over finite intervals along a trajectory. Using a\nJacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a\nlocal training objective that directly supervises finite-interval displacement\nwhile remaining consistent with the instantaneous-field constraint on the\ndiagonal. At inference, MeanFlowSE performs single-step generation via a\nbackward-in-time displacement, removing the need for multistep solvers; an\noptional few-step variant offers additional refinement. On VoiceBank-DEMAND,\nthe single-step model achieves strong intelligibility, fidelity, and perceptual\nquality with substantially lower computational cost than multistep baselines.\nThe method requires no knowledge distillation or external teachers, providing\nan efficient, high-fidelity framework for real-time generative speech\nenhancement.",
      "pdf_url": "http://arxiv.org/pdf/2509.14858v1",
      "published": "2025-09-18T11:24:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14858v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "title": "Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support",
      "authors": [
        "Xianrong Yao",
        "Dong She",
        "Chenxu Zhang",
        "Yimeng Zhang",
        "Yueru Sun",
        "Noman Ahmed",
        "Yang Gao",
        "Zhanpeng Jin"
      ],
      "abstract": "Empathy is critical for effective mental health support, especially when\naddressing Long Counseling Texts (LCTs). However, existing Large Language\nModels (LLMs) often generate replies that are semantically fluent but lack the\nstructured reasoning necessary for genuine psychological support, particularly\nin a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel\nframework that integrates a Chain-of-Empathy (CoE) reasoning process with\nReinforcement Learning (RL) to enhance response quality for LCTs. Inspired by\ncognitive-behavioral therapy, our CoE paradigm guides the model to sequentially\nreason about a help-seeker's emotions, causes, and intentions, making its\nthinking process both transparent and interpretable. Our framework is empowered\nby a new large-scale Chinese dataset, Empathy-QA, and a two-stage training\nprocess. First, Supervised Fine-Tuning instills the CoE's reasoning structure.\nSubsequently, RL, guided by a dedicated reward model, refines the therapeutic\nrelevance and contextual appropriateness of the final responses. Experiments\nshow that Empathy-R1 achieves strong performance on key automatic metrics. More\nimportantly, human evaluations confirm its superiority, showing a clear\npreference over strong baselines and achieving a Win@1 rate of 44.30% on our\nnew benchmark. By enabling interpretable and contextually nuanced responses,\nEmpathy-R1 represents a significant advancement in developing responsible and\ngenuinely beneficial AI for mental health support.",
      "pdf_url": "http://arxiv.org/pdf/2509.14851v1",
      "published": "2025-09-18T11:16:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14851v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "[Re] Improving Interpretation Faithfulness for Vision Transformers",
      "authors": [
        "Izabela Kurek",
        "Wojciech Trejter",
        "Stipe Frkovic",
        "Andro Erdelez"
      ],
      "abstract": "This work aims to reproduce the results of Faithful Vision Transformers\n(FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for\nVision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate\nclaims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised\nSmoothing (DDS) improves interpretability robustness to (1) attacks in a\nsegmentation task and (2) perturbation and attacks in a classification task. We\nalso extend the original study by investigating the authors' claims that adding\nDDS to any interpretability method can improve its robustness under attack.\nThis is tested on baseline methods and the recently proposed Attribution\nRollout method. In addition, we measure the computational costs and\nenvironmental impact of obtaining an FViT through DDS. Our results broadly\nagree with the original study's findings, although minor discrepancies were\nfound and discussed.",
      "pdf_url": "http://arxiv.org/pdf/2509.14846v1",
      "published": "2025-09-18T11:11:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14846v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution",
      "authors": [
        "Hongjun Wang",
        "Jiyuan Chen",
        "Zhengwei Yin",
        "Xuan Song",
        "Yinqiang Zheng"
      ],
      "abstract": "Generalizable Image Super-Resolution aims to enhance model generalization\ncapabilities under unknown degradations. To achieve this goal, the models are\nexpected to focus only on image content-related features instead of overfitting\ndegradations. Recently, numerous approaches such as Dropout and Feature\nAlignment have been proposed to suppress models' natural tendency to overfit\ndegradations and yield promising results. Nevertheless, these works have\nassumed that models overfit to all degradation types (e.g., blur, noise, JPEG),\nwhile through careful investigations in this paper, we discover that models\npredominantly overfit to noise, largely attributable to its distinct\ndegradation pattern compared to other degradation types. In this paper, we\npropose a targeted feature denoising framework, comprising noise detection and\ndenoising modules. Our approach presents a general solution that can be\nseamlessly integrated with existing super-resolution models without requiring\narchitectural modifications. Our framework demonstrates superior performance\ncompared to previous regularization-based methods across five traditional\nbenchmarks and datasets, encompassing both synthetic and real-world scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2509.14841v1",
      "published": "2025-09-18T11:04:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2509.14841v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}