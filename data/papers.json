{
  "last_updated": "2025-05-09T00:51:33.367061",
  "papers": [
    {
      "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning",
      "authors": [
        "Zhenghao Xing",
        "Xiaowei Hu",
        "Chi-Wing Fu",
        "Wenhai Wang",
        "Jifeng Dai",
        "Pheng-Ann Heng"
      ],
      "abstract": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research.",
      "pdf_url": "http://arxiv.org/pdf/2505.04623v1",
      "published": "2025-05-07T17:59:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04623v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ]
    },
    {
      "title": "Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond",
      "authors": [
        "Jessie Richter-Powell",
        "Antonio Torralba",
        "Jonathan Lorraine"
      ],
      "abstract": "We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)\nto text-conditioned audio diffusion models. While SDS was initially designed\nfor text-to-3D generation using image diffusion, its core idea of distilling a\npowerful generative prior into a separate parametric representation extends to\nthe audio domain. Leveraging a single pretrained model, Audio-SDS enables a\nbroad range of tasks without requiring specialized datasets. In particular, we\ndemonstrate how Audio-SDS can guide physically informed impact sound\nsimulations, calibrate FM-synthesis parameters, and perform prompt-specified\nsource separation. Our findings illustrate the versatility of\ndistillation-based methods across modalities and establish a robust foundation\nfor future work using generative priors in audio tasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.04621v1",
      "published": "2025-05-07T17:59:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04621v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS",
        "68T07",
        "I.2.6; H.5.5; H.5.1"
      ]
    },
    {
      "title": "WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales",
      "authors": [
        "Drew Prinster",
        "Xing Han",
        "Anqi Liu",
        "Suchi Saria"
      ],
      "abstract": "Responsibly deploying artificial intelligence (AI) / machine learning (ML)\nsystems in high-stakes settings arguably requires not only proof of system\nreliability, but moreover continual, post-deployment monitoring to quickly\ndetect and address any unsafe behavior. Statistical methods for nonparametric\nchange-point detection -- especially the tools of conformal test martingales\n(CTMs) and anytime-valid inference -- offer promising approaches to this\nmonitoring task. However, existing methods are restricted to monitoring limited\nhypothesis classes or ``alarm criteria,'' such as data shifts that violate\ncertain exchangeability assumptions, or do not allow for online adaptation in\nresponse to shifts. In this paper, we expand the scope of these monitoring\nmethods by proposing a weighted generalization of conformal test martingales\n(WCTMs), which lay a theoretical foundation for online monitoring for any\nunexpected changepoints in the data distribution while controlling\nfalse-alarms. For practical applications, we propose specific WCTM algorithms\nthat accommodate online adaptation to mild covariate shifts (in the marginal\ninput distribution) while raising alarms in response to more severe shifts,\nsuch as concept shifts (in the conditional label distribution) or extreme\n(out-of-support) covariate shifts that cannot be easily adapted to. On\nreal-world datasets, we demonstrate improved performance relative to\nstate-of-the-art baselines.",
      "pdf_url": "http://arxiv.org/pdf/2505.04608v1",
      "published": "2025-05-07T17:53:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04608v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions",
      "authors": [
        "Peter Barnett",
        "Aaron Scher"
      ],
      "abstract": "Humanity appears to be on course to soon develop AI systems that\nsubstantially outperform human experts in all cognitive domains and activities.\nWe believe the default trajectory has a high likelihood of catastrophe,\nincluding human extinction. Risks come from failure to control powerful AI\nsystems, misuse of AI by malicious rogue actors, war between great powers, and\nauthoritarian lock-in. This research agenda has two aims: to describe the\nstrategic landscape of AI development and to catalog important governance\nresearch questions. These questions, if answered, would provide important\ninsight on how to successfully reduce catastrophic risks.\n  We describe four high-level scenarios for the geopolitical response to\nadvanced AI development, cataloging the research questions most relevant to\neach. Our favored scenario involves building the technical, legal, and\ninstitutional infrastructure required to internationally restrict dangerous AI\ndevelopment and deployment (which we refer to as an Off Switch), which leads\ninto an internationally coordinated Halt on frontier AI activities at some\npoint in the future. The second scenario we describe is a US National Project\nfor AI, in which the US Government races to develop advanced AI systems and\nestablish unilateral control over global AI development. We also describe two\nadditional scenarios: a Light-Touch world similar to that of today and a Threat\nof Sabotage situation where countries use sabotage and deterrence to slow AI\ndevelopment.\n  In our view, apart from the Off Switch and Halt scenario, all of these\ntrajectories appear to carry an unacceptable risk of catastrophic harm. Urgent\naction is needed from the US National Security community and AI governance\necosystem to answer key research questions, build the capability to halt\ndangerous AI activities, and prepare for international AI agreements.",
      "pdf_url": "http://arxiv.org/pdf/2505.04592v1",
      "published": "2025-05-07T17:35:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04592v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization",
      "authors": [
        "Wenjun Cao"
      ],
      "abstract": "Reinforcement learning (RL) fine-tuning transforms large language models\nwhile creating a vulnerability we experimentally verify: Our experiment shows\nthat malicious RL fine-tuning dismantles safety guardrails with remarkable\nefficiency, requiring only 50 steps and minimal adversarial prompts, with\nharmful escalating from 0-2 to 7-9. This attack vector particularly threatens\nopen-source models with parameter-level access. Existing defenses targeting\nsupervised fine-tuning prove ineffective against RL's dynamic feedback\nmechanisms. We introduce Reward Neutralization, the first defense framework\nspecifically designed against RL fine-tuning attacks, establishing concise\nrejection patterns that render malicious reward signals ineffective. Our\napproach trains models to produce minimal-information rejections that attackers\ncannot exploit, systematically neutralizing attempts to optimize toward harmful\noutputs. Experiments validate that our approach maintains low harmful scores\n(no greater than 2) after 200 attack steps, while standard models rapidly\ndeteriorate. This work provides the first constructive proof that robust\ndefense against increasingly accessible RL attacks is achievable, addressing a\ncritical security gap for open-weight models.",
      "pdf_url": "http://arxiv.org/pdf/2505.04578v1",
      "published": "2025-05-07T17:18:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04578v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Purity Law for Generalizable Neural TSP Solvers",
      "authors": [
        "Wenzhao Liu",
        "Haoran Li",
        "Congying Han",
        "Zicheng Zhang",
        "Anqi Li",
        "Tiande Guo"
      ],
      "abstract": "Achieving generalization in neural approaches across different scales and\ndistributions remains a significant challenge for the Traveling Salesman\nProblem~(TSP). A key obstacle is that neural networks often fail to learn\nrobust principles for identifying universal patterns and deriving optimal\nsolutions from diverse instances. In this paper, we first uncover Purity Law\n(PuLa), a fundamental structural principle for optimal TSP solutions, defining\nthat edge prevalence grows exponentially with the sparsity of surrounding\nvertices. Statistically validated across diverse instances, PuLa reveals a\nconsistent bias toward local sparsity in global optima. Building on this\ninsight, we propose Purity Policy Optimization~(PUPO), a novel training\nparadigm that explicitly aligns characteristics of neural solutions with PuLa\nduring the solution construction process to enhance generalization. Extensive\nexperiments demonstrate that PUPO can be seamlessly integrated with popular\nneural solvers, significantly enhancing their generalization performance\nwithout incurring additional computational overhead during inference.",
      "pdf_url": "http://arxiv.org/pdf/2505.04558v1",
      "published": "2025-05-07T16:46:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04558v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions",
      "authors": [
        "Shanyu Han",
        "Yang Liu",
        "Xiang Yu"
      ],
      "abstract": "We propose a reinforcement learning (RL) framework under a broad class of\nrisk objectives, characterized by convex scoring functions. This class covers\nmany common risk measures, such as variance, Expected Shortfall, entropic\nValue-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,\nwe consider an augmented state space and an auxiliary variable and recast the\nproblem as a two-state optimization problem. We propose a customized\nActor-Critic algorithm and establish some theoretical approximation guarantees.\nA key theoretical contribution is that our results do not require the Markov\ndecision process to be continuous. Additionally, we propose an auxiliary\nvariable sampling method inspired by the alternating minimization algorithm,\nwhich is convergent under certain conditions. We validate our approach in\nsimulation experiments with a financial application in statistical arbitrage\ntrading, demonstrating the effectiveness of the algorithm.",
      "pdf_url": "http://arxiv.org/pdf/2505.04553v1",
      "published": "2025-05-07T16:31:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04553v1",
      "categories": [
        "q-fin.MF",
        "cs.AI",
        "q-fin.RM"
      ]
    },
    {
      "title": "Qualitative Analysis of $ω$-Regular Objectives on Robust MDPs",
      "authors": [
        "Ali Asadi",
        "Krishnendu Chatterjee",
        "Ehsan Kafshdar Goharshady",
        "Mehrdad Karrabi",
        "Ali Shafiee"
      ],
      "abstract": "Robust Markov Decision Processes (RMDPs) generalize classical MDPs that\nconsider uncertainties in transition probabilities by defining a set of\npossible transition functions. An objective is a set of runs (or infinite\ntrajectories) of the RMDP, and the value for an objective is the maximal\nprobability that the agent can guarantee against the adversarial environment.\nWe consider (a) reachability objectives, where given a target set of states,\nthe goal is to eventually arrive at one of them; and (b) parity objectives,\nwhich are a canonical representation for $\\omega$-regular objectives. The\nqualitative analysis problem asks whether the objective can be ensured with\nprobability 1.\n  In this work, we study the qualitative problem for reachability and parity\nobjectives on RMDPs without making any assumption over the structures of the\nRMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first\npresent efficient algorithms with oracle access to uncertainty sets that solve\nqualitative problems of reachability and parity objectives. We then report\nexperimental results demonstrating the effectiveness of our oracle-based\napproach on classical RMDP examples from the literature scaling up to thousands\nof states.",
      "pdf_url": "http://arxiv.org/pdf/2505.04539v1",
      "published": "2025-05-07T16:15:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04539v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review",
      "authors": [
        "Josh McGiff",
        "Nikola S. Nikolov"
      ],
      "abstract": "Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies.",
      "pdf_url": "http://arxiv.org/pdf/2505.04531v1",
      "published": "2025-05-07T16:04:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04531v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving",
      "authors": [
        "Qi Liu",
        "Xinhao Zheng",
        "Renqiu Xia",
        "Xingzhi Qi",
        "Qinxiang Cao",
        "Junchi Yan"
      ],
      "abstract": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
      "pdf_url": "http://arxiv.org/pdf/2505.04528v1",
      "published": "2025-05-07T16:02:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04528v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ]
    },
    {
      "title": "DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once",
      "authors": [
        "Qi Zhou",
        "Yukai Shi",
        "Xiaojun Yang",
        "Xiaoyu Xian",
        "Lunjia Liao",
        "Ruimao Zhang",
        "Liang Lin"
      ],
      "abstract": "Visible and infrared image fusion is one of the most crucial tasks in the\nfield of image fusion, aiming to generate fused images with clear structural\ninformation and high-quality texture features for high-level vision tasks.\nHowever, when faced with severe illumination degradation in visible images, the\nfusion results of existing image fusion methods often exhibit blurry and dim\nvisual effects, posing major challenges for autonomous driving. To this end, a\nDarkness-Free network is proposed to handle Visible and infrared image\ndisentanglement and fusion all at Once (DFVO), which employs a cascaded\nmulti-task approach to replace the traditional two-stage cascaded training\n(enhancement and fusion), addressing the issue of information entropy loss\ncaused by hierarchical data transmission. Specifically, we construct a\nlatent-common feature extractor (LCFE) to obtain latent features for the\ncascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised\nto acquire high-frequency semantic information. Secondly, we design a hyper\ncross-attention module (HCAM) to extract low-frequency information and preserve\ntexture features from source images. Finally, a relevant loss function is\ndesigned to guide the holistic network learning, thereby achieving better image\nfusion. Extensive experiments demonstrate that our proposed approach\noutperforms state-of-the-art alternatives in terms of qualitative and\nquantitative evaluations. Particularly, DFVO can generate clearer, more\ninformative, and more evenly illuminated fusion results in the dark\nenvironments, achieving best performance on the LLVIP dataset with 63.258 dB\nPSNR and 0.724 CC, providing more effective information for high-level vision\ntasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.",
      "pdf_url": "http://arxiv.org/pdf/2505.04526v1",
      "published": "2025-05-07T15:59:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04526v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "On some improvements to Unbounded Minimax",
      "authors": [
        "Quentin Cohen-Solal",
        "Tristan Cazenave"
      ],
      "abstract": "This paper presents the first experimental evaluation of four previously\nuntested modifications of Unbounded Best-First Minimax algorithm. This\nalgorithm explores the game tree by iteratively expanding the most promising\nsequences of actions based on the current partial game tree. We first evaluate\nthe use of transposition tables, which convert the game tree into a directed\nacyclic graph by merging duplicate states. Second, we compare the original\nalgorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which\ndiffers in its backpropagation strategy: instead of stopping when a stable\nvalue is encountered, it updates values up to the root. This change slightly\nimproves performance when value ties or transposition tables are involved.\nThird, we assess replacing the exact terminal evaluation function with the\nlearned heuristic function. While beneficial when exact evaluations are costly,\nthis modification reduces performance in inexpensive settings. Finally, we\nexamine the impact of the completion technique that prioritizes resolved\nwinning states and avoids resolved losing states. This technique also improves\nperformance. Overall, our findings highlight how targeted modifications can\nenhance the efficiency of Unbounded Best-First Minimax.",
      "pdf_url": "http://arxiv.org/pdf/2505.04525v1",
      "published": "2025-05-07T15:59:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04525v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Defining and Quantifying Creative Behavior in Popular Image Generators",
      "authors": [
        "Aditi Ramaswamy",
        "Hana Chockler",
        "Melane Navaratnarajah"
      ],
      "abstract": "Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition.",
      "pdf_url": "http://arxiv.org/pdf/2505.04497v2",
      "published": "2025-05-07T15:20:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04497v2",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4.m; I.2.m"
      ]
    },
    {
      "title": "Model-Based AI planning and Execution Systems for Robotics",
      "authors": [
        "Or Wertheim",
        "Ronen I. Brafman"
      ],
      "abstract": "Model-based planning and execution systems offer a principled approach to\nbuilding flexible autonomous robots that can perform diverse tasks by\nautomatically combining a host of basic skills. This idea is almost as old as\nmodern robotics. Yet, while diverse general-purpose reasoning architectures\nhave been proposed since, general-purpose systems that are integrated with\nmodern robotic platforms have emerged only recently, starting with the\ninfluential ROSPlan system. Since then, a growing number of model-based systems\nfor robot task-level control have emerged. In this paper, we consider the\ndiverse design choices and issues existing systems attempt to address, the\ndifferent solutions proposed so far, and suggest avenues for future\ndevelopment.",
      "pdf_url": "http://arxiv.org/pdf/2505.04493v1",
      "published": "2025-05-07T15:17:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04493v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments",
      "authors": [
        "Ziyi Zhang",
        "Zhen Sun",
        "Zongmin Zhang",
        "Zifan Peng",
        "Yuemeng Zhao",
        "Zichun Wang",
        "Zeren Luo",
        "Ruiting Zuo",
        "Xinlei He"
      ],
      "abstract": "The visually impaired population, especially the severely visually impaired,\nis currently large in scale, and daily activities pose significant challenges\nfor them. Although many studies use large language and vision-language models\nto assist the blind, most focus on static content and fail to meet real-time\nperception needs in dynamic and complex environments, such as daily activities.\nTo provide them with more effective intelligent assistance, it is imperative to\nincorporate advanced visual understanding technologies. Although real-time\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\nunderstanding, no prior work has systematically evaluated their effectiveness\nin assisting visually impaired individuals. In this work, we conduct the first\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\ncovering three categories of assistive tasks for visually impaired individuals:\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\nevaluate the models in both closed-world and open-world scenarios, further\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\nOne key issue we identify is the difficulty current models face in perceiving\npotential hazards in dynamic environments. To address this, we build an\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\nthat enables the model to proactively detect environmental risks. We hope this\nwork provides valuable insights and inspiration for future research in this\nfield.",
      "pdf_url": "http://arxiv.org/pdf/2505.04488v1",
      "published": "2025-05-07T15:03:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04488v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.MM"
      ]
    },
    {
      "title": "Efficient Flow Matching using Latent Variables",
      "authors": [
        "Anirban Samaddar",
        "Yixuan Sun",
        "Viktor Nilsson",
        "Sandeep Madireddy"
      ],
      "abstract": "Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features.",
      "pdf_url": "http://arxiv.org/pdf/2505.04486v1",
      "published": "2025-05-07T14:59:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04486v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution",
      "authors": [
        "Zhikai Zhao",
        "Chuanbo Hua",
        "Federico Berto",
        "Kanghoon Lee",
        "Zihan Ma",
        "Jiachen Li",
        "Jinkyoo Park"
      ],
      "abstract": "Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo.",
      "pdf_url": "http://arxiv.org/pdf/2505.04480v1",
      "published": "2025-05-07T14:51:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04480v1",
      "categories": [
        "cs.AI",
        "cs.NE",
        "cs.RO"
      ]
    },
    {
      "title": "Spectral and Temporal Denoising for Differentially Private Optimization",
      "authors": [
        "Hyeju Shin",
        "Kyudan Jung",
        "Seongwon Yun",
        "Juyoung Yun"
      ],
      "abstract": "This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a\ndifferentially private optimization method that addresses the challenge of\npreserving performance in DP-SGD, where added noise typically degrades model\nutility. FFTKF integrates frequency-domain noise shaping with Kalman filtering\nto enhance gradient quality while preserving $(\\varepsilon, \\delta)$-DP\nguarantees. It employs a high-frequency shaping mask in the Fourier domain to\nconcentrate differential privacy noise in less informative spectral components,\npreserving low-frequency gradient signals. A scalar-gain Kalman filter with\nfinite-difference Hessian approximation further refines the denoised gradients.\nWith a per-iteration complexity of $\\mathcal{O}(d \\log d)$, FFTKF demonstrates\nimproved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,\nand Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.\nTheoretical analysis confirms that FFTKF maintains equivalent privacy\nguarantees while achieving a tighter privacy-utility trade-off through reduced\nnoise and controlled bias.",
      "pdf_url": "http://arxiv.org/pdf/2505.04468v1",
      "published": "2025-05-07T14:38:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04468v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "cs.NE",
        "math.IT"
      ]
    },
    {
      "title": "Discriminative Ordering Through Ensemble Consensus",
      "authors": [
        "Louis Ohl",
        "Fredrik Lindsten"
      ],
      "abstract": "Evaluating the performance of clustering models is a challenging task where\nthe outcome depends on the definition of what constitutes a cluster. Due to\nthis design, current existing metrics rarely handle multiple clustering models\nwith diverse cluster definitions, nor do they comply with the integration of\nconstraints when available. In this work, we take inspiration from consensus\nclustering and assume that a set of clustering models is able to uncover hidden\nstructures in the data. We propose to construct a discriminative ordering\nthrough ensemble clustering based on the distance between the connectivity of a\nclustering model and the consensus matrix. We first validate the proposed\nmethod with synthetic scenarios, highlighting that the proposed score ranks the\nmodels that best match the consensus first. We then show that this simple\nranking score significantly outperforms other scoring methods when comparing\nsets of different clustering algorithms that are not restricted to a fixed\nnumber of clusters and is compatible with clustering constraints.",
      "pdf_url": "http://arxiv.org/pdf/2505.04464v1",
      "published": "2025-05-07T14:35:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04464v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "62H30",
        "G.3"
      ]
    },
    {
      "title": "A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities",
      "authors": [
        "Pengfei Jiao",
        "Hongjiang Chen",
        "Xuan Guo",
        "Zhidong Zhao",
        "Dongxiao He",
        "Di Jin"
      ],
      "abstract": "Temporal interaction graphs (TIGs), defined by sequences of timestamped\ninteraction events, have become ubiquitous in real-world applications due to\ntheir capability to model complex dynamic system behaviors. As a result,\ntemporal interaction graph representation learning (TIGRL) has garnered\nsignificant attention in recent years. TIGRL aims to embed nodes in TIGs into\nlow-dimensional representations that effectively preserve both structural and\ntemporal information, thereby enhancing the performance of downstream tasks\nsuch as classification, prediction, and clustering within constantly evolving\ndata environments. In this paper, we begin by introducing the foundational\nconcepts of TIGs and emphasize the critical role of temporal dependencies. We\nthen propose a comprehensive taxonomy of state-of-the-art TIGRL methods,\nsystematically categorizing them based on the types of information utilized\nduring the learning process to address the unique challenges inherent to TIGs.\nTo facilitate further research and practical applications, we curate the source\nof datasets and benchmarks, providing valuable resources for empirical\ninvestigations. Finally, we examine key open challenges and explore promising\nresearch directions in TIGRL, laying the groundwork for future advancements\nthat have the potential to shape the evolution of this field.",
      "pdf_url": "http://arxiv.org/pdf/2505.04461v1",
      "published": "2025-05-07T14:31:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04461v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ]
    },
    {
      "title": "Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform",
      "authors": [
        "Yohannis Telila",
        "Tommaso Cucinotta",
        "Davide Bacciu"
      ],
      "abstract": "Automatic music transcription (AMT) is the problem of analyzing an audio\nrecording of a musical piece and detecting notes that are being played. AMT is\na challenging problem, particularly when it comes to polyphonic music. The goal\nof AMT is to produce a score representation of a music piece, by analyzing a\nsound signal containing multiple notes played simultaneously. In this work, we\ndesign a processing pipeline that can transform classical piano audio files in\n.wav format into a music score representation. The features from the audio\nsignals are extracted using the constant-Q transform, and the resulting\ncoefficients are used as an input to the convolutional neural network (CNN)\nmodel.",
      "pdf_url": "http://arxiv.org/pdf/2505.04451v1",
      "published": "2025-05-07T14:20:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04451v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ]
    },
    {
      "title": "FedBWO: Enhancing Communication Efficiency in Federated Learning",
      "authors": [
        "Vahideh Hayyolalam",
        "Öznur Özkasap"
      ],
      "abstract": "Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a\nshared model is collaboratively trained by various clients using their local\ndatasets while keeping the data private. Considering resource-constrained\ndevices, FL clients often suffer from restricted transmission capacity. Aiming\nto enhance the system performance, the communication between clients and server\nneeds to be diminished. Current FL strategies transmit a tremendous amount of\ndata (model weights) within the FL process, which needs a high communication\nbandwidth. Considering resource constraints, increasing the number of clients\nand, consequently, the amount of data (model weights) can lead to a bottleneck.\nIn this paper, we introduce the Federated Black Widow Optimization (FedBWO)\ntechnique to decrease the amount of transmitted data by transmitting only a\nperformance score rather than the local model weights from clients. FedBWO\nemploys the BWO algorithm to improve local model updates. The conducted\nexperiments prove that FedBWO remarkably improves the performance of the global\nmodel and the communication efficiency of the overall system. According to the\nexperimental outcomes, FedBWO enhances the global model accuracy by an average\nof 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically\ndecreases the communication cost compared to other methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.04435v1",
      "published": "2025-05-07T14:02:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04435v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Recognizing Ornaments in Vocal Indian Art Music with Active Annotation",
      "authors": [
        "Sumit Kumar",
        "Parampreet Singh",
        "Vipul Arora"
      ],
      "abstract": "Ornamentations, embellishments, or microtonal inflections are essential to\nmelodic expression across many musical traditions, adding depth, nuance, and\nemotional impact to performances. Recognizing ornamentations in singing voices\nis key to MIR, with potential applications in music pedagogy, singer\nidentification, genre classification, and controlled singing voice generation.\nHowever, the lack of annotated datasets and specialized modeling approaches\nremains a major obstacle for progress in this research area. In this work, we\nintroduce R\\=aga Ornamentation Detection (ROD), a novel dataset comprising\nIndian classical music recordings curated by expert musicians. The dataset is\nannotated using a custom Human-in-the-Loop tool for six vocal ornaments marked\nas event-based labels. Using this dataset, we develop an ornamentation\ndetection model based on deep time-series analysis, preserving ornament\nboundaries during the chunking of long audio recordings. We conduct experiments\nusing different train-test configurations within the ROD dataset and also\nevaluate our approach on a separate, manually annotated dataset of Indian\nclassical concert recordings. Our experimental results support the superior\nperformance of our proposed approach over the baseline CRNN.",
      "pdf_url": "http://arxiv.org/pdf/2505.04419v1",
      "published": "2025-05-07T13:52:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04419v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models",
      "authors": [
        "Xiaoyu Xu",
        "Minxin Du",
        "Qingqing Ye",
        "Haibo Hu"
      ],
      "abstract": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2505.04416v1",
      "published": "2025-05-07T13:51:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04416v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation",
      "authors": [
        "Aidar Valeev",
        "Roman Garaev",
        "Vadim Lomshakov",
        "Irina Piontkovskaya",
        "Vladimir Ivanov",
        "Israel Adewuyi"
      ],
      "abstract": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++.",
      "pdf_url": "http://arxiv.org/pdf/2505.04406v1",
      "published": "2025-05-07T13:42:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04406v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "High-speed multiwavelength photonic temporal integration using silicon photonics",
      "authors": [
        "Yi Zhang",
        "Nikolaos Farmakidis",
        "Ioannis Roumpos",
        "Miltiadis Moralis-Pegios",
        "Apostolos Tsakyridis",
        "June Sang Lee",
        "Bowei Dong",
        "Yuhan He",
        "Samarth Aggarwal",
        "Nikolaos Pleros",
        "Harish Bhaskaran"
      ],
      "abstract": "Optical systems have been pivotal for energy-efficient computing, performing\nhigh-speed, parallel operations in low-loss carriers. While these predominantly\nanalog optical accelerators bypass digitization to perform parallel\nfloating-point computations, scaling optical hardware to map large-vector sizes\nfor AI tasks remains challenging. Here, we overcome this limitation by\nunfolding scalar operations in time and introducing a\nphotonic-heater-in-lightpath (PHIL) unit for all-optical temporal integration.\nCounterintuitively, we exploit a slow heat dissipation process to integrate\noptical signals modulated at 50 GHz bridging the speed gap between the widely\napplied thermo-optic effects and ultrafast photonics. This architecture\nsupports optical end-to-end signal processing, eliminates inefficient\nelectro-optical conversions, and enables both linear and nonlinear operations\nwithin a unified framework. Our results demonstrate a scalable path towards\nhigh-speed photonic computing through thermally driven integration.",
      "pdf_url": "http://arxiv.org/pdf/2505.04405v1",
      "published": "2025-05-07T13:39:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04405v1",
      "categories": [
        "physics.optics",
        "cs.AI",
        "physics.app-ph"
      ]
    },
    {
      "title": "In-Context Adaptation to Concept Drift for Learned Database Operations",
      "authors": [
        "Jiaqi Zhu",
        "Shaofeng Cai",
        "Yanyan Shen",
        "Gang Chen",
        "Fang Deng",
        "Beng Chin Ooi"
      ],
      "abstract": "Machine learning has demonstrated transformative potential for database\noperations, such as query optimization and in-database data analytics. However,\ndynamic database environments, characterized by frequent updates and evolving\ndata distributions, introduce concept drift, which leads to performance\ndegradation for learned models and limits their practical applicability.\nAddressing this challenge requires efficient frameworks capable of adapting to\nshifting concepts while minimizing the overhead of retraining or fine-tuning.\n  In this paper, we propose FLAIR, an online adaptation framework that\nintroduces a new paradigm called \\textit{in-context adaptation} for learned\ndatabase operations. FLAIR leverages the inherent property of data systems,\ni.e., immediate availability of execution results for predictions, to enable\ndynamic context construction. By formalizing adaptation as $f:(\\mathbf{x} \\,|\n\\,\\mathcal{C}_t) \\to \\mathbf{y}$, with $\\mathcal{C}_t$ representing a dynamic\ncontext memory, FLAIR delivers predictions aligned with the current concept,\neliminating the need for runtime parameter optimization. To achieve this, FLAIR\nintegrates two key modules: a Task Featurization Module for encoding\ntask-specific features into standardized representations, and a Dynamic\nDecision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly\nusing contextual information at runtime. Extensive experiments across key\ndatabase tasks demonstrate that FLAIR outperforms state-of-the-art baselines,\nachieving up to 5.2x faster adaptation and reducing error by 22.5% for\ncardinality estimation.",
      "pdf_url": "http://arxiv.org/pdf/2505.04404v1",
      "published": "2025-05-07T13:36:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04404v1",
      "categories": [
        "cs.DB",
        "cs.AI"
      ]
    },
    {
      "title": "Deep residual learning with product units",
      "authors": [
        "Ziyuan Li",
        "Uwe Jaekel",
        "Babette Dellen"
      ],
      "abstract": "We propose a deep product-unit residual neural network (PURe) that integrates\nproduct units into residual blocks to improve the expressiveness and parameter\nefficiency of deep convolutional networks. Unlike standard summation neurons,\nproduct units enable multiplicative feature interactions, potentially offering\na more powerful representation of complex patterns. PURe replaces conventional\nconvolutional layers with 2D product units in the second layer of each residual\nblock, eliminating nonlinear activation functions to preserve structural\ninformation. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,\nPURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper\nResNet152, while converging nearly five times faster and demonstrating strong\nrobustness to Poisson noise. On ImageNet, PURe architectures outperform\nstandard ResNet models at similar depths, with PURe34 achieving a top-1\naccuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet\nvariants (ResNet50, ResNet101) while utilizing significantly fewer parameters\nand computational resources. On CIFAR-10, PURe consistently outperforms ResNet\nvariants across varying depths, with PURe272 reaching 95.01% test accuracy,\ncomparable to ResNet1001 but at less than half the model size. These results\ndemonstrate that PURe achieves a favorable balance between accuracy,\nefficiency, and robustness. Compared to traditional residual networks, PURe not\nonly achieves competitive classification performance with faster convergence\nand fewer parameters, but also demonstrates greater robustness to noise. Its\neffectiveness across diverse datasets highlights the potential of\nproduct-unit-based architectures for scalable and reliable deep learning in\ncomputer vision.",
      "pdf_url": "http://arxiv.org/pdf/2505.04397v1",
      "published": "2025-05-07T13:21:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04397v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
      "authors": [
        "Dario Garcia-Gasulla",
        "Jordi Bayarri-Planas",
        "Ashwin Kumar Gururajan",
        "Enrique Lopez-Cuena",
        "Adrian Tormos",
        "Daniel Hinjos",
        "Pablo Bernabeu-Perez",
        "Anna Arias-Duart",
        "Pablo Agustin Martin-Torres",
        "Marta Gonzalez-Mallo",
        "Sergio Alvarez-Napagao",
        "Eduard Ayguadé-Parra",
        "Ulises Cortés"
      ],
      "abstract": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
      "pdf_url": "http://arxiv.org/pdf/2505.04388v1",
      "published": "2025-05-07T13:13:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04388v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic",
      "authors": [
        "Mohammad Elayan",
        "Wissam Kontar"
      ],
      "abstract": "Transportation systems have long been shaped by complexity and heterogeneity,\ndriven by the interdependency of agent actions and traffic outcomes. The\ndeployment of automated vehicles (AVs) in such systems introduces a new\nchallenge: achieving consensus across safety, interaction quality, and traffic\nperformance. In this work, we position consensus as a fundamental property of\nthe traffic system and aim to quantify it. We use high-resolution trajectory\ndata from the Third Generation Simulation (TGSIM) dataset to empirically\nanalyze AV and human-driven vehicle (HDV) behavior at a signalized urban\nintersection and around vulnerable road users (VRUs). Key metrics, including\nTime-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,\nheadways, and string stability, are evaluated across the three performance\ndimensions. Results show that full consensus across safety, interaction, and\nperformance is rare, with only 1.63% of AV-VRU interaction frames meeting all\nthree conditions. These findings highlight the need for AV models that\nexplicitly balance multi-dimensional performance in mixed-traffic environments.\nFull reproducibility is supported via our open-source codebase on\nhttps://github.com/wissamkontar/Consensus-AV-Analysis.",
      "pdf_url": "http://arxiv.org/pdf/2505.04379v1",
      "published": "2025-05-07T12:59:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04379v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise",
      "authors": [
        "Moseli Mots'oehli",
        "Hope Mogale",
        "Kyungim Baek"
      ],
      "abstract": "Fine-tuning pre-trained convolutional neural networks on ImageNet for\ndownstream tasks is well-established. Still, the impact of model size on the\nperformance of vision transformers in similar scenarios, particularly under\nlabel noise, remains largely unexplored. Given the utility and versatility of\ntransformer architectures, this study investigates their practicality under\nlow-budget constraints and noisy labels. We explore how classification accuracy\nand calibration are affected by symmetric label noise in active learning\nsettings, evaluating four vision transformer configurations (Base and Large\nwith 16x16 and 32x32 patch sizes) and three Swin Transformer configurations\n(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label\nnoise rates. Our findings show that larger ViT models (ViTl32 in particular)\nconsistently outperform their smaller counterparts in both accuracy and\ncalibration, even under moderate to high label noise, while Swin Transformers\nexhibit weaker robustness across all noise levels. We find that smaller patch\nsizes do not always lead to better performance, as ViTl16 performs consistently\nworse than ViTl32 while incurring a higher computational cost. We also find\nthat information-based Active Learning strategies only provide meaningful\naccuracy improvements at moderate label noise rates, but they result in poorer\ncalibration compared to models trained on randomly acquired labels, especially\nat high label noise rates. We hope these insights provide actionable guidance\nfor practitioners looking to deploy vision transformers in resource-constrained\nenvironments, where balancing model complexity, label noise, and compute\nefficiency is critical in model fine-tuning or distillation.",
      "pdf_url": "http://arxiv.org/pdf/2505.04375v1",
      "published": "2025-05-07T12:53:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04375v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows",
      "authors": [
        "Wenhao Li",
        "Bo Jin",
        "Mingyi Hong",
        "Changhong Lu",
        "Xiangfeng Wang"
      ],
      "abstract": "This position paper argues that optimization problem solving can transition\nfrom expert-dependent to evolutionary agentic workflows. Traditional\noptimization practices rely on human specialists for problem formulation,\nalgorithm selection, and hyperparameter tuning, creating bottlenecks that\nimpede industrial adoption of cutting-edge methods. We contend that an\nevolutionary agentic workflow, powered by foundation models and evolutionary\nsearch, can autonomously navigate the optimization space, comprising problem,\nformulation, algorithm, and hyperparameter spaces. Through case studies in\ncloud resource scheduling and ADMM parameter adaptation, we demonstrate how\nthis approach can bridge the gap between academic innovation and industrial\nimplementation. Our position challenges the status quo of human-centric\noptimization workflows and advocates for a more scalable, adaptive approach to\nsolving real-world optimization problems.",
      "pdf_url": "http://arxiv.org/pdf/2505.04354v1",
      "published": "2025-05-07T12:07:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04354v1",
      "categories": [
        "math.OC",
        "cs.AI"
      ]
    },
    {
      "title": "Uncertain Machine Ethics Planning",
      "authors": [
        "Simon Kolker",
        "Louise A. Dennis",
        "Ramon Fraga Pereira",
        "Mengwei Xu"
      ],
      "abstract": "Machine Ethics decisions should consider the implications of uncertainty over\ndecisions. Decisions should be made over sequences of actions to reach\npreferable outcomes long term. The evaluation of outcomes, however, may invoke\none or more moral theories, which might have conflicting judgements. Each\ntheory will require differing representations of the ethical situation. For\nexample, Utilitarianism measures numerical values, Deontology analyses duties,\nand Virtue Ethics emphasises moral character. While balancing potentially\nconflicting moral considerations, decisions may need to be made, for example,\nto achieve morally neutral goals with minimal costs. In this paper, we\nformalise the problem as a Multi-Moral Markov Decision Process and a\nMulti-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm\nbased on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical\nRetrospection procedure for ethical reasoning under uncertainty. Our approach\nis validated by a case study from Machine Ethics literature: the problem of\nwhether to steal insulin for someone who needs it.",
      "pdf_url": "http://arxiv.org/pdf/2505.04352v1",
      "published": "2025-05-07T12:03:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04352v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Multi-Granular Attention based Heterogeneous Hypergraph Neural Network",
      "authors": [
        "Hong Jin",
        "Kaicheng Zhou",
        "Jie Yin",
        "Lan You",
        "Zhifeng Zhou"
      ],
      "abstract": "Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong\nabilities to learn node representations by effectively extracting complex\nstructural and semantic information in heterogeneous graphs. Most of the\nprevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging\nmeta-path based message passing to learn latent node representations. However,\ndue to the pairwise nature of meta-paths, these models fail to capture\nhigh-order relations among nodes, resulting in suboptimal performance.\nAdditionally, the challenge of ``over-squashing'', where long-range message\npassing in HeteGNNs leads to severe information distortion, further limits the\nefficacy of these models. To address these limitations, this paper proposes\nMGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural\nNetwork for heterogeneous graph representation learning. MGA-HHN introduces two\nkey innovations: (1) a novel approach for constructing meta-path based\nheterogeneous hypergraphs that explicitly models higher-order semantic\ninformation in heterogeneous graphs through multiple views, and (2) a\nmulti-granular attention mechanism that operates at both the node and hyperedge\nlevels. This mechanism enables the model to capture fine-grained interactions\namong nodes sharing the same semantic context within a hyperedge type, while\npreserving the diversity of semantics across different hyperedge types. As\nsuch, MGA-HHN effectively mitigates long-range message distortion and generates\nmore expressive node representations. Extensive experiments on real-world\nbenchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art\nmodels, showcasing its effectiveness in node classification, node clustering\nand visualization tasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.04340v1",
      "published": "2025-05-07T11:42:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04340v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing",
      "authors": [
        "Jacob Glenn Ayers",
        "Buvaneswari A. Ramanan",
        "Manzoor A. Khan"
      ],
      "abstract": "As the adoption of deep learning models has grown beyond human capacity for\nverification, meta-algorithms are needed to ensure reliable model inference.\nConcept drift detection is a field dedicated to identifying statistical shifts\nthat is underutilized in monitoring neural networks that may encounter\ninference data with distributional characteristics diverging from their\ntraining data. Given the wide variety of model architectures, applications, and\ndatasets, it is important that concept drift detection algorithms are adaptable\nto different inference scenarios. In this paper, we introduce an application of\nthe $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection\nmeta-algorithm applied to a multilayer perceptron, a convolutional neural\nnetwork, and a transformer trained for machine vision as they are exposed to\nsimulated drift during inference. To that end, we demonstrate how unexpected\ndrops in accuracy due to concept drift can be detected without directly\nexamining the inference outputs. Our approach enhances safety by ensuring\nmodels are continually evaluated for reliability across varying conditions.",
      "pdf_url": "http://arxiv.org/pdf/2505.04318v1",
      "published": "2025-05-07T11:04:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04318v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "title": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning",
      "authors": [
        "Ruize Zhang",
        "Sirui Xiang",
        "Zelai Xu",
        "Feng Gao",
        "Shilong Ji",
        "Wenhao Tang",
        "Wenbo Ding",
        "Chao Yu",
        "Yu Wang"
      ],
      "abstract": "In this paper, we tackle the problem of learning to play 3v3 multi-drone\nvolleyball, a new embodied competitive task that requires both high-level\nstrategic coordination and low-level agile control. The task is turn-based,\nmulti-agent, and physically grounded, posing significant challenges due to its\nlong-horizon dependencies, tight inter-agent coupling, and the underactuated\ndynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play\n(HCSP), a hierarchical reinforcement learning framework that separates\ncentralized high-level strategic decision-making from decentralized low-level\nmotion control. We design a three-stage population-based training pipeline to\nenable both strategy and skill to emerge from scratch without expert\ndemonstrations: (I) training diverse low-level skills, (II) learning high-level\nstrategy via self-play with fixed low-level controllers, and (III) joint\nfine-tuning through co-self-play. Experiments show that HCSP achieves superior\nperformance, outperforming non-hierarchical self-play and rule-based\nhierarchical baselines with an average 82.9\\% win rate and a 71.5\\% win rate\nagainst the two-stage variant. Moreover, co-self-play leads to emergent team\nbehaviors such as role switching and coordinated formations, demonstrating the\neffectiveness of our hierarchical design and training scheme.",
      "pdf_url": "http://arxiv.org/pdf/2505.04317v1",
      "published": "2025-05-07T11:04:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04317v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge Representation and Reasoning",
      "authors": [
        "Stephen Richard Varey",
        "Alessandro Di Stefano",
        "The Anh Han"
      ],
      "abstract": "In this paper, we introduce KERAIA, a novel framework and software platform\nfor symbolic knowledge engineering designed to address the persistent\nchallenges of representing, reasoning with, and executing knowledge in dynamic,\ncomplex, and context-sensitive environments. The central research question that\nmotivates this work is: How can unstructured, often tacit, human expertise be\neffectively transformed into computationally tractable algorithms that AI\nsystems can efficiently utilise? KERAIA seeks to bridge this gap by building on\nfoundational concepts such as Minsky's frame-based reasoning and K-lines, while\nintroducing significant innovations. These include Clouds of Knowledge for\ndynamic aggregation, Dynamic Relations (DRels) for context-sensitive\ninheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and\nCloud Elaboration for adaptive knowledge transformation. This approach moves\nbeyond the limitations of traditional, often static, knowledge representation\nparadigms. KERAIA is designed with Explainable AI (XAI) as a core principle,\nensuring transparency and interpretability, particularly through the use of\nLoTs. The paper details the framework's architecture, the KSYNTH representation\nlanguage, and the General Purpose Paradigm Builder (GPPB) to integrate diverse\ninference methods within a unified structure. We validate KERAIA's versatility,\nexpressiveness, and practical applicability through detailed analysis of\nmultiple case studies spanning naval warfare simulation, industrial diagnostics\nin water treatment plants, and strategic decision-making in the game of RISK.\nFurthermore, we provide a comparative analysis against established knowledge\nrepresentation paradigms (including ontologies, rule-based systems, and\nknowledge graphs) and discuss the implementation aspects and computational\nconsiderations of the KERAIA platform.",
      "pdf_url": "http://arxiv.org/pdf/2505.04313v1",
      "published": "2025-05-07T10:56:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04313v1",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.SC"
      ]
    },
    {
      "title": "Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning",
      "authors": [
        "Simo Alami C.",
        "Rim Kaddah",
        "Jesse Read",
        "Marie-Paule Cani"
      ],
      "abstract": "We introduce a new architecture for Distributional Reinforcement Learning\n(DistRL) that models return distributions using normalizing flows. This\napproach enables flexible, unbounded support for return distributions, in\ncontrast to categorical approaches like C51 that rely on fixed or bounded\nrepresentations. It also offers richer modeling capacity to capture\nmulti-modality, skewness, and tail behavior than quantile based approaches. Our\nmethod is significantly more parameter-efficient than categorical approaches.\nStandard metrics used to train existing models like KL divergence or\nWasserstein distance either are scale insensitive or have biased sample\ngradients, especially when return supports do not overlap. To address this, we\npropose a novel surrogate for the Cram\\`er distance, that is geometry-aware and\ncomputable directly from the return distribution's PDF, avoiding the costly CDF\ncomputation. We test our model on the ATARI-5 sub-benchmark and show that our\napproach outperforms PDF based models while remaining competitive with quantile\nbased methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.04310v1",
      "published": "2025-05-07T10:49:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04310v1",
      "categories": [
        "cs.AI",
        "math.OC"
      ]
    },
    {
      "title": "Guardians of the Web: The Evolution and Future of Website Information Security",
      "authors": [
        "Md Saiful Islam",
        "Li Xiangdong"
      ],
      "abstract": "Website information security has become a critical concern in the digital\nage. This article explores the evolution of website information security,\nexamining its historical development, current practices, and future directions.\nThe early beginnings from the 1960s to the 1980s laid the groundwork for modern\ncybersecurity, with the development of ARPANET, TCP/IP, public-key\ncryptography, and the first antivirus programs. The 1990s marked a\ntransformative era, driven by the commercialization of the Internet and the\nemergence of web-based services. As the Internet grew, so did the range and\nsophistication of cyber threats, leading to advancements in security\ntechnologies such as the Secure Sockets Layer (SSL) protocol, password\nprotection, and firewalls. Current practices in website information security\ninvolve a multi-layered approach, including encryption, secure coding\npractices, regular security audits, and user education. The future of website\ninformation security is expected to be shaped by emerging technologies such as\nartificial intelligence, blockchain, and quantum computing, as well as the\nincreasing importance of international cooperation and standardization efforts.\nAs cyber threats continue to evolve, ongoing research and innovation in website\ninformation security will be essential to protect sensitive information and\nmaintain trust in the digital world.",
      "pdf_url": "http://arxiv.org/pdf/2505.04308v1",
      "published": "2025-05-07T10:46:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04308v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "F.2.2, I.2.7"
      ]
    },
    {
      "title": "Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning",
      "authors": [
        "Isabella Caranzano",
        "Corrado Pancotti",
        "Cesare Rollo",
        "Flavio Sartori",
        "Pietro Liò",
        "Piero Fariselli",
        "Tiziana Sanavia"
      ],
      "abstract": "Biologically-informed neural networks typically leverage pathway annotations\nto enhance performance in biomedical applications. We hypothesized that the\nbenefits of pathway integration does not arise from its biological relevance,\nbut rather from the sparsity it introduces. We conducted a comprehensive\nanalysis of all relevant pathway-based neural network models for predictive\ntasks, critically evaluating each study's contributions. From this review, we\ncurated a subset of methods for which the source code was publicly available.\nThe comparison of the biologically informed state-of-the-art deep learning\nmodels and their randomized counterparts showed that models based on randomized\ninformation performed equally well as biologically informed ones across\ndifferent metrics and datasets. Notably, in 3 out of the 15 analyzed models,\nthe randomized versions even outperformed their biologically informed\ncounterparts. Moreover, pathway-informed models did not show any clear\nadvantage in interpretability, as randomized models were still able to identify\nrelevant disease biomarkers despite lacking explicit pathway information. Our\nfindings suggest that pathway annotations may be too noisy or inadequately\nexplored by current methods. Therefore, we propose a methodology that can be\napplied to different domains and can serve as a robust benchmark for\nsystematically comparing novel pathway-informed models against their randomized\ncounterparts. This approach enables researchers to rigorously determine whether\nobserved performance improvements can be attributed to biological insights.",
      "pdf_url": "http://arxiv.org/pdf/2505.04300v1",
      "published": "2025-05-07T10:14:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04300v1",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance",
      "authors": [
        "Sofia Jamil",
        "Aryan Dabad",
        "Bollampalli Areen Reddy",
        "Sriparna Saha",
        "Rajiv Misra",
        "Adil A. Shakur"
      ],
      "abstract": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable.",
      "pdf_url": "http://arxiv.org/pdf/2505.04284v1",
      "published": "2025-05-07T09:40:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04284v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Non-stationary Diffusion For Probabilistic Time Series Forecasting",
      "authors": [
        "Weiwei Ye",
        "Zhuopeng Xu",
        "Ning Gui"
      ],
      "abstract": "Due to the dynamics of underlying physics and external influences, the\nuncertainty of time series often varies over time. However, existing Denoising\nDiffusion Probabilistic Models (DDPMs) often fail to capture this\nnon-stationary nature, constrained by their constant variance assumption from\nthe additive noise model (ANM). In this paper, we innovatively utilize the\nLocation-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of\nANM. A diffusion-based probabilistic forecasting framework, termed\nNon-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of\nmodeling the changing pattern of uncertainty. Specifically, NsDiff combines a\ndenoising diffusion-based conditional generative model with a pre-trained\nconditional mean and variance estimator, enabling adaptive endpoint\ndistribution modeling. Furthermore, we propose an uncertainty-aware noise\nschedule, which dynamically adjusts the noise levels to accurately reflect the\ndata uncertainty at each step and integrates the time-varying variances into\nthe diffusion process. Extensive experiments conducted on nine real-world and\nsynthetic datasets demonstrate the superior performance of NsDiff compared to\nexisting approaches. Code is available at https://github.com/wwy155/NsDiff.",
      "pdf_url": "http://arxiv.org/pdf/2505.04278v1",
      "published": "2025-05-07T09:29:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04278v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Object-Shot Enhanced Grounding Network for Egocentric Video",
      "authors": [
        "Yisen Feng",
        "Haoyu Zhang",
        "Meng Liu",
        "Weili Guan",
        "Liqiang Nie"
      ],
      "abstract": "Egocentric video grounding is a crucial task for embodied intelligence\napplications, distinct from exocentric video moment localization. Existing\nmethods primarily focus on the distributional differences between egocentric\nand exocentric videos but often neglect key characteristics of egocentric\nvideos and the fine-grained information emphasized by question-type queries. To\naddress these limitations, we propose OSGNet, an Object-Shot enhanced Grounding\nNetwork for egocentric video. Specifically, we extract object information from\nvideos to enrich video representation, particularly for objects highlighted in\nthe textual query but not directly captured in the video features.\nAdditionally, we analyze the frequent shot movements inherent to egocentric\nvideos, leveraging these features to extract the wearer's attention\ninformation, which enhances the model's ability to perform modality alignment.\nExperiments conducted on three datasets demonstrate that OSGNet achieves\nstate-of-the-art performance, validating the effectiveness of our approach. Our\ncode can be found at https://github.com/Yisen-Feng/OSGNet.",
      "pdf_url": "http://arxiv.org/pdf/2505.04270v1",
      "published": "2025-05-07T09:20:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04270v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper",
      "authors": [
        "Abdulrahman S Almuhaidib",
        "Azlan Mohd Zain",
        "Zalmiyah Zakaria",
        "Izyan Izzati Kamsani",
        "Abdulaziz S Almuhaidib"
      ],
      "abstract": "This, with the ever-increasing sophistication of cyberwar, calls for novel\nsolutions. In this regard, Large Language Models (LLMs) have emerged as a\nhighly promising tool for defensive and offensive cybersecurity-related\nstrategies. While existing literature has focused much on the defensive use of\nLLMs, when it comes to their offensive utilization, very little has been\nreported-namely, concerning Vulnerability Assessment (VA) report validation.\nConsequentially, this paper tries to fill that gap by investigating the\ncapabilities of LLMs in automating and improving the validation process of the\nreport of the VA. From the critical review of the related literature, this\npaper hereby proposes a new approach to using the LLMs in the automation of the\nanalysis and within the validation process of the report of the VA that could\npotentially reduce the number of false positives and generally enhance\nefficiency. These results are promising for LLM automatization for improving\nvalidation on reports coming from VA in order to improve accuracy while\nreducing human effort and security postures. The contribution of this paper\nprovides further evidence about the offensive and defensive LLM capabilities\nand therefor helps in devising more appropriate cybersecurity strategies and\ntools accordingly.",
      "pdf_url": "http://arxiv.org/pdf/2505.04265v1",
      "published": "2025-05-07T09:14:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04265v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering",
      "authors": [
        "Jessica Y. Bo",
        "Tianyu Xu",
        "Ishan Chatterjee",
        "Katrina Passarella-Ward",
        "Achin Kulshrestha",
        "D Shin"
      ],
      "abstract": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces.",
      "pdf_url": "http://arxiv.org/pdf/2505.04260v1",
      "published": "2025-05-07T09:10:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04260v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering",
      "authors": [
        "Krishna Ronanki"
      ],
      "abstract": "Multi-agent autonomous systems (MAS) are better at addressing challenges that\nspans across multiple domains than singular autonomous agents. This holds true\nwithin the field of software engineering (SE) as well. The state-of-the-art\nresearch on MAS within SE focuses on integrating LLMs at the core of autonomous\nagents to create LLM-based multi-agent autonomous (LMA) systems. However, the\nintroduction of LMA systems into SE brings a plethora of challenges. One of the\nmajor challenges is the strategic allocation of tasks between humans and the\nLMA system in a trustworthy manner. To address this challenge, a RACI-based\nframework is proposed in this work in progress article, along with\nimplementation guidelines and an example implementation of the framework. The\nproposed framework can facilitate efficient collaboration, ensure\naccountability, and mitigate potential risks associated with LLM-driven\nautomation while aligning with the Trustworthy AI guidelines. The future steps\nfor this work delineating the planned empirical validation method are also\npresented.",
      "pdf_url": "http://arxiv.org/pdf/2505.04251v1",
      "published": "2025-05-07T08:55:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04251v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "title": "FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning",
      "authors": [
        "Sanghyeon Park",
        "Soo-Mook Moon"
      ],
      "abstract": "Federated learning (FL) enables collaborative model training across\ndistributed clients while preserving data locality. Although FedAvg pioneered\nsynchronous rounds for global model averaging, slower devices can delay\ncollective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by\ncontinuously integrating client updates, yet naive implementations risk client\ndrift due to non-IID data and stale contributions. Some Blockchain-based FL\napproaches (e.g., BRAIN) employ robust weighting or scoring of updates to\nresist malicious or misaligned proposals. However, performance drops can still\npersist under severe data heterogeneity or high staleness, and synchronization\noverhead has emerged as a new concern due to its aggregator-free architectures.\n  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL\nmethod that mitigates these limitations by incorporating two key ideas. First,\nour FastSync strategy eliminates the need to replay past model versions,\nenabling newcomers and infrequent participants to efficiently approximate the\nglobal model. Second, we adopt spherical linear interpolation (SLERP) when\nmerging parameters, preserving models' directions and alleviating destructive\ninterference from divergent local training.\n  Experiments with a CNN image-classification model and a Transformer-based\nlanguage model demonstrate that FRAIN achieves more stable and robust\nconvergence than FedAvg, FedAsync, and BRAIN, especially under harsh\nenvironments: non-IID data distributions, networks that experience delays and\nrequire frequent re-synchronization, and the presence of malicious nodes.",
      "pdf_url": "http://arxiv.org/pdf/2505.04223v1",
      "published": "2025-05-07T08:20:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04223v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay",
      "authors": [
        "Soumik Dey",
        "Hansi Wu",
        "Binbin Li"
      ],
      "abstract": "E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). The relevance\nof advertiser keyphrases plays an important role in preventing the inundation\nof search systems with numerous irrelevant items that compete for attention in\nauctions, in addition to maintaining a healthy seller perception. In this work,\nwe describe the shortcomings of training Advertiser keyphrase relevance filter\nmodels on click/sales/search relevance signals and the importance of aligning\nwith human judgment, as sellers have the power to adopt or reject said\nkeyphrase recommendations. In this study, we frame Advertiser keyphrase\nrelevance as a complex interaction between 3 dynamical systems -- seller\njudgment, which influences seller adoption of our product, Advertising, which\nprovides the keyphrases to bid on, and Search, who holds the auctions for the\nsame keyphrases. This study discusses the practicalities of using human\njudgment via a case study at eBay Advertising and demonstrate that using\nLLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our\nrelevance models achieves a better harmony across the three systems -- provided\nthat they are bound by a meticulous evaluation framework grounded in business\nmetrics.",
      "pdf_url": "http://arxiv.org/pdf/2505.04209v1",
      "published": "2025-05-07T08:03:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04209v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement",
      "authors": [
        "Mustafa Yurdakul",
        "Şakir Tasdemir"
      ],
      "abstract": "Potholes cause vehicle damage and traffic accidents, creating serious safety\nand economic problems. Therefore, early and accurate detection of potholes is\ncrucial. Existing detection methods are usually only based on 2D RGB images and\ncannot accurately analyze the physical characteristics of potholes. In this\npaper, a publicly available dataset of RGB-D images (PothRGBD) is created and\nan improved YOLOv8-based model is proposed for both pothole detection and\npothole physical features analysis. The Intel RealSense D415 depth camera was\nused to collect RGB and depth data from the road surfaces, resulting in a\nPothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable\nfor segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg\narchitecture, which is structurally improved with Dynamic Snake Convolution\n(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit\n(GELU). The proposed model segmented potholes with irregular edge structure\nmore accurately, and performed perimeter and depth measurements on depth maps\nwith high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,\n85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to\n93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in\nprecision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model\nperforms pothole detection as well as perimeter and depth measurement with high\naccuracy and is suitable for real-time applications due to its low model\ncomplexity. In this way, a lightweight and effective model that can be used in\ndeep learning-based intelligent transportation solutions has been acquired.",
      "pdf_url": "http://arxiv.org/pdf/2505.04207v1",
      "published": "2025-05-07T07:58:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04207v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning",
      "authors": [
        "Trinh T. L. Vuong",
        "Jin Tae Kwak"
      ],
      "abstract": "We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA.",
      "pdf_url": "http://arxiv.org/pdf/2505.04192v1",
      "published": "2025-05-07T07:41:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.04192v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    }
  ]
}