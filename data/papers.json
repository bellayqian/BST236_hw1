{
  "last_updated": "2025-08-14T00:54:34.637774",
  "papers": [
    {
      "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models",
      "authors": [
        "Wen Wang",
        "Bozhen Fang",
        "Chenchen Jing",
        "Yongliang Shen",
        "Yangyi Shen",
        "Qiuyu Wang",
        "Hao Ouyang",
        "Hao Chen",
        "Chunhua Shen"
      ],
      "abstract": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.",
      "pdf_url": "http://arxiv.org/pdf/2508.09138v1",
      "published": "2025-08-12T17:59:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09138v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer",
      "authors": [
        "Zixin Yin",
        "Xili Dai",
        "Ling-Hao Chen",
        "Deyu Zhou",
        "Jianan Wang",
        "Duomin Wang",
        "Gang Yu",
        "Lionel M. Ni",
        "Lei Zhang",
        "Heung-Yeung Shum"
      ],
      "abstract": "Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.",
      "pdf_url": "http://arxiv.org/pdf/2508.09131v2",
      "published": "2025-08-12T17:57:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09131v2",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair",
      "authors": [
        "Xianghe Pang",
        "Shuo Tang",
        "Rui Ye",
        "Yuwen Du",
        "Yaxin Du",
        "Siheng Chen"
      ],
      "abstract": "Effective information seeking in the vast and ever-growing digital landscape\nrequires balancing expansive search with strategic reasoning. Current large\nlanguage model (LLM)-based agents struggle to achieve this balance due to\nlimitations in search breadth and reasoning depth, where slow, serial querying\nrestricts coverage of relevant sources and noisy raw inputs disrupt the\ncontinuity of multi-step reasoning. To address these challenges, we propose\nBrowseMaster, a scalable framework built around a programmatically augmented\nplanner-executor agent pair. The planner formulates and adapts search\nstrategies based on task constraints, while the executor conducts efficient,\ntargeted retrieval to supply the planner with concise, relevant evidence. This\ndivision of labor preserves coherent, long-horizon reasoning while sustaining\nbroad and systematic exploration, overcoming the trade-off that limits existing\nagents. Extensive experiments on challenging English and Chinese benchmarks\nshow that BrowseMaster consistently outperforms open-source and proprietary\nbaselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh,\nwhich demonstrates its strong capability in complex, reasoning-heavy\ninformation-seeking tasks at scale.",
      "pdf_url": "http://arxiv.org/pdf/2508.09129v1",
      "published": "2025-08-12T17:56:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09129v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "OpenCUA: Open Foundations for Computer-Use Agents",
      "authors": [
        "Xinyuan Wang",
        "Bowen Wang",
        "Dunjie Lu",
        "Junlin Yang",
        "Tianbao Xie",
        "Junli Wang",
        "Jiaqi Deng",
        "Xiaole Guo",
        "Yiheng Xu",
        "Chen Henry Wu",
        "Zhennan Shen",
        "Zhuokai Li",
        "Ryan Li",
        "Xiaochuan Li",
        "Junda Chen",
        "Boyuan Zheng",
        "Peihang Li",
        "Fangyu Lei",
        "Ruisheng Cao",
        "Yeqiao Fu",
        "Dongchan Shin",
        "Martin Shin",
        "Jiarui Hu",
        "Yuyan Wang",
        "Jixuan Chen",
        "Yuxiao Ye",
        "Danyang Zhang",
        "Dikang Du",
        "Hao Hu",
        "Huarong Chen",
        "Zaida Zhou",
        "Yipu Wang",
        "Heng Wang",
        "Diyi Yang",
        "Victor Zhong",
        "Flood Sung",
        "Y. Charles",
        "Zhilin Yang",
        "Tao Yu"
      ],
      "abstract": "Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.",
      "pdf_url": "http://arxiv.org/pdf/2508.09123v1",
      "published": "2025-08-12T17:52:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09123v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling",
      "authors": [
        "Shixuan Sun",
        "Siyuan Liang",
        "Ruoyu Chen",
        "Jianjie Huang",
        "Jingzhi Li",
        "Xiaochun Cao"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities. To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.09105v2",
      "published": "2025-08-12T17:32:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09105v2",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Towards Universal Neural Inference",
      "authors": [
        "Shreyas Bhat Brahmavar",
        "Yang Li",
        "Junier Oliva"
      ],
      "abstract": "Real-world data often appears in diverse, disjoint forms -- with varying\nschemas, inconsistent semantics, and no fixed feature ordering -- making it\nchallenging to build general-purpose models that can leverage information\nacross datasets. We introduce ASPIRE, Arbitrary Set-based Permutation-Invariant\nReasoning Engine, a Universal Neural Inference model for semantic reasoning and\nprediction over heterogeneous structured data. ASPIRE combines a\npermutation-invariant, set-based Transformer with a semantic grounding module\nthat incorporates natural language descriptions, dataset metadata, and\nin-context examples to learn cross-dataset feature dependencies. This\narchitecture allows ASPIRE to ingest arbitrary sets of feature--value pairs and\nsupport examples, align semantics across disjoint tables, and make predictions\nfor any specified target. Once trained, ASPIRE generalizes to new inference\ntasks without additional tuning. In addition to delivering strong results\nacross diverse benchmarks, ASPIRE naturally supports cost-aware active feature\nacquisition in an open-world setting, selecting informative features under\ntest-time budget constraints for an arbitrary unseen dataset. These\ncapabilities position ASPIRE as a step toward truly universal, semantics-aware\ninference over structured data.",
      "pdf_url": "http://arxiv.org/pdf/2508.09100v1",
      "published": "2025-08-12T17:26:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09100v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system",
      "authors": [
        "Jialiang Shi",
        "Yaguang Dou",
        "Tian Qi"
      ],
      "abstract": "Modeling multi-interests has arisen as a core problem in real-world RS.\nCurrent multi-interest retrieval methods pose three major challenges: 1)\nInterests, typically extracted from predefined external knowledge, are\ninvariant. Failed to dynamically evolve with users' real-time consumption\npreferences. 2) Online inference typically employs an over-exploited strategy,\nmainly matching users' existing interests, lacking proactive exploration and\ndiscovery of novel and long-tail interests. To address these challenges, we\npropose a novel retrieval framework named SPARC(Soft Probabilistic Adaptive\nRetrieval Model via Codebooks). Our contribution is two folds. First, the\nframework utilizes Residual Quantized Variational Autoencoder (RQ-VAE) to\nconstruct a discretized interest space. It achieves joint training of the\nRQ-VAE with the industrial large scale recommendation model, mining\nbehavior-aware interests that can perceive user feedback and evolve\ndynamically. Secondly, a probabilistic interest module that predicts the\nprobability distribution over the entire dynamic and discrete interest space.\nThis facilitates an efficient \"soft-search\" strategy during online inference,\nrevolutionizing the retrieval paradigm from \"passive matching\" to \"proactive\nexploration\" and thereby effectively promoting interest discovery. Online A/B\ntests on an industrial platform with tens of millions daily active users, have\nachieved substantial gains in business metrics: +0.9% increase in user view\nduration, +0.4% increase in user page views (PV), and a +22.7% improvement in\nPV500(new content reaching 500 PVs in 24 hours). Offline evaluations are\nconducted on open-source Amazon Product datasets. Metrics, such as Recall@K and\nNormalized Discounted Cumulative Gain@K(NDCG@K), also showed consistent\nimprovement. Both online and offline experiments validate the efficacy and\npractical value of the proposed method.",
      "pdf_url": "http://arxiv.org/pdf/2508.09090v2",
      "published": "2025-08-12T17:16:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09090v2",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring",
      "authors": [
        "Zihan Fang",
        "Zheng Lin",
        "Senkang Hu",
        "Yihang Tao",
        "Yiqin Deng",
        "Xianhao Chen",
        "Yuguang Fang"
      ],
      "abstract": "Outdoor health monitoring is essential to detect early abnormal health status\nfor safeguarding human health and safety. Conventional outdoor monitoring\nrelies on static multimodal deep learning frameworks, which requires extensive\ndata training from scratch and fails to capture subtle health status changes.\nMultimodal large language models (MLLMs) emerge as a promising alternative,\nutilizing only small datasets to fine-tune pre-trained information-rich models\nfor enabling powerful health status monitoring. Unfortunately, MLLM-based\noutdoor health monitoring also faces significant challenges: I) sensor data\ncontains input noise stemming from sensor data acquisition and fluctuation\nnoise caused by sudden changes in physiological signals due to dynamic outdoor\nenvironments, thus degrading the training performance; ii) current transformer\nbased MLLMs struggle to achieve robust multimodal fusion, as they lack a design\nfor fusing the noisy modality; iii) modalities with varying noise levels hinder\naccurate recovery of missing data from fluctuating distributions. To combat\nthese challenges, we propose an uncertainty-aware multimodal fusion framework,\nnamed DUAL-Health, for outdoor health monitoring in dynamic and noisy\nenvironments. First, to assess the impact of noise, we accurately quantify\nmodality uncertainty caused by input and fluctuation noise with current and\ntemporal features. Second, to empower efficient muitimodal fusion with\nlow-quality modalities,we customize the fusion weight for each modality based\non quantified and calibrated uncertainty. Third, to enhance data recovery from\nfluctuating noisy modalities, we align modality distributions within a common\nsemantic space. Extensive experiments demonstrate that our DUAL-Health\noutperforms state-of-the-art baselines in detection accuracy and robustness.",
      "pdf_url": "http://arxiv.org/pdf/2508.09085v1",
      "published": "2025-08-12T17:07:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09085v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks",
      "authors": [
        "Debdeep Mukherjee",
        "Eduardo Di Santi",
        "Clément Lefebvre",
        "Nenad Mijatovic",
        "Victor Martin",
        "Thierry Josse",
        "Jonathan Brown",
        "Kenza Saiah"
      ],
      "abstract": "Track circuits are critical for railway operations, acting as the main\nsignalling sub-system to locate trains. Continuous Variable Current Modulation\n(CVCM) is one such technology. Like any field-deployed, safety-critical asset,\nit can fail, triggering cascading disruptions. Many failures originate as\nsubtle anomalies that evolve over time, often not visually apparent in\nmonitored signals. Conventional approaches, which rely on clear signal changes,\nstruggle to detect them early. Early identification of failure types is\nessential to improve maintenance planning, minimising downtime and revenue\nloss. Leveraging deep neural networks, we propose a predictive maintenance\nframework that classifies anomalies well before they escalate into failures.\nValidated on 10 CVCM failure cases across different installations, the method\nis ISO-17359 compliant and outperforms conventional techniques, achieving\n99.31% overall accuracy with detection within 1% of anomaly onset. Through\nconformal prediction, we provide uncertainty estimates, reaching 99% confidence\nwith consistent coverage across classes. Given CVCMs global deployment, the\napproach is scalable and adaptable to other track circuits and railway systems,\nenhancing operational reliability.",
      "pdf_url": "http://arxiv.org/pdf/2508.09054v1",
      "published": "2025-08-12T16:13:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09054v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "68T07, 68T05",
        "I.2.6; I.5.1; I.5.4"
      ]
    },
    {
      "title": "Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy and AI Governance Exams",
      "authors": [
        "Zane Witherspoon",
        "Thet Mon Aye",
        "YingYing Hao"
      ],
      "abstract": "The rapid emergence of large language models (LLMs) has raised urgent\nquestions across the modern workforce about this new technology's strengths,\nweaknesses, and capabilities. For privacy professionals, the question is\nwhether these AI systems can provide reliable support on regulatory compliance,\nprivacy program management, and AI governance. In this study, we evaluate ten\nleading open and closed LLMs, including models from OpenAI, Anthropic, Google\nDeepMind, Meta, and DeepSeek, by benchmarking their performance on\nindustry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the\nInternational Association of Privacy Professionals (IAPP). Each model was\ntested using official sample exams in a closed-book setting and compared to\nIAPP's passing thresholds. Our findings show that several frontier models such\nas Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the\nstandards for professional human certification - demonstrating substantial\nexpertise in privacy law, technical controls, and AI governance. The results\nhighlight both the strengths and domain-specific gaps of current LLMs and offer\npractical insights for privacy officers, compliance leads, and technologists\nassessing the readiness of AI tools for high-stakes data governance roles. This\npaper provides an overview for professionals navigating the intersection of AI\nadvancement and regulatory risk and establishes a machine benchmark based on\nhuman-centric evaluations.",
      "pdf_url": "http://arxiv.org/pdf/2508.09036v1",
      "published": "2025-08-12T15:57:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09036v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding",
      "authors": [
        "Maxim A. Patratskiy",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "Vision-Language-Action models have demonstrated remarkable capabilities in\npredicting agent movements within virtual environments and real-world scenarios\nbased on visual observations and textual instructions. Although recent research\nhas focused on enhancing spatial and temporal understanding independently, this\npaper presents a novel approach that integrates both aspects through visual\nprompting. We introduce a method that projects visual traces of key points from\nobservations onto depth maps, enabling models to capture both spatial and\ntemporal information simultaneously. The experiments in SimplerEnv show that\nthe mean number of tasks successfully solved increased for 4% compared to\nSpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this\nenhancement can be achieved with minimal training data, making it particularly\nvaluable for real-world applications where data collection is challenging. The\nproject page is available at https://ampiromax.github.io/ST-VLA.",
      "pdf_url": "http://arxiv.org/pdf/2508.09032v1",
      "published": "2025-08-12T15:53:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09032v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "A First Look at Predictability and Explainability of Pre-request Passenger Waiting Time in Ridesharing Systems",
      "authors": [
        "Jie Wang",
        "Guang Wang"
      ],
      "abstract": "Passenger waiting time prediction plays a critical role in enhancing both\nridesharing user experience and platform efficiency. While most existing\nresearch focuses on post-request waiting time prediction with knowing the\nmatched driver information, pre-request waiting time prediction (i.e., before\nsubmitting a ride request and without matching a driver) is also important, as\nit enables passengers to plan their trips more effectively and enhance the\nexperience of both passengers and drivers. However, it has not been fully\nstudied by existing works. In this paper, we take the first step toward\nunderstanding the predictability and explainability of pre-request passenger\nwaiting time in ridesharing systems. Particularly, we conduct an in-depth\ndata-driven study to investigate the impact of demand&supply dynamics on\npassenger waiting time. Based on this analysis and feature engineering, we\npropose FiXGBoost, a novel feature interaction-based XGBoost model designed to\npredict waiting time without knowing the assigned driver information. We\nfurther perform an importance analysis to quantify the contribution of each\nfactor. Experiments on a large-scale real-world ridesharing dataset including\nover 30 million trip records show that our FiXGBoost can achieve a good\nperformance for pre-request passenger waiting time prediction with high\nexplainability.",
      "pdf_url": "http://arxiv.org/pdf/2508.09027v1",
      "published": "2025-08-12T15:42:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09027v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and Efficiency",
      "authors": [
        "Dongjie Xu",
        "Yue Cui",
        "Weijie Shi",
        "Qingzhi Ma",
        "Hanghui Guo",
        "Jiaming Li",
        "Yao Zhao",
        "Ruiyuan Zhang",
        "Shimin Di",
        "Jia Zhu",
        "Kai Zheng",
        "Jiajie Xu"
      ],
      "abstract": "SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in suboptimal or non-equivalent\nrewrites due to a lack of execution awareness and semantic grounding. To\naddress these challenges, We present E3-Rewrite, an LLM-based SQL rewriting\nframework that produces executable, equivalent, and efficient queries. It\nintegrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency.\nExtensive experiments show that E3-Rewrite achieves up to a 25.6\\% reduction in\nquery execution time compared to state-of-the-art methods across multiple SQL\nbenchmarks. Moreover, it delivers up to 24.4\\% more successful rewrites,\nexpanding coverage to complex queries that previous systems failed to handle.",
      "pdf_url": "http://arxiv.org/pdf/2508.09023v1",
      "published": "2025-08-12T15:38:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09023v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges",
      "authors": [
        "Zhiqiang Yang",
        "Renshuai Tao",
        "Xiaolong Zheng",
        "Guodong Yang",
        "Chunjie Zhang"
      ],
      "abstract": "Existing deepfake detection methods heavily depend on labeled training data.\nHowever, as AI-generated content becomes increasingly realistic, even\n\\textbf{human annotators struggle to distinguish} between deepfakes and\nauthentic images. This makes the labeling process both time-consuming and less\nreliable. Specifically, there is a growing demand for approaches that can\neffectively utilize large-scale unlabeled data from online social networks.\nUnlike typical unsupervised learning tasks, where categories are distinct,\nAI-generated faces closely mimic real image distributions and share strong\nsimilarities, causing performance drop in conventional strategies. In this\npaper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two key\nchallenges: (1) bridging the domain gap between faces from different generation\nmodels, and (2) utilizing unlabeled image samples. The method features two core\nmodules: text-guided cross-domain alignment, which uses learnable prompts to\nunify visual and textual embeddings into a domain-invariant feature space, and\ncurriculum-driven pseudo label generation, which dynamically exploit more\ninformative unlabeled samples. To prevent catastrophic forgetting, we also\nfacilitate bridging between domains via cross-domain knowledge distillation.\nExtensive experiments on \\textbf{11 popular datasets}, show that DPGNet\noutperforms SoTA approaches by \\textbf{6.3\\%}, highlighting its effectiveness\nin leveraging unlabeled data to address the annotation challenges posed by the\nincreasing realism of deepfakes.",
      "pdf_url": "http://arxiv.org/pdf/2508.09022v2",
      "published": "2025-08-12T15:37:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09022v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Attacks and Defenses Against LLM Fingerprinting",
      "authors": [
        "Kevin Kurian",
        "Ethan Holland",
        "Sean Oesch"
      ],
      "abstract": "As large language models are increasingly deployed in sensitive environments,\nfingerprinting attacks pose significant privacy and security risks. We present\na study of LLM fingerprinting from both offensive and defensive perspectives.\nOur attack methodology uses reinforcement learning to automatically optimize\nquery selection, achieving better fingerprinting accuracy with only 3 queries\ncompared to randomly selecting 3 queries from the same pool. Our defensive\napproach employs semantic-preserving output filtering through a secondary LLM\nto obfuscate model identity while maintaining semantic integrity. The defensive\nmethod reduces fingerprinting accuracy across tested models while preserving\noutput quality. These contributions show the potential to improve\nfingerprinting tools capabilities while providing practical mitigation\nstrategies against fingerprinting attacks.",
      "pdf_url": "http://arxiv.org/pdf/2508.09021v1",
      "published": "2025-08-12T15:36:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09021v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs",
      "authors": [
        "Shivam Dubey"
      ],
      "abstract": "As large language models (LLMs) become more integrated into societal systems,\nthe risk of them perpetuating and amplifying harmful biases becomes a critical\nsafety concern. Traditional methods for mitigating bias often rely on data\nfiltering or post-hoc output moderation, which treat the model as an opaque\nblack box. In this work, we introduce a complete, end-to-end system that uses\ntechniques from mechanistic interpretability to both identify and actively\nmitigate bias directly within a model's internal workings. Our method involves\ntwo primary stages. First, we train linear \"probes\" on the internal activations\nof a model to detect the latent representations of various biases (e.g.,\ngender, race, age). Our experiments on \\texttt{gpt2-large} demonstrate that\nthese probes can identify biased content with near-perfect accuracy, revealing\nthat bias representations become most salient in the model's later layers.\nSecond, we leverage these findings to compute \"steering vectors\" by contrasting\nthe model's activation patterns for biased and neutral statements. By adding\nthese vectors during inference, we can actively steer the model's generative\nprocess away from producing harmful, stereotypical, or biased content in\nreal-time. We demonstrate the efficacy of this activation steering technique,\nshowing that it successfully alters biased completions toward more neutral\nalternatives. We present our work as a robust and reproducible system that\noffers a more direct and interpretable approach to building safer and more\naccountable LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2508.09019v1",
      "published": "2025-08-12T15:34:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09019v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA",
      "authors": [
        "Adrián Gude",
        "Roi Santos-Ríos",
        "Francisco Prado-Valiño",
        "Ana Ezquerro",
        "Jesús Vilares"
      ],
      "abstract": "This paper describes our participation in SemEval 2025 Task 8, focused on\nTabular Question Answering. We developed a zero-shot pipeline that leverages an\nLarge Language Model to generate functional code capable of extracting the\nrelevant information from tabular data based on an input question. Our approach\nconsists of a modular pipeline where the main code generator module is\nsupported by additional components that identify the most relevant columns and\nanalyze their data types to improve extraction accuracy. In the event that the\ngenerated code fails, an iterative refinement process is triggered,\nincorporating the error feedback into a new generation prompt to enhance\nrobustness. Our results show that zero-shot code generation is a valid approach\nfor Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of\ntask-specific fine-tuning.",
      "pdf_url": "http://arxiv.org/pdf/2508.09012v1",
      "published": "2025-08-12T15:25:31+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09012v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
      "authors": [
        "Seonghwan Choi",
        "Beomseok Kang",
        "Dongwon Jo",
        "Jae-Joon Kim"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
      "pdf_url": "http://arxiv.org/pdf/2508.09001v1",
      "published": "2025-08-12T15:11:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.09001v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory",
      "authors": [
        "Sizhe Yuen",
        "Francisco Gomez Medina",
        "Ting Su",
        "Yali Du",
        "Adam J. Sobey"
      ],
      "abstract": "Multi-agent systems built on Large Language Models (LLMs) show exceptional\npromise for complex collaborative problem-solving, yet they face fundamental\nchallenges stemming from context window limitations that impair memory\nconsistency, role adherence, and procedural integrity. This paper introduces\nIntrinsic Memory Agents, a novel framework that addresses these limitations\nthrough structured agent-specific memories that evolve intrinsically with agent\noutputs. Specifically, our method maintains role-aligned memory templates that\npreserve specialized perspectives while focusing on task-relevant information.\nWe benchmark our approach on the PDDL dataset, comparing its performance to\nexisting state-of-the-art multi-agentic memory approaches and showing an\nimprovement of 38.6\\% with the highest token efficiency. An additional\nevaluation is performed on a complex data pipeline design task, we demonstrate\nthat our approach produces higher quality designs when comparing 5 metrics:\nscalability, reliability, usability, cost-effectiveness and documentation with\nadditional qualitative evidence of the improvements. Our findings suggest that\naddressing memory limitations through structured, intrinsic approaches can\nimprove the capabilities of multi-agent LLM systems on structured planning\ntasks.",
      "pdf_url": "http://arxiv.org/pdf/2508.08997v1",
      "published": "2025-08-12T15:05:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08997v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty",
      "authors": [
        "Rui Wang",
        "Qihan Lin",
        "Jiayu Liu",
        "Qing Zong",
        "Tianshi Zheng",
        "Weiqi Wang",
        "Yangqiu Song"
      ],
      "abstract": "Prospect Theory (PT) models human decision-making under uncertainty, while\nepistemic markers (e.g., maybe) serve to express uncertainty in language.\nHowever, it remains largely unexplored whether Prospect Theory applies to\ncontemporary Large Language Models and whether epistemic markers, which express\nhuman uncertainty, affect their decision-making behaviour. To address these\nresearch gaps, we design a three-stage experiment based on economic\nquestionnaires. We propose a more general and precise evaluation framework to\nmodel LLMs' decision-making behaviour under PT, introducing uncertainty through\nthe empirical probability values associated with commonly used epistemic\nmarkers in comparable contexts. We then incorporate epistemic markers into the\nevaluation framework based on their corresponding probability values to examine\ntheir influence on LLM decision-making behaviours. Our findings suggest that\nmodelling LLMs' decision-making with PT is not consistently reliable,\nparticularly when uncertainty is expressed in diverse linguistic forms. Our\ncode is released in https://github.com/HKUST-KnowComp/MarPT.",
      "pdf_url": "http://arxiv.org/pdf/2508.08992v1",
      "published": "2025-08-12T15:02:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08992v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Rational Inverse Reasoning",
      "authors": [
        "Ben Zandonati",
        "Tomás Lozano-Pérez",
        "Leslie Pack Kaelbling"
      ],
      "abstract": "Humans can observe a single, imperfect demonstration and immediately\ngeneralize to very different problem settings. Robots, in contrast, often\nrequire hundreds of examples and still struggle to generalize beyond the\ntraining conditions. We argue that this limitation arises from the inability to\nrecover the latent explanations that underpin intelligent behavior, and that\nthese explanations can take the form of structured programs consisting of\nhigh-level goals, sub-task decomposition, and execution constraints. In this\nwork, we introduce Rational Inverse Reasoning (RIR), a framework for inferring\nthese latent programs through a hierarchical generative model of behavior. RIR\nframes few-shot imitation as Bayesian program induction: a vision-language\nmodel iteratively proposes structured symbolic task hypotheses, while a\nplanner-in-the-loop inference scheme scores each by the likelihood of the\nobserved demonstration under that hypothesis. This loop yields a posterior over\nconcise, executable programs. We evaluate RIR on a suite of continuous\nmanipulation tasks designed to test one-shot and few-shot generalization across\nvariations in object pose, count, geometry, and layout. With as little as one\ndemonstration, RIR infers the intended task structure and generalizes to novel\nsettings, outperforming state-of-the-art vision-language model baselines.",
      "pdf_url": "http://arxiv.org/pdf/2508.08983v1",
      "published": "2025-08-12T14:49:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08983v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion",
      "authors": [
        "Seungeun Rho",
        "Kartik Garg",
        "Morgan Byrd",
        "Sehoon Ha"
      ],
      "abstract": "Exploration is crucial for enabling legged robots to learn agile locomotion\nbehaviors that can overcome diverse obstacles. However, such exploration is\ninherently challenging, and we often rely on extensive reward engineering,\nexpert demonstrations, or curriculum learning - all of which limit\ngeneralizability. In this work, we propose Skill Discovery as Exploration\n(SDAX), a novel learning framework that significantly reduces human engineering\neffort. SDAX leverages unsupervised skill discovery to autonomously acquire a\ndiverse repertoire of skills for overcoming obstacles. To dynamically regulate\nthe level of exploration during training, SDAX employs a bi-level optimization\nprocess that autonomously adjusts the degree of exploration. We demonstrate\nthat SDAX enables quadrupedal robots to acquire highly agile behaviors\nincluding crawling, climbing, leaping, and executing complex maneuvers such as\njumping off vertical walls. Finally, we deploy the learned policy on real\nhardware, validating its successful transfer to the real world.",
      "pdf_url": "http://arxiv.org/pdf/2508.08982v1",
      "published": "2025-08-12T14:49:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08982v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Urban-STA4CLC: Urban Theory-Informed Spatio-Temporal Attention Model for Predicting Post-Disaster Commercial Land Use Change",
      "authors": [
        "Ziyi Guo",
        "Yan Wang"
      ],
      "abstract": "Natural disasters such as hurricanes and wildfires increasingly introduce\nunusual disturbance on economic activities, which are especially likely to\nreshape commercial land use pattern given their sensitive to customer\nvisitation. However, current modeling approaches are limited in capturing such\ncomplex interplay between human activities and commercial land use change under\nand following disturbances. Such interactions have been more effectively\ncaptured in current resilient urban planning theories. This study designs and\ncalibrates a Urban Theory-Informed Spatio-Temporal Attention Model for\nPredicting Post-Disaster Commercial Land Use Change (Urban-STA4CLC) to predict\nboth the yearly decline and expansion of commercial land use at census block\nlevel under cumulative impact of disasters on human activities over two years.\nGuided by urban theories, Urban-STA4CLC integrates both spatial and temporal\nattention mechanisms with three theory-informed modules. Resilience theory\nguides a disaster-aware temporal attention module that captures visitation\ndynamics. Spatial economic theory informs a multi-relational spatial attention\nmodule for inter-block representation. Diffusion theory contributes a\nregularization term that constrains land use transitions. The model performs\nsignificantly better than non-theoretical baselines in predicting commercial\nland use change under the scenario of recurrent hurricanes, with around 19%\nimprovement in F1 score (0.8763). The effectiveness of the theory-guided\nmodules was further validated through ablation studies. The research\ndemonstrates that embedding urban theory into commercial land use modeling\nmodels may substantially enhance the capacity to capture its gains and losses.\nThese advances in commercial land use modeling contribute to land use research\nthat accounts for cumulative impacts of recurrent disasters and shifts in\neconomic activity patterns.",
      "pdf_url": "http://arxiv.org/pdf/2508.08976v1",
      "published": "2025-08-12T14:39:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08976v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Revealing the Role of Audio Channels in ASR Performance Degradation",
      "authors": [
        "Kuan-Tang Huang",
        "Li-Wei Chen",
        "Hung-Shin Lee",
        "Berlin Chen",
        "Hsin-Min Wang"
      ],
      "abstract": "Pre-trained automatic speech recognition (ASR) models have demonstrated\nstrong performance on a variety of tasks. However, their performance can\ndegrade substantially when the input audio comes from different recording\nchannels. While previous studies have demonstrated this phenomenon, it is often\nattributed to the mismatch between training and testing corpora. This study\nargues that variations in speech characteristics caused by different recording\nchannels can fundamentally harm ASR performance. To address this limitation, we\npropose a normalization technique designed to mitigate the impact of channel\nvariation by aligning internal feature representations in the ASR model with\nthose derived from a clean reference channel. This approach significantly\nimproves ASR performance on previously unseen channels and languages,\nhighlighting its ability to generalize across channel and language differences.",
      "pdf_url": "http://arxiv.org/pdf/2508.08967v1",
      "published": "2025-08-12T14:32:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08967v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "QAMRO: Quality-aware Adaptive Margin Ranking Optimization for Human-aligned Assessment of Audio Generation Systems",
      "authors": [
        "Chien-Chun Wang",
        "Kuan-Tang Huang",
        "Cheng-Yeh Yang",
        "Hung-Shin Lee",
        "Hsin-Min Wang",
        "Berlin Chen"
      ],
      "abstract": "Evaluating audio generation systems, including text-to-music (TTM),\ntext-to-speech (TTS), and text-to-audio (TTA), remains challenging due to the\nsubjective and multi-dimensional nature of human perception. Existing methods\ntreat mean opinion score (MOS) prediction as a regression problem, but standard\nregression losses overlook the relativity of perceptual judgments. To address\nthis limitation, we introduce QAMRO, a novel Quality-aware Adaptive Margin\nRanking Optimization framework that seamlessly integrates regression objectives\nfrom different perspectives, aiming to highlight perceptual differences and\nprioritize accurate ratings. Our framework leverages pre-trained audio-text\nmodels such as CLAP and Audiobox-Aesthetics, and is trained exclusively on the\nofficial AudioMOS Challenge 2025 dataset. It demonstrates superior alignment\nwith human evaluations across all dimensions, significantly outperforming\nrobust baseline models.",
      "pdf_url": "http://arxiv.org/pdf/2508.08957v1",
      "published": "2025-08-12T14:14:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08957v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Generalising Traffic Forecasting to Regions without Traffic Observations",
      "authors": [
        "Xinyu Su",
        "Majid Sarvi",
        "Feng Liu",
        "Egemen Tanin",
        "Jianzhong Qi"
      ],
      "abstract": "Traffic forecasting is essential for intelligent transportation systems.\nAccurate forecasting relies on continuous observations collected by traffic\nsensors. However, due to high deployment and maintenance costs, not all regions\nare equipped with such sensors. This paper aims to forecast for regions without\ntraffic sensors, where the lack of historical traffic observations challenges\nthe generalisability of existing models. We propose a model named GenCast, the\ncore idea of which is to exploit external knowledge to compensate for the\nmissing observations and to enhance generalisation. We integrate\nphysics-informed neural networks into GenCast, enabling physical principles to\nregularise the learning process. We introduce an external signal learning\nmodule to explore correlations between traffic states and external signals such\nas weather conditions, further improving model generalisability. Additionally,\nwe design a spatial grouping module to filter localised features that hinder\nmodel generalisability. Extensive experiments show that GenCast consistently\nreduces forecasting errors on multiple real-world datasets.",
      "pdf_url": "http://arxiv.org/pdf/2508.08947v1",
      "published": "2025-08-12T14:00:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08947v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning",
      "authors": [
        "Hasan Abed Al Kader Hammoud",
        "Kumail Alhamoud",
        "Abed Hammoud",
        "Elie Bou-Zeid",
        "Marzyeh Ghassemi",
        "Bernard Ghanem"
      ],
      "abstract": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo.",
      "pdf_url": "http://arxiv.org/pdf/2508.08940v1",
      "published": "2025-08-12T13:48:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08940v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models",
      "authors": [
        "Wei Cai",
        "Jian Zhao",
        "Yuchu Jiang",
        "Tianle Zhang",
        "Xuelong Li"
      ],
      "abstract": "Large Vision-Language Models face growing safety challenges with multimodal\ninputs. This paper introduces the concept of Implicit Reasoning Safety, a\nvulnerability in LVLMs. Benign combined inputs trigger unsafe LVLM outputs due\nto flawed or hidden reasoning. To showcase this, we developed Safe Semantics,\nUnsafe Interpretations, the first dataset for this critical issue. Our\ndemonstrations show that even simple In-Context Learning with SSUI\nsignificantly mitigates these implicit multimodal threats, underscoring the\nurgent need to improve cross-modal implicit reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2508.08926v1",
      "published": "2025-08-12T13:26:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08926v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "EGGCodec: A Robust Neural Encodec Framework for EGG Reconstruction and F0 Extraction",
      "authors": [
        "Rui Feng",
        "Yuang Chen",
        "Yu Hu",
        "Jun Du",
        "Jiahong Yuan"
      ],
      "abstract": "This letter introduces EGGCodec, a robust neural Encodec framework engineered\nfor electroglottography (EGG) signal reconstruction and F0 extraction. We\npropose a multi-scale frequency-domain loss function to capture the nuanced\nrelationship between original and reconstructed EGG signals, complemented by a\ntime-domain correlation loss to improve generalization and accuracy. Unlike\nconventional Encodec models that extract F0 directly from features, EGGCodec\nleverages reconstructed EGG signals, which more closely correspond to F0. By\nremoving the conventional GAN discriminator, we streamline EGGCodec's training\nprocess without compromising efficiency, incurring only negligible performance\ndegradation. Trained on a widely used EGG-inclusive dataset, extensive\nevaluations demonstrate that EGGCodec outperforms state-of-the-art F0\nextraction schemes, reducing mean absolute error (MAE) from 14.14 Hz to 13.69\nHz, and improving voicing decision error (VDE) by 38.2\\%. Moreover, extensive\nablation experiments validate the contribution of each component of EGGCodec.",
      "pdf_url": "http://arxiv.org/pdf/2508.08924v1",
      "published": "2025-08-12T13:20:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08924v1",
      "categories": [
        "eess.AS",
        "cs.AI"
      ]
    },
    {
      "title": "Shape Completion and Real-Time Visualization in Robotic Ultrasound Spine Acquisitions",
      "authors": [
        "Miruna-Alexandra Gafencu",
        "Reem Shaban",
        "Yordanka Velikova",
        "Mohammad Farid Azampour",
        "Nassir Navab"
      ],
      "abstract": "Ultrasound (US) imaging is increasingly used in spinal procedures due to its\nreal-time, radiation-free capabilities; however, its effectiveness is hindered\nby shadowing artifacts that obscure deeper tissue structures. Traditional\napproaches, such as CT-to-US registration, incorporate anatomical information\nfrom preoperative CT scans to guide interventions, but they are limited by\ncomplex registration requirements, differences in spine curvature, and the need\nfor recent CT imaging. Recent shape completion methods can offer an alternative\nby reconstructing spinal structures in US data, while being pretrained on large\nset of publicly available CT scans. However, these approaches are typically\noffline and have limited reproducibility. In this work, we introduce a novel\nintegrated system that combines robotic ultrasound with real-time shape\ncompletion to enhance spinal visualization. Our robotic platform autonomously\nacquires US sweeps of the lumbar spine, extracts vertebral surfaces from\nultrasound, and reconstructs the complete anatomy using a deep learning-based\nshape completion network. This framework provides interactive, real-time\nvisualization with the capability to autonomously repeat scans and can enable\nnavigation to target locations. This can contribute to better consistency,\nreproducibility, and understanding of the underlying anatomy. We validate our\napproach through quantitative experiments assessing shape completion accuracy\nand evaluations of multiple spine acquisition protocols on a phantom setup.\nAdditionally, we present qualitative results of the visualization on a\nvolunteer scan.",
      "pdf_url": "http://arxiv.org/pdf/2508.08923v1",
      "published": "2025-08-12T13:19:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08923v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning",
      "authors": [
        "Mahmoud Salhab",
        "Shameed Sait",
        "Mohammad Abusheikh",
        "Hasan Abusheikh"
      ],
      "abstract": "Automatic speech recognition (ASR) plays a vital role in enabling natural\nhuman-machine interaction across applications such as virtual assistants,\nindustrial automation, customer support, and real-time transcription. However,\ndeveloping accurate ASR systems for low-resource languages like Arabic remains\na significant challenge due to limited labeled data and the linguistic\ncomplexity introduced by diverse dialects. In this work, we present a scalable\ntraining pipeline that combines weakly supervised learning with supervised\nfine-tuning to develop a robust Arabic ASR model. In the first stage, we\npretrain the model on 15,000 hours of weakly labeled speech covering both\nModern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the\nsubsequent stage, we perform continual supervised fine-tuning using a mixture\nof filtered weakly labeled data and a small, high-quality annotated dataset.\nOur approach achieves state-of-the-art results, ranking first in the\nmulti-dialectal Arabic ASR challenge. These findings highlight the\neffectiveness of weak supervision paired with fine-tuning in overcoming data\nscarcity and delivering high-quality ASR for low-resource, dialect-rich\nlanguages.",
      "pdf_url": "http://arxiv.org/pdf/2508.08912v1",
      "published": "2025-08-12T13:02:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08912v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Compass-Thinker-7B Technical Report",
      "authors": [
        "Anxiang Zeng",
        "Haibo Zhang",
        "Kaixiang Mo",
        "Long Zhang",
        "Shuman Liu",
        "Yanhui Huang",
        "Yawen Liu",
        "Yuepeng Sheng",
        "Yuwei Huang"
      ],
      "abstract": "Recent R1-Zero-like research further demonstrates that reasoning extension\nhas given large language models (LLMs) unprecedented reasoning capabilities,\nand Reinforcement Learning is the core technology to elicit its complex\nreasoning. However, conducting RL experiments directly on hyperscale models\ninvolves high computational costs and resource demands, posing significant\nrisks. We propose the Compass-Thinker-7B model, which aims to explore the\npotential of Reinforcement Learning with less computational resources and\ncosts, and provides insights for further research into RL recipes for larger\nmodels. Compass-Thinker-7B is trained from an open source model through a\nspecially designed Reinforcement Learning Pipeline. we curate a dataset of 30k\nverifiable mathematics problems for the Reinforcement Learning Pipeline. By\nconfiguring data and training settings with different difficulty distributions\nfor different stages, the potential of the model is gradually released and the\ntraining efficiency is improved. Extensive evaluations show that\nCompass-Thinker-7B possesses exceptional reasoning potential, and achieves\nsuperior performance on mathematics compared to the same-sized RL\nmodel.Especially in the challenging AIME2024 evaluation, Compass-Thinker-7B\nachieves 40% accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.08909v1",
      "published": "2025-08-12T12:58:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08909v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs",
      "authors": [
        "Keyu Chen",
        "Zhifeng Shen",
        "Daohai Yu",
        "Haoqian Wu",
        "Wei Wen",
        "Jianfeng He",
        "Ruizhi Qiao",
        "Xing Sun"
      ],
      "abstract": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
      "pdf_url": "http://arxiv.org/pdf/2508.08895v1",
      "published": "2025-08-12T12:35:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08895v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Position: Causal Machine Learning Requires Rigorous Synthetic Experiments for Broader Adoption",
      "authors": [
        "Audrey Poinsot",
        "Panayiotis Panayiotou",
        "Alessandro Leite",
        "Nicolas Chesneau",
        "Özgür Şimşek",
        "Marc Schoenauer"
      ],
      "abstract": "Causal machine learning has the potential to revolutionize decision-making by\ncombining the predictive power of machine learning algorithms with the theory\nof causal inference. However, these methods remain underutilized by the broader\nmachine learning community, in part because current empirical evaluations do\nnot permit assessment of their reliability and robustness, undermining their\npractical utility. Specifically, one of the principal criticisms made by the\ncommunity is the extensive use of synthetic experiments. We argue, on the\ncontrary, that synthetic experiments are essential and necessary to precisely\nassess and understand the capabilities of causal machine learning methods. To\nsubstantiate our position, we critically review the current evaluation\npractices, spotlight their shortcomings, and propose a set of principles for\nconducting rigorous empirical analyses with synthetic data. Adopting the\nproposed principles will enable comprehensive evaluations that build trust in\ncausal machine learning methods, driving their broader adoption and impactful\nreal-world use.",
      "pdf_url": "http://arxiv.org/pdf/2508.08883v1",
      "published": "2025-08-12T12:13:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08883v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation",
      "authors": [
        "Dayu Wang",
        "Jiaye Yang",
        "Weikang Li",
        "Jiahui Liang",
        "Yang Li"
      ],
      "abstract": "Current tool-integrated mathematical reasoning systems often adopt a\nsingle-agent paradigm, where one large language model handles problem\nreasoning, code generation, and code execution in an integrated workflow. While\nthis design eases coordination, we hypothesize that it imposes cognitive load\ninterference, as the agent must interleave long-horizon reasoning with precise\nprogram synthesis. We validate this hypothesis through a controlled comparison\nbetween a reasoning-only agent and a reasoning-plus-code agent, finding that\nthe latter produces significantly fewer correct reasoning paths despite having\ntool-calling capabilities. To address this, we propose a dual-agent hybrid\nframework: a Reasoning Agent performs stepwise problem decomposition, and a\nCode Agent handles code generation and execution. Training combines imitation\nlearning and reinforcement learning: the Code Agent receives strong rewards for\nmatching intermediate ground-truth programs and weaker rewards for valid\nexecution, while the Reasoning Agent is optimized chiefly via final-answer\naccuracy using advantage estimation to credit intermediate steps. This\ndecoupled role design reduces cognitive interference and promotes stable\nreasoning-coding coordination.",
      "pdf_url": "http://arxiv.org/pdf/2508.08882v1",
      "published": "2025-08-12T12:10:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08882v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models",
      "authors": [
        "Haeun Yu",
        "Seogyeong Jeong",
        "Siddhesh Pawar",
        "Jisu Shin",
        "Jiho Jin",
        "Junho Myung",
        "Alice Oh",
        "Isabelle Augenstein"
      ],
      "abstract": "The growing deployment of large language models (LLMs) across diverse\ncultural contexts necessitates a better understanding of how the\novergeneralization of less documented cultures within LLMs' representations\nimpacts their cultural understanding. Prior work only performs extrinsic\nevaluation of LLMs' cultural competence, without accounting for how LLMs'\ninternal mechanisms lead to cultural (mis)representation. To bridge this gap,\nwe propose Culturescope, the first mechanistic interpretability-based method\nthat probes the internal representations of LLMs to elicit the underlying\ncultural knowledge space. CultureScope utilizes a patching method to extract\nthe cultural knowledge. We introduce a cultural flattening score as a measure\nof the intrinsic cultural biases. Additionally, we study how LLMs internalize\nWestern-dominance bias and cultural flattening, which allows us to trace how\ncultural biases emerge within LLMs. Our experimental results reveal that LLMs\nencode Western-dominance bias and cultural flattening in their cultural\nknowledge space. We find that low-resource cultures are less susceptible to\ncultural biases, likely due to their limited training resources. Our work\nprovides a foundation for future research on mitigating cultural biases and\nenhancing LLMs' cultural understanding. Our codes and data used for experiments\nare publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2508.08879v1",
      "published": "2025-08-12T12:05:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08879v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models",
      "authors": [
        "Fuyao Zhang",
        "Xinyu Yan",
        "Tiantong Wu",
        "Wenjie Li",
        "Tianxiang Chen",
        "Yang Cao",
        "Ran Yan",
        "Longtao Huang",
        "Wei Yang Bryan Lim",
        "Qiang Yang"
      ],
      "abstract": "Large Language Models (LLMs) increasingly leverage Federated Learning (FL) to\nutilize private, task-specific datasets for fine-tuning while preserving data\nprivacy. However, while federated LLM frameworks effectively enable\ncollaborative training without raw data sharing, they critically lack built-in\nmechanisms for regulatory compliance like GDPR's right to be forgotten.\nIntegrating private data heightens concerns over data quality and long-term\ngovernance, yet existing distributed training frameworks offer no principled\nway to selectively remove specific client contributions post-training. Due to\ndistributed data silos, stringent privacy constraints, and the intricacies of\ninterdependent model aggregation, federated LLM unlearning is significantly\nmore complex than centralized LLM unlearning. To address this gap, we introduce\nOblivionis, a lightweight learning and unlearning framework that enables\nclients to selectively remove specific private data during federated LLM\ntraining, enhancing trustworthiness and regulatory compliance. By unifying FL\nand unlearning as a dual optimization objective, we incorporate 6 FL and 5\nunlearning algorithms for comprehensive evaluation and comparative analysis,\nestablishing a robust pipeline for federated LLM unlearning. Extensive\nexperiments demonstrate that Oblivionis outperforms local training, achieving a\nrobust balance between forgetting efficacy and model utility, with\ncross-algorithm comparisons providing clear directions for future LLM\ndevelopment.",
      "pdf_url": "http://arxiv.org/pdf/2508.08875v1",
      "published": "2025-08-12T12:02:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08875v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "BiasGym: Fantastic Biases and How to Find (and Remove) Them",
      "authors": [
        "Sekh Mainul Islam",
        "Nadav Borenstein",
        "Siddhesh Milind Pawar",
        "Haeun Yu",
        "Arnav Arora",
        "Isabelle Augenstein"
      ],
      "abstract": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during training. We demonstrate the effectiveness\nof BiasGym in reducing real-world stereotypes (e.g., people from a country\nbeing `reckless drivers') and in probing fictional associations (e.g., people\nfrom a country having `blue skin'), showing its utility for both safety\ninterventions and interpretability research.",
      "pdf_url": "http://arxiv.org/pdf/2508.08855v1",
      "published": "2025-08-12T11:23:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08855v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Steering Towards Fairness: Mitigating Political Bias in LLMs",
      "authors": [
        "Afrozah Nadeem",
        "Mark Dras",
        "Usman Naseem"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases, particularly\nalong political and economic dimensions. In this paper, we propose a framework\nfor probing and mitigating such biases in decoder-based LLMs through analysis\nof internal model representations. Grounded in the Political Compass Test\n(PCT), our method uses contrastive pairs to extract and compare hidden layer\nactivations from models like Mistral and DeepSeek. We introduce a comprehensive\nactivation extraction pipeline capable of layer-wise analysis across multiple\nideological axes, revealing meaningful disparities linked to political framing.\nOur results show that decoder LLMs systematically encode representational bias\nacross layers, which can be leveraged for effective steering vector-based\nmitigation. This work provides new insights into how political bias is encoded\nin LLMs and offers a principled approach to debiasing beyond surface-level\noutput interventions.",
      "pdf_url": "http://arxiv.org/pdf/2508.08846v1",
      "published": "2025-08-12T11:09:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08846v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "The Roots of International Perceptions: Simulating US Attitude Changes Towards China with LLM Agents",
      "authors": [
        "Nicholas Sukiennik",
        "Yichuan Xu",
        "Yuqing Kan",
        "Jinghua Piao",
        "Yuwei Yan",
        "Chen Gao",
        "Yong Li"
      ],
      "abstract": "The rise of LLMs poses new possibilities in modeling opinion evolution, a\nlong-standing task in simulation, by leveraging advanced reasoning abilities to\nrecreate complex, large-scale human cognitive trends. While most prior works\nfocus on opinion evolution surrounding specific isolated events or the views\nwithin a country, ours is the first to model the large-scale attitude evolution\nof a population representing an entire country towards another -- US citizens'\nperspectives towards China. To tackle the challenges of this broad scenario, we\npropose a framework that integrates media data collection, user profile\ncreation, and cognitive architecture for opinion updates to successfully\nreproduce the real trend of US attitudes towards China over a 20-year period\nfrom 2005 to today. We also leverage LLMs' capabilities to introduce debiased\nmedia exposure, extracting neutral events from typically subjective news\ncontents, to uncover the roots of polarized opinion formation, as well as a\ndevils advocate agent to help explain the rare reversal from negative to\npositive attitudes towards China, corresponding with changes in the way\nAmericans obtain information about the country. The simulation results, beyond\nvalidating our framework architecture, also reveal the impact of biased framing\nand selection bias in shaping attitudes. Overall, our work contributes to a new\nparadigm for LLM-based modeling of cognitive behaviors in a large-scale,\nlong-term, cross-border social context, providing insights into the formation\nof international biases and offering valuable implications for media consumers\nto better understand the factors shaping their perspectives, and ultimately\ncontributing to the larger social need for bias reduction and cross-cultural\ntolerance.",
      "pdf_url": "http://arxiv.org/pdf/2508.08837v1",
      "published": "2025-08-12T10:54:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08837v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ]
    },
    {
      "title": "EditMF: Drawing an Invisible Fingerprint for Your Large Language Models",
      "authors": [
        "Jiaxuan Wu",
        "Yinghan Zhou",
        "Wanli Peng",
        "Yiming Xue",
        "Juan Wen",
        "Ping Zhong"
      ],
      "abstract": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing back-door-based methods suffer\nfrom limited stealth and efficiency. To simultaneously address these issues, we\npropose EditMF, a training-free fingerprinting paradigm that achieves highly\nimperceptible fingerprint embedding with minimal computational overhead.\nOwnership bits are mapped to compact, semantically coherent triples drawn from\nan encrypted artificial knowledge base (e.g., virtual author-novel-protagonist\nfacts). Causal tracing localizes the minimal set of layers influencing each\ntriple, and a zero-space update injects the fingerprint without perturbing\nunrelated knowledge. Verification requires only a single black-box query and\nsucceeds when the model returns the exact pre-embedded protagonist. Empirical\nresults on LLaMA and Qwen families show that EditMF combines high\nimperceptibility with negligible model's performance loss, while delivering\nrobustness far beyond LoRA-based fingerprinting and approaching that of SFT\nembeddings. Extensive experiments demonstrate that EditMF is an effective and\nlow-overhead solution for secure LLM ownership verification.",
      "pdf_url": "http://arxiv.org/pdf/2508.08836v1",
      "published": "2025-08-12T10:52:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08836v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "title": "An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems",
      "authors": [
        "Yuren Hao",
        "Xiang Wan",
        "Chengxiang Zhai"
      ],
      "abstract": "In this paper, we introduce a systematic framework beyond conventional method\nto assess LLMs' mathematical-reasoning robustness by stress-testing them on\nadvanced math problems that are mathematically equivalent but with linguistic\nand parametric variation. These transformations allow us to measure the\nsensitivity of LLMs to non-mathematical perturbations, thereby enabling a more\naccurate evaluation of their mathematical reasoning capabilities. Using this\nnew evaluation methodology, we created PutnamGAP, a new benchmark dataset with\nmultiple mathematically-equivalent variations of competition-level math\nproblems. With the new dataset, we evaluate multiple families of representative\nLLMs and examine their robustness. Across 18 commercial and open-source models\nwe observe sharp performance degradation on the variants. OpenAI's flagship\nreasoning model, O3, scores 49 % on the originals but drops by 4 percentage\npoints on surface variants, and by 10.5 percentage points on core-step-based\nvariants, while smaller models fare far worse. Overall, the results show that\nthe proposed new evaluation methodology is effective for deepening our\nunderstanding of the robustness of LLMs and generating new insights for further\nimproving their mathematical reasoning capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2508.08833v1",
      "published": "2025-08-12T10:40:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08833v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition",
      "authors": [
        "Mustafa Akben",
        "Vinayaka Gude",
        "Haya Ajjan"
      ],
      "abstract": "The ability to discern subtle emotional cues is fundamental to human social\nintelligence. As artificial intelligence (AI) becomes increasingly common, AI's\nability to recognize and respond to human emotions is crucial for effective\nhuman-AI interactions. In particular, whether such systems can match or surpass\nhuman experts remains to be seen. However, the emotional intelligence of AI,\nparticularly multimodal large language models (MLLMs), remains largely\nunexplored. This study evaluates the emotion recognition abilities of MLLMs\nusing the Reading the Mind in the Eyes Test (RMET) and its multiracial\ncounterpart (MRMET), and compares their performance against human participants.\nResults show that, on average, MLLMs outperform humans in accurately\nidentifying emotions across both tests. This trend persists even when comparing\nperformance across low, medium, and expert-level performing groups. Yet when we\naggregate independent human decisions to simulate collective intelligence,\nhuman groups significantly surpass the performance of aggregated MLLM\npredictions, highlighting the wisdom of the crowd. Moreover, a collaborative\napproach (augmented intelligence) that combines human and MLLM predictions\nachieves greater accuracy than either humans or MLLMs alone. These results\nsuggest that while MLLMs exhibit strong emotion recognition at the individual\nlevel, the collective intelligence of humans and the synergistic potential of\nhuman-AI collaboration offer the most promising path toward effective emotional\nAI. We discuss the implications of these findings for the development of\nemotionally intelligent AI systems and future research directions.",
      "pdf_url": "http://arxiv.org/pdf/2508.08830v1",
      "published": "2025-08-12T10:37:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08830v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ]
    },
    {
      "title": "Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination",
      "authors": [
        "Meng Gai",
        "Guoping Wang",
        "Sheng Li"
      ],
      "abstract": "Real-time rendering with global illumination is crucial to afford the user\nrealistic experience in virtual environments. We present a learning-based\nestimator to predict diffuse indirect illumination in screen space, which then\nis combined with direct illumination to synthesize globally-illuminated high\ndynamic range (HDR) results. Our approach tackles the challenges of capturing\nlong-range/long-distance indirect illumination when employing neural networks\nand is generalized to handle complex lighting and scenarios.\n  From the neural network thinking of the solver to the rendering equation, we\npresent a novel network architecture to predict indirect illumination. Our\nnetwork is equipped with a modified attention mechanism that aggregates global\ninformation guided by spacial geometry features, as well as a monochromatic\ndesign that encodes each color channel individually.\n  We conducted extensive evaluations, and the experimental results demonstrate\nour superiority over previous learning-based techniques. Our approach excels at\nhandling complex lighting such as varying-colored lighting and environment\nlighting. It can successfully capture distant indirect illumination and\nsimulates the interreflections between textured surfaces well (i.e., color\nbleeding effects); it can also effectively handle new scenes that are not\npresent in the training dataset.",
      "pdf_url": "http://arxiv.org/pdf/2508.08826v1",
      "published": "2025-08-12T10:36:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08826v1",
      "categories": [
        "cs.GR",
        "cs.AI"
      ]
    },
    {
      "title": "Wavelet Mixture of Experts for Time Series Forecasting",
      "authors": [
        "Zheng Zhou",
        "Yu-Jie Xiong",
        "Jia-Chen Zhang",
        "Chun-Ming Xia",
        "Xi-Jiong Xie"
      ],
      "abstract": "The field of time series forecasting is rapidly advancing, with recent\nlarge-scale Transformers and lightweight Multilayer Perceptron (MLP) models\nshowing strong predictive performance. However, conventional Transformer models\nare often hindered by their large number of parameters and their limited\nability to capture non-stationary features in data through smoothing.\nSimilarly, MLP models struggle to manage multi-channel dependencies\neffectively. To address these limitations, we propose a novel, lightweight time\nseries prediction model, WaveTS-B. This model combines wavelet transforms with\nMLP to capture both periodic and non-stationary characteristics of data in the\nwavelet domain. Building on this foundation, we propose a channel clustering\nstrategy that incorporates a Mixture of Experts (MoE) framework, utilizing a\ngating mechanism and expert network to handle multi-channel dependencies\nefficiently. We propose WaveTS-M, an advanced model tailored for multi-channel\ntime series prediction. Empirical evaluation across eight real-world time\nseries datasets demonstrates that our WaveTS series models achieve\nstate-of-the-art (SOTA) performance with significantly fewer parameters.\nNotably, WaveTS-M shows substantial improvements on multi-channel datasets,\nhighlighting its effectiveness.",
      "pdf_url": "http://arxiv.org/pdf/2508.08825v1",
      "published": "2025-08-12T10:32:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08825v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "OISMA: On-the-fly In-memory Stochastic Multiplication Architecture for Matrix-Multiplication Workloads",
      "authors": [
        "Shady Agwa",
        "Yihan Pan",
        "Georgios Papandroulidakis",
        "Themis Prodromakis"
      ],
      "abstract": "Artificial Intelligence models are currently driven by a significant\nup-scaling of their complexity, with massive matrix multiplication workloads\nrepresenting the major computational bottleneck. In-memory computing\narchitectures are proposed to avoid the Von Neumann bottleneck. However, both\ndigital/binary-based and analogue in-memory computing architectures suffer from\nvarious limitations, which significantly degrade the performance and energy\nefficiency gains. This work proposes OISMA, a novel in-memory computing\narchitecture that utilizes the computational simplicity of a quasi-stochastic\ncomputing domain (Bent-Pyramid system), while keeping the same efficiency,\nscalability, and productivity of digital memories. OISMA converts normal memory\nread operations into in-situ stochastic multiplication operations with a\nnegligible cost. An accumulation periphery then accumulates the output\nmultiplication bitstreams, achieving the matrix multiplication functionality.\nExtensive matrix multiplication benchmarking was conducted to analyze the\naccuracy of the Bent-Pyramid system, using matrix dimensions ranging from 4x4\nto 512x512. The accuracy results show a significant decrease in the average\nrelative Frobenius error, from 9.42% (for 4x4) to 1.81% (for 512x512), compared\nto 64-bit double precision floating-point format. A 1T1R OISMA array of 4 KB\ncapacity was implemented using a commercial 180nm technology node and in-house\nRRAM technology. At 50 MHz, OISMA achieves 0.891 TOPS/W and 3.98 GOPS/mm2 for\nenergy and area efficiency, respectively, occupying an effective computing area\nof 0.804241 mm2. Scaling OISMA from 180nm to 22nm technology shows a\nsignificant improvement of two orders of magnitude in energy efficiency and one\norder of magnitude in area efficiency, compared to dense matrix multiplication\nin-memory computing architectures.",
      "pdf_url": "http://arxiv.org/pdf/2508.08822v1",
      "published": "2025-08-12T10:24:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08822v1",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.ET",
        "cs.PF"
      ]
    },
    {
      "title": "Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation",
      "authors": [
        "Yuechen Wang",
        "Yuming Qiao",
        "Dan Meng",
        "Jun Yang",
        "Haonan Lu",
        "Zhenyu Yang",
        "Xudong Zhang"
      ],
      "abstract": "Multimodal Retrieval-Augmented Generation (mRAG) has emerged as a promising\nsolution to address the temporal limitations of Multimodal Large Language\nModels (MLLMs) in real-world scenarios like news analysis and trending topics.\nHowever, existing approaches often suffer from rigid retrieval strategies and\nunder-utilization of visual information. To bridge this gap, we propose\nE-Agent, an agent framework featuring two key innovations: a mRAG planner\ntrained to dynamically orchestrate multimodal tools based on contextual\nreasoning, and a task executor employing tool-aware execution sequencing to\nimplement optimized mRAG workflows. E-Agent adopts a one-time mRAG planning\nstrategy that enables efficient information retrieval while minimizing\nredundant tool invocations. To rigorously assess the planning capabilities of\nmRAG systems, we introduce the Real-World mRAG Planning (RemPlan) benchmark.\nThis novel benchmark contains both retrieval-dependent and\nretrieval-independent question types, systematically annotated with essential\nretrieval tools required for each instance. The benchmark's explicit mRAG\nplanning annotations and diverse question design enhance its practical\nrelevance by simulating real-world scenarios requiring dynamic mRAG decisions.\nExperiments across RemPlan and three established benchmarks demonstrate\nE-Agent's superiority: 13% accuracy gain over state-of-the-art mRAG methods\nwhile reducing redundant searches by 37%.",
      "pdf_url": "http://arxiv.org/pdf/2508.08816v1",
      "published": "2025-08-12T10:17:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08816v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "GRainsaCK: a Comprehensive Software Library for Benchmarking Explanations of Link Prediction Tasks on Knowledge Graphs",
      "authors": [
        "Roberto Barile",
        "Claudia d'Amato",
        "Nicola Fanizzi"
      ],
      "abstract": "Since Knowledge Graphs are often incomplete, link prediction methods are\nadopted for predicting missing facts. Scalable embedding based solutions are\nmostly adopted for this purpose, however, they lack comprehensibility, which\nmay be crucial in several domains. Explanation methods tackle this issue by\nidentifying supporting knowledge explaining the predicted facts. Regretfully,\nevaluating/comparing quantitatively the resulting explanations is challenging\nas there is no standard evaluation protocol and overall benchmarking resource.\nWe fill this important gap by proposing GRainsaCK, a reusable software resource\nthat fully streamlines all the tasks involved in benchmarking explanations,\ni.e., from model training to evaluation of explanations along the same\nevaluation protocol. Moreover, GRainsaCK furthers modularity/extensibility by\nimplementing the main components as functions that can be easily replaced.\nFinally, fostering its reuse, we provide extensive documentation including a\ntutorial.",
      "pdf_url": "http://arxiv.org/pdf/2508.08815v1",
      "published": "2025-08-12T10:15:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08815v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "TempOpt - Unsupervised Alarm Relation Learning for Telecommunication Networks",
      "authors": [
        "Sathiyanaryanan Sampath",
        "Pratyush Uppuluri",
        "Thirumaran Ekambaram"
      ],
      "abstract": "In a telecommunications network, fault alarms generated by network nodes are\nmonitored in a Network Operations Centre (NOC) to ensure network availability\nand continuous network operations. The monitoring process comprises of tasks\nsuch as active alarms analysis, root alarm identification, and resolution of\nthe underlying problem. Each network node potentially can generate alarms of\ndifferent types, while nodes can be from multiple vendors, a network can have\nhundreds of nodes thus resulting in an enormous volume of alarms at any time.\nSince network nodes are inter-connected, a single fault in the network would\ntrigger multiple sequences of alarms across a variety of nodes and from a\nmonitoring point of view, it is a challenging task for a NOC engineer to be\naware of relations between the various alarms, when trying to identify, for\nexample, a root alarm on which an action needs to be taken. To effectively\nidentify root alarms, it is essential to learn relation among the alarms for\naccurate and faster resolution. In this work we propose a novel unsupervised\nalarm relation learning technique Temporal Optimization (TempOpt) that is\npractical and overcomes the limitations of an existing class of alarm\nrelational learning method-temporal dependency methods. Experiments have been\ncarried on real-world network datasets, that demonstrate the improved quality\nof alarm relations learned by TempOpt as compared to temporal dependency\nmethod.",
      "pdf_url": "http://arxiv.org/pdf/2508.08814v2",
      "published": "2025-08-12T10:15:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08814v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Not in My Backyard! Temporal Voting Over Public Chores",
      "authors": [
        "Edith Elkind",
        "Tzeh Yuan Neoh",
        "Nicholas Teh"
      ],
      "abstract": "We study a temporal voting model where voters have dynamic preferences over a\nset of public chores -- projects that benefit society, but impose individual\ncosts on those affected by their implementation. We investigate the\ncomputational complexity of optimizing utilitarian and egalitarian welfare. Our\nresults show that while optimizing the former is computationally\nstraightforward, minimizing the latter is computationally intractable, even in\nvery restricted cases. Nevertheless, we identify several settings where this\nproblem can be solved efficiently, either exactly or by an approximation\nalgorithm. We also examine the effects of enforcing temporal fairness and its\nimpact on social welfare, and analyze the competitive ratio of online\nalgorithms. We then explore the strategic behavior of agents, providing\ninsights into potential malfeasance in such decision-making environments.\nFinally, we discuss a range of fairness measures and their suitability for our\nsetting.",
      "pdf_url": "http://arxiv.org/pdf/2508.08810v1",
      "published": "2025-08-12T10:06:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.08810v1",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA",
        "econ.TH"
      ]
    }
  ]
}