{
  "last_updated": "2025-05-02T00:50:58.117259",
  "papers": [
    {
      "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments",
      "authors": [
        "Sichang Tu",
        "Abigail Powers",
        "Stephen Doogan",
        "Jinho D. Choi"
      ],
      "abstract": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability.",
      "pdf_url": "http://arxiv.org/pdf/2504.21851v1",
      "published": "2025-04-30T17:58:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21851v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support",
      "authors": [
        "Justin B. Bullock",
        "Janet V. T. Pauketat",
        "Hsini Huang",
        "Yi-Fan Wang",
        "Jacy Reese Anthis"
      ],
      "abstract": "Governance institutions must respond to societal risks, including those posed\nby generative AI. This study empirically examines how public trust in\ninstitutions and AI technologies, along with perceived risks, shape preferences\nfor AI regulation. Using the nationally representative 2023 Artificial\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\ngovernment, AI companies, and AI technologies, as well as public support for\nregulatory measures such as slowing AI development or outright bans on advanced\nAI. Our findings reveal broad public support for AI regulation, with risk\nperception playing a significant role in shaping policy preferences.\nIndividuals with higher trust in government favor regulation, while those with\ngreater trust in AI companies and AI technologies are less inclined to support\nrestrictions. Trust in government and perceived risks significantly predict\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\nAI systems) regulatory interventions. These results highlight the importance of\npublic opinion in AI governance. As AI capabilities advance, effective\nregulation will require balancing public concerns about risks with trust in\ninstitutions. This study provides a foundational empirical baseline for\npolicymakers navigating AI governance and underscores the need for further\nresearch into public trust, risk perception, and regulatory strategies in the\nevolving AI landscape.",
      "pdf_url": "http://arxiv.org/pdf/2504.21849v1",
      "published": "2025-04-30T17:56:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21849v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "title": "Characterizing AI Agents for Alignment and Governance",
      "authors": [
        "Atoosa Kasirzadeh",
        "Iason Gabriel"
      ],
      "abstract": "The creation of effective governance mechanisms for AI agents requires a\ndeeper understanding of their core properties and how these properties relate\nto questions surrounding the deployment and operation of agents in the world.\nThis paper provides a characterization of AI agents that focuses on four\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\ndifferent gradations for each dimension, and argue that each dimension raises\nunique questions about the design, operation, and governance of these systems.\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\ntechnical and non-technical governance challenges posed by different classes of\nAI agents, ranging from narrow task-specific assistants to highly autonomous\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\nthis framework provides developers, policymakers, and members of the public\nwith the opportunity to develop governance approaches that better align with\ncollective societal goals.",
      "pdf_url": "http://arxiv.org/pdf/2504.21848v1",
      "published": "2025-04-30T17:55:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21848v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Active Light Modulation to Counter Manipulation of Speech Visual Content",
      "authors": [
        "Hadleigh Schwartz",
        "Xiaofeng Yan",
        "Charles J. Carver",
        "Xia Zhou"
      ],
      "abstract": "High-profile speech videos are prime targets for falsification, owing to\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\nand unobtrusive system for protecting live speech videos from visual\nfalsification of speaker identity and lip and facial motion. Unlike predominant\nfalsification detection methods operating in the digital domain, Spotlight\ncreates dynamic physical signatures at the event site and embeds them into all\nvideo recordings via imperceptible modulated light. These physical signatures\nencode semantically-meaningful features unique to the speech event, including\nthe speaker's identity and facial motion, and are cryptographically-secured to\nprevent spoofing. The signatures can be extracted from any video downstream and\nvalidated against the portrayed speech content to check its integrity. Key\nelements of Spotlight include (1) a framework for generating extremely compact\n(i.e., 150-bit), pose-invariant speech video features, based on\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\n>200 bps into video while remaining imperceptible both in video and live.\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\nvideos. Further, Spotlight is highly robust across recording conditions, video\npost-processing techniques, and white-box adversarial attacks on its video\nfeature extraction methodologies.",
      "pdf_url": "http://arxiv.org/pdf/2504.21846v1",
      "published": "2025-04-30T17:55:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21846v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization",
      "authors": [
        "Anas Anwarul Haq Khan",
        "Utkarsh Verma",
        "Prateek Chanda",
        "Ganesh Ramakrishnan"
      ],
      "abstract": "We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research.",
      "pdf_url": "http://arxiv.org/pdf/2504.21831v1",
      "published": "2025-04-30T17:37:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21831v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition",
      "authors": [
        "Z. Z. Ren",
        "Zhihong Shao",
        "Junxiao Song",
        "Huajian Xin",
        "Haocheng Wang",
        "Wanjia Zhao",
        "Liyue Zhang",
        "Zhe Fu",
        "Qihao Zhu",
        "Dejian Yang",
        "Z. F. Wu",
        "Zhibin Gou",
        "Shirong Ma",
        "Hongxuan Tang",
        "Yuxuan Liu",
        "Wenjun Gao",
        "Daya Guo",
        "Chong Ruan"
      ],
      "abstract": "We introduce DeepSeek-Prover-V2, an open-source large language model designed\nfor formal theorem proving in Lean 4, with initialization data collected\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\nstep-by-step reasoning, to create an initial cold start for reinforcement\nlearning. This process enables us to integrate both informal and formal\nmathematical reasoning into a unified model. The resulting model,\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\nevaluation, including 15 selected problems from the recent AIME competitions\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\nthese problems using majority voting, highlighting that the gap between formal\nand informal mathematical reasoning in large language models is substantially\nnarrowing.",
      "pdf_url": "http://arxiv.org/pdf/2504.21801v1",
      "published": "2025-04-30T16:57:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21801v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues",
      "authors": [
        "Suhas BN",
        "Dominik Mattioli",
        "Saeed Abdullah",
        "Rosa I. Arriaga",
        "Chris W. Wiese",
        "Andrew M. Sherrill"
      ],
      "abstract": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. Synthetic therapy dialogues closely match structural\nfeatures of real-world conversations (e.g., speaker switch ratio: 0.98 vs.\n0.99); however, they may not adequately reflect key fidelity markers (e.g.,\ndistress monitoring). We highlight gaps in existing evaluation frameworks and\nadvocate for fidelity-aware metrics that go beyond surface fluency to uncover\nclinically significant failures. Our findings clarify where synthetic data can\neffectively complement real-world datasets -- and where critical limitations\nremain.",
      "pdf_url": "http://arxiv.org/pdf/2504.21800v2",
      "published": "2025-04-30T16:56:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21800v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "68T50",
        "I.2.7; H.3.1"
      ]
    },
    {
      "title": "SWE-smith: Scaling Data for Software Engineering Agents",
      "authors": [
        "John Yang",
        "Kilian Leret",
        "Carlos E. Jimenez",
        "Alexander Wettig",
        "Kabir Khandpur",
        "Yanzhe Zhang",
        "Binyuan Hui",
        "Ofir Press",
        "Ludwig Schmidt",
        "Diyi Yang"
      ],
      "abstract": "Despite recent progress in Language Models (LMs) for software engineering,\ncollecting training data remains a significant pain point. Existing datasets\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\nrepositories. The procedures to curate such datasets are often complex,\nnecessitating hundreds of hours of human labor; companion execution\nenvironments also take up several terabytes of storage, severely limiting their\nscalability and usability. To address this pain point, we introduce SWE-smith,\na novel pipeline for generating software engineering training data at scale.\nGiven any Python codebase, SWE-smith constructs a corresponding execution\nenvironment, then automatically synthesizes 100s to 1,000s of task instances\nthat break existing test(s) in the codebase. Using SWE-smith, we create a\ndataset of 50k instances sourced from 128 GitHub repositories, an order of\nmagnitude larger than all previous works. We train SWE-agent-LM-32B, achieving\n40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art\namong open source models. We open source SWE-smith (collection procedure, task\ninstances, trajectories, models) to lower the barrier of entry for research in\nLM systems for automated software engineering. All assets available at\nhttps://swesmith.com.",
      "pdf_url": "http://arxiv.org/pdf/2504.21798v1",
      "published": "2025-04-30T16:56:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21798v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
      "authors": [
        "Xiaoxi Li",
        "Jiajie Jin",
        "Guanting Dong",
        "Hongjin Qian",
        "Yutao Zhu",
        "Yongkang Wu",
        "Ji-Rong Wen",
        "Zhicheng Dou"
      ],
      "abstract": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a \\textbf{Deep Web\nExplorer} module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\n\\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\n\\textbf{RL-based training strategy} via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.",
      "pdf_url": "http://arxiv.org/pdf/2504.21776v1",
      "published": "2025-04-30T16:25:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21776v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning",
      "authors": [
        "Rongguang Ye",
        "Ming Tang"
      ],
      "abstract": "Recent methods leverage a hypernet to handle the performance-fairness\ntrade-offs in federated learning. This hypernet maps the clients' preferences\nbetween model performance and fairness to preference-specifc models on the\ntrade-off curve, known as local Pareto front. However, existing methods\ntypically adopt a uniform preference sampling distribution to train the\nhypernet across clients, neglecting the inherent heterogeneity of their local\nPareto fronts. Meanwhile, from the perspective of generalization, they do not\nconsider the gap between local and global Pareto fronts on the global dataset.\nTo address these limitations, we propose HetPFL to effectively learn both local\nand global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)\nand Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the\noptimal preference sampling distribution for each client to accommodate\nheterogeneous local Pareto fronts. While PHF performs preference-aware fusion\nof clients' hypernets to ensure the performance of the global Pareto front. We\nprove that HetPFL converges linearly with respect to the number of rounds,\nunder weaker assumptions than existing methods. Extensive experiments on four\ndatasets show that HetPFL significantly outperforms seven baselines in terms of\nthe quality of learned local and global Pareto fronts.",
      "pdf_url": "http://arxiv.org/pdf/2504.21775v1",
      "published": "2025-04-30T16:25:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21775v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?",
      "authors": [
        "Jiuwu Hao",
        "Liguo Sun",
        "Yuting Wan",
        "Yueyang Wu",
        "Ti Xiang",
        "Haolin Song",
        "Pin Lv"
      ],
      "abstract": "Collaborative perception enhances environmental awareness through inter-agent\ncommunication and is regarded as a promising solution to intelligent\ntransportation systems. However, existing collaborative methods for Unmanned\nAerial Vehicles (UAVs) overlook the unique characteristics of the UAV\nperspective, resulting in substantial communication overhead. To address this\nissue, we propose a novel communication-efficient collaborative perception\nframework based on late-intermediate fusion, dubbed LIF. The core concept is to\nexchange informative and compact detection results and shift the fusion stage\nto the feature representation level. In particular, we leverage vision-guided\npositional embedding (VPE) and box-based virtual augmented feature (BoBEV) to\neffectively integrate complementary information from various agents.\nAdditionally, we innovatively introduce an uncertainty-driven communication\nmechanism that uses uncertainty evaluation to select high-quality and reliable\nshared areas. Experimental results demonstrate that our LIF achieves superior\nperformance with minimal communication bandwidth, proving its effectiveness and\npracticality. Code and models are available at https://github.com/uestchjw/LIF.",
      "pdf_url": "http://arxiv.org/pdf/2504.21774v1",
      "published": "2025-04-30T16:22:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21774v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness",
      "authors": [
        "Junsheng Huang",
        "Zhitao He",
        "Sandeep Polisetty",
        "Qingyun Wang",
        "May Fung"
      ],
      "abstract": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.",
      "pdf_url": "http://arxiv.org/pdf/2504.21773v1",
      "published": "2025-04-30T16:17:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21773v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline",
      "authors": [
        "Minwoo Oh",
        "Minsu Park",
        "Eunil Park"
      ],
      "abstract": "Short video platforms like YouTube Shorts and TikTok face significant\ncopyright compliance challenges, as infringers frequently embed arbitrary\nbackground music (BGM) to obscure original soundtracks (OST) and evade content\noriginality detection. To tackle this issue, we propose a novel pipeline that\nintegrates Music Source Separation (MSS) and cross-modal video-music retrieval\n(CMVMR). Our approach effectively separates arbitrary BGM from the original\nOST, enabling the restoration of authentic video audio tracks. To support this\nwork, we introduce two domain-specific datasets: OASD-20K for audio separation\nand OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips\nfeaturing mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset\ncomprising 1,121 video and mixed-audio pairs, specifically designed for short\nvideo restoration tasks. Experimental results demonstrate that our pipeline not\nonly removes arbitrary BGM with high accuracy but also restores OSTs, ensuring\ncontent integrity. This approach provides an ethical and scalable solution to\ncopyright challenges in user-generated content on short video platforms.",
      "pdf_url": "http://arxiv.org/pdf/2504.21772v1",
      "published": "2025-04-30T16:17:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21772v1",
      "categories": [
        "cs.MM",
        "cs.AI"
      ]
    },
    {
      "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning",
      "authors": [
        "Feiyu Lu",
        "Mengyu Chen",
        "Hsiang Hsu",
        "Pranav Deshpande",
        "Cheng Yao Wang",
        "Blair MacIntyre"
      ],
      "abstract": "Mixed Reality (MR) could assist users' tasks by continuously integrating\nvirtual content with their view of the physical environment. However, where and\nhow to place these content to best support the users has been a challenging\nproblem due to the dynamic nature of MR experiences. In contrast to prior work\nthat investigates optimization-based methods, we are exploring how\nreinforcement learning (RL) could assist with continuous 3D content placement\nthat is aware of users' poses and their surrounding environments. Through an\ninitial exploration and preliminary evaluation, our results demonstrate the\npotential of RL to position content that maximizes the reward for users on the\ngo. We further identify future directions for research that could harness the\npower of RL for personalized and optimized UI and content placement in MR.",
      "pdf_url": "http://arxiv.org/pdf/2504.21731v1",
      "published": "2025-04-30T15:21:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21731v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense",
      "authors": [
        "Ting Qiao",
        "Yingjia Wang",
        "Xing Liu",
        "Sixing Wu",
        "Jianbing Li",
        "Yiming Li"
      ],
      "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB.",
      "pdf_url": "http://arxiv.org/pdf/2504.21730v1",
      "published": "2025-04-30T15:21:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21730v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Sionna RT: Technical Report",
      "authors": [
        "Fayçal Aït Aoudia",
        "Jakob Hoydis",
        "Merlin Nimier-David",
        "Sebastian Cammerer",
        "Alexander Keller"
      ],
      "abstract": "Sionna is an open-source, GPU-accelerated library that, as of version 0.14,\nincorporates a ray tracer for simulating radio wave propagation. A unique\nfeature of Sionna RT is differentiability, enabling the calculation of\ngradients for the channel impulse responses (CIRs), radio maps, and other\nrelated metrics with respect to system and environmental parameters, such as\nmaterial properties, antenna patterns, and array geometries. The release of\nSionna 1.0 provides a complete overhaul of the ray tracer, significantly\nimproving its speed, memory efficiency, and extensibility. This document\ndetails the algorithms employed by Sionna RT to simulate radio wave propagation\nefficiently, while also addressing their current limitations. Given that the\ncomputation of CIRs and radio maps requires distinct algorithms, these are\ndetailed in separate sections. For CIRs, Sionna RT integrates shooting and\nbouncing of rays (SBR) with the image method and uses a hashing-based mechanism\nto efficiently eliminate duplicate paths. Radio maps are computed using a\npurely SBR-based approach.",
      "pdf_url": "http://arxiv.org/pdf/2504.21719v1",
      "published": "2025-04-30T15:05:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21719v1",
      "categories": [
        "cs.IT",
        "cs.AI",
        "eess.SP",
        "math.IT"
      ]
    },
    {
      "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics",
      "authors": [
        "Marc Glocker",
        "Peter Hönig",
        "Matthias Hirschmanner",
        "Markus Vincze"
      ],
      "abstract": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr.",
      "pdf_url": "http://arxiv.org/pdf/2504.21716v1",
      "published": "2025-04-30T15:00:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21716v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning",
      "authors": [
        "Anthony D Martin"
      ],
      "abstract": "We propose a generalization of modern representation learning objectives by\nreframing them as recursive divergence alignment processes over localized\nconditional distributions While recent frameworks like Information Contrastive\nLearning I-Con unify multiple learning paradigms through KL divergence between\nfixed neighborhood conditionals we argue this view underplays a crucial\nrecursive structure inherent in the learning process. We introduce Recursive KL\nDivergence Optimization RKDO a dynamic formalism where representation learning\nis framed as the evolution of KL divergences across data neighborhoods. This\nformulation captures contrastive clustering and dimensionality reduction\nmethods as static slices while offering a new path to model stability and local\nadaptation. Our experiments demonstrate that RKDO offers dual efficiency\nadvantages approximately 30 percent lower loss values compared to static\napproaches across three different datasets and 60 to 80 percent reduction in\ncomputational resources needed to achieve comparable results. This suggests\nthat RKDOs recursive updating mechanism provides a fundamentally more efficient\noptimization landscape for representation learning with significant\nimplications for resource constrained applications.",
      "pdf_url": "http://arxiv.org/pdf/2504.21707v1",
      "published": "2025-04-30T14:51:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21707v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "cs.NE",
        "math.IT"
      ]
    },
    {
      "title": "Vision Transformers in Precision Agriculture: A Comprehensive Survey",
      "authors": [
        "Saber Mehdipour",
        "Seyed Abolghasem Mirroshandel",
        "Seyed Amirhossein Tabatabaei"
      ],
      "abstract": "Detecting plant diseases is a crucial aspect of modern agriculture - it plays\na key role in maintaining crop health and increasing overall yield. Traditional\napproaches, though still valuable, often rely on manual inspection or\nconventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering benefits such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This survey\nexplores the application of ViTs in precision agriculture, covering tasks from\nclassification to detection and segmentation. We begin by introducing the\nfoundational architecture of ViTs and discuss their transition from Natural\nLanguage Processing (NLP) to computer vision. The discussion includes the\nconcept of inductive bias in traditional models like Convolutional Neural\nNetworks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive\nreview of recent literature, focusing on key methodologies, datasets, and\nperformance metrics. The survey also includes a comparative analysis of CNNs\nand ViTs, with a look at hybrid models and performance enhancements. Technical\nchallenges - such as data requirements, computational demands, and model\ninterpretability - are addressed alongside potential solutions. Finally, we\noutline potential research directions and technological advancements that could\nfurther support the integration of ViTs in real-world agricultural settings.\nOur goal with this study is to offer practitioners and researchers a deeper\nunderstanding of how ViTs are poised to transform smart and precision\nagriculture.",
      "pdf_url": "http://arxiv.org/pdf/2504.21706v1",
      "published": "2025-04-30T14:50:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21706v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs",
      "authors": [
        "Marco Arazzi",
        "Vignesh Kumar Kembu",
        "Antonino Nocera",
        "Vinod P"
      ],
      "abstract": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack.",
      "pdf_url": "http://arxiv.org/pdf/2504.21700v1",
      "published": "2025-04-30T14:44:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21700v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling",
      "authors": [
        "Stavrow A. Bahnam",
        "Christophe De Wagter",
        "Guido C. H. E. de Croon"
      ],
      "abstract": "Ego-motion estimation is vital for drones when flying in GPS-denied\nenvironments. Vision-based methods struggle when flight speed increases and\nclose-by objects lead to difficult visual conditions with considerable motion\nblur and large occlusions. To tackle this, vision is typically complemented by\nstate estimation filters that combine a drone model with inertial measurements.\nHowever, these drone models are currently learned in a supervised manner with\nground-truth data from external motion capture systems, limiting scalability to\ndifferent environments and drones. In this work, we propose a self-supervised\nlearning scheme to train a neural-network-based drone model using only onboard\nmonocular video and flight controller data (IMU and motor feedback). We achieve\nthis by first training a self-supervised relative pose estimation model, which\nthen serves as a teacher for the drone model. To allow this to work at high\nspeed close to obstacles, we propose an improved occlusion handling method for\ntraining self-supervised pose estimation models. Due to this method, the root\nmean squared error of resulting odometry estimates is reduced by an average of\n15%. Moreover, the student neural drone model can be successfully obtained from\nthe onboard data. It even becomes more accurate at higher speeds compared to\nits teacher, the self-supervised vision-based model. We demonstrate the value\nof the neural drone model by integrating it into a traditional filter-based VIO\nsystem (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing\ntrajectories near obstacles. Self-supervised learning of ego-motion estimation\nrepresents a significant step toward bridging the gap between flying in\ncontrolled, expensive lab environments and real-world drone applications. The\nfusion of vision and drone models will enable higher-speed flight and improve\nstate estimation, on any drone in any environment.",
      "pdf_url": "http://arxiv.org/pdf/2504.21695v1",
      "published": "2025-04-30T14:38:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21695v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation",
      "authors": [
        "Tom Westermann",
        "Malte Ramonat",
        "Johannes Hujer",
        "Felix Gehlhoff",
        "Alexander Fay"
      ],
      "abstract": "AutomationML has seen widespread adoption as an open data exchange format in\nthe automation domain. It is an open and vendor neutral standard based on the\nextensible markup language XML. However, AutomationML extends XML with\nadditional semantics, that limit the applicability of common XML-tools for\napplications like querying or data validation. This article provides\npractitioners with 1) an up-to-date ontology of the concepts in the\nAutomationML-standard, as well as 2) a declarative mapping to automatically\ntransform any AutomationML model into RDF triples. Together, these artifacts\nallow practitioners an easy integration of AutomationML information into\nindustrial knowledge graphs. A study on examples from the automation domain\nconcludes that transforming AutomationML to OWL opens up new powerful ways for\nquerying and validation that are impossible without transformation.",
      "pdf_url": "http://arxiv.org/pdf/2504.21694v1",
      "published": "2025-04-30T14:34:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21694v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction",
      "authors": [
        "Zihan Zhou",
        "Changrui Dai",
        "Aibo Song",
        "Xiaolin Fang"
      ],
      "abstract": "Successful video analysis relies on accurate recognition of pixels across\nframes, and frame reconstruction methods based on video correspondence learning\nare popular due to their efficiency. Existing frame reconstruction methods,\nwhile efficient, neglect the value of direct involvement of multiple reference\nframes for reconstruction and decision-making aspects, especially in complex\nsituations such as occlusion or fast movement. In this paper, we introduce a\nDynamic Memory Prediction (DMP) framework that innovatively utilizes multiple\nreference frames to concisely and directly enhance frame reconstruction. Its\ncore component is a Reference Frame Memory Engine that dynamically selects\nframes based on object pixel features to improve tracking accuracy. In\naddition, a Bidirectional Target Prediction Network is built to utilize\nmultiple reference frames to improve the robustness of the model. Through\nexperiments, our algorithm outperforms the state-of-the-art self-supervised\ntechniques on two fine-grained video object tracking tasks: object segmentation\nand keypoint tracking.",
      "pdf_url": "http://arxiv.org/pdf/2504.21692v1",
      "published": "2025-04-30T14:29:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21692v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning",
      "authors": [
        "Reem Abdel-Salam",
        "Mary Adewunmi"
      ],
      "abstract": "Health Mention Classification (HMC) plays a critical role in leveraging\nsocial media posts for real-time tracking and public health monitoring.\nNevertheless, the process of HMC presents significant challenges due to its\nintricate nature, primarily stemming from the contextual aspects of health\nmentions, such as figurative language and descriptive terminology, rather than\nexplicitly reflecting a personal ailment. To address this problem, we argue\nthat clearer mentions can be achieved through conventional fine-tuning with\nenhanced parameters of biomedical natural language methods (NLP). In this\nstudy, we explore different techniques such as the utilisation of\npart-of-speech (POS) tagger information, improving on PEFT techniques, and\ndifferent combinations thereof. Extensive experiments are conducted on three\nwidely used datasets: RHDM, PHM, and Illness. The results incorporated POS\ntagger information, and leveraging PEFT techniques significantly improves\nperformance in terms of F1-score compared to state-of-the-art methods across\nall three datasets by utilising smaller models and efficient training.\nFurthermore, the findings highlight the effectiveness of incorporating POS\ntagger information and leveraging PEFT techniques for HMC. In conclusion, the\nproposed methodology presents a potentially effective approach to accurately\nclassifying health mentions in social media posts while optimising the model\nsize and training efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2504.21685v1",
      "published": "2025-04-30T14:21:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21685v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Extension-ranking Semantics for Abstract Argumentation Preprint",
      "authors": [
        "Kenneth Skiba",
        "Tjitze Rienstra",
        "Matthias Thimm",
        "Jesse Heyninck",
        "Gabriele Kern-Isberner"
      ],
      "abstract": "In this paper, we present a general framework for ranking sets of arguments\nin abstract argumentation based on their plausibility of acceptance. We present\na generalisation of Dung's extension semantics as extension-ranking semantics,\nwhich induce a preorder over the power set of all arguments, allowing us to\nstate that one set is \"closer\" to being acceptable than another. To evaluate\nthe extension-ranking semantics, we introduce a number of principles that a\nwell-behaved extension-ranking semantics should satisfy. We consider several\nsimple base relations, each of which models a single central aspect of\nargumentative reasoning. The combination of these base relations provides us\nwith a family of extension-ranking semantics. We also adapt a number of\napproaches from the literature for ranking extensions to be usable in the\ncontext of extension-ranking semantics, and evaluate their behaviour.",
      "pdf_url": "http://arxiv.org/pdf/2504.21683v1",
      "published": "2025-04-30T14:19:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21683v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization",
      "authors": [
        "Haotian Luo",
        "Haiying He",
        "Yibo Wang",
        "Jinluan Yang",
        "Rui Liu",
        "Naiqiang Tan",
        "Xiaochun Cao",
        "Dacheng Tao",
        "Li Shen"
      ],
      "abstract": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
      "pdf_url": "http://arxiv.org/pdf/2504.21659v1",
      "published": "2025-04-30T14:01:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21659v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation",
      "authors": [
        "Luca Marzari",
        "Francesco Trotti",
        "Enrico Marchesini",
        "Alessandro Farinelli"
      ],
      "abstract": "Achieving safe autonomous navigation systems is critical for deploying robots\nin dynamic and uncertain real-world environments. In this paper, we propose a\nhierarchical control framework leveraging neural network verification\ntechniques to design control barrier functions (CBFs) and policy correction\nmechanisms that ensure safe reinforcement learning navigation policies. Our\napproach relies on probabilistic enumeration to identify unsafe regions of\noperation, which are then used to construct a safe CBF-based control layer\napplicable to arbitrary policies. We validate our framework both in simulation\nand on a real robot, using a standard mobile robot benchmark and a highly\ndynamic aquatic environmental monitoring task. These experiments demonstrate\nthe ability of the proposed solution to correct unsafe actions while preserving\nefficient navigation behavior. Our results show the promise of developing\nhierarchical verification-based systems to enable safe and robust navigation\nbehaviors in complex scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2504.21643v1",
      "published": "2025-04-30T13:47:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21643v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
      "authors": [
        "Zeina Aldallal",
        "Sara Chrouf",
        "Khalil Hennara",
        "Mohamed Motaism Hamed",
        "Muhammad Hreden",
        "Safwan AlModhayan"
      ],
      "abstract": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools.",
      "pdf_url": "http://arxiv.org/pdf/2504.21635v1",
      "published": "2025-04-30T13:37:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21635v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data",
      "authors": [
        "Chih-Cheng Rex Yuan",
        "Bow-Yaw Wang"
      ],
      "abstract": "Fairness auditing of AI systems can identify and quantify biases. However,\ntraditional auditing using real-world data raises security and privacy\nconcerns. It exposes auditors to security risks as they become custodians of\nsensitive information and targets for cyberattacks. Privacy risks arise even\nwithout direct breaches, as data analyses can inadvertently expose confidential\ninformation. To address these, we propose a framework that leverages\ndifferentially private synthetic data to audit the fairness of AI systems. By\napplying privacy-preserving mechanisms, it generates synthetic data that\nmirrors the statistical properties of the original dataset while ensuring\nprivacy. This method balances the goal of rigorous fairness auditing and the\nneed for strong privacy protections. Through experiments on real datasets like\nAdult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real\ndata. By analyzing the alignment and discrepancies between these metrics, we\nassess the capacity of synthetic data to preserve the fairness properties of\nreal data. Our results demonstrate the framework's ability to enable meaningful\nfairness evaluations while safeguarding sensitive information, proving its\napplicability across critical and sensitive domains.",
      "pdf_url": "http://arxiv.org/pdf/2504.21634v1",
      "published": "2025-04-30T13:36:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21634v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations",
      "authors": [
        "Jonas Gwozdz",
        "Andreas Both"
      ],
      "abstract": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy.",
      "pdf_url": "http://arxiv.org/pdf/2504.21605v1",
      "published": "2025-04-30T13:06:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21605v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning",
      "authors": [
        "Huihui Guo",
        "Huilong Pi",
        "Yunchuan Qin",
        "Zhuo Tang",
        "Kenli Li"
      ],
      "abstract": "With the rapid advancement of artificial intelligence, there is an increasing\ndemand for intelligent robots capable of assisting humans in daily tasks and\nperforming complex operations. Such robots not only require task planning\ncapabilities but must also execute tasks with stability and robustness. In this\npaper, we present a closed-loop task planning and acting system, LLM-PAS, which\nis assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans\nlong-horizon tasks in a manner similar to traditional task and motion planners,\nit also emphasizes the execution phase of the task. By transferring part of the\nconstraint-checking process from the planning phase to the execution phase,\nLLM-PAS enables exploration of the constraint space and delivers more accurate\nfeedback on environmental anomalies during execution. The reasoning\ncapabilities of the LLM allow it to handle anomalies that cannot be addressed\nby the robust executor. To further enhance the system's ability to assist the\nplanner during replanning, we propose the First Look Prompting (FLP) method,\nwhich induces LLM to generate effective PDDL goals. Through comparative\nprompting experiments and systematic experiments, we demonstrate the\neffectiveness and robustness of LLM-PAS in handling anomalous conditions during\ntask execution.",
      "pdf_url": "http://arxiv.org/pdf/2504.21596v1",
      "published": "2025-04-30T12:53:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21596v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing",
      "authors": [
        "Lisa Kluge",
        "Maximilian Kähler"
      ],
      "abstract": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts.",
      "pdf_url": "http://arxiv.org/pdf/2504.21589v1",
      "published": "2025-04-30T12:47:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21589v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL",
        "I.2.7"
      ]
    },
    {
      "title": "One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms",
      "authors": [
        "Robin Ferede",
        "Till Blaha",
        "Erin Lucassen",
        "Christophe De Wagter",
        "Guido C. H. E. de Croon"
      ],
      "abstract": "In high-speed quadcopter racing, finding a single controller that works well\nacross different platforms remains challenging. This work presents the first\nneural network controller for drone racing that generalizes across physically\ndistinct quadcopters. We demonstrate that a single network, trained with domain\nrandomization, can robustly control various types of quadcopters. The network\nrelies solely on the current state to directly compute motor commands. The\neffectiveness of this generalized controller is validated through real-world\ntests on two substantially different crafts (3-inch and 5-inch race\nquadcopters). We further compare the performance of this generalized controller\nwith controllers specifically trained for the 3-inch and 5-inch drone, using\ntheir identified model parameters with varying levels of domain randomization\n(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower\nspeeds compared to the fine-tuned models, it excels in adaptability across\ndifferent platforms. Our results show that no randomization fails sim-to-real\ntransfer while increasing randomization improves robustness but reduces speed.\nDespite this trade-off, our findings highlight the potential of domain\nrandomization for generalizing controllers, paving the way for universal AI\ncontrollers that can adapt to any platform.",
      "pdf_url": "http://arxiv.org/pdf/2504.21586v1",
      "published": "2025-04-30T12:44:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21586v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning",
      "authors": [
        "Yingzhuo Jiang",
        "Wenjun Huang",
        "Rongdun Lin",
        "Chenyang Miao",
        "Tianfu Sun",
        "Yunduan Cui"
      ],
      "abstract": "This paper tackles the challenge of learning multi-goal dexterous hand\nmanipulation tasks using model-based Reinforcement Learning. We propose\nGoal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing\nprobabilistic neural network ensembles to describe the high-dimensional\ndexterous hand dynamics and introducing an asynchronous MPC policy to meet the\ncontrol frequency requirements in real-world dexterous hand systems. Extensive\nevaluations on four simulated Shadow Hand manipulation scenarios with randomly\ngenerated goals demonstrate GC-PMPC's superior performance over\nstate-of-the-art baselines. It successfully drives a cable-driven Dexterous\nhand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn\nmanipulating a cubic die to three goal poses within approximately 80 minutes of\ninteractions, demonstrating exceptional learning efficiency and control\nperformance on a cost-effective dexterous hand platform.",
      "pdf_url": "http://arxiv.org/pdf/2504.21585v1",
      "published": "2025-04-30T12:44:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21585v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework",
      "authors": [
        "Qirui Mi",
        "Mengyue Yang",
        "Xiangning Yu",
        "Zhiyu Zhao",
        "Cheng Deng",
        "Bo An",
        "Haifeng Zhang",
        "Xu Chen",
        "Jun Wang"
      ],
      "abstract": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it arises from dynamic interactions among individuals.\nWhile large language models (LLMs) show promise for social simulation, existing\napproaches often exhibit deviations from real-world data. To address this gap,\nwe propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the\nfeedback loop between micro-level decisions and macro-level population. MF-LLM\nalternates between two models: a policy model that generates individual actions\nbased on personal states and group-level information, and a mean field model\nthat updates the population distribution from the latest individual decisions.\nTogether, they produce rollouts that simulate the evolving trajectories of\ncollective decision-making. To better match real-world data, we introduce\nIB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck\nprinciple, which maximizes the relevance of population distributions to future\nactions while minimizing redundancy with historical data. We evaluate MF-LLM on\na real-world social dataset, where it reduces KL divergence to human population\ndistributions by 47 percent over non-mean-field baselines, and enables accurate\ntrend forecasting and intervention planning. It generalizes across seven\ndomains and four LLM backbones, providing a scalable foundation for\nhigh-fidelity social simulation.",
      "pdf_url": "http://arxiv.org/pdf/2504.21582v1",
      "published": "2025-04-30T12:41:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21582v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks",
      "authors": [
        "Shui-jin Rong",
        "Wei Guo",
        "Da-qing Zhang"
      ],
      "abstract": "Aiming at the group decision - making problem with multi - objective\nattributes, this study proposes a group decision - making system that\nintegrates fuzzy inference and Bayesian network. A fuzzy rule base is\nconstructed by combining threshold values, membership functions, expert\nexperience, and domain knowledge to address quantitative challenges such as\nscale differences and expert linguistic variables. A hierarchical Bayesian\nnetwork is designed, featuring a directed acyclic graph with nodes selected by\nexperts, and maximum likelihood estimation is used to dynamically optimize the\nconditional probability table, modeling the nonlinear correlations among\nmultidimensional indices for posterior probability aggregation. In a\ncomprehensive student evaluation case, this method is compared with the\ntraditional weighted scoring approach. The results indicate that the proposed\nmethod demonstrates effectiveness in both rule criterion construction and\nranking consistency, with a classification accuracy of 86.0% and an F1 value\nimprovement of 53.4% over the traditional method. Additionally, computational\nexperiments on real - world datasets across various group decision scenarios\nassess the method's performance and robustness, providing evidence of its\nreliability in diverse contexts.",
      "pdf_url": "http://arxiv.org/pdf/2504.21568v1",
      "published": "2025-04-30T12:14:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21568v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Towards proactive self-adaptive AI for non-stationary environments with dataset shifts",
      "authors": [
        "David Fernández Narro",
        "Pablo Ferri",
        "Juan M. García-Gómez",
        "Carlos Sáez"
      ],
      "abstract": "Artificial Intelligence (AI) models deployed in production frequently face\nchallenges in maintaining their performance in non-stationary environments.\nThis issue is particularly noticeable in medical settings, where temporal\ndataset shifts often occur. These shifts arise when the distributions of\ntraining data differ from those of the data encountered during deployment over\ntime. Further, new labeled data to continuously retrain AI is not typically\navailable in a timely manner due to data access limitations. To address these\nchallenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,\nwhere we model the temporal trajectory of AI parameters, allowing us to\nshort-term forecast parameter values. To this end, we use polynomial spline\nbases, within an extensible Functional Data Analysis framework. We validate our\nmethodology with a logistic regression model addressing prior probability\nshift, covariate shift, and concept shift. This validation is conducted on both\na controlled simulated dataset and a publicly available real-world COVID-19\ndataset from Mexico, with various shifts occurring between 2020 and 2024. Our\nresults indicate that this approach enhances the performance of AI against\nshifts compared to baseline stable models trained at different time distances\nfrom the present, without requiring updated training data. This work lays the\nfoundation for pro-adaptive AI research against dynamic, non-stationary\nenvironments, being compatible with data protection, in resilient AI production\nenvironments for health.",
      "pdf_url": "http://arxiv.org/pdf/2504.21565v1",
      "published": "2025-04-30T12:09:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21565v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.8"
      ]
    },
    {
      "title": "eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes",
      "authors": [
        "Henry John Krumb",
        "Anirban Mukhopadhyay"
      ],
      "abstract": "Wireless Capsule Endoscopy is a non-invasive imaging method for the entire\ngastrointestinal tract, and is a pain-free alternative to traditional\nendoscopy. It generates extensive video data that requires significant review\ntime, and localizing the capsule after ingestion is a challenge. Techniques\nlike bleeding detection and depth estimation can help with localization of\npathologies, but deep learning models are typically too large to run directly\non the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and\ndepth estimation are trained on capsule endoscopic images. For monocular depth\nestimation, we distill a large foundation model into the lean NCA architecture,\nby treating the outputs of the foundation model as pseudo ground truth. We then\nport the trained NCA to the ESP32 microcontroller, enabling efficient image\nprocessing on hardware as small as a camera capsule. NCA are more accurate\n(Dice) than other portable segmentation models, while requiring more than 100x\nfewer parameters stored in memory than other small-scale models. The visual\nresults of NCA depth estimation look convincing, and in some cases beat the\nrealism and detail of the pseudo ground truth. Runtime optimizations on the\nESP32-S3 accelerate the average inference speed significantly, by more than\nfactor 3. With several algorithmic adjustments and distillation, it is possible\nto eNCApsulate NCA models into microcontrollers that fit into wireless capsule\nendoscopes. This is the first work that enables reliable bleeding segmentation\nand depth estimation on a miniaturized device, paving the way for precise\ndiagnosis combined with visual odometry as a means of precise localization of\nthe capsule -- on the capsule.",
      "pdf_url": "http://arxiv.org/pdf/2504.21562v1",
      "published": "2025-04-30T12:06:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21562v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models",
      "authors": [
        "Sangmin Woo",
        "Kang Zhou",
        "Yun Zhou",
        "Shuai Wang",
        "Sheng Guan",
        "Haibo Ding",
        "Lin Lee Cheong"
      ],
      "abstract": "Large Vision Language Models (LVLMs) often suffer from object hallucination,\nwhich undermines their reliability. Surprisingly, we find that simple\nobject-based visual prompting -- overlaying visual cues (e.g., bounding box,\ncircle) on images -- can significantly mitigate such hallucination; however,\ndifferent visual prompts (VPs) vary in effectiveness. To address this, we\npropose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify\noptimal VPs that enhance LVLM responses without needing access to model\ninternals. Our approach employs a pool of candidate VPs and trains a router\nmodel to dynamically select the most effective VP for a given input image. This\nblack-box approach is model-agnostic, making it applicable to both open-source\nand proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR\ndemonstrate that BBVPE effectively reduces object hallucination.",
      "pdf_url": "http://arxiv.org/pdf/2504.21559v1",
      "published": "2025-04-30T11:58:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21559v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Meta knowledge assisted Evolutionary Neural Architecture Search",
      "authors": [
        "Yangyang Li",
        "Guanlong Liu",
        "Ronghua Shang",
        "Licheng Jiao"
      ],
      "abstract": "Evolutionary computation (EC)-based neural architecture search (NAS) has\nachieved remarkable performance in the automatic design of neural\narchitectures. However, the high computational cost associated with evaluating\nsearched architectures poses a challenge for these methods, and a fixed form of\nlearning rate (LR) schedule means greater information loss on diverse searched\narchitectures. This paper introduces an efficient EC-based NAS method to solve\nthese problems via an innovative meta-learning framework. Specifically, a\nmeta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a\nsuitable LR schedule, which guides the training process with lower information\nloss when evaluating each individual. An adaptive surrogate model is designed\nthrough an adaptive threshold to select the potential architectures in a few\nepochs and then evaluate the potential architectures with complete epochs.\nAdditionally, a periodic mutation operator is proposed to increase the\ndiversity of the population, which enhances the generalizability and\nrobustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets\ndemonstrate that the proposed method achieves high performance comparable to\nthat of many state-of-the-art peer methods, with lower computational cost and\ngreater robustness.",
      "pdf_url": "http://arxiv.org/pdf/2504.21545v1",
      "published": "2025-04-30T11:43:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21545v1",
      "categories": [
        "cs.NE",
        "cs.AI"
      ]
    },
    {
      "title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery",
      "authors": [
        "Qinfeng Zhu",
        "Yunxi Jiang",
        "Lei Fan"
      ],
      "abstract": "We propose a result-level category-specific fusion architecture called\nClassWise-CRF. This architecture employs a two-stage process: first, it selects\nexpert networks that perform well in specific categories from a pool of\ncandidate networks using a greedy algorithm; second, it integrates the\nsegmentation predictions of these selected networks by adaptively weighting\ntheir contributions based on their segmentation performance in each category.\nInspired by Conditional Random Field (CRF), the ClassWise-CRF architecture\ntreats the segmentation predictions from multiple networks as confidence vector\nfields. It leverages segmentation metrics (such as Intersection over Union)\nfrom the validation set as priors and employs an exponential weighting strategy\nto fuse the category-specific confidence scores predicted by each network. This\nfusion method dynamically adjusts the weights of each network for different\ncategories, achieving category-specific optimization. Building on this, the\narchitecture further optimizes the fused results using unary and pairwise\npotentials in CRF to ensure spatial consistency and boundary accuracy. To\nvalidate the effectiveness of ClassWise-CRF, we conducted experiments on two\nremote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced\nsemantic segmentation networks. The results show that the ClassWise-CRF\narchitecture significantly improves segmentation performance: on the LoveDA\ndataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on\nthe validation set and by 0.68% on the test set; on the Vaihingen dataset, the\nmIoU improved by 0.87% on the validation set and by 0.91% on the test set.\nThese results fully demonstrate the effectiveness and generality of the\nClassWise-CRF architecture in semantic segmentation of remote sensing images.\nThe full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.",
      "pdf_url": "http://arxiv.org/pdf/2504.21491v1",
      "published": "2025-04-30T10:19:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21491v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ]
    },
    {
      "title": "TRIED: Truly Innovative and Effective AI Detection Benchmark, developed by WITNESS",
      "authors": [
        "Shirin Anlen",
        "Zuzanna Wojciak"
      ],
      "abstract": "The proliferation of generative AI and deceptive synthetic media threatens\nthe global information ecosystem, especially across the Global Majority. This\nreport from WITNESS highlights the limitations of current AI detection tools,\nwhich often underperform in real-world scenarios due to challenges related to\nexplainability, fairness, accessibility, and contextual relevance. In response,\nWITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)\nBenchmark, a new framework for evaluating detection tools based on their\nreal-world impact and capacity for innovation. Drawing on frontline\nexperiences, deceptive AI cases, and global consultations, the report outlines\nhow detection tools must evolve to become truly innovative and relevant by\nmeeting diverse linguistic, cultural, and technological contexts. It offers\npractical guidance for developers, policy actors, and standards bodies to\ndesign accountable, transparent, and user-centered detection solutions, and\nincorporate sociotechnical considerations into future AI standards, procedures\nand evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can\ndrive innovation, safeguard public trust, strengthen AI literacy, and\ncontribute to a more resilient global information credibility.",
      "pdf_url": "http://arxiv.org/pdf/2504.21489v2",
      "published": "2025-04-30T10:18:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21489v2",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense",
      "authors": [
        "Yuchen Ding",
        "Hongli Peng",
        "Xiaoqi Li"
      ],
      "abstract": "With the rapid advancement of blockchain technology, smart contracts have\nenabled the implementation of increasingly complex functionalities. However,\nensuring the security of smart contracts remains a persistent challenge across\nthe stages of development, compilation, and execution. Vulnerabilities within\nsmart contracts not only undermine the security of individual applications but\nalso pose significant risks to the broader blockchain ecosystem, as\ndemonstrated by the growing frequency of attacks since 2016, resulting in\nsubstantial financial losses. This paper provides a comprehensive analysis of\nkey security risks in Ethereum smart contracts, specifically those written in\nSolidity and executed on the Ethereum Virtual Machine (EVM). We focus on two\nprevalent and critical vulnerability types (reentrancy and integer overflow) by\nexamining their underlying mechanisms, replicating attack scenarios, and\nassessing effective countermeasures.",
      "pdf_url": "http://arxiv.org/pdf/2504.21480v1",
      "published": "2025-04-30T10:00:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21480v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers",
      "authors": [
        "Xinyu Li",
        "Qi Yao",
        "Yuanda Wang"
      ],
      "abstract": "Garment sewing patterns are fundamental design elements that bridge the gap\nbetween design concepts and practical manufacturing. The generative modeling of\nsewing patterns is crucial for creating diversified garments. However, existing\napproaches are limited either by reliance on a single input modality or by\nsuboptimal generation efficiency. In this work, we present\n\\textbf{\\textit{GarmentDiffusion}}, a new generative model capable of producing\ncentimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,\nimage, and incomplete sewing pattern). Our method efficiently encodes 3D sewing\npattern parameters into compact edge token representations, achieving a\nsequence length that is $\\textbf{10}\\times$ shorter than that of the\nautoregressive SewingGPT in DressCode. By employing a diffusion transformer, we\nsimultaneously denoise all edge tokens along the temporal axis, while\nmaintaining a constant number of denoising steps regardless of dataset-specific\nedge and panel statistics. With all combination of designs of our model, the\nsewing pattern generation speed is accelerated by $\\textbf{100}\\times$ compared\nto SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well\nas on the largest sewing pattern dataset, namely GarmentCodeData. The project\nwebsite is available at https://shenfu-research.github.io/Garment-Diffusion/.",
      "pdf_url": "http://arxiv.org/pdf/2504.21476v1",
      "published": "2025-04-30T09:56:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21476v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines",
      "authors": [
        "Serry Sibaee",
        "Samar Ahmed",
        "Abdullah Al Harbi",
        "Omer Nacar",
        "Adel Ammar",
        "Yasser Habashi",
        "Wadii Boulila"
      ],
      "abstract": "This study addresses the critical gap in Arabic natural language processing\nby developing an effective Arabic Reverse Dictionary (RD) system that enables\nusers to find words based on their descriptions or meanings. We present a novel\ntransformer-based approach with a semi-encoder neural network architecture\nfeaturing geometrically decreasing layers that achieves state-of-the-art\nresults for Arabic RD tasks. Our methodology incorporates a comprehensive\ndataset construction process and establishes formal quality standards for\nArabic lexicographic definitions. Experiments with various pre-trained models\ndemonstrate that Arabic-specific models significantly outperform general\nmultilingual embeddings, with ARBERTv2 achieving the best ranking score\n(0.0644). Additionally, we provide a formal abstraction of the reverse\ndictionary task that enhances theoretical understanding and develop a modular,\nextensible Python library (RDTL) with configurable training pipelines. Our\nanalysis of dataset quality reveals important insights for improving Arabic\ndefinition construction, leading to eight specific standards for building\nhigh-quality reverse dictionary resources. This work contributes significantly\nto Arabic computational linguistics and provides valuable tools for language\nlearning, academic writing, and professional communication in Arabic.",
      "pdf_url": "http://arxiv.org/pdf/2504.21475v1",
      "published": "2025-04-30T09:56:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21475v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging",
      "authors": [
        "Hadi Bayrami Asl Tekanlou",
        "Jafar Razmara",
        "Mahsa Sanaei",
        "Mostafa Rahgouy",
        "Hamed Babaei Giglou"
      ],
      "abstract": "This paper presents our system, Homa, for SemEval-2025 Task 5: Subject\nTagging, which focuses on automatically assigning subject labels to technical\nrecords from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage\nOntoAligner, a modular ontology alignment toolkit, to address this task by\nintegrating retrieval-augmented generation (RAG) techniques. Our approach\nformulates the subject tagging problem as an alignment task, where records are\nmatched to GND categories based on semantic similarity. We evaluate\nOntoAligner's adaptability for subject indexing and analyze its effectiveness\nin handling multilingual records. Experimental results demonstrate the\nstrengths and limitations of this method, highlighting the potential of\nalignment techniques for improving subject tagging in digital libraries.",
      "pdf_url": "http://arxiv.org/pdf/2504.21474v1",
      "published": "2025-04-30T09:52:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21474v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "xEEGNet: Towards Explainable AI in EEG Dementia Classification",
      "authors": [
        "Andrea Zanola",
        "Louis Fabrice Tshimanga",
        "Federico Del Pup",
        "Marco Baiesi",
        "Manfredo Atzori"
      ],
      "abstract": "This work presents xEEGNet, a novel, compact, and explainable neural network\nfor EEG data analysis. It is fully interpretable and reduces overfitting\nthrough major parameter reduction. As an applicative use case, we focused on\nclassifying common dementia conditions, Alzheimer's and frontotemporal\ndementia, versus controls. xEEGNet is broadly applicable to other neurological\nconditions involving spectral alterations. We initially used ShallowNet, a\nsimple and popular model from the EEGNet-family. Its structure was analyzed and\ngradually modified to move from a \"black box\" to a more transparent model,\nwithout compromising performance. The learned kernels and weights were examined\nfrom a clinical standpoint to assess medical relevance. Model variants,\nincluding ShallowNet and the final xEEGNet, were evaluated using robust\nNested-Leave-N-Subjects-Out cross-validation for unbiased performance\nestimates. Variability across data splits was explained using embedded EEG\nrepresentations, grouped by class and set, with pairwise separability to\nquantify group distinction. Overfitting was assessed through\ntraining-validation loss correlation and training speed. xEEGNet uses only 168\nparameters, 200 times fewer than ShallowNet, yet retains interpretability,\nresists overfitting, achieves comparable median performance (-1.5%), and\nreduces variability across splits. This variability is explained by embedded\nEEG representations: higher accuracy correlates with greater separation between\ntest set controls and Alzheimer's cases, without significant influence from\ntraining data. xEEGNet's ability to filter specific EEG bands, learn\nband-specific topographies, and use relevant spectral features demonstrates its\ninterpretability. While large deep learning models are often prioritized for\nperformance, this study shows smaller architectures like xEEGNet can be equally\neffective in EEG pathology classification.",
      "pdf_url": "http://arxiv.org/pdf/2504.21457v1",
      "published": "2025-04-30T09:24:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21457v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments",
      "authors": [
        "Federico Nesti",
        "Gianluca D'Amico",
        "Mauro Marinoni",
        "Giorgio Buttazzo"
      ],
      "abstract": "The use of machine learning in cyber-physical systems has attracted the\ninterest of both industry and academia. However, no general solution has yet\nbeen found against the unpredictable behavior of neural networks and\nreinforcement learning agents. Nevertheless, the improvements of\nphoto-realistic simulators have paved the way towards extensive testing of\ncomplex algorithms in different virtual scenarios, which would be expensive and\ndangerous to implement in the real world.\n  This paper presents SimPRIVE, a simulation framework for physical robot\ninteraction with virtual environments, which operates as a vehicle-in-the-loop\nplatform, rendering a virtual world while operating the vehicle in the real\nworld.\n  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be\nconfigured to move its digital twin in a virtual world built with the Unreal\nEngine 5 graphic engine, which can be populated with objects, people, or other\nvehicles with programmable behavior.\n  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds\nwhile being light-weight to contain execution times and allow fast rendering.\nIts main advantage lies in the possibility of testing complex algorithms on the\nfull software and hardware stack while minimizing the risks and costs of a test\ncampaign. The framework has been validated by testing a reinforcement learning\nagent trained for obstacle avoidance on an AgileX Scout Mini rover that\nnavigates a virtual office environment where everyday objects and people are\nplaced as obstacles. The physical rover moves with no collision in an indoor\nlimited space, thanks to a LiDAR-based heuristic.",
      "pdf_url": "http://arxiv.org/pdf/2504.21454v1",
      "published": "2025-04-30T09:22:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21454v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Rethinking Visual Layer Selection in Multimodal LLMs",
      "authors": [
        "Haoran Chen",
        "Junyan Lin",
        "Xinhao Chen",
        "Yue Fan",
        "Xin Jin",
        "Hui Su",
        "Jianfeng Dong",
        "Jinlan Fu",
        "Xiaoyu Shen"
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs.",
      "pdf_url": "http://arxiv.org/pdf/2504.21447v1",
      "published": "2025-04-30T09:07:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21447v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding",
      "authors": [
        "Chenkai Zhang",
        "Yiming Lei",
        "Zeming Liu",
        "Haitao Leng",
        "ShaoGuo Liu",
        "Tingting Gao",
        "Qingjie Liu",
        "Yunhong Wang"
      ],
      "abstract": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\n\\textbf{standalone} videos and mainly assess ``visual elements'' like human\nactions and object states. In reality, contemporary videos often encompass\ncomplex and continuous narratives, typically presented as a \\textbf{series}. To\naddress this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting\nof 105 carefully curated narrative-driven series, covering 28 specialized tasks\nthat require deep narrative understanding. Specifically, we first select a\ndiverse set of drama series spanning various genres. Then, we introduce a novel\nlong-span narrative annotation method, combined with a full-information\ntransformation approach to convert manual annotations into diverse task\nformats. To further enhance model capacity for detailed analysis of plot\nstructures and character relationships within series, we propose a novel\nnarrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on\n\\textbf{SeriesBench} indicate that existing MLLMs still face significant\nchallenges in understanding narrative-driven series, while \\textbf{PC-DCoT}\nenables these MLLMs to achieve performance improvements. Overall, our\n\\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of\nadvancing model capabilities to understand narrative-driven series, guiding the\nfuture development of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025.",
      "pdf_url": "http://arxiv.org/pdf/2504.21435v1",
      "published": "2025-04-30T08:48:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2504.21435v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    }
  ]
}