{
  "last_updated": "2025-05-23T00:52:13.273411",
  "papers": [
    {
      "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents",
      "authors": [
        "Yuqi Zhou",
        "Sunhao Dai",
        "Shuai Wang",
        "Kaiwen Zhou",
        "Qinglin Jia",
        "Jun Xu"
      ],
      "abstract": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.",
      "pdf_url": "http://arxiv.org/pdf/2505.15810v2",
      "published": "2025-05-21T17:59:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15810v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Neural Conditional Transport Maps",
      "authors": [
        "Carlos Rodriguez-Pardo",
        "Leonardo Chiani",
        "Emanuele Borgonovo",
        "Massimo Tavoni"
      ],
      "abstract": "We present a neural framework for learning conditional optimal transport (OT)\nmaps between probability distributions. Our approach introduces a conditioning\nmechanism capable of processing both categorical and continuous conditioning\nvariables simultaneously. At the core of our method lies a hypernetwork that\ngenerates transport layer parameters based on these inputs, creating adaptive\nmappings that outperform simpler conditioning methods. Comprehensive ablation\nstudies demonstrate the superior performance of our method over baseline\nconfigurations. Furthermore, we showcase an application to global sensitivity\nanalysis, offering high performance in computing OT-based sensitivity indices.\nThis work advances the state-of-the-art in conditional optimal transport,\nenabling broader application of optimal transport principles to complex,\nhigh-dimensional domains such as generative modeling and black-box model\nexplainability.",
      "pdf_url": "http://arxiv.org/pdf/2505.15808v1",
      "published": "2025-05-21T17:59:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15808v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.PR",
        "stat.AP",
        "stat.ML",
        "49Q22 (Primary) 68T07 (Secondary)",
        "I.5.1; I.2.0; G.3"
      ]
    },
    {
      "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models",
      "authors": [
        "Yuchen Yan",
        "Jin Jiang",
        "Zhenbang Ren",
        "Yijun Li",
        "Xudong Cai",
        "Yang Liu",
        "Xin Xu",
        "Mengdi Zhang",
        "Jian Shao",
        "Yongliang Shen",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "abstract": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved\nremarkable performance in the domain of reasoning. A key component of their\ntraining is the incorporation of verifiable rewards within reinforcement\nlearning (RL). However, existing reward benchmarks do not evaluate\nreference-based reward systems, leaving researchers with limited understanding\nof the accuracy of verifiers used in RL. In this paper, we introduce two\nbenchmarks, VerifyBench and VerifyBench-Hard, designed to assess the\nperformance of reference-based reward systems. These benchmarks are constructed\nthrough meticulous data collection and curation, followed by careful human\nannotation to ensure high quality. Current models still show considerable room\nfor improvement on both VerifyBench and VerifyBench-Hard, especially\nsmaller-scale models. Furthermore, we conduct a thorough and comprehensive\nanalysis of evaluation results, offering insights for understanding and\ndeveloping reference-based reward systems. Our proposed benchmarks serve as\neffective tools for guiding the development of verifier accuracy and the\nreasoning capabilities of models trained via RL in reasoning tasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.15801v1",
      "published": "2025-05-21T17:54:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15801v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Long-Form Information Alignment Evaluation Beyond Atomic Facts",
      "authors": [
        "Danna Zheng",
        "Mirella Lapata",
        "Jeff Z. Pan"
      ],
      "abstract": "Information alignment evaluators are vital for various NLG evaluation tasks\nand trustworthy LLM deployment, reducing hallucinations and enhancing user\ntrust. Current fine-grained methods, like FactScore, verify facts individually\nbut neglect inter-fact dependencies, enabling subtle vulnerabilities. In this\nwork, we introduce MontageLie, a challenging benchmark that constructs\ndeceptive narratives by \"montaging\" truthful statements without introducing\nexplicit hallucinations. We demonstrate that both coarse-grained LLM-based\nevaluators and current fine-grained frameworks are susceptible to this attack,\nwith AUC-ROC scores falling below 65%. To enable more robust fine-grained\nevaluation, we propose DoveScore, a novel framework that jointly verifies\nfactual accuracy and event-order consistency. By modeling inter-fact\nrelationships, DoveScore outperforms existing fine-grained methods by over 8%,\nproviding a more robust solution for long-form text alignment evaluation. Our\ncode and datasets are available at https://github.com/dannalily/DoveScore.",
      "pdf_url": "http://arxiv.org/pdf/2505.15792v1",
      "published": "2025-05-21T17:46:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15792v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Exploring the Innovation Opportunities for Pre-trained Models",
      "authors": [
        "Minjung Park",
        "Jodi Forlizzi",
        "John Zimmerman"
      ],
      "abstract": "Innovators transform the world by understanding where services are\nsuccessfully meeting customers' needs and then using this knowledge to identify\nfailsafe opportunities for innovation. Pre-trained models have changed the AI\ninnovation landscape, making it faster and easier to create new AI products and\nservices. Understanding where pre-trained models are successful is critical for\nsupporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained\nmodels makes it hard to know where AI can really be successful. To address\nthis, we investigated pre-trained model applications developed by HCI\nresearchers as a proxy for commercially successful applications. The research\napplications demonstrate technical capabilities, address real user needs, and\navoid ethical challenges. Using an artifact analysis approach, we categorized\ncapabilities, opportunity domains, data types, and emerging interaction design\npatterns, uncovering some of the opportunity space for innovation with\npre-trained models.",
      "pdf_url": "http://arxiv.org/pdf/2505.15790v1",
      "published": "2025-05-21T17:43:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15790v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Large Language Models as Computable Approximations to Solomonoff Induction",
      "authors": [
        "Jun Wan",
        "Lingrui Mei"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development.",
      "pdf_url": "http://arxiv.org/pdf/2505.15784v1",
      "published": "2025-05-21T17:35:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15784v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "IA-T2I: Internet-Augmented Text-to-Image Generation",
      "authors": [
        "Chuanhao Li",
        "Jianwen Sun",
        "Yukang Feng",
        "Mingliang Zhai",
        "Yifan Chang",
        "Kaipeng Zhang"
      ],
      "abstract": "Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2505.15779v1",
      "published": "2025-05-21T17:31:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15779v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space",
      "authors": [
        "Zhen Zhang",
        "Xuehai He",
        "Weixiang Yan",
        "Ao Shen",
        "Chenyang Zhao",
        "Shuohang Wang",
        "Yelong Shen",
        "Xin Eric Wang"
      ],
      "abstract": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
      "pdf_url": "http://arxiv.org/pdf/2505.15778v1",
      "published": "2025-05-21T17:29:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15778v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Constructing a 3D Town from a Single Image",
      "authors": [
        "Kaizhi Zheng",
        "Ruijian Zhang",
        "Jing Gu",
        "Jie Yang",
        "Xin Eric Wang"
      ],
      "abstract": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.",
      "pdf_url": "http://arxiv.org/pdf/2505.15765v1",
      "published": "2025-05-21T17:10:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15765v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Improving planning and MBRL with temporally-extended actions",
      "authors": [
        "Palash Chatterjee",
        "Roni Khardon"
      ],
      "abstract": "Continuous time systems are often modeled using discrete time dynamics but\nthis requires a small simulation step to maintain accuracy. In turn, this\nrequires a large planning horizon which leads to computationally demanding\nplanning problems and reduced performance. Previous work in model free\nreinforcement learning has partially addressed this issue using action repeats\nwhere a policy is learned to determine a discrete action duration. Instead we\npropose to control the continuous decision timescale directly by using\ntemporally-extended actions and letting the planner treat the duration of the\naction as an additional optimization variable along with the standard action\nvariables. This additional structure has multiple advantages. It speeds up\nsimulation time of trajectories and, importantly, it allows for deep horizon\nsearch in terms of primitive actions while using a shallow search depth in the\nplanner. In addition, in the model based reinforcement learning (MBRL) setting,\nit reduces compounding errors from model learning and improves training time\nfor models. We show that this idea is effective and that the range for action\ndurations can be automatically selected using a multi-armed bandit formulation\nand integrated into the MBRL framework. An extensive experimental evaluation\nboth in planning and in MBRL, shows that our approach yields faster planning,\nbetter solutions, and that it enables solutions to problems that are not solved\nin the standard formulation.",
      "pdf_url": "http://arxiv.org/pdf/2505.15754v1",
      "published": "2025-05-21T16:59:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15754v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "title": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval",
      "authors": [
        "Taiye Chen",
        "Zeming Wei",
        "Ang Li",
        "Yisen Wang"
      ],
      "abstract": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication.",
      "pdf_url": "http://arxiv.org/pdf/2505.15753v1",
      "published": "2025-05-21T16:58:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15753v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs",
      "authors": [
        "Kanan Kiguchi",
        "Yunhao Tu",
        "Katsuhiro Ajito",
        "Fady Alnajjar",
        "Kazuyuki Murase"
      ],
      "abstract": "We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research.",
      "pdf_url": "http://arxiv.org/pdf/2505.15747v2",
      "published": "2025-05-21T16:51:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15747v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6; I.2.1; H.3.1; J.3"
      ]
    },
    {
      "title": "Higher-order Structure Boosts Link Prediction on Temporal Graphs",
      "authors": [
        "Jingzhe Liu",
        "Zhigang Hua",
        "Yan Xie",
        "Bingheng Li",
        "Harry Shomer",
        "Yu Song",
        "Kaveh Hassani",
        "Jiliang Tang"
      ],
      "abstract": "Temporal Graph Neural Networks (TGNNs) have gained growing attention for\nmodeling and predicting structures in temporal graphs. However, existing TGNNs\nprimarily focus on pairwise interactions while overlooking higher-order\nstructures that are integral to link formation and evolution in real-world\ntemporal graphs. Meanwhile, these models often suffer from efficiency\nbottlenecks, further limiting their expressive power. To tackle these\nchallenges, we propose a Higher-order structure Temporal Graph Neural Network,\nwhich incorporates hypergraph representations into temporal graph learning. In\nparticular, we develop an algorithm to identify the underlying higher-order\nstructures, enhancing the model's ability to capture the group interactions.\nFurthermore, by aggregating multiple edge features into hyperedge\nrepresentations, HTGN effectively reduces memory cost during training. We\ntheoretically demonstrate the enhanced expressiveness of our approach and\nvalidate its effectiveness and efficiency through extensive experiments on\nvarious real-world temporal graphs. Experimental results show that HTGN\nachieves superior performance on dynamic link prediction while reducing memory\ncosts by up to 50\\% compared to existing methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.15746v1",
      "published": "2025-05-21T16:51:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15746v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Neuro-Argumentative Learning with Case-Based Reasoning",
      "authors": [
        "Adam Gould",
        "Francesca Toni"
      ],
      "abstract": "We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual\nAA-CBR), a data-driven, neurosymbolic classification model in which the outcome\nis determined by an argumentation debate structure that is learned\nsimultaneously with neural-based feature extractors. Each argument in the\ndebate is an observed case from the training data, favouring their labelling.\nCases attack or support those with opposing or agreeing labellings, with the\nstrength of each argument and relationship learned through gradient-based\nmethods. This argumentation debate structure provides human-aligned reasoning,\nimproving model interpretability compared to traditional neural networks (NNs).\nUnlike the existing purely symbolic variant, Abstract Argumentation for\nCase-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class\nclassification, automatic learning of feature and data point importance,\nassigning uncertainty values to outcomes, using all available data points, and\ndoes not require binary features. We show that Gradual AA-CBR performs\ncomparably to NNs whilst significantly outperforming existing AA-CBR\nformulations.",
      "pdf_url": "http://arxiv.org/pdf/2505.15742v1",
      "published": "2025-05-21T16:49:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15742v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis and Refinement",
      "authors": [
        "Jilin Hu",
        "Jianyu Zhang",
        "Yongwang Zhao",
        "Talia Ringer"
      ],
      "abstract": "Formal methods is pivotal for verifying the reliability of critical systems\nthrough rigorous mathematical proofs. However, its adoption is hindered by\nlabor-intensive manual proofs and the expertise required to use theorem\nprovers. Recent advancements in large language models (LLMs) offer new\nopportunities for automated theorem proving. Two promising approaches are\ngenerating tactics step by step and generating a whole proof directly with an\nLLM. However, existing work makes no attempt to combine the two approaches. In\nthis work, we introduce HybridProver, a dual-model proof synthesis framework\nthat combines tactic-based generation and whole-proof synthesis to harness the\nbenefits of both approaches. HybridProver generates whole proof candidates for\nevaluation directly, then extracts proof sketches from those candidates. It\nthen uses a tactic-based generation model that integrates automated tools to\ncomplete the sketches via stepwise refinement. We implement HybridProver for\nthe Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle\ndatasets. Evaluation on the miniF2F dataset illustrates HybridProver's\neffectiveness. We achieve a 59.4% success rate on miniF2F, where the previous\nSOTA is 56.1%. Our ablation studies show that this SOTA result is attributable\nto combining whole-proof and tactic-based generation. Additionally, we show how\nthe dataset quality, training parameters, and sampling diversity affect the\nfinal result during automated theorem proving with LLMs. All of our code,\ndatasets, and LLMs are open source.",
      "pdf_url": "http://arxiv.org/pdf/2505.15740v1",
      "published": "2025-05-21T16:45:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15740v1",
      "categories": [
        "cs.FL",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "title": "Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses",
      "authors": [
        "Xiaoxue Yang",
        "Bozhidar Stevanoski",
        "Matthieu Meeus",
        "Yves-Alexandre de Montjoye"
      ],
      "abstract": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2505.15738v1",
      "published": "2025-05-21T16:43:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15738v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning",
      "authors": [
        "Gaurav Srivastava",
        "Zhenyu Bi",
        "Meng Lu",
        "Xuan Wang"
      ],
      "abstract": "Large language models (LLMs) have improved significantly in their reasoning\nthrough extensive training on massive datasets. However, relying solely on\nadditional data for improvement is becoming increasingly impractical,\nhighlighting the need for models to autonomously enhance their reasoning\nwithout external supervision. In this paper, we propose Debate, Train, Evolve\n(DTE), a novel ground truth-free training framework that uses multi-agent\ndebate traces to evolve a single language model. We also introduce a new\nprompting strategy Reflect-Critique-Refine, to improve debate quality by\nexplicitly instructing agents to critique and refine their reasoning. Extensive\nevaluations on five reasoning benchmarks with six open-weight models show that\nour DTE framework achieve substantial improvements, with an average accuracy\ngain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe\nstrong cross-domain generalization, with an average accuracy gain of 5.8% on\nall other benchmarks, suggesting that our method captures general reasoning\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2505.15734v1",
      "published": "2025-05-21T16:40:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15734v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities",
      "authors": [
        "Xiaoyu Luo",
        "Yiyi Chen",
        "Johannes Bjerva",
        "Qiongxiu Li"
      ],
      "abstract": "We present the first comprehensive study of Memorization in Multilingual\nLarge Language Models (MLLMs), analyzing 95 languages using models across\ndiverse model scales, architectures, and memorization definitions. As MLLMs are\nincreasingly deployed, understanding their memorization behavior has become\ncritical. Yet prior work has focused primarily on monolingual models, leaving\nmultilingual memorization underexplored, despite the inherently long-tailed\nnature of training corpora. We find that the prevailing assumption, that\nmemorization is highly correlated with training data availability, fails to\nfully explain memorization patterns in MLLMs. We hypothesize that treating\nlanguages in isolation - ignoring their similarities - obscures the true\npatterns of memorization. To address this, we propose a novel graph-based\ncorrelation metric that incorporates language similarity to analyze\ncross-lingual memorization. Our analysis reveals that among similar languages,\nthose with fewer training tokens tend to exhibit higher memorization, a trend\nthat only emerges when cross-lingual relationships are explicitly modeled.\nThese findings underscore the importance of a language-aware perspective in\nevaluating and mitigating memorization vulnerabilities in MLLMs. This also\nconstitutes empirical evidence that language similarity both explains\nMemorization in MLLMs and underpins Cross-lingual Transferability, with broad\nimplications for multilingual NLP.",
      "pdf_url": "http://arxiv.org/pdf/2505.15722v1",
      "published": "2025-05-21T16:30:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15722v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning",
      "authors": [
        "Xiaodong Mei",
        "Sheng Wang",
        "Jie Cheng",
        "Yingbing Chen",
        "Dan Xu"
      ],
      "abstract": "Motion forecasting represents a critical challenge in autonomous driving\nsystems, requiring accurate prediction of surrounding agents' future\ntrajectories. While existing approaches predict future motion states with the\nextracted scene context feature from historical agent trajectories and road\nlayouts, they suffer from the information degradation during the scene feature\nencoding. To address the limitation, we propose HAMF, a novel motion\nforecasting framework that learns future motion representations with the scene\ncontext encoding jointly, to coherently combine the scene understanding and\nfuture motion state prediction. We first embed the observed agent states and\nmap information into 1D token sequences, together with the target multi-modal\nfuture motion features as a set of learnable tokens. Then we design a unified\nAttention-based encoder, which synergistically combines self-attention and\ncross-attention mechanisms to model the scene context information and aggregate\nfuture motion features jointly. Complementing the encoder, we implement the\nMamba module in the decoding stage to further preserve the consistency and\ncorrelations among the learned future motion representations, to generate the\naccurate and diverse final trajectories. Extensive experiments on Argoverse 2\nbenchmark demonstrate that our hybrid Attention-Mamba model achieves\nstate-of-the-art motion forecasting performance with the simple and lightweight\narchitecture.",
      "pdf_url": "http://arxiv.org/pdf/2505.15703v1",
      "published": "2025-05-21T16:16:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15703v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO",
      "authors": [
        "Xingyu Zhou",
        "Yulian Wu",
        "Francesco Orabona"
      ],
      "abstract": "In this paper, we theoretically investigate the effects of noisy labels in\noffline alignment, with a focus on the interplay between privacy and robustness\nagainst adversarial corruption. Specifically, under linear modeling\nassumptions, we present a unified analysis covering both reinforcement learning\nfrom human feedback (RLHF) and direct preference optimization (DPO) under\ndifferent privacy-corruption scenarios, such as Local differential\nprivacy-then-Corruption (LTC), where human preference labels are privatized\nbefore being corrupted by an adversary, and Corruption-then-Local differential\nprivacy (CTL), where labels are corrupted before privacy protection. Our\nanalysis leverages a reduction framework that reduces the offline alignment\nproblem under linear modeling assumptions to parameter estimation in logistic\nregression. This framework allows us to establish an interesting separation\nresult between LTC and CTL, demonstrating that LTC presents a greater challenge\nthan CTL in offline alignment, even under linear models. As important\nby-products, our findings also advance the state-of-the-art theoretical results\nin offline alignment under privacy-only or corruption-only scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2505.15694v1",
      "published": "2025-05-21T16:07:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15694v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives",
      "authors": [
        "Milad Kazemi",
        "Mateo Perez",
        "Fabio Somenzi",
        "Sadegh Soudjani",
        "Ashutosh Trivedi",
        "Alvaro Velasquez"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) have renewed focus on the\ndesign of reward functions that shape agent behavior. Manually designing reward\nfunctions is tedious and error-prone. A principled alternative is to specify\nbehaviors in a formal language that can be automatically translated into\nrewards. Omega-regular languages are a natural choice for this purpose, given\ntheir established role in formal verification and synthesis. However, existing\nmethods using omega-regular specifications typically rely on discounted reward\nRL in episodic settings, with periodic resets. This setup misaligns with the\nsemantics of omega-regular specifications, which describe properties over\ninfinite behavior traces. In such cases, the average reward criterion and the\ncontinuing setting -- where the agent interacts with the environment over a\nsingle, uninterrupted lifetime -- are more appropriate.\n  To address the challenges of infinite-horizon, continuing tasks, we focus on\nabsolute liveness specifications -- a subclass of omega-regular languages that\ncannot be violated by any finite behavior prefix, making them well-suited to\nthe continuing setting. We present the first model-free RL framework that\ntranslates absolute liveness specifications to average-reward objectives. Our\napproach enables learning in communicating MDPs without episodic resetting. We\nalso introduce a reward structure for lexicographic multi-objective\noptimization, aiming to maximize an external average-reward objective among the\npolicies that also maximize the satisfaction probability of a given\nomega-regular specification. Our method guarantees convergence in unknown\ncommunicating MDPs and supports on-the-fly reductions that do not require full\nknowledge of the environment, thus enabling model-free RL. Empirical results\nshow our average-reward approach in continuing setting outperforms\ndiscount-based methods across benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2505.15693v1",
      "published": "2025-05-21T16:06:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15693v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning",
      "authors": [
        "Zhe Xu",
        "Cheng Jin",
        "Yihui Wang",
        "Ziyi Liu",
        "Hao Chen"
      ],
      "abstract": "Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2505.15687v1",
      "published": "2025-05-21T16:03:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15687v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability",
      "authors": [
        "Zishuai Zhang",
        "Hainan Zhang",
        "Jiaying Zheng",
        "Ziwei Wang",
        "Yongxin Tong",
        "Jin Dong",
        "Zhiming Zheng"
      ],
      "abstract": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.",
      "pdf_url": "http://arxiv.org/pdf/2505.15683v1",
      "published": "2025-05-21T15:58:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15683v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DC"
      ]
    },
    {
      "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models",
      "authors": [
        "Miao Yu",
        "Liang Lin",
        "Guibin Zhang",
        "Xinfeng Li",
        "Junfeng Fang",
        "Ningyu Zhang",
        "Kun Wang",
        "Yang Wang"
      ],
      "abstract": "Large language models require iterative updates to address challenges such as\nknowledge conflicts and outdated information (e.g., incorrect, private, or\nillegal contents). Machine unlearning provides a systematic methodology for\ntargeted knowledge removal from trained models, enabling elimination of\nsensitive information influences. However, mainstream fine-tuning-based\nunlearning methods often fail to balance unlearning efficacy and model ability,\nfrequently resulting in catastrophic model collapse under extensive knowledge\nremoval. Meanwhile, in-context unlearning, which relies solely on contextual\nprompting without modifying the model's intrinsic mechanisms, suffers from\nlimited generalizability and struggles to achieve true unlearning. In this\nwork, we introduce UniErase, a novel unlearning paradigm that employs learnable\nparametric suffix (unlearning token) to steer language models toward targeted\nforgetting behaviors. UniErase operates through two key phases: (I) an\noptimization stage that binds desired unlearning outputs to the model's\nautoregressive probability distribution via token optimization, followed by\n(II) a lightweight model editing phase that activates the learned token to\nprobabilistically induce specified forgetting objective. Serving as a new\nresearch direction for token learning to induce unlearning target, UniErase\nachieves state-of-the-art (SOTA) performance across batch, sequential, and\nprecise unlearning under fictitious and real-world knowledge settings.\nRemarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%\nof the LLM parameters, outperforms previous forgetting SOTA baseline by around\n4.01 times for model ability with even better unlearning efficacy. Similarly,\nUniErase, maintaining more ability, also surpasses previous retaining SOTA by\n35.96% for unlearning efficacy, showing dual top-tier performances in current\nunlearing domain.",
      "pdf_url": "http://arxiv.org/pdf/2505.15674v1",
      "published": "2025-05-21T15:53:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15674v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification",
      "authors": [
        "Hamzeh Asgharnezhad",
        "Afshar Shamsi",
        "Roohallah Alizadehsani",
        "Arash Mohammadi",
        "Hamid Alinejad-Rokny"
      ],
      "abstract": "Knowing the uncertainty associated with the output of a deep neural network\nis of paramount importance in making trustworthy decisions, particularly in\nhigh-stakes fields like medical diagnosis and autonomous systems. Monte Carlo\nDropout (MCD) is a widely used method for uncertainty quantification, as it can\nbe easily integrated into various deep architectures. However, conventional MCD\noften struggles with providing well-calibrated uncertainty estimates. To\naddress this, we introduce innovative frameworks that enhances MCD by\nintegrating different search solutions namely Grey Wolf Optimizer (GWO),\nBayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an\nuncertainty-aware loss function, thereby improving the reliability of\nuncertainty quantification. We conduct comprehensive experiments using\ndifferent backbones, namely DenseNet121, ResNet50, and VGG16, on various\ndatasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic\ndataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3%\non average in terms of both conventional accuracy and uncertainty accuracy\nwhile achieving significantly better calibration. These results highlight the\npotential of our approach to enhance the trustworthiness of deep learning\nmodels in safety-critical applications.",
      "pdf_url": "http://arxiv.org/pdf/2505.15671v1",
      "published": "2025-05-21T15:50:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15671v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Neural Quantum Digital Twins for Optimizing Quantum Annealing",
      "authors": [
        "Jianlong Lu",
        "Hanqiu Peng",
        "Ying Chen"
      ],
      "abstract": "Quantum annealers have shown potential in addressing certain combinatorial\noptimization problems, though their performance is often limited by scalability\nand errors rates. In this work, we propose a Neural Quantum Digital Twin (NQDT)\nframework that reconstructs the energy landscape of quantum many-body systems\nrelevant to quantum annealing. The digital twin models both ground and excited\nstate dynamics, enabling detailed simulation of the adiabatic evolution\nprocess. We benchmark NQDT on systems with known analytical solutions and\ndemonstrate that it accurately captures key quantum phenomena, including\nquantum criticality and phase transitions. Leveraging this framework, one can\nidentify optimal annealing schedules that minimize excitation-related errors.\nThese findings highlight the utility of neural network-based digital twins as a\ndiagnostic and optimization tool for improving the performance of quantum\nannealers.",
      "pdf_url": "http://arxiv.org/pdf/2505.15662v1",
      "published": "2025-05-21T15:38:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15662v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought",
      "authors": [
        "Cheng Yan",
        "Felix Mohr",
        "Tom Viering"
      ],
      "abstract": "Sample-wise learning curves plot performance versus training set size. They\nare useful for studying scaling laws and speeding up hyperparameter tuning and\nmodel selection. Learning curves are often assumed to be well-behaved: monotone\n(i.e. improving with more data) and convex. By constructing the Learning Curves\nDatabase 1.1 (LCDB 1.1), a large-scale database with high-resolution learning\ncurves, we show that learning curves are less often well-behaved than\npreviously thought. Using statistically rigorous methods, we observe\nsignificant ill-behavior in approximately 14% of the learning curves, almost\ntwice as much as in previous estimates. We also identify which learners are to\nblame and show that specific learners are more ill-behaved than others.\nAdditionally, we demonstrate that different feature scalings rarely resolve\nill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such\nas learning curve fitting and model selection, and find it poses significant\nchallenges, underscoring the relevance and potential of LCDB 1.1 as a\nchallenging benchmark for future research.",
      "pdf_url": "http://arxiv.org/pdf/2505.15657v1",
      "published": "2025-05-21T15:32:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15657v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Second-Order Convergence in Private Stochastic Non-Convex Optimization",
      "authors": [
        "Youming Tao",
        "Zuyuan Zhang",
        "Dongxiao Yu",
        "Xiuzhen Cheng",
        "Falko Dressler",
        "Di Wang"
      ],
      "abstract": "We investigate the problem of finding second-order stationary points (SOSP)\nin differentially private (DP) stochastic non-convex optimization. Existing\nmethods suffer from two key limitations: (i) inaccurate convergence error rate\ndue to overlooking gradient variance in the saddle point escape analysis, and\n(ii) dependence on auxiliary private model selection procedures for identifying\nDP-SOSP, which can significantly impair utility, particularly in distributed\nsettings. To address these issues, we propose a generic perturbed stochastic\ngradient descent (PSGD) framework built upon Gaussian noise injection and\ngeneral gradient oracles. A core innovation of our framework is using model\ndrift distance to determine whether PSGD escapes saddle points, ensuring\nconvergence to approximate local minima without relying on second-order\ninformation or additional DP-SOSP identification. By leveraging the adaptive\nDP-SPIDER estimator as a specific gradient oracle, we develop a new DP\nalgorithm that rectifies the convergence error rates reported in prior work. We\nfurther extend this algorithm to distributed learning with arbitrarily\nheterogeneous data, providing the first formal guarantees for finding DP-SOSP\nin such settings. Our analysis also highlights the detrimental impacts of\nprivate selection procedures in distributed learning under high-dimensional\nmodels, underscoring the practical benefits of our design. Numerical\nexperiments on real-world datasets validate the efficacy of our approach.",
      "pdf_url": "http://arxiv.org/pdf/2505.15647v1",
      "published": "2025-05-21T15:25:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15647v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models",
      "authors": [
        "Zhen Sun",
        "Ziyi Zhang",
        "Zeren Luo",
        "Zeyang Sha",
        "Tianshuo Cong",
        "Zheng Li",
        "Shiwen Cui",
        "Weiqiang Wang",
        "Jiaheng Wei",
        "Xinlei He",
        "Qi Li",
        "Qian Wang"
      ],
      "abstract": "Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.",
      "pdf_url": "http://arxiv.org/pdf/2505.15644v1",
      "published": "2025-05-21T15:22:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15644v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions",
      "authors": [
        "David Thulke",
        "Jakob Kemmler",
        "Christian Dugast",
        "Hermann Ney"
      ],
      "abstract": "Large language models that use retrieval augmented generation have the\npotential to unlock valuable knowledge for researchers, policymakers, and the\npublic by making long and technical climate-related documents more accessible.\nWhile this approach can help alleviate factual hallucinations by relying on\nretrieved passages as additional context, its effectiveness depends on whether\nthe model's output remains faithful to these passages. To address this, we\nexplore the automatic assessment of faithfulness of different models in this\nsetting. We then focus on ClimateGPT, a large language model specialised in\nclimate science, to examine which factors in its instruction fine-tuning impact\nthe model's faithfulness. By excluding unfaithful subsets of the model's\ntraining data, we develop ClimateGPT Faithful+, which achieves an improvement\nin faithfulness from 30% to 57% in supported atomic claims according to our\nautomatic metric.",
      "pdf_url": "http://arxiv.org/pdf/2505.15633v1",
      "published": "2025-05-21T15:17:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15633v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
      "authors": [
        "Wei Liu",
        "Ruochen Zhou",
        "Yiyun Deng",
        "Yuzhen Huang",
        "Junteng Liu",
        "Yuntian Deng",
        "Yizhe Zhang",
        "Junxian He"
      ],
      "abstract": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.",
      "pdf_url": "http://arxiv.org/pdf/2505.15612v1",
      "published": "2025-05-21T15:03:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15612v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning",
      "authors": [
        "David Dinucu-Jianu",
        "Jakub Macina",
        "Nico Daheim",
        "Ido Hakimi",
        "Iryna Gurevych",
        "Mrinmaya Sachan"
      ],
      "abstract": "Large language models (LLMs) can transform education, but their optimization\nfor direct question-answering often undermines effective pedagogy which\nrequires strategically withholding answers. To mitigate this, we propose an\nonline reinforcement learning (RL)-based alignment framework that can quickly\nadapt LLMs into effective tutors using simulated student-tutor interactions by\nemphasizing pedagogical quality and guided problem-solving over simply giving\naway answers. We use our method to train a 7B parameter tutor model without\nhuman annotations which reaches similar performance to larger proprietary\nmodels like LearnLM. We introduce a controllable reward weighting to balance\npedagogical support and student solving accuracy, allowing us to trace the\nPareto frontier between these two objectives. Our models better preserve\nreasoning capabilities than single-turn SFT baselines and can optionally\nenhance interpretability through thinking tags that expose the model's\ninstructional planning.",
      "pdf_url": "http://arxiv.org/pdf/2505.15607v1",
      "published": "2025-05-21T15:00:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15607v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use",
      "authors": [
        "Xinyi Lu",
        "Aditya Mahesh",
        "Zejia Shen",
        "Mitchell Dudley",
        "Larissa Sano",
        "Xu Wang"
      ],
      "abstract": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback.",
      "pdf_url": "http://arxiv.org/pdf/2505.15596v1",
      "published": "2025-05-21T14:50:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15596v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off",
      "authors": [
        "Yury Belousov",
        "Brian Pulfer",
        "Vitaliy Kinakh",
        "Slava Voloshynovskiy"
      ],
      "abstract": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed.",
      "pdf_url": "http://arxiv.org/pdf/2505.15594v1",
      "published": "2025-05-21T14:49:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15594v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "World Models as Reference Trajectories for Rapid Motor Adaptation",
      "authors": [
        "Carlos Stein Brito",
        "Daniel McNamee"
      ],
      "abstract": "Deploying learned control policies in real-world environments poses a\nfundamental challenge. When system dynamics change unexpectedly, performance\ndegrades until models are retrained on new data. We introduce Reflexive World\nModels (RWM), a dual control framework that uses world model predictions as\nimplicit reference trajectories for rapid adaptation. Our method separates the\ncontrol problem into long-term reward maximization through reinforcement\nlearning and robust motor execution through rapid latent control. This dual\narchitecture achieves significantly faster adaptation with low online\ncomputational cost compared to model-based RL baselines, while maintaining\nnear-optimal performance. The approach combines the benefits of flexible policy\nlearning through reinforcement learning with rapid error correction\ncapabilities, providing a principled approach to maintaining performance in\nhigh-dimensional continuous control tasks under varying dynamics.",
      "pdf_url": "http://arxiv.org/pdf/2505.15589v1",
      "published": "2025-05-21T14:46:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15589v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset",
      "authors": [
        "Hua Li",
        "Shijie Lian",
        "Zhiyuan Li",
        "Runmin Cong",
        "Sam Kwong"
      ],
      "abstract": "With recent breakthroughs in large-scale modeling, the Segment Anything Model\n(SAM) has demonstrated significant potential in a variety of visual\napplications. However, due to the lack of underwater domain expertise, SAM and\nits variants face performance limitations in end-to-end underwater instance\nsegmentation tasks, while their higher computational requirements further\nhinder their application in underwater scenarios. To address this challenge, we\npropose a large-scale underwater instance segmentation dataset, UIIS10K, which\nincludes 10,048 images with pixel-level annotations for 10 categories. Then, we\nintroduce UWSAM, an efficient model designed for automatic and accurate\nsegmentation of underwater instances. UWSAM efficiently distills knowledge from\nthe SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the\nMask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective\nvisual representation learning. Furthermore, we design an End-to-end Underwater\nPrompt Generator (EUPG) for UWSAM, which automatically generates underwater\nprompts instead of explicitly providing foreground points or boxes as prompts,\nthus enabling the network to locate underwater instances accurately for\nefficient segmentation. Comprehensive experimental results show that our model\nis effective, achieving significant performance improvements over\nstate-of-the-art methods on multiple underwater instance datasets. Datasets and\ncodes are available at https://github.com/LiamLian0727/UIIS10K.",
      "pdf_url": "http://arxiv.org/pdf/2505.15581v1",
      "published": "2025-05-21T14:36:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15581v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback",
      "authors": [
        "Wangyang Ying",
        "Haoyue Bai",
        "Nanxu Gong",
        "Xinyuan Wang",
        "Sixun Dong",
        "Haifeng Chen",
        "Yanjie Fu"
      ],
      "abstract": "The data-to-equation (Data2Eqn) task aims to discover interpretable\nmathematical equations that map observed values to labels, offering physical\ninsights and broad applicability across academic and industrial domains.\nGenetic programming and traditional deep learning-based approaches suffer from\nsearch inefficiency and poor generalization on small task-specific datasets.\nFoundation models showed promise in this area, but existing approaches suffer\nfrom: 1) They are pretrained on general-purpose data distributions, making them\nless effective for domain-specific tasks; and 2) their training objectives\nfocus on token-level alignment, overlooking mathematical semantics, which can\nlead to inaccurate equations. To address these issues, we aim to enhance the\ndomain adaptability of foundation models for Data2Eqn tasks. In this work, we\npropose a reinforcement learning-based finetuning framework that directly\noptimizes the generation policy of a pretrained model through reward signals\nderived from downstream numerical fitness. Our method allows the model to adapt\nto specific and complex data distributions and generate mathematically\nmeaningful equations. Extensive experiments demonstrate that our approach\nimproves both the accuracy and robustness of equation generation under complex\ndistributions.",
      "pdf_url": "http://arxiv.org/pdf/2505.15572v1",
      "published": "2025-05-21T14:25:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15572v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes",
      "authors": [
        "Zixun Guo",
        "Simon Dixon"
      ],
      "abstract": "Moonbeam is a transformer-based foundation model for symbolic music,\npretrained on a large and diverse collection of MIDI data totaling 81.6K hours\nof music and 18 billion tokens. Moonbeam incorporates music-domain inductive\nbiases by capturing both absolute and relative musical attributes through the\nintroduction of a novel domain-knowledge-inspired tokenization method and\nMultidimensional Relative Attention (MRA), which captures relative music\ninformation without additional trainable parameters. Leveraging the pretrained\nMoonbeam, we propose 2 finetuning architectures with full anticipatory\ncapabilities, targeting 2 categories of downstream tasks: symbolic music\nunderstanding and conditional music generation (including music infilling). Our\nmodel outperforms other large-scale pretrained music models in most cases in\nterms of accuracy and F1 score across 3 downstream music classification tasks\non 4 datasets. Moreover, our finetuned conditional music generation model\noutperforms a strong transformer baseline with a REMI-like tokenizer. We\nopen-source the code, pretrained model, and generated samples on Github.",
      "pdf_url": "http://arxiv.org/pdf/2505.15559v1",
      "published": "2025-05-21T14:17:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15559v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "Robo-DM: Data Management For Large Robot Datasets",
      "authors": [
        "Kaiyuan Chen",
        "Letian Fu",
        "David Huang",
        "Yanxiang Zhang",
        "Lawrence Yunliang Chen",
        "Huang Huang",
        "Kush Hari",
        "Ashwin Balakrishna",
        "Ted Xiao",
        "Pannag R Sanketi",
        "John Kubiatowicz",
        "Ken Goldberg"
      ],
      "abstract": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2505.15558v1",
      "published": "2025-05-21T14:17:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15558v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ]
    },
    {
      "title": "DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion",
      "authors": [
        "Wendi Zhou",
        "Ameer Saadat-Yazdi",
        "Nadin Kökciyan"
      ],
      "abstract": "Critical questions are essential resources to provoke critical thinking when\nencountering an argumentative text. We present our system for the Critical\nQuestions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach\nleverages large language models (LLMs) with chain-of-thought prompting to\ngenerate critical questions guided by Walton's argumentation schemes. For each\ninput intervention, we conversationally prompt LLMs to instantiate the\ncorresponding argument scheme template to first obtain structured arguments,\nand then generate relevant critical questions. Following this, we rank all the\navailable critical questions by prompting LLMs to select the top 3 most helpful\nquestions based on the original intervention text. This combination of\nstructured argumentation theory and step-by-step reasoning enables the\ngeneration of contextually relevant and diverse critical questions. Our\npipeline achieves competitive performance in the final test set, showing its\npotential to foster critical thinking given argumentative text and detect\nmissing or uninformed claims. Code available at\n\\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.",
      "pdf_url": "http://arxiv.org/pdf/2505.15554v1",
      "published": "2025-05-21T14:15:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15554v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Social Bias in Popular Question-Answering Benchmarks",
      "authors": [
        "Angelie Kraft",
        "Judith Simon",
        "Sonja Schimmler"
      ],
      "abstract": "Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2505.15553v2",
      "published": "2025-05-21T14:14:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15553v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "title": "Oversmoothing, \"Oversquashing\", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning",
      "authors": [
        "Adrian Arnaiz-Rodriguez",
        "Federico Errica"
      ],
      "abstract": "After a renaissance phase in which researchers revisited the message-passing\nparadigm through the lens of deep learning, the graph machine learning\ncommunity shifted its attention towards a deeper and practical understanding of\nmessage-passing's benefits and limitations. In this position paper, we notice\nhow the fast pace of progress around the topics of oversmoothing and\noversquashing, the homophily-heterophily dichotomy, and long-range tasks, came\nwith the consolidation of commonly accepted beliefs and assumptions that are\nnot always true nor easy to distinguish from each other. We argue that this has\nled to ambiguities around the investigated problems, preventing researchers\nfrom focusing on and addressing precise research questions while causing a good\namount of misunderstandings. Our contribution wants to make such common beliefs\nexplicit and encourage critical thinking around these topics, supported by\nsimple but noteworthy counterexamples. The hope is to clarify the distinction\nbetween the different issues and promote separate but intertwined research\ndirections to address them.",
      "pdf_url": "http://arxiv.org/pdf/2505.15547v1",
      "published": "2025-05-21T14:11:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15547v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs",
      "authors": [
        "Lang Gao",
        "Kaiyang Wan",
        "Wei Liu",
        "Chenxi Wang",
        "Zirui Song",
        "Zixiang Xu",
        "Yanbo Wang",
        "Veselin Stoyanov",
        "Xiuying Chen"
      ],
      "abstract": "Bias in Large Language Models (LLMs) significantly undermines their\nreliability and fairness. We focus on a common form of bias: when two reference\nconcepts in the model's concept space, such as sentiment polarities (e.g.,\n\"positive\" and \"negative\"), are asymmetrically correlated with a third, target\nconcept, such as a reviewing aspect, the model exhibits unintended bias. For\ninstance, the understanding of \"food\" should not skew toward any particular\nsentiment. Existing bias evaluation methods assess behavioral differences of\nLLMs by constructing labeled data for different social groups and measuring\nmodel responses across them, a process that requires substantial human effort\nand captures only a limited set of social concepts. To overcome these\nlimitations, we propose BiasLens, a test-set-free bias analysis framework based\non the structure of the model's vector space. BiasLens combines Concept\nActivation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract\ninterpretable concept representations, and quantifies bias by measuring the\nvariation in representational similarity between the target concept and each of\nthe reference concepts. Even without labeled data, BiasLens shows strong\nagreement with traditional bias evaluation metrics (Spearman correlation r >\n0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect\nusing existing methods. For example, in simulated clinical scenarios, a\npatient's insurance status can cause the LLM to produce biased diagnostic\nassessments. Overall, BiasLens offers a scalable, interpretable, and efficient\nparadigm for bias discovery, paving the way for improving fairness and\ntransparency in LLMs.",
      "pdf_url": "http://arxiv.org/pdf/2505.15524v1",
      "published": "2025-05-21T13:50:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15524v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets",
      "authors": [
        "Kaiyuan Chen",
        "Shuangyu Xie",
        "Zehan Ma",
        "Ken Goldberg"
      ],
      "abstract": "Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2505.15517v1",
      "published": "2025-05-21T13:42:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15517v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Explainable embeddings with Distance Explainer",
      "authors": [
        "Christiaan Meijer",
        "E. G. Patrick Bos"
      ],
      "abstract": "While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces.",
      "pdf_url": "http://arxiv.org/pdf/2505.15516v1",
      "published": "2025-05-21T13:42:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15516v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "68T99",
        "I.2.m"
      ]
    },
    {
      "title": "AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization",
      "authors": [
        "Soham Sane"
      ],
      "abstract": "Proximal Policy Optimization (PPO) is a widely used reinforcement learning\nalgorithm that heavily relies on accurate advantage estimates for stable and\nefficient training. However, raw advantage signals can exhibit significant\nvariance, noise, and scale-related issues, impeding optimal learning\nperformance. To address this challenge, we introduce Advantage Modulation PPO\n(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage\nestimates using a dynamic, non-linear scaling mechanism. This adaptive\nmodulation employs an alpha controller that dynamically adjusts the scaling\nfactor based on evolving statistical properties of the advantage signals, such\nas their norm, variance, and a predefined target saturation level. By\nincorporating a tanh-based gating function driven by these adaptively scaled\nadvantages, AM-PPO reshapes the advantage signals to stabilize gradient updates\nand improve the conditioning of the policy gradient landscape. Crucially, this\nmodulation also influences value function training by providing consistent and\nadaptively conditioned learning targets. Empirical evaluations across standard\ncontinuous control benchmarks demonstrate that AM-PPO achieves superior reward\ntrajectories, exhibits sustained learning progression, and significantly\nreduces the clipping required by adaptive optimizers. These findings underscore\nthe potential of advantage modulation as a broadly applicable technique for\nenhancing reinforcement learning optimization.",
      "pdf_url": "http://arxiv.org/pdf/2505.15514v1",
      "published": "2025-05-21T13:38:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15514v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ]
    },
    {
      "title": "Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning",
      "authors": [
        "Mahesh Godavarti"
      ],
      "abstract": "We introduce a new algebraic structure for multi-dimensional compositional\nembeddings, built on directional non-commutative monoidal operators. The core\ncontribution of this work is this novel framework, which exhibits appealing\ntheoretical properties (associativity along each dimension and an interchange\nlaw ensuring global consistency) while remaining compatible with modern machine\nlearning architectures. Our construction defines a distinct composition\noperator circ_i for each axis i, ensuring associative combination along each\naxis without imposing global commutativity. Importantly, all axis-specific\noperators commute with one another, enforcing a global interchange law that\nenables consistent crossaxis compositions. This is, to our knowledge, the first\napproach that provides a common foundation that generalizes classical\nsequence-modeling paradigms (e.g., structured state-space models (SSMs) and\ntransformer self-attention) to a unified multi-dimensional framework. For\nexample, specific one-dimensional instances of our framework can recover the\nfamiliar affine transformation algebra, vanilla self-attention, and the\nSSM-style recurrence. The higher-dimensional generalizations naturally support\nrecursive, structure-aware operations in embedding spaces. We outline several\npotential applications unlocked by this structure-including structured\npositional encodings in Transformers, directional image embeddings, and\nsymbolic modeling of sequences or grids-indicating that it could inform future\ndeep learning model designs. We formally establish the algebraic properties of\nour framework and discuss efficient implementations. Finally, as our focus is\ntheoretical, we include no experiments here and defer empirical validation to\nfuture work, which we plan to undertake.",
      "pdf_url": "http://arxiv.org/pdf/2505.15507v1",
      "published": "2025-05-21T13:27:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15507v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "20-XX, 08A02",
        "F.4.1; I.2"
      ]
    },
    {
      "title": "Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification",
      "authors": [
        "Conghao Xiong",
        "Zhengrui Guo",
        "Zhe Xu",
        "Yifei Zhang",
        "Raymond Kai-Yu Tong",
        "Si Yong Yeo",
        "Hao Chen",
        "Joseph J. Y. Sung",
        "Irwin King"
      ],
      "abstract": "Deep learning has advanced computational pathology but expert annotations\nremain scarce. Few-shot learning mitigates annotation burdens yet suffers from\noverfitting and discriminative feature mischaracterization. In addition, the\ncurrent few-shot multiple instance learning (MIL) approaches leverage\npretrained vision-language models to alleviate these issues, but at the cost of\ncomplex preprocessing and high computational cost. We propose a\nSqueeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in\nMIL models to address these challenges. The SR block comprises two core\ncomponents: a pair of low-rank trainable matrices (squeeze pathway, SP) that\nreduces parameter count and imposes a bottleneck to prevent spurious feature\nlearning, and a frozen random recalibration matrix that preserves geometric\nstructure, diversifies feature directions, and redefines the optimization\nobjective for the SP. We provide theoretical guarantees that the SR block can\napproximate any linear mapping to arbitrary precision, thereby ensuring that\nthe performance of a standard MIL model serves as a lower bound for its\nSR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL\nmodels consistently outperform prior methods while requiring significantly\nfewer parameters and no architectural changes.",
      "pdf_url": "http://arxiv.org/pdf/2505.15504v1",
      "published": "2025-05-21T13:24:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15504v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs",
      "authors": [
        "Federico Ranaldi",
        "Andrea Zugarini",
        "Leonardo Ranaldi",
        "Fabio Massimo Zanzotto"
      ],
      "abstract": "We introduce the concept of protoknowledge to formalize and measure how\nsequences of tokens encoding Knowledge Graphs are internalized during\npretraining and utilized at inference time by Large Language Models (LLMs).\nIndeed, LLMs have demonstrated the ability to memorize vast amounts of token\nsequences during pretraining, and a central open question is how they leverage\nthis memorization as reusable knowledge through generalization. We then\ncategorize protoknowledge into lexical, hierarchical, and topological forms,\nvarying on the type of knowledge that needs to be activated. We measure\nprotoknowledge through Knowledge Activation Tasks (KATs), analyzing its general\nproperties such as semantic bias. We then investigate the impact of\nprotoknowledge on Text-to-SPARQL performance by varying prompting strategies\ndepending on input conditions. To this end, we adopt a novel analysis framework\nthat assesses whether model predictions align with the successful activation of\nthe relevant protoknowledge for each query. This methodology provides a\npractical tool to explore Semantic-Level Data Contamination and serves as an\neffective strategy for Closed-Pretraining models.",
      "pdf_url": "http://arxiv.org/pdf/2505.15501v1",
      "published": "2025-05-21T13:22:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15501v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models",
      "authors": [
        "Zhanyue Qin",
        "Yue Ding",
        "Deyuan Liu",
        "Qingbin Liu",
        "Junxian Cai",
        "Xi Chen",
        "Zhiying Tu",
        "Dianhui Chu",
        "Cuiyun Gao",
        "Dianbo Sui"
      ],
      "abstract": "Nowadays, Large Language Models (LLMs) have attracted widespread attention\ndue to their powerful performance. However, due to the unavoidable exposure to\nsocially biased data during training, LLMs tend to exhibit social biases,\nparticularly gender bias. To better explore and quantifying the degree of\ngender bias in LLMs, we propose a pair of datasets named GenBiasEval and\nGenHintEval, respectively. The GenBiasEval is responsible for evaluating the\ndegree of gender bias in LLMs, accompanied by an evaluation metric named\nAFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is\nused to assess whether LLMs can provide responses consistent with prompts that\ncontain gender hints, along with the accompanying evaluation metric UB-Score\n(UnBias Score). Besides, in order to mitigate gender bias in LLMs more\neffectively, we present the LFTF (Locating First and Then Fine-Tuning)\nalgorithm.The algorithm first ranks specific LLM blocks by their relevance to\ngender bias in descending order using a metric called BMI (Block Mitigating\nImportance Score). Based on this ranking, the block most strongly associated\nwith gender bias is then fine-tuned using a carefully designed loss function.\nNumerous experiments have shown that our proposed LFTF algorithm can\nsignificantly mitigate gender bias in LLMs while maintaining their general\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2505.15475v1",
      "published": "2025-05-21T12:49:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2505.15475v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}