{
  "last_updated": "2025-07-04T00:53:46.176322",
  "papers": [
    {
      "title": "AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation",
      "authors": [
        "Sixiang Chen",
        "Jiaming Liu",
        "Siyuan Qian",
        "Han Jiang",
        "Lily Li",
        "Renrui Zhang",
        "Zhuoyang Liu",
        "Chenyang Gu",
        "Chengkai Hou",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Shanghang Zhang"
      ],
      "abstract": "Recently, mobile manipulation has attracted increasing attention for enabling\nlanguage-conditioned robotic control in household tasks. However, existing\nmethods still face challenges in coordinating mobile base and manipulator,\nprimarily due to two limitations. On the one hand, they fail to explicitly\nmodel the influence of the mobile base on manipulator control, which easily\nleads to error accumulation under high degrees of freedom. On the other hand,\nthey treat the entire mobile manipulation process with the same visual\nobservation modality (e.g., either all 2D or all 3D), overlooking the distinct\nmultimodal perception requirements at different stages during mobile\nmanipulation. To address this, we propose the Adaptive Coordination Diffusion\nTransformer (AC-DiT), which enhances mobile base and manipulator coordination\nfor end-to-end mobile manipulation. First, since the motion of the mobile base\ndirectly influences the manipulator's actions, we introduce a mobility-to-body\nconditioning mechanism that guides the model to first extract base motion\nrepresentations, which are then used as context prior for predicting whole-body\nactions. This enables whole-body control that accounts for the potential impact\nof the mobile base's motion. Second, to meet the perception requirements at\ndifferent stages of mobile manipulation, we design a perception-aware\nmultimodal conditioning strategy that dynamically adjusts the fusion weights\nbetween various 2D visual images and 3D point clouds, yielding visual features\ntailored to the current perceptual needs. This allows the model to, for\nexample, adaptively rely more on 2D inputs when semantic information is crucial\nfor action prediction, while placing greater emphasis on 3D geometric\ninformation when precise spatial understanding is required. We validate AC-DiT\nthrough extensive experiments on both simulated and real-world mobile\nmanipulation tasks.",
      "pdf_url": "http://arxiv.org/pdf/2507.01961v2",
      "published": "2025-07-02T17:59:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01961v2",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation",
      "authors": [
        "Zhuoyang Zhang",
        "Luke J. Huang",
        "Chengyue Wu",
        "Shang Yang",
        "Kelly Peng",
        "Yao Lu",
        "Song Han"
      ],
      "abstract": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256$\\times$256 res.) and 1024 to 48 (512$\\times$512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4$\\times$ lower latency than previous parallelized\nautoregressive models.",
      "pdf_url": "http://arxiv.org/pdf/2507.01957v1",
      "published": "2025-07-02T17:59:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01957v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks",
      "authors": [
        "Rahul Ramachandran",
        "Ali Garjani",
        "Roman Bachmann",
        "Andrei Atanov",
        "OÄŸuzhan Fatih Kar",
        "Amir Zamir"
      ],
      "abstract": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.",
      "pdf_url": "http://arxiv.org/pdf/2507.01955v1",
      "published": "2025-07-02T17:59:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01955v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars",
      "authors": [
        "Xiaosheng Zhao",
        "Yang Huang",
        "Guirong Xue",
        "Xiao Kong",
        "Jifeng Liu",
        "Xiaoyu Tang",
        "Timothy C. Beers",
        "Yuan-Sen Ting",
        "A-Li Luo"
      ],
      "abstract": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.",
      "pdf_url": "http://arxiv.org/pdf/2507.01939v1",
      "published": "2025-07-02T17:49:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01939v1",
      "categories": [
        "astro-ph.IM",
        "astro-ph.SR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla",
      "authors": [
        "Md Sazzadul Islam Ridoy",
        "Sumi Akter",
        "Md. Aminur Rahman"
      ],
      "abstract": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.",
      "pdf_url": "http://arxiv.org/pdf/2507.01931v1",
      "published": "2025-07-02T17:44:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01931v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection",
      "authors": [
        "Samirah Bakker",
        "Yao Ma",
        "Seyed Sahand Mohammadi Ziabari"
      ],
      "abstract": "The complexity of mental healthcare billing enables anomalies, including\nfraud. While machine learning methods have been applied to anomaly detection,\nthey often struggle with class imbalance, label scarcity, and complex\nsequential patterns. This study explores a hybrid deep learning approach\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\ncontext of healthcare billing. The approach is evaluated on two real-world\nbilling datasets related to mental healthcare. The iForest LSTM baseline\nachieves the highest recall (0.963) on declaration-level data. On the\noperation-level data, the hybrid iForest-based model achieves the highest\nrecall (0.744), though at the cost of lower precision. These findings highlight\nthe potential of combining pseudo-labeling with hybrid deep learning in\ncomplex, imbalanced anomaly detection settings.",
      "pdf_url": "http://arxiv.org/pdf/2507.01924v1",
      "published": "2025-07-02T17:33:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01924v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning",
      "authors": [
        "Christian Bongiorno",
        "Efstratios Manolakis",
        "Rosario Nunzio Mantegna"
      ],
      "abstract": "We develop a rotation-invariant neural network that provides the global\nminimum-variance portfolio by jointly learning how to lag-transform historical\nreturns and how to regularise both the eigenvalues and the marginal\nvolatilities of large equity covariance matrices. This explicit mathematical\nmapping offers clear interpretability of each module's role, so the model\ncannot be regarded as a pure black-box. The architecture mirrors the analytical\nform of the global minimum-variance solution yet remains agnostic to dimension,\nso a single model can be calibrated on panels of a few hundred stocks and\napplied, without retraining, to one thousand US equities-a cross-sectional jump\nthat demonstrates robust out-of-sample generalisation. The loss function is the\nfuture realized minimum portfolio variance and is optimized end-to-end on real\ndaily returns. In out-of-sample tests from January 2000 to December 2024 the\nestimator delivers systematically lower realised volatility, smaller maximum\ndrawdowns, and higher Sharpe ratios than the best analytical competitors,\nincluding state-of-the-art non-linear shrinkage. Furthermore, although the\nmodel is trained end-to-end to produce an unconstrained (long-short)\nminimum-variance portfolio, we show that its learned covariance representation\ncan be used in general optimizers under long-only constraints with virtually no\nloss in its performance advantage over competing estimators. These gains\npersist when the strategy is executed under a highly realistic implementation\nframework that models market orders at the auctions, empirical slippage,\nexchange fees, and financing charges for leverage, and they remain stable\nduring episodes of acute market stress.",
      "pdf_url": "http://arxiv.org/pdf/2507.01918v1",
      "published": "2025-07-02T17:27:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01918v1",
      "categories": [
        "q-fin.PM",
        "cs.AI",
        "math.OC",
        "physics.data-an",
        "stat.ML",
        "91G10 (Primary) 68T07, 91G60, 62P05 (Secondary)",
        "I.2.6; I.5.1; G.3; J.4"
      ]
    },
    {
      "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models",
      "authors": [
        "Chengao Li",
        "Hanyu Zhang",
        "Yunkun Xu",
        "Hongyan Xue",
        "Xiang Ao",
        "Qing He"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.",
      "pdf_url": "http://arxiv.org/pdf/2507.01915v1",
      "published": "2025-07-02T17:25:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01915v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
      "authors": [
        "Qiguang Chen",
        "Mingda Yang",
        "Libo Qin",
        "Jinhao Liu",
        "Zheng Yan",
        "Jiannan Guan",
        "Dengyun Peng",
        "Yiyan Ji",
        "Hanjing Li",
        "Mengkang Hu",
        "Yimeng Zhang",
        "Yihao Liang",
        "Yuhang Zhou",
        "Jiaqi Wang",
        "Zhi Chen",
        "Wanxiang Che"
      ],
      "abstract": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.",
      "pdf_url": "http://arxiv.org/pdf/2507.01903v1",
      "published": "2025-07-02T17:19:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01903v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Towards Foundation Auto-Encoders for Time-Series Anomaly Detection",
      "authors": [
        "GastÃ³n GarcÃ­a GonzÃ¡lez",
        "Pedro Casas",
        "Emilio MartÃ­nez",
        "Alicia FernÃ¡ndez"
      ],
      "abstract": "We investigate a novel approach to time-series modeling, inspired by the\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\nmean a model pretrained on massive amounts of time-series data which can learn\ncomplex temporal patterns useful for accurate modeling, forecasting, and\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\nunivariate time-series modeling, which could eventually perform properly in\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\nconcepts of FAE, and present preliminary results in different multi-dimensional\ntime-series datasets from various domains, including a real dataset from an\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.",
      "pdf_url": "http://arxiv.org/pdf/2507.01875v1",
      "published": "2025-07-02T16:39:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01875v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents",
      "authors": [
        "Sanjay Krishna Anbalagan",
        "Xinrui Nie",
        "Umesh Mohan",
        "Vijay Kumar Kanamarlapudi",
        "Anughna Kommalapati",
        "Xiaodan Zhao"
      ],
      "abstract": "Domain specific chatbot applications often involve multi step interactions,\nsuch as refining search filters, selecting multiple items, or performing\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\ndata) actions, allowing back-end systems to track user intent unambiguously. In\ncontrast, conversational agents rely on subtle language cues, which can lead to\nconfusion and incomplete context management. This paper proposes modeling these\nGUI inspired metaphors acknowledgment (submit like) and context switching\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\nreasoning as structured session data, we preserve clarity, reduce user\nconfusion, and align domain-specific chatbot interactions with back-end logic.\nWe demonstrate our approach in hotel booking and customer management scenarios,\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\nefficiency.",
      "pdf_url": "http://arxiv.org/pdf/2507.01862v1",
      "published": "2025-07-02T16:24:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01862v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.2; I.2.7"
      ]
    },
    {
      "title": "Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics",
      "authors": [
        "Yi-Dong Shen",
        "Thomas Eiter"
      ],
      "abstract": "Non-monotonic logic programming is the basis for a declarative problem\nsolving paradigm known as answer set programming (ASP). Departing from the\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\nprograms, various answer set semantics have been proposed for extensions. We\nconsider two important questions: (1) Should the minimal model property,\nconstraint monotonicity and foundedness as defined in the literature be\nmandatory conditions for an answer set semantics in general? (2) If not, what\nother properties could be considered as general principles for answer set\nsemantics? We address the two questions. First, it seems that the three\naforementioned conditions may sometimes be too strong, and we illustrate with\nexamples that enforcing them may exclude expected answer sets. Second, we\nevolve the Gelfond answer set (GAS) principles for answer set construction by\nrefining the Gelfond's rationality principle to well-supportedness, minimality\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\nprinciple of well-supportedness guarantees that every answer set is\nconstructible from if-then rules obeying a level mapping and is thus free of\ncircular justification, while the two minimality principles ensure that the\nformalism minimizes knowledge both at the level of answer sets and of world\nviews. Third, to embody the refined GAS principles, we extend the notion of\nwell-supportedness substantially to answer sets and world views, respectively.\nFourth, we define new answer set semantics in terms of the refined GAS\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\nto intuitively assess the existing answer set semantics. Finally, we analyze\nthe computational complexity.",
      "pdf_url": "http://arxiv.org/pdf/2507.01833v1",
      "published": "2025-07-02T15:47:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01833v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling",
      "authors": [
        "Tristan Torchet",
        "Christian Metzner",
        "Laura Kriener",
        "Melika Payvand"
      ],
      "abstract": "Edge devices for temporal processing demand models that capture both short-\nand long- range dynamics under tight memory constraints. While Transformers\nexcel at sequence modeling, their quadratic memory scaling with sequence length\nmakes them impractical for such settings. Recurrent Neural Networks (RNNs)\noffer constant memory but train sequentially, and Temporal Convolutional\nNetworks (TCNs), though efficient, scale memory with kernel size. To address\nthis, we propose mGRADE (mininally Gated Recurrent Architecture with Delay\nEmbedding), a hybrid-memory system that integrates a temporal 1D-convolution\nwith learnable spacings followed by a minimal gated recurrent unit (minGRU).\nThis design allows the convolutional layer to realize a flexible delay\nembedding that captures rapid temporal variations, while the recurrent module\nefficiently maintains global context with minimal memory overhead. We validate\nour approach on two synthetic tasks, demonstrating that mGRADE effectively\nseparates and preserves multi-scale temporal features. Furthermore, on\nchallenging pixel-by-pixel image classification benchmarks, mGRADE consistently\noutperforms both pure convolutional and pure recurrent counterparts using\napproximately 20% less memory footprint, highlighting its suitability for\nmemory-constrained temporal processing at the edge. This highlights mGRADE's\npromise as an efficient solution for memory-constrained multi-scale temporal\nprocessing at the edge.",
      "pdf_url": "http://arxiv.org/pdf/2507.01829v1",
      "published": "2025-07-02T15:44:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01829v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "MILP-SAT-GNN: Yet Another Neural SAT Solver",
      "authors": [
        "Franco Alberto Cardillo",
        "Hamza Khyari",
        "Umberto Straccia"
      ],
      "abstract": "We proposes a novel method that enables Graph Neural Networks (GNNs) to solve\nSAT problems by leveraging a technique developed for applying GNNs to Mixed\nInteger Linear Programming (MILP). Specifically, k-CNF formulae are mapped into\nMILP problems, which are then encoded as weighted bipartite graphs and\nsubsequently fed into a GNN for training and testing. From a theoretical\nperspective: (i) we establish permutation and equivalence invariance results,\ndemonstrating that the method produces outputs that are stable under reordering\nof clauses and variables; (ii) we identify a theoretical limitation, showing\nthat for a class of formulae called foldable formulae, standard GNNs cannot\nalways distinguish satisfiable from unsatisfiable instances; (iii) we prove a\nuniversal approximation theorem, establishing that with Random Node\nInitialization (RNI), the method can approximate SAT solving to arbitrary\nprecision on finite datasets, that is, the GNN becomes approximately sound and\ncomplete on such datasets. Furthermore, we show that for unfoldable formulae,\nthe same approximation guarantee can be achieved without the need for RNI.\nFinally, we conduct an experimental evaluation of our approach, which show\nthat, despite the simplicity of the neural architecture, the method achieves\npromising results.",
      "pdf_url": "http://arxiv.org/pdf/2507.01825v1",
      "published": "2025-07-02T15:39:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01825v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems",
      "authors": [
        "Xiaoyu Ji",
        "Jessica Shorland",
        "Joshua Shank",
        "Pascal Delpe-Brice",
        "Latanya Sweeney",
        "Jan Allebach",
        "Ali Shakouri"
      ],
      "abstract": "Small- and medium-sized manufacturers need innovative data tools but, because\nof competition and privacy concerns, often do not want to share their\nproprietary data with researchers who might be interested in helping. This\npaper introduces a privacy-preserving platform by which manufacturers may\nsafely share their data with researchers through secure methods, so that those\nresearchers then create innovative tools to solve the manufacturers' real-world\nproblems, and then provide tools that execute solutions back onto the platform\nfor others to use with privacy and confidentiality guarantees. We illustrate\nthis problem through a particular use case which addresses an important problem\nin the large-scale manufacturing of food crystals, which is that quality\ncontrol relies on image analysis tools. Previous to our research, food crystals\nin the images were manually counted, which required substantial and\ntime-consuming human efforts, but we have developed and deployed a crystal\nanalysis tool which makes this process both more rapid and accurate. The tool\nenables automatic characterization of the crystal size distribution and numbers\nfrom microscope images while the natural imperfections from the sample\npreparation are automatically removed; a machine learning model to count high\nresolution translucent crystals and agglomeration of crystals was also\ndeveloped to aid in these efforts. The resulting algorithm was then packaged\nfor real-world use on the factory floor via a web-based app secured through the\noriginating privacy-preserving platform, allowing manufacturers to use it while\nkeeping their proprietary data secure. After demonstrating this full process,\nfuture directions are also explored.",
      "pdf_url": "http://arxiv.org/pdf/2507.01808v1",
      "published": "2025-07-02T15:25:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01808v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "68T01, 68T05, 68T45, 94A60"
      ]
    },
    {
      "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs",
      "authors": [
        "Reza Arabpour",
        "Haitz SÃ¡ez de OcÃ¡riz Borde",
        "Anastasis Kratsios"
      ],
      "abstract": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.",
      "pdf_url": "http://arxiv.org/pdf/2507.01806v1",
      "published": "2025-07-02T15:24:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01806v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ]
    },
    {
      "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?",
      "authors": [
        "Tianze Hua",
        "Tian Yun",
        "Ellie Pavlick"
      ],
      "abstract": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.",
      "pdf_url": "http://arxiv.org/pdf/2507.01790v1",
      "published": "2025-07-02T15:15:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01790v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging",
      "authors": [
        "Montasir Shams",
        "Chashi Mahiul Islam",
        "Shaeke Salman",
        "Phat Tran",
        "Xiuwen Liu"
      ],
      "abstract": "Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.01788v1",
      "published": "2025-07-02T15:14:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01788v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Probing Evaluation Awareness of Language Models",
      "authors": [
        "Jord Nguyen",
        "Khiem Hoang",
        "Carlo Leonardo Attubato",
        "Felix HofstÃ¤tter"
      ],
      "abstract": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.",
      "pdf_url": "http://arxiv.org/pdf/2507.01786v1",
      "published": "2025-07-02T15:12:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01786v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining",
      "authors": [
        "Zhixun Chen",
        "Ping Guo",
        "Wenhan Han",
        "Yifan Zhang",
        "Binbin Liu",
        "Haobin Lin",
        "Fengze Liu",
        "Yan Zhao",
        "Bingni Zhang",
        "Taifeng Wang",
        "Yin Zheng",
        "Meng Fang"
      ],
      "abstract": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.",
      "pdf_url": "http://arxiv.org/pdf/2507.01785v1",
      "published": "2025-07-02T15:11:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01785v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification",
      "authors": [
        "Dalia RodrÃ­guez-Salas",
        "Christian Riess"
      ],
      "abstract": "We introduce BranchNet, a neuro-symbolic learning framework that transforms\ndecision tree ensembles into sparse, partially connected neural networks. Each\nbranch, defined as a decision path from root to a parent of leaves, is mapped\nto a hidden neuron, preserving symbolic structure while enabling gradient-based\noptimization. The resulting models are compact, interpretable, and require no\nmanual architecture tuning. Evaluated on a suite of structured multi-class\nclassification benchmarks, BranchNet consistently outperforms XGBoost in\naccuracy, with statistically significant gains. We detail the architecture,\ntraining procedure, and sparsity dynamics, and discuss the model's strengths in\nsymbolic interpretability as well as its current limitations, particularly on\nbinary tasks where further adaptive calibration may be beneficial.",
      "pdf_url": "http://arxiv.org/pdf/2507.01781v1",
      "published": "2025-07-02T15:07:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01781v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T07 (Primary) 62H30, 68T05 (Secondary)"
      ]
    },
    {
      "title": "GPU-based complete search for nonlinear minimization subject to bounds",
      "authors": [
        "Guanglu Zhang",
        "Qihang Shan",
        "Jonathan Cagan"
      ],
      "abstract": "This paper introduces a GPU-based complete search method to enclose the\nglobal minimum of a nonlinear function subject to simple bounds on the\nvariables. Using interval analysis, coupled with the computational power and\narchitecture of GPU, the method iteratively rules out the regions in the search\ndomain where the global minimum cannot exist and leaves a finite set of regions\nwhere the global minimum must exist. For effectiveness, because of the rigor of\ninterval analysis, the method is guaranteed to enclose the global minimum of\nthe nonlinear function even in the presence of rounding errors. For efficiency,\nthe method employs a novel GPU-based single program, single data parallel\nprogramming style to circumvent major GPU performance bottlenecks, and a\nvariable cycling technique is also integrated into the method to reduce\ncomputational cost when minimizing large-scale nonlinear functions. The method\nis validated by minimizing 10 multimodal benchmark test functions with scalable\ndimensions, including the well-known Ackley function, Griewank function, Levy\nfunction, and Rastrigin function. These benchmark test functions represent\ngrand challenges of global optimization, and enclosing the guaranteed global\nminimum of these benchmark test functions with more than 80 dimensions has not\nbeen reported in the literature. Our method completely searches the feasible\ndomain and successfully encloses the guaranteed global minimum of these 10\nbenchmark test functions with up to 10,000 dimensions using only one GPU in a\nreasonable computation time, far exceeding the reported results in the\nliterature due to the unique method design and implementation based on GPU\narchitecture.",
      "pdf_url": "http://arxiv.org/pdf/2507.01770v1",
      "published": "2025-07-02T14:54:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01770v1",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.DC",
        "cs.MS",
        "cs.NA",
        "math.OC",
        "65G20, 65G30, 65G40, 90C06, 90C26, 90C30",
        "G.1.6; G.4"
      ]
    },
    {
      "title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage",
      "authors": [
        "Nicolas Salvy",
        "Hugues Talbot",
        "Bertrand Thirion"
      ],
      "abstract": "Although generative models have made remarkable progress in recent years,\ntheir use in critical applications has been hindered by their incapacity to\nreliably evaluate sample quality. Quality refers to at least two complementary\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\ninterpretable values due to an absence of calibration or insufficient\nrobustness to outliers. To address these shortcomings, we introduce two novel\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\nThrough analytical and empirical calibration, these metrics exhibit linear\nscore degradation as the proportion of poor samples increases. Thus, they can\nbe straightforwardly interpreted as equivalent proportions of good samples.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nClipped Density and Clipped Coverage outperform existing methods in terms of\nrobustness, sensitivity, and interpretability for evaluating generative models.",
      "pdf_url": "http://arxiv.org/pdf/2507.01761v1",
      "published": "2025-07-02T14:40:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01761v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training",
      "authors": [
        "Ismail Labiad",
        "Mathurin Videau",
        "Matthieu Kowalski",
        "Marc Schoenauer",
        "Alessandro Leite",
        "Julia Kempe",
        "Olivier Teytaud"
      ],
      "abstract": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.",
      "pdf_url": "http://arxiv.org/pdf/2507.01752v1",
      "published": "2025-07-02T14:29:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01752v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ]
    },
    {
      "title": "Joint Matching and Pricing for Crowd-shipping with In-store Customers",
      "authors": [
        "Arash Dehghan",
        "Mucahit Cevik",
        "Merve Bodur",
        "Bissan Ghaddar"
      ],
      "abstract": "This paper examines the use of in-store customers as delivery couriers in a\ncentralized crowd-shipping system, targeting the growing need for efficient\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\nsetting where shoppers are offered compensation to deliver time-sensitive\nonline orders. To manage this process, we propose a Markov Decision Process\n(MDP) model that captures key uncertainties, including the stochastic arrival\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\nrouting and accounts for offer acceptance uncertainty, aligning more closely\nwith real-world operations. Experimental results demonstrate that the\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\nefficiency, with up to 6.7\\% savings over NeurADP with fixed pricing and\napproximately 18\\% over myopic baselines. We also show that allowing flexible\ndelivery delays and enabling multi-destination routing further reduces\noperational costs by 8\\% and 17\\%, respectively. These findings underscore the\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\noffer practical guidance for urban logistics operators.",
      "pdf_url": "http://arxiv.org/pdf/2507.01749v1",
      "published": "2025-07-02T14:27:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01749v1",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving",
      "authors": [
        "Kai Chen",
        "Ruiyuan Gao",
        "Lanqing Hong",
        "Hang Xu",
        "Xu Jia",
        "Holger Caesar",
        "Dengxin Dai",
        "Bingbing Liu",
        "Dzmitry Tsishkou",
        "Songcen Xu",
        "Chunjing Xu",
        "Qiang Xu",
        "Huchuan Lu",
        "Dit-Yan Yeung"
      ],
      "abstract": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.",
      "pdf_url": "http://arxiv.org/pdf/2507.01735v1",
      "published": "2025-07-02T14:10:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01735v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America",
      "authors": [
        "Dorian Peters",
        "Fernanda Espinoza",
        "Marco da Re",
        "Guido Ivetta",
        "Luciana Benotti",
        "Rafael A. Calvo"
      ],
      "abstract": "There is justifiable interest in leveraging conversational AI (CAI) for\nhealth across the majority world, but to be effective, CAI must respond\nappropriately within culturally and linguistically diverse contexts. Therefore,\nwe need ways to address the fact that current LLMs exclude many lived\nexperiences globally. Various advances are underway which focus on top-down\napproaches and increasing training data. In this paper, we aim to complement\nthese with a bottom-up locally-grounded approach based on qualitative data\ncollected during participatory workshops in Latin America. Our goal is to\nconstruct a rich and human-centred understanding of: a) potential areas of\ncultural misalignment in digital health; b) regional perspectives on chatbots\nfor health and c)strategies for creating culturally-appropriate CAI; with a\nfocus on the understudied Latin American context. Our findings show that\nacademic boundaries on notions of culture lose meaning at the ground level and\ntechnologies will need to engage with a broader framework; one that\nencapsulates the way economics, politics, geography and local logistics are\nentangled in cultural experience. To this end, we introduce a framework for\n'Pluriversal Conversational AI for Health' which allows for the possibility\nthat more relationality and tolerance, rather than just more data, may be\ncalled for.",
      "pdf_url": "http://arxiv.org/pdf/2507.01719v1",
      "published": "2025-07-02T13:48:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01719v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI",
      "authors": [
        "Gopichand Kanumolu",
        "Ashok Urlana",
        "Charaka Vinayak Kumar",
        "Bala Mallikarjunarao Garlapati"
      ],
      "abstract": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.",
      "pdf_url": "http://arxiv.org/pdf/2507.01717v1",
      "published": "2025-07-02T13:47:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01717v1",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness",
      "authors": [
        "Zixin Chen",
        "Hongzhan Lin",
        "Kaixin Li",
        "Ziyang Luo",
        "Zhen Ye",
        "Guang Chen",
        "Zhiyong Huang",
        "Jing Ma"
      ],
      "abstract": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.",
      "pdf_url": "http://arxiv.org/pdf/2507.01702v1",
      "published": "2025-07-02T13:32:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01702v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture",
      "authors": [
        "Bochen Han",
        "Songmao Zhang"
      ],
      "abstract": "In this paper, we propose to incorporate the blackboard architecture into LLM\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\nthe information and others' messages during the whole problem-solving process,\n(2) agents that will take actions are selected based on the current content of\nthe blackboard, and (3) the selection and execution round is repeated until a\nconsensus is reached on the blackboard. We develop the first implementation of\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\nmathematical datasets. The results show that our system can be competitive with\nthe SOTA static and dynamic MASs by achieving the best average performance, and\nat the same time manage to spend less tokens. Our proposal has the potential to\nenable complex and dynamic problem-solving where well-defined structures or\nworkflows are unavailable.",
      "pdf_url": "http://arxiv.org/pdf/2507.01701v1",
      "published": "2025-07-02T13:30:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01701v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ]
    },
    {
      "title": "Relational Causal Discovery with Latent Confounders",
      "authors": [
        "Andrea Piras",
        "Matteo Negro",
        "Ragib Ahsan",
        "David Arbour",
        "Elena Zheleva"
      ],
      "abstract": "Estimating causal effects from real-world relational data can be challenging\nwhen the underlying causal model and potential confounders are unknown. While\nseveral causal discovery algorithms exist for learning causal models with\nlatent confounders from data, they assume that the data is independent and\nidentically distributed (i.i.d.) and are not well-suited for learning from\nrelational data. Similarly, existing relational causal discovery algorithms\nassume causal sufficiency, which is unrealistic for many real-world datasets.\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\nalgorithm for relational data with latent confounders. Our work builds upon the\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\nand it defines new graphical models, necessary to support causal discovery in\nrelational domains. We also establish soundness and completeness guarantees for\nrelational d-separation with latent confounders. We present experimental\nresults demonstrating the effectiveness of RelFCI in identifying the correct\ncausal structure in relational causal models with latent confounders.",
      "pdf_url": "http://arxiv.org/pdf/2507.01700v1",
      "published": "2025-07-02T13:29:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01700v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs",
      "authors": [
        "Adrians Skapars",
        "Edoardo Manino",
        "Youcheng Sun",
        "Lucas C. Cordeiro"
      ],
      "abstract": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879.",
      "pdf_url": "http://arxiv.org/pdf/2507.01693v1",
      "published": "2025-07-02T13:20:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01693v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling",
      "authors": [
        "Zeyu Huang",
        "Tianhao Cheng",
        "Zihan Qiu",
        "Zili Wang",
        "Yinghui Xu",
        "Edoardo M. Ponti",
        "Ivan Titov"
      ],
      "abstract": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.",
      "pdf_url": "http://arxiv.org/pdf/2507.01679v1",
      "published": "2025-07-02T13:04:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01679v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization",
      "authors": [
        "Giuseppe Ruggeri",
        "Renzo Andri",
        "Daniele Jahier Pagliari",
        "Lukas Cavigelli"
      ],
      "abstract": "Deep Recommender Models (DLRMs) inference is a fundamental AI workload\naccounting for more than 79% of the total AI workload in Meta's data centers.\nDLRMs' performance bottleneck is found in the embedding layers, which perform\nmany random memory accesses to retrieve small embedding vectors from tables of\nvarious sizes. We propose the design of tailored data flows to speedup\nembedding look-ups. Namely, we propose four strategies to look up an embedding\ntable effectively on one core, and a framework to automatically map the tables\nasymmetrically to the multiple cores of a SoC. We assess the effectiveness of\nour method using the Huawei Ascend AI accelerators, comparing it with the\ndefault Ascend compiler, and we perform high-level comparisons with Nvidia\nA100. Results show a speed-up varying from 1.5x up to 6.5x for real workload\ndistributions, and more than 20x for extremely unbalanced distributions.\nFurthermore, the method proves to be much more independent of the query\ndistribution than the baseline.",
      "pdf_url": "http://arxiv.org/pdf/2507.01676v1",
      "published": "2025-07-02T13:00:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01676v1",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR",
        "cs.IR",
        "C.4; D.1.3; H.3.3; H.3.4"
      ]
    },
    {
      "title": "Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis",
      "authors": [
        "Gjorgjina Cenikj",
        "GaÅ¡per Petelin",
        "Tome Eftimov"
      ],
      "abstract": "The field of numerical optimization has recently seen a surge in the\ndevelopment of \"novel\" metaheuristic algorithms, inspired by metaphors derived\nfrom natural or human-made processes, which have been widely criticized for\nobscuring meaningful innovations and failing to distinguish themselves from\nexisting approaches. Aiming to address these concerns, we investigate the\napplicability of statistical tests for comparing algorithms based on their\nsearch behavior. We utilize the cross-match statistical test to compare\nmultivariate distributions and assess the solutions produced by 114 algorithms\nfrom the MEALPY library. These findings are incorporated into an empirical\nanalysis aiming to identify algorithms with similar search behaviors.",
      "pdf_url": "http://arxiv.org/pdf/2507.01668v1",
      "published": "2025-07-02T12:51:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01668v1",
      "categories": [
        "cs.NE",
        "cs.AI"
      ]
    },
    {
      "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training",
      "authors": [
        "Zhenyu Han",
        "Ansheng You",
        "Haibo Wang",
        "Kui Luo",
        "Guang Yang",
        "Wenqi Shi",
        "Menglong Chen",
        "Sicheng Zhang",
        "Zeshun Lan",
        "Chunshi Deng",
        "Huazhong Ji",
        "Wenjie Liu",
        "Yu Huang",
        "Yixiang Zhang",
        "Chenyi Pan",
        "Jing Wang",
        "Xin Huang",
        "Chunsheng Li",
        "Jianping Wu"
      ],
      "abstract": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.",
      "pdf_url": "http://arxiv.org/pdf/2507.01663v1",
      "published": "2025-07-02T12:45:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01663v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective",
      "authors": [
        "Yuxin Mao",
        "Zhen Qin",
        "Jinxing Zhou",
        "Hui Deng",
        "Xuyang Shen",
        "Bin Fan",
        "Jing Zhang",
        "Yiran Zhong",
        "Yuchao Dai"
      ],
      "abstract": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
      "pdf_url": "http://arxiv.org/pdf/2507.01652v1",
      "published": "2025-07-02T12:27:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01652v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients",
      "authors": [
        "Yoav Gelberg",
        "Yam Eitan",
        "Aviv Navon",
        "Aviv Shamsian",
        "Theo",
        "Putterman",
        "Michael Bronstein",
        "Haggai Maron"
      ],
      "abstract": "Gradients of neural networks encode valuable information for optimization,\nediting, and analysis of models. Therefore, practitioners often treat gradients\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\nworks explore learning algorithms that operate directly on gradients but use\narchitectures that are not specifically designed for gradient processing,\nlimiting their applicability. In this paper, we present a principled approach\nfor designing architectures that process gradients. Our approach is guided by\nthree principles: (1) equivariant design that preserves neuron permutation\nsymmetries, (2) processing sets of gradients across multiple data points to\ncapture curvature information, and (3) efficient gradient representation\nthrough rank-1 decomposition. Based on these principles, we introduce\nGradMetaNet, a novel architecture for learning on gradients, constructed from\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\nshow that previous approaches cannot approximate natural gradient-based\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\non a diverse set of gradient-based tasks on MLPs and transformers, such as\nlearned optimization, INR editing, and estimating loss landscape curvature.",
      "pdf_url": "http://arxiv.org/pdf/2507.01649v1",
      "published": "2025-07-02T12:22:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01649v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance",
      "authors": [
        "Ana Nikolikj",
        "Gabriela Ochoa",
        "Tome Eftimov"
      ],
      "abstract": "We present an analysis of landscape features for predicting the performance\nof multi-objective combinatorial optimization algorithms. We consider features\nfrom the recently proposed compressed Pareto Local Optimal Solutions Networks\n(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a\nset of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness\nand objective correlation. We consider the performance of three algorithms --\nPareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and\nNon-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and\nhypervolume metrics. Our tailored analysis reveals feature combinations that\ninfluence algorithm performance specific to certain landscapes. This study\nprovides deeper insights into feature importance, tailored to specific\nrmnk-landscapes and algorithms.",
      "pdf_url": "http://arxiv.org/pdf/2507.01638v1",
      "published": "2025-07-02T12:11:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01638v1",
      "categories": [
        "cs.NE",
        "cs.AI"
      ]
    },
    {
      "title": "Depth Anything at Any Condition",
      "authors": [
        "Boyuan Sun",
        "Modi Jin",
        "Bowen Yin",
        "Qibin Hou"
      ],
      "abstract": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
      "pdf_url": "http://arxiv.org/pdf/2507.01634v1",
      "published": "2025-07-02T12:05:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01634v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation",
      "authors": [
        "Camille Billouard",
        "Dawa Derksen",
        "Alexandre Constantin",
        "Bruno Vallet"
      ],
      "abstract": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.",
      "pdf_url": "http://arxiv.org/pdf/2507.01631v1",
      "published": "2025-07-02T11:59:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01631v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ]
    },
    {
      "title": "Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss",
      "authors": [
        "Yuxiao Wang",
        "Yu Lei",
        "Zhenao Wei",
        "Weiying Xue",
        "Xinyu Jiang",
        "Nan Zhuang",
        "Qi Liu"
      ],
      "abstract": "The task of Human-Object conTact (HOT) detection involves identifying the\nspecific areas of the human body that are touching objects. Nevertheless,\ncurrent models are restricted to just one type of image, often leading to too\nmuch segmentation in areas with little interaction, and struggling to maintain\ncategory consistency within specific regions. To tackle this issue, a HOT\nframework, termed \\textbf{P3HOT}, is proposed, which blends \\textbf{P}rompt\nguidance and human \\textbf{P}roximal \\textbf{P}erception. To begin with, we\nutilize a semantic-driven prompt mechanism to direct the network's attention\ntowards the relevant regions based on the correlation between image and text.\nThen a human proximal perception mechanism is employed to dynamically perceive\nkey depth range around the human, using learnable parameters to effectively\neliminate regions where interactions are not expected. Calculating depth\nresolves the uncertainty of the overlap between humans and objects in a 2D\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\nthe shortcomings of existing methods in addressing negative samples.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art performance in four metrics across two benchmark datasets.\nSpecifically, our model achieves an improvement of \\textbf{0.7}$\\uparrow$,\n\\textbf{2.0}$\\uparrow$, \\textbf{1.6}$\\uparrow$, and \\textbf{11.0}$\\uparrow$ in\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\ndataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.",
      "pdf_url": "http://arxiv.org/pdf/2507.01630v1",
      "published": "2025-07-02T11:59:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01630v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Enhanced Influence-aware Group Recommendation for Online Media Propagation",
      "authors": [
        "Chengkun He",
        "Xiangmin Zhou",
        "Chen Wang",
        "Longbing Cao",
        "Jie Shao",
        "Xiaodong Li",
        "Guang Xu",
        "Carrie Jinqiu Hu",
        "Zahir Tari"
      ],
      "abstract": "Group recommendation over social media streams has attracted significant\nattention due to its wide applications in domains such as e-commerce,\nentertainment, and online news broadcasting. By leveraging social connections\nand group behaviours, group recommendation (GR) aims to provide more accurate\nand engaging content to a set of users rather than individuals. Recently,\ninfluence-aware GR has emerged as a promising direction, as it considers the\nimpact of social influence on group decision-making. In earlier work, we\nproposed Influence-aware Group Recommendation (IGR) to solve this task.\nHowever, this task remains challenging due to three key factors: the large and\never-growing scale of social graphs, the inherently dynamic nature of influence\npropagation within user groups, and the high computational overhead of\nreal-time group-item matching.\n  To tackle these issues, we propose an Enhanced Influence-aware Group\nRecommendation (EIGR) framework. First, we introduce a Graph Extraction-based\nSampling (GES) strategy to minimise redundancy across multiple temporal social\ngraphs and effectively capture the evolving dynamics of both groups and items.\nSecond, we design a novel DYnamic Independent Cascade (DYIC) model to predict\nhow influence propagates over time across social items and user groups.\nFinally, we develop a two-level hash-based User Group Index (UG-Index) to\nefficiently organise user groups and enable real-time recommendation\ngeneration. Extensive experiments on real-world datasets demonstrate that our\nproposed framework, EIGR, consistently outperforms state-of-the-art baselines\nin both effectiveness and efficiency.",
      "pdf_url": "http://arxiv.org/pdf/2507.01616v1",
      "published": "2025-07-02T11:34:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01616v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB"
      ]
    },
    {
      "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems",
      "authors": [
        "Quentin Le Roux",
        "Yannick Teglia",
        "Teddy Furon",
        "Philippe Loubet-Moundi",
        "Eric Bourbao"
      ],
      "abstract": "The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures.",
      "pdf_url": "http://arxiv.org/pdf/2507.01607v1",
      "published": "2025-07-02T11:21:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01607v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems",
      "authors": [
        "Zhaoyan Sun",
        "Jiayi Wang",
        "Xinyang Zhao",
        "Jiachi Wang",
        "Guoliang Li"
      ],
      "abstract": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.01599v1",
      "published": "2025-07-02T11:04:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01599v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning",
      "authors": [
        "Yuehang Si",
        "Zefan Zeng",
        "Jincai Huang",
        "Qing Cheng"
      ],
      "abstract": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.",
      "pdf_url": "http://arxiv.org/pdf/2507.01597v1",
      "published": "2025-07-02T11:02:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01597v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring",
      "authors": [
        "Ameer Hamza",
        "Zuhaib Hussain But",
        "Umar Arif",
        "Samiya",
        "M. Abdullah Asad",
        "Muhammad Naeem"
      ],
      "abstract": "This study presents a novel classroom surveillance system that integrates\nmultiple modalities, including drowsiness, tracking of mobile phone usage, and\nface recognition,to assess student attentiveness with enhanced precision.The\nsystem leverages the YOLOv8 model to detect both mobile phone and sleep\nusage,(Ghatge et al., 2024) while facial recognition is achieved through\nLResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These\nmodels work in synergy to provide comprehensive, real-time monitoring, offering\ninsights into student engagement and behavior.(S et al., 2023) The framework is\ntrained on specialized datasets, such as the RMFD dataset for face recognition\nand a Roboflow dataset for mobile phone detection. The extensive evaluation of\nthe system shows promising results. Sleep detection achieves 97. 42% mAP@50,\nface recognition achieves 86. 45% validation accuracy and mobile phone\ndetection reach 85. 89% mAP@50. The system is implemented within a core PHP web\napplication and utilizes ESP32-CAM hardware for seamless data capture.(Neto et\nal., 2024) This integrated approach not only enhances classroom monitoring, but\nalso ensures automatic attendance recording via face recognition as students\nremain seated in the classroom, offering scalability for diverse educational\nenvironments.(Banada,2025)",
      "pdf_url": "http://arxiv.org/pdf/2507.01590v1",
      "published": "2025-07-02T10:59:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01590v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder",
      "authors": [
        "Jing Luo",
        "Xinyu Yang",
        "Jie Wei"
      ],
      "abstract": "The creativity of classical music arises not only from composers who craft\nthe musical sheets but also from performers who interpret the static notations\nwith expressive nuances. This paper addresses the challenge of generating\nclassical piano performances from scratch, aiming to emulate the dual roles of\ncomposer and pianist in the creative process. We introduce the Expressive\nCompound Word (ECP) representation, which effectively captures both the\nmetrical structure and expressive nuances of classical performances. Building\non this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a\nmodel featuring two branches: a Vector Quantized Variational AutoEncoder\n(VQ-VAE) branch that generates score-related content, representing the\nComposer, and a vanilla VAE branch that produces expressive details, fulfilling\nthe role of Pianist. These branches are jointly trained with similar Seq2Seq\narchitectures, leveraging a multiscale encoder to capture beat-level contextual\ninformation and an orthogonal Transformer decoder for efficient compound tokens\ndecoding. Both objective and subjective evaluations demonstrate that XMVAE\ngenerates classical performances with superior musical quality compared to\nstate-of-the-art models. Furthermore, pretraining the Composer branch on extra\nmusical score datasets contribute to a significant performance gain.",
      "pdf_url": "http://arxiv.org/pdf/2507.01582v1",
      "published": "2025-07-02T10:54:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01582v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ]
    },
    {
      "title": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware",
      "authors": [
        "Marco Giordano",
        "Stefano Giacomelli",
        "Claudia Rinaldi",
        "Fabio Graziosi"
      ],
      "abstract": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices.",
      "pdf_url": "http://arxiv.org/pdf/2507.01563v1",
      "published": "2025-07-02T10:27:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01563v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "68T07 (Primary), 68T10 (Secondary)",
        "B.1.5; B.4.5; C.3; C.4; I.2; K.4; J.2"
      ]
    },
    {
      "title": "Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning",
      "authors": [
        "Wu Fei",
        "Hao Kong",
        "Shuxian Liang",
        "Yang Lin",
        "Yibo Yang",
        "Jing Tang",
        "Lei Chen",
        "Xiansheng Hua"
      ],
      "abstract": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.",
      "pdf_url": "http://arxiv.org/pdf/2507.01551v2",
      "published": "2025-07-02T10:05:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.01551v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    }
  ]
}